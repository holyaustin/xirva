[{"id": "1606.00226", "submitter": "Richard Combes", "authors": "Thomas Bonald and Richard Combes", "title": "A Minimax Optimal Algorithm for Crowdsourcing", "comments": "19 pages, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of accurately estimating the reliability of workers\nbased on noisy labels they provide, which is a fundamental question in\ncrowdsourcing. We propose a novel lower bound on the minimax estimation error\nwhich applies to any estimation procedure. We further propose Triangular\nEstimation (TE), an algorithm for estimating the reliability of workers. TE has\nlow complexity, may be implemented in a streaming setting when labels are\nprovided by workers in real time, and does not rely on an iterative procedure.\nWe further prove that TE is minimax optimal and matches our lower bound. We\nconclude by assessing the performance of TE and other state-of-the-art\nalgorithms on both synthetic and real-world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 11:18:21 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 16:19:38 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Bonald", "Thomas", ""], ["Combes", "Richard", ""]]}, {"id": "1606.00370", "submitter": "Sarah Ostadabbas", "authors": "Maria S. Perez-Rosero, Behnaz Rezaei, Murat Akcakaya, and Sarah\n  Ostadabbas", "title": "Decoding Emotional Experience through Physiological Signal Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing consensus among re- searchers that making a computer\nemotionally intelligent with the ability to decode human affective states would\nallow a more meaningful and natural way of human-computer interactions (HCIs).\nOne unobtrusive and non-invasive way of recognizing human affective states\nentails the exploration of how physiological signals vary under different\nemotional experiences. In particular, this paper explores the correlation\nbetween autonomically-mediated changes in multimodal body signals and discrete\nemotional states. In order to fully exploit the information in each modality,\nwe have provided an innovative classification approach for three specific\nphysiological signals including Electromyogram (EMG), Blood Volume Pressure\n(BVP) and Galvanic Skin Response (GSR). These signals are analyzed as inputs to\nan emotion recognition paradigm based on fusion of a series of weak learners.\nOur proposed classification approach showed 88.1% recognition accuracy, which\noutperformed the conventional Support Vector Machine (SVM) classifier with 17%\naccuracy improvement. Furthermore, in order to avoid information redundancy and\nthe resultant over-fitting, a feature reduction method is proposed based on a\ncorrelation analysis to optimize the number of features required for training\nand validating each weak learner. Results showed that despite the feature space\ndimensionality reduction from 27 to 18 features, our methodology preserved the\nrecognition accuracy of about 85.0%. This reduction in complexity will get us\none step closer towards embedding this human emotion encoder in the wireless\nand wearable HCI platforms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 17:52:30 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Perez-Rosero", "Maria S.", ""], ["Rezaei", "Behnaz", ""], ["Akcakaya", "Murat", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "1606.00401", "submitter": "Benjamin Cowley PhD", "authors": "Benjamin Ultan Cowley", "title": "How to advance general game playing artificial intelligence by player\n  modelling", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General game playing artificial intelligence has recently seen important\nadvances due to the various techniques known as 'deep learning'. However the\nadvances conceal equally important limitations in their reliance on: massive\ndata sets; fortuitously constructed problems; and absence of any human-level\ncomplexity, including other human opponents. On the other hand, deep learning\nsystems which do beat human champions, such as in Go, do not generalise well.\nThe power of deep learning simultaneously exposes its weakness. Given that deep\nlearning is mostly clever reconfigurations of well-established methods, moving\nbeyond the state of art calls for forward-thinking visionary solutions, not\njust more of the same. I present the argument that general game playing\nartificial intelligence will require a generalised player model. This is\nbecause games are inherently human artefacts which therefore, as a class of\nproblems, contain cases which require a human-style problem solving approach. I\nrelate this argument to the performance of state of art general game playing\nagents. I then describe a concept for a formal category theoretic basis to a\ngeneralised player model. This formal model approach integrates my existing\n'Behavlets' method for psychologically-derived player modelling:\n  Cowley, B., Charles, D. (2016). Behavlets: a Method for Practical Player\nModelling using Psychology-Based Player Traits and Domain Specific Features.\nUser Modeling and User-Adapted Interaction, 26(2), 257-306.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 19:07:48 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 13:15:53 GMT"}, {"version": "v3", "created": "Tue, 21 Jun 2016 12:18:24 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Cowley", "Benjamin Ultan", ""]]}, {"id": "1606.00474", "submitter": "Michael Grupp", "authors": "Michael Grupp, Philipp Kopp, Patrik Huber, Matthias R\\\"atsch", "title": "A 3D Face Modelling Approach for Pose-Invariant Face Recognition in a\n  Human-Robot Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face analysis techniques have become a crucial component of human-machine\ninteraction in the fields of assistive and humanoid robotics. However, the\nvariations in head-pose that arise naturally in these environments are still a\ngreat challenge. In this paper, we present a real-time capable 3D face\nmodelling framework for 2D in-the-wild images that is applicable for robotics.\nThe fitting of the 3D Morphable Model is based exclusively on automatically\ndetected landmarks. After fitting, the face can be corrected in pose and\ntransformed back to a frontal 2D representation that is more suitable for face\nrecognition. We conduct face recognition experiments with non-frontal images\nfrom the MUCT database and uncontrolled, in the wild images from the PaSC\ndatabase, the most challenging face recognition database to date, showing an\nimproved performance. Finally, we present our SCITOS G5 robot system, which\nincorporates our framework as a means of image pre-processing for face\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 21:28:27 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Grupp", "Michael", ""], ["Kopp", "Philipp", ""], ["Huber", "Patrik", ""], ["R\u00e4tsch", "Matthias", ""]]}, {"id": "1606.00822", "submitter": "Mehdi Ghayoumi", "authors": "Mehdi Ghayoumi, Arvind K Bansal", "title": "Unifying Geometric Features and Facial Action Units for Improved\n  Performance of Facial Expression Analysis", "comments": "8 pages, ISBN: 978-1-61804-285-9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous approaches to model and analyze facial expression analysis use three\ndifferent techniques: facial action units, geometric features and graph based\nmodelling. However, previous approaches have treated these technique\nseparately. There is an interrelationship between these techniques. The facial\nexpression analysis is significantly improved by utilizing these mappings\nbetween major geometric features involved in facial expressions and the subset\nof facial action units whose presence or absence are unique to a facial\nexpression. This paper combines dimension reduction techniques and image\nclassification with search space pruning achieved by this unique subset of\nfacial action units to significantly prune the search space. The performance\nresults on the publicly facial expression database shows an improvement in\nperformance by 70% over time while maintaining the emotion recognition\ncorrectness.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 19:43:29 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Ghayoumi", "Mehdi", ""], ["Bansal", "Arvind K", ""]]}, {"id": "1606.01292", "submitter": "Kaisheng Yao", "authors": "Kaisheng Yao and Baolin Peng and Geoffrey Zweig and Kam-Fai Wong", "title": "An Attentional Neural Conversation Model with Improved Specificity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a neural conversation model for conducting\ndialogues. We demonstrate the use of this model to generate help desk\nresponses, where users are asking questions about PC applications. Our model is\ndistinguished by two characteristics. First, it models intention across turns\nwith a recurrent network, and incorporates an attention model that is\nconditioned on the representation of intention. Secondly, it avoids generating\nnon-specific responses by incorporating an IDF term in the objective function.\nThe model is evaluated both as a pure generation model in which a help-desk\nresponse is generated from scratch, and as a retrieval model with performance\nmeasured using recall rates of the correct response. Experimental results\nindicate that the model outperforms previously proposed neural conversation\narchitectures, and that using specificity in the objective function\nsignificantly improves performances for both generation and retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 22:26:01 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Yao", "Kaisheng", ""], ["Peng", "Baolin", ""], ["Zweig", "Geoffrey", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "1606.01363", "submitter": "Marko Ter\\\"as", "authors": "Marko Ter\\\"as, Hanna Ter\\\"as and Torsten Reiners", "title": "The Lived User Experience of Virtual Environments: Initial Steps of a\n  Phenomenological Analysis in a Safety Training Setting", "comments": "ISBN# 978-0-646-95337-3 Presented at the Australasian Conference on\n  Information Systems 2015 (arXiv:1605.01032)", "journal-ref": null, "doi": null, "report-no": "ACIS/2015/118", "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual environments (VEs) are making their way into various sectors of life\nto enhance and support human activity, including learning. VEs have been used\nin various contexts for training, and in many cases they are designed to model\nor simulate - as accurately and authentically as possible - a specific work\ncontext. In striving for authenticity, visual and representative realism tends\nto receive most of the development input, despite of several studies that\nchallenge its importance. New training avenues have raised the importance of\nrigorous phenomenological descriptions for a deeper understanding of user\nexperience in the actual context of use. This paper reports the preliminary\nsteps in a phenomenological analysis of how employees working in actual\nhazardous settings experience virtual safety training environments. Such\nopen-ended research project can reveal new aspects of user experience that can\nadvice the development and evaluation of human-computer interaction in digital\ntechnology-enhanced training contexts.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 10:16:09 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Ter\u00e4s", "Marko", ""], ["Ter\u00e4s", "Hanna", ""], ["Reiners", "Torsten", ""]]}, {"id": "1606.01453", "submitter": "Chiranjoy Chattopadhyay", "authors": "Pratik Kalshetti, Manas Bundele, Parag Rahangdale, Dinesh Jangra,\n  Chiranjoy Chattopadhyay, Gaurav Harit, Abhay Elhence", "title": "An Interactive Medical Image Segmentation Framework Using Iterative\n  Refinement", "comments": "19 pages, 19 figures, Submitted for review in Computers in Biology\n  and Medicine", "journal-ref": null, "doi": "10.1016/j.compbiomed.2017.02.002", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is often performed on medical images for identifying\ndiseases in clinical evaluation. Hence it has become one of the major research\nareas. Conventional image segmentation techniques are unable to provide\nsatisfactory segmentation results for medical images as they contain\nirregularities. They need to be pre-processed before segmentation. In order to\nobtain the most suitable method for medical image segmentation, we propose a\ntwo stage algorithm. The first stage automatically generates a binary marker\nimage of the region of interest using mathematical morphology. This marker\nserves as the mask image for the second stage which uses GrabCut on the input\nimage thus resulting in an efficient segmented result. The obtained result can\nbe further refined by user interaction which can be done using the Graphical\nUser Interface (GUI). Experimental results show that the proposed method is\naccurate and provides satisfactory segmentation results with minimum user\ninteraction on medical as well as natural images.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 02:35:53 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Kalshetti", "Pratik", ""], ["Bundele", "Manas", ""], ["Rahangdale", "Parag", ""], ["Jangra", "Dinesh", ""], ["Chattopadhyay", "Chiranjoy", ""], ["Harit", "Gaurav", ""], ["Elhence", "Abhay", ""]]}, {"id": "1606.01836", "submitter": "Tauhid Zaman", "authors": "Carter Mundell, Juan Pablo Vielma, and Tauhid Zaman", "title": "Predicting Performance Under Stressful Conditions Using Galvanic Skin\n  Response", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of the availability of wearable biosensors has created the\nopportunity for using biological signals to measure worker performance. An\nimportant question is how to use such signals to not just measure, but actually\npredict worker performance on a task under stressful and potentially high risk\nconditions. Here we show that the biological signal known as galvanic skin\nresponse (GSR) allows such a prediction. We conduct an experiment where\nsubjects answer arithmetic questions under low and high stress conditions while\nhaving their GSR monitored using a wearable biosensor. Using only the GSR\nmeasured under low stress conditions, we are able to predict which subjects\nwill perform well under high stress conditions, achieving an area under the\ncurve (AUC) of 0.76. If we try to make similar predictions without using any\nbiometric signals, the AUC barely exceeds 0.50. Our result suggests that\nperformance in high stress conditions can be predicted using signals obtained\nfrom wearable biosensors in low stress conditions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 17:22:16 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Mundell", "Carter", ""], ["Vielma", "Juan Pablo", ""], ["Zaman", "Tauhid", ""]]}, {"id": "1606.02041", "submitter": "Nils Hammerla", "authors": "Katherine Middleton, Mobasher Butt, Nils Hammerla, Steven Hamblin,\n  Karan Mehta, Ali Parsa", "title": "Sorting out symptoms: design and evaluation of the 'babylon check'\n  automated triage system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior to seeking professional medical care it is increasingly common for\npatients to use online resources such as automated symptom checkers. Many such\nsystems attempt to provide a differential diagnosis based on the symptoms\nelucidated from the user, which may lead to anxiety if life or limb-threatening\nconditions are part of the list, a phenomenon termed 'cyberchondria' [1].\nSystems that provide advice on where to seek help, rather than a diagnosis, are\nequally popular, and in our view provide the most useful information. In this\ntechnical report we describe how such a triage system can be modelled\ncomputationally, how medical insights can be translated into triage flows, and\nhow such systems can be validated and tested. We present babylon check, our\ncommercially deployed automated triage system, as a case study, and illustrate\nits performance in a large, semi-naturalistic deployment study.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 06:55:42 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Middleton", "Katherine", ""], ["Butt", "Mobasher", ""], ["Hammerla", "Nils", ""], ["Hamblin", "Steven", ""], ["Mehta", "Karan", ""], ["Parsa", "Ali", ""]]}, {"id": "1606.02427", "submitter": "Jeremy Frey", "authors": "J\\'er\\'emy Frey (Potioc, UB)", "title": "VIF: Virtual Interactive Fiction (with a twist)", "comments": "Pervasive Play - CHI '16 Workshop, May 2016, San Jose, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays computer science can create digital worlds that deeply immerse\nusers; it can also process in real time brain activity to infer their inner\nstates. What marvels can we achieve with such technologies? Go back to\ndisplaying text. And unfold a story that follows and molds users as never\nbefore.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 07:14:24 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Frey", "J\u00e9r\u00e9my", "", "Potioc, UB"]]}, {"id": "1606.02438", "submitter": "Jeremy Frey", "authors": "J\\'er\\'emy Frey (UB, Potioc)", "title": "Comparison of an open-hardware electroencephalography amplifier with\n  medical grade device in brain-computer interface applications", "comments": "PhyCS - International Conference on Physiological Computing Systems,\n  Jul 2016, Lisbon, Portugal. SCITEPRESS, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interfaces (BCI) are promising communication devices between\nhumans and machines. BCI based on non-invasive neuroimaging techniques such as\nelectroencephalography (EEG) have many applications , however the dissemination\nof the technology is limited, in part because of the price of the hardware. In\nthis paper we compare side by side two EEG amplifiers, the consumer grade\nOpenBCI and the medical grade g.tec g.USBamp. For this purpose, we employed an\noriginal montage, based on the simultaneous recording of the same set of\nelectrodes. Two set of recordings were performed. During the first experiment a\nsimple adapter with a direct connection between the amplifiers and the\nelectrodes was used. Then, in a second experiment, we attempted to discard any\npossible interference that one amplifier could cause to the other by adding\n\"ideal\" diodes to the adapter. Both spectral and temporal features were tested\n-- the former with a workload monitoring task, the latter with an visual P300\nspeller task. Overall, the results suggest that the OpenBCI board -- or a\nsimilar solution based on the Texas Instrument ADS1299 chip -- could be an\neffective alternative to traditional EEG devices. Even though a medical grade\nequipment still outperforms the OpenBCI, the latter gives very close EEG\nreadings, resulting in practice in a classification accuracy that may be\nsuitable for popularizing BCI uses.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 08:07:09 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Frey", "J\u00e9r\u00e9my", "", "UB, Potioc"]]}, {"id": "1606.02440", "submitter": "Gregory Grefenstette", "authors": "Gregory Grefenstette (TAO), Lawrence Muchemi (TAO)", "title": "On the Place of Text Data in Lifelogs, and Text Analysis via Semantic\n  Facets", "comments": null, "journal-ref": "iConference 2016 SIE on Lifelogging, Mar 2016, Philadelphia,\n  United States. iConference 2016 SIE on Lifelogging, 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current research in lifelog data has not paid enough attention to analysis of\ncognitive activities in comparison to physical activities. We argue that as we\nlook into the future, wearable devices are going to be cheaper and more\nprevalent and textual data will play a more significant role. Data captured by\nlifelogging devices will increasingly include speech and text, potentially\nuseful in analysis of intellectual activities. Analyzing what a person hears,\nreads, and sees, we should be able to measure the extent of cognitive activity\ndevoted to a certain topic or subject by a learner. Test-based lifelog records\ncan benefit from semantic analysis tools developed for natural language\nprocessing. We show how semantic analysis of such text data can be achieved\nthrough the use of taxonomic subject facets and how these facets might be\nuseful in quantifying cognitive activity devoted to various topics in a\nperson's day. We are currently developing a method to automatically create\ntaxonomic topic vocabularies that can be applied to this detection of\nintellectual activity.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 08:11:54 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Grefenstette", "Gregory", "", "TAO"], ["Muchemi", "Lawrence", "", "TAO"]]}, {"id": "1606.02485", "submitter": "Laurel Riek", "authors": "Cory J. Hayes, Maryam Moosaei, Laurel D. Riek", "title": "Exploring Implicit Human Responses to Robot Mistakes in a Learning from\n  Demonstration Task", "comments": "7 pages, 2 figures, IEEE RO-MAN 2016, IEEE International Symposium on\n  Robot and Human Interactive Communication (RO-MAN 2016)", "journal-ref": null, "doi": "10.1109/ROMAN.2016.7745138", "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots enter human environments, they will be expected to accomplish a\ntremendous range of tasks. It is not feasible for robot designers to\npre-program these behaviors or know them in advance, so one way to address this\nis through end-user programming, such as via learning from demonstration (LfD).\nWhile significant work has been done on the mechanics of enabling robot\nlearning from human teachers, one unexplored aspect is enabling mutual feedback\nbetween both the human teacher and robot during the learning process, i.e.,\nimplicit learning. In this paper, we explore one aspect of this mutual\nunderstanding, grounding sequences, where both a human and robot provide\nnon-verbal feedback to signify their mutual understanding during interaction.\nWe conducted a study where people taught an autonomous humanoid robot a dance,\nand performed gesture analysis to measure people's responses to the robot\nduring correct and incorrect demonstrations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 10:07:01 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Hayes", "Cory J.", ""], ["Moosaei", "Maryam", ""], ["Riek", "Laurel D.", ""]]}, {"id": "1606.02552", "submitter": "Matt Higger", "authors": "Matt Higger, Mohammad Moghadamfalahi, Fernando Quivira and Deniz\n  Erdogmus", "title": "Fast Switch Scanning Keyboards: Minimal Expected Query Decision Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmentative and Alternative Communication (AAC) systems allow people with\ndisabilities to provide input to devices which empower them to more fully\ninteract with their environment. Within AAC, switch scanning is a common\nparadigm for spelling where a set of characters is highlighted and the user is\nqueried as to whether their target character is in the highlighted set. These\nqueries are used to traverse a decision tree which successively prunes away\ncharacters until only a single one remains (the estimate). This work seeks a\ndecision tree which requires the fewest expected queries per decision sequence\n(EQPD). In particular, we remove the constraint that the decision tree needs to\nbe a row-item or group-row-item style tree and minimize EQPD. We pose the\nproblem as a Huffman code with variable, integer cost and solve it with a mild\nextension of Golin's method in \"A dynamic programming algorithm for\nconstructing optimal prefix-free codes with unequal letter costs\", IEEE\nTransactions on Information Theory (1998). Additionally, we model the user on\nthe query level by their probability of detection and false alarm to derive\ntheir expected performance on the character level given some decision tree. We\nperform experiments which show that the min EQPD decision tree (Karp) may\nreduce selection times, especially for timed (single switch) switch scanning.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 13:34:03 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Higger", "Matt", ""], ["Moghadamfalahi", "Mohammad", ""], ["Quivira", "Fernando", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "1606.02603", "submitter": "David Cameron", "authors": "David Cameron and Ee Jing Loh and Adriel Chua and Emily Collins and\n  Jonathan M. Aitken and James Law", "title": "Robot-stated limitations but not intentions promote user assistance", "comments": "5th International Symposium on New Frontiers in Human-Robot\n  Interaction 2016 (arXiv:1602.05456)", "journal-ref": null, "doi": null, "report-no": "AISB-NFHRI/2016/07", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Robot-Interaction (HRI) research is typically built around the premise\nthat the robot serves to assist a human in achieving a human-led goal or shared\ntask. However, there are many circumstances during HRI in which a robot may\nneed the assistance of a human in shared tasks or to achieve goals. We use the\nROBO-GUIDE model as a case study, and insights from social psychology, to\nexamine how a robot's personality can impact on user cooperation. A study of\n364 participants indicates that individuals may prefer to use likable social\nrobots ahead of those designed to appear more capable; this outcome reflects\nknown social decisions in human interpersonal relationships. This work further\ndemonstrates the value of social psychology in developing social robots and\nexploring HRI.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 15:25:13 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Cameron", "David", ""], ["Loh", "Ee Jing", ""], ["Chua", "Adriel", ""], ["Collins", "Emily", ""], ["Aitken", "Jonathan M.", ""], ["Law", "James", ""]]}, {"id": "1606.02632", "submitter": "Nick DePalma", "authors": "Nick DePalma and Cynthia Breazeal", "title": "Towards learning through robotic interaction alone: the joint guided\n  search task", "comments": "5th International Symposium on New Frontiers in Human-Robot\n  Interaction 2016 (arXiv:1602.05456)", "journal-ref": null, "doi": null, "report-no": "AISB-NFHRI/2016/08", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a biologically inspired approach that focuses on attention\nsystems that are able to inhibit or constrain what is relevant at any one\nmoment. We propose a radically new approach to making progress in human-robot\njoint attention called \"the joint guided search task\". Visual guided search is\nthe activity of the eye as it saccades from position to position recognizing\nobjects in each fixation location until the target object is found. Our\nresearch focuses on the exchange of nonverbal behavior toward changing the\nfixation location while also performing object recognition. Our main goal is a\nvery ambitious goal of sharing attention through probing synthetic foreground\nmaps (i.e. what is being considered by the robotic agent) and the biological\nattention system of the human.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 16:46:26 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["DePalma", "Nick", ""], ["Breazeal", "Cynthia", ""]]}, {"id": "1606.02711", "submitter": "Ferran Gal\\'an", "authors": "Ferran Gal\\'an, Stuart N. Baker, Monica A. Perez", "title": "ChinMotion Rapidly Enables 3D Computer Interaction after Tetraplegia", "comments": "The .ps file contains main manuscript and supplementary information.\n  The .ps file is accompanied with ancillary files (supplementary files)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Individuals with severe paralysis require hands-free interfaces to control\nassistive devices that can improve their quality of life. We present\nChinMotion, an interface that noninvasively harnesses preserved chin, lip and\ntongue sensorimotor function after tetraplegia to convey intuitive control\ncommands. After two hours of practice, ChinMotion enables superior\npoint-and-click performance over existing interfaces and it facilitates\naccurate 3D control of a virtual robotic arm.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 14:21:53 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Gal\u00e1n", "Ferran", ""], ["Baker", "Stuart N.", ""], ["Perez", "Monica A.", ""]]}, {"id": "1606.02736", "submitter": "Daniel Davison", "authors": "Daniel Davison and Louisa Schindler and Dennis Reidsma", "title": "Physical extracurricular activities in educational child-robot\n  interaction", "comments": "5th International Symposium on New Frontiers in Human-Robot\n  Interaction 2016 (arXiv:1602.05456)", "journal-ref": null, "doi": null, "report-no": "AISB-NFHRI/2016/06", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an exploratory study on educational child-robot interaction we investigate\nthe effect of alternating a learning activity with an additional shared\nactivity. Our aim is to enhance and enrich the relationship between child and\nrobot by introducing \"physical extracurricular activities\". This enriched\nrelationship might ultimately influence the way the child and robot interact\nwith the learning material. We use qualitative measurement techniques to\nevaluate the effect of the additional activity on the child-robot relationship.\nWe also explore how these metrics can be integrated in a highly exploratory\ncumulative score for the relationship between child and robot. This cumulative\nscore suggests a difference in the overall child-robot relationship between\nchildren who engage in a physical extracurricular activity with the robot, and\nchildren who only engage in the learning activity with the robot.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 20:16:55 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Davison", "Daniel", ""], ["Schindler", "Louisa", ""], ["Reidsma", "Dennis", ""]]}, {"id": "1606.02807", "submitter": "Vivek Veeriah", "authors": "Vivek Veeriah, Patrick M. Pilarski, Richard S. Sutton", "title": "Face valuing: Training user interfaces with facial expressions and\n  reinforcement learning", "comments": "7 pages, 4 figures, IJCAI 2016 - Interactive Machine Learning\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important application of interactive machine learning is extending or\namplifying the cognitive and physical capabilities of a human. To accomplish\nthis, machines need to learn about their human users' intentions and adapt to\ntheir preferences. In most current research, a user has conveyed preferences to\na machine using explicit corrective or instructive feedback; explicit feedback\nimposes a cognitive load on the user and is expensive in terms of human effort.\nThe primary objective of the current work is to demonstrate that a learning\nagent can reduce the amount of explicit feedback required for adapting to the\nuser's preferences pertaining to a task by learning to perceive a value of its\nbehavior from the human user, particularly from the user's facial\nexpressions---we call this face valuing. We empirically evaluate face valuing\non a grip selection task. Our preliminary results suggest that an agent can\nquickly adapt to a user's changing preferences with minimal explicit feedback\nby learning a value function that maps facial features extracted from a camera\nimage to expected future reward. We believe that an agent learning to perceive\na value from the body language of its human user is complementary to existing\ninteractive machine learning approaches and will help in creating successful\nhuman-machine interactive applications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 03:06:46 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Veeriah", "Vivek", ""], ["Pilarski", "Patrick M.", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1606.03236", "submitter": "Vincent C. M\\\"uller", "authors": "Vincent C. M\\\"uller", "title": "Interaction and resistance: The recognition of intentions in new\n  human-computer interaction", "comments": null, "journal-ref": "(2011) In A. Esposito, A. M. Esposito, R. Martone, V. C. M\\\"uller,\n  & G. Scarpetta (Eds.), Towards autonomous, adaptive, and context-aware\n  multimodal interfaces: Theoretical and practical issues (Vol. 6456, pp. 1-7).\n  Berlin: Springer", "doi": "10.1007/978-3-642-18184-9_1", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just as AI has moved away from classical AI, human-computer interaction (HCI)\nmust move away from what I call 'good old fashioned HCI' to 'new HCI' - it must\nbecome a part of cognitive systems research where HCI is one case of the\ninteraction of intelligent agents (we now know that interaction is essential\nfor intelligent agents anyway). For such interaction, we cannot just 'analyze\nthe data', but we must assume intentions in the other, and I suggest these are\nlargely recognized through resistance to carrying out one's own intentions.\nThis does not require fully cognitive agents but can start at a very basic\nlevel. New HCI integrates into cognitive systems research and designs\nintentional systems that provide resistance to the human agent.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 09:11:52 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["M\u00fcller", "Vincent C.", ""]]}, {"id": "1606.03509", "submitter": "Kirsten Ellis", "authors": "Kirsten Ellis, Julie Fisher, Louisa Willoughby and Jan Carlo Barca", "title": "A design science exploration of a visual-spatial learning system with\n  feedback", "comments": "Research-in-progress ISBN# 978-0-646-95337-3 Presented at the\n  Australasian Conference on Information Systems 2015 (arXiv:1605.01032)", "journal-ref": null, "doi": null, "report-no": "ACIS/2015/201", "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Our paper is research in progress that is research investigating the use of\ngames technology to enhance the learning of a physical skill. The Microsoft\nKinect is a system designed for gaming with the capability to track the\nmovement of users. Our research explored whether such a system could be used to\nprovide feedback when teaching sign vocabulary. Whilst there are technologies\navailable for teaching sign language, currently none provide feedback on the\naccuracy of the users' attempts at making signs. In this paper we report how\nthe three-dimensional dsplay capability of the technology can enhance the\nusers' experience. Also, when using tracking to identify errors in physical\nmovements, how and when should feedback be given. A design science approach was\nundertaken to find a solution to this real world problem. The design and\nimplementation of the solution provides interesting insights into how\ntechnology can not only emulate but also improve upon traditional learning of\nphysical skills.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 00:21:04 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Ellis", "Kirsten", ""], ["Fisher", "Julie", ""], ["Willoughby", "Louisa", ""], ["Barca", "Jan Carlo", ""]]}, {"id": "1606.03544", "submitter": "Sisira Adikari", "authors": "Sisira Adikari, Craig McDonald and John Campbell", "title": "Quantitative Analysis of Desirability in User Experience", "comments": "ISBN# 978-0-646-95337-3 Presented at the Australasian Conference on\n  Information Systems 2015 (arXiv:1605.01032)", "journal-ref": null, "doi": null, "report-no": "ACIS/2015/230", "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The multi-dimensional nature of user experience warrants rigorous assessment\nof the interactive experience in systems. User experience assessments are based\non product evaluations and subsequent analysis of the collected data using\nquantitative and qualitative techniques. The quality of user experience\nassessments are dependent on the effectiveness of the techniques deployed. This\npaper presents the results of a quantitative analysis of desirability aspects\nof the user experience in a comparative product evaluation study. The data\ncollection was conducted using 118 item Microsoft Product Reaction Cards (PRC)\ntool followed by data analysis based on the Surface Measure of Overall\nPerformance (SMOP) approach. The results of this study suggest that the\nincorporation of SMOP as an approach for PRC data analysis derive conclusive\nevidence of desirability in user experience. The significance of the paper is\nthat it presents a novel analysis method incorporating product reaction cards\nand surface measure of overall performance approach for an effective\nquantitative analysis which can be used in academic research and industrial\npractice.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 03:43:39 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Adikari", "Sisira", ""], ["McDonald", "Craig", ""], ["Campbell", "John", ""]]}, {"id": "1606.03551", "submitter": "James Birt", "authors": "James Birt, Dirk Hovorka and Jonathan Nelson", "title": "Interdisciplinary Translation of Comparative Visualization", "comments": "ISBN# 978-0-646-95337-3 Presented at the Australasian Conference on\n  Information Systems 2015 (arXiv:1605.01032)", "journal-ref": null, "doi": null, "report-no": "ACIS/2015/252", "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Spatial visualisation skills and interpretations are critical in the design\nprofessions, but traditionally difficult to effectively teach. Visualization\nand multimedia presentation studies show positive improvements in learner\noutcomes for specific learning domains. But the development and translation of\na comparative visualization pedagogy between disciplines is poorly understood.\nThis research seeks to identify an approach to developing comparable multimodal\nand interactive visualizations and attendant student reflections for curriculum\ndesigners in courses that can utilize visualizations and manipulations. Results\nfrom previous use of comparative multimodal visualization pedagogy in a\nmultimedia 3D modelling class are used as a guide to translation of pedagogy to\narchitecture design. The focus is how to guide the use of comparative\nmultimodal visualizations through media properties, lesson sequencing, and\nreflection to inform effective instruction and learning.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 04:15:53 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Birt", "James", ""], ["Hovorka", "Dirk", ""], ["Nelson", "Jonathan", ""]]}, {"id": "1606.03636", "submitter": "Harishchandra Dubey", "authors": "Harishchandra Dubey, Matthias R. Mehl, Kunal Mankodiya", "title": "BigEAR: Inferring the Ambient and Emotional Correlates from\n  Smartphone-based Acoustic Big Data", "comments": "6 pages, 10 equations, 1 Table, 5 Figures, IEEE International\n  Workshop on Big Data Analytics for Smart and Connected Health 2016, June 27,\n  2016, Washington DC, USA", "journal-ref": null, "doi": "10.1109/CHASE.2016.46", "report-no": null, "categories": "cs.SD cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel BigEAR big data framework that employs\npsychological audio processing chain (PAPC) to process smartphone-based\nacoustic big data collected when the user performs social conversations in\nnaturalistic scenarios. The overarching goal of BigEAR is to identify moods of\nthe wearer from various activities such as laughing, singing, crying, arguing,\nand sighing. These annotations are based on ground truth relevant for\npsychologists who intend to monitor/infer the social context of individuals\ncoping with breast cancer. We pursued a case study on couples coping with\nbreast cancer to know how the conversations affect emotional and social well\nbeing. In the state-of-the-art methods, psychologists and their team have to\nhear the audio recordings for making these inferences by subjective evaluations\nthat not only are time-consuming and costly, but also demand manual data coding\nfor thousands of audio files. The BigEAR framework automates the audio\nanalysis. We computed the accuracy of BigEAR with respect to the ground truth\nobtained from a human rater. Our approach yielded overall average accuracy of\n88.76% on real-world data from couples coping with breast cancer.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 22:02:04 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Dubey", "Harishchandra", ""], ["Mehl", "Matthias R.", ""], ["Mankodiya", "Kunal", ""]]}, {"id": "1606.03713", "submitter": "Kai Li", "authors": "Kai Li, Chau Yuen, Salil S. Kanhere, Kun Hu, Wei Zhang, Fan Jiang,\n  Xiang Liu", "title": "SenseFlow: An Experimental Study for Tracking People", "comments": "25 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main challenges in large-scale people tracking are the recognition of\npeople density in a specific area and tracking the people flow path. To address\nthese challenges, we present SenseFlow, a lightweight people tracking system.\nSenseFlow utilises off-the-shelf devices which sniff probe requests\nperiodically polled by user's smartphones in a passive manner. We demonstrate\nthe feasibility of SenseFlow by building a proof-of-concept prototype and\nundertaking extensive evaluations in real-world settings. We deploy the system\nin one laboratory to study office hours of researchers, a crowded public area\nin city to evaluate the scalability and performance \"in the wild\", and four\nclassrooms in the university to monitor the number of students. We also\nevaluate SenseFlow with varying walking speeds and different models of\nsmartphones to investigate the people flow tracking performance.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 14:01:02 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 02:49:50 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 02:13:22 GMT"}, {"version": "v4", "created": "Fri, 17 Jun 2016 06:42:31 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Li", "Kai", ""], ["Yuen", "Chau", ""], ["Kanhere", "Salil S.", ""], ["Hu", "Kun", ""], ["Zhang", "Wei", ""], ["Jiang", "Fan", ""], ["Liu", "Xiang", ""]]}, {"id": "1606.03846", "submitter": "Astrid Weiss", "authors": "Astrid Weiss and Andreas Huber", "title": "User Experience of a Smart Factory Robot: Assembly Line Workers Demand\n  Adaptive Robots", "comments": "5th International Symposium on New Frontiers in Human-Robot\n  Interaction 2016 (arXiv:1602.05456)", "journal-ref": null, "doi": null, "report-no": "AISB-NFHRI/2016/02", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports a case study on the User Experience (UX)of an industrial\nrobotic prototype in the context of human-robot cooperation in an automotive\nassembly line. The goal was to find out what kinds of suggestions the assembly\nline workers, who actually use the new robotic system, propose in order to\nimprove the human-robot interaction (HRI). The operators working with the\nrobotic prototype were interviewed three weeks after the deployment using\nestablished UX narrative interview guidelines. Our results show that the\ncooperation with a robot that executes predefined working steps actually\nimpedes the user in terms of flexibility and individual speed. This results in\na change of working routine for the operators, impacts the UX, and potentially\nleads to a decrease in productivity. We present the results of the interviews\nas well as first thoughts on technical solutions in order to enhance the\nadaptivity and subsequently the UX of the human-robot cooperation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 07:43:24 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Weiss", "Astrid", ""], ["Huber", "Andreas", ""]]}, {"id": "1606.03875", "submitter": "Pablo Gomez Esteban", "authors": "Pablo G\\'omez Esteban, Hoang-Long Cao, Albert De Beir, Greet Van de\n  Perre, Dirk Lefeber and Bram Vanderborght", "title": "A multilayer reactive system for robots interacting with children with\n  autism", "comments": "5th International Symposium on New Frontiers in Human-Robot\n  Interaction 2016 (arXiv:1602.05456)", "journal-ref": null, "doi": null, "report-no": "AISB-NFHRI/2016/10", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a lack of autonomy on traditional Robot-Assisted Therapy systems\ninteracting with children with autism. To overcome this limitation a supervised\nautonomous robot controller is being built. In this paper we present a\nmultilayer reactive system within such controller. The goal of this Reactive\nsystem is to allow the robot to appropriately react to the child's behavior\ncreating the illusion of being alive.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 09:42:14 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Esteban", "Pablo G\u00f3mez", ""], ["Cao", "Hoang-Long", ""], ["De Beir", "Albert", ""], ["Van de Perre", "Greet", ""], ["Lefeber", "Dirk", ""], ["Vanderborght", "Bram", ""]]}, {"id": "1606.03992", "submitter": "Ori Novanda", "authors": "Ori Novanda, Maha Salem, Joe Saunders, Michael L. Walters, and Kerstin\n  Dautenhahn", "title": "What Communication Modalities Do Users Prefer in Real Time HRI?", "comments": "5th International Symposium on New Frontiers in Human-Robot\n  Interaction 2016 (arXiv:1602.05456)", "journal-ref": null, "doi": null, "report-no": "AISB-NFHRI/2016/04", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates users' preferred interaction modalities when playing\nan imitation game with KASPAR, a small child-sized humanoid robot. The study\ninvolved 16 adult participants teaching the robot to mime a nursery rhyme via\none of three interaction modalities in a real-time Human-Robot Interaction\n(HRI) experiment: voice, guiding touch and visual demonstration. The findings\nsuggest that the users appeared to have no preference in terms of human effort\nfor completing the task. However, there was a significant difference in human\nenjoyment preferences of input modality and a marginal difference in the\nrobot's perceived ability to imitate.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 15:27:28 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Novanda", "Ori", ""], ["Salem", "Maha", ""], ["Saunders", "Joe", ""], ["Walters", "Michael L.", ""], ["Dautenhahn", "Kerstin", ""]]}, {"id": "1606.04033", "submitter": "Elaine Sedenberg", "authors": "Elaine Sedenberg, John Chuang, Deirdre Mulligan", "title": "Designing Commercial Therapeutic Robots for Privacy Preserving Systems\n  and Ethical Research Practices within the Home", "comments": "International Journal of Social Robotics, (2016), 1-13", "journal-ref": null, "doi": "10.1007/s12369-016-0362-y", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The migration of robots from the laboratory into sensitive home settings as\ncommercially available therapeutic agents represents a significant transition\nfor information privacy and ethical imperatives. We present new privacy\nparadigms and apply the Fair Information Practices (FIPs) to investigate\nconcerns unique to the placement of therapeutic robots in private home\ncontexts. We then explore the importance and utility of research ethics as\noperationalized by existing human subjects research frameworks to guide the\nconsideration of therapeutic robotic users -- a step vital to the continued\nresearch and development of these platforms. Together, privacy and research\nethics frameworks provide two complementary approaches to protect users and\nensure responsible yet robust information sharing for technology development.\nWe make recommendations for the implementation of these principles -- paying\nparticular attention to specific principles that apply to vulnerable\nindividuals (i.e., children, disabled, or elderly persons)--to promote the\nadoption and continued improvement of long-term, responsible, and\nresearch-enabled robotics in private settings.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 16:56:21 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 06:03:46 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Sedenberg", "Elaine", ""], ["Chuang", "John", ""], ["Mulligan", "Deirdre", ""]]}, {"id": "1606.04165", "submitter": "Sarah Ostadabbas", "authors": "Katie Hoemann, Behnaz Rezaei, Stacy C. Marsella, Sarah Ostadabbas", "title": "Using Virtual Humans to Understand Real Ones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human interactions are characterized by explicit as well as implicit channels\nof communication. While the explicit channel transmits overt messages, the\nimplicit ones transmit hidden messages about the communicator (e.g., his/her\nintentions and attitudes). There is a growing consensus that providing a\ncomputer with the ability to manipulate implicit affective cues should allow\nfor a more meaningful and natural way of studying particular non-verbal signals\nof human-human communications by human-computer interactions. In this pilot\nstudy, we created a non-dynamic human-computer interaction while manipulating\nthree specific non-verbal channels of communication: gaze pattern, facial\nexpression, and gesture. Participants rated the virtual agent on affective\ndimensional scales (pleasure, arousal, and dominance) while their physiological\nsignal (electrodermal activity, EDA) was captured during the interaction.\nAssessment of the behavioral data revealed a significant and complex three-way\ninteraction between gaze, gesture, and facial configuration on the dimension of\npleasure, as well as a main effect of gesture on the dimension of dominance.\nThese results suggest a complex relationship between different non-verbal cues\nand the social context in which they are interpreted. Qualifying considerations\nas well as possible next steps are further discussed in light of these\nexploratory findings.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 22:41:50 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Hoemann", "Katie", ""], ["Rezaei", "Behnaz", ""], ["Marsella", "Stacy C.", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "1606.04721", "submitter": "Alessandro Bessi", "authors": "Alessandro Bessi", "title": "Personality Traits and Echo Chambers on Facebook", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online social networks, users tend to select information that adhere to\ntheir system of beliefs and to form polarized groups of like minded people.\nPolarization as well as its effects on online social interactions have been\nextensively investigated. Still, the relation between group formation and\npersonality traits remains unclear. A better understanding of the cognitive and\npsychological determinants of online social dynamics might help to design more\nefficient communication strategies and to challenge the digital misinformation\nthreat. In this work, we focus on users commenting posts published by US\nFacebook pages supporting scientific and conspiracy-like narratives, and we\nclassify the personality traits of those users according to their online\nbehavior. We show that different and conflicting communities are populated by\nusers showing similar psychological profiles, and that the dominant personality\nmodel is the same in both scientific and conspiracy echo chambers. Moreover, we\nobserve that the permanence within echo chambers slightly shapes users'\npsychological profiles. Our results suggest that the presence of specific\npersonality traits in individuals lead to their considerable involvement in\nsupporting narratives inside virtual echo chambers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 11:08:24 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Bessi", "Alessandro", ""]]}, {"id": "1606.04836", "submitter": "Cindy Grimm", "authors": "Leo Bowen-Biggs and Suzanne Dazo and Yili Zhang and Alex Hubers and\n  Matthew Rueben and Ross Sowell and William D. Smart, Cindy Grimm", "title": "Sketched Floor plans versus SLAM maps: A Comparison", "comments": "11 pages singe column, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maps --- specifically floor plans --- are useful for a variety of tasks from\narranging furniture to designating conceptual or functional spaces (e.g.,\nkitchen, walkway). We present a simple algorithm for quickly laying a floor\nplan (or other conceptual map) onto a SLAM map, creating a one-to-one mapping\nbetween them. Our goal was to enable using a floor plan (or other hand-drawn or\nannotated map) in robotic applications instead of the typical SLAM map created\nby the robot. We look at two use cases, specifying \"no-go\" regions within a\nroom and locating objects within a scanned room. Although a user study showed\nno statistical difference between the two types of maps in terms of performance\non this spatial memory task, we argue that floor plans are closer to the mental\nmaps people would naturally draw to characterize spaces.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 16:15:00 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Bowen-Biggs", "Leo", ""], ["Dazo", "Suzanne", ""], ["Zhang", "Yili", ""], ["Hubers", "Alex", ""], ["Rueben", "Matthew", ""], ["Sowell", "Ross", ""], ["Smart", "William D.", ""], ["Grimm", "Cindy", ""]]}, {"id": "1606.04929", "submitter": "Koushik Sinha", "authors": "Koushik Sinha, Geetha Manjunath, Bidyut Gupta and Shahram Rahimi", "title": "Designing a Human-Machine Hybrid Computing System for Unstructured Data\n  Analytics", "comments": "conference pre-print version in Proc. 31st Intl. Conf. on Computers\n  and Their Applications (CATA), Las Vegas, USA, April 4-6, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current machine algorithms for analysis of unstructured data typically show\nlow accuracies due to the need for human-like intelligence. Conversely, though\nhumans are much better than machine algorithms on analyzing unstructured data,\nthey are unpredictable, slower and can be erroneous or even malicious as\ncomputing agents. Therefore, a hybrid platform that can intelligently\norchestrate machine and human computing resources would potentially be capable\nof providing significantly better benefits compared to either type of computing\nagent in isolation. In this paper, we propose a new hybrid human-machine\ncomputing platform with integrated service level objectives (SLO) management\nfor complex tasks that can be decomposed into a dependency graph where nodes\nrepresent subtasks. Initial experimental results are highly encouraging. To the\nbest of our knowledge, ours is the first work that attempts to design such a\nhybrid human-machine computing platform with support for addressing the three\nSLO parameters of accuracy, budget and completion time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 19:38:12 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Sinha", "Koushik", ""], ["Manjunath", "Geetha", ""], ["Gupta", "Bidyut", ""], ["Rahimi", "Shahram", ""]]}, {"id": "1606.05006", "submitter": "Jean Vettel PhD", "authors": "Jean M. Vettel, Justin Kantner, Matthew Jaswa, Michael Miller", "title": "Animated 3D Human Models for Use in Person Recognition Experiments", "comments": "In revision from BRM", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of increasingly realistic experimental stimuli and task\nenvironments is important for understanding behavior outside the laboratory. We\nreport a process for generating 3D human model stimuli that combines commonly\nused graphics software and enables the flexible generation of animated human\nmodels while providing parametric control over individualized identity\nfeatures. Our approach creates novel head models using FaceGen Modeller,\nattaches them to commercially-purchased 3D avatar bodies in 3D Studio Max, and\ngenerates Cal3D human models that are compatible with many virtual 3D\nenvironments. Stimuli produced by this method can be embedded as animated 3D\navatars in interactive simulations or presented as 2D images embedded in scenes\nfor use in traditional laboratory experiments. The inherent flexibility in this\nmethod makes the stimuli applicable to a broad range of basic and applied\nresearch questions in the domain of person perception. We describe the steps of\nthe stimulus generation process, provide an example of their use in a\nrecognition memory paradigm, and highlight the adaptability of the method for\nrelated avenues of research.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 23:43:53 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Vettel", "Jean M.", ""], ["Kantner", "Justin", ""], ["Jaswa", "Matthew", ""], ["Miller", "Michael", ""]]}, {"id": "1606.05017", "submitter": "Shreyas Sen", "authors": "Shreyas Sen", "title": "SocialHBC: Social Networking and Secure Authentication using\n  Interference-Robust Human Body Communication", "comments": "Accepted for Publication in International Symposium on Low Power\n  Electronics and Design (ISLPED)", "journal-ref": null, "doi": "10.1145/2934583.2934609", "report-no": null, "categories": "cs.HC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of cheap computing through five decades of continued\nminiaturization following Moores Law, wearable devices are becoming\nincreasingly popular. These wearable devices are typically interconnected using\nwireless body area network (WBAN). Human body communication (HBC) provides an\nalternate energy-efficient communication technique between on-body wearable\ndevices by using the human body as a conducting medium. This allows order of\nmagnitude lower communication power, compared to WBAN, due to lower loss and\nbroadband signaling. Moreover, HBC is significantly more secure than WBAN, as\nthe information is contained within the human body and cannot be snooped on\nunless the person is physically touched. In this paper, we highlight\napplications of HBC as (1) Social Networking (e.g. LinkedIn/Facebook friend\nrequest sent during Handshaking in a meeting/party), (2) Secure Authentication\nusing human-human or human-machine dynamic HBC and (3) ultra-low power, secure\nBAN using intra-human HBC. One of the biggest technical bottlenecks of HBC has\nbeen the interference (e.g. FM) picked up by the human body acting like an\nantenna. In this work, for the first time, we introduce an integrating dual\ndata rate (DDR) receiver technique, that allows notch filtering (>20 dB) of the\ninterference for interference-robust HBC.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 00:48:13 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Sen", "Shreyas", ""]]}, {"id": "1606.05374", "submitter": "Jacob Steinhardt", "authors": "Jacob Steinhardt and Gregory Valiant and Moses Charikar", "title": "Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer\n  Prediction", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a crowdsourcing model in which $n$ workers are asked to rate the\nquality of $n$ items previously generated by other workers. An unknown set of\n$\\alpha n$ workers generate reliable ratings, while the remaining workers may\nbehave arbitrarily and possibly adversarially. The manager of the experiment\ncan also manually evaluate the quality of a small number of items, and wishes\nto curate together almost all of the high-quality items with at most an\n$\\epsilon$ fraction of low-quality items. Perhaps surprisingly, we show that\nthis is possible with an amount of work required of the manager, and each\nworker, that does not scale with $n$: the dataset can be curated with\n$\\tilde{O}\\Big(\\frac{1}{\\beta\\alpha^3\\epsilon^4}\\Big)$ ratings per worker, and\n$\\tilde{O}\\Big(\\frac{1}{\\beta\\epsilon^2}\\Big)$ ratings by the manager, where\n$\\beta$ is the fraction of high-quality items. Our results extend to the more\ngeneral setting of peer prediction, including peer grading in online\nclassrooms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 21:45:14 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Steinhardt", "Jacob", ""], ["Valiant", "Gregory", ""], ["Charikar", "Moses", ""]]}, {"id": "1606.05753", "submitter": "Radoslava Kraleva Dr.", "authors": "Radoslava Kraleva, Velin Kralev, Dafina Kostadinova", "title": "A Conceptual Design of Mobile Learning Applications for Preschool\n  Children", "comments": "6 pages, 2 figures, pp. 259-267, International Journal of Computer\n  Science and Information Security (IJCSIS), Vol. 14 (5), pp. 259-264, 2016", "journal-ref": "International Journal of Computer Science and Information Security\n  (IJCSIS), Vol. 14 (5), pp. 259-264, 2016", "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the possibilities of using mobile learning in the\nBulgarian preschool education of young children. The state preschool\neducational regulations are presented and discussed. The problem concerning the\nchildren's safety when using mobile devices in terms of access to information\non the Internet is revealed and analyzed. Two conceptual models of applications\nfor mobile learning aimed at preschool children are designed. Their advantages\nand disadvantages are outlined and discussed. Keywords - mobile applications\nfor children, mobile learning, design of mobile learning component, software\nengineering\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 14:04:22 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Kraleva", "Radoslava", ""], ["Kralev", "Velin", ""], ["Kostadinova", "Dafina", ""]]}, {"id": "1606.06104", "submitter": "David Cameron", "authors": "David Cameron, Samuel Fernando, Emily Collins, Abigail Millings, Roger\n  Moore, Amanda Sharkey, Tony Prescott", "title": "Impact of robot responsiveness and adult involvement on children's\n  social behaviours in human-robot interaction", "comments": "5th International Symposium on New Frontiers in Human-Robot\n  Interaction 2016 (arXiv:1602.05456)", "journal-ref": null, "doi": null, "report-no": "AISB-NFHRI/2016/07", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in developing engaging social robots is creating convincing,\nautonomous and responsive agents, which users perceive, and treat, as social\nbeings. As a part of the collaborative project: Expressive Agents for Symbiotic\nEducation and Learning (EASEL), this study examines the impact of autonomous\nresponse to children's speech, by the humanoid robot Zeno, on their\ninteractions with it as a social entity. Results indicate that robot autonomy\nand adult assistance during HRI can substantially influence children's\nbehaviour during interaction and their affect after. Children working with a\nfully-autonomous, responsive robot demonstrated greater physical activity\nfollowing robot instruction than those working with a less responsive robot,\nwhich required adult assistance to interact with. During dialogue with the\nrobot, children working with the fully-autonomous robot also looked towards the\nrobot in anticipation of its vocalisations on more occasions. In contrast, a\nless responsive robot, requiring adult assistance to interact with, led to\ngreater self-report positive affect and more occasions of children looking to\nthe robot in response to its vocalisations. We discuss the broader implications\nof these findings in terms of anthropomorphism of social robots and in relation\nto the overall project strategy to further the understanding of how\ninteractions with social robots could lead to task-appropriate symbiotic\nrelationships.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 13:11:37 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Cameron", "David", ""], ["Fernando", "Samuel", ""], ["Collins", "Emily", ""], ["Millings", "Abigail", ""], ["Moore", "Roger", ""], ["Sharkey", "Amanda", ""], ["Prescott", "Tony", ""]]}, {"id": "1606.06140", "submitter": "Tim Weninger PhD", "authors": "Maria Glenski, Tim Weninger", "title": "Rating Effects on Social News Posts and Comments", "comments": "18 pages, 7 figures, accepted to ACM TIST", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  At a time when information seekers first turn to digital sources for news and\nopinion, it is critical that we understand the role that social media plays in\nhuman behavior. This is especially true when information consumers also act as\ninformation producers and editors through their online activity. In order to\nbetter understand the effects that editorial ratings have on online human\nbehavior, we report the results of a two large-scale in-vivo experiments in\nsocial media. We find that small, random rating manipulations on social media\nposts and comments created significant changes in downstream ratings resulting\nin significantly different final outcomes. We found positive herding effects\nfor positive treatments on posts, increasing the final rating by 11.02% on\naverage, but not for positive treatments on comments. Contrary to the results\nof related work, we found negative herding effects for negative treatments on\nposts and comments, decreasing the final ratings on average, of posts by 5.15%\nand of comments by 37.4%. Compared to the control group, the probability of\nreaching a high rating (>=2000) for posts is increased by 24.6% when posts\nreceive the positive treatment and for comments is decreased by 46.6% when\ncomments receive the negative treatment.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 14:32:06 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Glenski", "Maria", ""], ["Weninger", "Tim", ""]]}, {"id": "1606.06434", "submitter": "Prem Prakash Jayaraman", "authors": "Prem Prakash Jayaraman, and Jean-Paul Calbimonte, Hoan Nguyen Mau Quoc", "title": "The Schema Editor of OpenIoT for Semantic Sensor Networks", "comments": "First Joint International Workshop on SEMANTIC SENSOR NETWORKS AND\n  TERRA COGNITA, The 14th International Semantic Web Conference Workshops,\n  October 11-15, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies provide conceptual abstractions over data, in domains such as the\nInternet of Things, in a way that sensor data can be harvested and interpreted\nby people and applications. The Semantic Sensor Network (SSN) ontology is the\nde-facto standard for semantic representation of sensor observations and\nmetadata, and it is used at the core of the open source platform for the\nInternet of Things, OpenIoT. In this paper we present a Schema Editor that\nprovides an intuitive web interface for defining new types of sensors, and\nconcrete instances of them, using the SSN ontology as the core model. This\neditor is fully integrated with the OpenIoT platform for generating virtual\nsensor descriptions and automating their semantic annotation and registration\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 06:20:22 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Jayaraman", "Prem Prakash", ""], ["Calbimonte", "Jean-Paul", ""], ["Quoc", "Hoan Nguyen Mau", ""]]}, {"id": "1606.06702", "submitter": "Ji Hwan Park", "authors": "Ji Hwan Park, Seyedkoosha Mirhosseini, Saad Nadeem, Joseph Marino,\n  Arie Kaufman, Kevin Baker, Matthew Barish", "title": "Crowdsourcing for Identification of Polyp-Free Segments in Virtual\n  Colonoscopy Videos", "comments": null, "journal-ref": "Proc. SPIE Medical Imaging 2017, 101380V", "doi": "10.1117/12.2252281", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual colonoscopy (VC) allows a physician to virtually navigate within a\nreconstructed 3D colon model searching for colorectal polyps. Though VC is\nwidely recognized as a highly sensitive and specific test for identifying\npolyps, one limitation is the reading time, which can take over 30 minutes per\npatient. Large amounts of the colon are often devoid of polyps, and a way of\nidentifying these polyp-free segments could be of valuable use in reducing the\nrequired reading time for the interrogating radiologist. To this end, we have\ntested the ability of the collective crowd intelligence of non-expert workers\nto identify polyp candidates and polyp-free regions. We presented twenty short\nvideos flying through a segment of a virtual colon to each worker, and the\ncrowd was asked to determine whether or not a possible polyp was observed\nwithin that video segment. We evaluated our framework on Amazon Mechanical Turk\nand found that the crowd was able to achieve a sensitivity of 80.0% and\nspecificity of 86.5% in identifying video segments which contained a clinically\nproven polyp. Since each polyp appeared in multiple consecutive segments, all\npolyps were in fact identified. Using the crowd results as a first pass, 80% of\nthe video segments could in theory be skipped by the radiologist, equating to a\nsignificant time savings and enabling more VC examinations to be performed.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 18:48:03 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 16:51:44 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 18:59:50 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Park", "Ji Hwan", ""], ["Mirhosseini", "Seyedkoosha", ""], ["Nadeem", "Saad", ""], ["Marino", "Joseph", ""], ["Kaufman", "Arie", ""], ["Baker", "Kevin", ""], ["Barish", "Matthew", ""]]}, {"id": "1606.06840", "submitter": "Corneliu Florea", "authors": "Oana Miu, Adrian Zamfir and Corneliu Florea", "title": "Person Identification Based on Hand Tremor Characteristics", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of biometric measures have been proposed in the past. In this\npaper we introduce a new potential biometric measure: the human tremor. We\npresent a new method for identifying the user of a handheld device using\ncharacteristics of the hand tremor measured with a smartphone built-in inertial\nsensors (accelerometers and gyroscopes). The main challenge of the proposed\nmethod is related to the fact that human normal tremor is very subtle while we\naim to address real-life scenarios. To properly address the issue, we have\nrelied on weighted Fourier linear combiner for retrieving only the tremor data\nfrom the hand movement and random forest for actual recognition. We have\nevaluated our method on a database with 10 000 samples from 17 persons reaching\nan accuracy of 76%.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 07:50:05 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Miu", "Oana", ""], ["Zamfir", "Adrian", ""], ["Florea", "Corneliu", ""]]}, {"id": "1606.06873", "submitter": "Sharath Chandra Guntuku", "authors": "Sharath Chandra Guntuku, Michael James Scott, Gheorghita Ghinea, Weisi\n  Lin", "title": "Personality, Culture, and System Factors - Impact on Affective Response\n  to Multimedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst affective responses to various forms and genres of multimedia content\nhave been well researched, precious few studies have investigated the combined\nimpact that multimedia system parameters and human factors have on affect.\nConsequently, in this paper we explore the role that two primordial dimensions\nof human factors - personality and culture - in conjunction with system factors\n- frame rate, resolution, and bit rate - have on user affect and enjoyment of\nmultimedia presentations. To this end, a two-site, cross-cultural study was\nundertaken, the results of which produced three predictve models. Personality\nand Culture traits were shown statistically to represent 5.6% of the variance\nin positive affect, 13.6% in negative affect and 9.3% in enjoyment. The\ncorrelation between affect and enjoyment, was significant. Predictive modeling\nincorporating human factors showed about 8%, 7% and 9% improvement in\npredicting positive affect, negative affect and enjoyment respectively when\ncompared to models trained only on system factors. Results and analysis\nindicate the significant role played by human factors in influencing affect\nthat users experience while watching multimedia.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 10:02:39 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 08:45:14 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Guntuku", "Sharath Chandra", ""], ["Scott", "Michael James", ""], ["Ghinea", "Gheorghita", ""], ["Lin", "Weisi", ""]]}, {"id": "1606.06979", "submitter": "Kory W Mathewson", "authors": "Kory W. Mathewson and Patrick M. Pilarski", "title": "Simultaneous Control and Human Feedback in the Training of a Robotic\n  Agent with Actor-Critic Reinforcement Learning", "comments": "7 pages, 3 figures, Accepted at the Interactive Machine Learning\n  Workshop at IJCAI 2016 (IML): Connecting Humans and Machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes a preliminary report on the advantages and\ndisadvantages of incorporating simultaneous human control and feedback signals\nin the training of a reinforcement learning robotic agent. While robotic\nhuman-machine interfaces have become increasingly complex in both form and\nfunction, control remains challenging for users. This has resulted in an\nincreasing gap between user control approaches and the number of robotic motors\nwhich can be controlled. One way to address this gap is to shift some autonomy\nto the robot. Semi-autonomous actions of the robotic agent can then be shaped\nby human feedback, simplifying user control. Most prior work on agent shaping\nby humans has incorporated training with feedback, or has included indirect\ncontrol signals. By contrast, in this paper we explore how a human can provide\nconcurrent feedback signals and real-time myoelectric control signals to train\na robot's actor-critic reinforcement learning control system. Using both a\nphysical and a simulated robotic system, we compare training performance on a\nsimple movement task when reward is derived from the environment, when reward\nis provided by the human, and combinations of these two approaches. Our results\nindicate that some benefit can be gained with the inclusion of human generated\nfeedback.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 15:09:04 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Mathewson", "Kory W.", ""], ["Pilarski", "Patrick M.", ""]]}, {"id": "1606.07247", "submitter": "Sayem Mohammad Siam", "authors": "Sayem Mohammad Siam, Jahidul Adnan Sakel, and Md. Hasanul Kabir", "title": "Human Computer Interaction Using Marker Based Hand Gesture Recognition", "comments": "8 Pages, didn't submit to any conference yet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Computer Interaction (HCI) has been redefined in this era. People want\nto interact with their devices in such a way that has physical significance in\nthe real world, in other words, they want ergonomic input devices. In this\npaper, we propose a new method of interaction with computing devices having a\nconsumer grade camera, that uses two colored markers (red and green) worn on\ntips of the fingers to generate desired hand gestures, and for marker detection\nand tracking we used template matching with kalman filter. We have implemented\nall the usual system commands, i.e., cursor movement, right click, left click,\ndouble click, going forward and backward, zoom in and out through different\nhand gestures. Our system can easily recognize these gestures and give\ncorresponding system commands. Our system is suitable for both desktop devices\nand devices where touch screen is not feasible like large screens or projected\nscreens.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 09:49:15 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Siam", "Sayem Mohammad", ""], ["Sakel", "Jahidul Adnan", ""], ["Kabir", "Md. Hasanul", ""]]}, {"id": "1606.07487", "submitter": "Santiago Ontanon", "authors": "Adam James Summerville, Sam Snodgrass, Michael Mateas, Santiago\n  Onta\\~n\\'on", "title": "The VGLC: The Video Game Level Corpus", "comments": "To appear in proceedings of the 7th Workshop on Procedural Content\n  Generation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Levels are a key component of many different video games, and a large body of\nwork has been produced on how to procedurally generate game levels. Recently,\nMachine Learning techniques have been applied to video game level generation\ntowards the purpose of automatically generating levels that have the properties\nof the training corpus. Towards that end we have made available a corpora of\nvideo game levels in an easy to parse format ideal for different machine\nlearning and other game AI research purposes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 21:36:36 GMT"}, {"version": "v2", "created": "Sun, 3 Jul 2016 20:04:55 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Summerville", "Adam James", ""], ["Snodgrass", "Sam", ""], ["Mateas", "Michael", ""], ["Onta\u00f1\u00f3n", "Santiago", ""]]}, {"id": "1606.07781", "submitter": "Peter Bull", "authors": "Peter Bull, Isaac Slavitt, Greg Lipstein", "title": "Harnessing the Power of the Crowd to Increase Capacity for Data Science\n  in the Social Sector", "comments": "Presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present three case studies of organizations using a data science\ncompetition to answer a pressing question. The first is in education where a\nnonprofit that creates smart school budgets wanted to automatically tag budget\nline items. The second is in public health, where a low-cost, nonprofit women's\nhealth care provider wanted to understand the effect of demographic and\nbehavioral questions on predicting which services a woman would need. The third\nand final example is in government innovation: using online restaurant reviews\nfrom Yelp, competitors built models to forecast which restaurants were most\nlikely to have hygiene violations when visited by health inspectors. Finally,\nwe reflect on the unique benefits of the open, public competition model.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 18:30:35 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Bull", "Peter", ""], ["Slavitt", "Isaac", ""], ["Lipstein", "Greg", ""]]}, {"id": "1606.07841", "submitter": "Satya Gautam Vadlamudi", "authors": "Satya Gautam Vadlamudi, Tathagata Chakraborti, Yu Zhang, Subbarao\n  Kambhampati", "title": "Proactive Decision Support using Automated Planning", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proactive decision support (PDS) helps in improving the decision making\nexperience of human decision makers in human-in-the-loop planning environments.\nHere both the quality of the decisions and the ease of making them are\nenhanced. In this regard, we propose a PDS framework, named RADAR, based on the\nresearch in Automated Planning in AI, that aids the human decision maker with\nher plan to achieve her goals by providing alerts on: whether such a plan can\nsucceed at all, whether there exist any resource constraints that may foil her\nplan, etc. This is achieved by generating and analyzing the landmarks that must\nbe accomplished by any successful plan on the way to achieving the goals. Note\nthat, this approach also supports naturalistic decision making which is being\nacknowledged as a necessary element in proactive decision support, since it\nonly aids the human decision maker through suggestions and alerts rather than\nenforcing fixed plans or decisions. We demonstrate the utility of the proposed\nframework through search-and-rescue examples in a fire-fighting domain.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 21:54:28 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Vadlamudi", "Satya Gautam", ""], ["Chakraborti", "Tathagata", ""], ["Zhang", "Yu", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1606.08065", "submitter": "Weiren Wang", "authors": "Weiren Wang, Miseon Park, Yuanzhe Fan, Thad Starner, Gregory D. Abowd", "title": "Face Card: An Information-sharing Framework on Google Glass", "comments": "The paper needs to be revised. I need to discuss this my professors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable devices such as Google Glass can provide an efficient way to get\naround users information. We present Face Card, a system builds on Google Glass\nto provide information-sharing service with around people. With a look at\nGoogle Glass, users can quickly get information of nearby and coming users.\nUtilizing Bluetooth Low Energy (BLE) and proper user interface, Face Card\ndemonstrates the potential of being an efficient information sharing system\nframework.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 18:40:57 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 20:16:55 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Wang", "Weiren", ""], ["Park", "Miseon", ""], ["Fan", "Yuanzhe", ""], ["Starner", "Thad", ""], ["Abowd", "Gregory D.", ""]]}, {"id": "1606.08157", "submitter": "Peter Wittek", "authors": "Peter Wittek, Ying-Hsang Liu, S\\'andor Dar\\'anyi, Tom Gedeon, Ik Soo\n  Lim", "title": "Risk and Ambiguity in Information Seeking: Eye Gaze Patterns Reveal\n  Contextual Behaviour in Dealing with Uncertainty", "comments": "20 pages, 3 figures", "journal-ref": "Frontiers in Psychology, 7, 1790, 2016", "doi": "10.3389/fpsyg.2016.01790", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information foraging connects optimal foraging theory in ecology with how\nhumans search for information. The theory suggests that, following an\ninformation scent, the information seeker must optimize the tradeoff between\nexploration by repeated steps in the search space vs. exploitation, using the\nresources encountered. We conjecture that this tradeoff characterizes how a\nuser deals with uncertainty and its two aspects, risk and ambiguity in economic\ntheory. Risk is related to the perceived quality of the actually visited patch\nof information, and can be reduced by exploiting and understanding the patch to\na better extent. Ambiguity, on the other hand, is the opportunity cost of\nhaving higher quality patches elsewhere in the search space. The aforementioned\ntradeoff depends on many attributes, including traits of the user: at the two\nextreme ends of the spectrum, analytic and wholistic searchers employ entirely\ndifferent strategies. The former type focuses on exploitation first,\ninterspersed with bouts of exploration, whereas the latter type prefers to\nexplore the search space first and consume later. Based on an eye-tracking\nstudy of experts' interactions with novel search interfaces in the biomedical\ndomain, we demonstrate that perceived risk shifts the balance between\nexploration and exploitation in either type of users, tilting it against vs. in\nfavour of ambiguity minimization. Since the pattern of behaviour in information\nforaging is quintessentially sequential, risk and ambiguity minimization cannot\nhappen simultaneously, leading to a fundamental limit on how good such a\ntradeoff can be. This in turn connects information seeking with the emergent\nfield of quantum decision theory.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 08:14:42 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Wittek", "Peter", ""], ["Liu", "Ying-Hsang", ""], ["Dar\u00e1nyi", "S\u00e1ndor", ""], ["Gedeon", "Tom", ""], ["Lim", "Ik Soo", ""]]}, {"id": "1606.08207", "submitter": "Sanja \\v{S}\\'cepanovi\\'c", "authors": "Sanja \\v{S}\\'cepanovi\\'c, Igor Mishkovski, Bruno Gon\\c{c}alves, Nguyen\n  Trung Hieu, Pan Hui", "title": "Semantic homophily in online communication: evidence from Twitter", "comments": "19 pages, 11 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are observed to assortatively connect on a set of traits. This\nphenomenon, termed assortative mixing or sometimes homophily, can be quantified\nthrough assortativity coefficient in social networks. Uncovering the exact\ncauses of strong assortative mixing found in social networks has been a\nresearch challenge. Among the main suggested causes from sociology are the\ntendency of similar individuals to connect (often itself referred as homophily)\nand the social influence among already connected individuals. An important\nquestion to researchers and in practice can be tackled, as we present here:\nunderstanding the exact mechanisms of interplay between these tendencies and\nthe underlying social network structure. Namely, in addition to the mentioned\nassortativity coefficient, there are several other static and temporal network\nproperties and substructures that can be linked to the tendencies of homophily\nand social influence in the social network and we herein investigate those.\nConcretely, we tackle a computer-mediated \\textit{communication network} (based\non Twitter mentions) and a particular type of assortative mixing that can be\ninferred from the semantic features of communication content that we term\n\\textit{semantic homophily}. Our work, to the best of our knowledge, is the\nfirst to offer an in-depth analysis on semantic homophily in a communication\nnetwork and the interplay between them. We quantify diverse levels of semantic\nhomophily, identify the semantic aspects that are the drivers of observed\nhomophily, show insights in its temporal evolution and finally, we present its\nintricate interplay with the communication network on Twitter. By analyzing\nthese mechanisms we increase understanding on what are the semantic aspects\nthat shape and how they shape the human computer-mediated communication.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 11:04:41 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 16:47:22 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 16:22:29 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["\u0160\u0107epanovi\u0107", "Sanja", ""], ["Mishkovski", "Igor", ""], ["Gon\u00e7alves", "Bruno", ""], ["Hieu", "Nguyen Trung", ""], ["Hui", "Pan", ""]]}, {"id": "1606.08528", "submitter": "Bo Tang", "authors": "Bo Tang, Chao Jiang, Haibo He, and Yi Guo", "title": "Probabilistic Human Mobility Model in Indoor Environment", "comments": "8 pages, 9 figures, International Joint Conference on Neural Networks\n  (IJCNN) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human mobility is important for the development of intelligent\nmobile service robots as it can provide prior knowledge and predictions of\nhuman distribution for robot-assisted activities. In this paper, we propose a\nprobabilistic method to model human motion behaviors which is determined by\nboth internal and external factors in an indoor environment. While the internal\nfactors are represented by the individual preferences, aims and interests, the\nexternal factors are indicated by the stimulation of the environment. We model\nthe randomness of human macro-level movement, e.g., the probability of visiting\na specific place and staying time, under the Bayesian framework, considering\nthe influence of both internal and external variables. We use two case studies\nin a shopping mall and in a college student dorm building to show the\neffectiveness of our proposed probabilistic human mobility model. Real\nsurveillance camera data are used to validate the proposed model together with\nsurvey data in the case study of student dorm.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 01:26:15 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Tang", "Bo", ""], ["Jiang", "Chao", ""], ["He", "Haibo", ""], ["Guo", "Yi", ""]]}, {"id": "1606.09296", "submitter": "Mihajlo Grbovic", "authors": "Mihajlo Grbovic, Guy Halawi, Zohar Karnin, Yoelle Maarek", "title": "How Many Folders Do You Really Need?", "comments": "10 pages, 12 figures, Proceedings of the 23rd ACM International\n  Conference on Information and Knowledge Management (CIKM 2014), Shanghai,\n  China", "journal-ref": "Proceedings of the 23rd ACM International Conference on\n  Information and Knowledge Management (CIKM 2014), Shanghai, China", "doi": "10.1145/2661829.2662018.", "report-no": null, "categories": "cs.AI cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Email classification is still a mostly manual task. Consequently, most Web\nmail users never define a single folder. Recently however, automatic\nclassification offering the same categories to all users has started to appear\nin some Web mail clients, such as AOL or Gmail. We adopt this approach, rather\nthan previous (unsuccessful) personalized approaches because of the change in\nthe nature of consumer email traffic, which is now dominated by (non-spam)\nmachine-generated email. We propose here a novel approach for (1) automatically\ndistinguishing between personal and machine-generated email and (2) classifying\nmessages into latent categories, without requiring users to have defined any\nfolder. We report how we have discovered that a set of 6 \"latent\" categories\n(one for human- and the others for machine-generated messages) can explain a\nsignificant portion of email traffic. We describe in details the steps involved\nin building a Web-scale email categorization system, from the collection of\nground-truth labels, the selection of features to the training of models.\nExperimental evaluation was performed on more than 500 billion messages\nreceived during a period of six months by users of Yahoo mail service, who\nelected to be part of such research studies. Our system achieved precision and\nrecall rates close to 90% and the latent categories we discovered were shown to\ncover 70% of both email traffic and email search queries. We believe that these\nresults pave the way for a change of approach in the Web mail industry, and\ncould support the invention of new large-scale email discovery paradigms that\nhad not been possible before.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 21:35:24 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Grbovic", "Mihajlo", ""], ["Halawi", "Guy", ""], ["Karnin", "Zohar", ""], ["Maarek", "Yoelle", ""]]}, {"id": "1606.09610", "submitter": "Jacob Whitehill", "authors": "Jacob Whitehill and Margo Seltzer", "title": "A Crowdsourcing Approach To Collecting Tutorial Videos -- Toward\n  Personalized Learning-at-Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigated the feasibility of crowdsourcing full-fledged tutorial videos\nfrom ordinary people on the Web on how to solve math problems related to\nlogarithms. This kind of approach (a form of learnersourcing) to efficiently\ncollecting tutorial videos and other learning resources could be useful for\nrealizing personalized learning-at-scale, whereby students receive specific\nlearning resources -- drawn from a large and diverse set -- that are tailored\nto their individual and time-varying needs. Results of our study, in which we\ncollected 399 videos from 66 unique \"teachers\" on Mechanical Turk, suggest that\n(1) approximately 100 videos -- over $80\\%$ of which are mathematically fully\ncorrect -- can be crowdsourced per week for \\$5/video; (2) the crowdsourced\nvideos exhibit significant diversity in terms of language style, presentation\nmedia, and pedagogical approach; (3) the average learning gains (posttest minus\npretest score) associated with watching the videos was stat.~sig.~higher than\nfor a control video ($0.105$ versus $0.045$); and (4) the average learning\ngains ($0.1416$) from watching the best tested crowdsourced videos was\ncomparable to the learning gains ($0.1506$) from watching a popular Khan\nAcademy video on logarithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 18:41:30 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 21:56:45 GMT"}, {"version": "v3", "created": "Sat, 22 Apr 2017 18:39:21 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Whitehill", "Jacob", ""], ["Seltzer", "Margo", ""]]}]