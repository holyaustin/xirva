[{"id": "1608.00447", "submitter": "Jihyun Lee", "authors": "Jihyun Lee, Byungmoon Kim, Bongwon Suh, and Eunyee Koh", "title": "Exploring the Front Touch Interface for Virtual Reality Headsets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new interface for virtual reality headset: a\ntouchpad in front of the headset. To demonstrate the feasibility of the front\ntouch interface, we built a prototype device, explored VR UI design space\nexpansion, and performed various user studies. We started with preliminary\ntests to see how intuitively and accurately people can interact with the front\ntouchpad. Then, we further experimented various user interfaces such as a\nbinary selection, a typical menu layout, and a keyboard. Two-Finger and\nDrag-n-Tap were also explored to find the appropriate selection technique. As a\nlow-cost, light-weight, and in low power budget technology, a touch sensor can\nmake an ideal interface for mobile headset. Also, front touch area can be large\nenough to allow wide range of interaction types such as multi-finger\ninteractions. With this novel front touch interface, we paved a way to new\nvirtual reality interaction methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 14:34:50 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Lee", "Jihyun", ""], ["Kim", "Byungmoon", ""], ["Suh", "Bongwon", ""], ["Koh", "Eunyee", ""]]}, {"id": "1608.01611", "submitter": "Harits Ar Rosyid", "authors": "Harits Ar Rosyid, Matt Palmerlee, Ke Chen", "title": "Deploying learning materials to game content for serious education game\n  development: A case study", "comments": null, "journal-ref": null, "doi": "10.1016/j.entcom.2018.01.001", "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate goals of serious education games (SEG) are to facilitate\nlearning and maximizing enjoyment during playing SEGs. In SEG development,\nthere are normally two spaces to be taken into account: knowledge space\nregarding learning materials and content space regarding games to be used to\nconvey learning materials. How to deploy the learning materials seamlessly and\neffectively into game content becomes one of the most challenging problems in\nSEG development. Unlike previous work where experts in education have to be\nused heavily, we proposed a novel approach that works toward minimizing the\nefforts of education experts in mapping learning materials to content space.\nFor a proof-of-concept, we apply the proposed approach in developing an SEG\ngame, named \\emph{Chem Dungeon}, as a case study in order to demonstrate the\neffectiveness of our proposed approach. This SEG game has been tested with a\nnumber of users, and the user survey suggests our method works reasonably well.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 16:56:31 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Rosyid", "Harits Ar", ""], ["Palmerlee", "Matt", ""], ["Chen", "Ke", ""]]}, {"id": "1608.02657", "submitter": "Bin Guo", "authors": "Yan Liu, Bin Guo, Yang Wang, Wenle Wu, Zhiwen Yu, Daqing Zhang", "title": "TaskMe: Multi-Task Allocation in Mobile Crowd Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task allocation or participant selection is a key issue in Mobile Crowd\nSensing (MCS). While previous participant selection approaches mainly focus on\nselecting a proper subset of users for a single MCS task, multi-task-oriented\nparticipant selection is essential and useful for the efficiency of large-scale\nMCS platforms. This paper proposes TaskMe, a participant selection framework\nfor multi-task MCS environments. In particular, two typical multi-task\nallocation situations with bi-objective optimization goals are studied: (1) For\nFPMT (few participants, more tasks), each participant is required to complete\nmultiple tasks and the optimization goal is to maximize the total number of\naccomplished tasks while minimizing the total movement distance. (2) For MPFT\n(more participants, few tasks), each participant is selected to perform one\ntask based on pre-registered working areas in view of privacy, and the\noptimization objective is to minimize total incentive payments while minimizing\nthe total traveling distance. Two optimal algorithms based on the Minimum Cost\nMaximum Flow theory are proposed for FPMT, and two algorithms based on the\nmulti-objective optimization theory are proposed for MPFT. Experiments verify\nthat the proposed algorithms outperform baselines based on a large-scale\nreal-word dataset under different experiment settings (the number of tasks,\nvarious task distributions, etc.).\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 23:43:15 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Liu", "Yan", ""], ["Guo", "Bin", ""], ["Wang", "Yang", ""], ["Wu", "Wenle", ""], ["Yu", "Zhiwen", ""], ["Zhang", "Daqing", ""]]}, {"id": "1608.02661", "submitter": "Bin Guo", "authors": "Bin Guo, Yan Liu, Wenle Wu, Zhiwen Yu, Qi Han", "title": "ActiveCrowd: A Framework for Optimized Multi-Task Allocation in Mobile\n  Crowdsensing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worker selection is a key issue in Mobile Crowd Sensing (MCS). While previous\nworker selection approaches mainly focus on selecting a proper subset of\nworkers for a single MCS task, multi-task-oriented worker selection is\nessential and useful for the efficiency of large-scale MCS platforms. This\npaper proposes ActiveCrowd, a worker selection framework for multi-task MCS\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 23:51:47 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Guo", "Bin", ""], ["Liu", "Yan", ""], ["Wu", "Wenle", ""], ["Yu", "Zhiwen", ""], ["Han", "Qi", ""]]}, {"id": "1608.02829", "submitter": "Ravi Chugh", "authors": "Brian Hempel and Ravi Chugh", "title": "Semi-Automated SVG Programming via Direct Manipulation", "comments": "In 29th ACM User Interface Software and Technology Symposium (UIST\n  2016)", "journal-ref": null, "doi": "10.1145/2984511.2984575", "report-no": null, "categories": "cs.HC cs.GR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct manipulation interfaces provide intuitive and interactive features to\na broad range of users, but they often exhibit two limitations: the built-in\nfeatures cannot possibly cover all use cases, and the internal representation\nof the content is not readily exposed. We believe that if direct manipulation\ninterfaces were to (a) use general-purpose programs as the representation\nformat, and (b) expose those programs to the user, then experts could customize\nthese systems in powerful new ways and non-experts could enjoy some of the\nbenefits of programmable systems.\n  In recent work, we presented a prototype SVG editor called Sketch-n-Sketch\nthat offered a step towards this vision. In that system, the user wrote a\nprogram in a general-purpose lambda-calculus to generate a graphic design and\ncould then directly manipulate the output to indirectly change design\nparameters (i.e. constant literals) in the program in real-time during the\nmanipulation. Unfortunately, the burden of programming the desired\nrelationships rested entirely on the user.\n  In this paper, we design and implement new features for Sketch-n-Sketch that\nassist in the programming process itself. Like typical direct manipulation\nsystems, our extended Sketch-n-Sketch now provides GUI-based tools for drawing\nshapes, relating shapes to each other, and grouping shapes together. Unlike\ntypical systems, however, each tool carries out the user's intention by\ntransforming their general-purpose program. This novel, semi-automated\nprogramming workflow allows the user to rapidly create high-level, reusable\nabstractions in the program while at the same time retaining direct\nmanipulation capabilities. In future work, our approach may be extended with\nmore graphic design features or realized for other application domains.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 15:17:46 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Hempel", "Brian", ""], ["Chugh", "Ravi", ""]]}, {"id": "1608.02833", "submitter": "Mundher Al-Shabi", "authors": "Mundher Al-Shabi, Wooi Ping Cheah, Tee Connie", "title": "Facial Expression Recognition Using a Hybrid CNN-SIFT Aggregator", "comments": "To be appear in LNAI", "journal-ref": null, "doi": "10.1007/978-3-319-69456-6_12", "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving an effective facial expression recognition component is important\nfor a successful human-computer interaction system. Nonetheless, recognizing\nfacial expression remains a challenging task. This paper describes a novel\napproach towards facial expression recognition task. The proposed method is\nmotivated by the success of Convolutional Neural Networks (CNN) on the face\nrecognition problem. Unlike other works, we focus on achieving good accuracy\nwhile requiring only a small sample data for training. Scale Invariant Feature\nTransform (SIFT) features are used to increase the performance on small data as\nSIFT does not require extensive training data to generate useful features. In\nthis paper, both Dense SIFT and regular SIFT are studied and compared when\nmerged with CNN features. Moreover, an aggregator of the models is developed.\nThe proposed approach is tested on the FER-2013 and CK+ datasets. Results\ndemonstrate the superiority of CNN with Dense SIFT over conventional CNN and\nCNN with SIFT. The accuracy even increased when all the models are aggregated\nwhich generates state-of-art results on FER-2013 and CK+ datasets, where it\nachieved 73.4% on FER-2013 and 99.1% on CK+.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 15:21:33 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 07:44:59 GMT"}, {"version": "v3", "created": "Sat, 3 Dec 2016 11:21:39 GMT"}, {"version": "v4", "created": "Thu, 22 Dec 2016 09:07:30 GMT"}, {"version": "v5", "created": "Sat, 12 Aug 2017 02:58:13 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Al-Shabi", "Mundher", ""], ["Cheah", "Wooi Ping", ""], ["Connie", "Tee", ""]]}, {"id": "1608.02977", "submitter": "Tanmay Sinha", "authors": "Tanmay Sinha", "title": "Similarity in Observable Behaviors: A Synthesis of Studies with\n  Implications for Socially-Aware Educational Technology Design", "comments": "LTI Student Research Symposium 2016, Carnegie Mellon University,\n  Pittsburgh PA 15213, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversation is like an intricate partner dance and behavioral convergence,\nor the similarity in observable behaviors of partners over time, can lead to\nshared understanding, changed beliefs and increased rapport. This article\ndescribes a synthesis of three strands of our work on fine-grained analysis of\nconversational interaction in peer tutoring at the paralinguistic and verbal\nlevels, in an attempt to better understand the phenomenon of behavioral\nconvergence and its relationship to social and cognitive constructs.\nImplications for development of socially-aware agents that can improve task\nperformance through convergence to and from the human learner's behavior are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 20:17:50 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Sinha", "Tanmay", ""]]}, {"id": "1608.02991", "submitter": "Eriglen Gani", "authors": "Eriglen Gani, Alda Kika", "title": "Albanian Sign Language (AlbSL) Number Recognition from Both Hand's\n  Gestures Acquired by Kinect Sensors", "comments": null, "journal-ref": null, "doi": "10.14569/IJACSA.2016.070729", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Albanian Sign Language (AlbSL) is relatively new and until now there doesn't\nexist a system that is able to recognize Albanian signs by using natural user\ninterfaces (NUI). The aim of this paper is to present a real-time gesture\nrecognition system that is able to automatically recognize number signs for\nAlbanian Sign Language, captured from signer's both hands. Kinect device is\nused to obtain data streams. Every pixel generated from Kinect device contains\ndepth data information which is used to construct a depth map. Hands\nsegmentation process is performed by applying a threshold constant to depth\nmap. In order to differentiate signer's hands a K-means clustering algorithm is\napplied to partition pixels into two groups corresponding to each signer's\nhands. Centroid distance function is calculated in each hand after extracting\nhand's contour pixels. Fourier descriptors, derived form centroid distance is\nused as a hand shape representation. For each number gesture there are 15\nFourier descriptors coefficients generated which represent uniquely that\ngesture. Every input data is compared against training data set by calculating\nEuclidean distance, using Fourier coefficients. Sign with the lowest Euclidean\ndistance is considered as a match. The system is able to recognize number signs\ncaptured from one hand or both hands. When both signer's hands are used, some\nof the methodology processes are executed in parallel in order to improve the\noverall performance. The proposed system achieves an accuracy of 91% and is\nable to process 55 frames per second.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 22:14:23 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Gani", "Eriglen", ""], ["Kika", "Alda", ""]]}, {"id": "1608.03026", "submitter": "Lucius Schoenbaum", "authors": "Lucius Schoenbaum", "title": "Towards Visual Type Theory as a Mathematical Tool and Mathematical User\n  Interface", "comments": "19 pages, to appear in Joint Proceedings of the FM4M, MathUI, and\n  ThEdu Workshops, Doctoral Program, and Work in Progress at the Conference on\n  Intelligent Computer Mathematics, Bialystok, Poland 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A visual type theory is a cognitive tool that has much in common with\nlanguage, and may be regarded as an exceptional form of spatial text adjunct. A\nmathematical visual type theory, called NPM, has been under development that\ncan be viewed as an early-stage project in mathematical knowledge management\nand mathematical user interface development. We discuss in greater detail the\nnotion of a visual type theory, report on progress towards a usable\nmathematical visual type theory, and discuss the outlook for future work on\nthis project.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 02:10:40 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Schoenbaum", "Lucius", ""]]}, {"id": "1608.03061", "submitter": "Tom Crick", "authors": "Mohamed Mostafa, Tom Crick, Ana C. Calderon and Giles Oatley", "title": "Incorporating Emotion and Personality-Based Analysis in User-Centered\n  Modelling", "comments": "Full paper submitted to the 36th SGAI International Conference on\n  Artificial Intelligence (AI-2016); 15 pages, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding complex user behaviour under various conditions, scenarios and\njourneys can be fundamental to the improvement of the user-experience for a\ngiven system. Predictive models of user reactions, responses -- and in\nparticular, emotions -- can aid in the design of more intuitive and usable\nsystems. Building on this theme, the preliminary research presented in this\npaper correlates events and interactions in an online social network against\nuser behaviour, focusing on personality traits. Emotional context and tone is\nanalysed and modelled based on varying types of sentiments that users express\nin their language using the IBM Watson Developer Cloud tools. The data\ncollected in this study thus provides further evidence towards supporting the\nhypothesis that analysing and modelling emotions, sentiments and personality\ntraits provides valuable insight into improving the user experience of complex\nsocial computer systems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 07:26:56 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Mostafa", "Mohamed", ""], ["Crick", "Tom", ""], ["Calderon", "Ana C.", ""], ["Oatley", "Giles", ""]]}, {"id": "1608.03396", "submitter": "Lun Liu", "authors": "Lun Liu, Hui Wang, Chunyang Wu", "title": "A machine learning method for the large-scale evaluation of urban visual\n  environment", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the size of modern cities in the urbanising age, it is beyond the\nperceptual capacity of most people to develop a good knowledge about the beauty\nand ugliness of the city at every street corner. Correspondingly, for planners,\nit is also difficult to accurately answer questions like 'where are the\nworst-looking places in the city that regeneration should give first\nconsideration', or 'in the fast urbanising cities, how is the city appearance\nchanging', etc. To address this issue, we here present a computer vision method\nfor the large-scale and automatic evaluation of the urban visual environment,\nby leveraging state-of-the-art machine learning techniques and the\nwide-coverage street view images. From the various factors that are at work, we\nchoose two key features, the visual quality of street facade and the continuity\nof street wall, as the starting point of this line of analysis. In order to\ntest the validity of this method, we further compare the machine ratings with\nratings collected on site from 752 passers-by on fifty-six locations. We show\nthat the machine learning model can produce a good estimation of people's real\nvisual experience, and it holds much potential for various tasks in terms of\nurban design evaluation, culture identification, etc.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 08:29:07 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Liu", "Lun", ""], ["Wang", "Hui", ""], ["Wu", "Chunyang", ""]]}, {"id": "1608.03430", "submitter": "Bin Guo", "authors": "Tong Xin, Bin Guo, Zhu Wang, Mingyang Li, Zhiwen Yu", "title": "FreeSense:Indoor Human Identification with WiFi Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human identification plays an important role in human-computer interaction.\nThere have been numerous methods proposed for human identification (e.g., face\nrecognition, gait recognition, fingerprint identification, etc.). While these\nmethods could be very useful under different conditions, they also suffer from\ncertain shortcomings (e.g., user privacy, sensing coverage range). In this\npaper, we propose a novel approach for human identification, which leverages\nWIFI signals to enable non-intrusive human identification in domestic\nenvironments. It is based on the observation that each person has specific\ninfluence patterns to the surrounding WIFI signal while moving indoors,\nregarding their body shape characteristics and motion patterns. The influence\ncan be captured by the Channel State Information (CSI) time series of WIFI.\nSpecifically, a combination of Principal Component Analysis (PCA), Discrete\nWavelet Transform (DWT) and Dynamic Time Warping (DTW) techniques is used for\nCSI waveform-based human identification. We implemented the system in a 6m*5m\nsmart home environment and recruited 9 users for data collection and\nevaluation. Experimental results indicate that the identification accuracy is\nabout 88.9% to 94.5% when the candidate user set changes from 6 to 2, showing\nthat the proposed human identification method is effective in domestic\nenvironments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 12:05:07 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Xin", "Tong", ""], ["Guo", "Bin", ""], ["Wang", "Zhu", ""], ["Li", "Mingyang", ""], ["Yu", "Zhiwen", ""]]}, {"id": "1608.03507", "submitter": "Ramin Rahnamoun", "authors": "Ramin Rahnamoun, Reza Rawassizadeh, Arash Maskooki", "title": "Learning Mobile App Usage Routine through Learning Automata", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its conception, smart app market has grown exponentially. Success in\nthe app market depends on many factors among which the quality of the app is a\nsignificant contributor, such as energy use. Nevertheless, smartphones, as a\nsubset of mobile computing devices. inherit the limited power resource\nconstraint. Therefore, there is a challenge of maintaining the resource while\nincreasing the target app quality. This paper introduces Learning Automata (LA)\nas an online learning method to learn and predict the app usage routines of the\nusers. Such prediction can leverage the app cache functionality of the\noperating system and thus (i) decreases app launch time and (ii) preserve\nbattery. Our algorithm, which is an online learning approach, temporally\nupdates and improves the internal states of itself. In particular, it learns\nthe transition probabilities between app launching. Each App launching instance\nupdates the transition probabilities related to that App, and this will result\nin improving the prediction. We benefit from a real-world lifelogging dataset\nand our experimental results show considerable success with respect to the two\nbaseline methods that are used currently for smartphone app prediction\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 15:43:55 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 08:08:35 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Rahnamoun", "Ramin", ""], ["Rawassizadeh", "Reza", ""], ["Maskooki", "Arash", ""]]}, {"id": "1608.03569", "submitter": "Peter Mancini Peter Mancini", "authors": "Peter Mancini, Benjamin Bengfort, Ben Shneiderman", "title": "Interactive Exploration of the Employment Situation Report: From Fixed\n  Tables to Dynamic Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monthly Bureau of Labor Statistics Employment Situation Report is widely\nanticipated by economists, journalists, and politicians as it is used to\nforecast the economic condition of the United States. The report has broad\nimpact on public and corporate economic confidence; however, the online access\nto this data employs outdated techniques, using a PDF format containing solely\ntext and fixed tabular information. Creating an interactive interface for\ndynamic discovery on the BLS website could elicit more dialogue between the\npublic and government spheres, drawing more traffic to government websites and\ntriggering greater civic engagement. Our work suggests that the implementation\nof interactive visual analysis techniques to enable dynamic discovery leads to\nrapid interpretation of data as well as provides the means to explore the data\nfor further insights. This paper presents two inspirational prototypes: a\ndashboard of interactive visualizations and an interactive time series\nexplorer, allowing for temporal and spatial analyses and enabling users to\ncombine data sets to create their own customized visualization.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 19:01:45 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Mancini", "Peter", ""], ["Bengfort", "Benjamin", ""], ["Shneiderman", "Ben", ""]]}, {"id": "1608.04042", "submitter": "Arturo Deza", "authors": "Arturo Deza and Miguel P. Eckstein", "title": "Can Peripheral Representations Improve Clutter Metrics on Complex\n  Scenes?", "comments": "Pre-Print to be presented at NIPS 2016 in Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have proposed image-based clutter measures that correlate\nwith human search times and/or eye movements. However, most models do not take\ninto account the fact that the effects of clutter interact with the foveated\nnature of the human visual system: visual clutter further from the fovea has an\nincreasing detrimental influence on perception. Here, we introduce a new\nfoveated clutter model to predict the detrimental effects in target search\nutilizing a forced fixation search task. We use Feature Congestion (Rosenholtz\net al.) as our non foveated clutter model, and we stack a peripheral\narchitecture on top of Feature Congestion for our foveated model. We introduce\nthe Peripheral Integration Feature Congestion (PIFC) coefficient, as a\nfundamental ingredient of our model that modulates clutter as a non-linear gain\ncontingent on eccentricity. We finally show that Foveated Feature Congestion\n(FFC) clutter scores r(44) = -0.82 correlate better with target detection (hit\nrate) than regular Feature Congestion r(44) = -0.19 in forced fixation search.\nThus, our model allows us to enrich clutter perception research by computing\nfixation specific clutter maps. A toolbox for creating peripheral\narchitectures: Piranhas: Peripheral Architectures for Natural, Hybrid and\nArtificial Systems will be made available.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 01:07:29 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Deza", "Arturo", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "1608.04162", "submitter": "Stephen Makonin", "authors": "Laura Guzman, Stephen Makonin, Roger Alex Clapp", "title": "CarbonKit: Designing A Personal Carbon Tracking Platform", "comments": "Accepted to be presented at SocialSense '19: Fourth International\n  Workshop on Social Sensing", "journal-ref": null, "doi": "10.1145/3313294.3313385", "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ubiquitous technology platforms have been created to track and improve health\nand fitness; similar technologies can help individuals monitor and reduce their\ncarbon footprints. This paper proposes CarbonKit, a platform combining\ntechnology, markets, and incentives to empower and reward people for reducing\ntheir carbon footprint. We argue that a goal-and-reward behavioral feedback\nloop can be combined with the Big Data available from tracked activities, apps,\nand social media to make CarbonKit an integral part of individuals daily lives.\nCarbonKit comprises five modules that link personal carbon tracking, health and\nfitness, social media, and economic incentives. Protocols for safeguarding\nsecurity, privacy and individuals control over their own data are essential to\nthe design of the CarbonKit. Initially CarbonKit would operate on a voluntary\nbasis, but such a system can also serve as part of a mandatory region-wide\ninitiative. We use the example of the British Columbia to illustrate the\nregulatory framework and participating stakeholders that would be required to\nsupport the CarbonKit in specific jurisdictions.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 23:59:39 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 15:28:17 GMT"}, {"version": "v3", "created": "Thu, 25 Aug 2016 02:13:35 GMT"}, {"version": "v4", "created": "Tue, 19 Feb 2019 20:34:06 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Guzman", "Laura", ""], ["Makonin", "Stephen", ""], ["Clapp", "Roger Alex", ""]]}, {"id": "1608.04236", "submitter": "Andrew Brock", "authors": "Andrew Brock, Theodore Lim, J.M. Ritchie, Nick Weston", "title": "Generative and Discriminative Voxel Modeling with Convolutional Neural\n  Networks", "comments": "9 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working with three-dimensional data, choice of representation is key. We\nexplore voxel-based models, and present evidence for the viability of\nvoxellated representations in applications including shape modeling and object\nclassification. Our key contributions are methods for training voxel-based\nvariational autoencoders, a user interface for exploring the latent space\nlearned by the autoencoder, and a deep convolutional neural network\narchitecture for object classification. We address challenges unique to\nvoxel-based representations, and empirically evaluate our models on the\nModelNet benchmark, where we demonstrate a 51.5% relative improvement in the\nstate of the art for object classification.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 11:14:35 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 08:06:24 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Brock", "Andrew", ""], ["Lim", "Theodore", ""], ["Ritchie", "J. M.", ""], ["Weston", "Nick", ""]]}, {"id": "1608.04652", "submitter": "Francesco Alderisio", "authors": "Francesco Alderisio, Maria Lombardi, Gianfranco Fiore, Mario di\n  Bernardo", "title": "Study of movement coordination in human ensembles via a novel\n  computer-based set-up", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC physics.data-an physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Movement coordination in human ensembles has been studied little in the\ncurrent literature. In the existing experimental works, situations where all\nsubjects are connected with each other through direct visual and auditory\ncoupling, and social interaction affects their coordination, have been\ninvestigated. Here, we study coordination in human ensembles via a novel\ncomputer-based set-up that enables individuals to coordinate each other's\nmotion from a distance so as to minimize the influence of social interaction.\nThe proposed platform makes it possible to implement different visual\ninteraction patterns among the players, so that participants take into\nconsideration the motion of a designated subset of the others. This allows the\nevaluation of the exclusive effects on coordination of the structure of\ninterconnections among the players and their own dynamics. Our set-up enables\nalso the deployment of virtual players to investigate dyadic interaction\nbetween a human and a virtual agent, as well as group synchronization in mixed\nteams of human and virtual agents. We use this novel set-up to study\ncoordination both in dyads and in groups over different structures of\ninterconnections, with and without virtual agents. We find that, in dual\ninteraction, virtual players manage to interact with participants in a\nhuman-like fashion, thus confirming findings in previous work. We also observe\nthat, in group interaction, the level of coordination among humans in the\nabsence of direct visual and auditory coupling depends on the structure of\ninterconnections among participants. This confirms, as recently suggested in\nthe literature, that different coordination levels are achieved over diverse\nvisual pairings in the presence and in the absence of social interaction. We\npresent preliminary experimental results on the effect on group coordination of\ndeploying virtual computer agents in the human ensemble.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 14:24:21 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Alderisio", "Francesco", ""], ["Lombardi", "Maria", ""], ["Fiore", "Gianfranco", ""], ["di Bernardo", "Mario", ""]]}, {"id": "1608.04953", "submitter": "Kapil Dev", "authors": "Kapil Dev, Manfred Lau, Ligang Liu", "title": "A Perceptual Aesthetics Measure for 3D Shapes", "comments": "12 Pages, 8 Figures, Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the problem of image aesthetics has been well explored, the study of 3D\nshape aesthetics has focused on specific manually defined features. In this\npaper, we learn an aesthetics measure for 3D shapes autonomously from raw voxel\ndata and without manually-crafted features by leveraging the strength of deep\nlearning. We collect data from humans on their aesthetics preferences for\nvarious 3D shape classes. We take a deep convolutional 3D shape ranking\napproach to compute a measure that gives an aesthetics score for a 3D shape. We\ndemonstrate our approach with various types of shapes and for applications such\nas aesthetics-based visualization, search, and scene composition.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 13:07:27 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Dev", "Kapil", ""], ["Lau", "Manfred", ""], ["Liu", "Ligang", ""]]}, {"id": "1608.05661", "submitter": "Hamza Harkous", "authors": "Hamza Harkous, Rameez Rahman, Bojan Karlas, Karl Aberer", "title": "The Curious Case of the PDF Converter that Likes Mozart: Dissecting and\n  Mitigating the Privacy Risk of Personal Cloud Apps", "comments": null, "journal-ref": "Proceedings on Privacy Enhancing Technologies. Volume 2016, Issue\n  4, Pages 123-143, ISSN (Online) 2299-0984", "doi": "10.1515/popets-2016-0032", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Third party apps that work on top of personal cloud services such as Google\nDrive and Dropbox, require access to the user's data in order to provide some\nfunctionality. Through detailed analysis of a hundred popular Google Drive apps\nfrom Google's Chrome store, we discover that the existing permission model is\nquite often misused: around two thirds of analyzed apps are over-privileged,\ni.e., they access more data than is needed for them to function. In this work,\nwe analyze three different permission models that aim to discourage users from\ninstalling over-privileged apps. In experiments with 210 real users, we\ndiscover that the most successful permission model is our novel ensemble method\nthat we call Far-reaching Insights. Far-reaching Insights inform the users\nabout the data-driven insights that apps can make about them (e.g., their\ntopics of interest, collaboration and activity patterns etc.) Thus, they seek\nto bridge the gap between what third parties can actually know about users and\nusers perception of their privacy leakage. The efficacy of Far-reaching\nInsights in bridging this gap is demonstrated by our results, as Far-reaching\nInsights prove to be, on average, twice as effective as the current model in\ndiscouraging users from installing over-privileged apps. In an effort for\npromoting general privacy awareness, we deploy a publicly available privacy\noriented app store that uses Far-reaching Insights. Based on the knowledge\nextracted from data of the store's users (over 115 gigabytes of Google Drive\ndata from 1440 users with 662 installed apps), we also delineate the ecosystem\nfor third-party cloud apps from the standpoint of developers and cloud\nproviders. Finally, we present several general recommendations that can guide\nother future works in the area of privacy for the cloud.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 07:36:11 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Harkous", "Hamza", ""], ["Rahman", "Rameez", ""], ["Karlas", "Bojan", ""], ["Aberer", "Karl", ""]]}, {"id": "1608.06986", "submitter": "Benjamin Cowley PhD", "authors": "Benjamin Ultan Cowley, Jari Torniainen", "title": "A short review and primer on electrodermal activity in human computer\n  interaction applications", "comments": "11 pages, 3 figures. Part of journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of psychophysiology in human-computer interaction is a\ngrowing field with significant potential for future smart personalised systems.\nWorking in this emerging field requires comprehension of an array of\nphysiological signals and analysis techniques.\n  One of the most widely used signals is electrodermal activity, or EDA, also\nknown as galvanic skin response or GSR. This signal is commonly used as a proxy\nfor physiological arousal, but recent advances of interpretation and analysis\nsuggest that traditional approaches should be revised. We present a short\nreview on the application of EDA in human-computer interaction.\n  This paper aims to serve as a primer for the novice, enabling rapid\nfamiliarisation with the latest core concepts. We put special emphasis on\neveryday human-computer interface applications to distinguish from the more\ncommon clinical or sports uses of psychophysiology.\n  This paper is an extract from a comprehensive review of the entire field of\nambulatory psychophysiology, including 12 similar chapters, plus application\nguidelines and systematic review. Thus any citation should be made using the\nfollowing reference:\n  B. Cowley, M. Filetti, K. Lukander, J. Torniainen, A. Henelius, L. Ahonen, O.\nBarral, I. Kosunen, T. Valtonen, M. Huotilainen, N. Ravaja, G. Jacucci. The\nPsychophysiology Primer: a guide to methods and a broad review with a focus on\nhuman-computer interaction. Foundations and Trends in Human-Computer\nInteraction, vol. 9, no. 3-4, pp. 150--307, 2016.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 23:38:18 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 12:05:49 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2016 17:54:46 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Cowley", "Benjamin Ultan", ""], ["Torniainen", "Jari", ""]]}, {"id": "1608.07192", "submitter": "Santiago Hors-Fraile", "authors": "Santiago Hors-Fraile, Francisco J N\\'u\\~nez Benjumea, Laura Carrasco\n  Hern\\'andez, Francisco Ortega Ruiz, Luis Fernandez-Luque", "title": "Design of two combined health recommender systems for tailoring messages\n  in a smoking cessation app", "comments": "Please, cite as: Hors-Fraile, S., N\\'u\\~nez Benjumea, F.J., Carrasco\n  Hern\\'andez, L., Ruiz, F.O., Fernandez-Luque, L. (2016) Design of two\n  combined health recommender systems for tailoring messages in a smoking\n  cessation app. International Workshop on Engendering Health with RecSys\n  co-located with ACM RecSys 2016. Boston, MA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we describe the design of two recommender systems (RS)\ndesigned to support the smoking cessation process through a mobile application.\nWe plan to use a hybrid RS (content-based, utility-based, and demographic\nfiltering) to tailor health recommendation messages, and a content-based RS to\nschedule a timely delivery of the message. We also define metrics that we will\nuse to assess their performance, helping people quit smoking when we run the\npilot.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 15:16:47 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 13:32:00 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 16:03:58 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Hors-Fraile", "Santiago", ""], ["Benjumea", "Francisco J N\u00fa\u00f1ez", ""], ["Hern\u00e1ndez", "Laura Carrasco", ""], ["Ruiz", "Francisco Ortega", ""], ["Fernandez-Luque", "Luis", ""]]}, {"id": "1608.07323", "submitter": "Sherry Ruan", "authors": "Sherry Ruan, Jacob O. Wobbrock, Kenny Liou, Andrew Ng, James Landay", "title": "Comparing Speech and Keyboard Text Entry for Short Messages in Two\n  Languages on Touchscreen Phones", "comments": "23 pages", "journal-ref": "Journal Proceedings of the ACM on Interactive, Mobile, Wearable\n  and Ubiquitous Technologies archive Volume 1 Issue 4, December 2017", "doi": "10.1145/3161187", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ubiquity of mobile touchscreen devices like smartphones, two widely\nused text entry methods have emerged: small touch-based keyboards and speech\nrecognition. Although speech recognition has been available on desktop\ncomputers for years, it has continued to improve at a rapid pace, and it is\ncurrently unknown how today's modern speech recognizers compare to\nstate-of-the-art mobile touch keyboards, which also have improved considerably\nsince their inception. To discover both methods' \"upper-bound performance,\" we\nevaluated them in English and Mandarin Chinese on an Apple iPhone 6 Plus in a\nlaboratory setting. Our experiment was carried out using Baidu's Deep Speech 2,\na deep learning-based speech recognition system, and the built-in Qwerty\n(English) or Pinyin (Mandarin) Apple iOS keyboards. We found that with speech\nrecognition, the English input rate was 2.93 times faster (153 vs. 52 WPM), and\nthe Mandarin Chinese input rate was 2.87 times faster (123 vs. 43 WPM) than the\nkeyboard for short message transcription under laboratory conditions for both\nmethods. Furthermore, although speech made fewer errors during entry (5.30% vs.\n11.22% corrected error rate), it left slightly more errors in the final\ntranscribed text (1.30% vs. 0.79% uncorrected error rate). Our results show\nthat comparatively, under ideal conditions for both methods, upper-bound speech\nrecognition performance has greatly improved compared to prior systems, and\nmight see greater uptake in the future, although further study is required to\nquantify performance in non-laboratory settings for both methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 22:09:02 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 02:14:37 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Ruan", "Sherry", ""], ["Wobbrock", "Jacob O.", ""], ["Liou", "Kenny", ""], ["Ng", "Andrew", ""], ["Landay", "James", ""]]}, {"id": "1608.07403", "submitter": "Matt Webster", "authors": "Matt Webster, David Western, Dejanira Araiza-Illan, Clare Dixon,\n  Kerstin Eder, Michael Fisher, Anthony G. Pipe", "title": "A Corroborative Approach to Verification and Validation of Human--Robot\n  Teams", "comments": "49 pages", "journal-ref": null, "doi": "10.1177/0278364919883338", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for the verification and validation (V&V) of robot\nassistants in the context of human-robot interactions (HRI), to demonstrate\ntheir trustworthiness through corroborative evidence of their safety and\nfunctional correctness. Key challenges include the complex and unpredictable\nnature of the real world in which assistant and service robots operate, the\nlimitations on available V&V techniques when used individually, and the\nconsequent lack of confidence in the V&V results. Our approach, called\ncorroborative V&V, addresses these challenges by combining several different\nV&V techniques; in this paper we use formal verification (model checking),\nsimulation-based testing, and user validation in experiments with a real robot.\nWe demonstrate our corroborative V&V approach through a handover task, the most\ncritical part of a complex cooperative manufacturing scenario, for which we\npropose some safety and liveness requirements to verify and validate. We\nconstruct formal models, simulations and an experimental test rig for the HRI.\nTo capture requirements we use temporal logic properties, assertion checkers\nand textual descriptions. This combination of approaches allows V&V of the HRI\ntask at different levels of modelling detail and thoroughness of exploration,\nthus overcoming the individual limitations of each technique. Should the\nresulting V&V evidence present discrepancies, an iterative process between the\ndifferent V&V techniques takes place until corroboration between the V&V\ntechniques is gained from refining and improving the assets (i.e., system and\nrequirement models) to represent the HRI task in a more truthful manner.\nTherefore, corroborative V&V affords a systematic approach to 'meta-V&V,' in\nwhich different V&V techniques can be used to corroborate and check one\nanother, increasing the level of certainty in the results of V&V.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 09:31:14 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 16:55:26 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 18:59:29 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Webster", "Matt", ""], ["Western", "David", ""], ["Araiza-Illan", "Dejanira", ""], ["Dixon", "Clare", ""], ["Eder", "Kerstin", ""], ["Fisher", "Michael", ""], ["Pipe", "Anthony G.", ""]]}, {"id": "1608.07743", "submitter": "Olivia Das", "authors": "Arindam Das, Olivia Das", "title": "Effect of Human Learning on the Transient Performance of Cloud-based\n  Tiered Applications", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud based tiered applications are increasingly becoming popular, be it on\nphones or on desktops. End users of these applications range from novice to\nexpert depending on how experienced they are in using them. With repeated usage\n(practice) of an application, a user's think time gradually decreases, known as\nlearning phenomenon. In contrast to the popular notion of constant mean think\ntime of users across all practice sessions, decrease in mean think time over\npractice sessions does occur due to learning. This decrease gives rise to a\ndifferent system workload thereby affecting the application's short-term\nperformance. However, such impact of learning on performance has never been\naccounted for. In this work we propose a model that accounts for human learning\nbehavior in analyzing the transient (short-term) performance of a 3-tier cloud\nbased application. Our approach is based on a closed queueing network model. We\nsolve the model using discrete event simulation. In addition to the overall\nmean System Response Time (SRT), our model solution also generates the mean\nSRTs for various types (novice, intermediate, expert) of requests submitted by\nusers at various levels of their expertise. We demonstrate that our model can\nbe used to evaluate various what-if scenarios to decide the number of VMs we\nneed for each tier-a VM configuration-that would meet the response time SLA.\nThe results show that the lack of accountability of learning may lead to a\nselection of an inappropriate VM configuration. The results further show that\nthe mean SRTs for various types of requests are better measures to consider in\nVM allocation process in comparison to the overall mean SRT.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 21:44:44 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Das", "Arindam", ""], ["Das", "Olivia", ""]]}, {"id": "1608.07895", "submitter": "Olfa Nasraoui", "authors": "Olfa Nasraoui and Patrick Shafto", "title": "Human-Algorithm Interaction Biases in the Big Data Cycle: A Markov Chain\n  Iterated Learning Framework", "comments": "This research was supported by National Science Foundation grant\n  NSF-1549981", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early supervised machine learning algorithms have relied on reliable expert\nlabels to build predictive models. However, the gates of data generation have\nrecently been opened to a wider base of users who started participating\nincreasingly with casual labeling, rating, annotating, etc. The increased\nonline presence and participation of humans has led not only to a\ndemocratization of unchecked inputs to algorithms, but also to a wide\ndemocratization of the \"consumption\" of machine learning algorithms' outputs by\ngeneral users. Hence, these algorithms, many of which are becoming essential\nbuilding blocks of recommender systems and other information filters, started\ninteracting with users at unprecedented rates. The result is machine learning\nalgorithms that consume more and more data that is unchecked, or at the very\nleast, not fitting conventional assumptions made by various machine learning\nalgorithms. These include biased samples, biased labels, diverging training and\ntesting sets, and cyclical interaction between algorithms, humans, information\nconsumed by humans, and data consumed by algorithms. Yet, the continuous\ninteraction between humans and algorithms is rarely taken into account in\nmachine learning algorithm design and analysis. In this paper, we present a\npreliminary theoretical model and analysis of the mutual interaction between\nhumans and algorithms, based on an iterated learning framework that is inspired\nfrom the study of human language evolution. We also define the concepts of\nhuman and algorithm blind spots and outline machine learning approaches to mend\niterated bias through two novel notions: antidotes and reactive learning.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 02:37:21 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Nasraoui", "Olfa", ""], ["Shafto", "Patrick", ""]]}, {"id": "1608.08041", "submitter": "Benjamin Cowley PhD", "authors": "Niklas Ravaja and Benjamin Cowley and Jari Torniainen", "title": "A short review and primer on electromyography in human computer\n  interaction applications", "comments": "8 pages, 2 figures. Part of a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of psychophysiology in human-computer interaction is a\ngrowing field with significant potential for future smart personalised systems.\nWorking in this emerging field requires comprehension of an array of\nphysiological signals and analysis techniques.\n  Electromyography (EMG) is a useful signal to estimate the emotional context\nof individuals, because it is relatively robust, and simple to record and\nanalyze. Common uses are to infer emotional valence in response to a stimulus,\nand to index some symptoms of stress. However, in order to interpret EMG\nsignals, they must be considered alongside data on physical, social and\nintentional context. Here we present a short review on the application of EMG\nin human-computer interaction.\n  This paper aims to serve as a primer for the novice, enabling rapid\nfamiliarisation with the latest core concepts. We put special emphasis on\neveryday human-computer interface applications to distinguish from the more\ncommon clinical or sports uses of psychophysiology.\n  This paper is an extract from a comprehensive review of the entire field of\nambulatory psychophysiology, including 12 similar chapters, plus application\nguidelines and systematic review. Thus any citation should be made using the\nfollowing reference:\n  B. Cowley, M. Filetti, K. Lukander, J. Torniainen, A. Henelius, L. Ahonen, O.\nBarral, I. Kosunen, T. Valtonen, M. Huotilainen, N. Ravaja, G. Jacucci. The\nPsychophysiology Primer: a guide to methods and a broad review with a focus on\nhuman-computer interaction. Foundations and Trends in Human-Computer\nInteraction, vol. 9, no. 3-4, pp. 150--307, 2016.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 13:23:59 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 17:57:56 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Ravaja", "Niklas", ""], ["Cowley", "Benjamin", ""], ["Torniainen", "Jari", ""]]}, {"id": "1608.08188", "submitter": "Danna Gurari", "authors": "Danna Gurari and Kristen Grauman", "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) systems are emerging from a desire to empower\nusers to ask any natural language question about visual content and receive a\nvalid answer in response. However, close examination of the VQA problem reveals\nan unavoidable, entangled problem that multiple humans may or may not always\nagree on a single answer to a visual question. We train a model to\nautomatically predict from a visual question whether a crowd would agree on a\nsingle answer. We then propose how to exploit this system in a novel\napplication to efficiently allocate human effort to collect answers to visual\nquestions. Specifically, we propose a crowdsourcing system that automatically\nsolicits fewer human responses when answer agreement is expected and more human\nresponses when answer disagreement is expected. Our system improves upon\nexisting crowdsourcing systems, typically eliminating at least 20% of human\neffort with no loss to the information collected from the crowd.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 19:24:25 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Gurari", "Danna", ""], ["Grauman", "Kristen", ""]]}, {"id": "1608.08353", "submitter": "Benjamin Cowley PhD", "authors": "Minna Huotilainen and Benjamin Cowley and Lauri Ahonen", "title": "A short review and primer on event-related potentials in human computer\n  interaction applications", "comments": "8 pages, 1 figure. Part of a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of psychophysiology in human-computer interaction is a\ngrowing field with significant potential for future smart personalised systems.\nWorking in this emerging field requires comprehension of an array of\nphysiological signals and analysis techniques.\n  Event-related potentials, termed ERPs, are a stimulus- or action-locked\nwaveform indicating a characteristic neural response. ERPs derived from\nelectroencephalography have been extensively studied in basic research, and\nhave been applied especially in the field of brain-computer interfaces. For\necologically-valid settings there are considerable challenges to application,\nhowever recent work shows some promise for ERPs outside the lab. Here we\npresent a short review on the application of ERPs in human-computer\ninteraction.\n  This paper aims to serve as a primer for the novice, enabling rapid\nfamiliarisation with the latest core concepts. We put special emphasis on\neveryday human-computer interface applications to distinguish from the more\ncommon clinical or sports uses of psychophysiology.\n  This paper is an extract from a comprehensive review of the entire field of\nambulatory psychophysiology, including 12 similar chapters, plus application\nguidelines and systematic review. Thus any citation should be made using the\nfollowing reference:\n  B. Cowley, M. Filetti, K. Lukander, J. Torniainen, A. Henelius, L. Ahonen, O.\nBarral, I. Kosunen, T. Valtonen, M. Huotilainen, N. Ravaja, G. Jacucci. The\nPsychophysiology Primer: a guide to methods and a broad review with a focus on\nhuman-computer interaction. Foundations and Trends in Human-Computer\nInteraction, vol. 9, no. 3-4, pp. 150--307, 2016.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 08:06:13 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 17:59:59 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Huotilainen", "Minna", ""], ["Cowley", "Benjamin", ""], ["Ahonen", "Lauri", ""]]}, {"id": "1608.08492", "submitter": "Per B{\\ae}kgaard", "authors": "Per B{\\ae}kgaard, Michael Kai Petersen, Jakob Eg Larsen", "title": "Separating Components of Attention and Surprise", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive processes involved in both allocation of attention during decision\nmaking as well as surprise when making mistakes trigger release of the\nneurotransmitter norepinephrine, which has been shown to be correlated with an\nincrease in pupil dilation, in turn reflecting raised levels of arousal.\nExtending earlier experiments based on the Attention Network Test (ANT),\nseparating the neural components of alertness and spatial re-orientation from\nthe attention involved in more demanding conflict resolution tasks, we\ndemonstrate that these signatures of attention are so robust that they may be\nretrieved even when applying low cost eye tracking in an everyday mobile\ncomputing context. Furthermore we find that the reaction of surprise elicited\nwhen committing mistakes in a decision task, which in the neuroimaging EEG\nliterature have been referred to as a negativity feedback error correction\nsignal, may likewise be retrieved solely based on an increase in pupil\ndilation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 15:10:33 GMT"}], "update_date": "2016-09-03", "authors_parsed": [["B\u00e6kgaard", "Per", ""], ["Petersen", "Michael Kai", ""], ["Larsen", "Jakob Eg", ""]]}, {"id": "1608.08711", "submitter": "Ghassem Tofighi", "authors": "Maria Frank, Ghassem Tofighi, Haisong Gu, Renate Fruchter", "title": "Engagement Detection in Meetings", "comments": "The paper has been published on ICCCBE 2016.\n  http://www.see.eng.osaka-u.ac.jp/seeit/icccbe2016/\n  http://www.see.eng.osaka-u.ac.jp/seeit/icccbe2016/download/Tentative_Time_Table_ICCCBE2016_2016-05-10.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Group meetings are frequent business events aimed to develop and conduct\nproject work, such as Big Room design and construction project meetings. To be\neffective in these meetings, participants need to have an engaged mental state.\nThe mental state of participants however, is hidden from other participants,\nand thereby difficult to evaluate. Mental state is understood as an inner\nprocess of thinking and feeling, that is formed of a conglomerate of mental\nrepresentations and propositional attitudes. There is a need to create\ntransparency of these hidden states to understand, evaluate and influence them.\nFacilitators need to evaluate the meeting situation and adjust for higher\nengagement and productivity. This paper presents a framework that defines a\nspectrum of engagement states and an array of classifiers aimed to detect the\nengagement state of participants in real time. The Engagement Framework\nintegrates multi-modal information from 2D and 3D imaging and sound. Engagement\nis detected and evaluated at participants and aggregated at group level. We use\nempirical data collected at the lab of Konica Minolta, Inc. to test initial\napplications of this framework. The paper presents examples of the tested\nengagement classifiers, which are based on research in psychology,\ncommunication, and human computer interaction. Their accuracy is illustrated in\ndyadic interaction for engagement detection. In closing we discuss the\npotential extension to complex group collaboration settings and future feedback\nimplementations.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 02:46:37 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Frank", "Maria", ""], ["Tofighi", "Ghassem", ""], ["Gu", "Haisong", ""], ["Fruchter", "Renate", ""]]}, {"id": "1608.08807", "submitter": "Oswald Barral", "authors": "Oswald Barral", "title": "A short review and primer on pupillometry in human computer interaction\n  applications", "comments": null, "journal-ref": "Foundations and Trends in Human-Computer Interaction, vol. 9, no.\n  3-4, pp. 150-307, 2016", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of psychophysiological signals in human-computer interaction\nis a growing field with significant potential for future smart personalised\nsystems. Working in this emerging field requires comprehension of an array of\nphysiological signals and analysis techniques.\n  Pupillometry has been studied for over a century, but it has just recently\nstarted being used in human-computer interaction setups. Traditionally, pupil\nsize has been used as an indicator of cognitive workload and mental effort.\nHowever, pupil size has been linked to other cognitive processes as well,\nranging from attention to affective processing. We present a short review on\nthe application of pupillometry in human-computer interaction.\n  This paper aims to serve as a primer for the novice, enabling rapid\nfamiliarisation with the latest core concepts. We put special emphasis on\neveryday human-computer interface applications to distinguish from the more\ncommon clinical or sports uses of psychophysiology.\n  This paper is an extract from a comprehensive review of the entire field of\nambulatory psychophysiology, including 12 similar chapters, plus application\nguidelines and systematic review. Thus any citation should be made using the\nfollowing reference: B. Cowley, M. Filetti, K. Lukander, J. Torniainen, A.\nHenelius, L. Ahonen, O. Barral, I. Kosunen, T. Valtonen, M. Huotilainen, N.\nRavaja, G. Jacucci. The Psychophysiology Primer: a guide to methods and a broad\nreview with a focus on human-computer interaction. Foundations and Trends in\nHuman-Computer Interaction, vol. 9, no. 3-4, pp. 150-307, 2016.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 11:10:36 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Barral", "Oswald", ""]]}, {"id": "1608.08936", "submitter": "Paolo Federico", "authors": "Paolo Federico and Silvia Miksch", "title": "Evaluation of two interaction techniques for visualization of dynamic\n  graphs", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several techniques for visualization of dynamic graphs are based on different\nspatial arrangements of a temporal sequence of node-link diagrams. Many studies\nin the literature have investigated the importance of maintaining the user's\nmental map across this temporal sequence, but usually each layout is considered\nas a static graph drawing and the effect of user interaction is disregarded. We\nconducted a task-based controlled experiment to assess the effectiveness of two\nbasic interaction techniques: the adjustment of the layout stability and the\nhighlighting of adjacent nodes and edges. We found that generally both\ninteraction techniques increase accuracy, sometimes at the cost of longer\ncompletion times, and that the highlighting outclasses the stability adjustment\nfor many tasks except the most complex ones.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 16:44:26 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Federico", "Paolo", ""], ["Miksch", "Silvia", ""]]}, {"id": "1608.08953", "submitter": "Margrit Betke", "authors": "Mehrnoosh Sameki, Mattia Gentil, Kate K. Mays, Lei Guo, and Margrit\n  Betke", "title": "Dynamic Allocation of Crowd Contributions for Sentiment Analysis during\n  the 2016 U.S. Presidential Election", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinions about the 2016 U.S. Presidential Candidates have been expressed in\nmillions of tweets that are challenging to analyze automatically. Crowdsourcing\nthe analysis of political tweets effectively is also difficult, due to large\ninter-rater disagreements when sarcasm is involved. Each tweet is typically\nanalyzed by a fixed number of workers and majority voting. We here propose a\ncrowdsourcing framework that instead uses a dynamic allocation of the number of\nworkers. We explore two dynamic-allocation methods: (1) The number of workers\nqueried to label a tweet is computed offline based on the predicted difficulty\nof discerning the sentiment of a particular tweet. (2) The number of crowd\nworkers is determined online, during an iterative crowd sourcing process, based\non inter-rater agreements between labels.We applied our approach to 1,000\ntwitter messages about the four U.S. presidential candidates Clinton, Cruz,\nSanders, and Trump, collected during February 2016. We implemented the two\nproposed methods using decision trees that allocate more crowd efforts to\ntweets predicted to be sarcastic. We show that our framework outperforms the\ntraditional static allocation scheme. It collects opinion labels from the crowd\nat a much lower cost while maintaining labeling accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:20:09 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 18:05:46 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Sameki", "Mehrnoosh", ""], ["Gentil", "Mattia", ""], ["Mays", "Kate K.", ""], ["Guo", "Lei", ""], ["Betke", "Margrit", ""]]}]