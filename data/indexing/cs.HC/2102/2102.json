[{"id": "2102.00036", "submitter": "Soya Park", "authors": "Soya Park, April Wang, Ban Kawas, Q. Vera Liao, David Piorkowski,\n  Marina Danilevsky", "title": "Facilitating Knowledge Sharing from Domain Experts to Data Scientists\n  for Building NLP Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data scientists face a steep learning curve in understanding a new domain for\nwhich they want to build machine learning (ML) models. While input from domain\nexperts could offer valuable help, such input is often limited, expensive, and\ngenerally not in a form readily consumable by a model development pipeline. In\nthis paper, we propose Ziva, a framework to guide domain experts in sharing\nessential domain knowledge to data scientists for building NLP models. With\nZiva, experts are able to distill and share their domain knowledge using domain\nconcept extractors and five types of label justification over a representative\ndata sample. The design of Ziva is informed by preliminary interviews with data\nscientists, in order to understand current practices of domain knowledge\nacquisition process for ML development projects. To assess our design, we run a\nmix-method case-study to evaluate how Ziva can facilitate interaction of domain\nexperts and data scientists. Our results highlight that (1) domain experts are\nable to use Ziva to provide rich domain knowledge, while maintaining low mental\nload and stress levels; and (2) data scientists find Ziva's output helpful for\nlearning essential information about the domain, offering scalability of\ninformation, and lowering the burden on domain experts to share knowledge. We\nconclude this work by experimenting with building NLP models using the Ziva\noutput by our case study.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 19:37:05 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Park", "Soya", ""], ["Wang", "April", ""], ["Kawas", "Ban", ""], ["Liao", "Q. Vera", ""], ["Piorkowski", "David", ""], ["Danilevsky", "Marina", ""]]}, {"id": "2102.00159", "submitter": "Theerawit Wilaiprasitporn", "authors": "Soravitt Sangnark, Phairot Autthasan, Puntawat Ponglertnapakorn,\n  Phudit Chalekarn, Thapanun Sudhawiyangkul, Manatsanan Trakulruangroj, Sarita\n  Songsermsawad, Rawin Assabumrungrat, Supalak Amplod, Kajornvut Ounjai, and\n  Theerawit Wilaiprasitporn", "title": "Revealing Preference in Popular Music Through Familiarity and Brain\n  Response", "comments": null, "journal-ref": "IEEE Sensors Journal, 2021", "doi": "10.1109/JSEN.2021.3073040", "report-no": null, "categories": "cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Music preference was reported as a factor, which could elicit innermost music\nemotion, entailing accurate ground-truth data and music therapy efficiency.\nThis study executes statistical analysis to investigate the distinction of\nmusic preference through familiarity scores, response times (response rates),\nand brain response (EEG). Twenty participants did self-assessment after\nlistening to two types of popular music's chorus section: music without lyrics\n(Melody) and music with lyrics (Song). \\textcolor{red}{We then conduct a music\npreference classification using a support vector machine, random forest, and\nk-nearest neighbors with the familiarity scores, the response rates, and EEG as\nthe feature vectors. The statistical analysis and F1-score of EEG are\ncongruent, which is the brain's right side outperformed its left side in\nclassification performance.} Finally, these behavioral and brain studies\nsupport that preference, familiarity, and response rates can contribute to the\nmusic emotion experiment's design to understand music, emotion, and listener.\nNot only to the music industry, the biomedical and healthcare industry can also\nexploit this experiment to collect data from patients to improve the efficiency\nof healing by music.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 05:56:29 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 12:35:37 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sangnark", "Soravitt", ""], ["Autthasan", "Phairot", ""], ["Ponglertnapakorn", "Puntawat", ""], ["Chalekarn", "Phudit", ""], ["Sudhawiyangkul", "Thapanun", ""], ["Trakulruangroj", "Manatsanan", ""], ["Songsermsawad", "Sarita", ""], ["Assabumrungrat", "Rawin", ""], ["Amplod", "Supalak", ""], ["Ounjai", "Kajornvut", ""], ["Wilaiprasitporn", "Theerawit", ""]]}, {"id": "2102.00179", "submitter": "Tiffany Hwu", "authors": "Tiffany Hwu, Mia Levy, Steven Skorheim, David Huber", "title": "Matching Representations of Explainable Artificial Intelligence and Eye\n  Gaze for Human-Machine Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid non-verbal communication of task-based stimuli is a challenge in\nhuman-machine teaming, particularly in closed-loop interactions such as\ndriving. To achieve this, we must understand the representations of information\nfor both the human and machine, and determine a basis for bridging these\nrepresentations. Techniques of explainable artificial intelligence (XAI) such\nas layer-wise relevance propagation (LRP) provide visual heatmap explanations\nfor high-dimensional machine learning techniques such as deep neural networks.\nOn the side of human cognition, visual attention is driven by the bottom-up and\ntop-down processing of sensory input related to the current task. Since both\nXAI and human cognition should focus on task-related stimuli, there may be\noverlaps between their representations of visual attention, potentially\nproviding a means of nonverbal communication between the human and machine. In\nthis work, we examine the correlations between LRP heatmap explanations of a\nneural network trained to predict driving behavior and eye gaze heatmaps of\nhuman drivers. The analysis is used to determine the feasibility of using such\na technique for enhancing driving performance. We find that LRP heatmaps show\nincreasing levels of similarity with eye gaze according to the task specificity\nof the neural network. We then propose how these findings may assist humans by\nvisually directing attention towards relevant areas. To our knowledge, our work\nprovides the first known analysis of LRP and eye gaze for driving tasks.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 07:42:56 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hwu", "Tiffany", ""], ["Levy", "Mia", ""], ["Skorheim", "Steven", ""], ["Huber", "David", ""]]}, {"id": "2102.00188", "submitter": "Letizia Jaccheri", "authors": "Letizia Jaccheri, Cristina Pereira, Swetlana Fast", "title": "Gender Issues in Computer Science: Lessons Learnt and Reflections for\n  the Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Women are underrepresented in Computer Science disciplines at all levels,\nfrom undergraduate and graduate studies to participation and leadership in\nacademia and industry. Increasing female representation in the field is a grand\nchallenge for academics, policymakers, and society. Although the problem has\nbeen addressed for many years, progress has been difficult to be measured and\ncompared across countries and institutions, and has been invariably slow,\ndespite all the momentum and impulse for change taking place across several\ncountries. Therefore, it is important to reflect on knowledge, experiences,\nsuccesses, and challenges of existing policies, initiatives and interventions.\nThe main goal of this paper is to provide an overview of several initiatives,\nstudies, projects, and their outcomes. It contributes to building a body of\nknowledge about gender aspects in several areas: research, education, projects,\nnetworks and resources. This paper is mainly based on discussions in working\ngroups and the material collected for and during a series of talks on the topic\nheld by the first author and by feedback received by the community. This paper\nprovides the academic community, policymakers, industry and other stakeholders\nwith numerous examples of best practices, as well as studies and\nrecommendations on how to address key challenges about attracting, retaining,\nencouraging, and inspiring women to pursue a career in Computer Science. Future\nwork should address the issue in a systematic and research based way.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 08:47:39 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Jaccheri", "Letizia", ""], ["Pereira", "Cristina", ""], ["Fast", "Swetlana", ""]]}, {"id": "2102.00226", "submitter": "Matteo Macchini", "authors": "Matteo Macchini, Manana Lortkipanidze, Fabrizio Schiano and Dario\n  Floreano", "title": "The Impact of Virtual Reality and Viewpoints in Body Motion Based Drone\n  Teleoperation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The operation of telerobotic systems can be a challenging task, requiring\nintuitive and efficient interfaces to enable inexperienced users to attain a\nhigh level of proficiency. Body-Machine Interfaces (BoMI) represent a promising\nalternative to standard control devices, such as joysticks, because they\nleverage intuitive body motion and gestures. It has been shown that the use of\nVirtual Reality (VR) and first-person view perspectives can increase the user's\nsense of presence in avatars. However, it is unclear if these beneficial\neffects occur also in the teleoperation of non-anthropomorphic robots that\ndisplay motion patterns different from those of humans. Here we describe\nexperimental results on teleoperation of a non-anthropomorphic drone showing\nthat VR correlates with a higher sense of spatial presence, whereas viewpoints\nmoving coherently with the robot are associated with a higher sense of\nembodiment. Furthermore, the experimental results show that spontaneous body\nmotion patterns are affected by VR and viewpoint conditions in terms of\nvariability, amplitude, and robot correlates, suggesting that the design of\nBoMIs for drone teleoperation must take into account the use of Virtual Reality\nand the choice of the viewpoint.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 13:33:21 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Macchini", "Matteo", ""], ["Lortkipanidze", "Manana", ""], ["Schiano", "Fabrizio", ""], ["Floreano", "Dario", ""]]}, {"id": "2102.00259", "submitter": "Panagiotis Kourtesis P.K.", "authors": "Sebastian Vizcay, Panagiotis Kourtesis, Ferran Argelaguet, Claudio\n  Pacchierotti, and Maud Marchal", "title": "Electrotactile Feedback in Virtual Reality For Precise and Accurate\n  Contact Rendering", "comments": "8 pages, 1 table, 7 figures, under review in Transactions on Haptics.\n  This work has been submitted to the IEEE for possible publication. Copyright\n  may be transferred without notice, after which this version may no longer be\n  accessible.Upon acceptance of the article by IEEE, the preprint article will\n  be replaced with the accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a wearable electrotactile feedback system to enable\nprecise and accurate contact rendering with virtual objects for mid-air\ninteractions. In particular, we propose the use of electrotactile feedback to\nrender the interpenetration distance between the user's finger and the virtual\ncontent is touched. Our approach consists of modulating the perceived intensity\n(frequency and pulse width modulation) of the electrotactile stimuli according\nto the registered interpenetration distance. In a user study (N=21), we\nassessed the performance of four different interpenetration feedback\napproaches: electrotactile-only, visual-only, electrotactile and visual, and no\ninterpenetration feedback. First, the results showed that contact precision and\naccuracy were significantly improved when using interpenetration feedback.\nSecond, and more interestingly, there were no significant differences between\nvisual and electrotactile feedback when the calibration was optimized and the\nuser was familiarized with electrotactile feedback. Taken together, these\nresults suggest that electrotactile feedback could be an efficient replacement\nof visual feedback for accurate and precise contact rendering in virtual\nreality avoiding the need of active visual focus and the rendering of\nadditional visual artefacts.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 16:05:29 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Vizcay", "Sebastian", ""], ["Kourtesis", "Panagiotis", ""], ["Argelaguet", "Ferran", ""], ["Pacchierotti", "Claudio", ""], ["Marchal", "Maud", ""]]}, {"id": "2102.00322", "submitter": "Vaneet Aggarwal", "authors": "Mayank Gupta and Lingjun Chen and Denny Yu and Vaneet Aggarwal", "title": "A Supervised Learning Approach for Robust Health Monitoring using Face\n  Videos", "comments": "The main part of the paper appeared in DFHS'20: Proceedings of the\n  2nd ACM Workshop on Device-Free Human Sensing; while the Supplementary did\n  not appear in the proceedings", "journal-ref": "Proceedings of the 2nd ACM Workshop on Device-Free Human Sensing\n  (DFHS 2020) Nov. 2020 pp. 6-10", "doi": "10.1145/3427772.3429392", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring of cardiovascular activity is highly desired and can enable novel\napplications in diagnosing potential cardiovascular diseases and maintaining an\nindividual's well-being. Currently, such vital signs are measured using\nintrusive contact devices such as an electrocardiogram (ECG), chest straps, and\npulse oximeters that require the patient or the health provider to manually\nimplement. Non-contact, device-free human sensing methods can eliminate the\nneed for specialized heart and blood pressure monitoring equipment. Non-contact\nmethods can have additional advantages since they are scalable with any\nenvironment where video can be captured, can be used for continuous\nmeasurements, and can be used on patients with varying levels of dexterity and\nindependence, from people with physical impairments to infants (e.g., baby\ncamera). In this paper, we used a non-contact method that only requires face\nvideos recorded using commercially-available webcams. These videos were\nexploited to predict the health attributes like pulse rate and variance in\npulse rate. The proposed approach used facial recognition to detect the face in\neach frame of the video using facial landmarks, followed by supervised learning\nusing deep neural networks to train the machine learning model. The videos\ncaptured subjects performing different physical activities that result in\nvarying cardiovascular responses. The proposed method did not require training\ndata from every individual and thus the prediction can be obtained for the new\nindividuals for which there is no prior data; critical in approach\ngeneralization. The approach was also evaluated on a dataset of people with\ndifferent ethnicity. The proposed approach had less than a 4.6\\% error in\npredicting the pulse rate.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 22:03:16 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gupta", "Mayank", ""], ["Chen", "Lingjun", ""], ["Yu", "Denny", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "2102.00414", "submitter": "Michael Thomas Knierim", "authors": "Michael Thomas Knierim, Christoph Berger, Pierluigi Reali", "title": "Open-Source Concealed EEG Data Collection for Brain-Computer-Interfaces\n  -- Real-World Neural Observation Through OpenBCI Amplifiers with\n  Around-the-Ear cEEGrid Electrodes", "comments": "28 pages, 14 figures, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Observing brain activity in real-world settings offers exciting possibilities\nlike the support of physical health, mental well-being, and thought-controlled\ninteraction modalities. The development of such applications is, however,\nstrongly impeded by poor accessibility to research-grade neural data and by a\nlack of easy-to-use and comfortable sensors. This work presents the\ncost-effective adaptation of concealed around-the-ear EEG electrodes (cEEGrids)\nto the open-source OpenBCI EEG signal acquisition platform to provide a\npromising new toolkit. An integrated system design is described, that combines\npublicly available electronics components with newly designed 3D-printed parts\nto form an easily replicable, versatile, single-unit around-the-ear EEG\nrecording system for prolonged use and easy application development. To\ndemonstrate the system's feasibility, observations of experimentally induced\nchanges in visual stimulation and mental workload are presented. Lastly, as\nthere have been no applications of the cEEGrids to HCI contexts, a novel\napplication area for the system is investigated, namely the observation of flow\nexperiences through observation of temporal Alpha power changes. Support for a\nlink between temporal Alpha power and flow is found, which indicates an\nefficient engagement of verbal-analytic reasoning with intensified flow\nexperiences, and specifically intensified task absorption.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 08:57:50 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Knierim", "Michael Thomas", ""], ["Berger", "Christoph", ""], ["Reali", "Pierluigi", ""]]}, {"id": "2102.00423", "submitter": "Reza Hadi Mogavi", "authors": "Reza Hadi Mogavi, Xiaojuan Ma, Pan Hui", "title": "Characterizing Student Engagement Moods for Dropout Prediction in\n  Question Pool Websites", "comments": "Accepted in the 24th ACM Conference on Computer-Supported Cooperative\n  Work and Social Computing (CSCW 2021)", "journal-ref": null, "doi": "10.1145/3449086", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problem-Based Learning (PBL) is a popular approach to instruction that\nsupports students to get hands-on training by solving problems. Question Pool\nwebsites (QPs) such as LeetCode, Code Chef, and Math Playground help PBL by\nsupplying authentic, diverse, and contextualized questions to students.\nNonetheless, empirical findings suggest that 40% to 80% of students registered\nin QPs drop out in less than two months. This research is the first attempt to\nunderstand and predict student dropouts from QPs via exploiting students'\nengagement moods. Adopting a data-driven approach, we identify five different\nengagement moods for QP students, which are namely challenge-seeker,\nsubject-seeker, interest-seeker, joy-seeker, and non-seeker. We find that\nstudents have collective preferences for answering questions in each engagement\nmood, and deviation from those preferences increases their probability of\ndropping out significantly. Last but not least, this paper contributes by\nintroducing a new hybrid machine learning model (we call Dropout-Plus) for\npredicting student dropouts in QPs. The test results on a popular QP in China,\nwith nearly 10K students, show that Dropout-Plus can exceed the rival\nalgorithms' dropout prediction performance in terms of accuracy, F1-measure,\nand AUC. We wrap up our work by giving some design suggestions to QP managers\nand online learning professionals to reduce their student dropouts.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 10:30:19 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 19:15:09 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Mogavi", "Reza Hadi", ""], ["Ma", "Xiaojuan", ""], ["Hui", "Pan", ""]]}, {"id": "2102.00426", "submitter": "Teja Kanchinadam", "authors": "Teja Kanchinadam, Qian You, Keith Westpfahl, James Kim, Siva Gunda,\n  Sebastian Seith, Glenn Fung", "title": "A Simple yet Brisk and Efficient Active Learning Platform for Text\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, we propose the use of a fully managed machine learning service,\nwhich utilizes active learning to directly build models from unstructured data.\nWith this tool, business users can quickly and easily build machine learning\nmodels and then directly deploy them into a production ready hosted environment\nwithout much involvement from data scientists. Our approach leverages\nstate-of-the-art text representation like OpenAI's GPT2 and a fast\nimplementation of the active learning workflow that relies on a simple\nconstruction of incremental learning using linear models, thus providing a\nbrisk and efficient labeling experience for the users. Experiments on both\npublicly available and real-life insurance datasets empirically show why our\nchoices of simple and fast classification algorithms are ideal for the task at\nhand.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 10:44:04 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Kanchinadam", "Teja", ""], ["You", "Qian", ""], ["Westpfahl", "Keith", ""], ["Kim", "James", ""], ["Gunda", "Siva", ""], ["Seith", "Sebastian", ""], ["Fung", "Glenn", ""]]}, {"id": "2102.00464", "submitter": "Kelly Wagman", "authors": "Kelly B. Wagman and Lisa Parks", "title": "Beyond the Command: Feminist STS Research and Critical Issues for the\n  Design of Social Machines", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machines, from artificially intelligent digital assistants to embodied\nrobots, are becoming more pervasive in everyday life. Drawing on feminist\nscience and technology studies (STS) perspectives, we demonstrate how machine\ndesigners are not just crafting neutral objects, but relationships between\nmachines and humans that are entangled in human social issues such as gender\nand power dynamics. Thus, in order to create a more ethical and just future,\nthe dominant assumptions currently underpinning the design of these\nhuman-machine relations must be challenged and reoriented toward relations of\njustice and inclusivity. This paper contributes the \"social machine\" as a model\nfor technology designers who seek to recognize the importance, diversity and\ncomplexity of the social in their work, and to engage with the agential power\nof machines. In our model, the social machine is imagined as a potentially\nequitable relationship partner that has agency and as an \"other\" that is\ndistinct from, yet related to, humans, objects, and animals. We critically\nexamine and contrast our model with tendencies in robotics that consider robots\nas tools, human companions, animals or creatures, and/or slaves. In doing so,\nwe demonstrate ingrained dominant assumptions about human-machine relations and\nreveal the challenges of radical thinking in the social machine design space.\nFinally, we present two design challenges based on non-anthropomorphic\nfiguration and mutuality, and call for experimentation, unlearning dominant\ntendencies, and reimagining of sociotechnical futures.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 15:00:35 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wagman", "Kelly B.", ""], ["Parks", "Lisa", ""]]}, {"id": "2102.00482", "submitter": "Alejandro Bellogin", "authors": "Alejandro Bellog\\'in and Alan Said", "title": "Improving Accountability in Recommender Systems Research Through\n  Reproducibility", "comments": "Submitted in Nov 2020 to the Special Issue on \"Fair, Accountable, and\n  Transparent Recommender Systems\" at User Modeling and User-Adapted\n  Interaction journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibility is a key requirement for scientific progress. It allows the\nreproduction of the works of others, and, as a consequence, to fully trust the\nreported claims and results. In this work, we argue that, by facilitating\nreproducibility of recommender systems experimentation, we indirectly address\nthe issues of accountability and transparency in recommender systems research\nfrom the perspectives of practitioners, designers, and engineers aiming to\nassess the capabilities of published research works. These issues have become\nincreasingly prevalent in recent literature. Reasons for this include societal\nmovements around intelligent systems and artificial intelligence striving\ntowards fair and objective use of human behavioral data (as in Machine\nLearning, Information Retrieval, or Human-Computer Interaction). Society has\ngrown to expect explanations and transparency standards regarding the\nunderlying algorithms making automated decisions for and around us.\n  This work surveys existing definitions of these concepts, and proposes a\ncoherent terminology for recommender systems research, with the goal to connect\nreproducibility to accountability. We achieve this by introducing several\nguidelines and steps that lead to reproducible and, hence, accountable\nexperimental workflows and research. We additionally analyze several\ninstantiations of recommender system implementations available in the\nliterature, and discuss the extent to which they fit in the introduced\nframework. With this work, we aim to shed light on this important problem, and\nfacilitate progress in the field by increasing the accountability of research.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 16:24:13 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bellog\u00edn", "Alejandro", ""], ["Said", "Alan", ""]]}, {"id": "2102.00576", "submitter": "Ruolin Wang", "authors": "Ruolin Wang, Zixuan Chen, Mingrui \"Ray\" Zhang, Zhaoheng Li, Zhixiu\n  Liu, Zihan Dang, Chun Yu, Xiang \"Anthony\" Chen", "title": "Revamp: Enhancing Accessible Information Seeking Experience of Online\n  Shopping for Blind or Low Vision Users", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445547", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online shopping has become a valuable modern convenience, but blind or low\nvision (BLV) users still face significant challenges using it, because of: 1)\ninadequate image descriptions and 2) the inability to filter large amounts of\ninformation using screen readers. To address those challenges, we propose\nRevamp, a system that leverages customer reviews for interactive information\nretrieval. Revamp is a browser integration that supports review-based\nquestion-answering interactions on a reconstructed product page. From our\ninterview, we identified four main aspects (color, logo, shape, and size) that\nare vital for BLV users to understand the visual appearance of a product. Based\non the findings, we formulated syntactic rules to extract review snippets,\nwhich were used to generate image descriptions and responses to users' queries.\nEvaluations with eight BLV users showed that Revamp 1) provided useful\ndescriptive information for understanding product appearance and 2) helped the\nparticipants locate key information efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 00:53:09 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wang", "Ruolin", ""], ["Chen", "Zixuan", ""], ["Zhang", "Mingrui \"Ray\"", ""], ["Li", "Zhaoheng", ""], ["Liu", "Zhixiu", ""], ["Dang", "Zihan", ""], ["Yu", "Chun", ""], ["Chen", "Xiang \"Anthony\"", ""]]}, {"id": "2102.00581", "submitter": "Karthik Mahadevan", "authors": "Karthik Mahadevan, Maur\\'icio Sousa, Anthony Tang, Tovi Grossman", "title": "\"Grip-that-there\": An Investigation of Explicit and Implicit Task\n  Allocation Techniques for Human-Robot Collaboration", "comments": "To be published in Proceedings of the 2021 CHI Conference on Human\n  Factors in Computing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ad-hoc human-robot collaboration (HRC), humans and robots work on a task\nwithout pre-planning the robot's actions prior to execution; instead, task\nallocation occurs in real-time. However, prior research has largely focused on\ntask allocations that are pre-planned - there has not been a comprehensive\nexploration or evaluation of techniques where task allocation is adjusted in\nreal-time. Inspired by HCI research on territoriality and proxemics, we propose\na design space of novel task allocation techniques including both explicit\ntechniques, where the user maintains agency, and implicit techniques, where the\nefficiency of automation can be leveraged. The techniques were implemented and\nevaluated using a tabletop HRC simulation in VR. A 16-participant study, which\npresented variations of a collaborative block stacking task, showed that\nimplicit techniques enable efficient task completion and task parallelization,\nand should be augmented with explicit mechanisms to provide users with\nfine-grained control.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 01:09:17 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 02:34:20 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Mahadevan", "Karthik", ""], ["Sousa", "Maur\u00edcio", ""], ["Tang", "Anthony", ""], ["Grossman", "Tovi", ""]]}, {"id": "2102.00589", "submitter": "Demetrios Lambropoulos", "authors": "Demetrios Lambropoulos, Mohammad Yousefvand, Narayan Mandayam", "title": "Tale of Seven Alerts: Enhancing Wireless Emergency Alerts (WEAs) to\n  Reduce Cellular Network Usage During Disasters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In weather disasters, first responders access dedicated communication\nchannels different from civilian commercial channels to facilitate rescues.\nHowever, rescues in recent disasters have increasingly involved civilian and\nvolunteer forces, requiring civilian channels not to be overloaded with\ntraffic. We explore seven enhancements to the wording of Wireless Emergency\nAlerts (WEAs) and their effectiveness in getting smartphone users to comply,\nincluding reducing frivolous mobile data consumption during critical weather\ndisasters. We conducted a between-subjects survey (N=898), in which\nparticipants were either assigned no alert (control) or an alert framed as\nBasic Information, Altruism, Multimedia, Negative Feedback, Positive Feedback,\nReward, or Punishment. We find that Basic Information alerts resulted in the\nlargest reduction of multimedia and video services usage; we also find that\nPunishment alerts have the lowest absolute compliance. This work has\nimplications for creating more effective WEAs and providing a better\nunderstanding of how wording can affect emergency alert compliance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 02:03:26 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lambropoulos", "Demetrios", ""], ["Yousefvand", "Mohammad", ""], ["Mandayam", "Narayan", ""]]}, {"id": "2102.00593", "submitter": "Maia Jacobs", "authors": "Maia Jacobs, Jeffrey He, Melanie F. Pradier, Barbara Lam, Andrew C.\n  Ahn, Thomas H. McCoy, Roy H. Perlis, Finale Doshi-Velez, Krzysztof Z. Gajos", "title": "Designing AI for Trust and Collaboration in Time-Constrained Medical\n  Decisions: A Sociotechnical Lens", "comments": "To appear in ACM CHI Conference on Human Factors in Computing Systems\n  (CHI '21), May 8-13, 2021, Yokohama, Japan", "journal-ref": null, "doi": "10.1145/3411764.3445385", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Major depressive disorder is a debilitating disease affecting 264 million\npeople worldwide. While many antidepressant medications are available, few\nclinical guidelines support choosing among them. Decision support tools (DSTs)\nembodying machine learning models may help improve the treatment selection\nprocess, but often fail in clinical practice due to poor system integration.\n  We use an iterative, co-design process to investigate clinicians' perceptions\nof using DSTs in antidepressant treatment decisions. We identify ways in which\nDSTs need to engage with the healthcare sociotechnical system, including\nclinical processes, patient preferences, resource constraints, and domain\nknowledge. Our results suggest that clinical DSTs should be designed as\nmulti-user systems that support patient-provider collaboration and offer\non-demand explanations that address discrepancies between predictions and\ncurrent standards of care. Through this work, we demonstrate how current trends\nin explainable AI may be inappropriate for clinical environments and consider\npaths towards designing these tools for real-world medical systems.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 02:25:30 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Jacobs", "Maia", ""], ["He", "Jeffrey", ""], ["Pradier", "Melanie F.", ""], ["Lam", "Barbara", ""], ["Ahn", "Andrew C.", ""], ["McCoy", "Thomas H.", ""], ["Perlis", "Roy H.", ""], ["Doshi-Velez", "Finale", ""], ["Gajos", "Krzysztof Z.", ""]]}, {"id": "2102.00625", "submitter": "Gabriel Lima", "authors": "Gabriel Lima, Nina Grgi\\'c-Hla\\v{c}a, Meeyoung Cha", "title": "Human Perceptions on Moral Responsibility of AI: A Case Study in\n  AI-Assisted Bail Decision-Making", "comments": "17 Pages, 5 Figures, ACM CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445260", "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to attribute responsibility for autonomous artificial intelligence (AI)\nsystems' actions has been widely debated across the humanities and social\nscience disciplines. This work presents two experiments ($N$=200 each) that\nmeasure people's perceptions of eight different notions of moral responsibility\nconcerning AI and human agents in the context of bail decision-making. Using\nreal-life adapted vignettes, our experiments show that AI agents are held\ncausally responsible and blamed similarly to human agents for an identical\ntask. However, there was a meaningful difference in how people perceived these\nagents' moral responsibility; human agents were ascribed to a higher degree of\npresent-looking and forward-looking notions of responsibility than AI agents.\nWe also found that people expect both AI and human decision-makers and advisors\nto justify their decisions regardless of their nature. We discuss policy and\nHCI implications of these findings, such as the need for explainable AI in\nhigh-stakes scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 04:07:38 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lima", "Gabriel", ""], ["Grgi\u0107-Hla\u010da", "Nina", ""], ["Cha", "Meeyoung", ""]]}, {"id": "2102.00672", "submitter": "Carlo Pinciroli", "authors": "Jayam Patel, Tyagaraja Ramaswamy, Zhi Li, Carlo Pinciroli", "title": "Direct and Indirect Communication in Multi-Human Multi-Robot Interaction", "comments": "10 pages, submitted to IEEE Transactions on Human-Machine Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can multiple humans interact with multiple robots? The goal of our\nresearch is to create an effective interface that allows multiple operators to\ncollaboratively control teams of robots in complex tasks. In this paper, we\nfocus on a key aspect that affects our exploration of the design space of\nhuman-robot interfaces -- inter-human communication. More specifically, we\nstudy the impact of direct and indirect communication on several metrics, such\nas awareness, workload, trust, and interface usability. In our experiments, the\nparticipants can engage directly through verbal communication, or indirectly by\nrepresenting their actions and intentions through our interface. We report the\nresults of a user study based on a collective transport task involving 18 human\nsubjects and 9 robots. Our study suggests that combining both direct and\nindirect communication is the best approach for effective multi-human /\nmulti-robot interaction.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 07:19:10 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Patel", "Jayam", ""], ["Ramaswamy", "Tyagaraja", ""], ["Li", "Zhi", ""], ["Pinciroli", "Carlo", ""]]}, {"id": "2102.01024", "submitter": "Chenglong Wang", "authors": "Chenglong Wang and Yu Feng and Rastislav Bodik and Isil Dillig and\n  Alvin Cheung and Amy J. Ko", "title": "Falx: Synthesis-Powered Visualization Authoring", "comments": "CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445249", "report-no": null, "categories": "cs.HC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern visualization tools aim to allow data analysts to easily create\nexploratory visualizations. When the input data layout conforms to the\nvisualization design, users can easily specify visualizations by mapping data\ncolumns to visual channels of the design. However, when there is a mismatch\nbetween data layout and the design, users need to spend significant effort on\ndata transformation.\n  We propose Falx, a synthesis-powered visualization tool that allows users to\nspecify visualizations in a similarly simple way but without needing to worry\nabout data layout. In Falx, users specify visualizations using examples of how\nconcrete values in the input are mapped to visual channels, and Falx\nautomatically infers the visualization specification and transforms the data to\nmatch the design. In a study with 33 data analysts on four visualization tasks\ninvolving data transformation, we found that users can effectively adopt Falx\nto create visualizations they otherwise cannot implement.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:01:20 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wang", "Chenglong", ""], ["Feng", "Yu", ""], ["Bodik", "Rastislav", ""], ["Dillig", "Isil", ""], ["Cheung", "Alvin", ""], ["Ko", "Amy J.", ""]]}, {"id": "2102.01074", "submitter": "Iuliana Marin", "authors": "Iuliana Marin, Ioana-Andreea Dinescu, Teodora-Coralia Deleanu, Lujain\n  Alshikh Sulaiman, Sarmad Monadel Sabree Al-Gayar, Nicolae Goga", "title": "Brain Performance Analysis based on an Electroencephalogram Headset", "comments": null, "journal-ref": "Electronics, Computers and Artificial Intelligence 2020\n  INTERNATIONAL CONFERENCE", "doi": "10.1109/ECAI50035.2020.9223232", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deficit of attention, anxiety, sleep disorders are some of the problems which\naffect many persons. As these issues can evolve into severe conditions, more\nfactors should be taken into consideration. The paper proposes a conception\nwhich aims to help students to enhance their brain performance. An\nelectrocephalogram headset is used to trigger the brainwaves, along with a web\napplication which manages the input data which comes from the headset and from\nthe user. Factors like current activity, mood, focus, stress, relaxation,\nengagement, excitement and interest are provided in numerical format through\nthe use of the headset. The users offer information about their activities\nrelated to relaxation, listening to music, watching a movie, and studying.\nBased on the analysis, it was found that the users consider the application\neasy to use. As the users are more equilibrated emotionally, their results are\nimproved. This allowed the persons to be more confident on themselves. In the\ncase of students, the neurofeedback can be studied for the better sport and\nartistic performances, including the case of the attention deficit\nhyperactivity disorder. Aptitudes for a subject can be determined based on the\nrelevant generated brainwaves. The learning environment is an important factor\nduring the analysis of the results. Teachers, professors, students and parents\ncan collaborate and, based on the gathered data, new teaching methods can be\nadopted in the classroom and at home. The proposed solution can guide the\nstudents while studying, as well as the persons who wish to be more productive\nwhile solving their tasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 10:57:49 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Marin", "Iuliana", ""], ["Dinescu", "Ioana-Andreea", ""], ["Deleanu", "Teodora-Coralia", ""], ["Sulaiman", "Lujain Alshikh", ""], ["Al-Gayar", "Sarmad Monadel Sabree", ""], ["Goga", "Nicolae", ""]]}, {"id": "2102.01116", "submitter": "Michael Chary", "authors": "Michael Chary, Ed W Boyer, Michele M Burns", "title": "Diagnosis of Acute Poisoning Using Explainable Artificial Intelligence", "comments": "Parts submitted to HICSS 54", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical toxicology is the clinical specialty that treats the toxic effects of\nsubstances, be it an overdose, a medication error, or a scorpion sting. The\nvolume of toxicological knowledge and research has, as with other medical\nspecialties, outstripped the ability of the individual clinician to entirely\nmaster and stay current with it. The application of machine learning techniques\nto medical toxicology is challenging because initial treatment decisions are\noften based on a few pieces of textual data and rely heavily on prior\nknowledge. ML techniques often do not represent knowledge in a way that is\ntransparent for the physician, raising barriers to usability. Rule-based\nsystems and decision tree learning are more transparent approaches, but often\ngeneralize poorly and require expert curation to implement and maintain. Here,\nwe construct a probabilistic logic network to represent a portion of the\nknowledge base of a medical toxicologist. Our approach transparently mimics the\nknowledge representation and clinical decision-making of practicing clinicians.\nThe software, dubbed Tak, performs comparably to humans on straightforward\ncases and intermediate difficulty cases, but is outperformed by humans on\nchallenging clinical cases. Tak outperforms a decision tree classifier at all\nlevels of difficulty. Probabilistic logic provides one form of explainable\nartificial intelligence that may be more acceptable for use in healthcare, if\nit can achieve acceptable levels of performance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 19:16:59 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chary", "Michael", ""], ["Boyer", "Ed W", ""], ["Burns", "Michele M", ""]]}, {"id": "2102.01196", "submitter": "Hao-Fei Cheng", "authors": "Hao-Fei Cheng, Logan Stapleton, Ruiqi Wang, Paige Bullock, Alexandra\n  Chouldechova, Zhiwei Steven Wu and Haiyi Zhu", "title": "Soliciting Stakeholders' Fairness Notions in Child Maltreatment\n  Predictive Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445308", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in fair machine learning has proposed dozens of technical\ndefinitions of algorithmic fairness and methods for enforcing these\ndefinitions. However, we still lack an understanding of how to develop machine\nlearning systems with fairness criteria that reflect relevant stakeholders'\nnuanced viewpoints in real-world contexts. To address this gap, we propose a\nframework for eliciting stakeholders' subjective fairness notions. Combining a\nuser interface that allows stakeholders to examine the data and the algorithm's\npredictions with an interview protocol to probe stakeholders' thoughts while\nthey are interacting with the interface, we can identify stakeholders' fairness\nbeliefs and principles. We conduct a user study to evaluate our framework in\nthe setting of a child maltreatment predictive system. Our evaluations show\nthat the framework allows stakeholders to comprehensively convey their fairness\nviewpoints. We also discuss how our results can inform the design of predictive\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 21:49:27 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Cheng", "Hao-Fei", ""], ["Stapleton", "Logan", ""], ["Wang", "Ruiqi", ""], ["Bullock", "Paige", ""], ["Chouldechova", "Alexandra", ""], ["Wu", "Zhiwei Steven", ""], ["Zhu", "Haiyi", ""]]}, {"id": "2102.01264", "submitter": "Andrew Ross", "authors": "Andrew Slavin Ross, Nina Chen, Elisa Zhao Hang, Elena L. Glassman,\n  Finale Doshi-Velez", "title": "Evaluating the Interpretability of Generative Models by Interactive\n  Reconstruction", "comments": "CHI 2021 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For machine learning models to be most useful in numerous sociotechnical\nsystems, many have argued that they must be human-interpretable. However,\ndespite increasing interest in interpretability, there remains no firm\nconsensus on how to measure it. This is especially true in representation\nlearning, where interpretability research has focused on \"disentanglement\"\nmeasures only applicable to synthetic datasets and not grounded in human\nfactors. We introduce a task to quantify the human-interpretability of\ngenerative model representations, where users interactively modify\nrepresentations to reconstruct target instances. On synthetic datasets, we find\nperformance on this task much more reliably differentiates entangled and\ndisentangled models than baseline approaches. On a real dataset, we find it\ndifferentiates between representation learning methods widely believed but\nnever shown to produce more or less interpretable models. In both cases, we ran\nsmall-scale think-aloud studies and large-scale experiments on Amazon\nMechanical Turk to confirm that our qualitative and quantitative results\nagreed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 02:38:14 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ross", "Andrew Slavin", ""], ["Chen", "Nina", ""], ["Hang", "Elisa Zhao", ""], ["Glassman", "Elena L.", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "2102.01273", "submitter": "Samuel Reinders", "authors": "Matthew Butler, Leona Holloway, Samuel Reinders, Cagatay Goncu, Kim\n  Marriott", "title": "Technology Developments in Touch-Based Accessible Graphics: A Systematic\n  Review of Research 2010-2020", "comments": "To appear in ACM CHI Conference on Human Factors in Computing Systems\n  (CHI '21), May 8-13, 2021, Yokohama, Japan", "journal-ref": null, "doi": "10.1145/3411764.3445207", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a systematic literature review of 292 publications from\n97 unique venues on touch-based graphics for people who are blind or have low\nvision, from 2010 to mid-2020. It is the first review of its kind on\ntouch-based accessible graphics. It is timely because it allows us to assess\nthe impact of new technologies such as commodity 3D printing and low-cost\nelectronics on the production and presentation of accessible graphics. As\nexpected our review shows an increase in publications from 2014 that we can\nattribute to these developments. It also reveals the need to: broaden\napplication areas, especially to the workplace; broaden end-user participation\nthroughout the full design process; and conduct more in situ evaluation. This\nwork is linked to an online living resource to be shared with the wider\ncommunity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 03:13:49 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Butler", "Matthew", ""], ["Holloway", "Leona", ""], ["Reinders", "Samuel", ""], ["Goncu", "Cagatay", ""], ["Marriott", "Kim", ""]]}, {"id": "2102.01275", "submitter": "Xingjun Li", "authors": "Xingjun Li, Yuanxin Wang, Hong Wang, Yang Wang, and Jian Zhao", "title": "NBSearch: Semantic Search and Visual Exploration of Computational\n  Notebooks", "comments": "14 pages", "journal-ref": null, "doi": "10.1145/3411764.3445048", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code search is an important and frequent activity for developers using\ncomputational notebooks (e.g., Jupyter). The flexibility of notebooks brings\nchallenges for effective code search, where classic search interfaces for\ntraditional software code may be limited. In this paper, we propose, NBSearch,\na novel system that supports semantic code search in notebook collections and\ninteractive visual exploration of search results. NBSearch leverages advanced\nmachine learning models to enable natural language search queries and intuitive\nvisualizations to present complicated intra- and inter-notebook relationships\nin the returned results. We developed NBSearch through an iterative\nparticipatory design process with two experts from a large software company. We\nevaluated the models with a series of experiments and the whole system with a\ncontrolled user study. The results indicate the feasibility of our analytical\npipeline and the effectiveness of NBSearch to support code search in large\nnotebook collections.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 03:15:44 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Li", "Xingjun", ""], ["Wang", "Yuanxin", ""], ["Wang", "Hong", ""], ["Wang", "Yang", ""], ["Zhao", "Jian", ""]]}, {"id": "2102.01330", "submitter": "Aoyu Wu", "authors": "Aoyu Wu, Yun Wang, Xinhuan Shu, Dominik Moritz, Weiwei Cui, Haidong\n  Zhang, Dongmei Zhang, Huamin Qu", "title": "AI4VIS: Survey on Artificial Intelligence Approaches for Data\n  Visualization", "comments": "Accepted to IEEE TVCG. 20 pages, 12 figures and 8 tables. The\n  associated website is https://ai4vis.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visualizations themselves have become a data format. Akin to other data\nformats such as text and images, visualizations are increasingly created,\nstored, shared, and (re-)used with artificial intelligence (AI) techniques. In\nthis survey, we probe the underlying vision of formalizing visualizations as an\nemerging data format and review the recent advance in applying AI techniques to\nvisualization data (AI4VIS). We define visualization data as the digital\nrepresentations of visualizations in computers and focus on data visualization\n(e.g., charts and infographics). We build our survey upon a corpus spanning ten\ndifferent fields in computer science with an eye toward identifying important\ncommon interests. Our resulting taxonomy is organized around WHAT is\nvisualization data and its representation, WHY and HOW to apply AI to\nvisualization data. We highlight a set of common tasks that researchers apply\nto the visualization data and present a detailed discussion of AI approaches\ndeveloped to accomplish those tasks. Drawing upon our literature review, we\ndiscuss several important research questions surrounding the management and\nexploitation of visualization data, as well as the role of AI in support of\nthose processes. We make the list of surveyed papers and related material\navailable online at ai4vis.github.io.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:14:51 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 16:56:18 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wu", "Aoyu", ""], ["Wang", "Yun", ""], ["Shu", "Xinhuan", ""], ["Moritz", "Dominik", ""], ["Cui", "Weiwei", ""], ["Zhang", "Haidong", ""], ["Zhang", "Dongmei", ""], ["Qu", "Huamin", ""]]}, {"id": "2102.01367", "submitter": "Jessica Van Brummelen", "authors": "Jessica Van Brummelen and Viktoriya Tabunshchyk and Tommy Heng", "title": "\"Alexa, Can I Program You?\": Student Perceptions of Conversational\n  Artificial Intelligence Before and After Programming Alexa", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing up in an artificial intelligence-filled world, with Siri and Amazon\nAlexa often within arm's - or speech's - reach, could have significant impact\non children. Conversational agents could influence how students\nanthropomorphize computer systems or develop a theory of mind. Previous\nresearch has explored how conversational agents are used and perceived by\nchildren within and outside of learning contexts. This study investigates how\nmiddle and high school students' perceptions of Alexa change through\nprogramming their own conversational agents in week-long AI education\nworkshops. Specifically, we investigate the workshops' influence on student\nperceptions of Alexa's intelligence, friendliness, aliveness, safeness,\ntrustworthiness, human-likeness, and feelings of closeness. We found that\nstudents felt Alexa was more intelligent and felt closer to Alexa after the\nworkshops. We also found strong correlations between students' perceptions of\nAlexa's friendliness and trustworthiness, and safeness and trustworthiness.\nFinally, we explored how students tended to more frequently use computer\nscience-related diction and ideas after the workshops. Based on our findings,\nwe recommend designers carefully consider personification, transparency,\nplayfulness and utility when designing CAs for learning contexts.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 07:38:01 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Van Brummelen", "Jessica", ""], ["Tabunshchyk", "Viktoriya", ""], ["Heng", "Tommy", ""]]}, {"id": "2102.01405", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Juan Carlos Ruiz-Garcia, Ruben Vera-Rodriguez, Jaime\n  Herreros-Rodriguez, Sergio Romero-Tapiador, Aythami Morales, Julian Fierrez", "title": "Child-Computer Interaction: Recent Works, New Dataset, and Age Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We overview recent research in Child-Computer Interaction and describe our\nframework ChildCI intended for: i) generating a better understanding of the\ncognitive and neuromotor development of children while interacting with mobile\ndevices, and ii) enabling new applications in e-learning and e-health, among\nothers. Our framework includes a new mobile application, specific data\nacquisition protocols, and a first release of the ChildCI dataset (ChildCIdb\nv1), which is planned to be extended yearly to enable longitudinal studies. In\nour framework children interact with a tablet device, using both a pen stylus\nand the finger, performing different tasks that require different levels of\nneuromotor and cognitive skills. ChildCIdb comprises more than 400 children\nfrom 18 months to 8 years old, considering therefore the first three\ndevelopment stages of the Piaget's theory. In addition, and as a demonstration\nof the potential of the ChildCI framework, we include experimental results for\none of the many applications enabled by ChildCIdb: children age detection based\non device interaction. Different machine learning approaches are evaluated,\nproposing a new set of 34 global features to automatically detect age groups,\nachieving accuracy results over 90% and interesting findings in terms of the\ntype of features more useful for this task.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 09:51:58 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Tolosana", "Ruben", ""], ["Ruiz-Garcia", "Juan Carlos", ""], ["Vera-Rodriguez", "Ruben", ""], ["Herreros-Rodriguez", "Jaime", ""], ["Romero-Tapiador", "Sergio", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""]]}, {"id": "2102.01429", "submitter": "Iuliana Marin", "authors": "Iuliana Marin, Myssar Jabbar Hammood Al-Battbootti, Nicolae Goga", "title": "Drone Control based on Mental Commands and Facial Expressions", "comments": null, "journal-ref": "Electronics, Computers and Artificial Intelligence 2020\n  INTERNATIONAL CONFERENCE", "doi": "10.1109/ECAI50035.2020.9223246", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When it is tried to control drones, there are many different ways through\nvarious devices, using either motions like facial motion, special gloves with\nsensors, red, green, blue cameras on the laptop or even using smartwatches by\nperforming gestures that are picked up by motion sensors. The paper proposes a\nwork on how drones could be controlled using brainwaves without any of those\ndevices. The drone control system of the current research was developed using\nelectroencephalogram signals took by an Emotiv Insight headset. The\nelectroencephalogram signals are collected from the users brain. The processed\nsignal is then sent to the computer via Bluetooth. The headset employs\nBluetooth Low Energy for wireless transmission. The brain of the user is\ntrained in order to use the generated electroencephalogram data. The final\nsignal is transmitted to Raspberry Pi zero via the MQTT messaging protocol. The\nRaspberry Pi controls the movement of the drone through the incoming signal\nfrom the headset. After years, brain control can replace many normal input\nsources like keyboards, touch screens or other traditional ways, so it enhances\ninteractive experiences and provides new ways for disabled people to engage\nwith their surroundings.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 10:55:04 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Marin", "Iuliana", ""], ["Al-Battbootti", "Myssar Jabbar Hammood", ""], ["Goga", "Nicolae", ""]]}, {"id": "2102.01468", "submitter": "Yinbo Yu", "authors": "Yinbo Yu and Jiajia Liu", "title": "TAPInspector: Safety and Liveness Verification of Concurrent\n  Trigger-Action IoT Systems", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.NI cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trigger-action programming (TAP) is a popular end-user programming framework\nthat can simplify the Internet of Things (IoT) automation with simple\ntrigger-action rules. However, it also introduces new security and safety\nthreats. A lot of advanced techniques have been proposed to address this\nproblem. Rigorously reasoning about the security of a TAP-based IoT system\nrequires a well-defined model and verification method both against rule\nsemantics and physical-world states, e.g., concurrency, rule latency, and\nconnection-based interactions, which has been missing until now. This paper\npresents TAPInspector, a novel system to detect vulnerabilities in concurrent\nTAP-based IoT systems using model checking. It automatically extracts TAP rules\nfrom IoT apps, translates them into a hybrid model with model slicing and state\ncompression, and performs model checking with various safety and liveness\nproperties. Our experiments corroborate that TAPInspector is effective: it\nidentifies 533 violations with 9 new types of violations from 1108 real-world\nmarket IoT apps and is 60000 times faster than the baseline without\noptimization at least.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 12:39:59 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Yu", "Yinbo", ""], ["Liu", "Jiajia", ""]]}, {"id": "2102.01692", "submitter": "Marvin Coto Mr.", "authors": "Ana Lilia Alvarez-Blanco, Eugenia Cordoba-Warner, Marvin Coto-Jimenez,\n  Vivian Fallas-Lopez, Maribel Morales Rodriguez", "title": "Generacion de voces artificiales infantiles en castellano con acento\n  costarricense", "comments": "12 pages, in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article evaluates a first experience of generating artificial children's\nvoices with a Costa Rican accent, using the technique of statistical parametric\nspeech synthesis based on Hidden Markov Models. The process of recording the\nvoice samples used for learning the models, the fundamentals of the technique\nused and the subjective evaluation of the results through the perception of a\ngroup of people is described. The results show that the intelligibility of the\nresults, evaluated in isolated words, is lower than the voices recorded by the\ngroup of participating children. Similarly, the detection of the age and gender\nof the speaking person is significantly affected in artificial voices, relative\nto recordings of natural voices. These results show the need to obtain larger\namounts of data, in addition to becoming a numerical reference for future\ndevelopments resulting from new data or from processes to improve results in\nthe same technique.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 02:12:28 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Alvarez-Blanco", "Ana Lilia", ""], ["Cordoba-Warner", "Eugenia", ""], ["Coto-Jimenez", "Marvin", ""], ["Fallas-Lopez", "Vivian", ""], ["Rodriguez", "Maribel Morales", ""]]}, {"id": "2102.01770", "submitter": "Brendan David-John", "authors": "Brendan David-John, Diane Hosfelt, Kevin Butler, Eakta Jain", "title": "A privacy-preserving approach to streaming eye-tracking data", "comments": "12 pages, 4 figures, to appear in IEEE TVCG Special Issue on IEEE VR\n  2021", "journal-ref": null, "doi": "10.1109/TVCG.2021.3067787", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye-tracking technology is being increasingly integrated into mixed reality\ndevices. Although critical applications are being enabled, there are\nsignificant possibilities for violating user privacy expectations. We show that\nthere is an appreciable risk of unique user identification even under natural\nviewing conditions in virtual reality. This identification would allow an app\nto connect a user's personal ID with their work ID without needing their\nconsent, for example. To mitigate such risks we propose a framework that\nincorporates gatekeeping via the design of the application programming\ninterface and via software-implemented privacy mechanisms. Our results indicate\nthat these mechanisms can reduce the rate of identification from as much as 85%\nto as low as 30%. The impact of introducing these mechanisms is less than\n1.5$^\\circ$ error in gaze position for gaze prediction. Gaze data streams can\nthus be made private while still allowing for gaze prediction, for example,\nduring foveated rendering. Our approach is the first to support\nprivacy-by-design in the flow of eye-tracking data within mixed reality use\ncases.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 21:43:01 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 20:36:31 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["David-John", "Brendan", ""], ["Hosfelt", "Diane", ""], ["Butler", "Kevin", ""], ["Jain", "Eakta", ""]]}, {"id": "2102.01864", "submitter": "Geza Kovacs", "authors": "Geza Kovacs, Darren Edge", "title": "QuizCram: A Quiz-Driven Lecture Viewing Interface", "comments": "Extended version of \"QuizCram: A Quiz-Driven Lecture Viewing\n  Interface\", which was published as part of the Student Research Competition\n  at CHI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QuizCram is an interface for navigating lecture videos that uses quizzes to\nhelp users determine what they should view. We developed it in response to\nobserving peaks in video seeking behaviors centered around Coursera's in-video\nquizzes. QuizCram shows users a question to answer, with an associated video\nsegment. Users can use these questions to navigate through video segments, and\nfind video segments they need to review. We also allow users to review using a\ntimeline of previously answered questions and videos. To encourage users to\nreview the material, QuizCram keeps track of their question-answering and\nvideo-watching history and schedules sections they likely have not mastered for\nreview. QuizCram-format materials can be generated from existing lectures with\nin-video quizzes. Our user study comparing QuizCram to in-video quizzes found\nthat users practice answering and reviewing questions more when using QuizCram,\nand are better able to remember answers to questions they encountered.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 03:56:36 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Kovacs", "Geza", ""], ["Edge", "Darren", ""]]}, {"id": "2102.01865", "submitter": "Geza Kovacs", "authors": "Geza Kovacs", "title": "Edvertisements: Adding Microlearning to Social News Feeds and Websites", "comments": "Extended version of \"FeedLearn: Using facebook feeds for\n  microlearning\", which was published as an extended abstract at CHI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many long-term goals, such as learning a language, require people to\nregularly practice every day to achieve mastery. At the same time, people\nregularly surf the web and read social news feeds in their spare time. We have\nbuilt a browser extension that teaches vocabulary to users in the context of\nFacebook feeds and arbitrary websites, by showing users interactive quizzes\nthey can answer without leaving the website. On Facebook, the quizzes show up\nas part of the news feed, while on other sites, the quizzes appear where\nadvertisements normally would. In our user study, we examined the effectiveness\nof inserting microlearning tasks into social news feeds. We compared vocabulary\nlearning rates when we inserted interactive quizzes into feeds, versus\ninserting links that lead them to a website where they could do the quizzes.\nOur results suggest that users engage with and learn from our embedded quizzes,\nand engagement increases when the quizzes can be done directly within their\nfeeds.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 03:57:04 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Kovacs", "Geza", ""]]}, {"id": "2102.01974", "submitter": "Alasdair Tran", "authors": "Minjeong Shin, Alasdair Tran, Siqi Wu, Alexander Mathews, Rong Wang,\n  Georgiana Lyall, Lexing Xie", "title": "AttentionFlow: Visualising Influence in Networks of Time Series", "comments": "Published in WSDM 2021. The demo is available at\n  https://attentionflow.ml and code is available at\n  https://github.com/alasdairtran/attentionflow", "journal-ref": "The Proceedings of the Fourteenth ACM International Conference on\n  Web Search and Data Mining (WSDM), 2021", "doi": "10.1145/3437963.3441703", "report-no": null, "categories": "cs.SI cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collective attention on online items such as web pages, search terms, and\nvideos reflects trends that are of social, cultural, and economic interest.\nMoreover, attention trends of different items exhibit mutual influence via\nmechanisms such as hyperlinks or recommendations. Many visualisation tools\nexist for time series, network evolution, or network influence; however, few\nsystems connect all three. In this work, we present AttentionFlow, a new system\nto visualise networks of time series and the dynamic influence they have on one\nanother. Centred around an ego node, our system simultaneously presents the\ntime series on each node using two visual encodings: a tree ring for an\noverview and a line chart for details. AttentionFlow supports interactions such\nas overlaying time series of influence and filtering neighbours by time or\nflux. We demonstrate AttentionFlow using two real-world datasets, VevoMusic and\nWikiTraffic. We show that attention spikes in songs can be explained by\nexternal events such as major awards, or changes in the network such as the\nrelease of a new song. Separate case studies also demonstrate how an artist's\ninfluence changes over their career, and that correlated Wikipedia traffic is\ndriven by cultural interests. More broadly, AttentionFlow can be generalised to\nvisualise networks of time series on physical infrastructures such as road\nnetworks, or natural phenomena such as weather and geological measurements.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 09:44:46 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Shin", "Minjeong", ""], ["Tran", "Alasdair", ""], ["Wu", "Siqi", ""], ["Mathews", "Alexander", ""], ["Wang", "Rong", ""], ["Lyall", "Georgiana", ""], ["Xie", "Lexing", ""]]}, {"id": "2102.02024", "submitter": "Sebastian Cmentowski", "authors": "Sebastian Cmentowski, Andrey Krekhov, Andr\\'e Zenner, Daniel\n  Kucharski, Jens Kr\\\"uger", "title": "Towards Sneaking as a Playful Input Modality for Virtual Environments", "comments": "to appear: accepted IEEE VR 2021 conference paper", "journal-ref": null, "doi": "10.1109/VR50410.2021.00071", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using virtual reality setups, users can fade out of their surroundings and\ndive fully into a thrilling and appealing virtual environment. The success of\nsuch immersive experiences depends heavily on natural and engaging interactions\nwith the virtual world. As developers tend to focus on intuitive hand controls,\nother aspects of the broad range of full-body capabilities are easily left\nvacant. One repeatedly overlooked input modality is the user's gait. Even\nthough users may walk physically to explore the environment, it usually does\nnot matter how they move. However, gait-based interactions, using the variety\nof information contained in human gait, could offer interesting benefits for\nimmersive experiences. For instance, stealth VR-games could profit from this\nadditional range of interaction fidelity in the form of a sneaking-based input\nmodality.\n  In our work, we explore the potential of sneaking as a playful input modality\nfor virtual environments. Therefore, we discuss possible sneaking-based\ngameplay mechanisms and develop three technical approaches, including precise\nfoot-tracking and two abstraction levels. Our evaluation reveals the potential\nof sneaking-based interactions in IVEs, offering unique challenges and\nthrilling gameplay. For these interactions, precise tracking of individual\nfootsteps is unnecessary, as a more abstract approach focusing on the players'\nintention offers the same experience while providing better comprehensible\nfeedback. Based on these findings, we discuss the broader potential and\nindividual strengths of our gait-centered interactions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 11:55:28 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 08:21:44 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Cmentowski", "Sebastian", ""], ["Krekhov", "Andrey", ""], ["Zenner", "Andr\u00e9", ""], ["Kucharski", "Daniel", ""], ["Kr\u00fcger", "Jens", ""]]}, {"id": "2102.02041", "submitter": "Linping Yuan", "authors": "Lin-Ping Yuan, Ziqi Zhou, Jian Zhao, Yiqiu Guo, Fan Du, Huamin Qu", "title": "InfoColorizer: Interactive Recommendation of Color Palettes for\n  Infographics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When designing infographics, general users usually struggle with getting\ndesired color palettes using existing infographic authoring tools, which\nsometimes sacrifice customizability, require design expertise, or neglect the\ninfluence of elements' spatial arrangement. We propose a data-driven method\nthat provides flexibility by considering users' preferences, lowers the\nexpertise barrier via automation, and tailors suggested palettes to the spatial\nlayout of elements. We build a recommendation engine by utilizing deep learning\ntechniques to characterize good color design practices from data, and further\ndevelop InfoColorizer, a tool that allows users to obtain color palettes for\ntheir infographics in an interactive and dynamic manner. To validate our\nmethod, we conducted a comprehensive four-part evaluation, including case\nstudies, a controlled user study, a survey study, and an interview study. The\nresults indicate that InfoColorizer can provide compelling palette\nrecommendations with adequate flexibility, allowing users to effectively obtain\nhigh-quality color design for input infographics with low effort.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 13:02:26 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Yuan", "Lin-Ping", ""], ["Zhou", "Ziqi", ""], ["Zhao", "Jian", ""], ["Guo", "Yiqiu", ""], ["Du", "Fan", ""], ["Qu", "Huamin", ""]]}, {"id": "2102.02094", "submitter": "Philip Doyle", "authors": "Philip R Doyle, Leigh Clark and Benjamin R Cowan", "title": "What Do We See in Them? Identifying Dimensions of Partner Models for\n  Speech Interfaces Using a Psycholexical Approach", "comments": "Pre-print version. 14 pages (inc. bib), 3 figures, 5 tables", "journal-ref": null, "doi": "10.1145/3411764.3445206", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptions of system competence and communicative ability, termed partner\nmodels, play a significant role in speech interface interaction. Yet we do not\nknow what the core dimensions of this concept are. Taking a psycholexical\napproach, our paper is the first to identify the key dimensions that define\npartner models in speech agent interaction. Through a repertory grid study\n(N=21), a review of key subjective questionnaires, an expert review of\nresulting word pairs and an online study of 356 user of speech interfaces, we\nidentify three key dimensions that make up a users' partner model: 1)\nperceptions toward competence and capability; 2) assessment of human-likeness;\nand 3) a system's perceived cognitive flexibility. We discuss the implications\nfor partner modelling as a concept, emphasising the importance of salience and\nthe dynamic nature of these perceptions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 14:57:08 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 09:43:53 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Doyle", "Philip R", ""], ["Clark", "Leigh", ""], ["Cowan", "Benjamin R", ""]]}, {"id": "2102.02132", "submitter": "Alarith Uhde", "authors": "Alarith Uhde and Matthias Laschke and Marc Hassenzahl", "title": "Design and Appropriation of Computer-supported Self-scheduling Practices\n  in Healthcare Shift Work", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": "10.1145/3449219", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shift scheduling impacts healthcare workers' well-being because it sets the\nframe for their social life and recreational activities. Since it is complex\nand time-consuming, it has become a target for automation. However, existing\nsystems mostly focus on improving efficiency. The workers' needs and their\nactive participation do not play a pronounced role. Contrasting this trend, we\ndesigned a social practice-based, worker-centered, and well-being-oriented\nself-scheduling system which gives healthcare workers more control during shift\nplanning. In a following nine month appropriation study, we found that workers\nwho were cautious about their social standing in the group or who had a more\nspontaneous personal lifestyle used our system less often than others.\nMoreover, we revealed several conflict prevention practices and suggest to\nshift the focus away from a competitive shift distribution paradigm towards\nsupporting these pro-social practices. We conclude with guidelines to support\nindividual planning practices, self-leadership, and for dealing with conflicts.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 16:18:56 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 17:19:22 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Uhde", "Alarith", ""], ["Laschke", "Matthias", ""], ["Hassenzahl", "Marc", ""]]}, {"id": "2102.02437", "submitter": "Weina Jin", "authors": "Weina Jin, Jianyu Fan, Diane Gromala, Philippe Pasquier, Ghassan\n  Hamarneh", "title": "EUCA: A Practical Prototyping Framework towards End-User-Centered\n  Explainable Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to explain decisions to its end-users is a necessity to deploy AI\nas critical decision support. Yet making AI explainable to end-users is a\nrelatively ignored and challenging problem. To bridge the gap, we first\nidentified twelve end-user-friendly explanatory forms that do not require\ntechnical knowledge to comprehend, including feature-, example-, and rule-based\nexplanations. We then instantiated the explanatory forms as prototyping cards\nin four AI-assisted critical decision-making tasks, and conducted a user study\nto co-design low-fidelity prototypes with 32 layperson participants. The\nresults verified the relevance of using the explanatory forms as building\nblocks of explanations, and identified their proprieties (pros, cons,\napplicable explainability needs, and design implications). The explanatory\nforms, their proprieties, and prototyping support constitute the\nEnd-User-Centered explainable AI framework EUCA. It serves as a practical\nprototyping toolkit for HCI/AI practitioners and researchers to build\nend-user-centered explainable AI.\n  The EUCA framework is available at http://weina.me/end-user-xai\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 06:39:31 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Jin", "Weina", ""], ["Fan", "Jianyu", ""], ["Gromala", "Diane", ""], ["Pasquier", "Philippe", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "2102.02510", "submitter": "Auriol Degbelo", "authors": "Lasse Einfeldt, Auriol Degbelo", "title": "User Interface Factors of Mobile UX: A Study with an Incident Reporting\n  Application", "comments": "Paper accepted for presentation at the 5th International Conference\n  on Human Computer Interaction Theory and Applications (HUCAPP@VISIGRAPP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smartphones are now ubiquitous, yet our understanding of user interface\nfactors that maximize mobile user experience (UX), is still limited. This work\npresents a controlled experiment, which investigated factors that affect the\nusability and UX of a mobile incident reporting app. The results indicate that\nsequence of user interface elements matters while striving to increase UX, and\nthat there is no difference between tab and scrolling as navigation modalities\nin short forms. These findings can serve as building blocks for\nempirically-derived guidelines for mobile incident reporting.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 09:40:41 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Einfeldt", "Lasse", ""], ["Degbelo", "Auriol", ""]]}, {"id": "2102.02639", "submitter": "Matthew Taylor", "authors": "Matthew E. Taylor, Nicholas Nissen, Yuan Wang, Neda Navidi", "title": "Improving Reinforcement Learning with Human Assistance: An Argument for\n  Human Subject Studies with HIPPO Gym", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Reinforcement learning (RL) is a popular machine learning paradigm for game\nplaying, robotics control, and other sequential decision tasks. However, RL\nagents often have long learning times with high data requirements because they\nbegin by acting randomly. In order to better learn in complex tasks, this\narticle argues that an external teacher can often significantly help the RL\nagent learn.\n  OpenAI Gym is a common framework for RL research, including a large number of\nstandard environments and agents, making RL research significantly more\naccessible. This article introduces our new open-source RL framework, the Human\nInput Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions\nthat went into its creation. The goal of this platform is to facilitate\nhuman-RL research, again lowering the bar so that more researchers can quickly\ninvestigate different ways that human teachers could assist RL agents,\nincluding learning from demonstrations, learning from feedback, or curriculum\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 12:56:02 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Taylor", "Matthew E.", ""], ["Nissen", "Nicholas", ""], ["Wang", "Yuan", ""], ["Navidi", "Neda", ""]]}, {"id": "2102.02654", "submitter": "Maximilian Felde", "authors": "Maximilian Felde and Gerd Stumme", "title": "Triadic Exploration and Exploration with Multiple Experts", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal Concept Analysis (FCA) provides a method called attribute exploration\nwhich helps a domain expert discover structural dependencies in knowledge\ndomains that can be represented by a formal context (a cross table of objects\nand attributes). Triadic Concept Analysis is an extension of FCA that\nincorporates the notion of conditions. Many extensions and variants of\nattribute exploration have been studied but only few attempts at incorporating\nmultiple experts have been made. In this paper we present triadic exploration\nbased on Triadic Concept Analysis to explore conditional attribute implications\nin a triadic domain. We then adapt this approach to formulate attribute\nexploration with multiple experts that have different views on a domain.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 14:49:53 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Felde", "Maximilian", ""], ["Stumme", "Gerd", ""]]}, {"id": "2102.02729", "submitter": "Dongrui Wu", "authors": "Dongrui Wu, Weili Fang, Yi Zhang, Liuqing Yang, Xiaodong Xu, Hanbin\n  Luo and Xiang Yu", "title": "Adversarial Attacks and Defenses in Physiological Computing: A\n  Systematic Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physiological computing uses human physiological data as system inputs in\nreal time. It includes, or significantly overlaps with, brain-computer\ninterfaces, affective computing, adaptive automation, health informatics, and\nphysiological signal based biometrics. Physiological computing increases the\ncommunication bandwidth from the user to the computer, but is also subject to\nvarious types of adversarial attacks, in which the attacker deliberately\nmanipulates the training and/or test examples to hijack the machine learning\nalgorithm output, leading to possibly user confusion, frustration, injury, or\neven death. However, the vulnerability of physiological computing systems has\nnot been paid enough attention to, and there does not exist a comprehensive\nreview on adversarial attacks to it. This paper fills this gap, by providing a\nsystematic review on the main research areas of physiological computing,\ndifferent types of adversarial attacks and their applications to physiological\ncomputing, and the corresponding defense strategies. We hope this review will\nattract more research interests on the vulnerability of physiological computing\nsystems, and more importantly, defense strategies to make them more secure.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 16:40:12 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 22:24:25 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 17:15:30 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Wu", "Dongrui", ""], ["Fang", "Weili", ""], ["Zhang", "Yi", ""], ["Yang", "Liuqing", ""], ["Xu", "Xiaodong", ""], ["Luo", "Hanbin", ""], ["Yu", "Xiang", ""]]}, {"id": "2102.02783", "submitter": "Lia Morra", "authors": "F. Gabriele Prattic\\`o, Fabrizio Lamberti, Alberto Cannav\\`o, Lia\n  Morra, Paolo Montuschi", "title": "Comparing State-of-the-Art and Emerging Augmented Reality Interfaces for\n  Autonomous Vehicle-to-Pedestrian Communication", "comments": "Accepted for publication in IEEE Transactions on Vehicular Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing pedestrians and other vulnerable road users with a clear indication\nabout a fully autonomous vehicle status and intentions is crucial to make them\ncoexist. In the last few years, a variety of external interfaces have been\nproposed, leveraging different paradigms and technologies including\nvehicle-mounted devices (like LED panels), short-range on-road projections, and\nroad infrastructure interfaces (e.g., special asphalts with embedded displays).\nThese designs were experimented in different settings, using mockups, specially\nprepared vehicles, or virtual environments, with heterogeneous evaluation\nmetrics. Promising interfaces based on Augmented Reality (AR) have been\nproposed too, but their usability and effectiveness have not been tested yet.\nThis paper aims to complement such body of literature by presenting a\ncomparison of state-of-the-art interfaces and new designs under common\nconditions. To this aim, an immersive Virtual Reality-based simulation was\ndeveloped, recreating a well-known scenario represented by pedestrians crossing\nin urban environments under non-regulated conditions. A user study was then\nperformed to investigate the various dimensions of vehicle-to-pedestrian\ninteraction leveraging objective and subjective metrics. Even though no\ninterface clearly stood out over all the considered dimensions, one of the AR\ndesigns achieved state-of-the-art results in terms of safety and trust, at the\ncost of higher cognitive effort and lower intuitiveness compared to LED panels\nshowing anthropomorphic features. Together with rankings on the various\ndimensions, indications about advantages and drawbacks of the various\nalternatives that emerged from this study could provide important information\nfor next developments in the field.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 18:03:06 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Prattic\u00f2", "F. Gabriele", ""], ["Lamberti", "Fabrizio", ""], ["Cannav\u00f2", "Alberto", ""], ["Morra", "Lia", ""], ["Montuschi", "Paolo", ""]]}, {"id": "2102.02853", "submitter": "Fayika Farhat Nova", "authors": "Fayika Farhat Nova, Michael Ann Devito, Pratyasha Saha, Kazi Shohanur\n  Rashid, Shashwata Roy Turzo, Sadia Afrin, Shion Guha", "title": "\"Facebook Promotes More Harassment\": Social Media Ecosystem, Skill and\n  Marginalized Hijra Identity in Bangladesh", "comments": "35 pages, 9 figures, CSCW 2021", "journal-ref": null, "doi": "10.1145/3449231", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social interaction across multiple online platforms is a challenge for gender\nand sexual minorities (GSM) due to the stigmatization they face, which\nincreases the complexity of their self-presentation decisions. These online\ninteractions and identity disclosures can be more complicated for GSM in\nnon-Western contexts due to consequentially different audiences and perceived\naffordances by the users, and limited baseline understanding of the conflation\nof these two with local norms and the opportunities they practically represent.\nUsing focus group discussions and semi-structured interviews, we engaged with\n61 \\textit{Hijra} individuals from Bangladesh, a severely stigmatized GSM from\nsouth Asia, to understand their overall online participation and disclosure\nbehaviors through the lens of personal social media ecosystems. We find that\nalong with platform audiences, affordances, and norms, participant\nskill/knowledge, and cultural influences also impact navigation through\nmultiple platforms, resulting in differential benefits from privacy features.\nThis impacts how Hijra perceive online spaces, and shape their\nself-presentation and disclosure behaviors over time.\n  Content Warning: This paper discusses graphic contents (e.g. rape and sexual\nharassment) related to Hijra.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 19:39:34 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Nova", "Fayika Farhat", ""], ["Devito", "Michael Ann", ""], ["Saha", "Pratyasha", ""], ["Rashid", "Kazi Shohanur", ""], ["Turzo", "Shashwata Roy", ""], ["Afrin", "Sadia", ""], ["Guha", "Shion", ""]]}, {"id": "2102.03179", "submitter": "Matti Pouke", "authors": "Matti Pouke, Katherine J. Mimnaugh, Alexis Chambers, Timo Ojala,\n  Steven M. LaValle", "title": "The Plausibility Paradox for Resized Users in Virtual Environments", "comments": "Preprint. arXiv admin note: substantial text overlap with\n  arXiv:1912.01947", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper identifies and confirms a perceptual phenomenon: when users\ninteract with simulated objects in a virtual environment where the users' scale\ndeviates greatly from normal, there is a mismatch between the object physics\nthey consider realistic and the object physics that would be correct at that\nscale. We report the findings of two studies investigating the relationship\nbetween perceived realism and a physically accurate approximation of reality in\na virtual reality experience in which the user has been scaled by a factor of\nten. Study 1 investigated perception of physics when scaled-down by a factor of\nten, whereas Study 2 focused on enlargement by a similar amount. Studies were\ncarried out as within-subjects experiments in which a total of 84 subjects\nperformed simple interaction tasks with objects under two different physics\nsimulation conditions. In the true physics condition, the objects, when dropped\nand thrown, behaved accurately according to the physics that would be correct\nat that either reduced or enlarged scale in the real world. In the movie\nphysics condition, the objects behaved in a similar manner as they would if no\nscaling of the user had occurred. We found that a significant majority of the\nusers considered the movie physics condition to be the more realistic one.\nHowever, at enlarged scale, many users considered true physics to match their\nexpectations even if they ultimately believed movie physics to be the realistic\ncondition. We argue that our findings have implications for many virtual\nreality and telepresence applications involving operation with simulated or\nphysical objects in abnormal and especially small scales.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 13:55:31 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Pouke", "Matti", ""], ["Mimnaugh", "Katherine J.", ""], ["Chambers", "Alexis", ""], ["Ojala", "Timo", ""], ["LaValle", "Steven M.", ""]]}, {"id": "2102.03241", "submitter": "Garrick Cabour", "authors": "Garrick Cabour, \\'Elise Ledoux, Samuel Bassetto", "title": "Extending System Performance Past the Boundaries of Technical Maturity:\n  Human-Agent Teamwork Perspective for Industrial Inspection", "comments": "Inclusion in International Ergonomics Association 21th Triennial\n  Congress, proceedings in Advances in Intelligent Systems and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cyber-Physical-Social Systems (CPSS) performance for industry 4.0 is highly\ncontext-dependent, where three design areas arise: the artifact itself, the\nhuman-agent collaboration, and the organizational settings. Current HF&E tools\nare limited to conceptualize and anticipate future human-agent work situations\nwith a fine-grained perspective. This paper explores how rich in-sights from\nwork analysis can be translated into formative design patterns that provide\nfiner guidance in conceptualizing the human-agent collaboration and the\norganizational settings. The current manual work content elicited is\ndisaggregated into functional requirements. Each function is then scrutinized\nby a multidisciplinary design team that decides its feasibility and nature\n(autonomy function, human function, or hybrid function). By doing so, we\nuncover the technical capabilities of the CPSS in comparison with\nsubject-matter experts` work activity. We called this concept technological\ncoverage. The framework thereof allows close collaboration with design\nstakeholders to define detailed HAT configurations. We then imagined joint\nactivity scenarios based on end-users work activity, the technological\ncapabilities, and the interaction requirements to perform the work. We use a\nstudy on technological innovation in the aircraft maintenance domain to\nillustrate the framework`s early phases\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:38:57 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Cabour", "Garrick", ""], ["Ledoux", "\u00c9lise", ""], ["Bassetto", "Samuel", ""]]}, {"id": "2102.03382", "submitter": "Tu Le", "authors": "Tu Le, Danny Yuxing Huang, Noah Apthorpe, Yuan Tian", "title": "SkillBot: Identifying Risky Content for Children in Alexa Skills", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CL cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many households include children who use voice personal assistants (VPA) such\nas Amazon Alexa. Children benefit from the rich functionalities of VPAs and\nthird-party apps but are also exposed to new risks in the VPA ecosystem (e.g.,\ninappropriate content or information collection). To study the risks VPAs pose\nto children, we build a Natural Language Processing (NLP)-based system to\nautomatically interact with VPA apps and analyze the resulting conversations to\nidentify contents risky to children. We identify 28 child-directed apps with\nrisky contents and maintain a growing dataset of 31,966 non-overlapping app\nbehaviors collected from 3,434 Alexa apps. Our findings suggest that although\nvoice apps designed for children are subject to more policy requirements and\nintensive vetting, children are still vulnerable to risky content. We then\nconduct a user study showing that parents are more concerned about VPA apps\nwith inappropriate content than those that ask for personal information, but\nmany parents are not aware that risky apps of either type exist. Finally, we\nidentify a new threat to users of VPA apps: confounding utterances, or voice\ncommands shared by multiple apps that may cause a user to invoke or interact\nwith a different app than intended. We identify 4,487 confounding utterances,\nincluding 581 shared by child-directed and non-child-directed apps.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 19:07:39 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Le", "Tu", ""], ["Huang", "Danny Yuxing", ""], ["Apthorpe", "Noah", ""], ["Tian", "Yuan", ""]]}, {"id": "2102.03629", "submitter": "Saleh Kalantari", "authors": "Jesus G. Cruz-Garza, Michael Darfler, James D. Rounds, Elita Gao,\n  Saleh Kalantari", "title": "EEG-based Investigation of the Impact of Classroom Design on Cognitive\n  Performance of Students", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study investigated the neural dynamics associated with short-term\nexposure to different virtual classroom designs with different window placement\nand room dimension. Participants engaged in five brief cognitive tasks in each\ndesign condition including the Stroop Test, the Digit Span Test, the Benton\nTest, a Visual Memory Test, and an Arithmetic Test. Performance on the\ncognitive tests and Electroencephalogram (EEG) data were analyzed by\ncontrasting various classroom design conditions. The cognitive-test-performance\nresults showed no significant differences related to the architectural design\nfeatures studied. We computed frequency band-power and connectivity EEG\nfeatures to identify neural patterns associated to environmental conditions. A\nleave one out machine learning classification scheme was implemented to assess\nthe robustness of the EEG features, with the classification accuracy evaluation\nof the trained model repeatedly performed against an unseen participant's data.\nThe classification results located consistent differences in the EEG features\nacross participants in the different classroom design conditions, with a\npredictive power that was significantly higher compared to a baseline\nclassification learning outcome using scrambled data. These findings were most\nrobust during the Visual Memory Test, and were not found during the Stroop Test\nand the Arithmetic Test. The most discriminative EEG features were observed in\nbilateral occipital, parietal, and frontal regions in the theta and alpha\nfrequency bands. While the implications of these findings for student learning\nare yet to be determined, this study provides rigorous evidence that brain\nactivity features during cognitive tasks are affected by the design elements of\nwindow placement and room dimensions.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 18:06:18 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Cruz-Garza", "Jesus G.", ""], ["Darfler", "Michael", ""], ["Rounds", "James D.", ""], ["Gao", "Elita", ""], ["Kalantari", "Saleh", ""]]}, {"id": "2102.03673", "submitter": "Leena Mathur", "authors": "Leena Mathur and Maja J Matari\\'c", "title": "Unsupervised Audio-Visual Subspace Alignment for High-Stakes Deception\n  Detection", "comments": "Accepted at ICASSP 2021 \\c{opyright} 2021 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, in current or future media, including reprinting/republishing this\n  material for advertising or promotional purposes, creating new collective\n  works, for resale or redistribution to servers or lists, or reuse of\n  copyrighted components of this work", "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9413550", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated systems that detect deception in high-stakes situations can enhance\nsocietal well-being across medical, social work, and legal domains. Existing\nmodels for detecting high-stakes deception in videos have been supervised, but\nlabeled datasets to train models can rarely be collected for most real-world\napplications. To address this problem, we propose the first multimodal\nunsupervised transfer learning approach that detects real-world, high-stakes\ndeception in videos without using high-stakes labels. Our subspace-alignment\n(SA) approach adapts audio-visual representations of deception in\nlab-controlled low-stakes scenarios to detect deception in real-world,\nhigh-stakes situations. Our best unsupervised SA models outperform models\nwithout SA, outperform human ability, and perform comparably to a number of\nexisting supervised models. Our research demonstrates the potential for\nintroducing subspace-based transfer learning to model high-stakes deception and\nother social behaviors in real-world contexts with a scarcity of labeled\nbehavioral data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 21:53:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mathur", "Leena", ""], ["Matari\u0107", "Maja J", ""]]}, {"id": "2102.03684", "submitter": "Felix Putze", "authors": "Tanja Schultz, Felix Putze, Thorsten Fehr, Moritz Meier, Celeste\n  Mason, Florian Ahrens, Manfred Herrmann", "title": "Linking Labs: Interconnecting Experimental Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the concept of LabLinking: a technology-based interconnection of\nexperimental laboratories across institutions, disciplines, cultures,\nlanguages, and time zones - in other words experiments without borders. In\nparticular, we introduce LabLinking levels (LLL), which define the degree of\ntightness of empirical interconnection between labs. We describe the\ntechnological infrastructure in terms of hard- and software required for the\nrespective LLLs and present examples of linked laboratories along with insights\nabout the challenges and benefits. In sum, we argue that linked labs provide a\nunique platform for a continuous exchange between scientists and experimenters,\nthereby enabling a time synchronous execution of experiments performed with and\nby decentralized user and researchers, improving outreach and ease of subject\nrecruitment, allowing to establish new experimental designs and to incorporate\na panoply of complementary biosensors, devices, hard- and software solutions.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 23:27:02 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Schultz", "Tanja", ""], ["Putze", "Felix", ""], ["Fehr", "Thorsten", ""], ["Meier", "Moritz", ""], ["Mason", "Celeste", ""], ["Ahrens", "Florian", ""], ["Herrmann", "Manfred", ""]]}, {"id": "2102.03702", "submitter": "Jialun Aaron Jiang", "authors": "Jialun Aaron Jiang, Kandrea Wade, Casey Fiesler, Jed R. Brubaker", "title": "Supporting Serendipity: Opportunities and Challenges for Human-AI\n  Collaboration in Qualitative Analysis", "comments": "23 pages. Accepted to ACM CSCW 2021", "journal-ref": null, "doi": "10.1145/3449168", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Qualitative inductive methods are widely used in CSCW and HCI research for\ntheir ability to generatively discover deep and contextualized insights, but\nthese inherently manual and human-resource-intensive processes are often\ninfeasible for analyzing large corpora. Researchers have been increasingly\ninterested in ways to apply qualitative methods to \"big\" data problems, hoping\nto achieve more generalizable results from larger amounts of data while\npreserving the depth and richness of qualitative methods. In this paper, we\ndescribe a study of qualitative researchers' work practices and their\nchallenges, with an eye towards whether this is an appropriate domain for\nhuman-AI collaboration and what successful collaborations might entail. Our\nfindings characterize participants' diverse methodological practices and\nnuanced collaboration dynamics, and identify areas where they might benefit\nfrom AI-based tools. While participants highlight the messiness and uncertainty\nof qualitative inductive analysis, they still want full agency over the process\nand believe that AI should not interfere. Our study provides a deep\ninvestigation of task delegability in human-AI collaboration in the context of\nqualitative analysis, and offers directions for the design of AI assistance\nthat honor serendipity, human agency, and ambiguity.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 02:37:05 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Jiang", "Jialun Aaron", ""], ["Wade", "Kandrea", ""], ["Fiesler", "Casey", ""], ["Brubaker", "Jed R.", ""]]}, {"id": "2102.03742", "submitter": "Geza Kovacs", "authors": "Geza Kovacs", "title": "Reconstructing Detailed Browsing Activities from Browser History", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users' detailed browsing activity - such as what sites they are spending time\non and for how long, and what tabs they have open and which one is focused at\nany given time - is useful for a number of research and practical applications.\nGathering such data, however, requires that users install and use a monitoring\ntool over long periods of time. In contrast, browser extensions can gain\ninstantaneous access months of browser history data. However, the browser\nhistory is incomplete: it records only navigation events, missing important\ninformation such as time spent or tab focused. In this work, we aim to\nreconstruct time spent on sites with only users' browsing histories. We\ngathered three months of browsing history and two weeks of ground-truth\ndetailed browsing activity from 185 participants. We developed a machine\nlearning algorithm that predicts whether the browser window is focused and\nactive at one second-level granularity with an F1-score of 0.84. During periods\nwhen the browser is active, the algorithm can predict which the domain the user\nwas looking at with 76.2% accuracy. We can use these results to reconstruct the\ntotal time spent online for each user with an R^2 value of 0.96, and the total\ntime each user spent on each domain with an R^2 value of 0.92.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 08:29:29 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Kovacs", "Geza", ""]]}, {"id": "2102.03777", "submitter": "Zhen Liang Jane", "authors": "Zhen Liang, Rushuang Zhou, Li Zhang, Linling Li, Gan Huang, Zhiguo\n  Zhang and Shin Ishii", "title": "EEGFuseNet: Hybrid Unsupervised Deep Feature Characterization and Fusion\n  for High-Dimensional EEG with An Application to Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to effectively and efficiently extract valid and reliable features from\nhigh-dimensional electroencephalography (EEG), particularly how to fuse the\nspatial and temporal dynamic brain information into a better feature\nrepresentation, is a critical issue in brain data analysis. Most current EEG\nstudies are working on handcrafted features with a supervised modeling, which\nwould be limited by experience and human feedbacks to a great extent. In this\npaper, we propose a practical hybrid unsupervised deep CNN-RNN-GAN based EEG\nfeature characterization and fusion model, which is termed as EEGFuseNet.\nEEGFuseNet is trained in an unsupervised manner, and deep EEG features covering\nspatial and temporal dynamics are automatically characterized. Comparing to the\nhandcrafted features, the deep EEG features could be considered to be more\ngeneric and independent of any specific EEG task. The performance of the\nextracted deep and low-dimensional features by EEGFuseNet is carefully\nevaluated in an unsupervised emotion recognition application based on a famous\npublic emotion database. The results demonstrate the proposed EEGFuseNet is a\nrobust and reliable model, which is easy to train and manage and perform\nefficiently in the representation and fusion of dynamic EEG features. In\nparticular, EEGFuseNet is established as an optimal unsupervised fusion model\nwith promising subject-based leave-one-out results in the recognition of four\nemotion dimensions (valence, arousal, dominance and liking), which demonstrates\nthe possibility of realizing EEG based cross-subject emotion recognition in a\npure unsupervised manner.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 11:09:16 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Liang", "Zhen", ""], ["Zhou", "Rushuang", ""], ["Zhang", "Li", ""], ["Li", "Linling", ""], ["Huang", "Gan", ""], ["Zhang", "Zhiguo", ""], ["Ishii", "Shin", ""]]}, {"id": "2102.03796", "submitter": "Akinari Onishi", "authors": "Akinari Onishi", "title": "Brain-computer interface with rapid serial multimodal presentation using\n  artificial facial images and voice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Electroencephalography (EEG) signals elicited by multimodal stimuli can drive\nbrain-computer interfaces (BCIs), and research has demonstrated that visual and\nauditory stimuli can be employed simultaneously to improve BCI performance.\nHowever, no studies have investigated the effect of multimodal stimuli in rapid\nserial visual presentation (RSVP) BCIs. In the present study, we propose a\nrapid serial multimodal presentation (RSMP) BCI that incorporates artificial\nfacial images and artificial voice stimuli. To clarify the effect of\naudiovisual stimuli on the RSMP BCI, scrambled images and masked sounds were\napplied instead of visual and auditory stimuli, respectively. Our findings\nindicated that the audiovisual stimuli improved the performance of the RSMP\nBCI, and that the P300 at Pz contributed to classification accuracy. Online\naccuracy of BCI reached 85.7+-11.5%. Taken together, these findings may aid in\nthe development of better gaze-independent BCI systems.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 13:31:36 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 03:53:27 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Onishi", "Akinari", ""]]}, {"id": "2102.03955", "submitter": "Eduardo Velloso", "authors": "Eduardo Velloso, Carlos Hitoshi Morimoto", "title": "A Probabilistic Interpretation of Motion Correlation Selection\n  Techniques", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445184", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Motion correlation interfaces are those that present targets moving in\ndifferent patterns, which the user can select by matching their motion. In this\npaper, we re-formulate the task of target selection as a probabilistic\ninference problem. We demonstrate that previous interaction techniques can be\nmodelled using a Bayesian approach and that how modelling the selection task as\ntransmission of information can help us make explicit the assumptions behind\nsimilarity measures. We propose ways of incorporating uncertainty into the\ndecision-making process and demonstrate how the concept of entropy can\nilluminate the measurement of the quality of a design. We apply these\ntechniques in a case study and suggest guidelines for future work.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 00:35:59 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Velloso", "Eduardo", ""], ["Morimoto", "Carlos Hitoshi", ""]]}, {"id": "2102.03985", "submitter": "Erik Blasch", "authors": "Erik Blasch, James Sung, Tao Nguyen", "title": "Multisource AI Scorecard Table for System Evaluation", "comments": "Presented at AAAI FSS-20: Artificial Intelligence in Government and\n  Public Sector, Washington, DC, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The paper describes a Multisource AI Scorecard Table (MAST) that provides the\ndeveloper and user of an artificial intelligence (AI)/machine learning (ML)\nsystem with a standard checklist focused on the principles of good analysis\nadopted by the intelligence community (IC) to help promote the development of\nmore understandable systems and engender trust in AI outputs. Such a scorecard\nenables a transparent, consistent, and meaningful understanding of AI tools\napplied for commercial and government use. A standard is built on compliance\nand agreement through policy, which requires buy-in from the stakeholders.\nWhile consistency for testing might only exist across a standard data set, the\ncommunity requires discussion on verification and validation approaches which\ncan lead to interpretability, explainability, and proper use. The paper\nexplores how the analytic tradecraft standards outlined in Intelligence\nCommunity Directive (ICD) 203 can provide a framework for assessing the\nperformance of an AI system supporting various operational needs. These include\nsourcing, uncertainty, consistency, accuracy, and visualization. Three use\ncases are presented as notional examples that support security for comparative\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 03:37:40 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Blasch", "Erik", ""], ["Sung", "James", ""], ["Nguyen", "Tao", ""]]}, {"id": "2102.03994", "submitter": "Md Zakir Hossain", "authors": "Ruiqi Chen, Atiqul Islam, Tom Gedeon and Md Zakir Hossain", "title": "Observers Pupillary Responses in Recognising Real and Posed Smiles: A\n  Preliminary Study", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pupillary responses (PR) change differently for different types of stimuli.\nThis study aims to check whether observers PR can recognise real and posed\nsmiles from a set of smile images and videos. We showed the smile images and\nsmile videos stimuli to observers, and recorded their pupillary responses\nconsidering four different situations, namely paired videos, paired images,\nsingle videos, and single images. When the same smiler was viewed by observers\nin both real and posed smile forms, we refer them as paired; otherwise we use\nthe term single. The primary analysis on pupil data revealed that the\ndifferences of pupillary response between real and posed smiles are more\nsignificant in case of paired videos compared to others. This result is found\nfrom timeline analysis, KS-test, and ANOVA test. Overall, our model can\nrecognise real and posed smiles from observers pupillary responses instead of\nsmilers responses. Our research will be applicable in affective computing and\ncomputer-human interaction for measuring emotional authenticity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 04:07:04 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chen", "Ruiqi", ""], ["Islam", "Atiqul", ""], ["Gedeon", "Tom", ""], ["Hossain", "Md Zakir", ""]]}, {"id": "2102.04051", "submitter": "Yota Ueda", "authors": "Yota Ueda, Kazuki Fujii, Yuki Saito, Shinnosuke Takamichi, Yukino\n  Baba, Hiroshi Saruwatari", "title": "HumanACGAN: conditional generative adversarial network with human-based\n  auxiliary classifier and its evaluation in phoneme perception", "comments": "5 pages, 6 figures, to be published in 2021 IEEE International\n  Conference on Acoustics, Speech and Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a conditional generative adversarial network (GAN) incorporating\nhumans' perceptual evaluations. A deep neural network (DNN)-based generator of\na GAN can represent a real-data distribution accurately but can never represent\na human-acceptable distribution, which are ranges of data in which humans\naccept the naturalness regardless of whether the data are real or not. A\nHumanGAN was proposed to model the human-acceptable distribution. A DNN-based\ngenerator is trained using a human-based discriminator, i.e., humans'\nperceptual evaluations, instead of the GAN's DNN-based discriminator. However,\nthe HumanGAN cannot represent conditional distributions. This paper proposes\nthe HumanACGAN, a theoretical extension of the HumanGAN, to deal with\nconditional human-acceptable distributions. Our HumanACGAN trains a DNN-based\nconditional generator by regarding humans as not only a discriminator but also\nan auxiliary classifier. The generator is trained by deceiving the human-based\ndiscriminator that scores the unconditioned naturalness and the human-based\nclassifier that scores the class-conditioned perceptual acceptability. The\ntraining can be executed using the backpropagation algorithm involving humans'\nperceptual evaluations. Our experimental results in phoneme perception\ndemonstrate that our HumanACGAN can successfully train this conditional\ngenerator.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 08:25:29 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ueda", "Yota", ""], ["Fujii", "Kazuki", ""], ["Saito", "Yuki", ""], ["Takamichi", "Shinnosuke", ""], ["Baba", "Yukino", ""], ["Saruwatari", "Hiroshi", ""]]}, {"id": "2102.04163", "submitter": "Ivan Sekulic", "authors": "Ivan Sekuli\\'c and Mohammad Aliannejadi and Fabio Crestani", "title": "User Engagement Prediction for Clarification in Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clarification is increasingly becoming a vital factor in various topics of\ninformation retrieval, such as conversational search and modern Web search\nengines. Prompting the user for clarification in a search session can be very\nbeneficial to the system as the user's explicit feedback helps the system\nimprove retrieval massively. However, it comes with a very high risk of\nfrustrating the user in case the system fails in asking decent clarifying\nquestions. Therefore, it is of great importance to determine when and how to\nask for clarification. To this aim, in this work, we model search clarification\nprediction as user engagement problem. We assume that the better a\nclarification is, the higher user engagement with it would be. We propose a\nTransformer-based model to tackle the task. The comparison with competitive\nbaselines on large-scale real-life clarification engagement data proves the\neffectiveness of our model. Also, we analyse the effect of all result page\nelements on the performance and find that, among others, the ranked list of the\nsearch engine leads to considerable improvements. Our extensive analysis of\ntask-specific features guides future research.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 12:28:02 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Sekuli\u0107", "Ivan", ""], ["Aliannejadi", "Mohammad", ""], ["Crestani", "Fabio", ""]]}, {"id": "2102.04174", "submitter": "Carlos de la Torre-Ortiz", "authors": "Aur\\'elien Nioche, Pierre-Alexandre Murena, Carlos de la Torre-Ortiz,\n  Antti Oulasvirta", "title": "Improving Artificial Teachers by Considering How People Learn and Forget", "comments": "15 pages, 5 figures, to be published in IUI'21", "journal-ref": null, "doi": "10.1145/3397481.3450696", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The paper presents a novel model-based method for intelligent tutoring, with\nparticular emphasis on the problem of selecting teaching interventions in\ninteraction with humans. Whereas previous work has focused on either\npersonalization of teaching or optimization of teaching intervention sequences,\nthe proposed individualized model-based planning approach represents\nconvergence of these two lines of research. Model-based planning picks the best\ninterventions via interactive learning of a user memory model's parameters. The\napproach is novel in its use of a cognitive model that can account for several\nkey individual- and material-specific characteristics related to\nrecall/forgetting, along with a planning technique that considers users'\npractice schedules. Taking a rule-based approach as a baseline, the authors\nevaluated the method's benefits in a controlled study of artificial teaching in\nsecond-language vocabulary learning (N=53).\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 13:05:58 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 15:48:53 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 12:51:42 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Nioche", "Aur\u00e9lien", ""], ["Murena", "Pierre-Alexandre", ""], ["de la Torre-Ortiz", "Carlos", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "2102.04209", "submitter": "Michael Stuart", "authors": "Michael T. Stuart and Markus Kneer", "title": "Guilty Artificial Minds", "comments": "20 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The concepts of blameworthiness and wrongness are of fundamental importance\nin human moral life. But to what extent are humans disposed to blame\nartificially intelligent agents, and to what extent will they judge their\nactions to be morally wrong? To make progress on these questions, we adopted\ntwo novel strategies. First, we break down attributions of blame and wrongness\ninto more basic judgments about the epistemic and conative state of the agent,\nand the consequences of the agent's actions. In this way, we are able to\nexamine any differences between the way participants treat artificial agents in\nterms of differences in these more basic judgments. our second strategy is to\ncompare attributions of blame and wrongness across human, artificial, and group\nagents (corporations). Others have compared attributions of blame and wrongness\nbetween human and artificial agents, but the addition of group agents is\nsignificant because these agents seem to provide a clear middle-ground between\nhuman agents (for whom the notions of blame and wrongness were created) and\nartificial agents (for whom the question remains open).\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 21:37:35 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Stuart", "Michael T.", ""], ["Kneer", "Markus", ""]]}, {"id": "2102.04256", "submitter": "Jack Bandy", "authors": "Jack Bandy", "title": "Problematic Machine Behavior: A Systematic Literature Review of\n  Algorithm Audits", "comments": "To Appear in the Proceedings of the ACM (PACM) Human-Computer\n  Interaction, CSCW '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While algorithm audits are growing rapidly in commonality and public\nimportance, relatively little scholarly work has gone toward synthesizing prior\nwork and strategizing future research in the area. This systematic literature\nreview aims to do just that, following PRISMA guidelines in a review of over\n500 English articles that yielded 62 algorithm audit studies. The studies are\nsynthesized and organized primarily by behavior (discrimination, distortion,\nexploitation, and misjudgement), with codes also provided for domain (e.g.\nsearch, vision, advertising, etc.), organization (e.g. Google, Facebook,\nAmazon, etc.), and audit method (e.g. sock puppet, direct scrape,\ncrowdsourcing, etc.). The review shows how previous audit studies have exposed\npublic-facing algorithms exhibiting problematic behavior, such as search\nalgorithms culpable of distortion and advertising algorithms culpable of\ndiscrimination. Based on the studies reviewed, it also suggests some behaviors\n(e.g. discrimination on the basis of intersectional identities), domains (e.g.\nadvertising algorithms), methods (e.g. code auditing), and organizations (e.g.\nTwitter, TikTok, LinkedIn) that call for future audit attention. The paper\nconcludes by offering the common ingredients of successful audits, and\ndiscussing algorithm auditing in the context of broader research working toward\nalgorithmic justice.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 19:21:11 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Bandy", "Jack", ""]]}, {"id": "2102.04427", "submitter": "Austin Wright", "authors": "Austin P Wright, Omar Shaikh, Haekyu Park, Will Epperson, Muhammed\n  Ahmed, Stephane Pinel, Duen Horng Chau, Diyi Yang", "title": "RECAST: Enabling User Recourse and Interpretability of Toxicity\n  Detection Models with Interactive Visualization", "comments": "26 pages, 5 figures, CSCW '21", "journal-ref": null, "doi": "10.1145/3449280", "report-no": null, "categories": "cs.HC cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of toxic language online, platforms are increasingly\nusing automated systems that leverage advances in natural language processing\nto automatically flag and remove toxic comments. However, most automated\nsystems -- when detecting and moderating toxic language -- do not provide\nfeedback to their users, let alone provide an avenue of recourse for these\nusers to make actionable changes. We present our work, RECAST, an interactive,\nopen-sourced web tool for visualizing these models' toxic predictions, while\nproviding alternative suggestions for flagged toxic language. Our work also\nprovides users with a new path of recourse when using these automated\nmoderation tools. RECAST highlights text responsible for classifying toxicity,\nand allows users to interactively substitute potentially toxic phrases with\nneutral alternatives. We examined the effect of RECAST via two large-scale user\nevaluations, and found that RECAST was highly effective at helping users reduce\ntoxicity as detected through the model. Users also gained a stronger\nunderstanding of the underlying toxicity criterion used by black-box models,\nenabling transparency and recourse. In addition, we found that when users focus\non optimizing language for these models instead of their own judgement (which\nis the implied incentive and goal of deploying automated models), these models\ncease to be effective classifiers of toxicity compared to human annotations.\nThis opens a discussion for how toxicity detection models work and should work,\nand their effect on the future of online discourse.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:37:50 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 14:42:17 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Wright", "Austin P", ""], ["Shaikh", "Omar", ""], ["Park", "Haekyu", ""], ["Epperson", "Will", ""], ["Ahmed", "Muhammed", ""], ["Pinel", "Stephane", ""], ["Chau", "Duen Horng", ""], ["Yang", "Diyi", ""]]}, {"id": "2102.04527", "submitter": "Michael Stuart", "authors": "Markus Kneer and Michael T. Stuart", "title": "Playing the Blame Game with Robots", "comments": "5 pages, 2 figures, 2 tables, HRI'21", "journal-ref": null, "doi": "10.1145/3434074.3447202", "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent research shows -- somewhat astonishingly -- that people are willing to\nascribe moral blame to AI-driven systems when they cause harm [1]-[4]. In this\npaper, we explore the moral-psychological underpinnings of these findings. Our\nhypothesis was that the reason why people ascribe moral blame to AI systems is\nthat they consider them capable of entertaining inculpating mental states (what\nis called mens rea in the law). To explore this hypothesis, we created a\nscenario in which an AI system runs a risk of poisoning people by using a novel\ntype of fertilizer. Manipulating the computational (or quasi-cognitive)\nabilities of the AI system in a between-subjects design, we tested whether\npeople's willingness to ascribe knowledge of a substantial risk of harm (i.e.,\nrecklessness) and blame to the AI system. Furthermore, we investigated whether\nthe ascription of recklessness and blame to the AI system would influence the\nperceived blameworthiness of the system's user (or owner). In an experiment\nwith 347 participants, we found (i) that people are willing to ascribe blame to\nAI systems in contexts of recklessness, (ii) that blame ascriptions depend\nstrongly on the willingness to attribute recklessness and (iii) that the\nlatter, in turn, depends on the perceived \"cognitive\" capacities of the system.\nFurthermore, our results suggest (iv) that the higher the computational\nsophistication of the AI system, the more blame is shifted from the human user\nto the AI system.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 20:53:42 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Kneer", "Markus", ""], ["Stuart", "Michael T.", ""]]}, {"id": "2102.04559", "submitter": "Henrietta Lyons", "authors": "Henrietta Lyons, Eduardo Velloso, Tim Miller", "title": "Designing for Contestation: Insights from Administrative Law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A paper presented at the Workshop on Contestability in Algorithmic Systems at\nCSCW 2019. Challenging algorithmic decisions is important to decision subjects,\nyet numerous factors can limit a person's ability to contest such decisions. We\npropose that administrative law systems, which were created to ensure that\ngovernments are kept accountable for their actions and decision making in\nrelation to individuals, can provide guidance on how to design contestation\nsystems for algorithmic decision-making.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 22:24:40 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Lyons", "Henrietta", ""], ["Velloso", "Eduardo", ""], ["Miller", "Tim", ""]]}, {"id": "2102.04958", "submitter": "Robyn Kozierok", "authors": "Robyn Kozierok, John Aberdeen, Cheryl Clark, Christopher Garay,\n  Bradley Goodman, Tonia Korves, Lynette Hirschman, Patricia L. McDermott,\n  Matthew W. Peterson", "title": "Hallmarks of Human-Machine Collaboration: A framework for assessment in\n  the DARPA Communicating with Computers Program", "comments": "20 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": "MITRE Document Number: MTR210002", "categories": "cs.HC cs.AI cs.CL cs.MA cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing desire to create computer systems that can communicate\neffectively to collaborate with humans on complex, open-ended activities.\nAssessing these systems presents significant challenges. We describe a\nframework for evaluating systems engaged in open-ended complex scenarios where\nevaluators do not have the luxury of comparing performance to a single right\nanswer. This framework has been used to evaluate human-machine creative\ncollaborations across story and music generation, interactive block building,\nand exploration of molecular mechanisms in cancer. These activities are\nfundamentally different from the more constrained tasks performed by most\ncontemporary personal assistants as they are generally open-ended, with no\nsingle correct solution, and often no obvious completion criteria.\n  We identified the Key Properties that must be exhibited by successful\nsystems. From there we identified \"Hallmarks\" of success -- capabilities and\nfeatures that evaluators can observe that would be indicative of progress\ntoward achieving a Key Property. In addition to being a framework for\nassessment, the Key Properties and Hallmarks are intended to serve as goals in\nguiding research direction.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 17:13:53 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Kozierok", "Robyn", ""], ["Aberdeen", "John", ""], ["Clark", "Cheryl", ""], ["Garay", "Christopher", ""], ["Goodman", "Bradley", ""], ["Korves", "Tonia", ""], ["Hirschman", "Lynette", ""], ["McDermott", "Patricia L.", ""], ["Peterson", "Matthew W.", ""]]}, {"id": "2102.05216", "submitter": "Sara Bunian", "authors": "Sara Bunian, Kai Li, Chaima Jemmali, Casper Harteveld, Yun Fu, Magy\n  Seif El-Nasr", "title": "VINS: Visual Search for Mobile User Interface Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for relative mobile user interface (UI) design examples can aid\ninterface designers in gaining inspiration and comparing design alternatives.\nHowever, finding such design examples is challenging, especially as current\nsearch systems rely on only text-based queries and do not consider the UI\nstructure and content into account. This paper introduces VINS, a visual search\nframework, that takes as input a UI image (wireframe, high-fidelity) and\nretrieves visually similar design examples. We first survey interface designers\nto better understand their example finding process. We then develop a\nlarge-scale UI dataset that provides an accurate specification of the\ninterface's view hierarchy (i.e., all the UI components and their specific\nlocation). By utilizing this dataset, we propose an object-detection based\nimage retrieval framework that models the UI context and hierarchical\nstructure. The framework achieves a mean Average Precision of 76.39\\% for the\nUI detection and high performance in querying similar UI designs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 01:46:33 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Bunian", "Sara", ""], ["Li", "Kai", ""], ["Jemmali", "Chaima", ""], ["Harteveld", "Casper", ""], ["Fu", "Yun", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "2102.05268", "submitter": "Sean Kross", "authors": "Sean Kross, Eszter Hargittai, Elissa M. Redmiles", "title": "Characterizing the Online Learning Landscape: What and How People Learn\n  Online", "comments": "19 pages, To Appear in the Proceedings of the ACM (PACM)\n  Human-Computer Interaction, CSCW 2021", "journal-ref": null, "doi": "10.1145/3449220", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Hundreds of millions of people learn something new online every day.\nSimultaneously, the study of online education has blossomed within the human\ncomputer interaction community, with new systems, experiments, and observations\ncreating and exploring previously undiscovered online learning environments. In\nthis study we endeavor to characterize this entire landscape of online learning\nexperiences using a national survey of 2260 US adults who are balanced to match\nthe demographics of the U.S. We examine the online learning resources that they\nconsult, and we analyze the subjects that they pursue using those resources.\nFurthermore, we compare both formal and informal online learning experiences on\na larger scale than has ever been done before, to our knowledge, to better\nunderstand which subjects people are seeking for intensive study. We find that\nthere is a core set of online learning experiences that are central to other\nexperiences and these are shared among the majority of people who learn online.\nWe conclude by showing how looking outside of these core online learning\nexperiences can reveal opportunities for innovation in online education.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 05:18:59 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Kross", "Sean", ""], ["Hargittai", "Eszter", ""], ["Redmiles", "Elissa M.", ""]]}, {"id": "2102.05361", "submitter": "Stefan Krumpen", "authors": "Stefan Krumpen, Reinhard Klein and Michael Weinmann", "title": "Towards Tangible Cultural Heritage Experiences -- Enriching VR-Based\n  Object Inspection with Haptic Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VR/AR technology is a key enabler for new ways of immersively experiencing\ncultural heritage artifacts based on their virtual counterparts obtained from a\ndigitization process. In this paper, we focus on enriching VR-based object\ninspection by additional haptic feedback, thereby creating tangible cultural\nheritage experiences. For this purpose, we present an approach for interactive\nand collaborative VR-based object inspection and annotation. Our system\nsupports high-quality 3D models with accurate reflectance characteristics while\nadditionally providing haptic feedback regarding the object shape features\nbased on a 3D printed replica. The digital object model in terms of a printable\nrepresentation of the geometry as well as reflectance characteristics are\nstored in a compact and streamable representation on a central server, which\nstreams the data to remotely connected users/clients. The latter can jointly\nperform an interactive inspection of the object in VR with additional haptic\nfeedback through the 3D printed replica. Evaluations regarding system\nperformance, visual quality of the considered models as well as insights from a\nuser study indicate an improved interaction, assessment and experience of the\nconsidered objects.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 10:01:21 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Krumpen", "Stefan", ""], ["Klein", "Reinhard", ""], ["Weinmann", "Michael", ""]]}, {"id": "2102.05374", "submitter": "Pierre Le Bras", "authors": "Tanya Howden, Pierre Le Bras, Thomas S. Methven, Stefano Padilla, Mike\n  J. Chantler", "title": "Enhancing Reading Strategies by Exploring A Theme-based Approach to\n  Literature Surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Searching large digital repositories can be extremely frustrating, as common\nlist-based formats encourage users to adopt a convenience-sampling approach\nthat favours chance discovery and random search, over meaningful exploration.\nWe have designed a methodology that allows users to visually and thematically\nexplore corpora, while developing personalised holistic reading strategies. We\ndescribe the results of a three-phase qualitative study, in which experienced\nresearchers used our interactive visualisation approach to analyse a set of\npublications and select relevant themes and papers. Using in-depth\nsemi-structured interviews and stimulated recall, we found that users: (i)\nselected papers that they otherwise would not have read, (ii) developed a more\ncoherent reading strategy, and (iii) understood the thematic structure and\nrelationships between papers more effectively. Finally, we make six design\nrecommendations to enhance current digital repositories that we have shown\nencourage users to adopt a more holistic and thematic research approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 10:36:45 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Howden", "Tanya", ""], ["Bras", "Pierre Le", ""], ["Methven", "Thomas S.", ""], ["Padilla", "Stefano", ""], ["Chantler", "Mike J.", ""]]}, {"id": "2102.05460", "submitter": "Juliana Ferreira J", "authors": "Juliana Jansen Ferreira and Mateus Monteiro", "title": "The human-AI relationship in decision-making: AI explanation to support\n  people on justifying their decisions", "comments": "Pre-print of paper accepted in Workshop on Transparency And\n  Explanations In Smart Systems (TEXSS) held in conjunction with ACM\n  Intelligent User Interfaces (IUI) (April 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The explanation dimension of Artificial Intelligence (AI) based system has\nbeen a hot topic for the past years. Different communities have raised concerns\nabout the increasing presence of AI in people's everyday tasks and how it can\naffect people's lives. There is a lot of research addressing the\ninterpretability and transparency concepts of explainable AI (XAI), which are\nusually related to algorithms and Machine Learning (ML) models. But in\ndecision-making scenarios, people need more awareness of how AI works and its\noutcomes to build a relationship with that system. Decision-makers usually need\nto justify their decision to others in different domains. If that decision is\nsomehow based on or influenced by an AI-system outcome, the explanation about\nhow the AI reached that result is key to building trust between AI and humans\nin decision-making scenarios. In this position paper, we discuss the role of\nXAI in decision-making scenarios, our vision of Decision-Making with AI-system\nin the loop, and explore one case from the literature about how XAI can impact\npeople justifying their decisions, considering the importance of building the\nhuman-AI relationship for those scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 14:28:34 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 14:27:15 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ferreira", "Juliana Jansen", ""], ["Monteiro", "Mateus", ""]]}, {"id": "2102.05506", "submitter": "Beibei Li", "authors": "Anindya Ghose, Xitong Guo, Beibei Li, Yuanyuan Dang", "title": "Empowering Patients Using Smart Mobile Health Platforms: Evidence From A\n  Randomized Field Experiment", "comments": "Forthcoming at MIS Quarterly (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.HC q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With today's technological advancements, mobile phones and wearable devices\nhave become extensions of an increasingly diffused and smart digital\ninfrastructure. In this paper, we examine mobile health (mHealth) platforms and\ntheir health and economic impacts on the outcomes of chronic disease patients.\nWe partnered with a major mHealth firm that provides one of the largest mHealth\napps in Asia specializing in diabetes care. We designed a randomized field\nexperiment based on detailed patient health activities (e.g., exercises, sleep,\nfood intake) and blood glucose values from 1,070 diabetes patients over several\nmonths. We find the adoption of the mHealth app leads to an improvement in\nhealth behavior, which leads to both short term metrics (reduction in patients'\nblood glucose and glycated hemoglobin levels) and longer-term metrics (hospital\nvisits and medical expenses). Patients who adopted the mHealth app undertook\nmore exercise, consumed healthier food, walked more steps and slept for longer\ntimes. They also were more likely to substitute offline visits with telehealth.\nA comparison of mobile vs. PC version of the same app demonstrates that mobile\nhas a stronger effect than PC in helping patients make these behavioral\nmodifications with respect to diet, exercise and lifestyle, which leads to an\nimprovement in their healthcare outcomes. We also compared outcomes when the\nplatform facilitates personalized health reminders to patients vs. generic\nreminders. Surprisingly, we find personalized mobile messages with\npatient-specific guidance can have an inadvertent (smaller) effect on patient\napp engagement and lifestyle changes, leading to a lower health improvement.\nHowever, they are more like to encourage a substitution of offline visits by\ntelehealth. Overall, our findings indicate the massive potential of mHealth\ntechnologies and platform design in achieving better healthcare outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 15:48:09 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 16:44:47 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Ghose", "Anindya", ""], ["Guo", "Xitong", ""], ["Li", "Beibei", ""], ["Dang", "Yuanyuan", ""]]}, {"id": "2102.05586", "submitter": "Yuki Matsuda", "authors": "Yuki Matsuda, Shogo Kawanaka, Hirohiko Suwa, Yutaka Arakawa, Keiichi\n  Yasumoto", "title": "ParmoSense: A Scenario-based Participatory Mobile Urban Sensing Platform\n  with User Motivation Engine", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid proliferation of mobile devices with various sensors have enabled\nParticipatory Mobile Sensing (PMS). Several PMS platforms provide multiple\nfunctions for various sensing purposes, but they are suffering from the open\nissues: limited use of their functions for a specific scenario/case and\nrequiring technical knowledge for organizers. In this paper, we propose a novel\nPMS platform named ParmoSense for easily and flexibly collecting urban\nenvironmental information. To reduce the burden on both organizers and\nparticipants, in ParmoSense, we employ two novel features: modularization of\nfunctions and scenario-based PMS system description. For modularization, we\nprovide the essential PMS functions as modules which can be easily chosen and\ncombined for sensing in different scenarios. The scenario-based description\nfeature allows organizers to easily and quickly set up a new participatory\nsensing instance and participants to easily install the corresponding scenario\nand participate in the sensing. Moreover, ParmoSense provides GUI tools as well\nfor creating and distributing PMS system easily, editing and visualizing\ncollected data quickly. It also provides multiple functions for encouraging\nparticipants' motivation for sustainable operation of the system. Through\nperformance comparison with existing PMS platforms, we confirmed ParmoSense\nshows the best cost-performance in the perspective of the workload for\npreparing PMS system and varieties of functions. In addition, to evaluate the\navailability and usability of ParmoSense, we conducted 19 case studies, which\nhave different locations, scales, and purposes, over 4 years with cooperation\nfrom ordinary citizens. Through the case studies and the questionnaire survey\nfor participants and organizers, we confirmed that ParmoSense can be easily\noperated and participated by ordinary citizens including non-technical persons.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 17:32:31 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Matsuda", "Yuki", ""], ["Kawanaka", "Shogo", ""], ["Suwa", "Hirohiko", ""], ["Arakawa", "Yutaka", ""], ["Yasumoto", "Keiichi", ""]]}, {"id": "2102.05612", "submitter": "Zehui Wang", "authors": "Pavlos Athanasios Apostolopoulos, Zehui Wang, Hanson Wang, Chad Zhou,\n  Kittipat Virochsiri, Norm Zhou, Igor L. Markov", "title": "Personalization for Web-based Services using Offline Reinforcement\n  Learning", "comments": "9 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale Web-based services present opportunities for improving UI\npolicies based on observed user interactions. We address challenges of learning\nsuch policies through model-free offline Reinforcement Learning (RL) with\noff-policy training. Deployed in a production system for user authentication in\na major social network, it significantly improves long-term objectives. We\narticulate practical challenges, compare several ML techniques, provide\ninsights on training and evaluation of RL models, and discuss generalizations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 18:17:00 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Apostolopoulos", "Pavlos Athanasios", ""], ["Wang", "Zehui", ""], ["Wang", "Hanson", ""], ["Zhou", "Chad", ""], ["Virochsiri", "Kittipat", ""], ["Zhou", "Norm", ""], ["Markov", "Igor L.", ""]]}, {"id": "2102.05741", "submitter": "Christa Cody", "authors": "Christa Cody, Mehak Maniktala, Nicholas Lytle, Min Chi, Tiffany Barnes", "title": "The Impact of Looking Further Ahead: A Comparison of Two Data-driven\n  Unsolicited Hint Types on Performance in an Intelligent Data-driven Logic\n  Tutor", "comments": "to be published in the International Journal of Artificial\n  Intelligence in Education (IJAIED)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has shown assistance can provide many benefits to novices lacking\nthe mental models needed for problem solving in a new domain. However, varying\napproaches to assistance, such as subgoals and next-step hints, have been\nimplemented with mixed results. Next-Step hints are common in data-driven\ntutors due to their straightforward generation from historical student data, as\nwell as research showing positive impacts on student learning. However, there\nis a lack of research exploring the possibility of extending data-driven\nmethods to provide higher-level assistance. Therefore, we modified our\ndata-driven Next-Step hint generator to provide Waypoints, hints that are a few\nsteps ahead, representing problem-solving subgoals. We hypothesized that\nWaypoints would benefit students with high prior knowledge, and that Next-Step\nhints would most benefit students with lower prior knowledge. In this study, we\ninvestigated the influence of data-driven hint type, Waypoints versus Next-Step\nhints, on student learning in a logic proof tutoring system, Deep Thought, in a\ndiscrete mathematics course. We found that Next-Step hints were more beneficial\nfor the majority of students in terms of time, efficiency, and accuracy on the\nposttest. However, higher totals of successfully used Waypoints were correlated\nwith improvements in efficiency and time in the posttest. These results suggest\nthat Waypoint hints could be beneficial, but more scaffolding may be needed to\nhelp students follow them.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 21:20:01 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Cody", "Christa", ""], ["Maniktala", "Mehak", ""], ["Lytle", "Nicholas", ""], ["Chi", "Min", ""], ["Barnes", "Tiffany", ""]]}, {"id": "2102.05756", "submitter": "Jess Hohenstein", "authors": "Jess Hohenstein and Dominic DiFranzo and Rene F. Kizilcec and Zhila\n  Aghajari and Hannah Mieczkowski and Karen Levy and Mor Naaman and Jeff\n  Hancock and Malte Jung", "title": "Artificial intelligence in communication impacts language and social\n  relationships", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Artificial intelligence (AI) is now widely used to facilitate social\ninteraction, but its impact on social relationships and communication is not\nwell understood. We study the social consequences of one of the most pervasive\nAI applications: algorithmic response suggestions (\"smart replies\"). Two\nrandomized experiments (n = 1036) provide evidence that a commercially-deployed\nAI changes how people interact with and perceive one another in pro-social and\nanti-social ways. We find that using algorithmic responses increases\ncommunication efficiency, use of positive emotional language, and positive\nevaluations by communication partners. However, consistent with common\nassumptions about the negative implications of AI, people are evaluated more\nnegatively if they are suspected to be using algorithmic responses. Thus, even\nthough AI can increase communication efficiency and improve interpersonal\nperceptions, it risks changing users' language production and continues to be\nviewed negatively.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 22:05:11 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Hohenstein", "Jess", ""], ["DiFranzo", "Dominic", ""], ["Kizilcec", "Rene F.", ""], ["Aghajari", "Zhila", ""], ["Mieczkowski", "Hannah", ""], ["Levy", "Karen", ""], ["Naaman", "Mor", ""], ["Hancock", "Jeff", ""], ["Jung", "Malte", ""]]}, {"id": "2102.05998", "submitter": "Alexander Sch\\\"afer", "authors": "Alexander Sch\\\"afer, Gerd Reis, Didier Stricker", "title": "A Survey on Synchronous Augmented, Virtual and Mixed Reality Remote\n  Collaboration Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote collaboration systems have become increasingly important in today's\nsociety, especially during times where physical distancing is advised.\nIndustry, research and individuals face the challenging task of collaborating\nand networking over long distances. While video and teleconferencing are\nalready widespread, collaboration systems in augmented, virtual, and mixed\nreality are still a niche technology. We provide an overview of recent\ndevelopments of synchronous remote collaboration systems and create a taxonomy\nby dividing them into three main components that form such systems:\nEnvironment, Avatars, and Interaction. A thorough overview of existing systems\nis given, categorising their main contributions in order to help researchers\nworking in different fields by providing concise information about specific\ntopics such as avatars, virtual environment, visualisation styles and\ninteraction. The focus of this work is clearly on synchronised collaboration\nfrom a distance. A total of 82 unique systems for remote collaboration are\ndiscussed, including more than 100 publications and 25 commercial systems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 13:33:51 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Sch\u00e4fer", "Alexander", ""], ["Reis", "Gerd", ""], ["Stricker", "Didier", ""]]}, {"id": "2102.06231", "submitter": "Michael Xieyang Liu", "authors": "Michael Xieyang Liu, Aniket Kittur, Brad A. Myers", "title": "To Reuse or Not To Reuse? A Framework and System for Evaluating\n  Summarized Knowledge", "comments": null, "journal-ref": "Proc. ACM Hum.-Comput. Interact.5, CSCW1, Article 166(April 2021),\n  35 pages", "doi": "10.1145/3449240", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the amount of information online continues to grow, a correspondingly\nimportant opportunity is for individuals to reuse knowledge which has been\nsummarized by others rather than starting from scratch. However, appropriate\nreuse requires judging the relevance, trustworthiness, and thoroughness of\nothers' knowledge in relation to an individual's goals and context. In this\nwork, we explore augmenting judgements of the appropriateness of reusing\nknowledge in the domain of programming, specifically of reusing artifacts that\nresult from other developers' searching and decision making. Through an\nanalysis of prior research on sensemaking and trust, along with new interviews\nwith developers, we synthesized a framework for reuse judgements. The\ninterviews also validated that developers express a desire for help with\njudging whether to reuse an existing decision. From this framework, we\ndeveloped a set of techniques for capturing the initial decision maker's\nbehavior and visualizing signals calculated based on the behavior, to\nfacilitate subsequent consumers' reuse decisions, instantiated in a prototype\nsystem called Strata. Results of a user study suggest that the system\nsignificantly improves the accuracy, depth, and speed of reusing decisions.\nThese results have implications for systems involving user-generated content in\nwhich other users need to evaluate the relevance and trustworthiness of that\ncontent.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 19:23:09 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 00:24:19 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Liu", "Michael Xieyang", ""], ["Kittur", "Aniket", ""], ["Myers", "Brad A.", ""]]}, {"id": "2102.06343", "submitter": "Xin Qian", "authors": "Xin Qian, Ryan A. Rossi, Fan Du, Sungchul Kim, Eunyee Koh, Sana Malik,\n  Tak Yeon Lee, Nesreen K. Ahmed", "title": "Personalized Visualization Recommendation", "comments": "37 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visualization recommendation work has focused solely on scoring\nvisualizations based on the underlying dataset and not the actual user and\ntheir past visualization feedback. These systems recommend the same\nvisualizations for every user, despite that the underlying user interests,\nintent, and visualization preferences are likely to be fundamentally different,\nyet vitally important. In this work, we formally introduce the problem of\npersonalized visualization recommendation and present a generic learning\nframework for solving it. In particular, we focus on recommending\nvisualizations personalized for each individual user based on their past\nvisualization interactions (e.g., viewed, clicked, manually created) along with\nthe data from those visualizations. More importantly, the framework can learn\nfrom visualizations relevant to other users, even if the visualizations are\ngenerated from completely different datasets. Experiments demonstrate the\neffectiveness of the approach as it leads to higher quality visualization\nrecommendations tailored to the specific user intent and preferences. To\nsupport research on this new problem, we release our user-centric visualization\ncorpus consisting of 17.4k users exploring 94k datasets with 2.3 million\nattributes and 32k user-generated visualizations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 04:06:34 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Qian", "Xin", ""], ["Rossi", "Ryan A.", ""], ["Du", "Fan", ""], ["Kim", "Sungchul", ""], ["Koh", "Eunyee", ""], ["Malik", "Sana", ""], ["Lee", "Tak Yeon", ""], ["Ahmed", "Nesreen K.", ""]]}, {"id": "2102.06391", "submitter": "Laria Reynolds", "authors": "Laria Reynolds and Kyle McDonell", "title": "Multiversal views on language models", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The virtuosity of language models like GPT-3 opens a new world of possibility\nfor human-AI collaboration in writing. In this paper, we present a framework in\nwhich generative language models are conceptualized as multiverse generators.\nThis framework also applies to human imagination and is core to how we read and\nwrite fiction. We call for exploration into this commonality through new forms\nof interfaces which allow humans to couple their imagination to AI to write,\nexplore, and understand non-linear fiction. We discuss the early insights we\nhave gained from actively pursuing this approach by developing and testing a\nnovel multiversal GPT-3-assisted writing interface.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 08:28:28 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 05:25:35 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Reynolds", "Laria", ""], ["McDonell", "Kyle", ""]]}, {"id": "2102.06422", "submitter": "Riku Arakawa", "authors": "Riku Arakawa and Hiromu Yakura", "title": "Reaction or Speculation: Building Computational Support for Users in\n  Catching-Up Series Based on an Emerging Media Consumption Phenomenon", "comments": "To appear in Proceedings of the ACM on Human-Computer Interaction,\n  Vol. 5, No. CSCW1 (151)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of people are using catch-up TV services rather than\nwatching simultaneously with other audience members at the time of broadcast.\nHowever, computational support for such catching-up users has not been well\nexplored. In particular, we are observing an emerging phenomenon in online\nmedia consumption experiences in which speculation plays a vital role. As the\nphenomenon of speculation implicitly assumes simultaneity in media consumption,\nthere is a gap for catching-up users, who cannot directly appreciate the\nconsumption experiences. This conversely suggests that there is potential for\ncomputational support to enhance the consumption experiences of catching-up\nusers. Accordingly, we conducted a series of studies to pave the way for\ndeveloping computational support for catching-up users. First, we conducted\nsemi-structured interviews to understand how people are engaging with\nspeculation during media consumption. As a result, we discovered the\ndistinctive aspects of speculation-based consumption experiences in contrast to\nsocial viewing experiences sharing immediate reactions that have been discussed\nin previous studies. We then designed two prototypes for supporting catching-up\nusers based on our quantitative analysis of Twitter data in regard to reaction-\nand speculation-based media consumption. Lastly, we evaluated the prototypes in\na user experiment and, based on its results, discussed ways to empower\ncatching-up users with computational supports in response to recent\ntransformations in media consumption.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 10:01:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Arakawa", "Riku", ""], ["Yakura", "Hiromu", ""]]}, {"id": "2102.06690", "submitter": "Youngjun Cho", "authors": "Youngjun Cho", "title": "Rethinking Eye-blink: Assessing Task Difficulty through Physiological\n  Representation of Spontaneous Blinking", "comments": "[Accepted version] In Proceedings of CHI Conference on Human Factors\n  in Computing Systems (CHI '21), May 8-13, 2021, Yokohama, Japan. ACM, New\n  York, NY, USA. 19 Pages. https://doi.org/10.1145/3411764.3445577", "journal-ref": null, "doi": "10.1145/3411764.3445577", "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous assessment of task difficulty and mental workload is essential in\nimproving the usability and accessibility of interactive systems. Eye tracking\ndata has often been investigated to achieve this ability, with reports on the\nlimited role of standard blink metrics. Here, we propose a new approach to the\nanalysis of eye-blink responses for automated estimation of task difficulty.\nThe core module is a time-frequency representation of eye-blink, which aims to\ncapture the richness of information reflected on blinking. In our first study,\nwe show that this method significantly improves the sensitivity to task\ndifficulty. We then demonstrate how to form a framework where the represented\npatterns are analyzed with multi-dimensional Long Short-Term Memory recurrent\nneural networks for their non-linear mapping onto difficulty-related\nparameters. This framework outperformed other methods that used hand-engineered\nfeatures. This approach works with any built-in camera, without requiring\nspecialized devices. We conclude by discussing how Rethinking Eye-blink can\nbenefit real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 18:47:13 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Cho", "Youngjun", ""]]}, {"id": "2102.06755", "submitter": "Preeti Ramaraj", "authors": "Preeti Ramaraj, Charles L. Ortiz, Jr., and Shiwali Mohan", "title": "Unpacking Human Teachers' Intentions For Natural Interactive Task\n  Learning", "comments": "8 pages, 5 figures, paper revised for submission to conference,\n  authors updated, to be presented at RO-MAN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive Task Learning (ITL) is an emerging research agenda that studies\nthe design of complex intelligent robots that can acquire new knowledge through\nnatural human teacher-robot learner interactions. ITL methods are particularly\nuseful for designing intelligent robots whose behavior can be adapted by humans\ncollaborating with them. Various research communities are contributing methods\nfor ITL and a large subset of this research is \\emph{robot-centered} with a\nfocus on developing algorithms that can learn online, quickly. This paper\nstudies the ITL problem from a \\emph{human-centered} perspective to provide\nguidance for robot design so that human teachers can naturally teach ITL\nrobots. In this paper, we present 1) a qualitative bidirectional analysis of an\ninteractive teaching study (N=10) through which we characterize various aspects\nof actions intended and executed by human teachers when teaching a robot; 2) an\nin-depth discussion of the teaching approach employed by two participants to\nunderstand the need for personal adaptation to individual teaching styles; and\n3) requirements for ITL robot design based on our analyses and informed by a\ncomputational theory of collaborative interactions, SharedPlans.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 20:19:43 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 21:48:59 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ramaraj", "Preeti", ""], ["Ortiz,", "Charles L.", "Jr."], ["Mohan", "Shiwali", ""]]}, {"id": "2102.06757", "submitter": "Abhinav Godavarthi", "authors": "Manik Kuchroo, Abhinav Godavarthi, Guy Wolf, Smita Krishnaswamy", "title": "Multimodal data visualization, denoising and clustering with integrated\n  diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method called integrated diffusion for combining multimodal\ndatasets, or data gathered via several different measurements on the same\nsystem, to create a joint data diffusion operator. As real world data suffers\nfrom both local and global noise, we introduce mechanisms to optimally\ncalculate a diffusion operator that reflects the combined information from both\nmodalities. We show the utility of this joint operator in data denoising,\nvisualization and clustering, performing better than other methods to integrate\nand analyze multimodal data. We apply our method to multi-omic data generated\nfrom blood cells, measuring both gene expression and chromatin accessibility.\nOur approach better visualizes the geometry of the joint data, captures known\ncross-modality associations and identifies known cellular populations. More\ngenerally, integrated diffusion is broadly applicable to multimodal datasets\ngenerated in many medical and biological systems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 20:22:06 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kuchroo", "Manik", ""], ["Godavarthi", "Abhinav", ""], ["Wolf", "Guy", ""], ["Krishnaswamy", "Smita", ""]]}, {"id": "2102.06893", "submitter": "Susannah Devitt", "authors": "Susannah Kate Devitt, Tamara Rose Pearce, Alok Kumar Chowdhury and\n  Kerrie Mengersen", "title": "A Bayesian social platform for inclusive and evidence-based decision\n  making", "comments": "38 pages, 3 tables, 13 figures submitted for peer review for\n  inclusion in M. Alfano, C. Klein and J de Ridder (Eds.) Social Virtue\n  Epistemology. Routledge [forthcoming]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Against the backdrop of a social media reckoning, this paper seeks to\ndemonstrate the potential of social tools to build virtuous behaviours online.\nWe must assume that human behaviour is flawed, the truth can be elusive, and as\ncommunities we must commit to mechanisms to encourage virtuous social digital\nbehaviours. Societies that use social platforms should be inclusive, responsive\nto evidence, limit punitive actions and allow productive discord and respectful\ndisagreement. Social media success, we argue, is in the hypothesis. Documents\nare valuable to the degree that they are evidence in service of, or to\nchallenge an idea for a purpose. We outline how a Bayesian social platform can\nfacilitate virtuous behaviours to build evidence-based collective rationality.\nThe chapter outlines the epistemic architecture of the platform's algorithms\nand user interface in conjunction with explicit community management to ensure\npsychological safety. The BetterBeliefs platform rewards users who demonstrate\nepistemically virtuous behaviours and exports evidence-based propositions for\ndecision-making. A Bayesian social network can make virtuous ideas powerful.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 10:24:09 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Devitt", "Susannah Kate", ""], ["Pearce", "Tamara Rose", ""], ["Chowdhury", "Alok Kumar", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2102.07024", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen, Dipendra Misra, Robert Schapire, Miro Dud\\'ik, Patrick\n  Shafto", "title": "Interactive Learning from Activity Description", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a novel interactive learning protocol that enables training\nrequest-fulfilling agents by verbally describing their activities. Unlike\nimitation learning (IL), our protocol allows the teaching agent to provide\nfeedback in a language that is most appropriate for them. Compared with reward\nin reinforcement learning (RL), the description feedback is richer and allows\nfor improved sample complexity. We develop a probabilistic framework and an\nalgorithm that practically implements our protocol. Empirical results in two\nchallenging request-fulfilling problems demonstrate the strengths of our\napproach: compared with RL baselines, it is more sample-efficient; compared\nwith IL baselines, it achieves competitive success rates without requiring the\nteaching agent to be able to demonstrate the desired behavior using the\nlearning agent's actions. Apart from empirical evaluation, we also provide\ntheoretical guarantees for our algorithm under certain assumptions about the\nteacher and the environment.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 22:51:11 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 23:40:40 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Nguyen", "Khanh", ""], ["Misra", "Dipendra", ""], ["Schapire", "Robert", ""], ["Dud\u00edk", "Miro", ""], ["Shafto", "Patrick", ""]]}, {"id": "2102.07070", "submitter": "Doris Jung-Lin Lee", "authors": "Doris Jung-Lin Lee, Vidya Setlur, Melanie Tory, Karrie Karahalios,\n  Aditya Parameswaran", "title": "Deconstructing Categorization in Visualization Recommendation: A\n  Taxonomy and Comparative Study", "comments": "10 pages. This work has been submitted to IEEE TVCG. Copyright may be\n  transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visualization recommendation (VisRec) systems provide users with suggestions\nfor potentially interesting and useful next steps during exploratory data\nanalysis. These recommendations are typically organized into categories based\non their analytical actions, i.e., operations employed to transition from the\ncurrent exploration state to a recommended visualization. However, despite the\nemergence of a plethora of VisRec systems in recent work, the utility of the\ncategories employed by these systems in analytical workflows has not been\nsystematically investigated. Our paper explores the efficacy of recommendation\ncategories by formalizing a taxonomy of common categories and developing a\nsystem, Frontier, that implements these categories. Using Frontier, we evaluate\nworkflow strategies adopted by users and how categories influence those\nstrategies. Participants found recommendations that add attributes to enhance\nthe current visualization and recommendations that filter to sub-populations to\nbe comparatively most useful during data exploration. Our findings pave the way\nfor next-generation VisRec systems that are adaptive and personalized via\ncarefully chosen, effective recommendation categories.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 05:08:23 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Lee", "Doris Jung-Lin", ""], ["Setlur", "Vidya", ""], ["Tory", "Melanie", ""], ["Karahalios", "Karrie", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "2102.07090", "submitter": "Rico Picone", "authors": "Rico A.R. Picone, Dane Webb, Finbarr Obierefu, Jotham Lentz", "title": "New methods for metastimuli: architecture, embeddings, and neural\n  network optimization", "comments": "To appear in the Springer Lecture Notes in Artificial Intelligence\n  for the Human-Computer Interaction Conference 2021, Augmented Cognition\n  thematic area", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Six significant new methodological developments of the previously-presented\n\"metastimuli architecture\" for human learning through machine learning of\nspatially correlated structural position within a user's personal information\nmanagement system (PIMS), providing the basis for haptic metastimuli, are\npresented. These include architectural innovation, recurrent (RNN) artificial\nneural network (ANN) application, a variety of atom embedding techniques\n(including a novel technique we call \"nabla\" embedding inspired by\nlinguistics), ANN hyper-parameter (one that affects the network but is not\ntrained, e.g. the learning rate) optimization, and meta-parameter (one that\ndetermines the system performance but is not trained and not a hyper-parameter,\ne.g. the atom embedding technique) optimization for exploring the large design\nspace. A technique for using the system for automatic atom categorization in a\nuser's PIMS is outlined. ANN training and hyper- and meta-parameter\noptimization results are presented and discussed in service of methodological\nrecommendations.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 07:28:40 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Picone", "Rico A. R.", ""], ["Webb", "Dane", ""], ["Obierefu", "Finbarr", ""], ["Lentz", "Jotham", ""]]}, {"id": "2102.07127", "submitter": "Samrat Kumar Dey", "authors": "Md. Mahbubur Rahman, Akash Poddar, Md. Golam Rabiul Alam, and Samrat\n  Kumar Dey", "title": "Affective State Recognition through EEG Signals Feature Level Fusion and\n  Ensemble Classifier", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Human affects are complex paradox and an active research domain in affective\ncomputing. Affects are traditionally determined through a self-report based\npsychometric questionnaire or through facial expression recognition. However,\nfew state-of-the-arts pieces of research have shown the possibilities of\nrecognizing human affects from psychophysiological and neurological signals. In\nthis article, electroencephalogram (EEG) signals are used to recognize human\naffects. The electroencephalogram (EEG) of 100 participants are collected where\nthey are given to watch one-minute video stimuli to induce different affective\nstates. The videos with emotional tags have a variety range of affects\nincluding happy, sad, disgust, and peaceful. The experimental stimuli are\ncollected and analyzed intensively. The interrelationship between the EEG\nsignal frequencies and the ratings given by the participants are taken into\nconsideration for classifying affective states. Advanced feature extraction\ntechniques are applied along with the statistical features to prepare a fused\nfeature vector of affective state recognition. Factor analysis methods are also\napplied to select discriminative features. Finally, several popular supervised\nmachine learning classifier is applied to recognize different affective states\nfrom the discriminative feature vector. Based on the experiment, the designed\nrandom forest classifier produces 89.06% accuracy in classifying four basic\naffective states.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 10:56:08 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Rahman", "Md. Mahbubur", ""], ["Poddar", "Akash", ""], ["Alam", "Md. Golam Rabiul", ""], ["Dey", "Samrat Kumar", ""]]}, {"id": "2102.07193", "submitter": "Vignesh Prasad", "authors": "Vignesh Prasad, Ruth Stock-Homburg, Jan Peters", "title": "Human-Robot Handshaking: A Review", "comments": "Pre-print version. Accepted for publication in the International\n  Journal of Social Robotics", "journal-ref": null, "doi": "10.1007/s12369-021-00763-z", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For some years now, the use of social, anthropomorphic robots in various\nsituations has been on the rise. These are robots developed to interact with\nhumans and are equipped with corresponding extremities. They already support\nhuman users in various industries, such as retail, gastronomy, hotels,\neducation and healthcare. During such Human-Robot Interaction (HRI) scenarios,\nphysical touch plays a central role in the various applications of social\nrobots as interactive non-verbal behaviour is a key factor in making the\ninteraction more natural. Shaking hands is a simple, natural interaction used\ncommonly in many social contexts and is seen as a symbol of greeting, farewell\nand congratulations. In this paper, we take a look at the existing state of\nHuman-Robot Handshaking research, categorise the works based on their focus\nareas, draw out the major findings of these areas while analysing their\npitfalls. We mainly see that some form of synchronisation exists during the\ndifferent phases of the interaction. In addition to this, we also find that\nadditional factors like gaze, voice facial expressions etc. can affect the\nperception of a robotic handshake and that internal factors like personality\nand mood can affect the way in which handshaking behaviours are executed by\nhumans. Based on the findings and insights, we finally discuss possible ways\nforward for research on such physically interactive behaviours.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 16:46:24 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Prasad", "Vignesh", ""], ["Stock-Homburg", "Ruth", ""], ["Peters", "Jan", ""]]}, {"id": "2102.07312", "submitter": "Shoya Ishimaru", "authors": "Shoya Ishimaru, Takanori Maruichi, Andreas Dengel and Koichi Kise", "title": "Confidence-Aware Learning Assistant", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not only correctness but also self-confidence play an important role in\nimproving the quality of knowledge. Undesirable situations such as confident\nincorrect and unconfident correct knowledge prevent learners from revising\ntheir knowledge because it is not always easy for them to perceive the\nsituations. To solve this problem, we propose a system that estimates\nself-confidence while solving multiple-choice questions by eye tracking and\ngives feedback about which question should be reviewed carefully. We report the\nresults of three studies measuring its effectiveness. (1) On a well-controlled\ndataset with 10 participants, our approach detected confidence and unconfidence\nwith 81% and 79% average precision. (2) With the help of 20 participants, we\nobserved that correct answer rates of questions were increased by 14% and 17%\nby giving feedback about correct answers without confidence and incorrect\nanswers with confidence, respectively. (3) We conducted a large-scale data\nrecording in a private school (72 high school students solved 14,302 questions)\nto investigate effective features and the number of required training samples.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 02:47:11 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Ishimaru", "Shoya", ""], ["Maruichi", "Takanori", ""], ["Dengel", "Andreas", ""], ["Kise", "Koichi", ""]]}, {"id": "2102.07421", "submitter": "Ioanna Lykourentzou", "authors": "Ioanna Lykourentzou, Federica Lucia Vinella, Faez Ahmed, Costas\n  Papastathis, Konstantinos Papangelis, Vassilis-Javed Khan, Judith Masthoff", "title": "Self-Organizing Teams in Online Work Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the volume and complexity of distributed online work increases, the\ncollaboration among people who have never worked together in the past is\nbecoming increasingly necessary. Recent research has proposed algorithms to\nmaximize the performance of such teams by grouping workers according to a set\nof predefined decision criteria. This approach micro-manages workers, who have\nno say in the team formation process. Depriving users of control over who they\nwill work with stifles creativity, causes psychological discomfort and results\nin less-than-optimal collaboration results. In this work, we propose an\nalternative model, called Self-Organizing Teams (SOTs), which relies on the\ncrowd of online workers itself to organize into effective teams. Supported but\nnot guided by an algorithm, SOTs are a new human-centered computational\nstructure, which enables participants to control, correct and guide the output\nof their collaboration as a collective. Experimental results, comparing SOTs to\ntwo benchmarks that do not offer user agency over the collaboration, reveal\nthat participants in the SOTs condition produce results of higher quality and\nreport higher teamwork satisfaction. We also find that, similarly to machine\nlearning-based self-organization, human SOTs exhibit emergent collective\nproperties, including the presence of an objective function and the tendency to\nform more distinct clusters of compatible teammates.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 09:49:33 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Lykourentzou", "Ioanna", ""], ["Vinella", "Federica Lucia", ""], ["Ahmed", "Faez", ""], ["Papastathis", "Costas", ""], ["Papangelis", "Konstantinos", ""], ["Khan", "Vassilis-Javed", ""], ["Masthoff", "Judith", ""]]}, {"id": "2102.07470", "submitter": "Pawe{\\l} W. Wo\\'zniak", "authors": "Pawe{\\l} W. Wo\\'zniak, Jakob Karolus, Florian Lang, Caroline Eckherth,\n  Johannes Sch\\\"oning, Yvonne Rogers, Jasmin Niess", "title": "Creepy Technology: What Is It and How Do You Measure It?", "comments": "13 pages", "journal-ref": "Creepy Technology: What Is It and How Do You Measure It?. In CHI\n  Conference on Human Factors in Computing Systems (CHI '21), May 8-13, 2021,\n  Yokohama, Japan. ACM, New York, NY, USA", "doi": "10.1145/3411764.3445299", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive technologies are getting closer to our bodies and permeate the\ninfrastructure of our homes. While such technologies offer many benefits, they\ncan also cause an initial feeling of unease in users. It is important for\nHuman-Computer Interaction to manage first impressions and avoid designing\ntechnologies that appear creepy. To that end, we developed the Perceived\nCreepiness of Technology Scale (PCTS), which measures how creepy a technology\nappears to a user in an initial encounter with a new artefact. The scale was\ndeveloped based on past work on creepiness and a set of ten focus groups\nconducted with users from diverse backgrounds. We followed a structured process\nof analytically developing and validating the scale. The PCTS is designed to\nenable designers and researchers to quickly compare interactive technologies\nand ensure that they do not design technologies that produce initial feelings\nof creepiness in users.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 11:27:59 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wo\u017aniak", "Pawe\u0142 W.", ""], ["Karolus", "Jakob", ""], ["Lang", "Florian", ""], ["Eckherth", "Caroline", ""], ["Sch\u00f6ning", "Johannes", ""], ["Rogers", "Yvonne", ""], ["Niess", "Jasmin", ""]]}, {"id": "2102.07537", "submitter": "Diogo Carvalho", "authors": "Diogo S. Carvalho, Joana Campos, Manuel Guimar\\~aes, Ana Antunes,\n  Jo\\~ao Dias, Pedro A. Santos", "title": "CHARET: Character-centered Approach to Emotion Tracking in Stories", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous agents that can engage in social interactions witha human is the\nultimate goal of a myriad of applications. A keychallenge in the design of\nthese applications is to define the socialbehavior of the agent, which requires\nextensive content creation.In this research, we explore how we can leverage\ncurrent state-of-the-art tools to make inferences about the emotional state ofa\ncharacter in a story as events unfold, in a coherent way. Wepropose a character\nrole-labelling approach to emotion tracking thataccounts for the semantics of\nemotions. We show that by identifyingactors and objects of events and\nconsidering the emotional stateof the characters, we can achieve better\nperformance in this task,when compared to end-to-end approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 13:17:21 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 09:01:53 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Carvalho", "Diogo S.", ""], ["Campos", "Joana", ""], ["Guimar\u00e3es", "Manuel", ""], ["Antunes", "Ana", ""], ["Dias", "Jo\u00e3o", ""], ["Santos", "Pedro A.", ""]]}, {"id": "2102.07548", "submitter": "Jichen Zhu", "authors": "Jichen Zhu, Santiago Onta\\~n\\'on", "title": "Player-Centered AI for Automatic Game Personalization: Open Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer games represent an ideal research domain for the next generation of\npersonalized digital applications. This paper presents a player-centered\nframework of AI for game personalization, complementary to the commonly used\nsystem-centered approaches. Built on the Structure of Actions theory, the paper\nmaps out the current landscape of game personalization research and identifies\neight open problems that need further investigation. These problems require\ndeep collaboration between technological advancement and player experience\ndesign.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 13:34:38 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhu", "Jichen", ""], ["Onta\u00f1\u00f3n", "Santiago", ""]]}, {"id": "2102.07621", "submitter": "Luana M\\\"uller", "authors": "Luana M\\\"uller, Camila Moser, Guilherme Paris, Lucas Freitas, Mayara\n  Oliveira, Wagner Signoretti, Isabel Harb Manssour, Milene Selbach Silveira", "title": "Hit by the Data: a visual data analysis regarding the effects of traffic\n  public policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of Open Government Data (OGD) provides means for citizens to\nunderstand and follow governmental policies and decisions, showing evidence of\nhow the latter have contributed to both the place they live in and their lives.\nIn such a scenario, one of the proposals is the use of visualizations to\nsupport the process of data analysis and interpretation. Herein, we present the\nuse of three different visualization tools, a commercial one and two academic\nones, applied to two specific Brazilian cases: the implementation of the Drink\nDriving Law and the construction of a new overpass in an important city avenue.\nOur focus was on the analysis of how visualization could help in the\nidentification of the effects of such traffic public policies. As our main\ncontributions, we present details on the effects of the observed policies, as\nwell as new cases showing how visualization tools can assist users to interpret\nOGD.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 18:23:03 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["M\u00fcller", "Luana", ""], ["Moser", "Camila", ""], ["Paris", "Guilherme", ""], ["Freitas", "Lucas", ""], ["Oliveira", "Mayara", ""], ["Signoretti", "Wagner", ""], ["Manssour", "Isabel Harb", ""], ["Silveira", "Milene Selbach", ""]]}, {"id": "2102.07735", "submitter": "Hsiang-Yun Wu", "authors": "Thomas K\\\"oppel and M. Eduard Gr\\\"oller and Hsiang-Yun Wu", "title": "Context-Responsive Labeling in Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Route planning and navigation are common tasks that often require additional\ninformation on points of interest. Augmented Reality (AR) enables mobile users\nto utilize text labels, in order to provide a composite view associated with\nadditional information in a real-world environment. Nonetheless, displaying all\nlabels for points of interest on a mobile device will lead to unwanted overlaps\nbetween information, and thus a context-responsive strategy to properly arrange\nlabels is expected. The technique should remove overlaps, show the right\nlevel-of-detail, and maintain label coherence. This is necessary as the viewing\nangle in an AR system may change rapidly due to users' behaviors. Coherence\nplays an essential role in retaining user experience and knowledge, as well as\navoiding motion sickness. In this paper, we develop an approach that\nsystematically manages label visibility and levels-of-detail, as well as\neliminates unexpected incoherent movement. We introduce three label management\nstrategies, including (1) occlusion management, (2) level-of-detail management,\nand (3) coherence management by balancing the usage of the mobile phone screen.\nA greedy approach is developed for fast occlusion handling in AR. A\nlevel-of-detail scheme is adopted to arrange various types of labels. A 3D\nscene manipulation is then built to simultaneously suppress the incoherent\nbehaviors induced by viewing angle changes. Finally, we present the feasibility\nand applicability of our approach through one synthetic and two real-world\nscenarios, followed by a qualitative user study.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:33:19 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["K\u00f6ppel", "Thomas", ""], ["Gr\u00f6ller", "M. Eduard", ""], ["Wu", "Hsiang-Yun", ""]]}, {"id": "2102.07817", "submitter": "Markus Langer Dr.", "authors": "Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena\n  K\\\"astner, Eva Schmidt, Andreas Sesing, Kevin Baum", "title": "What Do We Want From Explainable Artificial Intelligence (XAI)? -- A\n  Stakeholder Perspective on XAI and a Conceptual Model Guiding\n  Interdisciplinary XAI Research", "comments": "57 pages, 2 figures, 1 table, to be published in Artificial\n  Intelligence, Markus Langer, Daniel Oster and Timo Speith share\n  first-authorship of this paper", "journal-ref": null, "doi": "10.1016/j.artint.2021.103473", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Previous research in Explainable Artificial Intelligence (XAI) suggests that\na main aim of explainability approaches is to satisfy specific interests,\ngoals, expectations, needs, and demands regarding artificial systems (we call\nthese stakeholders' desiderata) in a variety of contexts. However, the\nliterature on XAI is vast, spreads out across multiple largely disconnected\ndisciplines, and it often remains unclear how explainability approaches are\nsupposed to achieve the goal of satisfying stakeholders' desiderata. This paper\ndiscusses the main classes of stakeholders calling for explainability of\nartificial systems and reviews their desiderata. We provide a model that\nexplicitly spells out the main concepts and relations necessary to consider and\ninvestigate when evaluating, adjusting, choosing, and developing explainability\napproaches that aim to satisfy stakeholders' desiderata. This model can serve\nresearchers from the variety of different disciplines involved in XAI as a\ncommon ground. It emphasizes where there is interdisciplinary potential in the\nevaluation and the development of explainability approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 19:54:33 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Langer", "Markus", ""], ["Oster", "Daniel", ""], ["Speith", "Timo", ""], ["Hermanns", "Holger", ""], ["K\u00e4stner", "Lena", ""], ["Schmidt", "Eva", ""], ["Sesing", "Andreas", ""], ["Baum", "Kevin", ""]]}, {"id": "2102.07844", "submitter": "Samir Passi", "authors": "Samir Passi, Phoebe Sengers", "title": "\"From What I see, this makes sense\": Seeing meaning in algorithmic\n  results", "comments": "Paper presented at \"Algorithms at work: Empirical Diversity, Analytic\n  Vocabularies, and Design Implications\" workshop at the 2016 ACM conference on\n  Computer Supported Cooperative Work & Social Computing (CSCW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this workshop paper, we use an empirical example from our ongoing\nfieldwork, to showcase the complexity and situatedness of the process of making\nsense of algorithmic results; i.e. how to evaluate, validate, and contextualize\nalgorithmic outputs. So far, in our research work, we have focused on such\nsense-making processes in data analytic learning environments such as\nclassrooms and training workshops. Multiple moments in our fieldwork suggest\nthat meaning, in data analytics, is constructed through an iterative and\nreflexive dialogue between data, code, assumptions, prior knowledge, and\nalgorithmic results. A data analytic result is nothing short of a\nsociotechnical accomplishment - one in which it is extremely difficult, if not\nat times impossible, to clearly distinguish between 'human' and 'technical'\nforms of data analytic work. We conclude this paper with a set of questions\nthat we would like to explore further in this workshop.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 20:50:11 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 17:14:38 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Passi", "Samir", ""], ["Sengers", "Phoebe", ""]]}, {"id": "2102.07958", "submitter": "HaiLong Liu", "authors": "Hailong Liu, Takatsugu Hirayama, Masaya Watanabe", "title": "Importance of Instruction for Pedestrian-Automated Driving Vehicle\n  Interaction with an External Human Machine Interface: Effects on Pedestrians'\n  Situation Awareness, Trust, Perceived Risks and Decision Making", "comments": "5 figures, Accepted by IEEE IV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to a manual driving vehicle (MV), an automated driving vehicle lacks\na way to communicate with the pedestrian through the driver when it interacts\nwith the pedestrian because the driver usually does not participate in driving\ntasks. Thus, an external human machine interface (eHMI) can be viewed as a\nnovel explicit communication method for providing driving intentions of an\nautomated driving vehicle (AV) to pedestrians when they need to negotiate in an\ninteraction, e.g., an encountering scene. However, the eHMI may not guarantee\nthat the pedestrians will fully recognize the intention of the AV. In this\npaper, we propose that the instruction of the eHMI's rationale can help\npedestrians correctly understand the driving intentions and predict the\nbehavior of the AV, and thus their subjective feelings (i.e., dangerous\nfeeling, trust in the AV, and feeling of relief) and decision-making are also\nimproved. The results of an interaction experiment in a road-crossing scene\nindicate that the participants were more difficult to be aware of the situation\nwhen they encountered an AV w/o eHMI compared to when they encountered an MV;\nfurther, the participants' subjective feelings and hesitation in\ndecision-making also deteriorated significantly. When the eHMI was used in the\nAV, the situational awareness, subjective feelings and decision-making of the\nparticipants regarding the AV w/ eHMI were improved. After the instruction, it\nwas easier for the participants to understand the driving intention and predict\ndriving behavior of the AV w/ eHMI. Further, the subjective feelings and the\nhesitation related to decision-making were improved and reached the same\nstandards as that for the MV.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 04:58:35 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 02:31:25 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 10:34:04 GMT"}, {"version": "v4", "created": "Mon, 31 May 2021 00:50:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liu", "Hailong", ""], ["Hirayama", "Takatsugu", ""], ["Watanabe", "Masaya", ""]]}, {"id": "2102.08235", "submitter": "Fannie Liu", "authors": "Fannie Liu, Chunjong Park, Yu Jiang Tham, Tsung-Yu Tsai, Laura\n  Dabbish, Geoff Kaufman and Andr\\'es Monroy-Hern\\'andez", "title": "Significant Otter: Understanding the Role of Biosignals in Communication", "comments": "CHI Conference on Human Factors in Computing Systems (CHI '21), May\n  8--13, 2021, Yokohama, Japan", "journal-ref": null, "doi": "10.1145/3411764.3445200", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the growing ubiquity of wearable devices, sensed physiological responses\nprovide new means to connect with others. While recent research demonstrates\nthe expressive potential for biosignals, the value of sharing these personal\ndata remains unclear. To understand their role in communication, we created\nSignificant Otter, an Apple Watch/iPhone app that enables romantic partners to\nshare and respond to each other's biosignals in the form of animated otter\navatars. In a one-month study with 20 couples, participants used Significant\nOtter with biosignals sensing OFF and ON. We found that while sensing OFF\nenabled couples to keep in touch, sensing ON enabled easier and more authentic\ncommunication that fostered social connection. However, the addition of\nbiosignals introduced concerns about autonomy and agency over the messages they\nsent. We discuss design implications and future directions for communication\nsystems that recommend messages based on biosignals.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 15:48:53 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 20:43:07 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Liu", "Fannie", ""], ["Park", "Chunjong", ""], ["Tham", "Yu Jiang", ""], ["Tsai", "Tsung-Yu", ""], ["Dabbish", "Laura", ""], ["Kaufman", "Geoff", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""]]}, {"id": "2102.08273", "submitter": "Mahin Ramezani", "authors": "Mohammad Karim, Mahin Ramezani, Tenaya Sunbury, Robert Ohsfeldt,\n  Hye-Chung Kum", "title": "VIEW: a framework for organization level interactive record linkage to\n  support reproducible data science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective: To design and evaluate a general framework for interactive record\nlinkage using a convenient algorithm combined with tractable Human Intelligent\nTasks (HITs; i.e. micro tasks requiring human judgment) that can support\nreproducible data science. Materials and Methods: Accurate linkage of real data\nrequires both automatic processing of well-defined tasks and human processing\nof tasks that require human judgment (i.e., HITs) on messy data. We present a\nreproducible, interactive, and iterative framework for record linkage called\nVIEW (Visual Interactive Entity-resolution Workbench). We implemented and\nevaluated VIEW by integrating two commonly used hospital databases, the\nAmerican Hospital Association (AHA) Annual Survey of Hospitals and the Medicare\nCost Reports for Hospitals from CMS. Results: Using VIEW to iteratively\nstandardize and clean the data, we linked all Texas hospitals common in both\ndatabases with 100% precision by confirming 78 approximate linkages using HITs\nand manually linking 28 hospitals using HITs. Discussion: Similarities in\nhospital names and addresses and the dynamic nature of hospital attributes over\ntime make it impossible to build a fully automated linkage system for hospitals\nthat can be maintained over time. VIEW is a software that supports a\nreproducible semi-automated process that can generate and track HITs to be\nreviewed and linked manually for messy data elements such as hospitals that\nhave been merged. Conclusion: Effective software that can support the\ninteractive and iterative process of record linkage, and well-designed HITs can\nstreamline the linkage processes to support high quality replicable research\nusing messy real data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 16:52:31 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Karim", "Mohammad", ""], ["Ramezani", "Mahin", ""], ["Sunbury", "Tenaya", ""], ["Ohsfeldt", "Robert", ""], ["Kum", "Hye-Chung", ""]]}, {"id": "2102.08445", "submitter": "Nancy Xin Ru Wang", "authors": "Nancy Xin Ru Wang, Douglas Burdick, Yunyao Li", "title": "TableLab: An Interactive Table Extraction System with Adaptive Deep\n  Learning", "comments": "Accepted at IUI'21", "journal-ref": "26th International Conference on Intelligent User Interfaces 2021", "doi": "10.1145/3397482.3450718", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Table extraction from PDF and image documents is a ubiquitous task in the\nreal-world. Perfect extraction quality is difficult to achieve with one single\nout-of-box model due to (1) the wide variety of table styles, (2) the lack of\ntraining data representing this variety and (3) the inherent ambiguity and\nsubjectivity of table definitions between end-users. Meanwhile, building\ncustomized models from scratch can be difficult due to the expensive nature of\nannotating table data. We attempt to solve these challenges with TableLab by\nproviding a system where users and models seamlessly work together to quickly\ncustomize high-quality extraction models with a few labelled examples for the\nuser's document collection, which contains pages with tables. Given an input\ndocument collection, TableLab first detects tables with similar structures\n(templates) by clustering embeddings from the extraction model. Document\ncollections often contain tables created with a limited set of templates or\nsimilar structures. It then selects a few representative table examples already\nextracted with a pre-trained base deep learning model. Via an easy-to-use user\ninterface, users provide feedback to these selections without necessarily\nhaving to identify every single error. TableLab then applies such feedback to\nfinetune the pre-trained model and returns the results of the finetuned model\nback to the user. The user can choose to repeat this process iteratively until\nobtaining a customized model with satisfactory performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 20:52:44 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Wang", "Nancy Xin Ru", ""], ["Burdick", "Douglas", ""], ["Li", "Yunyao", ""]]}, {"id": "2102.08453", "submitter": "Boris Ruf", "authors": "Boris Ruf and Marcin Detyniecki", "title": "Towards the Right Kind of Fairness in AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness is a concept of justice. Various definitions exist, some of them\nconflicting with each other. In the absence of an uniformly accepted notion of\nfairness, choosing the right kind for a specific situation has always been a\ncentral issue in human history. When it comes to implementing sustainable\nfairness in artificial intelligence systems, this old question plays a key role\nonce again: How to identify the most appropriate fairness metric for a\nparticular application? The answer is often a matter of context, and the best\nchoice depends on ethical standards and legal requirements. Since ethics\nguidelines on this topic are kept rather general for now, we aim to provide\nmore hands-on guidance with this document. Therefore, we first structure the\ncomplex landscape of existing fairness metrics and explain the different\noptions by example. Furthermore, we propose the \"Fairness Compass\", a tool\nwhich formalises the selection process and makes identifying the most\nappropriate fairness definition for a given system a simple, straightforward\nprocedure. Because this process also allows to document the reasoning behind\nthe respective decisions, we argue that this approach can help to build trust\nfrom the user through explaining and justifying the implemented fairness.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 21:12:30 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 10:21:55 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 14:09:44 GMT"}, {"version": "v4", "created": "Thu, 20 May 2021 10:06:32 GMT"}, {"version": "v5", "created": "Wed, 30 Jun 2021 20:15:22 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ruf", "Boris", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "2102.08507", "submitter": "Vaibhav Vasant Unhelkar", "authors": "Sangwon Seo, Lauren R. Kennedy-Metz, Marco A. Zenati, Julie A. Shah,\n  Roger D. Dias, Vaibhav V. Unhelkar", "title": "Towards an AI Coach to Infer Team Mental Model Alignment in Healthcare", "comments": "Submitted to the 2021 IEEE Conference on Cognitive and Computational\n  Aspects of Situation Management (CogSIMA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared mental models are critical to team success; however, in practice, team\nmembers may have misaligned models due to a variety of factors. In\nsafety-critical domains (e.g., aviation, healthcare), lack of shared mental\nmodels can lead to preventable errors and harm. Towards the goal of mitigating\nsuch preventable errors, here, we present a Bayesian approach to infer\nmisalignment in team members' mental models during complex healthcare task\nexecution. As an exemplary application, we demonstrate our approach using two\nsimulated team-based scenarios, derived from actual teamwork in cardiac\nsurgery. In these simulated experiments, our approach inferred model\nmisalignment with over 75% recall, thereby providing a building block for\nenabling computer-assisted interventions to augment human cognition in the\noperating room and improve teamwork.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 00:14:08 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Seo", "Sangwon", ""], ["Kennedy-Metz", "Lauren R.", ""], ["Zenati", "Marco A.", ""], ["Shah", "Julie A.", ""], ["Dias", "Roger D.", ""], ["Unhelkar", "Vaibhav V.", ""]]}, {"id": "2102.08512", "submitter": "Alyssa Donawa", "authors": "Alyssa Donawa, Corey E. Baker", "title": "Addressing the Need for Remote Patient Monitoring Applications in\n  Appalachian Areas", "comments": "Short Paper: 2 pages (not including ref), 0 figures. Accepted and\n  presented at the 2020 American Medical Informatics Association (AMIA) -\n  Workshop on Interactive Systems in Healthcare (WISH)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a need to address the urban-rural disparities in healthcare\nregarding equal access and quality of care. Due to higher rates of chronic\ndisease, reduced access to providers, and a continuous decline in rural\nhospitals, it is imperative that Appalachian cancer patients adopt the use of\nhealth information technology (HIT). The NCCN Distress Thermometer and Problem\nList (DT) is under-utilized, not patient-centered, does not consider provider\nneeds, and is outdated in the current digital landscape. Digitizing patient\ndistress screening poses advantages, such as allowing for more frequent\nscreenings, removing geographical barriers, and rural patient autonomy. In this\npaper, we discuss how knowledge gained from patient-centered design led to the\nunderpinnings of developing a rural remote patient monitoring app that provides\ndelightful and insightful experiences to users.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 00:35:00 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Donawa", "Alyssa", ""], ["Baker", "Corey E.", ""]]}, {"id": "2102.08540", "submitter": "Harini Suresh", "authors": "Harini Suresh, Kathleen M. Lewis, John V. Guttag, Arvind Satyanarayan", "title": "Intuitively Assessing ML Model Reliability through Example-Based\n  Explanations and Editing Model Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability methods aim to help users build trust in and understand the\ncapabilities of machine learning models. However, existing approaches often\nrely on abstract, complex visualizations that poorly map to the task at hand or\nrequire non-trivial ML expertise to interpret. Here, we present two visual\nanalytics modules that facilitate an intuitive assessment of model reliability.\nTo help users better characterize and reason about a model's uncertainty, we\nvisualize raw and aggregate information about a given input's nearest\nneighbors. Using an interactive editor, users can manipulate this input in\nsemantically-meaningful ways, determine the effect on the output, and compare\nagainst their prior expectations. We evaluate our interface using an\nelectrocardiogram beat classification case study. Compared to a baseline\nfeature importance interface, we find that 14 physicians are better able to\nalign the model's uncertainty with domain-relevant factors and build intuition\nabout its capabilities and limitations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 02:41:32 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 16:07:14 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Suresh", "Harini", ""], ["Lewis", "Kathleen M.", ""], ["Guttag", "John V.", ""], ["Satyanarayan", "Arvind", ""]]}, {"id": "2102.08542", "submitter": "Nagashri Lakshminarayana", "authors": "Nagashri Lakshminarayana, Yifang Liu, Karthik Dantu, Venu Govindaraju,\n  Nils Napp", "title": "Active Face Frontalization using Commodity Unmanned Aerial Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a system by which Unmanned Aerial Vehicles (UAVs) can\ngather high-quality face images that can be used in biometric identification\ntasks. Success in face-based identification depends in large part on the image\nquality, and a major factor is how frontal the view is. Face recognition\nsoftware pipelines can improve identification rates by synthesizing frontal\nviews from non-frontal views by a process call {\\em frontalization}. Here we\nexploit the high mobility of UAVs to actively gather frontal images using\ncomponents of a synthetic frontalization pipeline. We define a frontalization\nerror and show that it can be used to guide an UAVs to capture frontal views.\nFurther, we show that the resulting image stream improves matching quality of a\ntypical face recognition similarity metric. The system is implemented using an\noff-the-shelf hardware and software components and can be easily transfered to\nany ROS enabled UAVs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 02:55:57 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lakshminarayana", "Nagashri", ""], ["Liu", "Yifang", ""], ["Dantu", "Karthik", ""], ["Govindaraju", "Venu", ""], ["Napp", "Nils", ""]]}, {"id": "2102.08555", "submitter": "Corey Lammie", "authors": "Corey Lammie, Wei Xiang, Mostafa Rahimi Azghadi", "title": "Towards Memristive Deep Learning Systems for Real-time Mobile Epileptic\n  Seizure Prediction", "comments": "Accepted at 2021 IEEE International Symposium on Circuits and Systems\n  (ISCAS)", "journal-ref": "2021 IEEE International Symposium on Circuits and Systems (ISCAS)", "doi": null, "report-no": null, "categories": "cs.ET cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unpredictability of seizures continues to distress many people with\ndrug-resistant epilepsy. On account of recent technological advances,\nconsiderable efforts have been made using different hardware technologies to\nrealize smart devices for the real-time detection and prediction of seizures.\nIn this paper, we investigate the feasibility of using Memristive Deep Learning\nSystems (MDLSs) to perform real-time epileptic seizure prediction on the edge.\nUsing the MemTorch simulation framework and the Children's Hospital Boston\n(CHB)-Massachusetts Institute of Technology (MIT) dataset we determine the\nperformance of various simulated MDLS configurations. An average sensitivity of\n77.4% and a Area Under the Receiver Operating Characteristic Curve (AUROC) of\n0.85 are reported for the optimal configuration that can process\nElectroencephalogram (EEG) spectrograms with 7,680 samples in 1.408ms while\nconsuming 0.0133W and occupying an area of 0.1269mm$^2$ in a 65nm Complementary\nMetal-Oxide-Semiconductor (CMOS) process.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 03:49:25 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lammie", "Corey", ""], ["Xiang", "Wei", ""], ["Azghadi", "Mostafa Rahimi", ""]]}, {"id": "2102.08770", "submitter": "Adegboyega Akinsiku", "authors": "Adegboyega Akinsiku, Ignacio Avellino, Yasmin Graham, Helena M. Mentis", "title": "It's Not Just the Movement: Experiential Information Needed for Stroke\n  Telerehabilitation", "comments": "11 pages", "journal-ref": null, "doi": "10.1145/3411764.3445663", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Telerehabilitation systems for stroke survivors have been predominantly\ndesigned to measure and quantify movement in order to guide and encourage\nrehabilitation regular exercises at home. We set out to study what aspect of\nthe movement data was essential, to better inform sensor design. We\ninvestigated face-to-face stroke rehabilitation sessions through a series of\ninterviews and observations involving 16 stroke rehabilitation specialists\nincluding physiatrists, physical therapists, and occupational therapists. We\nfound that specialists are not solely interested in movement data, and that\nexperiential information about stroke survivors' lived experience plays an\nessential role in specialists interpreting movement data and creating a\nrehabilitation plan. We argue for a reconceptualization in stroke\ntelerehabilitation that is more inclusive of non-movement data, and present\ndesign implications to better account for experiential information in\ntelerehabilitation systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 14:04:54 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Akinsiku", "Adegboyega", ""], ["Avellino", "Ignacio", ""], ["Graham", "Yasmin", ""], ["Mentis", "Helena M.", ""]]}, {"id": "2102.08873", "submitter": "Aang Subiyakto", "authors": "Aang Subiyakto, Noni Erlina, Yuni Sugiarti, Nashrul Hakiem, Moh.\n  Irfan, and Abd. Rahman Ahlan", "title": "Assessing Mobile Learning System Performance in Indonesia: Reports of\n  the Model Development and Its Instrument Testing", "comments": "7 pages, 3 figures, SMIC2020", "journal-ref": null, "doi": "10.1063/5.0041678", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is undeniable that people life patterns and technological developments are\ninterrelated within a supply and demand cycle. In the education world, the\nemergence of the internet and mobile technologies has opened the learning\nboundaries through the use of mobile learning (m-learning). In Indonesia, the\nlearning service industry has been begun to enliven the outside school\neducation sector for almost five years ago. Even though the learning has been\ndiscussed around a decade ago, however, it is still rare studies that discuss\nthe performance of the m-learning system based on the end-user perceptions in\nparticular. Therefore, the study may still indispensable, especially from the\nperspectives of a developing nation. This paper elucidates the preliminary\nstage results of the above-mentioned study, including the results of the model\ndevelopment and its instrument testing. The DeLone and Mclean information\nsystem (IS) success model was adopted, combined with the individual motivation\nand organizational culture theories, and then adapted into the processional and\ncausal logic of the success model. Around 50 respondent data were collected\nonline and processed and analyzed based on the outer model assessments of the\nPLS-SEM method using SmartPLS 3.0 to know the reliability and validity of each\nindicator. The result shows that two of 31 are rejected indicators. The\nrejections may be the revision considerations for the next study stages.\nAlthough this may be trivial for experts, the clarity of its methodological\nexplanations may guide the novice researchers, how to develop a research model\nand its instrument testing.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 17:08:17 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Subiyakto", "Aang", ""], ["Erlina", "Noni", ""], ["Sugiarti", "Yuni", ""], ["Hakiem", "Nashrul", ""], ["Irfan", "Moh.", ""], ["Ahlan", "Abd. Rahman", ""]]}, {"id": "2102.08892", "submitter": "Rudolf Rosa", "authors": "Rudolf Rosa and Tom\\'a\\v{s} Musil and Ond\\v{r}ej Du\\v{s}ek and Dominik\n  Jurko and Patr\\'icia Schmidtov\\'a and David Mare\\v{c}ek and Ond\\v{r}ej Bojar\n  and Tom Kocmi and Daniel Hrbek and David Ko\\v{s}\\v{t}\\'ak and Martina\n  Kinsk\\'a and Marie Nov\\'akov\\'a and Josef Dole\\v{z}al and Kl\\'ara Voseck\\'a\n  and Tom\\'a\\v{s} Studen\\'ik and Petr \\v{Z}abka", "title": "THEaiTRE 1.0: Interactive generation of theatre play scripts", "comments": "Submitted to Text2Story workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the first version of a system for interactive generation of\ntheatre play scripts. The system is based on a vanilla GPT-2 model with several\nadjustments, targeting specific issues we encountered in practice. We also list\nother issues we encountered but plan to only solve in a future version of the\nsystem. The presented system was used to generate a theatre play script planned\nfor premiere in February 2021.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 17:40:33 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Rosa", "Rudolf", ""], ["Musil", "Tom\u00e1\u0161", ""], ["Du\u0161ek", "Ond\u0159ej", ""], ["Jurko", "Dominik", ""], ["Schmidtov\u00e1", "Patr\u00edcia", ""], ["Mare\u010dek", "David", ""], ["Bojar", "Ond\u0159ej", ""], ["Kocmi", "Tom", ""], ["Hrbek", "Daniel", ""], ["Ko\u0161\u0165\u00e1k", "David", ""], ["Kinsk\u00e1", "Martina", ""], ["Nov\u00e1kov\u00e1", "Marie", ""], ["Dole\u017eal", "Josef", ""], ["Voseck\u00e1", "Kl\u00e1ra", ""], ["Studen\u00edk", "Tom\u00e1\u0161", ""], ["\u017dabka", "Petr", ""]]}, {"id": "2102.08909", "submitter": "Colin M. Gray", "authors": "Shruthi Sai Chivukula, Ziqing Li, Anne C. Pivonka, Jingning Chen,\n  Colin M. Gray", "title": "Surveying the Landscape of Ethics-Focused Design Methods", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Over the past decade, HCI researchers, design researchers, and practitioners\nhave increasingly addressed ethics-focused issues through a range of\ntheoretical, methodological and pragmatic contributions to the field. While\nmany forms of design knowledge have been proposed and described, we focus\nexplicitly on knowledge that has been codified as \"methods,\" which we define as\nany supports for everyday work practices of designers. In this paper, we\nidentify, analyze, and map a collection of 63 existing ethics-focused methods\nintentionally designed for ethical impact. We present a content analysis,\nproviding a descriptive record of how they operationalize ethics, their\nintended audience or context of use, their \"core\" or \"script,\" and the means by\nwhich these methods are formulated, articulated, and languaged. Building on\nthese results, we provide an initial definition of ethics-focused methods,\nidentifying potential opportunities for the development of future methods to\nsupport design practice and research.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 18:03:43 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Chivukula", "Shruthi Sai", ""], ["Li", "Ziqing", ""], ["Pivonka", "Anne C.", ""], ["Chen", "Jingning", ""], ["Gray", "Colin M.", ""]]}, {"id": "2102.08976", "submitter": "Ozan \\\"Ozdenizci", "authors": "Ozan Ozdenizci, Safaa Eldeeb, Andac Demir, Deniz Erdogmus, Murat\n  Akcakaya", "title": "EEG-based Texture Roughness Classification in Active Tactile Exploration\n  with Invariant Representation Learning Networks", "comments": "Accepted for publication at Biomedical Signal Processing and Control", "journal-ref": null, "doi": "10.1016/j.bspc.2021.102507", "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  During daily activities, humans use their hands to grasp surrounding objects\nand perceive sensory information which are also employed for perceptual and\nmotor goals. Multiple cortical brain regions are known to be responsible for\nsensory recognition, perception and motor execution during sensorimotor\nprocessing. While various research studies particularly focus on the domain of\nhuman sensorimotor control, the relation and processing between motor execution\nand sensory processing is not yet fully understood. Main goal of our work is to\ndiscriminate textured surfaces varying in their roughness levels during active\ntactile exploration using simultaneously recorded electroencephalogram (EEG)\ndata, while minimizing the variance of distinct motor exploration movement\npatterns. We perform an experimental study with eight healthy participants who\nwere instructed to use the tip of their dominant hand index finger while\nrubbing or tapping three different textured surfaces with varying levels of\nroughness. We use an adversarial invariant representation learning neural\nnetwork architecture that performs EEG-based classification of different\ntextured surfaces, while simultaneously minimizing the discriminability of\nmotor movement conditions (i.e., rub or tap). Results show that the proposed\napproach can discriminate between three different textured surfaces with\naccuracies up to 70%, while suppressing movement related variability from\nlearned representations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 19:07:13 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 18:08:14 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ozdenizci", "Ozan", ""], ["Eldeeb", "Safaa", ""], ["Demir", "Andac", ""], ["Erdogmus", "Deniz", ""], ["Akcakaya", "Murat", ""]]}, {"id": "2102.09039", "submitter": "Yang Li", "authors": "Mingyuan Zhong, Gang Li, Yang Li", "title": "Spacewalker: Rapid UI Design Exploration Using Lightweight Markup\n  Enhancement and Crowd Genetic Programming", "comments": "10 pages, CHI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User interface design is a complex task that involves designers examining a\nwide range of options. We present Spacewalker, a tool that allows designers to\nrapidly search a large design space for an optimal web UI with integrated\nsupport. Designers first annotate each attribute they want to explore in a\ntypical HTML page, using a simple markup extension we designed. Spacewalker\nthen parses the annotated HTML specification, and intelligently generates and\ndistributes various configurations of the web UI to crowd workers for\nevaluation. We enhanced a genetic algorithm to accommodate crowd worker\nresponses from pairwise comparison of UI designs, which is crucial for\nobtaining reliable feedback. Based on our experiments, Spacewalker allows\ndesigners to effectively search a large design space of a UI, using the\nlanguage they are familiar with, and improve their design rapidly at a minimal\ncost.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 21:54:49 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Zhong", "Mingyuan", ""], ["Li", "Gang", ""], ["Li", "Yang", ""]]}, {"id": "2102.09087", "submitter": "Yang Li", "authors": "Michael Xuelin Huang, Yang Li, Nazneen Nazneen, Alexander Chao, Shumin\n  Zhai", "title": "TapNet: The Design, Training, Implementation, and Applications of a\n  Multi-Task Learning CNN for Off-Screen Mobile Input", "comments": "10 pages. CHI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make off-screen interaction without specialized hardware practical, we\ninvestigate using deep learning methods to process the common built-in IMU\nsensor (accelerometers and gyroscopes) on mobile phones into a useful set of\none-handed interaction events. We present the design, training, implementation\nand applications of TapNet, a multi-task network that detects tapping on the\nsmartphone. With phone form factor as auxiliary information, TapNet can jointly\nlearn from data across devices and simultaneously recognize multiple tap\nproperties, including tap direction and tap location. We developed two datasets\nconsisting of over 135K training samples, 38K testing samples, and 32\nparticipants in total. Experimental evaluation demonstrated the effectiveness\nof the TapNet design and its significant improvement over the state of the art.\nAlong with the datasets,\n(https://sites.google.com/site/michaelxlhuang/datasets/tapnet-dataset), and\nextensive experiments, TapNet establishes a new technical foundation for\noff-screen mobile input.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 00:45:41 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Huang", "Michael Xuelin", ""], ["Li", "Yang", ""], ["Nazneen", "Nazneen", ""], ["Chao", "Alexander", ""], ["Zhai", "Shumin", ""]]}, {"id": "2102.09150", "submitter": "Decky Aspandi", "authors": "Decky Aspandi, Federico Sukno, Bj\\\"orn Schuller and Xavier Binefa", "title": "An Enhanced Adversarial Network with Combined Latent Features for\n  Spatio-Temporal Facial Affect Estimation in the Wild", "comments": "Accepted Version on VISAPP 2021", "journal-ref": null, "doi": "10.5220/0010332001720181", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective Computing has recently attracted the attention of the research\ncommunity, due to its numerous applications in diverse areas. In this context,\nthe emergence of video-based data allows to enrich the widely used spatial\nfeatures with the inclusion of temporal information. However, such\nspatio-temporal modelling often results in very high-dimensional feature spaces\nand large volumes of data, making training difficult and time consuming. This\npaper addresses these shortcomings by proposing a novel model that efficiently\nextracts both spatial and temporal features of the data by means of its\nenhanced temporal modelling based on latent features. Our proposed model\nconsists of three major networks, coined Generator, Discriminator, and\nCombiner, which are trained in an adversarial setting combined with curriculum\nlearning to enable our adaptive attention modules. In our experiments, we show\nthe effectiveness of our approach by reporting our competitive results on both\nthe AFEW-VA and SEWA datasets, suggesting that temporal modelling improves the\naffect estimates both in qualitative and quantitative terms. Furthermore, we\nfind that the inclusion of attention mechanisms leads to the highest accuracy\nimprovements, as its weights seem to correlate well with the appearance of\nfacial movements, both in terms of temporal localisation and intensity.\nFinally, we observe the sequence length of around 160\\,ms to be the optimum one\nfor temporal modelling, which is consistent with other relevant findings\nutilising similar lengths.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 04:10:12 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Aspandi", "Decky", ""], ["Sukno", "Federico", ""], ["Schuller", "Bj\u00f6rn", ""], ["Binefa", "Xavier", ""]]}, {"id": "2102.09171", "submitter": "Minghong Fang", "authors": "Minghong Fang, Minghao Sun, Qi Li, Neil Zhenqiang Gong, Jin Tian, Jia\n  Liu", "title": "Data Poisoning Attacks and Defenses to Crowdsourcing Systems", "comments": "To appear in the Web Conference 2021 (WWW '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge of big data analytics is how to collect a large volume of\n(labeled) data. Crowdsourcing aims to address this challenge via aggregating\nand estimating high-quality data (e.g., sentiment label for text) from\npervasive clients/users. Existing studies on crowdsourcing focus on designing\nnew methods to improve the aggregated data quality from unreliable/noisy\nclients. However, the security aspects of such crowdsourcing systems remain\nunder-explored to date. We aim to bridge this gap in this work. Specifically,\nwe show that crowdsourcing is vulnerable to data poisoning attacks, in which\nmalicious clients provide carefully crafted data to corrupt the aggregated\ndata. We formulate our proposed data poisoning attacks as an optimization\nproblem that maximizes the error of the aggregated data. Our evaluation results\non one synthetic and two real-world benchmark datasets demonstrate that the\nproposed attacks can substantially increase the estimation errors of the\naggregated data. We also propose two defenses to reduce the impact of malicious\nclients. Our empirical results show that the proposed defenses can\nsubstantially reduce the estimation errors of the data poisoning attacks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 06:03:48 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 23:10:31 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Fang", "Minghong", ""], ["Sun", "Minghao", ""], ["Li", "Qi", ""], ["Gong", "Neil Zhenqiang", ""], ["Tian", "Jin", ""], ["Liu", "Jia", ""]]}, {"id": "2102.09258", "submitter": "Marta Gomez-Barrero", "authors": "Marta Gomez-Barrero, Pawel Drozdowski, Christian Rathgeb, Jose Patino,\n  Massimmiliano Todisco, Andras Nautsch, Naser Damer, Jannis Priesnitz,\n  Nicholas Evans, Christoph Busch", "title": "Biometrics in the Era of COVID-19: Challenges and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Since early 2020 the COVID-19 pandemic has had a considerable impact on many\naspects of daily life. A range of different measures have been implemented\nworldwide to reduce the rate of new infections and to manage the pressure on\nnational health services. A primary strategy has been to reduce gatherings and\nthe potential for transmission through the prioritisation of remote working and\neducation. Enhanced hand hygiene and the use of facial masks have decreased the\nspread of pathogens when gatherings are unavoidable. These particular measures\npresent challenges for reliable biometric recognition, e.g. for facial-, voice-\nand hand-based biometrics. At the same time, new challenges create new\nopportunities and research directions, e.g. renewed interest in non-constrained\niris or periocular recognition, touch-less fingerprint- and vein-based\nauthentication and the use of biometric characteristics for disease detection.\nThis article presents an overview of the research carried out to address those\nchallenges and emerging opportunities.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 10:32:59 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Gomez-Barrero", "Marta", ""], ["Drozdowski", "Pawel", ""], ["Rathgeb", "Christian", ""], ["Patino", "Jose", ""], ["Todisco", "Massimmiliano", ""], ["Nautsch", "Andras", ""], ["Damer", "Naser", ""], ["Priesnitz", "Jannis", ""], ["Evans", "Nicholas", ""], ["Busch", "Christoph", ""]]}, {"id": "2102.09370", "submitter": "Lara Gauder", "authors": "Lara Gauder, Leonardo Pepino, Pablo Riera, Silvina Brussino, Jazm\\'in\n  Vidal, Agust\\'in Gravano, Luciana Ferrer", "title": "A Study on the Manifestation of Trust in Speech", "comments": "arXiv admin note: text overlap with arXiv:2007.15711,\n  arXiv:2006.05977", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Research has shown that trust is an essential aspect of human-computer\ninteraction directly determining the degree to which the person is willing to\nuse a system. An automatic prediction of the level of trust that a user has on\na certain system could be used to attempt to correct potential distrust by\nhaving the system take relevant actions like, for example, apologizing or\nexplaining its decisions. In this work, we explore the feasibility of\nautomatically detecting the level of trust that a user has on a virtual\nassistant (VA) based on their speech. We developed a novel protocol for\ncollecting speech data from subjects induced to have different degrees of trust\nin the skills of a VA. The protocol consists of an interactive session where\nthe subject is asked to respond to a series of factual questions with the help\nof a virtual assistant. In order to induce subjects to either trust or distrust\nthe VA's skills, they are first informed that the VA was previously rated by\nother users as being either good or bad; subsequently, the VA answers the\nsubjects' questions consistently to its alleged abilities. All interactions are\nspeech-based, with subjects and VAs communicating verbally, which allows the\nrecording of speech produced under different trust conditions. Using this\nprotocol, we collected a speech corpus in Argentine Spanish. We show clear\nevidence that the protocol effectively succeeded in influencing subjects into\nthe desired mental state of either trusting or distrusting the agent's skills,\nand present results of a perceptual study of the degree of trust performed by\nexpert listeners. Finally, we found that the subject's speech can be used to\ndetect which type of VA they were using, which could be considered a proxy for\nthe user's trust toward the VA's abilities, with an accuracy up to 76%,\ncompared to a random baseline of 50%.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 13:08:54 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Gauder", "Lara", ""], ["Pepino", "Leonardo", ""], ["Riera", "Pablo", ""], ["Brussino", "Silvina", ""], ["Vidal", "Jazm\u00edn", ""], ["Gravano", "Agust\u00edn", ""], ["Ferrer", "Luciana", ""]]}, {"id": "2102.09453", "submitter": "Joaquim Jorge", "authors": "Bruno R. Martins, Joaquim A. Jorge and Ezequiel R. Zorzal", "title": "Towards augmented reality for corporate training", "comments": "This paper is published in the Journal of Interactive Learning\n  Environments (Routledge) 2021", "journal-ref": "Interactive Learning Environments 0 (2021) 1-19", "doi": "10.1080/10494820.2021.1879872", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Corporate training relates to employees acquiring essential skills to operate\nequipment or effectively performing required tasks both competently and safely.\nUnlike formal education, training can be incorporated into the task workflow\nand performed during working hours. Increasingly, organizations adopt different\ntechnologies to develop both individual skills and improve their organization.\nStudies indicate that Augmented Reality (AR) is quickly becoming an effective\ntechnology for training programs. This systematic literature review (SLR) aims\nto screen works published on AR for corporate training. We describe AR training\napplications, discuss current challenges, literature gaps, opportunities, and\ntendencies of corporate AR solutions. We structured a protocol to define\nkeywords, the semantics of research, and databases used as sources of this SLR.\nFrom a primary analysis, we considered 1952 articles in the review for\nqualitative synthesis. We selected 60 among the selected articles for this\nstudy. The survey shows a large number of 41.7% of applications focused on\nautomotive and medical training. Additionally, 20% of selected publications use\na camera-display with a tablet device, while 40% refer to\nhead-mounted-displays, and many surveyed approaches (45%) adopt marker-based\ntracking. Results indicate that publications on AR for corporate training\nincreased significantly in recent years. AR has been used in many areas,\nexhibiting high quality and provides viable approaches to On-The-Job training.\nFinally, we discuss future research issues related to increasing relevance\nregarding AR for corporate training.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 16:19:27 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Martins", "Bruno R.", ""], ["Jorge", "Joaquim A.", ""], ["Zorzal", "Ezequiel R.", ""]]}, {"id": "2102.09692", "submitter": "Zana Bu\\c{c}inca", "authors": "Zana Bu\\c{c}inca, Maja Barbara Malaya, Krzysztof Z. Gajos", "title": "To Trust or to Think: Cognitive Forcing Functions Can Reduce\n  Overreliance on AI in AI-assisted Decision-making", "comments": null, "journal-ref": null, "doi": "10.1145/3449287", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  People supported by AI-powered decision support tools frequently overrely on\nthe AI: they accept an AI's suggestion even when that suggestion is wrong.\nAdding explanations to the AI decisions does not appear to reduce the\noverreliance and some studies suggest that it might even increase it. Informed\nby the dual-process theory of cognition, we posit that people rarely engage\nanalytically with each individual AI recommendation and explanation, and\ninstead develop general heuristics about whether and when to follow the AI\nsuggestions. Building on prior research on medical decision-making, we designed\nthree cognitive forcing interventions to compel people to engage more\nthoughtfully with the AI-generated explanations. We conducted an experiment\n(N=199), in which we compared our three cognitive forcing designs to two simple\nexplainable AI approaches and to a no-AI baseline. The results demonstrate that\ncognitive forcing significantly reduced overreliance compared to the simple\nexplainable AI approaches. However, there was a trade-off: people assigned the\nleast favorable subjective ratings to the designs that reduced the overreliance\nthe most. To audit our work for intervention-generated inequalities, we\ninvestigated whether our interventions benefited equally people with different\nlevels of Need for Cognition (i.e., motivation to engage in effortful mental\nactivities). Our results show that, on average, cognitive forcing interventions\nbenefited participants higher in Need for Cognition more. Our research suggests\nthat human cognitive motivation moderates the effectiveness of explainable AI\nsolutions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 00:38:53 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Bu\u00e7inca", "Zana", ""], ["Malaya", "Maja Barbara", ""], ["Gajos", "Krzysztof Z.", ""]]}, {"id": "2102.09761", "submitter": "Tom Hope", "authors": "Tom Hope, Ronen Tamari, Hyeonsu Kang, Daniel Hershcovich, Joel Chan,\n  Aniket Kittur, Dafna Shahaf", "title": "Scaling Creative Inspiration with Fine-Grained Functional Facets of\n  Product Ideas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web-scale repositories of products, patents and scientific papers offer an\nopportunity for creating automated systems that scour millions of ideas and\nassist users in discovering inspirations and solutions. Yet the common\nrepresentation of ideas is in the form of raw textual descriptions, lacking\nimportant structure that is required for supporting creative innovation. Prior\nwork has pointed to the importance of functional structure -- capturing the\nmechanisms and purposes of inventions -- for allowing users to discover\nstructural connections across ideas and creatively adapt existing technologies.\nHowever, the use of functional representations was either coarse and limited in\nexpressivity, or dependent on curated knowledge bases with poor coverage and\nsignificant manual effort from users.\n  To help bridge this gap and unlock the potential of large-scale idea mining,\nwe propose a novel computational representation that automatically breaks up\nproducts into fine-grained functional facets. We train a model to extract these\nfacets from a challenging real-world corpus of invention descriptions, and\nrepresent each product as a set of facet embeddings. We design similarity\nmetrics that support granular matching between functional facets across ideas,\nand use them to build a novel functional search capability that enables\nexpressive queries for mechanisms and purposes. We construct a graph capturing\nhierarchical relations between purposes and mechanisms across an entire corpus\nof products, and use the graph to help problem-solvers explore the design space\naround a focal problem and view related problem perspectives. In empirical user\nstudies, our approach leads to a significant boost in search accuracy and in\nthe quality of creative inspirations, outperforming strong baselines and\nstate-of-art representations of product texts by 50-60%.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 06:30:41 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Hope", "Tom", ""], ["Tamari", "Ronen", ""], ["Kang", "Hyeonsu", ""], ["Hershcovich", "Daniel", ""], ["Chan", "Joel", ""], ["Kittur", "Aniket", ""], ["Shahaf", "Dafna", ""]]}, {"id": "2102.09803", "submitter": "Ross Cutler", "authors": "Ross Cutler, Yasaman Hosseinkashi, Jamie Pool, Senja Filipi, Robert\n  Aichner, Yuan Tu, Johannes Gehrke", "title": "Meeting Effectiveness and Inclusiveness in Remote Collaboration", "comments": null, "journal-ref": null, "doi": "10.1145/3449247", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A primary goal of remote collaboration tools is to provide effective and\ninclusive meetings for all participants. To study meeting effectiveness and\nmeeting inclusiveness, we first conducted a large-scale email survey (N=4,425;\nafter filtering N=3,290) at a large technology company (pre-COVID-19); using\nthis data we derived a multivariate model of meeting effectiveness and show how\nit correlates with meeting inclusiveness, participation, and feeling\ncomfortable to contribute. We believe this is the first such model of meeting\neffectiveness and inclusiveness. The large size of the data provided the\nopportunity to analyze correlations that are specific to sub-populations such\nas the impact of video. The model shows the following factors are correlated\nwith inclusiveness, effectiveness, participation, and feeling comfortable to\ncontribute in meetings: sending a pre-meeting communication, sending a\npost-meeting summary, including a meeting agenda, attendee location,\nremote-only meeting, audio/video quality and reliability, video usage, and\nmeeting size. The model and survey results give a quantitative understanding of\nhow and where to improve meeting effectiveness and inclusiveness and what the\npotential returns are.\n  Motivated by the email survey results, we implemented a post-meeting survey\ninto a leading computer-mediated communication (CMC) system to directly measure\nmeeting effectiveness and inclusiveness (during COVID-19). Using initial\nresults based on internal flighting we created a similar model of effectiveness\nand inclusiveness, with many of the same findings as the email survey. This\nshows a method of measuring and understanding these metrics which are both\npractical and useful in a commercial CMC system.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 08:36:06 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Cutler", "Ross", ""], ["Hosseinkashi", "Yasaman", ""], ["Pool", "Jamie", ""], ["Filipi", "Senja", ""], ["Aichner", "Robert", ""], ["Tu", "Yuan", ""], ["Gehrke", "Johannes", ""]]}, {"id": "2102.09847", "submitter": "Juan Gonz\\'alez Salinas", "authors": "Juan Gonz\\'alez, Fernando Boronat, Almanzor Sapena, Javier Pastor", "title": "Key Technologies for Networked Virtual Environments", "comments": "Submitted to Springer Multimedia Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the improvements experienced in technology in the last few years,\nmost especially in virtual reality systems, the number and potential of\nnetworked virtual environments or NVEs and their users are increasing. NVEs aim\nto give distributed users a feeling of immersion in a virtual world and the\npossibility of interacting with other users or with virtual objects inside it,\nlike when they interact in the real world. Being able to provide that feeling\nand natural interactions when the users are geographically separated is one of\nthe goals of these systems. Nevertheless, this goal is especially sensitive to\ndifferent issues, such as different connections with heterogeneous throughput\nor different network latencies, which can lead to consistency and\nsynchronization problems and, thus, to a worsening of the users' quality of\nexperience or QoE. With the purpose of solving these issues, researchers have\nproposed and evaluated numerous technical solutions, in fields like network\narchitectures, data distribution and filtering, resource balancing, computing\nmodels, predictive modeling and synchronization in NVEs. This paper gathers and\nclassifies them, summarizing their advantages and disadvantages, using a new\nway of classification. With the current increase of the number of NVEs and the\nmultiple solutions proposed so far, this work aims to become a useful tool and\na starting point not only for future researchers in this field but also for\nthose who are new in NVEs development, in which guaranteeing a good users' QoE\nis essential.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 10:32:06 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 13:57:04 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gonz\u00e1lez", "Juan", ""], ["Boronat", "Fernando", ""], ["Sapena", "Almanzor", ""], ["Pastor", "Javier", ""]]}, {"id": "2102.09940", "submitter": "Martin Burghart", "authors": "Martin Burghart and Julie L. O'Sullivan and Robert Spang and\n  Jan-Niklas Voigt-Antons", "title": "DemSelf, a Mobile App for Self-Administered Touch-Based Cognitive\n  Screening: Participatory Design With Stakeholders", "comments": "This paper has been accepted for publication in the Human-Computer\n  Interaction International conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of mild cognitive impairment and dementia is vital as many\ntherapeutic interventions are particularly effective at an early stage. A\nself-administered touch-based cognitive screening instrument, called DemSelf,\nwas developed by adapting an examiner-administered paper-based instrument, the\nQuick Mild Cognitive Impairment (Qmci) screen. We conducted five\nsemi-structured expert interviews including a think-aloud phase to evaluate\nusability problems. The extent to which the characteristics of the original\nsubtests change by the adaption, as well as the conditions and appropriate\ncontext for practical application, were also in question. The participants had\nexpertise in the domain of usability and human-machine interaction and/or in\nthe domain of dementia and neuropsychological assessment. Participants\nidentified usability issues in all components of the DemSelf prototype. For\nexample, confirmation of answers was not consistent across subtests. Answers\nwere sometimes logged directly when a button is tapped and cannot be corrected.\nThis can lead to frustration and bias in test results, especially for people\nwith vision or motor impairments. The direct adoption of time limits from the\noriginal paper-based instrument or the simultaneous verbal and textual item\npresentation also caused usability problems. DemSelf is a different test than\nQmci and needs to be re-validated. Visual recognition instead of a free verbal\nrecall is one of the main differences. Reading skill level seems to be an\nimportant confounding variable. Participants would generally prefer if the test\nis conducted in a medical office rather than at a patient's home so that\nsomeone is present for support and the result can be discussed directly.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 14:03:27 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Burghart", "Martin", ""], ["O'Sullivan", "Julie L.", ""], ["Spang", "Robert", ""], ["Voigt-Antons", "Jan-Niklas", ""]]}, {"id": "2102.10387", "submitter": "Nalin Chhibber", "authors": "Nalin Chhibber, Edith Law", "title": "Towards Teachable Conversational Agents", "comments": "9 Pages, 3 Figures, 2 Tables, Presented at NeurIPS 2020: Human in the\n  Loop Dialogue Systems Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The traditional process of building interactive machine learning systems can\nbe viewed as a teacher-learner interaction scenario where the machine-learners\nare trained by one or more human-teachers. In this work, we explore the idea of\nusing a conversational interface to investigate the interaction between\nhuman-teachers and interactive machine-learners. Specifically, we examine\nwhether teachable AI agents can reliably learn from human-teachers through\nconversational interactions, and how this learning compare with traditional\nsupervised learning algorithms. Results validate the concept of teachable\nconversational agents and highlight the factors relevant for the development of\nmachine learning systems that intend to learn from conversational interactions.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 16:56:24 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Chhibber", "Nalin", ""], ["Law", "Edith", ""]]}, {"id": "2102.10435", "submitter": "Shayan Hassantabar", "authors": "Shayan Hassantabar, Joe Zhang, Hongxu Yin, and Niraj K. Jha", "title": "MHDeep: Mental Health Disorder Detection System based on Body-Area and\n  Deep Neural Networks", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mental health problems impact quality of life of millions of people around\nthe world. However, diagnosis of mental health disorders is a challenging\nproblem that often relies on self-reporting by patients about their behavioral\npatterns. Therefore, there is a need for new strategies for diagnosis of mental\nhealth problems. The recent introduction of body-area networks consisting of a\nplethora of accurate sensors embedded in smartwatches and smartphones and deep\nneural networks (DNNs) points towards a possible solution. However, disease\ndiagnosis based on WMSs and DNNs, and their deployment on edge devices, remains\na challenging problem. To this end, we propose a framework called MHDeep that\nutilizes commercially available WMSs and efficient DNN models to diagnose three\nimportant mental health disorders: schizoaffective, major depressive, and\nbipolar. MHDeep uses eight different categories of data obtained from sensors\nintegrated in a smartwatch and smartphone. Due to limited available data,\nMHDeep uses a synthetic data generation module to augment real data with\nsynthetic data drawn from the same probability distribution. We use the\nsynthetic dataset to pre-train the DNN models, thus imposing a prior on the\nweights. We use a grow-and-prune DNN synthesis approach to learn both the\narchitecture and weights during the training process. We use three different\ndata partitions to evaluate the MHDeep models trained with data collected from\n74 individuals. We conduct data instance level and patient level evaluations.\nMHDeep achieves an average test accuracy of 90.4%, 87.3%, and 82.4%,\nrespectively, for classifications between healthy instances and schizoaffective\ndisorder instances, major depressive disorder instances, and bipolar disorder\ninstances. At the patient level, MHDeep DNNs achieve an accuracy of 100%, 100%,\nand 90.0% for the three mental health disorders, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 20:17:07 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Hassantabar", "Shayan", ""], ["Zhang", "Joe", ""], ["Yin", "Hongxu", ""], ["Jha", "Niraj K.", ""]]}, {"id": "2102.10448", "submitter": "Panagiotis Kourtesis", "authors": "Panagiotis Kourtesis, Simona Collina, Leonidas A.A. Doumas, and Sarah\n  E. MacPherson", "title": "An ecologically valid examination of event-based and time-based\n  prospective memory using immersive virtual reality: the effects of delay and\n  task type on everyday prospective memory", "comments": "9 Figures, 4 Tables", "journal-ref": null, "doi": "10.1080/09658211.2021.1904996", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent research has focused on assessing either event- or time-based\nprospective memory (PM) using laboratory tasks. Yet, the findings pertaining to\nPM performance on laboratory tasks are often inconsistent with the findings on\ncorresponding naturalistic experiments. Ecologically valid neuropsychological\ntasks resemble the complexity and cognitive demands of everyday tasks, offer an\nadequate level of experimental control, and allow a generalisation of the\nfindings to everyday performance. The Virtual Reality Everyday Assessment Lab\n(VR-EAL), an immersive virtual reality neuropsychological battery with enhanced\necological validity, was implemented to comprehensively assess everyday PM\n(i.e., focal and non-focal event-based, and time-based). The effects of the\nlength of delay between encoding and initiating the PM intention and the type\nof PM task on everyday PM performance were examined. The results revealed that\neveryday PM performance was affected by the length of delay rather than the\ntype of PM task. The effect of the length of delay differentially affected\nperformance on the focal, non-focal, and time-based tasks and was proportional\nto the PM cue focality (i.e., semantic relationship with the intended action).\nThis study also highlighted methodological considerations such as the\ndifferentiation between functioning and ability, distinction of cue attributes,\nand the necessity of ecological validity.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 21:24:12 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Kourtesis", "Panagiotis", ""], ["Collina", "Simona", ""], ["Doumas", "Leonidas A. A.", ""], ["MacPherson", "Sarah E.", ""]]}, {"id": "2102.10497", "submitter": "Sang Hun Lee", "authors": "Sang Hun Lee and Se-One Yoon", "title": "User interface for in-vehicle systems with on-wheel finger spreading\n  gestures and head-up displays", "comments": "This paper was published in the Journal of Computational Design and\n  Engineering (Oxford University Press) in December 2020\n  https://academic.oup.com/jcde/article/7/6/700/5859941", "journal-ref": "Journal of Computational Design and Engineering 7 (2020) 700-721", "doi": "10.1093/jcde/qwaa052", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interacting with an in-vehicle system through a central console is known to\ninduce visual and biomechanical distractions, thereby delaying the danger\nrecognition and response times of the driver and significantly increasing the\nrisk of an accident. To address this problem, various hand gestures have been\ndeveloped. Although such gestures can reduce visual demand, they are limited in\nnumber, lack passive feedback, and can be vague and imprecise, difficult to\nunderstand and remember, and culture-bound. To overcome these limitations, we\ndeveloped a novel on-wheel finger spreading gestural interface combined with a\nhead-up display (HUD) allowing the user to choose a menu displayed in the HUD\nwith a gesture. This interface displays audio and air conditioning functions on\nthe central console of a HUD and enables their control using a specific number\nof fingers while keeping both hands on the steering wheel. We compared the\neffectiveness of the newly proposed hybrid interface against a traditional\ntactile interface for a central console using objective measurements and\nsubjective evaluations regarding both the vehicle and driver behaviour. A total\nof 32 subjects were recruited to conduct experiments on a driving simulator\nequipped with the proposed interface under various scenarios. The results\nshowed that the proposed interface was approximately 20% faster in emergency\nresponse than the traditional interface, whereas its performance in maintaining\nvehicle speed and lane was not significantly different from that of the\ntraditional one.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 03:14:08 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lee", "Sang Hun", ""], ["Yoon", "Se-One", ""]]}, {"id": "2102.10678", "submitter": "Justin Kasowski", "authors": "Justin Kasowski, Nathan Wu, Michael Beyeler", "title": "Towards Immersive Virtual Reality Simulations of Bionic Vision", "comments": "3 pages, 2 figures, to be presented at Augmented Humans", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bionic vision is a rapidly advancing field aimed at developing visual\nneuroprostheses ('bionic eyes') to restore useful vision to people who are\nblind. However, a major outstanding challenge is predicting what people 'see'\nwhen they use their devices. The limited field of view of current devices\nnecessitates head movements to scan the scene, which is difficult to simulate\non a computer screen. In addition, many computational models of bionic vision\nlack biological realism. To address these challenges, we propose to embed\nbiologically realistic models of simulated prosthetic vision (SPV) in immersive\nvirtual reality (VR) so that sighted subjects can act as 'virtual patients' in\nreal-world tasks.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 20:38:20 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Kasowski", "Justin", ""], ["Wu", "Nathan", ""], ["Beyeler", "Michael", ""]]}, {"id": "2102.10685", "submitter": "Esha Shandilya", "authors": "Esha Shandilya, Yiwen Wang, Xuan Zhao and Mingming Fan", "title": "EvoK: Connecting loved ones through Heart Rate sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present EvoK, a new way of sharing one's heart rate with\nfeedback from their close contacts to alleviate social isolation and\nloneliness. EvoK consists of a pair of wearable prototype devices (i.e., sender\nand receiver). The sender is designed as a headband enabling continuous sensing\nof heart rate with aesthetic designs to maximize social acceptance. The\nreceiver is designed as a wristwatch enabling unobtrusive receiving of the\nloved one's continuous heart rate with multi-modal notification systems.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 21:04:16 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Shandilya", "Esha", ""], ["Wang", "Yiwen", ""], ["Zhao", "Xuan", ""], ["Fan", "Mingming", ""]]}, {"id": "2102.10787", "submitter": "Henrietta Lyons", "authors": "Henrietta Lyons, Eduardo Velloso and Tim Miller", "title": "Fair and Responsible AI: A Focus on the Ability to Contest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the use of artificial intelligence (AI) in high-stakes decision-making\nincreases, the ability to contest such decisions is being recognised in AI\nethics guidelines as an important safeguard for individuals. Yet, there is\nlittle guidance on how AI systems can be designed to support contestation. In\nthis paper we explain that the design of a contestation process is important\ndue to its impact on perceptions of fairness and satisfaction. We also consider\ndesign challenges, including a lack of transparency as well as the numerous\ndesign options that decision-making entities will be faced with. We argue for a\nhuman-centred approach to designing for contestability to ensure that the needs\nof decision subjects, and the community, are met.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 05:49:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lyons", "Henrietta", ""], ["Velloso", "Eduardo", ""], ["Miller", "Tim", ""]]}, {"id": "2102.11162", "submitter": "David Puljiz", "authors": "David Puljiz, Bowen Zhou, Ke Ma, Bj\\\"orn Hein", "title": "HAIR: Head-mounted AR Intention Recognition", "comments": "As accepted to the 4th Virtual, Augmented and Mixed reality\n  Human-robot interaction workshop (VAM-HRI), HRI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human teams exhibit both implicit and explicit intention sharing. To further\ndevelopment of human-robot collaboration, intention recognition is crucial on\nboth sides. Present approaches rely on a vast sensor suite on and around the\nrobot to achieve intention recognition. This relegates intuitive human-robot\ncollaboration purely to such bulky systems, which are inadequate for\nlarge-scale, real-world scenarios due to their complexity and cost. In this\npaper we propose an intention recognition system that is based purely on a\nportable head-mounted display. In addition robot intention visualisation is\nalso supported. We present experiments to show the quality of our human goal\nestimation component and some basic interactions with an industrial robot. HAIR\nshould raise the quality of interaction between robots and humans, instead of\nsuch interactions raising the hair on the necks of the human coworkers.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:38:22 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Puljiz", "David", ""], ["Zhou", "Bowen", ""], ["Ma", "Ke", ""], ["Hein", "Bj\u00f6rn", ""]]}, {"id": "2102.11207", "submitter": "Radiah Rivu", "authors": "Radiah Rivu, Ville M\\\"akel\\\"a, Sarah Prange, Sarah Delgado Rodriguez,\n  Robin Piening, Yumeng Zhou, Kay K\\\"ohle, Ken Pfeuffer, Yomna Abdelrahman,\n  Matthias Hoppe, Albrecht Schmidt, Florian Alt", "title": "Remote VR Studies -- A Framework for Running Virtual Reality Studies\n  Remotely Via Participant-Owned HMDs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We investigate the opportunities and challenges of running virtual reality\n(VR) studies remotely. Today, many consumers own head-mounted displays (HMDs),\nallowing them to participate in scientific studies from their homes using their\nown equipment. Researchers can benefit from this approach by being able to\nreach a more diverse study population and to conduct research at times when it\nis difficult to get people into the lab (cf. the COVID pandemic). We first\nconducted an online survey (N=227), assessing HMD owners' demographics, their\nVR setups, and their attitudes towards remote participation. We then identified\ndifferent approaches to running remote studies and conducted two case studies\nfor an in-depth understanding. We synthesize our findings into a framework for\nremote VR studies, discuss the strengths and weaknesses of the different\napproaches, and derive best practices. Our work is valuable for HCI researchers\nconducting VR studies outside labs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 17:36:15 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Rivu", "Radiah", ""], ["M\u00e4kel\u00e4", "Ville", ""], ["Prange", "Sarah", ""], ["Rodriguez", "Sarah Delgado", ""], ["Piening", "Robin", ""], ["Zhou", "Yumeng", ""], ["K\u00f6hle", "Kay", ""], ["Pfeuffer", "Ken", ""], ["Abdelrahman", "Yomna", ""], ["Hoppe", "Matthias", ""], ["Schmidt", "Albrecht", ""], ["Alt", "Florian", ""]]}, {"id": "2102.11211", "submitter": "Jip Van Stijn", "authors": "Jip van Stijn", "title": "Moral Decision-Making in Medical Hybrid Intelligent Systems: A Team\n  Design Patterns Approach to the Bias Mitigation and Data Sharing Design\n  Problems", "comments": "87 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Increasing automation in the healthcare sector calls for a Hybrid\nIntelligence (HI) approach to closely study and design the collaboration of\nhumans and autonomous machines. Ensuring that medical HI systems'\ndecision-making is ethical is key. The use of Team Design Patterns (TDPs) can\nadvance this goal by describing successful and reusable configurations of\ndesign problems in which decisions have a moral component, as well as through\nfacilitating communication in multidisciplinary teams designing HI systems. For\nthis research, TDPs were developed to describe a set of solutions for two\ndesign problems in a medical HI system: (1) mitigating harmful biases in\nmachine learning algorithms and (2) sharing health and behavioral patient data\nwith healthcare professionals and system developers. The Socio-Cognitive\nEngineering methodology was employed, integrating operational demands, human\nfactors knowledge, and a technological analysis into a set of TDPs. A survey\nwas created to assess the usability of the patterns on their understandability,\neffectiveness, and generalizability. The results showed that TDPs are a useful\nmethod to unambiguously describe solutions for diverse HI design problems with\na moral component on varying abstraction levels, that are usable by a\nheterogeneous group of multidisciplinary researchers. Additionally, results\nindicated that the SCE approach and the developed questionnaire are suitable\nmethods for creating and assessing TDPs. The study concludes with a set of\nproposed improvements to TDPs, including their integration with Interaction\nDesign Patterns, the inclusion of several additional concepts, and a number of\nmethodological improvements. Finally, the thesis recommends directions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 17:09:43 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["van Stijn", "Jip", ""]]}, {"id": "2102.11617", "submitter": "Taras Kucherenko", "authors": "Taras Kucherenko, Patrik Jonell, Youngwoo Yoon, Pieter Wolfert, and\n  Gustav Eje Henter", "title": "A large, crowdsourced evaluation of gesture generation systems on common\n  data: The GENEA Challenge 2020", "comments": "Accepted for publication at the 26th International Conference on\n  Intelligent User Interfaces (IUI'21). 11 pages, 5 figures", "journal-ref": null, "doi": "10.1145/3397481.3450692", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-speech gestures, gestures that accompany speech, play an important role in\nhuman communication. Automatic co-speech gesture generation is thus a key\nenabling technology for embodied conversational agents (ECAs), since humans\nexpect ECAs to be capable of multi-modal communication. Research into gesture\ngeneration is rapidly gravitating towards data-driven methods. Unfortunately,\nindividual research efforts in the field are difficult to compare: there are no\nestablished benchmarks, and each study tends to use its own dataset, motion\nvisualisation, and evaluation methodology. To address this situation, we\nlaunched the GENEA Challenge, a gesture-generation challenge wherein\nparticipating teams built automatic gesture-generation systems on a common\ndataset, and the resulting systems were evaluated in parallel in a large,\ncrowdsourced user study using the same motion-rendering pipeline. Since\ndifferences in evaluation outcomes between systems now are solely attributable\nto differences between the motion-generation methods, this enables benchmarking\nrecent approaches against one another in order to get a better impression of\nthe state of the art in the field. This paper reports on the purpose, design,\nresults, and implications of our challenge.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 10:54:58 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Kucherenko", "Taras", ""], ["Jonell", "Patrik", ""], ["Yoon", "Youngwoo", ""], ["Wolfert", "Pieter", ""], ["Henter", "Gustav Eje", ""]]}, {"id": "2102.11652", "submitter": "Panagiotis Kourtesis", "authors": "Panagiotis Kourtesis and Sarah E. MacPherson", "title": "An ecologically valid examination of event-based and time-based\n  prospective memory using immersive virtual reality: the influence of\n  attention, memory, and executive function processes on real-world prospective\n  memory", "comments": "4 Figures , 4 Tables. arXiv admin note: text overlap with\n  arXiv:2102.10448", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Studies on prospective memory (PM) predominantly assess either event- or\ntime-based PM by implementing non-ecological laboratory-based tasks. The\nresults deriving from these paradigms have provided findings that are\ndiscrepant with ecologically valid research paradigms that converge on the\ncomplexity and cognitive demands of everyday tasks. The Virtual Reality\nEveryday Assessment Lab (VR-EAL), an immersive virtual reality (VR)\nneuropsychological battery with enhanced ecological validity, was implemented\nto assess everyday event- and time-based PM, as well as the influence of other\ncognitive functions on everyday PM functioning. The results demonstrated the\nimportance of delayed recognition, planning, and visuospatial attention on\neveryday PM. Delayed recognition and planning ability were found to be central\nin event- and time-based PM respectively. In order of importance, delayed\nrecognition, visuospatial attention speed, and planning ability were found to\nbe involved in event-based PM functioning. Comparably, planning, visuospatial\nattention accuracy, delayed recognition, and multitasking/task-shifting ability\nwere found to be involved in time-based PM functioning. These findings further\nsuggest the importance of ecological validity in the study of PM, which may be\nachieved using immersive VR paradigms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 12:19:34 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Kourtesis", "Panagiotis", ""], ["MacPherson", "Sarah E.", ""]]}, {"id": "2102.11819", "submitter": "Konrad Kollnig", "authors": "Konrad Kollnig, Siddhartha Datta, Max Van Kleek", "title": "I Want My App That Way: Reclaiming Sovereignty Over Personal Devices", "comments": "Accepted as Late-Breaking Work (LBW) for CHI 2021", "journal-ref": null, "doi": "10.1145/3411763.3451632", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dark patterns in mobile apps take advantage of cognitive biases of end-users\nand can have detrimental effects on people's lives. Despite growing research in\nidentifying remedies for dark patterns and established solutions for desktop\nbrowsers, there exists no established methodology to reduce dark patterns in\nmobile apps. Our work introduces GreaseDroid, a community-driven app\nmodification framework enabling non-expert users to disable dark patterns in\napps selectively.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 17:44:36 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Kollnig", "Konrad", ""], ["Datta", "Siddhartha", ""], ["Van Kleek", "Max", ""]]}, {"id": "2102.11824", "submitter": "Lisa Elkin", "authors": "Lisa A. Elkin, Matthew Kay, James J. Higgins, and Jacob O. Wobbrock", "title": "An Aligned Rank Transform Procedure for Multifactor Contrast Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data from multifactor HCI experiments often violates the normality assumption\nof parametric tests (i.e., nonconforming data). The Aligned Rank Transform\n(ART) is a popular nonparametric analysis technique that can find main and\ninteraction effects in nonconforming data, but leads to incorrect results when\nused to conduct contrast tests. We created a new algorithm called ART-C for\nconducting contrasts within the ART paradigm and validated it on 72,000 data\nsets. Our results indicate that ART-C does not inflate Type I error rates,\nunlike contrasts based on ART, and that ART-C has more statistical power than a\nt-test, Mann-Whitney U test, Wilcoxon signed-rank test, and ART. We also\nextended a tool called ARTool with our ART-C algorithm for both Windows and R.\nOur validation had some limitations (e.g., only six distribution types, no\nmixed factorial designs, no random slopes), and data drawn from Cauchy\ndistributions should not be analyzed with ART-C.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 17:54:14 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Elkin", "Lisa A.", ""], ["Kay", "Matthew", ""], ["Higgins", "James J.", ""], ["Wobbrock", "Jacob O.", ""]]}, {"id": "2102.12030", "submitter": "Andreas Bueckle", "authors": "Andreas Bueckle, Kilian Buehling, Patrick C. Shih, Katy B\\\"orner", "title": "Comparing Completion Time, Accuracy, and Satisfaction in Virtual Reality\n  vs. Desktop Implementation of the Common Coordinate Framework Registration\n  User Interface (CCF RUI)", "comments": "34 pages/9 figures in main text; 6 pages/1 figure/2 tables in\n  Supporting Information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Working with organs and tissue blocks is an essential task in medical\nenvironments. In order to prepare specimens for further analysis, wet-bench\nworkers must dissect tissue and collect spatial metadata. The Registration User\nInterface (RUI) was developed to allow stakeholders in the Human Biomolecular\nAtlas Program (HuBMAP) to register tissue blocks by size, position, and\norientation. The RUI has been used by tissue mapping centers across the HuBMAP\nconsortium to register a total of 45 kidney, spleen, and colon tissue blocks.\nIn this paper, we compare three setups for registering one 3D tissue block\nobject to another 3D reference organ (target) object. The first setup is a 2D\nDesktop implementation featuring a traditional screen, mouse, and keyboard\ninterface. The remaining setups are both virtual reality (VR) versions of the\nRUI: VR Tabletop, where users sit at a physical desk; VR Standup, where users\nstand upright. We ran a user study involving 42 human subjects completing 14\nincreasingly difficult and then 30 identical tasks and report position\naccuracy, rotation accuracy, completion time, and satisfaction. We found that\nwhile VR Tabletop and VR Standup users are about three times as fast and about\na third more accurate in terms of rotation than 2D Desktop users, there are no\nsignificant differences for position accuracy. The performance values for the\n2D Desktop version (22.6 seconds per task, 5.9 degrees rotation, and 1.32 mm\nposition accuracy) confirm that the 2D Desktop interface is well-suited for\nregistering tissue blocks at a speed and accuracy that meets the needs of\nexperts performing tissue dissection. In addition, the 2D Desktop setup is\ncheaper, easier to learn, and more practical for wet-bench environments than\nthe VR setups. All three setups were implemented using the Unity game engine,\nand study materials were made available, alongside videos documenting our\nsetups.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 02:30:35 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Bueckle", "Andreas", ""], ["Buehling", "Kilian", ""], ["Shih", "Patrick C.", ""], ["B\u00f6rner", "Katy", ""]]}, {"id": "2102.12302", "submitter": "Rajmund Nagy", "authors": "Rajmund Nagy, Taras Kucherenko, Birger Moell, Andr\\'e Pereira, Hedvig\n  Kjellstr\\\"om and Ulysses Bernardet", "title": "A Framework for Integrating Gesture Generation Models into Interactive\n  Conversational Agents", "comments": "Rajmund Nagy and Taras Kucherenko contributed equally to this work.\n  To be published in the Proceedings of the 20th International Conference on\n  Autonomous Agents and Multiagent Systems (AAMAS 2021), Online, May 3-7, 2021,\n  IFAA-MAS, 3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied conversational agents (ECAs) benefit from non-verbal behavior for\nnatural and efficient interaction with users. Gesticulation - hand and arm\nmovements accompanying speech - is an essential part of non-verbal behavior.\nGesture generation models have been developed for several decades: starting\nwith rule-based and ending with mainly data-driven methods. To date, recent\nend-to-end gesture generation methods have not been evaluated in a real-time\ninteraction with users. We present a proof-of-concept framework, which is\nintended to facilitate evaluation of modern gesture generation models in\ninteraction.\n  We demonstrate an extensible open-source framework that contains three\ncomponents: 1) a 3D interactive agent; 2) a chatbot backend; 3) a gesticulating\nsystem. Each component can be replaced, making the proposed framework\napplicable for investigating the effect of different gesturing models in\nreal-time interactions with different communication modalities, chatbot\nbackends, or different agent appearances. The code and video are available at\nthe project page https://nagyrajmund.github.io/project/gesturebot.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:31:21 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Nagy", "Rajmund", ""], ["Kucherenko", "Taras", ""], ["Moell", "Birger", ""], ["Pereira", "Andr\u00e9", ""], ["Kjellstr\u00f6m", "Hedvig", ""], ["Bernardet", "Ulysses", ""]]}, {"id": "2102.12313", "submitter": "Xiang Li", "authors": "Xiang Li, Yuzheng Chen, Rakesh Patibanda, Florian 'Floyd' Mueller", "title": "vrCAPTCHA: Exploring CAPTCHA Designs in Virtual Reality", "comments": "4 pages, 3 figures, CHI EA' 2021", "journal-ref": null, "doi": "10.1145/3411763.3451985", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the popularity of online access in virtual reality (VR) devices, it will\nbecome important to investigate exclusive and interactive CAPTCHA (Completely\nAutomated Public Turing test to tell Computers and Humans Apart) designs for VR\ndevices. In this paper, we first present four traditional two-dimensional (2D)\nCAPTCHAs (i.e., text-based, image-rotated, image-puzzled, and image-selected\nCAPTCHAs) in VR. Then, based on the three-dimensional (3D) interaction\ncharacteristics of VR devices, we propose two vrCAPTCHA design prototypes\n(i.e., task-driven and bodily motion-based CAPTCHAs). We conducted a user study\nwith six participants for exploring the feasibility of our two vrCAPTCHAs and\ntraditional CAPTCHAs in VR. We believe that our two vrCAPTCHAs can be an\ninspiration for the further design of CAPTCHAs in VR.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:43:11 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 16:13:02 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Li", "Xiang", ""], ["Chen", "Yuzheng", ""], ["Patibanda", "Rakesh", ""], ["Mueller", "Florian 'Floyd'", ""]]}, {"id": "2102.12518", "submitter": "Jinghui Cheng", "authors": "Jazlyn Hellman, Jinghui Cheng, Jin L.C. Guo", "title": "Facilitating Asynchronous Participatory Design of Open Source Software:\n  Bringing End Users into the Loop", "comments": "7 pages, 3 figures, CHI 2021 Extended Abstracts", "journal-ref": null, "doi": "10.1145/3411763.3451643", "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As open source software (OSS) becomes increasingly mature and popular, there\nare significant challenges with properly accounting for usability concerns for\nthe diverse end users. Participatory design, where multiple stakeholders\ncollaborate on iterating the design, can be an efficient way to address the\nusability concerns for OSS projects. However, barriers such as a code-centric\nmindset and insufficient tool support often prevent OSS teams from effectively\nincluding end users in participatory design methods. This paper proposes\npreliminary contributions to this problem through the user-centered exploration\nof (1) a set of design guidelines that capture the needs of OSS participatory\ndesign tools, (2) two personas that represent the characteristics of OSS\ndesigners and end users, and (3) a low-fidelity prototype tool for end user\ninvolvement in OSS projects. This work paves the road for future studies about\ntool design that would eventually help improve OSS usability.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:23:11 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Hellman", "Jazlyn", ""], ["Cheng", "Jinghui", ""], ["Guo", "Jin L. C.", ""]]}, {"id": "2102.12523", "submitter": "Chunjong Park", "authors": "Chunjong Park, Morelle Arian, Xin Liu, Leon Sasson, Jeffrey Kahn,\n  Shwetak Patel, Alex Mariakakis, Tim Althoff", "title": "Online Mobile App Usage as an Indicator of Sleep Behavior and Job\n  Performance", "comments": null, "journal-ref": null, "doi": "10.1145/3442381.3450093", "report-no": null, "categories": "cs.HC cs.CY q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sleep is critical to human function, mediating factors like memory, mood,\nenergy, and alertness; therefore, it is commonly conjectured that a good\nnight's sleep is important for job performance. However, both real-world sleep\nbehavior and job performance are hard to measure at scale. In this work, we\nshow that people's everyday interactions with online mobile apps can reveal\ninsights into their job performance in real-world contexts. We present an\nobservational study in which we objectively tracked the sleep behavior and job\nperformance of salespeople (N = 15) and athletes (N = 19) for 18 months, using\na mattress sensor and online mobile app. We first demonstrate that cumulative\nsleep measures are correlated with job performance metrics, showing that an\nhour of daily sleep loss for a week was associated with a 9.0% and 9.5%\nreduction in performance of salespeople and athletes, respectively. We then\nexamine the utility of online app interaction time as a passively collectible\nand scalable performance indicator. We show that app interaction time is\ncorrelated with the performance of the athletes, but not the salespeople. To\nsupport that our app-based performance indicator captures meaningful variation\nin psychomotor function and is robust against potential confounds, we conducted\na second study to evaluate the relationship between sleep behavior and app\ninteraction time in a cohort of 274 participants. Using a generalized additive\nmodel to control for per-participant random effects, we demonstrate that\nparticipants who lost one hour of daily sleep for a week exhibited 5.0% slower\napp interaction times. We also find that app interaction time exhibits\nmeaningful chronobiologically consistent correlations with sleep history, time\nawake, and circadian rhythms. Our findings reveal an opportunity for online app\ndevelopers to generate new insights regarding cognition and productivity.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:30:39 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Park", "Chunjong", ""], ["Arian", "Morelle", ""], ["Liu", "Xin", ""], ["Sasson", "Leon", ""], ["Kahn", "Jeffrey", ""], ["Patel", "Shwetak", ""], ["Mariakakis", "Alex", ""], ["Althoff", "Tim", ""]]}, {"id": "2102.12548", "submitter": "Franklin Mingzhe Li", "authors": "Wei Sun, Franklin Mingzhe Li, Benjamin Steeper, Songlin Xu, Feng Tian,\n  Cheng Zhang", "title": "TeethTap: Recognizing Discrete Teeth Gestures Using Motion and Acoustic\n  Sensing on an Earpiece", "comments": "26th International Conference on Intelligent User Interfaces (IUI\n  '21), April 14--17, 2021, College Station, TX, USA", "journal-ref": null, "doi": "10.1145/3397481.3450645", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Teeth gestures become an alternative input modality for different situations\nand accessibility purposes. In this paper, we present TeethTap, a novel\neyes-free and hands-free input technique, which can recognize up to 13 discrete\nteeth tapping gestures. TeethTap adopts a wearable 3D printed earpiece with an\nIMU sensor and a contact microphone behind both ears, which works in tandem to\ndetect jaw movement and sound data, respectively. TeethTap uses a support\nvector machine to classify gestures from noise by fusing acoustic and motion\ndata, and implements K-Nearest-Neighbor (KNN) with a Dynamic Time Warping (DTW)\ndistance measurement using motion data for gesture classification. A user study\nwith 11 participants demonstrated that TeethTap could recognize 13 gestures\nwith a real-time classification accuracy of 90.9% in a laboratory environment.\nWe further uncovered the accuracy differences on different teeth gestures when\nhaving sensors on single vs. both sides. Moreover, we explored the activation\ngesture under real-world environments, including eating, speaking, walking and\njumping. Based on our findings, we further discussed potential applications and\npractical challenges of integrating TeethTap into future devices.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 20:39:32 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Sun", "Wei", ""], ["Li", "Franklin Mingzhe", ""], ["Steeper", "Benjamin", ""], ["Xu", "Songlin", ""], ["Tian", "Feng", ""], ["Zhang", "Cheng", ""]]}, {"id": "2102.12592", "submitter": "April Wang", "authors": "April Yi Wang, Dakuo Wang, Jaimie Drozdal, Michael Muller, Soya Park,\n  Justin D. Weisz, Xuye Liu, Lingfei Wu, Casey Dugan", "title": "Themisto: Towards Automated Documentation Generation in Computational\n  Notebooks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational notebooks allow data scientists to express their ideas through\na combination of code and documentation. However, data scientists often pay\nattention only to the code, and neglect creating or updating their\ndocumentation during quick iterations, which leads to challenges in sharing\ntheir notebooks with others and future selves. Inspired by human documentation\npractices from analyzing 80 highly-voted Kaggle notebooks, we design and\nimplement Themisto, an automated documentation generation system to explore the\nHuman-AI Collaboration opportunity in the code documentation scenario. Themisto\nfacilitates the creation of different types of documentation via three\napproaches: a deep-learning-based approach to generate documentation for source\ncode (fully automated), a query-based approach to retrieve the online API\ndocumentation for source code (fully automated), and a user prompt approach to\nmotivate users to write more documentation (semi-automated). We evaluated\nThemisto in a within-subjects experiment with 24 data science practitioners,\nand found that automated documentation generation techniques reduced the time\nfor writing documentation, reminded participants to document code they would\nhave ignored, and improved participants' satisfaction with their computational\nnotebook.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 22:46:16 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Wang", "April Yi", ""], ["Wang", "Dakuo", ""], ["Drozdal", "Jaimie", ""], ["Muller", "Michael", ""], ["Park", "Soya", ""], ["Weisz", "Justin D.", ""], ["Liu", "Xuye", ""], ["Wu", "Lingfei", ""], ["Dugan", "Casey", ""]]}, {"id": "2102.12606", "submitter": "Nahyun Kwon", "authors": "Nahyun Kwon, Chen Liang and Jeeeun Kim", "title": "3D4ALL: Toward an Inclusive Pipeline to Classify 3D Contents", "comments": "9 pages, 2 figures, TExSS, ACM IUI 2021 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Algorithmic content moderation manages an explosive number of user-created\ncontent shared online everyday. Despite a massive number of 3D designs that are\nfree to be downloaded, shared, and 3D printed by the users, detecting\nsensitivity with transparency and fairness has been controversial. Although\nsensitive 3D content might have a greater impact than other media due to its\npossible reproducibility and replicability without restriction, prevailed\nunawareness resulted in proliferation of sensitive 3D models online and a lack\nof discussion on transparent and fair 3D content moderation. As the 3D content\nexists as a document on the web mainly consisting of text and images, we first\nstudy the existing algorithmic efforts based on text and images and the prior\nendeavors to encompass transparency and fairness in moderation, which can also\nbe useful in a 3D printing domain. At the same time, we identify 3D specific\nfeatures that should be addressed to advance a 3D specialized algorithmic\nmoderation. As a potential solution, we suggest a human-in-the-loop pipeline\nusing augmented learning, powered by various stakeholders with different\nbackgrounds and perspectives in understanding the content. Our pipeline aims to\nminimize personal biases by enabling diverse stakeholders to be vocal in\nreflecting various factors to interpret the content. We add our initial\nproposal for redesigning metadata of open 3D repositories, to invoke users'\nresponsible actions of being granted consent from the subject upon sharing\ncontents for free in the public spaces.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 23:58:07 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Kwon", "Nahyun", ""], ["Liang", "Chen", ""], ["Kim", "Jeeeun", ""]]}, {"id": "2102.12773", "submitter": "Fengshi Tian Clarence", "authors": "Fengshi Tian, Jie Yang, Shiqi Zhao, Mohamad Sawan", "title": "A New Neuromorphic Computing Approach for Epileptic Seizure Prediction", "comments": "Accepted to 2021 IEEE International Symposium on Circuits and Systems\n  (ISCAS)", "journal-ref": "2021 IEEE International Symposium on Circuits and Systems (ISCAS)", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several high specificity and sensitivity seizure prediction methods with\nconvolutional neural networks (CNNs) are reported. However, CNNs are\ncomputationally expensive and power hungry. These inconveniences make CNN-based\nmethods hard to be implemented on wearable devices. Motivated by the\nenergy-efficient spiking neural networks (SNNs), a neuromorphic computing\napproach for seizure prediction is proposed in this work. This approach uses a\ndesigned gaussian random discrete encoder to generate spike sequences from the\nEEG samples and make predictions in a spiking convolutional neural network\n(Spiking-CNN) which combines the advantages of CNNs and SNNs. The experimental\nresults show that the sensitivity, specificity and AUC can remain 95.1%, 99.2%\nand 0.912 respectively while the computation complexity is reduced by 98.58%\ncompared to CNN, indicating that the proposed Spiking-CNN is hardware friendly\nand of high precision.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 10:39:18 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Tian", "Fengshi", ""], ["Yang", "Jie", ""], ["Zhao", "Shiqi", ""], ["Sawan", "Mohamad", ""]]}, {"id": "2102.12893", "submitter": "Somit Gupta", "authors": "Soheil Sadeghi, Somit Gupta, Stefan Gramatovici, Jiannan Lu, Hao Ai,\n  Ruhan Zhang", "title": "Novelty and Primacy: A Long-Term Estimator for Online Experiments", "comments": "Submitted to the Technometrics Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online experiments are the gold standard for evaluating impact on user\nexperience and accelerating innovation in software. However, since experiments\nare typically limited in duration, observed treatment effects are not always\npermanently stable, sometimes revealing increasing or decreasing patterns over\ntime. There are multiple causes for a treatment effect to change over time. In\nthis paper, we focus on a particular cause, user-learning, which is primarily\nassociated with novelty or primacy. Novelty describes the desire to use new\ntechnology that tends to diminish over time. Primacy describes the growing\nengagement with technology as a result of adoption of the innovation.\nUser-learning estimation is critical because it holds experimentation\nresponsible for trustworthiness, empowers organizations to make better\ndecisions by providing a long-term view of expected impact, and prevents user\ndissatisfaction. In this paper, we propose an observational approach, based on\ndifference-in-differences technique to estimate user-learning at scale. We use\nthis approach to test and estimate user-learning in many experiments at\nMicrosoft. We compare our approach with the existing experimental method to\nshow its benefits in terms of ease of use and higher statistical power, and to\ndiscuss its limitation in presence of other forms of treatment interaction with\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 00:49:53 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Sadeghi", "Soheil", ""], ["Gupta", "Somit", ""], ["Gramatovici", "Stefan", ""], ["Lu", "Jiannan", ""], ["Ai", "Hao", ""], ["Zhang", "Ruhan", ""]]}, {"id": "2102.13004", "submitter": "Vijay Keswani", "authors": "Vijay Keswani, Matthew Lease, Krishnaram Kenthapadi", "title": "Towards Unbiased and Accurate Deferral to Multiple Experts", "comments": "This paper has been accepted for publication at the AAAI/ACM\n  Conference on Artificial Intelligence, Ethics, and Society (AIES 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are often implemented in cohort with humans in the\npipeline, with the model having an option to defer to a domain expert in cases\nwhere it has low confidence in its inference. Our goal is to design mechanisms\nfor ensuring accuracy and fairness in such prediction systems that combine\nmachine learning model inferences and domain expert predictions. Prior work on\n\"deferral systems\" in classification settings has focused on the setting of a\npipeline with a single expert and aimed to accommodate the inaccuracies and\nbiases of this expert to simultaneously learn an inference model and a deferral\nsystem. Our work extends this framework to settings where multiple experts are\navailable, with each expert having their own domain of expertise and biases. We\npropose a framework that simultaneously learns a classifier and a deferral\nsystem, with the deferral system choosing to defer to one or more human experts\nin cases of input where the classifier has low confidence. We test our\nframework on a synthetic dataset and a content moderation dataset with biased\nsynthetic experts, and show that it significantly improves the accuracy and\nfairness of the final predictions, compared to the baselines. We also collect\ncrowdsourced labels for the content moderation task to construct a real-world\ndataset for the evaluation of hybrid machine-human frameworks and show that our\nproposed learning framework outperforms baselines on this real-world dataset as\nwell.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 17:08:39 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 20:34:31 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Keswani", "Vijay", ""], ["Lease", "Matthew", ""], ["Kenthapadi", "Krishnaram", ""]]}, {"id": "2102.13008", "submitter": "Vinicius G. Goecks", "authors": "Ritwik Bera, Vinicius G. Goecks, Gregory M. Gremillion, Vernon J.\n  Lawhern, John Valasek, Nicholas R. Waytowich", "title": "Gaze-Informed Multi-Objective Imitation Learning from Human\n  Demonstrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of human-robot interaction, teaching learning agents from human\ndemonstrations via supervised learning has been widely studied and successfully\napplied to multiple domains such as self-driving cars and robot manipulation.\nHowever, the majority of the work on learning from human demonstrations\nutilizes only behavioral information from the demonstrator, i.e. what actions\nwere taken, and ignores other useful information. In particular, eye gaze\ninformation can give valuable insight towards where the demonstrator is\nallocating their visual attention, and leveraging such information has the\npotential to improve agent performance. Previous approaches have only studied\nthe utilization of attention in simple, synchronous environments, limiting\ntheir applicability to real-world domains. This work proposes a novel imitation\nlearning architecture to learn concurrently from human action demonstration and\neye tracking data to solve tasks where human gaze information provides\nimportant context. The proposed method is applied to a visual navigation task,\nin which an unmanned quadrotor is trained to search for and navigate to a\ntarget vehicle in a real-world, photorealistic simulated environment. When\ncompared to a baseline imitation learning architecture, results show that the\nproposed gaze augmented imitation learning model is able to learn policies that\nachieve significantly higher task completion rates, with more efficient paths,\nwhile simultaneously learning to predict human visual attention. This research\naims to highlight the importance of multimodal learning of visual attention\ninformation from additional human input modalities and encourages the community\nto adopt them when training agents from human demonstrations to perform\nvisuomotor tasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 17:13:13 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Bera", "Ritwik", ""], ["Goecks", "Vinicius G.", ""], ["Gremillion", "Gregory M.", ""], ["Lawhern", "Vernon J.", ""], ["Valasek", "John", ""], ["Waytowich", "Nicholas R.", ""]]}, {"id": "2102.13034", "submitter": "Yuan Shen", "authors": "Yuan Shen, Niviru Wijayaratne, Peter Du, Shanduojiao Jiang, Katherine\n  Driggs Campbell", "title": "AutoPreview: A Framework for Autopilot Behavior Understanding", "comments": "7 pages, 5 figures, CHI 2021 Late breaking Work", "journal-ref": "CHI Conference on Human Factors in Computing Systems Extended\n  Abstracts (CHI '21 Extended Abstracts), May 8 to 13, 2021, Yokohama, Japan", "doi": "10.1145/3411763.3451591", "report-no": null, "categories": "cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behavior of self driving cars may differ from people expectations, (e.g.\nan autopilot may unexpectedly relinquish control). This expectation mismatch\ncan cause potential and existing users to distrust self driving technology and\ncan increase the likelihood of accidents. We propose a simple but effective\nframework, AutoPreview, to enable consumers to preview a target autopilot\npotential actions in the real world driving context before deployment. For a\ngiven target autopilot, we design a delegate policy that replicates the target\nautopilot behavior with explainable action representations, which can then be\nqueried online for comparison and to build an accurate mental model. To\ndemonstrate its practicality, we present a prototype of AutoPreview integrated\nwith the CARLA simulator along with two potential use cases of the framework.\nWe conduct a pilot study to investigate whether or not AutoPreview provides\ndeeper understanding about autopilot behavior when experiencing a new autopilot\npolicy for the first time. Our results suggest that the AutoPreview method\nhelps users understand autopilot behavior in terms of driving style\ncomprehension, deployment preference, and exact action timing prediction.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 17:40:59 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Shen", "Yuan", ""], ["Wijayaratne", "Niviru", ""], ["Du", "Peter", ""], ["Jiang", "Shanduojiao", ""], ["Campbell", "Katherine Driggs", ""]]}, {"id": "2102.13167", "submitter": "Alireza Karduni", "authors": "Alireza Karduni, Ryan Wesslen, Douglas Markant, Wenwen Dou", "title": "Images, Emotions, and Credibility: Effect of Emotional Facial Images on\n  Perceptions of News Content Bias and Source Credibility in Social Media", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Images are an indispensable part of the news content we consume. Highly\nemotional images from sources of misinformation can greatly influence our\njudgements. We present two studies on the effects of emotional facial images on\nusers' perception of bias in news content and the credibility of sources. In\nstudy 1, we investigate the impact of happy and angry facial images on users'\ndecisions. In study 2, we focus on sources' systematic emotional treatment of\nspecific politicians. Our results show that depending on the political\norientation of the source, the cumulative effect of angry facial emotions\nimpacts users' perceived content bias and source credibility. When sources\nsystematically portray specific politicians as angry, users are more likely to\nfind those sources as less credible and their content as more biased. These\nresults highlight how implicit visual propositions manifested by emotions in\nfacial expressions might have a substantial effect on our trust of news content\nand sources.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 20:47:21 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Karduni", "Alireza", ""], ["Wesslen", "Ryan", ""], ["Markant", "Douglas", ""], ["Dou", "Wenwen", ""]]}, {"id": "2102.13173", "submitter": "Hong Sun", "authors": "Hong Sun and Vincenzo De Florio", "title": "Perspectives and solutions towards intelligent ambient assisted living\n  systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The population of the elderly people has kept increasing rapidly over the\nworld in the past decades. Solutions that are able to effectively support the\nelderly people to live independently at their home are thus urgently needed.\nAmbient assisted living (AAL) aims to provide products and services with\nambient intelligence to build a safe environment around people in need. With\nthe high prevalence of multiple chronic diseases, the elderly people often need\ndifferent levels of care management to prolong independent living at home. An\neffective AAL system should provide the required clinical support as an\nextension to the services provided in hospitals. Following the rapid growth of\navailable data, together with the wide application of machine learning\ntechnologies, we are now able to build intelligent ambient assisted systems to\nfulfil such a request. This paper discusses different levels of intelligence in\nAAL. We also introduce our solution for building an intelligent AAL system with\nthe discussed technologies. Taking semantic web technology as its backbone,\nsuch an AAL system is able to aggregate information from different sources,\nsolve the semantic gap between different data sources, and perform adaptive and\npersonalized carepath management based on the ambient environment.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 21:03:42 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 15:32:43 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Sun", "Hong", ""], ["De Florio", "Vincenzo", ""]]}, {"id": "2102.13350", "submitter": "Jihye Park", "authors": "Seokgi Kim, Jihye Park, Kihong Seong, Namwoo Cho, Junho Min, Hwajung\n  Hong", "title": "Music-Circles: Can Music Be Represented With Numbers?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world today is experiencing an abundance of music like no other time, and\nattempts to group music into clusters have become increasingly prevalent.\nCommon standards for grouping music were songs, artists, and genres, with\nartists or songs exploring similar genres of music seen as related. These\nclustering attempts serve critical purposes for various stakeholders involved\nin the music industry. For end users of music services, they may want to group\ntheir music so that they can easily navigate inside their music library; for\nmusic streaming platforms like Spotify, companies may want to establish a solid\ndataset of related songs in order to successfully provide personalized music\nrecommendations and coherent playlists to their users. Due to increased\ncompetition in the streaming market, platforms are trying their best to find\nnovel ways of learning similarities between audio to gain competitive\nadvantage. Our team, comprised of music lovers with different tastes, was\ninterested in the same issue, and created Music-Circles, an interactive\nvisualization of music from the Billboard. Music-Circles links audio feature\ndata offered by Spotify to popular songs to create unique vectors for each\nsong, and calculate similarities between these vectors to cluster them. Through\ninteracting with Music-Circles, users can gain understandings of audio\nfeatures, view characteristic trends in popular music, and find out which music\ncluster they belong to.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 08:00:47 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Kim", "Seokgi", ""], ["Park", "Jihye", ""], ["Seong", "Kihong", ""], ["Cho", "Namwoo", ""], ["Min", "Junho", ""], ["Hong", "Hwajung", ""]]}, {"id": "2102.13407", "submitter": "Davide Andreoletti", "authors": "Davide Andreoletti, Luca Luceri, Tiziano Leidi, Achille Peternier,\n  Silvia Giordano", "title": "The Virtual Emotion Loop: Towards Emotion-Driven Services via Virtual\n  Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The importance of emotions in service and in product design is well known.\nHowever, it is still not very well understood how users' emotions can be\nincorporated in a product or service lifecycle. We argue that this gap is due\nto a lack of a methodological framework for an effective investigation of the\nemotional response of persons when using products and services. Indeed, the\nemotional response of users is generally investigated by means of methods\n(e.g., surveys) that are not effective for this purpose. In our view, Virtual\nReality (VR) technologies represent the perfect medium to evoke and recognize\nusers' emotional response, as well as to prototype products and services (and,\nfor the latter, even deliver them). In this paper, we first provide our\ndefinition of emotion-driven services, and then we propose a novel\nmethodological framework, referred to as the Virtual-Reality-Based\nEmotion-Elicitation-and-Recognition loop (VEE-loop), that can be exploited to\nrealize it. Specifically, the VEE-loop consists in a continuous monitoring of\nusers' emotions, which are then provided to service designers as an implicit\nusers' feedback. This information is used to dynamically change the content of\nthe VR environment, until the desired affective state is solicited. Finally, we\ndiscuss issues and opportunities of this VEE-loop, and we also present\npotential applications of the VEE-loop in research and in various application\nareas.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:36:29 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 14:57:39 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 07:48:11 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Andreoletti", "Davide", ""], ["Luceri", "Luca", ""], ["Leidi", "Tiziano", ""], ["Peternier", "Achille", ""], ["Giordano", "Silvia", ""]]}, {"id": "2102.13461", "submitter": "Hendrik Heuer", "authors": "Hendrik Heuer, Daniel Buschek", "title": "Methods for the Design and Evaluation of HCI+NLP Systems", "comments": "Accepted at the EACL 2021 Workshop on Bridging Human-Computer\n  Interaction and Natural Language Processing (HCI+NLP Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  HCI and NLP traditionally focus on different evaluation methods. While HCI\ninvolves a small number of people directly and deeply, NLP traditionally relies\non standardized benchmark evaluations that involve a larger number of people\nindirectly. We present five methodological proposals at the intersection of HCI\nand NLP and situate them in the context of ML-based NLP models. Our goal is to\nfoster interdisciplinary collaboration and progress in both fields by\nemphasizing what the fields can learn from each other.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 13:37:10 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Heuer", "Hendrik", ""], ["Buschek", "Daniel", ""]]}, {"id": "2102.13508", "submitter": "Daniel Buschek", "authors": "Sarah Theres V\\\"olkel, Daniel Buschek, Malin Eiband, Benjamin R.\n  Cowan, Heinrich Hussmann", "title": "Eliciting and Analysing Users' Envisioned Dialogues with Perfect Voice\n  Assistants", "comments": "15 pages, 4 figures, 3 tables, ACM CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445536", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dialogue elicitation study to assess how users envision\nconversations with a perfect voice assistant (VA). In an online survey, N=205\nparticipants were prompted with everyday scenarios, and wrote the lines of both\nuser and VA in dialogues that they imagined as perfect. We analysed the\ndialogues with text analytics and qualitative analysis, including number of\nwords and turns, social aspects of conversation, implied VA capabilities, and\nthe influence of user personality. The majority envisioned dialogues with a VA\nthat is interactive and not purely functional; it is smart, proactive, and has\nknowledge about the user. Attitudes diverged regarding the assistant's role as\nwell as it expressing humour and opinions. An exploratory analysis suggested a\nrelationship with personality for these aspects, but correlations were low\noverall. We discuss implications for research and design of future VAs,\nunderlining the vision of enabling conversational UIs, rather than single\ncommand \"Q&As\".\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 14:38:57 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 06:35:25 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["V\u00f6lkel", "Sarah Theres", ""], ["Buschek", "Daniel", ""], ["Eiband", "Malin", ""], ["Cowan", "Benjamin R.", ""], ["Hussmann", "Heinrich", ""]]}]