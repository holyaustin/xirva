[{"id": "1709.00098", "submitter": "Duc Nguyen", "authors": "Duc T. Nguyen and Blair Kaneshiro", "title": "AudExpCreator: A GUI-based Matlab tool for designing and creating\n  auditory experiments with the Psychophysics Toolbox", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present AudExpCreator, a GUI-based Matlab tool for designing and creating\nauditory experiments. AudExpCreator allows users to generate auditory\nexperiments that run on Matlab's Psychophysics Toolbox without having to write\nany code; rather, users simply follow instructions in GUIs to specify desired\ndesign parameters. The software comprises five auditory study types, including\nbehavioral studies and integration with EEG and physiological response\ncollection systems. Advanced features permit more complicated experimental\ndesigns as well as maintenance and update of previously created experiments.\nAudExpCreator alleviates programming barriers while providing a free,\nopen-source alternative to commercial experimental design software.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 22:15:12 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Nguyen", "Duc T.", ""], ["Kaneshiro", "Blair", ""]]}, {"id": "1709.00111", "submitter": "Francisco Queiroz", "authors": "Francisco Queiroz, Raniere Silva, Jonah Miller, Sandor Brockhauser,\n  Hans Fangohr", "title": "Good Usability Practices in Scientific Software Development", "comments": null, "journal-ref": null, "doi": "10.6084/m9.figshare.5331814.v3", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific software often presents very particular requirements regarding\nusability, which is often completely overlooked in this setting. As\ncomputational science has emerged as its own discipline, distinct from\ntheoretical and experimental science, it has put new requirements on future\nscientific software developments. In this paper, we discuss the background of\nthese problems and introduce nine aspects of good usability. We also highlight\nbest practices for each aspect with an emphasis on applications in\ncomputational science.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 23:47:37 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Queiroz", "Francisco", ""], ["Silva", "Raniere", ""], ["Miller", "Jonah", ""], ["Brockhauser", "Sandor", ""], ["Fangohr", "Hans", ""]]}, {"id": "1709.00293", "submitter": "Radu Jianu", "authors": "Mershack Okoe and Radu Jianu and Stephen Kobourov", "title": "Revisited Experimental Comparison of Node-Link and Matrix\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing network data is applicable in domains such as biology,\nengineering, and social sciences. We report the results of a study comparing\nthe effectiveness of the two primary techniques for showing network data:\nnode-link diagrams and adjacency matrices. Specifically, an evaluation with a\nlarge number of online participants revealed statistically significant\ndifferences between the two visualizations. Our work adds to existing research\nin several ways. First, we explore a broad spectrum of network tasks, many of\nwhich had not been previously evaluated. Second, our study uses a large\ndataset, typical of many real-life networks not explored by previous studies.\nThird, we leverage crowdsourcing to evaluate many tasks with many participants.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 15:18:46 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Okoe", "Mershack", ""], ["Jianu", "Radu", ""], ["Kobourov", "Stephen", ""]]}, {"id": "1709.00372", "submitter": "Daniel Archambault", "authors": "Paolo Simonetto, Daniel Archambault and Stephen Kobourov", "title": "Drawing Dynamic Graphs Without Timeslices", "comments": "Appears in the Proceedings of the 25th International Symposium on\n  Graph Drawing and Network Visualization (GD 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timeslices are often used to draw and visualize dynamic graphs. While\ntimeslices are a natural way to think about dynamic graphs, they are routinely\nimposed on continuous data. Often, it is unclear how many timeslices to select:\ntoo few timeslices can miss temporal features such as causality or even graph\nstructure while too many timeslices slows the drawing computation. We present a\nmodel for dynamic graphs which is not based on timeslices, and a dynamic graph\ndrawing algorithm, DynNoSlice, to draw graphs in this model. In our evaluation,\nwe demonstrate the advantages of this approach over timeslicing on continuous\ndata sets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 15:45:41 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Simonetto", "Paolo", ""], ["Archambault", "Daniel", ""], ["Kobourov", "Stephen", ""]]}, {"id": "1709.00665", "submitter": "Vincent Yang", "authors": "Vincent Yang, Harrison Nguyen, Norman Matloff, Yingkang Xie", "title": "Top-Frequency Parallel Coordinates Plots", "comments": "29 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel coordinates plotting is one of the most popular methods for\nmultivariate visualization. However, when applied to larger data sets, there\ntends to be a \"black screen problem,\" with the screen becoming so cluttered and\nfull that patterns are difficult or impossible to discern. Xie and Matloff\n(2014) proposed remedying this problem by plotting only the most\nfrequently-appearing patterns, with frequency defined in terms of\nnonparametrically estimated multivariate density. This approach displays\n\"typical\" patterns, which may reveal important insights for the data. However,\nthis remedy does not cover variables that are discrete or categorical. An\nalternate method, still frequency-based, is presented here for such cases. We\ndiscretize all continuous variables, retaining the discrete/categorical ones,\nand plot the patterns having the highest counts in the dataset. In addition, we\npropose some novel approaches to handling missing values in parallel\ncoordinates settings.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 05:25:42 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Yang", "Vincent", ""], ["Nguyen", "Harrison", ""], ["Matloff", "Norman", ""], ["Xie", "Yingkang", ""]]}, {"id": "1709.00698", "submitter": "Carlos Bermejo", "authors": "Carlos Bermejo and Pan Hui", "title": "A survey on haptic technologies for mobile augmented reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality (AR) and Mobile Augmented Reality (MAR) applications have\ngained much research and industry attention these days. The mobile nature of\nMAR applications limits users' interaction capabilities such as inputs, and\nhaptic feedbacks. This survey reviews current research issues in the area of\nhuman computer interaction for MAR and haptic devices. The survey first\npresents human sensing capabilities and their applicability in AR applications.\nWe classify haptic devices into two groups according to the triggered sense:\ncutaneous/tactile: touch, active surfaces, and mid-air, kinesthetic:\nmanipulandum, grasp, and exoskeleton. Due to the mobile capabilities of MAR\napplications, we mainly focus our study on wearable haptic devices for each\ncategory and their AR possibilities. To conclude, we discuss the future paths\nthat haptic feedbacks should follow for MAR applications and their challenges.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 11:12:15 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 09:47:41 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 07:39:35 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Bermejo", "Carlos", ""], ["Hui", "Pan", ""]]}, {"id": "1709.00904", "submitter": "Michael Lyons", "authors": "Andreas Wiratanaya and Michael J. Lyons", "title": "A Mimetic Strategy to Engage Voluntary Physical Activity In Interactive\n  Entertainment", "comments": "6 pages, 7 figures, ECAG08 workshop", "journal-ref": "Proceedings of the Workshop on Facial and Bodily Expressions for\n  Control and Adaptation of Games (2008) pp. 63 - 69", "doi": "10.6084/m9.figshare.5372875.v1", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the design and implementation of a vision based interactive\nentertainment system that makes use of both involuntary and voluntary control\nparadigms. Unintentional input to the system from a potential viewer is used to\ndrive attention-getting output and encourage the transition to voluntary\ninteractive behaviour. The iMime system consists of a character animation\nengine based on the interaction metaphor of a mime performer that simulates\nnon-verbal communication strategies, without spoken dialogue, to capture and\nhold the attention of a viewer. The system was developed in the context of a\nproject studying care of dementia sufferers. Care for a dementia sufferer can\nplace unreasonable demands on the time and attentional resources of their\ncaregivers or family members. Our study contributes to the eventual development\nof a system aimed at providing relief to dementia caregivers, while at the same\ntime serving as a source of pleasant interactive entertainment for viewers. The\nwork reported here is also aimed at a more general study of the design of\ninteractive entertainment systems involving a mixture of voluntary and\ninvoluntary control.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 11:22:43 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Wiratanaya", "Andreas", ""], ["Lyons", "Michael J.", ""]]}, {"id": "1709.00927", "submitter": "Carlos Alberto Lara-Alvarez", "authors": "Carlos Lara-Alvarez, Hugo Mitre-Hernandez, Juan Flores, Maria Fuentes", "title": "A Fuzzy Control System for Inductive Video Games", "comments": "It needs to be reviewed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that the emotional state of students has an important\nrelationship with learning; for instance, engaged concentration is positively\ncorrelated with learning. This paper proposes the Inductive Control (IC) for\neducational games. Unlike conventional approaches that only modify the game\nlevel, the proposed technique also induces emotions in the player for\nsupporting the learning process. This paper explores a fuzzy system that\nanalyzes the players' performance and their emotional state for controlling the\nlevel and aesthetic content of an educational video game. The emotional state\nof the player is recognized through voice analysis. A total of 20 subjects\nplayed a video game designed to practice basic math skills; for each trial, a\nstudent plays two times in a row the same game but each time the game was\ncontrolled by one of the two approaches ---Dynamic Difficulty Adjustment (DDA)\nand IC, the playing order was assigned randomly. Results show that when the\nproposed approach is used the participants changed faster from Unpleasant--low\nto pleasant or high emotions, and reached softly and kept in the flow zone.\nThese experiments demonstrate that the inductive control technique improves the\nlearning effectiveness through detection and stimulation of positive emotions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 12:49:21 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 18:10:24 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Lara-Alvarez", "Carlos", ""], ["Mitre-Hernandez", "Hugo", ""], ["Flores", "Juan", ""], ["Fuentes", "Maria", ""]]}, {"id": "1709.00965", "submitter": "Jens Grubert", "authors": "Daniel Schneider and Jens Grubert", "title": "Feasibility of Corneal Imaging for Handheld Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones are a popular device class for mobile Augmented Reality but\nsuffer from a limited input space. Around-device interaction techniques aim at\nextending this input space using various sensing modalities. In this paper we\npresent our work towards extending the input area of mobile devices using\nfront-facing device-centered cameras that capture reflections in the cornea. As\ncurrent generation mobile devices lack high resolution front-facing cameras, we\nstudy the feasibility of around-device interaction using corneal reflective\nimaging based on a high resolution camera. We present a workflow, a technical\nprototype and a feasibility evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 14:00:59 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Schneider", "Daniel", ""], ["Grubert", "Jens", ""]]}, {"id": "1709.00966", "submitter": "Jens Grubert", "authors": "Daniel Schneider and Jens Grubert", "title": "Towards Around-Device Interaction using Corneal Imaging", "comments": null, "journal-ref": null, "doi": "10.1145/3132272.3134127", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Around-device interaction techniques aim at extending the input space using\nvarious sensing modalities on mobile and wearable devices. In this paper, we\npresent our work towards extending the input area of mobile devices using\nfront-facing device-centered cameras that capture reflections in the human eye.\nAs current generation mobile devices lack high resolution front-facing cameras\nwe study the feasibility of around-device interaction using corneal reflective\nimaging based on a high resolution camera. We present a workflow, a technical\nprototype and an evaluation, including a migration path from high resolution to\nlow resolution imagers. Our study indicates, that under optimal conditions a\nspatial sensing resolution of 5 cm in the vicinity of a mobile phone is\npossible.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 14:01:45 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Schneider", "Daniel", ""], ["Grubert", "Jens", ""]]}, {"id": "1709.01007", "submitter": "Kathrin Ballweg", "authors": "Kathrin Ballweg, Margit Pohl, G\\\"unter Wallner, Tatiana von\n  Landesberger", "title": "Visual Similarity Perception of Directed Acyclic Graphs: A Study on\n  Influencing Factors", "comments": "Graph Drawing 2017 - arXiv Version; Keywords: Graphs, Perception,\n  Similarity, Comparison, Visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While visual comparison of directed acyclic graphs (DAGs) is commonly\nencountered in various disciplines (e.g., finance, biology), knowledge about\nhumans' perception of graph similarity is currently quite limited. By graph\nsimilarity perception we mean how humans perceive commonalities and differences\nin graphs and herewith come to a similarity judgment. As a step toward filling\nthis gap the study reported in this paper strives to identify factors which\ninfluence the similarity perception of DAGs. In particular, we conducted a\ncard-sorting study employing a qualitative and quantitative analysis approach\nto identify 1) groups of DAGs that are perceived as similar by the participants\nand 2) the reasons behind their choice of groups. Our results suggest that\nsimilarity is mainly influenced by the number of levels, the number of nodes on\na level, and the overall shape of the graph.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 15:44:23 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 21:36:08 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Ballweg", "Kathrin", ""], ["Pohl", "Margit", ""], ["Wallner", "G\u00fcnter", ""], ["von Landesberger", "Tatiana", ""]]}, {"id": "1709.01020", "submitter": "Jens Grubert", "authors": "Jens Grubert", "title": "Die Zukunft sehen: Die Chancen und Herausforderungen der Erweiterten und\n  Virtuellen Realit\\\"at f\\\"ur industrielle Anwendungen", "comments": "in German", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitalization offers chances as well as risks for industrial companies. This\narticle describes how the area of Mixed Reality, with its manifestations\nAugmented and Virtual Reality, can support industrial applications in the age\nof digitalization. Starting from a historical perspective on Augmented and\nVirtual Reality, this article surveys recent developments in the domain of\nMixed Reality, relevant for industrial use cases.\n  ---\n  Die Digitalisierung bietet f\\\"ur Industrieunternehmen neue Chancen, stellt\ndiese jedoch auch vor Herausforderungen. Dieser Artikel beleuchtet wie das\nGebiet der vermischten Realit\\\"at mit seinen Auspr\\\"agungen der erweiterten\nRealit\\\"at und der virtuellen Realit\\\"at f\\\"ur industriellen Anwendungen im\nZeitalter der Digitalisierung Vorteile schaffen kann. Ausgehend von einer\nhistorischen Betrachtung, werden aktuelle Entwicklungen auf dem Gebiet der\nerweiterten und virtuellen Realit\\\"at diskutiert.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 16:07:35 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Grubert", "Jens", ""]]}, {"id": "1709.01116", "submitter": "Dimitrios Adamos Dr", "authors": "Fotis Kalaganis (1), Dimitrios A. Adamos (2 and 3), Nikos Laskaris (1\n  and 3) ((1) AIIA Lab, Department of Informatics, Aristotle University of\n  Thessaloniki, (2) School of Music Studies, Aristotle University of\n  Thessaloniki, (3) Neuroinformatics GRoup, Aristotle University of\n  Thessaloniki)", "title": "Musical NeuroPicks: a consumer-grade BCI for on-demand music streaming\n  services", "comments": null, "journal-ref": "Neurocomputing 2017", "doi": null, "report-no": null, "categories": "q-bio.NC cs.CY cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigated the possibility of using a machine-learning scheme in\nconjunction with commercial wearable EEG-devices for translating listener's\nsubjective experience of music into scores that can be used in popular\non-demand music streaming services. Our study resulted into two variants,\ndiffering in terms of performance and execution time, and hence, subserving\ndistinct applications in online streaming music platforms. The first method,\nNeuroPicks, is extremely accurate but slower. It is based on the\nwell-established neuroscientific concepts of brainwave frequency bands,\nactivation asymmetry index and cross frequency coupling (CFC). The second\nmethod, NeuroPicksVQ, offers prompt predictions of lower credibility and relies\non a custom-built version of vector quantization procedure that facilitates a\nnovel parameterization of the music-modulated brainwaves. Beyond the feature\nengineering step, both methods exploit the inherent efficiency of extreme\nlearning machines (ELMs) so as to translate, in a personalized fashion, the\nderived patterns into a listener's score. NeuroPicks method may find\napplications as an integral part of contemporary music recommendation systems,\nwhile NeuroPicksVQ can control the selection of music tracks. Encouraging\nexperimental results, from a pragmatic use of the systems, are presented.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 18:55:35 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Kalaganis", "Fotis", "", "2 and 3"], ["Adamos", "Dimitrios A.", "", "2 and 3"], ["Laskaris", "Nikos", "", "1\n  and 3"]]}, {"id": "1709.01188", "submitter": "Zhichao Hu", "authors": "Zhichao Hu, Marilyn A. Walker, Michael Neff and Jean E. Fox Tree", "title": "Storytelling Agents with Personality and Adaptivity", "comments": "Related dataset: https://nlds.soe.ucsc.edu/sdg", "journal-ref": "In International Conference on Intelligent Virtual Agents, pp.\n  181-193. Springer, Cham, 2015", "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the expression of personality and adaptivity through the gestures\nof virtual agents in a storytelling task. We conduct two experiments using four\ndifferent dialogic stories. We manipulate agent personality on the extraversion\nscale, whether the agents adapt to one another in their gestural performance\nand agent gender. Our results show that subjects are able to perceive the\nintended variation in extraversion between different virtual agents,\nindependently of the story they are telling and the gender of the agent. A\nsecond study shows that subjects also prefer adaptive to nonadaptive virtual\nagents.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 23:06:05 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Hu", "Zhichao", ""], ["Walker", "Marilyn A.", ""], ["Neff", "Michael", ""], ["Tree", "Jean E. Fox", ""]]}, {"id": "1709.01293", "submitter": "Jens Grubert", "authors": "Olivier Balet and Boriana Koleva and Jens Grubert and Kwang Moo Yi and\n  Marco Gunia and Angelos Katsis and Julien Castet", "title": "Authoring and Living Next-Generation Location-Based Experiences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authoring location-based experiences involving multiple participants,\ncollaborating or competing in both indoor and outdoor mixed realities, is\nextremely complex and bound to serious technical challenges. In this work, we\npresent the first results of the MAGELLAN European project and how these\ngreatly simplify this creative process using novel authoring, augmented reality\n(AR) and indoor geolocalisation techniques.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 09:04:05 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Balet", "Olivier", ""], ["Koleva", "Boriana", ""], ["Grubert", "Jens", ""], ["Yi", "Kwang Moo", ""], ["Gunia", "Marco", ""], ["Katsis", "Angelos", ""], ["Castet", "Julien", ""]]}, {"id": "1709.01613", "submitter": "Hedvig Kjellstr\\\"om", "authors": "Patrik Jonell, Joseph Mendelson, Thomas Storskog, Goran Hagman, Per\n  Ostberg, Iolanda Leite, Taras Kucherenko, Olga Mikheeva, Ulrika Akenine,\n  Vesna Jelic, Alina Solomon, Jonas Beskow, Joakim Gustafson, Miia Kivipelto,\n  Hedvig Kjellstrom", "title": "Machine Learning and Social Robotics for Detecting Early Signs of\n  Dementia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the EACare project, an ambitious multi-disciplinary\ncollaboration with the aim to develop an embodied system, capable of carrying\nout neuropsychological tests to detect early signs of dementia, e.g., due to\nAlzheimer's disease. The system will use methods from Machine Learning and\nSocial Robotics, and be trained with examples of recorded clinician-patient\ninteractions. The interaction will be developed using a participatory design\napproach. We describe the scope and method of the project, and report on a\nfirst Wizard of Oz prototype.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 22:27:27 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Jonell", "Patrik", ""], ["Mendelson", "Joseph", ""], ["Storskog", "Thomas", ""], ["Hagman", "Goran", ""], ["Ostberg", "Per", ""], ["Leite", "Iolanda", ""], ["Kucherenko", "Taras", ""], ["Mikheeva", "Olga", ""], ["Akenine", "Ulrika", ""], ["Jelic", "Vesna", ""], ["Solomon", "Alina", ""], ["Beskow", "Jonas", ""], ["Gustafson", "Joakim", ""], ["Kivipelto", "Miia", ""], ["Kjellstrom", "Hedvig", ""]]}, {"id": "1709.01683", "submitter": "Abhinav Shukla", "authors": "Abhinav Shukla, Shruti Shriya Gullapuram, Harish Katti, Karthik\n  Yadati, Mohan Kankanhalli, Ramanathan Subramanian", "title": "Affect Recognition in Ads with Application to Computational Advertising", "comments": "Accepted at the ACM International Conference on Multimedia (ACM MM)\n  2017", "journal-ref": null, "doi": "10.1145/3123266.3123444", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advertisements (ads) often include strongly emotional content to leave a\nlasting impression on the viewer. This work (i) compiles an affective ad\ndataset capable of evoking coherent emotions across users, as determined from\nthe affective opinions of five experts and 14 annotators; (ii) explores the\nefficacy of convolutional neural network (CNN) features for encoding emotions,\nand observes that CNN features outperform low-level audio-visual emotion\ndescriptors upon extensive experimentation; and (iii) demonstrates how enhanced\naffect prediction facilitates computational advertising, and leads to better\nviewing experience while watching an online video stream embedded with ads\nbased on a study involving 17 users. We model ad emotions based on subjective\nhuman opinions as well as objective multimodal features, and show how\neffectively modeling ad emotions can positively impact a real-life application.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 06:16:52 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Shukla", "Abhinav", ""], ["Gullapuram", "Shruti Shriya", ""], ["Katti", "Harish", ""], ["Yadati", "Karthik", ""], ["Kankanhalli", "Mohan", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1709.01684", "submitter": "Abhinav Shukla", "authors": "Abhinav Shukla, Shruti Shriya Gullapuram, Harish Katti, Karthik\n  Yadati, Mohan Kankanhalli, Ramanathan Subramanian", "title": "Evaluating Content-centric vs User-centric Ad Affect Recognition", "comments": "Accepted at the ACM International Conference on Multimodal Interation\n  (ICMI) 2017", "journal-ref": null, "doi": "10.1145/3136755.3136796", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that advertisements (ads) often include strongly emotional\ncontent, very little work has been devoted to affect recognition (AR) from ads.\nThis work explicitly compares content-centric and user-centric ad AR\nmethodologies, and evaluates the impact of enhanced AR on computational\nadvertising via a user study. Specifically, we (1) compile an affective ad\ndataset capable of evoking coherent emotions across users; (2) explore the\nefficacy of content-centric convolutional neural network (CNN) features for\nencoding emotions, and show that CNN features outperform low-level emotion\ndescriptors; (3) examine user-centered ad AR by analyzing Electroencephalogram\n(EEG) responses acquired from eleven viewers, and find that EEG signals encode\nemotional information better than content descriptors; (4) investigate the\nrelationship between objective AR and subjective viewer experience while\nwatching an ad-embedded online video stream based on a study involving 12\nusers. To our knowledge, this is the first work to (a) expressly compare user\nvs content-centered AR for ads, and (b) study the relationship between modeling\nof ad emotions and its impact on a real-life advertising application.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 06:17:06 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Shukla", "Abhinav", ""], ["Gullapuram", "Shruti Shriya", ""], ["Katti", "Harish", ""], ["Yadati", "Karthik", ""], ["Kankanhalli", "Mohan", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1709.01775", "submitter": "Ekta Vats", "authors": "Ekta Vats and Anders Hast", "title": "On-the-fly Historical Handwritten Text Annotation", "comments": null, "journal-ref": "14th IAPR International Conference on Document Analysis and\n  Recognition (ICDAR), Volume 8, IEEE, Kyoto, Japan, 2017, pp. 10-14", "doi": "10.1109/ICDAR.2017.374", "report-no": null, "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of information retrieval algorithms depends upon the\navailability of ground truth labels annotated by experts. This is an important\nprerequisite, and difficulties arise when the annotated ground truth labels are\nincorrect or incomplete due to high levels of degradation. To address this\nproblem, this paper presents a simple method to perform on-the-fly annotation\nof degraded historical handwritten text in ancient manuscripts. The proposed\nmethod aims at quick generation of ground truth and correction of inaccurate\nannotations such that the bounding box perfectly encapsulates the word, and\ncontains no added noise from the background or surroundings. This method will\npotentially be of help to historians and researchers in generating and\ncorrecting word labels in a document dynamically. The effectiveness of the\nannotation method is empirically evaluated on an archival manuscript collection\nfrom well-known publicly available datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:27:41 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Vats", "Ekta", ""], ["Hast", "Anders", ""]]}, {"id": "1709.01779", "submitter": "Filipe Rodrigues", "authors": "Filipe Rodrigues and Francisco Pereira", "title": "Deep learning from crowds", "comments": "10 pages, The Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, deep learning has revolutionized the field of\nmachine learning by dramatically improving the state-of-the-art in various\ndomains. However, as the size of supervised artificial neural networks grows,\ntypically so does the need for larger labeled datasets. Recently, crowdsourcing\nhas established itself as an efficient and cost-effective solution for labeling\nlarge sets of data in a scalable manner, but it often requires aggregating\nlabels from multiple noisy contributors with different levels of expertise. In\nthis paper, we address the problem of learning deep neural networks from\ncrowds. We begin by describing an EM algorithm for jointly learning the\nparameters of the network and the reliabilities of the annotators. Then, a\nnovel general-purpose crowd layer is proposed, which allows us to train deep\nneural networks end-to-end, directly from the noisy labels of multiple\nannotators, using only backpropagation. We empirically show that the proposed\napproach is able to internally capture the reliability and biases of different\nannotators and achieve new state-of-the-art results for various crowdsourced\ndatasets across different settings, namely classification, regression and\nsequence labeling.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:41:19 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 12:30:12 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Rodrigues", "Filipe", ""], ["Pereira", "Francisco", ""]]}, {"id": "1709.01796", "submitter": "Daniel Sonntag", "authors": "Daniel Sonntag", "title": "Interakt---A Multimodal Multisensory Interactive Cognitive Assessment\n  Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive assistance may be valuable in applications for doctors and\ntherapists that reduce costs and improve quality in healthcare systems. Use\ncases and scenarios include the assessment of dementia. In this paper, we\npresent our approach to the (semi-)automatic assessment of dementia.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 12:10:11 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Sonntag", "Daniel", ""]]}, {"id": "1709.02014", "submitter": "Andr\\'es Lucero", "authors": "Andr\\'es Lucero and Marcos Serrano", "title": "Towards Proxemic Mobile Collocated Interactions", "comments": "10 pages", "journal-ref": "Andr\\'es Lucero & Marcos Serrano. (2017). Towards Proxemic Mobile\n  Collocated Interactions. International Journal of Mobile Human Computer\n  Interaction (IJMHCI) 9 (4), 15-24", "doi": "10.4018/IJMHCI.2017100102", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on mobile collocated interactions has been exploring situations\nwhere collocated users engage in collaborative activities using their personal\nmobile devices (e.g., smartphones and tablets), thus going from\npersonal/individual toward shared/multiuser experiences and interactions. The\nproliferation of ever-smaller computers that can be worn on our wrists (e.g.,\nApple Watch) and other parts of the body (e.g., Google Glass), have expanded\nthe possibilities and increased the complexity of interaction in what we term\nmobile collocated situations. Research on F-formations (or facing formations)\nhas been conducted in traditional settings (e.g., home, office, parties) where\nthe context and the presence of physical elements (e.g., furniture) can\nstrongly influence the way people socially interact with each other. While we\nmay be aware of how people arrange themselves spatially and interact with each\nother at a dinner table, in a classroom, or at a waiting room in a hospital,\nthere are other less-structured, dynamic, and larger-scale spaces that present\ndifferent types of challenges and opportunities for technology to enrich how\npeople experience these (semi-) public spaces. In this article, the authors\nexplore proxemic mobile collocated interactions by looking at F-formations in\nthe wild. They discuss recent efforts to observe how people socially interact\nin dynamic, unstructured, non-traditional settings. The authors also report the\nresults of exploratory F-formation observations conducted in the wild (i.e.,\ntourist attraction).\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 22:13:45 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Lucero", "Andr\u00e9s", ""], ["Serrano", "Marcos", ""]]}, {"id": "1709.02414", "submitter": "Taylan Sen", "authors": "Tayan Sen, Md Kamrul Hasan, Zach Teicher, M. Ehsan Hoque", "title": "Automated Dyadic Data Recorder (ADDR) Framework and Analysis of Facial\n  Cues in Deceptive Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed an online framework that can automatically pair two\ncrowd-sourced participants, prompt them to follow a research protocol, and\nrecord their audio and video on a remote server. The framework comprises two\nweb applications: an Automatic Quality Gatekeeper for ensuring only high\nquality crowd-sourced participants are recruited for the study, and a Session\nController which directs participants to play a research protocol, such as an\ninterrogation game. This framework was used to run a research study for\nanalyzing facial expressions during honest and deceptive communication using a\nnovel interrogation protocol. The protocol gathers two sets of nonverbal facial\ncues in participants: features expressed during questions relating to the\ninterrogation topic and features expressed during control questions. The\nframework and protocol were used to gather 151 dyadic conversations (1.3\nmillion video frames). Interrogators who were lied to expressed the\nsmile-related lip corner puller cue more often than interrogators who were\nbeing told the truth, suggesting that facial cues from interrogators may be\nuseful in evaluating the honesty of witnesses in some contexts. Overall, these\nresults demonstrate that this framework is capable of gathering high quality\ndata which can identify statistically significant results in a communication\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 19:25:26 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Sen", "Tayan", ""], ["Hasan", "Md Kamrul", ""], ["Teicher", "Zach", ""], ["Hoque", "M. Ehsan", ""]]}, {"id": "1709.02482", "submitter": "Timnit Gebru", "authors": "Timnit Gebru, Jonathan Krause, Jia Deng, Li Fei-Fei", "title": "Scalable Annotation of Fine-Grained Categories Without Experts", "comments": "CHI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a crowdsourcing workflow to collect image annotations for visually\nsimilar synthetic categories without requiring experts. In animals, there is a\ndirect link between taxonomy and visual similarity: e.g. a collie (type of dog)\nlooks more similar to other collies (e.g. smooth collie) than a greyhound\n(another type of dog). However, in synthetic categories such as cars, objects\nwith similar taxonomy can have very different appearance: e.g. a 2011 Ford\nF-150 Supercrew-HD looks the same as a 2011 Ford F-150 Supercrew-LL but very\ndifferent from a 2011 Ford F-150 Supercrew-SVT. We introduce a graph based\ncrowdsourcing algorithm to automatically group visually indistinguishable\nobjects together. Using our workflow, we label 712,430 images by ~1,000 Amazon\nMechanical Turk workers; resulting in the largest fine-grained visual dataset\nreported to date with 2,657 categories of cars annotated at 1/20th the cost of\nhiring experts.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 23:08:26 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Gebru", "Timnit", ""], ["Krause", "Jonathan", ""], ["Deng", "Jia", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1709.02541", "submitter": "Siqi Wu", "authors": "Siqi Wu, Marian-Andrei Rizoiu, Lexing Xie", "title": "Beyond Views: Measuring and Predicting Engagement in Online Videos", "comments": "Proceedings of ICWSM 2018, the code and datasets are publicly\n  available at https://github.com/avalanchesiqi/youtube-engagement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The share of videos in the internet traffic has been growing, therefore\nunderstanding how videos capture attention on a global scale is also of growing\nimportance. Most current research focus on modeling the number of views, but we\nargue that video engagement, or time spent watching is a more appropriate\nmeasure for resource allocation problems in attention, networking, and\npromotion activities. In this paper, we present a first large-scale measurement\nof video-level aggregate engagement from publicly available data streams, on a\ncollection of 5.3 million YouTube videos published over two months in 2016. We\nstudy a set of metrics including time and the average percentage of a video\nwatched. We define a new metric, relative engagement, that is calibrated\nagainst video properties and strongly correlate with recognized notions of\nquality. Moreover, we find that engagement measures of a video are stable over\ntime, thus separating the concerns for modeling engagement and those for\npopularity -- the latter is known to be unstable over time and driven by\nexternal promotions. We also find engagement metrics predictable from a\ncold-start setup, having most of its variance explained by video context,\ntopics and channel information -- R2=0.77. Our observations imply several\nprospective uses of engagement metrics -- choosing engaging topics for video\nproduction, or promoting engaging videos in recommender systems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 05:02:28 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 02:20:06 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 11:56:39 GMT"}, {"version": "v4", "created": "Tue, 10 Apr 2018 21:44:36 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Wu", "Siqi", ""], ["Rizoiu", "Marian-Andrei", ""], ["Xie", "Lexing", ""]]}, {"id": "1709.02737", "submitter": "Mahdi Miraz", "authors": "Mahdi H. Miraz, Peter Excell and and Maaruf Ali", "title": "User Interface (UI) Design Issues for the Multilingual Users: A Case\n  Study", "comments": null, "journal-ref": "Universal Access in the Information Society, Print ISSN:\n  1615-5289, E- ISSN: 1615-5297, August 2016, Vol. 15, No. 3, pp 431-444,\n  Published by Springer-Verlag 10.1007/s10209-014-0397-5", "doi": "10.1007/s10209-014-0397-5", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A multitude of web and desktop applications are now widely available in\ndiverse human languages. This paper explores the design issues that are\nspecifically relevant for multilingual users. It reports on the continued\nstudies of Information System (IS) issues and users' behaviour across\ncross-cultural and transnational boundaries. Taking the BBC website as a model\nthat is internationally recognised, usability tests were conducted to compare\ndifferent versions of the website. The dependant variables derived from the\nquestionnaire were analysed (via descriptive statistics) to elucidate the\nmultilingual UI design issues. Using Principal Component Analysis (PCA), five\nde-correlated variables were identified which were then used for hypotheses\ntests. A modified version of Herzberg's Hygiene-motivational Theory about the\nWorkplace was applied to assess the components used in the website. Overall, it\nwas concluded that the English versions of the website gave superior usability\nresults and this implies the need for deeper study of the problems in usability\nof the translated versions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 14:32:01 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Miraz", "Mahdi H.", ""], ["Excell", "Peter", ""], ["Ali", "and Maaruf", ""]]}, {"id": "1709.02739", "submitter": "James Bagrow", "authors": "Mark D. Wagy, Josh C. Bongard, James P. Bagrow and Paul D. H. Hines", "title": "Crowdsourcing Predictors of Residential Electric Energy Usage", "comments": "11 pages, 7 figures", "journal-ref": "IEEE Systems Journal, 2018", "doi": "10.1109/JSYST.2017.2778144", "report-no": null, "categories": "cs.HC physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has been successfully applied in many domains including\nastronomy, cryptography and biology. In order to test its potential for useful\napplication in a Smart Grid context, this paper investigates the extent to\nwhich a crowd can contribute predictive hypotheses to a model of residential\nelectric energy consumption. In this experiment, the crowd generated hypotheses\nabout factors that make one home different from another in terms of monthly\nenergy usage. To implement this concept, we deployed a web-based system within\nwhich 627 residential electricity customers posed 632 questions that they\nthought predictive of energy usage. While this occurred, the same group\nprovided 110,573 answers to these questions as they accumulated. Thus users\nboth suggested the hypotheses that drive a predictive model and provided the\ndata upon which the model is built. We used the resulting question and answer\ndata to build a predictive model of monthly electric energy consumption, using\nrandom forest regression. Because of the sparse nature of the answer data,\ncareful statistical work was needed to ensure that these models are valid. The\nresults indicate that the crowd can generate useful hypotheses, despite the\nsparse nature of the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 15:17:14 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Wagy", "Mark D.", ""], ["Bongard", "Josh C.", ""], ["Bagrow", "James P.", ""], ["Hines", "Paul D. H.", ""]]}, {"id": "1709.03008", "submitter": "Patrick O. Glauner", "authors": "Patrick Glauner, Niklas Dahringer, Oleksandr Puhachov, Jorge Augusto\n  Meira, Petko Valtchev, Radu State, Diogo Duarte", "title": "Identifying Irregular Power Usage by Turning Predictions into\n  Holographic Spatial Visualizations", "comments": "Proceedings of the 17th IEEE International Conference on Data Mining\n  Workshops (ICDMW 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power grids are critical infrastructure assets that face non-technical losses\n(NTL) such as electricity theft or faulty meters. NTL may range up to 40% of\nthe total electricity distributed in emerging countries. Industrial NTL\ndetection systems are still largely based on expert knowledge when deciding\nwhether to carry out costly on-site inspections of customers. Electricity\nproviders are reluctant to move to large-scale deployments of automated systems\nthat learn NTL profiles from data due to the latter's propensity to suggest a\nlarge number of unnecessary inspections. In this paper, we propose a novel\nsystem that combines automated statistical decision making with expert\nknowledge. First, we propose a machine learning framework that classifies\ncustomers into NTL or non-NTL using a variety of features derived from the\ncustomers' consumption data. The methodology used is specifically tailored to\nthe level of noise in the data. Second, in order to allow human experts to feed\ntheir knowledge in the decision loop, we propose a method for visualizing\nprediction results at various granularity levels in a spatial hologram. Our\napproach allows domain experts to put the classification results into the\ncontext of the data and to incorporate their knowledge for making the final\ndecisions of which customers to inspect. This work has resulted in appreciable\nresults on a real-world data set of 3.6M customers. Our system is being\ndeployed in a commercial NTL detection software.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 21:27:06 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Glauner", "Patrick", ""], ["Dahringer", "Niklas", ""], ["Puhachov", "Oleksandr", ""], ["Meira", "Jorge Augusto", ""], ["Valtchev", "Petko", ""], ["State", "Radu", ""], ["Duarte", "Diogo", ""]]}, {"id": "1709.03252", "submitter": "Ehsan Arbabi", "authors": "Ehsan Arbabi and Mohammad Bagher Shamsollahi", "title": "Evaluation of Classical Features and Classifiers in Brain-Computer\n  Interface Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-Computer Interface (BCI) uses brain signals in order to provide a new\nmethod for communication between human and outside world. Feature extraction,\nselection and classification are among the main matters of concerns in signal\nprocessing stage of BCI. In this article, we present our findings about the\nmost effective features and classifiers in some brain tasks. Six different\ngroups of classical features and twelve classifiers have been examined in nine\ndatasets of brain signal. The results indicate that energy of brain signals in\n{\\alpha} and \\b{eta} frequency bands, together with some statistical parameters\nare more effective, comparing to the other types of extracted features. In\naddition, Bayesian classifier with Gaussian distribution assumption and also\nSupport Vector Machine (SVM) show to classify different BCI datasets more\naccurately than the other classifiers. We believe that the results can give an\ninsight about a strategy for blind classification of brain signals in\nbrain-computer interface.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 05:57:07 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 07:42:32 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Arbabi", "Ehsan", ""], ["Shamsollahi", "Mohammad Bagher", ""]]}, {"id": "1709.03495", "submitter": "Tony T. Luo", "authors": "Tie Luo and Leonit Zeynalvand", "title": "Reshaping Mobile Crowd Sensing using Cross Validation to Improve Data\n  Credibility", "comments": "To be published in the Proceedings of IEEE GLOBECOM, December 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data credibility is a crucial issue in mobile crowd sensing (MCS) and, more\ngenerally, people-centric Internet of Things (IoT). Prior work takes approaches\nsuch as incentive mechanism design and data mining to address this issue, while\noverlooking the power of crowds itself, which we exploit in this paper. In\nparticular, we propose a cross validation approach which seeks a validating\ncrowd to verify the data credibility of the original sensing crowd, and uses\nthe verification result to reshape the original sensing dataset into a more\ncredible posterior belief of the ground truth. Following this approach, we\ndesign a specific cross validation mechanism, which integrates four sampling\ntechniques with a privacy-aware competency-adaptive push (PACAP) algorithm and\nis applicable to time-sensitive and quality-critical MCS applications. It does\nnot require redesigning a new MCS system but rather functions as a lightweight\n\"plug-in\", making it easier for practical adoption. Our results demonstrate\nthat the proposed mechanism substantially improves data credibility in terms of\nboth reinforcing obscure truths and scavenging hidden truths.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 06:18:56 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Luo", "Tie", ""], ["Zeynalvand", "Leonit", ""]]}, {"id": "1709.03577", "submitter": "Niranjan Bidargaddi", "authors": "Niranjan Bidargaddi", "title": "Learning from development of a third-party patient-oriented application\n  using Australian national personal health records system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale national level Personal Health Record (PHR) has been implemented\nin Australia. However, usability, data quality and poor functionalities have\nresulted in low utility affecting enrollment and participation rates by both\npatients and clinicians alike. Development of new applications deriving\nsecondary utility of data can enhance use of PHRs but there is limited\nunderstanding on processes involved in development of third-party applications\nwith nationally run PHRs. This paper prsents an analysis of processes and\nregulatory requirements for developing applications of data from My Health\nRecord, Australian nationally run PHR and subsequently implementation of a\npatient oriented software application using data sourced from My Health Record.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 18:16:28 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Bidargaddi", "Niranjan", ""]]}, {"id": "1709.03717", "submitter": "Ehsan Arbabi", "authors": "Ehsan Arbabi, Mohammad Shabani and Ali Yarigholi", "title": "A low cost non-wearable gaze detection system based on infrared image\n  processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human eye gaze detection plays an important role in various fields, including\nhuman-computer interaction, virtual reality and cognitive science. Although\ndifferent relatively accurate systems of eye tracking and gaze detection exist,\nthey are usually either too expensive to be bought for low cost applications or\ntoo complex to be implemented easily. In this article, we propose a\nnon-wearable system for eye tracking and gaze detection with low complexity and\ncost. The proposed system provides a medium accuracy which makes it suitable\nfor general applications in which low cost and easy implementation is more\nimportant than achieving very precise gaze detection. The proposed method\nincludes pupil and marker detection using infrared image processing, and gaze\nevaluation using an interpolation-based strategy. The interpolation-based\nstrategy exploits the positions of the detected pupils and markers in a target\ncaptured image and also in some previously captured training images for\nestimating the position of a point that the user is gazing at. The proposed\nsystem has been evaluated by three users in two different lighting conditions.\nThe experimental results show that the accuracy of this low cost system can be\nbetween 90% and 100% for finding major gazing directions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 07:46:05 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Arbabi", "Ehsan", ""], ["Shabani", "Mohammad", ""], ["Yarigholi", "Ali", ""]]}, {"id": "1709.03730", "submitter": "Huijun Wu", "authors": "Huijun Wu, Chen Wang, Jie Yin, Kai Lu, Liming Zhu", "title": "Interpreting Shared Deep Learning Models via Explicable Boundary Trees", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite outperforming the human in many tasks, deep neural network models are\nalso criticized for the lack of transparency and interpretability in decision\nmaking. The opaqueness results in uncertainty and low confidence when deploying\nsuch a model in model sharing scenarios, when the model is developed by a third\nparty. For a supervised machine learning model, sharing training process\nincluding training data provides an effective way to gain trust and to better\nunderstand model predictions. However, it is not always possible to share all\ntraining data due to privacy and policy constraints. In this paper, we propose\na method to disclose a small set of training data that is just sufficient for\nusers to get the insight of a complicated model. The method constructs a\nboundary tree using selected training data and the tree is able to approximate\nthe complicated model with high fidelity. We show that traversing data points\nin the tree gives users significantly better understanding of the model and\npaves the way for trustworthy model sharing.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 08:13:24 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Wu", "Huijun", ""], ["Wang", "Chen", ""], ["Yin", "Jie", ""], ["Lu", "Kai", ""], ["Zhu", "Liming", ""]]}, {"id": "1709.03968", "submitter": "Nabiha Asghar", "authors": "Nabiha Asghar, Pascal Poupart, Jesse Hoey, Xin Jiang, Lili Mou", "title": "Affective Neural Response Generation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing neural conversational models process natural language primarily on a\nlexico-syntactic level, thereby ignoring one of the most crucial components of\nhuman-to-human dialogue: its affective content. We take a step in this\ndirection by proposing three novel ways to incorporate affective/emotional\naspects into long short term memory (LSTM) encoder-decoder neural conversation\nmodels: (1) affective word embeddings, which are cognitively engineered, (2)\naffect-based objective functions that augment the standard cross-entropy loss,\nand (3) affectively diverse beam search for decoding. Experiments show that\nthese techniques improve the open-domain conversational prowess of\nencoder-decoder networks by enabling them to produce emotionally rich responses\nthat are more interesting and natural.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 17:41:30 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Asghar", "Nabiha", ""], ["Poupart", "Pascal", ""], ["Hoey", "Jesse", ""], ["Jiang", "Xin", ""], ["Mou", "Lili", ""]]}, {"id": "1709.04228", "submitter": "Fabrizio Frati", "authors": "Fabrizio Frati and Kwan-Liu Ma", "title": "Proceedings of the 25th International Symposium on Graph Drawing and\n  Network Visualization (GD 2017)", "comments": "Electronic self-archived proceedings. Proceedings are also to be\n  published by Springer in the Lecture Notes in Computer Science series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS cs.HC cs.SI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the arXiv index for the electronic proceedings of the 25th\nInternational Symposium on Graph Drawing and Network Visualization (GD 2017),\nwhich was held in Boston, U.S.A., September 25-27 2017. It contains the\npeer-reviewed and revised accepted papers with an optional appendix.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 10:05:37 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Frati", "Fabrizio", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "1709.04299", "submitter": "Jens Grubert", "authors": "Jens Grubert, Yuta Itoh, Kenneth Moser and J. Edward Swan II", "title": "A Survey of Calibration Methods for Optical See-Through Head-Mounted\n  Displays", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2017.2754257", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical see-through head-mounted displays (OST HMDs) are a major output\nmedium for Augmented Reality, which have seen significant growth in popularity\nand usage among the general public due to the growing release of\nconsumer-oriented models, such as the Microsoft Hololens. Unlike Virtual\nReality headsets, OST HMDs inherently support the addition of\ncomputer-generated graphics directly into the light path between a user's eyes\nand their view of the physical world. As with most Augmented and Virtual\nReality systems, the physical position of an OST HMD is typically determined by\nan external or embedded 6-Degree-of-Freedom tracking system. However, in order\nto properly render virtual objects, which are perceived as spatially aligned\nwith the physical environment, it is also necessary to accurately measure the\nposition of the user's eyes within the tracking system's coordinate frame. For\nover 20 years, researchers have proposed various calibration methods to\ndetermine this needed eye position. However, to date, there has not been a\ncomprehensive overview of these procedures and their requirements. Hence, this\npaper surveys the field of calibration methods for OST HMDs. Specifically, it\nprovides insights into the fundamentals of calibration techniques, and presents\nan overview of both manual and automatic approaches, as well as evaluation\nmethods and metrics. Finally, it also identifies opportunities for future\nresearch. % relative to the tracking coordinate system, and, hence, its\nposition in 3D space.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 12:55:45 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Grubert", "Jens", ""], ["Itoh", "Yuta", ""], ["Moser", "Kenneth", ""], ["Swan", "J. Edward", "II"]]}, {"id": "1709.04412", "submitter": "Przemyslaw Biecek", "authors": "Agnieszka Sitko, Przemyslaw Biecek", "title": "The Merging Path Plot: adaptive fusing of k-groups with likelihood-based\n  model selection", "comments": "Submitted to Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many statistical tests that verify the null hypothesis: the\nvariable of interest has the same distribution among k-groups. But once the\nnull hypothesis is rejected, how to present the structure of dissimilarity\nbetween groups? In this article, we introduce The Merging Path Plot - a\nmethodology, and factorMerger - an R package, for exploration and visualization\nof k-group dissimilarities. Comparison of k-groups is one of the most important\nissues in exploratory analyses and it has zillions of applications. The\nclassical solution is to test a~null hypothesis that observations from all\ngroups come from the same distribution. If the global null hypothesis is\nrejected, a~more detailed analysis of differences among pairs of groups is\nperformed. The traditional approach is to use pairwise post hoc tests in order\nto verify which groups differ significantly. However, this approach fails with\na large number of groups in both interpretation and visualization layer.\nThe~Merging Path Plot methodology solves this problem by using an\neasy-to-understand description of dissimilarity among groups based on\nLikelihood Ratio Test (LRT) statistic.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 16:46:11 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 09:12:21 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Sitko", "Agnieszka", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "1709.04453", "submitter": "Alexander Lex", "authors": "Yan Zheng, Yi Ou, Alexander Lex, Jeff M. Phillips", "title": "Visualization of Big Spatial Data using Coresets for Kernel Density\n  Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The size of large, geo-located datasets has reached scales where\nvisualization of all data points is inefficient. Random sampling is a method to\nreduce the size of a dataset, yet it can introduce unwanted errors. We describe\na method for subsampling of spatial data suitable for creating kernel density\nestimates from very large data and demonstrate that it results in less error\nthan random sampling. We also introduce a method to ensure that thresholding of\nlow values based on sampled data does not omit any regions above the desired\nthreshold when working with sampled data. We demonstrate the effectiveness of\nour approach using both, artificial and real-world large geospatial datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 17:55:59 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Zheng", "Yan", ""], ["Ou", "Yi", ""], ["Lex", "Alexander", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1709.04517", "submitter": "Kartik Talamadupula", "authors": "Tathagata Chakraborti and Kshitij P. Fadnis and Kartik Talamadupula\n  and Mishal Dholakia and Biplav Srivastava and Jeffrey O. Kephart and Rachel\n  K. E. Bellamy", "title": "Visualizations for an Explainable Planning Agent", "comments": "PREVIOUSLY Mr. Jones -- Towards a Proactive Smart Room Orchestrator\n  (appeared in AAAI 2017 Fall Symposium on Human-Agent Groups)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report on the visualization capabilities of an Explainable\nAI Planning (XAIP) agent that can support human in the loop decision making.\nImposing transparency and explainability requirements on such agents is\nespecially important in order to establish trust and common ground with the\nend-to-end automated planning system. Visualizing the agent's internal\ndecision-making processes is a crucial step towards achieving this. This may\ninclude externalizing the \"brain\" of the agent -- starting from its sensory\ninputs, to progressively higher order decisions made by it in order to drive\nits planning components. We also show how the planner can bootstrap on the\nlatest techniques in explainable planning to cast plan visualization as a plan\nexplanation problem, and thus provide concise model-based visualization of its\nplans. We demonstrate these functionalities in the context of the automated\nplanning components of a smart assistant in an instrumented meeting space.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 19:50:44 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 18:35:30 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Chakraborti", "Tathagata", ""], ["Fadnis", "Kshitij P.", ""], ["Talamadupula", "Kartik", ""], ["Dholakia", "Mishal", ""], ["Srivastava", "Biplav", ""], ["Kephart", "Jeffrey O.", ""], ["Bellamy", "Rachel K. E.", ""]]}, {"id": "1709.04574", "submitter": "Victor Shih", "authors": "Victor Shih, David C Jangraw, Paul Sajda, Sameer Saproo", "title": "Towards personalized human AI interaction - adapting the behavior of AI\n  agents using neural signatures of subjective interest", "comments": "11 pages, 9 figures, 1 table, Submitted to IEEE Trans. on Neural\n  Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning AI commonly uses reward/penalty signals that are\nobjective and explicit in an environment -- e.g. game score, completion time,\netc. -- in order to learn the optimal strategy for task performance. However,\nHuman-AI interaction for such AI agents should include additional reinforcement\nthat is implicit and subjective -- e.g. human preferences for certain AI\nbehavior -- in order to adapt the AI behavior to idiosyncratic human\npreferences. Such adaptations would mirror naturally occurring processes that\nincrease trust and comfort during social interactions. Here, we show how a\nhybrid brain-computer-interface (hBCI), which detects an individual's level of\ninterest in objects/events in a virtual environment, can be used to adapt the\nbehavior of a Deep Reinforcement Learning AI agent that is controlling a\nvirtual autonomous vehicle. Specifically, we show that the AI learns a driving\nstrategy that maintains a safe distance from a lead vehicle, and most novelly,\npreferentially slows the vehicle when the human passengers of the vehicle\nencounter objects of interest. This adaptation affords an additional 20\\%\nviewing time for subjectively interesting objects. This is the first\ndemonstration of how an hBCI can be used to provide implicit reinforcement to\nan AI agent in a way that incorporates user preferences into the control\nsystem.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 01:27:44 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Shih", "Victor", ""], ["Jangraw", "David C", ""], ["Sajda", "Paul", ""], ["Saproo", "Sameer", ""]]}, {"id": "1709.05021", "submitter": "Ervin Teng", "authors": "Ervin Teng, Jo\\~ao Diogo Falc\\~ao, Bob Iannucci", "title": "ClickBAIT: Click-based Accelerated Incremental Training of Convolutional\n  Neural Networks", "comments": "11 pages, 14 figures. Datasets available at\n  http://clickbait.crossmobile.info", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's general-purpose deep convolutional neural networks (CNN) for image\nclassification and object detection are trained offline on large static\ndatasets. Some applications, however, will require training in real-time on\nlive video streams with a human-in-the-loop. We refer to this class of problem\nas Time-ordered Online Training (ToOT) - these problems will require a\nconsideration of not only the quantity of incoming training data, but the human\neffort required to tag and use it. In this paper, we define training benefit as\na metric to measure the effectiveness of a sequence in using each user\ninteraction. We demonstrate and evaluate a system tailored to performing ToOT\nin the field, capable of training an image classifier on a live video stream\nthrough minimal input from a human operator. We show that by exploiting the\ntime-ordered nature of the video stream through optical flow-based object\ntracking, we can increase the effectiveness of human actions by about 8 times.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 00:58:38 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Teng", "Ervin", ""], ["Falc\u00e3o", "Jo\u00e3o Diogo", ""], ["Iannucci", "Bob", ""]]}, {"id": "1709.05168", "submitter": "Evgeny Krivosheev", "authors": "Evgeny Krivosheev, Fabio Casati, Valentina Caforio, Boualem Benatallah", "title": "Crowdsourcing Paper Screening in Systematic Literature Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature reviews allow scientists to stand on the shoulders of giants,\nshowing promising directions, summarizing progress, and pointing out existing\nchallenges in research. At the same time conducting a systematic literature\nreview is a laborious and consequently expensive process. In the last decade,\nthere have a few studies on crowdsourcing in literature reviews. This paper\nexplores the feasibility of crowdsourcing for facilitating the literature\nreview process in terms of results, time and effort, as well as to identify\nwhich crowdsourcing strategies provide the best results based on the budget\navailable. In particular we focus on the screening phase of the literature\nreview process and we contribute and assess methods for identifying the size of\ntests, labels required per paper, and classification functions as well as\nmethods to split the crowdsourcing process in phases to improve results.\nFinally, we present our findings based on experiments run on Crowdflower.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 11:59:15 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Krivosheev", "Evgeny", ""], ["Casati", "Fabio", ""], ["Caforio", "Valentina", ""], ["Benatallah", "Boualem", ""]]}, {"id": "1709.05298", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko, Ilya Markov and Maarten de Rijke", "title": "Conversational Exploratory Search via Interactive Storytelling", "comments": "Accepted at ICTIR'17 Workshop on Search-Oriented Conversational AI\n  (SCAI 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational interfaces are likely to become more efficient, intuitive and\nengaging way for human-computer interaction than today's text or touch-based\ninterfaces. Current research efforts concerning conversational interfaces focus\nprimarily on question answering functionality, thereby neglecting support for\nsearch activities beyond targeted information lookup. Users engage in\nexploratory search when they are unfamiliar with the domain of their goal,\nunsure about the ways to achieve their goals, or unsure about their goals in\nthe first place. Exploratory search is often supported by approaches from\ninformation visualization. However, such approaches cannot be directly\ntranslated to the setting of conversational search.\n  In this paper we investigate the affordances of interactive storytelling as a\ntool to enable exploratory search within the framework of a conversational\ninterface. Interactive storytelling provides a way to navigate a document\ncollection in the pace and order a user prefers. In our vision, interactive\nstorytelling is to be coupled with a dialogue-based system that provides verbal\nexplanations and responsive design. We discuss challenges and sketch the\nresearch agenda required to put this vision into life.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 16:42:17 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Markov", "Ilya", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1709.05723", "submitter": "Sean McKenna", "authors": "Sean McKenna, Alexander Lex, Miriah Meyer", "title": "Worksheets for Guiding Novices through the Visualization Design Process", "comments": "Pedagogy of Data Visualization Workshop, Pedagogy Data Vis., IEEE VIS\n  Workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For visualization pedagogy, an important but challenging notion to teach is\ndesign, from making to evaluating visualization encodings, user interactions,\nor data visualization systems. In our previous work, we introduced the design\nactivity framework to codify the high-level activities of the visualization\ndesign process. This framework has helped structure experts' design processes\nto create visualization systems, but the framework's four activities lack a\nbreakdown into steps with a concrete example to help novices utilizing this\nframework in their own real-world design process. To provide students with such\nconcrete guidelines, we created worksheets for each design activity:\nunderstand, ideate, make, and deploy. Each worksheet presents a high-level\nsummary of the activity with actionable, guided steps for a novice designer to\nfollow. We validated the use of this framework and the worksheets in a\ngraduate-level visualization course taught at our university. For this\nevaluation, we surveyed the class and conducted 13 student interviews to garner\nqualitative, open-ended feedback and suggestions on the worksheets. We conclude\nthis work with a discussion and highlight various areas for future work on\nimproving visualization design pedagogy.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 22:38:02 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["McKenna", "Sean", ""], ["Lex", "Alexander", ""], ["Meyer", "Miriah", ""]]}, {"id": "1709.05828", "submitter": "Jungkook Park", "authors": "Jungkook Park, Yeong Hoon Park, Alice Oh", "title": "Non-Linear Editor for Text-Based Screencast", "comments": "To appear in Adjunct Proceedings of the 30th Annual ACM Symposium on\n  User Interface Software & Technology (UIST 2017, Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screencasts, where computer screen is broadcast to a large audience on the\nweb, are becoming popular as an online educational tool. Among various types of\nscreencast content, popular are the contents that involve text editing,\nincluding computer programming. There are emerging platforms that support such\ntext-based screencasts by recording every character insertion/deletion from the\ncreator and reconstructing its playback on the viewer's screen. However, these\nplatforms lack rich support for creating and editing the screencast itself,\nmainly due to the difficulty of manipulating recorded text changes; the changes\nare tightly coupled in sequence, thus modifying arbitrary part of the sequence\nis not trivial. We present a non-linear editing tool for text-based\nscreencasts. With the proposed selective history rewrite process, our editor\nallows users to substitute an arbitrary part of a text-based screencast while\npreserving overall consistency of the rest of the text-based screencast.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 09:21:47 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Park", "Jungkook", ""], ["Park", "Yeong Hoon", ""], ["Oh", "Alice", ""]]}, {"id": "1709.06067", "submitter": "Michael Jones", "authors": "Michael Jones and Kevin Seppi", "title": "Sculpt, Deploy, Repeat: Fast Prototyping of Interactive Physical Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a deployable PhysiComp that merges form and function typically\ninvolves a significant investment of time and skill in digital electronics, 3D\nmodeling and mechanical design. We aim to help designers quickly create\nprototypes by removing technical barriers in that process. Other methods for\nconstructing PhysiComp prototypes either lack fidelity in representing shape\nand function or are confined to use in the studio next to a workstation, camera\nor projector system. Software 3D CAD tools can be used to design the shape but\ndo not provide immediate tactile feedback on fit and feel. In this work,\nsculpting around 3D printed replicas of electronics combines electronics and\nform in a fluid design environment. The sculptures are scanned, modified for\nassembly and then printed on a 3D printer. Using this process, functional\nprototypes can be created with about 4 hours of focused effort over a day and a\nhalf with most of that time spent waiting for the 3D printer. The process lends\nitself to concurrent exploration of several designs and to rapid iteration.\nThis allows the design process to converge quickly to a PhysiComp that is\ncomfortable and useful.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 17:44:23 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Jones", "Michael", ""], ["Seppi", "Kevin", ""]]}, {"id": "1709.06818", "submitter": "Zhibin Niu", "authors": "Yan Ji, Licheng Liu, Hongcui Wang, Zhilei Liu, Zhibin Niu, Bruce Denby", "title": "Updating the silent speech challenge benchmark with deep learning", "comments": "25 pages, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2010 Silent Speech Challenge benchmark is updated with new results\nobtained in a Deep Learning strategy, using the same input features and\ndecoding strategy as in the original article. A Word Error Rate of 6.4% is\nobtained, compared to the published value of 17.4%. Additional results\ncomparing new auto-encoder-based features with the original features at reduced\ndimensionality, as well as decoding scenarios on two different language models,\nare also presented. The Silent Speech Challenge archive has been updated to\ncontain both the original and the new auto-encoder features, in addition to the\noriginal raw data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 11:28:40 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Ji", "Yan", ""], ["Liu", "Licheng", ""], ["Wang", "Hongcui", ""], ["Liu", "Zhilei", ""], ["Niu", "Zhibin", ""], ["Denby", "Bruce", ""]]}, {"id": "1709.08165", "submitter": "Nalin Asanka Gamagedara Arachchilage", "authors": "Nicholas Micallef, Nalin Asanka Gamagedara Arachchilage", "title": "A Model for Enhancing Human Behaviour with Security Questions: A\n  Theoretical Perspective", "comments": "11, Australasian Conference on Information Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security questions are one of the mechanisms used to recover passwords.\nStrong answers to security questions (i.e. high entropy) are hard for attackers\nto guess or obtain using social engineering techniques (e.g. monitoring of\nsocial networking profiles), but at the same time are difficult to remember.\nInstead, weak answers to security questions (i.e. low entropy) are easy to\nremember, which makes them more vulnerable to cyber-attacks. Convenience leads\nusers to use the same answers to security questions on multiple accounts, which\nexposes these accounts to numerous cyber-threats. Hence, current security\nquestions implementations rarely achieve the required security and memorability\nrequirements. This research study is the first step in the development of a\nmodel which investigates the determinants that influence users' behavioural\nintentions through motivation to select strong and memorable answers to\nsecurity questions. This research also provides design recommendations for\nnovel security questions mechanisms.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 09:08:24 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Micallef", "Nicholas", ""], ["Arachchilage", "Nalin Asanka Gamagedara", ""]]}, {"id": "1709.08167", "submitter": "Nalin Asanka Gamagedara Arachchilage", "authors": "Nicholas Micallef, Nalin Asanka Gamagedara Arachchilage", "title": "A Serious Game Design: Nudging Users' Memorability of Security Questions", "comments": "11, Australasian Conference on Information Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security questions are one of the techniques used to recover passwords. The\nmain limitation of security questions is that users find strong answers\ndifficult to remember. This leads users to trade-off security for the\nconvenience of an improved memorability. Previous research found that increased\nfun and enjoyment can lead to an enhanced memorability, which provides a better\nlearning experience. Hence, we empirically investigate whether a serious game\nhas the potential of improving the memorability of strong answers to security\nquestions. For our serious game, we adapted the popular \"4 Pics 1 word\" mobile\ngame because of its use of pictures and cues, which psychology research found\nto be important to help with memorability. Our findings indicate that the\nproposed serious game could potentially improve the memorability of answers to\nsecurity questions. This potential improvement in memorability, could\neventually help reduce the trade-off between usability and security in\nfall-back authentication.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 09:19:18 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Micallef", "Nicholas", ""], ["Arachchilage", "Nalin Asanka Gamagedara", ""]]}, {"id": "1709.08546", "submitter": "Bahador Saket", "authors": "Bahador Saket, Alex Endert, Cagatay Demiralp", "title": "Task-Based Effectiveness of Basic Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizations of tabular data are widely used; understanding their\neffectiveness in different task and data contexts is fundamental to scaling\ntheir impact. However, little is known about how basic tabular data\nvisualizations perform across varying data analysis tasks and data attribute\ntypes. In this paper, we report results from a crowdsourced experiment to\nevaluate the effectiveness of five visualization types --- Table, Line Chart,\nBar Chart, Scatterplot, and Pie Chart --- across ten common data analysis tasks\nand three data attribute types using two real-world datasets. We found the\neffectiveness of these visualization types significantly varies across task and\ndata attribute types, suggesting that visualization design would benefit from\nconsidering context dependent effectiveness. Based on our findings, we derive\nrecommendations on which visualizations to choose based on different tasks. We\nfinally train a decision tree on the data we collected to drive a recommender,\nshowcasing how to effectively engineer experimental user data into practical\nvisualization systems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 15:17:57 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 17:14:18 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 16:34:09 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Saket", "Bahador", ""], ["Endert", "Alex", ""], ["Demiralp", "Cagatay", ""]]}, {"id": "1709.08590", "submitter": "Ali Al-Taei", "authors": "Ali Al-Taei", "title": "Ensemble Classifier for Eye State Classification using EEG Signals", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing importance and utilization of measuring brain waves (e.g. EEG\nsignals of eye state) in brain-computer interface (BCI) applications\nhighlighted the need for suitable classification methods. In this paper, a\ncomparison between three of well-known classification methods (i.e. support\nvector machine (SVM), hidden Markov map (HMM), and radial basis function (RBF))\nfor EEG based eye state classification was achieved. Furthermore, a suggested\nmethod that is based on ensemble model was tested. The suggested (ensemble\nsystem) method based on a voting algorithm with two kernels: random forest (RF)\nand Kstar classification methods. The performance was tested using three\nmeasurement parameters: accuracy, mean absolute error (MAE), and confusion\nmatrix. Results showed that the proposed method outperforms the other tested\nmethods. For instance, the suggested method's performance was 97.27% accuracy\nand 0.13 MAE.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 16:44:39 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 03:19:37 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Al-Taei", "Ali", ""]]}, {"id": "1709.08623", "submitter": "Nalin Asanka Gamagedara Arachchilage", "authors": "Nicholas Micallef, Nalin Asanka Gamagedara Arachchilage", "title": "Changing users' security behaviour towards security questions: A game\n  based learning approach", "comments": "6, Military Communications and Information Systems Conference\n  (MilCIS), 2017. arXiv admin note: substantial text overlap with\n  arXiv:1707.08073", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fallback authentication is used to retrieve forgotten passwords. Security\nquestions are one of the main techniques used to conduct fallback\nauthentication. In this paper, we propose a serious game design that uses\nsystem-generated security questions with the aim of improving the usability of\nfallback authentication. For this purpose, we adopted the popular picture-based\n\"4 Pics 1 word\" mobile game. This game was selected because of its use of\npictures and cues, which previous psychology research found to be crucial to\naid memorability. This game asks users to pick the word that relates to the\ngiven pictures. We then customized this game by adding features which help\nmaximize the following memory retrieval skills: (a) verbal cues - by providing\nhints with verbal descriptions, (b) spatial cues - by maintaining the same\norder of pictures, (c) graphical cues - by showing 4 images for each challenge,\n(d) interactivity/engaging nature of the game.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 08:57:32 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Micallef", "Nicholas", ""], ["Arachchilage", "Nalin Asanka Gamagedara", ""]]}, {"id": "1709.08774", "submitter": "Zhutian Chen", "authors": "Zhutian Chen, Yifang Wang, Tianchen Sun, Xiang Gao, Wei Chen, Zhigeng\n  Pan, Huamin Qu, Yingcai Wu", "title": "Exploring the Design Space of Immersive Urban Analytics", "comments": "23 pages,11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the rapid development and wide adoption of\nimmersive head-mounted devices, such as HTC VIVE, Oculus Rift, and Microsoft\nHoloLens. These immersive devices have the potential to significantly extend\nthe methodology of urban visual analytics by providing critical 3D context\ninformation and creating a sense of presence. In this paper, we propose an\ntheoretical model to characterize the visualizations in immersive urban\nanalytics. Further more, based on our comprehensive and concise model, we\ncontribute a typology of combination methods of 2D and 3D visualizations that\ndistinguish between linked views, embedded views, and mixed views. We also\npropose a supporting guideline to assist users in selecting a proper view under\ncertain circumstances by considering visual geometry and spatial distribution\nof the 2D and 3D visualizations. Finally, based on existing works, possible\nfuture research opportunities are explored and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 01:15:26 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Chen", "Zhutian", ""], ["Wang", "Yifang", ""], ["Sun", "Tianchen", ""], ["Gao", "Xiang", ""], ["Chen", "Wei", ""], ["Pan", "Zhigeng", ""], ["Qu", "Huamin", ""], ["Wu", "Yingcai", ""]]}, {"id": "1709.08820", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Lina Yao, Quan Z. Sheng, Salil S. Kanhere, Tao Gu, Dalin\n  Zhang", "title": "Converting Your Thoughts to Texts: Enabling Brain Typing via Deep\n  Feature Learning of EEG Signals", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An electroencephalography (EEG) based Brain Computer Interface (BCI) enables\npeople to communicate with the outside world by interpreting the EEG signals of\ntheir brains to interact with devices such as wheelchairs and intelligent\nrobots. More specifically, motor imagery EEG (MI-EEG), which reflects a\nsubjects active intent, is attracting increasing attention for a variety of BCI\napplications. Accurate classification of MI-EEG signals while essential for\neffective operation of BCI systems, is challenging due to the significant noise\ninherent in the signals and the lack of informative correlation between the\nsignals and brain activities. In this paper, we propose a novel deep neural\nnetwork based learning framework that affords perceptive insights into the\nrelationship between the MI-EEG data and brain activities. We design a joint\nconvolutional recurrent neural network that simultaneously learns robust\nhigh-level feature presentations through low-dimensional dense embeddings from\nraw MI-EEG signals. We also employ an Autoencoder layer to eliminate various\nartifacts such as background activities. The proposed approach has been\nevaluated extensively on a large- scale public MI-EEG dataset and a limited but\neasy-to-deploy dataset collected in our lab. The results show that our approach\noutperforms a series of baselines and the competitive state-of-the- art\nmethods, yielding a classification accuracy of 95.53%. The applicability of our\nproposed approach is further demonstrated with a practical BCI system for\ntyping.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 04:20:34 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Zhang", "Xiang", ""], ["Yao", "Lina", ""], ["Sheng", "Quan Z.", ""], ["Kanhere", "Salil S.", ""], ["Gu", "Tao", ""], ["Zhang", "Dalin", ""]]}, {"id": "1709.08945", "submitter": "Pei Xu", "authors": "Pei Xu", "title": "Gesture-based Human-robot Interaction for Field Programmable Autonomous\n  Underwater Robots", "comments": "7 pages, 12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The uncertainty and variability of underwater environment propose the request\nto control underwater robots in real time and dynamically, especially in the\nscenarios where human and robots need to work collaboratively in the field.\nHowever, the underwater environment imposes harsh restrictions on the\napplication of typical control and communication methods. Considering that\ngestures are a natural and efficient interactive way for human, we, utilizing\nconvolution neural network, implement a real-time gesture-based recognition\nsystem, who can recognize 50 kinds of gestures from images captured by one\nnormal monocular camera, and apply this recognition system in human and\nunderwater robot interaction. We design A Flexible and Extendable Interaction\nScheme (AFEIS) through which underwater robots can be programmed in situ\nunderwater by human operators using customized gesture-based sign language.\nThis paper elaborates the design of gesture recognition system and AFEIS, and\npresents our field trial results when applying this system and scheme on\nunderwater robots.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 11:26:08 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Xu", "Pei", ""]]}, {"id": "1709.08957", "submitter": "Garreth Tigwell", "authors": "Michael Mauderer, Garreth W. Tigwell, Benjamin M. Gorman, David R.\n  Flatla", "title": "Beyond Accessibility: Lifting Perceptual Limitations for Everyone", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose that accessibility research can lay the foundation for technology\nthat can be used to augment the perception of everyone. To show how this can be\nachieved, we present three case studies of our research in which we demonstrate\nour approaches for impaired colour vision, situational visual impairments and\nsituational hearing impairment.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 12:03:36 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Mauderer", "Michael", ""], ["Tigwell", "Garreth W.", ""], ["Gorman", "Benjamin M.", ""], ["Flatla", "David R.", ""]]}, {"id": "1709.09077", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Lina Yao, Dalin Zhang, Xianzhi Wang, Quan Z. Sheng, Tao\n  Gu", "title": "Multi-Person Brain Activity Recognition via Comprehensive EEG Signal\n  Analysis", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An electroencephalography (EEG) based brain activity recognition is a\nfundamental field of study for a number of significant applications such as\nintention prediction, appliance control, and neurological disease diagnosis in\nsmart home and smart healthcare domains. Existing techniques mostly focus on\nbinary brain activity recognition for a single person, which limits their\ndeployment in wider and complex practical scenarios. Therefore, multi-person\nand multi-class brain activity recognition has obtained popularity recently.\nAnother challenge faced by brain activity recognition is the low recognition\naccuracy due to the massive noises and the low signal-to-noise ratio in EEG\nsignals. Moreover, the feature engineering in EEG processing is time-consuming\nand highly re- lies on the expert experience. In this paper, we attempt to\nsolve the above challenges by proposing an approach which has better EEG\ninterpretation ability via raw Electroencephalography (EEG) signal analysis for\nmulti-person and multi-class brain activity recognition. Specifically, we\nanalyze inter-class and inter-person EEG signal characteristics, based on which\nto capture the discrepancy of inter-class EEG data. Then, we adopt an\nAutoencoder layer to automatically refine the raw EEG signals by eliminating\nvarious artifacts. We evaluate our approach on both a public and a local EEG\ndatasets and conduct extensive experiments to explore the effect of several\nfactors (such as normalization methods, training data size, and Autoencoder\nhidden neuron size) on the recognition results. The experimental results show\nthat our approach achieves a high accuracy comparing to competitive\nstate-of-the-art methods, indicating its potential in promoting future research\non multi-person EEG recognition.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 15:03:13 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Zhang", "Xiang", ""], ["Yao", "Lina", ""], ["Zhang", "Dalin", ""], ["Wang", "Xianzhi", ""], ["Sheng", "Quan Z.", ""], ["Gu", "Tao", ""]]}, {"id": "1709.09093", "submitter": "R.Stuart Geiger", "authors": "R. Stuart Geiger", "title": "Beyond opening up the black box: Investigating the role of algorithmic\n  systems in Wikipedian organizational culture", "comments": "14 pages, typo fixed in v2", "journal-ref": "Big Data & Society 4(2). 2017", "doi": "10.1177/2053951717730735", "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scholars and practitioners across domains are increasingly concerned with\nalgorithmic transparency and opacity, interrogating the values and assumptions\nembedded in automated, black-boxed systems, particularly in user-generated\ncontent platforms. I report from an ethnography of infrastructure in Wikipedia\nto discuss an often understudied aspect of this topic: the local, contextual,\nlearned expertise involved in participating in a highly automated\nsocial-technical environment. Today, the organizational culture of Wikipedia is\ndeeply intertwined with various data-driven algorithmic systems, which\nWikipedians rely on to help manage and govern the \"anyone can edit\"\nencyclopedia at a massive scale. These bots, scripts, tools, plugins, and\ndashboards make Wikipedia more efficient for those who know how to work with\nthem, but like all organizational culture, newcomers must learn them if they\nwant to fully participate. I illustrate how cultural and organizational\nexpertise is enacted around algorithmic agents by discussing two\nautoethnographic vignettes, which relate my personal experience as a veteran in\nWikipedia. I present thick descriptions of how governance and gatekeeping\npractices are articulated through and in alignment with these automated\ninfrastructures. Over the past 15 years, Wikipedian veterans and administrators\nhave made specific decisions to support administrative and editorial workflows\nwith automation in particular ways and not others. I use these cases of\nWikipedia's bot-supported bureaucracy to discuss several issues in the fields\nof critical algorithms studies, critical data studies, and fairness,\naccountability, and transparency in machine learning -- most principally\narguing that scholarship and practice must go beyond trying to \"open up the\nblack box\" of such systems and also examine sociocultural processes like\nnewcomer socialization.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 15:38:26 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 15:24:56 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Geiger", "R. Stuart", ""]]}, {"id": "1709.09148", "submitter": "Juan Quiroz", "authors": "Juan C. Quiroz, Min Hooi Yong, Elena Geangu", "title": "Emotion-Recognition Using Smart Watch Accelerometer Data: Preliminary\n  Findings", "comments": "Mental Health and Well-being: Sensing and Intervention, UBICOMP 2017\n  Workshop", "journal-ref": null, "doi": "10.1145/3123024.3125614", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the use of accelerometer data from a smart watch to\ninfer an individual's emotional state. We present our preliminary findings on a\nuser study with 50 participants. Participants were primed either with\naudio-visual (movie clips) or audio (classical music) to elicit emotional\nresponses. Participants then walked while wearing a smart watch on one wrist\nand a heart rate strap on their chest. Our hypothesis is that the accelerometer\nsignal will exhibit different patterns for participants in response to\ndifferent emotion priming. We divided the accelerometer data using sliding\nwindows, extracted features from each window, and used the features to train\nsupervised machine learning algorithms to infer an individual's emotion from\ntheir walking pattern. Our discussion includes a description of the\nmethodology, data collected, and early results.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 17:32:51 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Quiroz", "Juan C.", ""], ["Yong", "Min Hooi", ""], ["Geangu", "Elena", ""]]}, {"id": "1709.09359", "submitter": "Jinqiang Bai", "authors": "Jinqiang Bai, Shiguo Lian, Zhaoxiang Liu, Kai Wang, Dijun Liu", "title": "Smart Guiding Glasses for Visually Impaired People in Indoor Environment", "comments": "9 pages,15 figures, IEEE transaction on consumer electronics received", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To overcome the travelling difficulty for the visually impaired group, this\npaper presents a novel ETA (Electronic Travel Aids)-smart guiding device in the\nshape of a pair of eyeglasses for giving these people guidance efficiently and\nsafely. Different from existing works, a novel multi sensor fusion based\nobstacle avoiding algorithm is proposed, which utilizes both the depth sensor\nand ultrasonic sensor to solve the problems of detecting small obstacles, and\ntransparent obstacles, e.g. the French door. For totally blind people, three\nkinds of auditory cues were developed to inform the direction where they can go\nahead. Whereas for weak sighted people, visual enhancement which leverages the\nAR (Augment Reality) technique and integrates the traversable direction is\nadopted. The prototype consisting of a pair of display glasses and several low\ncost sensors is developed, and its efficiency and accuracy were tested by a\nnumber of users. The experimental results show that the smart guiding glasses\ncan effectively improve the user's travelling experience in complicated indoor\nenvironment. Thus it serves as a consumer device for helping the visually\nimpaired people to travel safely.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 06:58:20 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Bai", "Jinqiang", ""], ["Lian", "Shiguo", ""], ["Liu", "Zhaoxiang", ""], ["Wang", "Kai", ""], ["Liu", "Dijun", ""]]}, {"id": "1709.09733", "submitter": "Michael Lyons", "authors": "Michael J. Lyons", "title": "NIME: A Community of Communities", "comments": null, "journal-ref": "A NIME Reader: Fifteen Years of New Interfaces for Musical\n  Expression, A.R. Jensenius and M.J. Lyons (eds.), Springer, pp.477-478, 2017", "doi": "10.6084/m9.figshare.5386786", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commentary on the article Fourteen Years of NIME: The Value and Meaning of\nCommunity in Interactive Music Research by A. Marquez-Borbon and P. Stapleton.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 19:25:48 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Lyons", "Michael J.", ""]]}, {"id": "1709.09741", "submitter": "Raj Korpan", "authors": "Raj Korpan, Susan L. Epstein, Anoop Aroor, Gil Dekel", "title": "WHY: Natural Explanations from a Robot Navigator", "comments": "Accepted at AAAI 2017 Fall Symposium on Natural Communication for\n  Human-Robot Collaboration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective collaboration between a robot and a person requires natural\ncommunication. When a robot travels with a human companion, the robot should be\nable to explain its navigation behavior in natural language. This paper\nexplains how a cognitively-based, autonomous robot navigation system produces\ninformative, intuitive explanations for its decisions. Language generation here\nis based upon the robot's commonsense, its qualitative reasoning, and its\nlearned spatial model. This approach produces natural explanations in real time\nfor a robot as it navigates in a large, complex indoor environment.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 21:30:53 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Korpan", "Raj", ""], ["Epstein", "Susan L.", ""], ["Aroor", "Anoop", ""], ["Dekel", "Gil", ""]]}, {"id": "1709.09931", "submitter": "Angela He", "authors": "Angela He", "title": "Educational game design: game elements for promoting engagement", "comments": "54 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engagement in educational games, a recently popular academic topic, has been\nshown to increase learning performance, as well as a number of attitudinal\nfactors, such as intrinsic interest and motivation. However, there is a lack of\nresearch on how games can be designed to promote engagement. This mixed methods\ncase study aimed to discover effective game elements for promoting 17-18 year\nold high school students' engagement with an educational game. Using\nwithin-case and cross-case analyses and triangulated data, 10 elements emerged\nand were categorized into the constructs of story, gameplay, and atmosphere.\nExamples and connections to the literature for each element are reported.\nFindings implicate that educational game design for both learning and\nengagement is composed of educational-game specific elements, game design for\nsolely engagement is similar for both educational and entertainment games, and\na gap on educational game design technique instead of theory should be\naddressed to further benefit educational game development.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 02:48:27 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["He", "Angela", ""]]}, {"id": "1709.10120", "submitter": "Marco Del Tutto", "authors": "Marco Del Tutto", "title": "VENu: The Virtual Environment for Neutrinos", "comments": "Talk presented at the APS Division of Particles and Fields Meeting\n  (DPF 2017), July 31-August 4, 2017, Fermilab. C170731", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.pop-ph cs.HC hep-ex physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Virtual Environment for Neutrinos (VENu) is a virtual reality-based\nvisualisation of the MicroBooNE detector. MicroBooNE is a liquid-argon-based\nneutrino experiment, which is currently operating in Fermilab's Booster\nneutrino beam. The new VENu smartphone app provides informative explanations\nabout neutrinos and uses real MicroBooNE neutrino data that can be visualised\ninside a virtual representation of the MicroBooNE detector. Available for both\niOS and Android, the VENu app can be downloaded for free from the Apple and\nGoogle marketplaces. The app enables users to immerse themselves inside the\nMicroBooNE particle detector and to see particle tracks inside. This can be\ndone in Virtual Reality mode, where the users can pair their smartphone with\nany consumer virtual reality headset and see the detector in 3D. To encourage\nlearning in a fun environment, a game is also available, guiding users to learn\nabout neutrinos and how to detect them. They can also try to \"catch\"' neutrinos\nthemselves in 3D mode. The app is currently being pursued for a QuarkNet\nneutrino master class and outreach events at several universities and labs\nworldwide.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 18:18:37 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 01:23:14 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Del Tutto", "Marco", ""]]}, {"id": "1709.10257", "submitter": "Divesh Lala", "authors": "Divesh Lala, Koji Inoue, Pierrick Milhorat, Tatsuya Kawahara", "title": "Detection of social signals for recognizing engagement in human-robot\n  interaction", "comments": "AAAI Fall Symposium on Natural Communication for Human-Robot\n  Collaboration, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of engagement during a conversation is an important function of\nhuman-robot interaction. The level of user engagement can influence the\ndialogue strategy of the robot. Our motivation in this work is to detect\nseveral behaviors which will be used as social signal inputs for a real-time\nengagement recognition model. These behaviors are nodding, laughter, verbal\nbackchannels and eye gaze. We describe models of these behaviors which have\nbeen learned from a large corpus of human-robot interactions with the android\nrobot ERICA. Input data to the models comes from a Kinect sensor and a\nmicrophone array. Using our engagement recognition model, we can achieve\nreasonable performance using the inputs from automatic social signal detection,\ncompared to using manual annotation as input.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 07:17:39 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Lala", "Divesh", ""], ["Inoue", "Koji", ""], ["Milhorat", "Pierrick", ""], ["Kawahara", "Tatsuya", ""]]}, {"id": "1709.10299", "submitter": "Souneil Park", "authors": "Souneil Park, Joan Serra, Enrique Frias Martinez, Nuria Oliver", "title": "MobInsight: A Framework Using Semantic Neighborhood Features for\n  Localized Interpretations of Urban Mobility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective urban mobility embodies the residents' local insights on the city.\nMobility practices of the residents are produced from their spatial choices,\nwhich involve various considerations such as the atmosphere of destinations,\ndistance, past experiences, and preferences. The advances in mobile computing\nand the rise of geo-social platforms have provided the means for capturing the\nmobility practices; however, interpreting the residents' insights is\nchallenging due to the scale and complexity of an urban environment, and its\nunique context. In this paper, we present MobInsight, a framework for making\nlocalized interpretations of urban mobility that reflect various aspects of the\nurbanism. MobInsight extracts a rich set of neighborhood features through\nholistic semantic aggregation, and models the mobility between all-pairs of\nneighborhoods. We evaluate MobInsight with the mobility data of Barcelona and\ndemonstrate diverse localized and semantically-rich interpretations.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 09:31:59 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Park", "Souneil", ""], ["Serra", "Joan", ""], ["Martinez", "Enrique Frias", ""], ["Oliver", "Nuria", ""]]}, {"id": "1709.10460", "submitter": "Yustinus Soelistio Eko", "authors": "Novita Belinda Wunarso, Yustinus Eko Soelistio", "title": "Towards Indonesian Speech-Emotion Automatic Recognition (I-SpEAR)", "comments": "4 pages, 3 tables, published in 4th International Conference on New\n  Media (Conmedia) on 8-10 Nov. 2017 (http://conmedia.umn.ac.id/) [in print as\n  in Sept. 17, 2017]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though speech-emotion recognition (SER) has been receiving much\nattention as research topic, there are still some disputes about which vocal\nfeatures can identify certain emotion. Emotion expression is also known to be\ndiffered according to the cultural backgrounds that make it important to study\nSER specific to the culture where the language belongs to. Furthermore, only a\nfew studies addresses the SER in Indonesian which what this study attempts to\nexplore. In this study, we extract simple features from 3420 voice data\ngathered from 38 participants. The features are compared by means of linear\nmixed effect model which shows that people who are in emotional and\nnon-emotional state can be differentiated by their speech duration. Using SVM\nand speech duration as input feature, we achieve 76.84% average accuracy in\nclassifying emotional and non-emotional speech.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 09:14:59 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Wunarso", "Novita Belinda", ""], ["Soelistio", "Yustinus Eko", ""]]}, {"id": "1709.10513", "submitter": "Cagatay Demiralp", "authors": "\\c{C}a\\u{g}atay Demiralp and Peter J. Haas and Srinivasan\n  Parthasarathy and Tejaswini Pedapati", "title": "Foresight: Rapid Data Exploration Through Guideposts", "comments": "IEEE VIS'17 Data Systems and Interactive Analysis (DSIA) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current tools for exploratory data analysis (EDA) require users to manually\nselect data attributes, statistical computations and visual encodings. This can\nbe daunting for large-scale, complex data. We introduce Foresight, a\nvisualization recommender system that helps the user rapidly explore large\nhigh-dimensional datasets through \"guideposts.\" A guidepost is a visualization\ncorresponding to a pronounced instance of a statistical descriptor of the\nunderlying data, such as a strong linear correlation between two attributes,\nhigh skewness or concentration about the mean of a single attribute, or a\nstrong clustering of values. For each descriptor, Foresight initially presents\nvisualizations of the \"strongest\" instances, based on an appropriate ranking\nmetric. Given these initial guideposts, the user can then look at \"nearby\"\nguideposts by issuing \"guidepost queries\" containing constraints on metric\ntype, metric strength, data attributes, and data values. Thus, the user can\ndirectly explore the network of guideposts, rather than the overwhelming space\nof data attributes and visual encodings. Foresight also provides for each\ndescriptor a global visualization of ranking-metric values to both help orient\nthe user and ensure a thorough exploration process. Foresight facilitates\ninteractive exploration of large datasets using fast, approximate sketching to\ncompute ranking metrics. We also contribute insights on EDA practices of data\nscientists, summarizing results from an interview study we conducted to inform\nthe design of Foresight.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 17:42:52 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Demiralp", "\u00c7a\u011fatay", ""], ["Haas", "Peter J.", ""], ["Parthasarathy", "Srinivasan", ""], ["Pedapati", "Tejaswini", ""]]}]