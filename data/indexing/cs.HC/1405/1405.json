[{"id": "1405.0006", "submitter": "William Patera", "authors": "Moritz Kassner, William Patera, Andreas Bulling", "title": "Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile\n  Gaze-based Interaction", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Commercial head-mounted eye trackers provide useful features to customers in\nindustry and research but are expensive and rely on closed source hardware and\nsoftware. This limits the application areas and use of mobile eye tracking to\nexpert users and inhibits user-driven development, customisation, and\nextension. In this paper we present Pupil -- an accessible, affordable, and\nextensible open source platform for mobile eye tracking and gaze-based\ninteraction. Pupil comprises 1) a light-weight headset with high-resolution\ncameras, 2) an open source software framework for mobile eye tracking, as well\nas 3) a graphical user interface (GUI) to playback and visualize video and gaze\ndata. Pupil features high-resolution scene and eye cameras for monocular and\nbinocular gaze estimation. The software and GUI are platform-independent and\ninclude state-of-the-art algorithms for real-time pupil detection and tracking,\ncalibration, and accurate gaze estimation. Results of a performance evaluation\nshow that Pupil can provide an average gaze estimation accuracy of 0.6 degree\nof visual angle (0.08 degree precision) with a latency of the processing\npipeline of only 0.045 seconds.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 16:21:56 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Kassner", "Moritz", ""], ["Patera", "William", ""], ["Bulling", "Andreas", ""]]}, {"id": "1405.0101", "submitter": "Rashmi Jain", "authors": "Dr. Manju Kaushik and Rashmi Jain", "title": "Natural User Interfaces: Trend in Virtual Interaction", "comments": "3 pages", "journal-ref": "International journal Of Latest technology in\n  Engineering,Management & Applied Science (IJLTEMAS)3(4),April 2014,141-143\n  published by International Standards Publication", "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the fundamental constraints of natural way of interacting such as\nspeech, touch, contextual and environmental awareness,immersive 3D\nexperiences-all with a goal of a computer that can see listen, learn talk and\nact. We drive a set of trends prevailing for the next generation of user\ninterface: Natural User Interface (NUI).New technologies are pushing the\nboundaries of what is possible without touching or clicking an interface-\npaving the way of interaction to information visualization and opportunities in\nhuman towards more natural interaction than ever before. In this paper we\nconsider the trends in computer interaction through that must be taken into\nconsideration to come up-in the near future with a well-designed-NUI.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 05:58:19 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Kaushik", "Dr. Manju", ""], ["Jain", "Rashmi", ""]]}, {"id": "1405.0910", "submitter": "Michelle Silv\\'eria", "authors": "Michelle Kr\\\"uger Silv\\'eria", "title": "Virtual Windshields: Merging Reality and Digital Content to Improve the\n  Driving Experience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the use of the automobile as the primary mode of\ntransportation has been increasing and driving has become an important part of\ndaily life. Driving is a multi-sensory experience as drivers rely on their\nsenses to provide them with important information. In a vehicular context human\nsenses are all too often limited and obstructed. Today, road accidents\nconstitute the eighth leading cause of death. The escalation of technology has\npropelled new ways in which driver's senses may be augmented. The enclosed\naspect of a car, allied with the configuration of the controls and displays\ndirected towards the driver, offer significant advantages for augmented reality\n(AR) systems when considering the amount of immersion it can provide to the\nuser. In addition, the inherent mobility and virtually unlimited power autonomy\ntransform cars into perfect mobile computing platforms. However, automobiles\ncurrently present limited network connectivity and thus the created augmented\nobjects are merely providing information captured by in-vehicle sensors,\ncameras and other databases. By combining the new paradigm of Vehicular Ad Hoc\nNetworking (VANET) with AR human machine interfaces, we show that it is\npossible to design novel cooperative Advanced Driver Assistance Systems (ADAS),\nthat base the creation of AR content on the information collected from\nneighbouring vehicles or roadside infrastructures. As such we implement\nprototypes of both visual and acoustic AR systems, which can significantly\nimprove the driving experience. We believe our results contribute to the\nformulation of a vision where the vehicle is perceived as an extension of the\nbody which permeates the human senses to the world outside the vessel, where\nthe car is used as a better, multi-sensory immersive version of a mobile phone\nthat integrates touch, vision and sound enhancements, leveraging unique\nproperties of VANET.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 14:28:13 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Silv\u00e9ria", "Michelle Kr\u00fcger", ""]]}, {"id": "1405.3758", "submitter": "Andrea Kohlhase", "authors": "Andrea Kohlhase", "title": "Search Interfaces for Mathematicians", "comments": "conference article \"CICM'14: International Conference on Computer\n  Mathematics 2014\", DML-Track: Digital Math Libraries 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DL cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to mathematical knowledge has changed dramatically in recent years,\ntherefore changing mathematical search practices. Our aim with this study is to\nscrutinize professional mathematicians' search behavior. With this\nunderstanding we want to be able to reason why mathematicians use which tool\nfor what search problem in what phase of the search process. To gain these\ninsights we conducted 24 repertory grid interviews with mathematically inclined\npeople (ranging from senior professional mathematicians to non-mathematicians).\nFrom the interview data we elicited patterns for the user group\n\"mathematicians\" that can be applied when understanding design issues or\ncreating new designs for mathematical search interfaces.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 06:55:21 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Kohlhase", "Andrea", ""]]}, {"id": "1405.4013", "submitter": "Anurag Bhardwaj", "authors": "Anurag Bhardwaj, Vignesh Jagadeesh, Wei Di, Robinson Piramuthu,\n  Elizabeth Churchill", "title": "Enhancing Visual Fashion Recommendations with Users in the Loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a completely automated large scale visual recommendation system\nfor fashion. Existing approaches have primarily relied on purely computational\nmodels to solving this problem that ignore the role of users in the system. In\nthis paper, we propose to overcome this limitation by incorporating a\nuser-centric design of visual fashion recommendations. Specifically, we propose\na technique that augments 'user preferences' in models by exploiting elasticity\nin fashion choices. We further design a user study on these choices and gather\nresults from the 'wisdom of crowd' for deeper analysis. Our key insights learnt\nthrough these results suggest that fashion preferences when constrained to a\nparticular class, contain important behavioral signals that are often ignored\nin recommendation design. Further, presence of such classes also reflect strong\ncorrelations to visual perception which can be utilized to provide\naesthetically pleasing user experiences. Finally, we illustrate that user\napproval of visual fashion recommendations can be substantially improved by\ncarefully incorporating these user-centric feedback into the system framework.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 20:58:17 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Bhardwaj", "Anurag", ""], ["Jagadeesh", "Vignesh", ""], ["Di", "Wei", ""], ["Piramuthu", "Robinson", ""], ["Churchill", "Elizabeth", ""]]}, {"id": "1405.4354", "submitter": "Shinichi Konomi", "authors": "Tomoyo Sasao, Shin'ichi Konomi, Masatoshi Arikawa, Hideyuki Fujita", "title": "Touch Survey: Comparison with Paper and Web Questionnaires", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a prototype of touch-based survey tool for tablets and conducted\nan experiment to compare interaction patterns of touch-based, PC-based, and\npaper-based questionnaires. Our findings suggest that a touch-based interface\nallows users to complete ranking questions easily, quickly, and accurately\nalthough it can increase the time to complete a location input task for\nwell-known, prominent locations.\n", "versions": [{"version": "v1", "created": "Sat, 17 May 2014 06:01:03 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Sasao", "Tomoyo", ""], ["Konomi", "Shin'ichi", ""], ["Arikawa", "Masatoshi", ""], ["Fujita", "Hideyuki", ""]]}, {"id": "1405.4989", "submitter": "Jiawei Xu", "authors": "Jiawei Xu, Shigang Yue, Ruisheng Wang and Loo Chu Kiong", "title": "Perceiving Motion Cues Inspired by Microsoft Kinect Sensor on Game\n  Experiencing", "comments": "4 pages,4 figures Confernece: 1st International Workshop on\n  Bio-neuromorphic Systems and Human-Robot Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a novel method to replace the traditional mouse\ncontroller by using Microsoft Kinect Sensor to realize the functional\nimplementation on human-machine interaction. With human hand gestures and\nmovements, Kinect Sensor could accurately recognize the participants intention\nand transmit our order to desktop or laptop. In addition, the trend in current\nHCI market is giving the customer more freedom and experiencing feeling by\ninvolving human cognitive factors more deeply. Kinect sensor receives the\nmotion cues continuously from the humans intention and feedback the reaction\nduring the experiments. The comparison accuracy between the hand movement and\nmouse cursor demonstrates the efficiency for the proposed method. In addition,\nthe experimental results on hit rate in the game of Fruit Ninja and Shape\nTouching proves the real-time ability of the proposed framework. The\nperformance evaluation built up a promise foundation for the further\napplications in the field of human-machine interaction. The contribution of\nthis work is the expansion on hand gesture perception and early formulation on\nMac iPad.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 08:24:20 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Xu", "Jiawei", ""], ["Yue", "Shigang", ""], ["Wang", "Ruisheng", ""], ["Kiong", "Loo Chu", ""]]}, {"id": "1405.5047", "submitter": "Michael Burke Mr", "authors": "Michael Burke and Joan Lasenby", "title": "Single camera pose estimation using Bayesian filtering and Kinect motion\n  priors", "comments": "25 pages, Technical report, related to Burke and Lasenby, AMDO 2014\n  conference paper. Code sample: https://github.com/mgb45/SignerBodyPose Video:\n  https://www.youtube.com/watch?v=dJMTSo7-uFE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to upper body pose estimation using monocular vision\nrely on complex body models and a large variety of geometric constraints. We\nargue that this is not ideal and somewhat inelegant as it results in large\nprocessing burdens, and instead attempt to incorporate these constraints\nthrough priors obtained directly from training data. A prior distribution\ncovering the probability of a human pose occurring is used to incorporate\nlikely human poses. This distribution is obtained offline, by fitting a\nGaussian mixture model to a large dataset of recorded human body poses, tracked\nusing a Kinect sensor. We combine this prior information with a random walk\ntransition model to obtain an upper body model, suitable for use within a\nrecursive Bayesian filtering framework. Our model can be viewed as a mixture of\ndiscrete Ornstein-Uhlenbeck processes, in that states behave as random walks,\nbut drift towards a set of typically observed poses. This model is combined\nwith measurements of the human head and hand positions, using recursive\nBayesian estimation to incorporate temporal information. Measurements are\nobtained using face detection and a simple skin colour hand detector, trained\nusing the detected face. The suggested model is designed with analytical\ntractability in mind and we show that the pose tracking can be\nRao-Blackwellised using the mixture Kalman filter, allowing for computational\nefficiency while still incorporating bio-mechanical properties of the upper\nbody. In addition, the use of the proposed upper body model allows reliable\nthree-dimensional pose estimates to be obtained indirectly for a number of\njoints that are often difficult to detect using traditional object recognition\nstrategies. Comparisons with Kinect sensor results and the state of the art in\n2D pose estimation highlight the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 11:54:04 GMT"}, {"version": "v2", "created": "Tue, 17 Jun 2014 12:15:42 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Burke", "Michael", ""], ["Lasenby", "Joan", ""]]}, {"id": "1405.5249", "submitter": "Mohamed Ali Mahjoub", "authors": "Elbahi Anis, Mohamed Ali Mahjoub, Mohamed Nazih Omri", "title": "Hidden Markov Model for Inferring Learner Task Using Mouse Movement", "comments": "Fourth International Conference on Information and Communication\n  Technology and Accessibility (ICTA), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the issues of e-learning web based application is to understand how\nthe learner interacts with an e-learning application to perform a given task.\nThis study proposes a methodology to analyze learner mouse movement in order to\ninfer the task performed. To do this, a Hidden Markov Model is used for\nmodeling the interaction of the learner with an e-learning application. The\nobtained results show the ability of our model to analyze the interaction in\norder to recognize the task performed by the learner.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 22:08:32 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Anis", "Elbahi", ""], ["Mahjoub", "Mohamed Ali", ""], ["Omri", "Mohamed Nazih", ""]]}, {"id": "1405.5263", "submitter": "Stephen Makonin", "authors": "Stephen Makonin and Maryam H. Kashani and Lyn Bartram", "title": "The Affect of Lifestyle Factors on Eco-Visualization Design", "comments": "In the proceedings of Computer Graphics International (CGI) 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As people become more concerned with the need to conserve their power\nconsumption we need to find ways to inform them of how electricity is being\nconsumed within the home. There are a number of devices that have been designed\nusing different forms, sizes, and technologies. We are interested in large\nambient displays that can be read at a glance and from a distance as\ninformative art. However, from these objectives come a number of questions that\nneed to be explored and answered. To what degree might lifestyle factors\ninfluence the design of eco-visualizations? To answer this we need to ask how\npeople with varying lifestyle factors perceive the utility of such devices and\ntheir placement within a home. We explore these questions by creating four\nambient display prototypes. We take our prototypes and subject them to a user\nstudy to gain insight as to the questions posed above. This paper discusses our\nprototypes in detail and the results and findings of our user study.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 23:45:53 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Makonin", "Stephen", ""], ["Kashani", "Maryam H.", ""], ["Bartram", "Lyn", ""]]}, {"id": "1405.5523", "submitter": "Sean Blakley", "authors": "Bob Blakley, G R Blakley, Sean M Blakley", "title": "How to Draw Graphs: Seeing and Redrafting Large Networks in Security and\n  Biology", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is a mathematical object consisting of a set of vertices and a set of\nedges connecting vertices. Graphs can be drawn on paper in various ways, but\nuntil recently all published methods of drawing graphs have had undesirable\nproperties: (i) for graphs which are not plane embeddable, intersections\nbetween the lines representing edges appear at points which are not vertices,\ncreating the appearance of vertices where none exist, (ii) vertex labels can be\nplaced inside vertex symbols, but there is no consistent, logical, and visually\nclean place to put edge labels, and (iii) representations of large graphs are\nvisually dense and difficult to interpret. This paper describes a new\ncartographic method of drawing graphs which solves all of these problems, and\nhas other advantages as well. Complements, comparisons and contrasts of graphs\nare usually better shown cartographically than in node-link form.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 14:25:10 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Blakley", "Bob", ""], ["Blakley", "G R", ""], ["Blakley", "Sean M", ""]]}, {"id": "1405.6334", "submitter": "Renato dos Santos", "authors": "Renato P. dos Santos", "title": "TATI -- A Logo-like interface for microworlds and simulations for\n  physics teaching in Second Life", "comments": "12 pages, 4 figures, Proceedings of ESERA 2013 - 10th biannual\n  Conference of the European Science Education Research Association, September\n  2nd - 7th 2013, Nicosia, Cyprus. Nicosia: University of Cyprus, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ed-ph cs.CY cs.HC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Student difficulties in learning Physics have been thoroughly discussed in\nthe scientific literature. Already in 1980, Papert complained that schools\nteach Newtonian motion by manipulating equations rather than by manipulating\nthe Newtonian objects themselves, what would be possible in a 'physics\nmicroworld'. On the other hand, Second Life and its scripting language have a\nremarkable learning curve that discourages most teachers at using it as an\nenvironment for educational computer simulations and microworlds. The objective\nof this work is to describe TATI, a textual interface which, through TATILogo,\nan accessible Logo language extension, allows the generation of various physics\nmicroworlds in Second Life, containing different types of objects that follow\ndifferent physical laws, providing a learning path into Newtonian Physics.\n", "versions": [{"version": "v1", "created": "Sat, 24 May 2014 19:08:29 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Santos", "Renato P. dos", ""]]}, {"id": "1405.6627", "submitter": "arXiv Admin", "authors": "Rajkumar N, Anand M.G and Barathiraja N", "title": "Portable Camera-Based Product Label Reading For Blind People", "comments": "This article has been withdrawn by arXiv administrators due to\n  verbatim text overlap from external sources", "journal-ref": "IJETT,V10(11),521-524 April 2014.ISSN:2231-5381", "doi": "10.14445/22315381/IJETT-V10P303", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a camera-based assistive text reading framework to help blind\npersons read text labels and product packaging from hand-held objects in their\ndaily life. To isolate the object from untidy backgrounds or other surrounding\nobjects in the camera vision, we initially propose an efficient and effective\nmotion based method to define a region of interest (ROI) in the video by asking\nthe user to tremble the object. This scheme extracts moving object region by a\nmixture-of-Gaussians-based background subtraction technique. In the extracted\nROI, text localization and recognition are conducted to acquire text details.\nTo automatically focus the text regions from the object ROI, we offer a novel\ntext localization algorithm by learning gradient features of stroke\norientations and distributions of edge pixels in an Adaboost model. Text\ncharacters in the localized text regions are then binarized and recognized by\noff-the-shelf optical character identification software. The renowned text\ncodes are converted into audio output to the blind users. Performance of the\nsuggested text localization algorithm is quantitatively evaluated on ICDAR-2003\nand ICDAR-2011 Robust Reading Datasets. Experimental results demonstrate that\nour algorithm achieves the highest level of developments at present time. The\nproof-of-concept example is also evaluated on a dataset collected using ten\nblind persons to evaluate the effectiveness of the scheme. We explore the user\ninterface issues and robustness of the algorithm in extracting and reading text\nfrom different objects with complex backgrounds.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 11:32:25 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 21:54:57 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["N", "Rajkumar", ""], ["G", "Anand M.", ""], ["N", "Barathiraja", ""]]}, {"id": "1405.7857", "submitter": "Peter Mutschke", "authors": "Peter Mutschke, Andrea Scharnhorst, Christophe Gu\\'eret, Philipp Mayr,\n  Preben Hansen, Aida Slavic", "title": "Knowledge Maps and Information Retrieval (KMIR)", "comments": "6 pages, accepted workshop proposal for Digital Libraries 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information systems usually show as a particular point of failure the\nvagueness between user search terms and the knowledge orders of the information\nspace in question. Some kind of guided searching therefore becomes more and\nmore important in order to precisely discover information without knowing the\nright search terms. Knowledge maps of digital library collections are promising\nnavigation tools through knowledge spaces but still far away from being\napplicable for searching digital libraries. However, there is no continuous\nknowledge exchange between the \"map makers\" on the one hand and the Information\nRetrieval (IR) specialists on the other hand. Thus, there is also a lack of\nmodels that properly combine insights of the two strands. The proposed workshop\naims at bringing together these two communities: experts in IR reflecting on\nvisual enhanced search interfaces and experts in knowledge mapping reflecting\non visualizations of the content of a collection that might also present a\ncontext for a search term in a visual manner. The intention of the workshop is\nto raise awareness of the potential of interactive knowledge maps for\ninformation seeking purposes and to create a common ground for experiments\naiming at the incorporation of knowledge maps into IR models at the level of\nthe user interface.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 13:30:15 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Mutschke", "Peter", ""], ["Scharnhorst", "Andrea", ""], ["Gu\u00e9ret", "Christophe", ""], ["Mayr", "Philipp", ""], ["Hansen", "Preben", ""], ["Slavic", "Aida", ""]]}]