[{"id": "1711.00218", "submitter": "Andrew Simmons", "authors": "Andrew Simmons, Rajesh Vasa", "title": "Spatio-Temporal Reference Frames as Geographic Objects", "comments": "4 pages. To be published in the proceedings of the ACM SIGSPATIAL\n  conference 2017. Minor typo corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often desirable to analyse trajectory data in local coordinates\nrelative to a reference location. Similarly, temporal data also needs to be\ntransformed to be relative to an event. Together, temporal and spatial\ncontextualisation permits comparative analysis of similar trajectories taken\nacross multiple reference locations. To the GIS professional, the procedures to\nestablish a reference frame at a location and reproject the data into local\ncoordinates are well known, albeit tedious. However, GIS tools are now often\nused by subject matter experts who may not have the deep knowledge of\ncoordinate frames and projections required to use these techniques effectively.\n  We introduce a novel method for representing spatio-temporal reference frames\nusing ordinary geographic objects available in GIS tools. We argue that our\nmethod both reduces the number of manual steps required to reproject data to a\nlocal reference frame, in addition to reducing the number of concepts a novice\nuser would need to learn.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 06:24:14 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 05:28:14 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Simmons", "Andrew", ""], ["Vasa", "Rajesh", ""]]}, {"id": "1711.00304", "submitter": "Marc Miquel-Ribe", "authors": "Marc Miquel-Ribe", "title": "From Attention to Participation: Reviewing and Modelling Engagement with\n  Computers", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decades, the Internet and mobile technology have consolidated\nthe digital as a public sphere of life. Designers are asked to create engaging\ndigital experiences. However, in some cases engagement is seen as a\npsychological state, while in others it emphasizes a participative vein. In\nthis paper, I review and discuss both and propose a new definition to clarify\nthe concept engagement with computers. Thus, engagement is a quality of an\nactive connection between a user and a computing product, either a website or a\nmobile phone app. Studying it requires understanding a set of aspects like the\nuser's affect, motivation and attention, as well as the product's design,\ncontent and composition. Finally, I propose explaining these concepts aligned\nwith engagement and integrate them into a preliminary model to measure the\nmanifestations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 12:12:38 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Miquel-Ribe", "Marc", ""]]}, {"id": "1711.00714", "submitter": "Sreya Guha", "authors": "Sreya Guha", "title": "Doris: A tool for interactive exploration of historic corpora (Extended\n  Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insights into social phenomenon can be gleaned from trends and patterns in\ncorpora of documents associated with that phenomenon. Recent years have\nwitnessed the use of computational techniques, mostly based on keywords, to\nanalyze large corpora for these purposes. In this paper, we extend these\ntechniques to incorporate semantic features. We introduce Doris, an interactive\nexploration tool that combines semantic features with information retrieval\ntechniques to enable exploration of document corpora corresponding to the\nsocial phenomenon. We discuss the semantic techniques and describe an\nimplementation on a corpus of United States (US) presidential speeches. We\nillustrate, with examples, how the ability to combine syntactic and semantic\nfeatures in a visualization helps researchers more easily gain insights into\nthe underlying phenomenon.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 18:24:33 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Guha", "Sreya", ""]]}, {"id": "1711.00967", "submitter": "Angus Forbes", "authors": "Angus G. Forbes, Andrew Burks, Kristine Lee, Xing Li, Pierre\n  Boutillier, Jean Krivine, Walter Fontana", "title": "Dynamic Influence Networks for Rule-based Models", "comments": "Accepted to TVCG, in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC cs.SI q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Dynamic Influence Network (DIN), a novel visual analytics\ntechnique for representing and analyzing rule-based models of protein-protein\ninteraction networks. Rule-based modeling has proved instrumental in developing\nbiological models that are concise, comprehensible, easily extensible, and that\nmitigate the combinatorial complexity of multi-state and multi-component\nbiological molecules. Our technique visualizes the dynamics of these rules as\nthey evolve over time. Using the data produced by KaSim, an open source\nstochastic simulator of rule-based models written in the Kappa language, DINs\nprovide a node-link diagram that represents the influence that each rule has on\nthe other rules. That is, rather than representing individual biological\ncomponents or types, we instead represent the rules about them (as nodes) and\nthe current influence of these rules (as links). Using our interactive DIN-Viz\nsoftware tool, researchers are able to query this dynamic network to find\nmeaningful patterns about biological processes, and to identify salient aspects\nof complex rule-based models. To evaluate the effectiveness of our approach, we\ninvestigate a simulation of a circadian clock model that illustrates the\noscillatory behavior of the KaiC protein phosphorylation cycle.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 22:59:23 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Forbes", "Angus G.", ""], ["Burks", "Andrew", ""], ["Lee", "Kristine", ""], ["Li", "Xing", ""], ["Boutillier", "Pierre", ""], ["Krivine", "Jean", ""], ["Fontana", "Walter", ""]]}, {"id": "1711.01224", "submitter": "Francisco Monteiro-Guerra", "authors": "Francisco Monteiro-Guerra, Octavio Rivera-Romero, Vasiliki\n  Mylonopoulou, Gabriel R. Signorelli, Francisco Zambrana, Luis Fernandez-Luque", "title": "The Design of a Mobile App for Promotion of Physical Activity and\n  Self-Management in Prostate Cancer Survivors: Personas, Feature Ideation and\n  Low-Fidelity Prototyping", "comments": "Published in the conference proceedings of the 30th IEEE\n  International Symposium on Computer-Based Medical Systems - IEEE CBMS 2017", "journal-ref": null, "doi": "10.1109/CBMS.2017.75", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most prostate cancer survivors are confronted with disease-related and\ntreatment-related side effects that impact their quality of life. A tool that\ncombines specific physical activity coaching with the promotion of a healthy\nlifestyle and self-management guidance might be a successful method to enhance\na lifestyle change in these patients. As a prerequisite for useful health\ntechnology, it is important to consider a design process centred in the\npatients. The aim of this study was to investigate the context of the problem\nand the user needs to support the ideation of a low-fidelity prototype of a\ntool to promote a healthy lifestyle among early-stage prostate cancer\nsurvivors. A user-centred design approach was followed involving a\nmultidisciplinary team. The prototype was developed in 3 phases. In phase 1,\nthe context was studied with 2 systematic reviews of the state of practice and\nconsulting with 3 specialists in Oncology, resulting in a global use case and\nmain requirements. In phase 2, the needs and barriers of the users were studied\nbased on literature research and validated with 3 specialists, resulting in the\ncreation of 3 personas. In phase 3, 2 sessions were held to ideate and\nprioritize possible app features, based on brainstorming and selection\ntechniques. Using the Ninja Mock and Proto.io software a low-fidelity prototype\nwas developed, resulting in 25 interactive screens. Understanding the user\nneeds and context seems to be essential to highlight key goals, hence\nfacilitating the bridge between ideation of the tool and the intended users\ntasks and experiences. The conclusion of this first stage of the design process\nbrings valuable details (such as barriers of the users to technology and to\nphysical activity) for future iterations of design of the mobile app.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 16:25:18 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 16:05:10 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Monteiro-Guerra", "Francisco", ""], ["Rivera-Romero", "Octavio", ""], ["Mylonopoulou", "Vasiliki", ""], ["Signorelli", "Gabriel R.", ""], ["Zambrana", "Francisco", ""], ["Fernandez-Luque", "Luis", ""]]}, {"id": "1711.01775", "submitter": "Athanasia Zlatintsi", "authors": "A. Zlatintsi, I. Rodomagoulakis, P. Koutras, A. C. Dometios, V.\n  Pitsikalis, C. S. Tzafestas, and P. Maragos", "title": "Multimodal Signal Processing and Learning Aspects of Human-Robot\n  Interaction for an Assistive Bathing Robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore new aspects of assistive living on smart human-robot interaction\n(HRI) that involve automatic recognition and online validation of speech and\ngestures in a natural interface, providing social features for HRI. We\nintroduce a whole framework and resources of a real-life scenario for elderly\nsubjects supported by an assistive bathing robot, addressing health and hygiene\ncare issues. We contribute a new dataset and a suite of tools used for data\nacquisition and a state-of-the-art pipeline for multimodal learning within the\nframework of the I-Support bathing robot, with emphasis on audio and RGB-D\nvisual streams. We consider privacy issues by evaluating the depth visual\nstream along with the RGB, using Kinect sensors. The audio-gestural recognition\ntask on this new dataset yields up to 84.5%, while the online validation of the\nI-Support system on elderly users accomplishes up to 84% when the two\nmodalities are fused together. The results are promising enough to support\nfurther research in the area of multimodal recognition for assistive social\nHRI, considering the difficulties of the specific task. Upon acceptance of the\npaper part of the data will be publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 08:16:07 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Zlatintsi", "A.", ""], ["Rodomagoulakis", "I.", ""], ["Koutras", "P.", ""], ["Dometios", "A. C.", ""], ["Pitsikalis", "V.", ""], ["Tzafestas", "C. S.", ""], ["Maragos", "P.", ""]]}, {"id": "1711.02068", "submitter": "Sandeep Vidyapu", "authors": "Vidyapu Sandeep, V Vijaya Saradhi, Samit Bhattacharya", "title": "From Multimodal to Unimodal Webpages for Developing Countries", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multimodal web elements such as text and images are associated with\ninherent memory costs to store and transfer over the Internet. With the limited\nnetwork connectivity in developing countries, webpage rendering gets delayed in\nthe presence of high-memory demanding elements such as images (relative to\ntext). To overcome this limitation, we propose a Canonical Correlation Analysis\n(CCA) based computational approach to replace high-cost modality with an\nequivalent low-cost modality. Our model learns a common subspace for low-cost\nand high-cost modalities that maximizes the correlation between their visual\nfeatures. The obtained common subspace is used for determining the low-cost\n(text) element of a given high-cost (image) element for the replacement. We\nanalyze the cost-saving performance of the proposed approach through an\neye-tracking experiment conducted on real-world webpages. Our approach reduces\nthe memory-cost by at least 83.35% by replacing images with text.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 18:32:59 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Sandeep", "Vidyapu", ""], ["Saradhi", "V Vijaya", ""], ["Bhattacharya", "Samit", ""]]}, {"id": "1711.02128", "submitter": "Qiyu Kang", "authors": "Qiyu Kang and Wee Peng Tay", "title": "Sequential Multi-Class Labeling in Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a crowdsourcing platform where workers' responses to questions\nposed by a crowdsourcer are used to determine the hidden state of a multi-class\nlabeling problem. As workers may be unreliable, we propose to perform\nsequential questioning in which the questions posed to the workers are designed\nbased on previous questions and answers. We propose a Partially-Observable\nMarkov Decision Process (POMDP) framework to determine the best questioning\nstrategy, subject to the crowdsourcer's budget constraint. As this POMDP\nformulation is in general intractable, we develop a suboptimal approach based\non a $q$-ary Ulam-R\\'enyi game. We also propose a sampling heuristic, which can\nbe used in tandem with standard POMDP solvers, using our Ulam-R\\'enyi strategy.\nWe demonstrate through simulations that our approaches outperform a\nnon-sequential strategy based on error correction coding and which does not\nutilize workers' previous responses.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:23:07 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 03:28:03 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Kang", "Qiyu", ""], ["Tay", "Wee Peng", ""]]}, {"id": "1711.02231", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang, Chen Fang, Zhaowen Wang, Julian McAuley", "title": "Visually-Aware Fashion Recommendation and Design with Generative Image\n  Models", "comments": "10 pages, 6 figures. Accepted by ICDM'17 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building effective recommender systems for domains like fashion is\nchallenging due to the high level of subjectivity and the semantic complexity\nof the features involved (i.e., fashion styles). Recent work has shown that\napproaches to `visual' recommendation (e.g.~clothing, art, etc.) can be made\nmore accurate by incorporating visual signals directly into the recommendation\nobjective, using `off-the-shelf' feature representations derived from deep\nnetworks. Here, we seek to extend this contribution by showing that\nrecommendation performance can be significantly improved by learning `fashion\naware' image representations directly, i.e., by training the image\nrepresentation (from the pixel level) and the recommender system jointly; this\ncontribution is related to recent work using Siamese CNNs, though we are able\nto show improvements over state-of-the-art recommendation techniques such as\nBPR and variants that make use of pre-trained visual features. Furthermore, we\nshow that our model can be used \\emph{generatively}, i.e., given a user and a\nproduct category, we can generate new images (i.e., clothing items) that are\nmost consistent with their personal taste. This represents a first step towards\nbuilding systems that go beyond recommending existing items from a product\ncorpus, but which can be used to suggest styles and aid the design of new\nproducts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 00:17:51 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["Fang", "Chen", ""], ["Wang", "Zhaowen", ""], ["McAuley", "Julian", ""]]}, {"id": "1711.02427", "submitter": "Carlos Eduardo Cancino-Chac\\'on", "authors": "Carlos Cancino-Chac\\'on and Martin Bonev and Amaury Durand and Maarten\n  Grachten and Andreas Arzt and Laura Bishop and Werner Goebl and Gerhard\n  Widmer", "title": "The ACCompanion v0.1: An Expressive Accompaniment System", "comments": "Presented at the Late-Breaking Demo Session of the 18th International\n  Society for Music Information Retrieval Conference (ISMIR 2017), Suzhou,\n  China, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present a preliminary version of the ACCompanion, an\nexpressive accompaniment system for MIDI input. The system uses a probabilistic\nmonophonic score follower to track the position of the soloist in the score,\nand a linear Gaussian model to compute tempo updates. The expressiveness of the\nsystem is powered by the Basis-Mixer, a state-of-the-art computational model of\nexpressive music performance. The system allows for expressive dynamics, timing\nand articulation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 12:13:30 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Cancino-Chac\u00f3n", "Carlos", ""], ["Bonev", "Martin", ""], ["Durand", "Amaury", ""], ["Grachten", "Maarten", ""], ["Arzt", "Andreas", ""], ["Bishop", "Laura", ""], ["Goebl", "Werner", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1711.02629", "submitter": "June Wang", "authors": "Jue Wang, Keith J. Bennett and Edward A. Guinness", "title": "Virtual Astronaut for Scientific Visualization - A Prototype for Santa\n  Maria Crater on Mars", "comments": "20 pages, 11 figures", "journal-ref": "Future Internet 2012, 4, 1049-1068", "doi": "10.3390/fi40x000x", "report-no": null, "categories": "astro-ph.IM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To support scientific visualization of multiple-mission data from Mars, the\nVirtual Astronaut (VA) creates an interactive virtual 3D environment built on\nthe Unity3D Game Engine. A prototype study was conducted based on orbital and\nOpportunity Rover data covering Santa Maria Crater in Meridiani Planum on Mars.\nThe VA at Santa Maria provides dynamic visual representations of the imaging,\ncompositional, and mineralogical information. The VA lets one navigate through\nthe scene and provides geomorphic and geologic contexts for the rover\noperations. User interactions include in-situ observations visualization,\nfeature measurement, and an animation control of rover drives. This paper\ncovers our approach and implementation of the VA system. A brief summary of the\nprototype system functions and user feedback is also covered. Based on external\nreview and comments by the science community, the prototype at Santa Maria has\nproven the VA to be an effective tool for virtual geovisual analysis.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 17:47:51 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Wang", "Jue", ""], ["Bennett", "Keith J.", ""], ["Guinness", "Edward A.", ""]]}, {"id": "1711.02760", "submitter": "Christoph Trattner", "authors": "Christoph Trattner, David Elsweiler", "title": "Food Recommender Systems: Important Contributions, Challenges and Future\n  Research Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recommendation of food items is important for many reasons. Attaining\ncooking inspiration via digital sources is becoming evermore popular; as are\nsystems, which recommend other types of food, such as meals in restaurants or\nproducts in supermarkets. Researchers have been studying these kinds of systems\nfor many years, suggesting not only that can they be a means to help people\nfind food they might want to eat, but also help them nourish themselves more\nhealthily. This paper provides a summary of the state-of-the-art of so-called\nfood recommender systems, highlighting both seminal and most recent approaches\nto the problem, as well as important specializations, such as food\nrecommendation systems for groups of users or systems which promote healthy\neating. We moreover discuss the diverse challenges involved in designing recsys\nfor food, summarise the lessons learned from past research and outline what we\nbelieve to be important future directions and open questions for the field. In\nproviding these contributions we hope to provide a useful resource for\nresearchers and practitioners alike.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 22:52:12 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 10:23:29 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Trattner", "Christoph", ""], ["Elsweiler", "David", ""]]}, {"id": "1711.03065", "submitter": "Saturnino Luz", "authors": "Saturnino Luz and Masood Masoodian", "title": "An Application of Mosaic Diagrams to the Visualization of Set\n  Relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an application of mosaic diagrams to the visualisation of set\nrelations. Venn and Euler diagrams are the best known visual representations of\nsets and their relationships (intersections, containment or subsets, exclusion\nor disjointness). In recent years, alternative forms of visualisation have been\nproposed. Among them, linear diagrams have been shown to compare favourably to\nVenn and Euler diagrams, in supporting non-interactive assessment of set\nrelationships. Recent studies that compared several variants of linear diagrams\nhave demonstrated that users perform best at tasks involving identification of\nintersections, disjointness and subsets when using a horizontally drawn linear\ndiagram with thin lines representing sets, and employing vertical lines as\nguide lines. The essential visual task the user needs to perform in order to\ninterpret this kind of diagram is vertical alignment of parallel lines and\ndetection of overlaps. Space-filling mosaic diagrams which support this same\nvisual task have been used in other applications, such as the visualization of\nschedules of activities, where they have been shown to be superior to linear\nGantt charts. In this paper, we present an application of mosaic diagrams for\nvisualization of set relationships, and compare it to linear diagrams in terms\nof accuracy, time-to-answer, and subjective ratings of perceived task\ndifficulty. The study participants exhibited similar performance on both\nvisualisations, suggesting that mosaic diagrams are a good alternative to Venn\nand Euler diagrams, and that the choice between linear diagrams and mosaics may\nbe solely guided by visual design considerations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 17:45:09 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Luz", "Saturnino", ""], ["Masoodian", "Masood", ""]]}, {"id": "1711.03115", "submitter": "Lisa Posch", "authors": "Lisa Posch, Arnim Bleier, Fabian Fl\\\"ock, Markus Strohmaier", "title": "A Cross-Country Comparison of Crowdworker Motivations", "comments": "3rd Annual International Conference on Computational Social Science\n  (IC2S2), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd employment is a new form of short term employment that has been rapidly\nbecoming a source of income for a vast number of people around the globe. It\ndiffers considerably from more traditional forms of work, yet similar ethical\nand optimization issues arise. One key to tackle such challenges is to\nunderstand what motivates the international crowd workforce. In this work, we\nstudy the motivation of workers involved in one particularly prevalent type of\ncrowd employment: micro-tasks. We report on the results of applying the\nMultidimensional Crowdworker Motivation Scale (MCMS) in ten countries, which\nunveil significant international differences.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 19:00:58 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Posch", "Lisa", ""], ["Bleier", "Arnim", ""], ["Fl\u00f6ck", "Fabian", ""], ["Strohmaier", "Markus", ""]]}, {"id": "1711.03676", "submitter": "Patrick M. Pilarski", "authors": "Patrick M. Pilarski, Richard S. Sutton, Kory W. Mathewson, Craig\n  Sherstan, Adam S. R. Parker, Ann L. Edwards", "title": "Communicative Capital for Prosthetic Agents", "comments": "33 pages, 10 figures; unpublished technical report undergoing peer\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an overarching perspective on the role that machine\nintelligence can play in enhancing human abilities, especially those that have\nbeen diminished due to injury or illness. As a primary contribution, we develop\nthe hypothesis that assistive devices, and specifically artificial arms and\nhands, can and should be viewed as agents in order for us to most effectively\nimprove their collaboration with their human users. We believe that increased\nagency will enable more powerful interactions between human users and next\ngeneration prosthetic devices, especially when the sensorimotor space of the\nprosthetic technology greatly exceeds the conventional control and\ncommunication channels available to a prosthetic user. To more concretely\nexamine an agency-based view on prosthetic devices, we propose a new schema for\ninterpreting the capacity of a human-machine collaboration as a function of\nboth the human's and machine's degrees of agency. We then introduce the idea of\ncommunicative capital as a way of thinking about the communication resources\ndeveloped by a human and a machine during their ongoing interaction. Using this\nschema of agency and capacity, we examine the benefits and disadvantages of\nincreasing the agency of a prosthetic limb. To do so, we present an analysis of\nexamples from the literature where building communicative capital has enabled a\nprogression of fruitful, task-directed interactions between prostheses and\ntheir human users. We then describe further work that is needed to concretely\nevaluate the hypothesis that prostheses are best thought of as agents. The\nagent-based viewpoint developed in this article significantly extends current\nthinking on how best to support the natural, functional use of increasingly\ncomplex prosthetic enhancements, and opens the door for more powerful\ninteractions between humans and their assistive technologies.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 03:19:59 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Pilarski", "Patrick M.", ""], ["Sutton", "Richard S.", ""], ["Mathewson", "Kory W.", ""], ["Sherstan", "Craig", ""], ["Parker", "Adam S. R.", ""], ["Edwards", "Ann L.", ""]]}, {"id": "1711.03800", "submitter": "Arun Balajee Vasudevan", "authors": "Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool", "title": "Object Referring in Visual Scene with Spoken Language", "comments": "10 pages, Submitted to WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object referring has important applications, especially for human-machine\ninteraction. While having received great attention, the task is mainly attacked\nwith written language (text) as input rather than spoken language (speech),\nwhich is more natural. This paper investigates Object Referring with Spoken\nLanguage (ORSpoken) by presenting two datasets and one novel approach. Objects\nare annotated with their locations in images, text descriptions and speech\ndescriptions. This makes the datasets ideal for multi-modality learning. The\napproach is developed by carefully taking down ORSpoken problem into three\nsub-problems and introducing task-specific vision-language interactions at the\ncorresponding levels. Experiments show that our method outperforms competing\nmethods consistently and significantly. The approach is also evaluated in the\npresence of audio noise, showing the efficacy of the proposed vision-language\ninteraction methods in counteracting background noise.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 13:04:55 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 15:12:24 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Vasudevan", "Arun Balajee", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1711.03846", "submitter": "Brett Israelsen", "authors": "Brett W Israelsen, Nisar R Ahmed", "title": "\"Dave...I can assure you...that it's going to be all right...\" -- A\n  definition, case for, and survey of algorithmic assurances in human-autonomy\n  trust relationships", "comments": "final version of accepted manuscript", "journal-ref": null, "doi": "10.1145/3267338", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People who design, use, and are affected by autonomous artificially\nintelligent agents want to be able to \\emph{trust} such agents -- that is, to\nknow that these agents will perform correctly, to understand the reasoning\nbehind their actions, and to know how to use them appropriately. Many\ntechniques have been devised to assess and influence human trust in\nartificially intelligent agents. However, these approaches are typically ad\nhoc, and have not been formally related to each other or to formal trust\nmodels. This paper presents a survey of \\emph{algorithmic assurances}, i.e.\nprogrammed components of agent operation that are expressly designed to\ncalibrate user trust in artificially intelligent agents. Algorithmic assurances\nare first formally defined and classified from the perspective of formally\nmodeled human-artificially intelligent agent trust relationships. Building on\nthese definitions, a synthesis of research across communities such as machine\nlearning, human-computer interaction, robotics, e-commerce, and others reveals\nthat assurance algorithms naturally fall along a spectrum in terms of their\nimpact on an agent's core functionality, with seven notable classes ranging\nfrom integral assurances (which impact an agent's core functionality) to\nsupplemental assurances (which have no direct effect on agent performance).\nCommon approaches within each of these classes are identified and discussed;\nbenefits and drawbacks of different approaches are also investigated.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 19:00:29 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 17:38:47 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 19:03:43 GMT"}, {"version": "v4", "created": "Tue, 28 Aug 2018 17:07:30 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Israelsen", "Brett W", ""], ["Ahmed", "Nisar R", ""]]}, {"id": "1711.04036", "submitter": "Daniel Lopez-Martinez", "authors": "Daniel Lopez-Martinez, Ognjen Rudovic, Rosalind Picard", "title": "Physiological and behavioral profiling for nociceptive pain estimation\n  using personalized multitask learning", "comments": "NIPS Machine Learning for Health 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pain is a subjective experience commonly measured through patient's self\nreport. While there exist numerous situations in which automatic pain\nestimation methods may be preferred, inter-subject variability in physiological\nand behavioral pain responses has hindered the development of such methods. In\nthis work, we address this problem by introducing a novel personalized\nmultitask machine learning method for pain estimation based on individual\nphysiological and behavioral pain response profiles, and show its advantages in\na dataset containing multimodal responses to nociceptive heat pain.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 22:36:27 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Lopez-Martinez", "Daniel", ""], ["Rudovic", "Ognjen", ""], ["Picard", "Rosalind", ""]]}, {"id": "1711.04216", "submitter": "Todd Davies", "authors": "Stanley J. Rosenschein and Todd Davies", "title": "Coordination Technology for Active Support Networks: Context,\n  Needfinding, and Design", "comments": "16 pages, 4 figures, Scheduled to appear in AI & Society, 33(1), 2018", "journal-ref": "AI & Society 33(1):113-123, 2018", "doi": "10.1007/s00146-017-0778-4", "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordination is a key problem for addressing goal-action gaps in many human\nendeavors. We define interpersonal coordination as a type of communicative\naction characterized by low interpersonal belief and goal conflict. Such\nsituations are particularly well described as having collectively\n\"intelligent\", \"common good\" solutions, viz., ones that almost everyone would\nagree constitute social improvements. Coordination is useful across the\nspectrum of interpersonal communication -- from isolated individuals to\norganizational teams. Much attention has been paid to coordination in teams and\norganizations. In this paper we focus on the looser interpersonal structures we\ncall active support networks (ASNs), and on technology that meets their needs.\nWe describe two needfinding investigations focused on social support, which\nexamined (a) four application areas for improving coordination in ASNs: (i)\nacademic coaching, (ii) vocational training, (iii) early learning intervention,\nand (iv) volunteer coordination; and (b) existing technology relevant to ASNs.\nWe find a thus-far unmet need for personal task management software that allows\nsmooth integration with an individual's active support network. Based on\nidentified needs, we then describe an open architecture for coordination that\nhas been developed into working software. The design includes a set of\ncapabilities we call \"social prompting,\" as well as templates for accomplishing\nmulti-task goals, and an engine that controls coordination in the network. The\nresulting tool is currently available and in continuing development. We explain\nits use in ASNs with an example. Follow-up studies are underway in which the\ntechnology is being applied in existing support networks.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 01:47:02 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Rosenschein", "Stanley J.", ""], ["Davies", "Todd", ""]]}, {"id": "1711.04513", "submitter": "Sung Cho", "authors": "Sung Jin Cho", "title": "COMBINE: a novel drug discovery platform designed to capture insight and\n  experience of users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The insight and experience gained by a researcher are often lost because the\ncurrent productive and analytics software are inherently data-centric,\ndisconnected, and scattered. The connected nature of insight and experience can\nbe captured if the applications themselves are connected. How connected\napplications concept is implemented in COnstruct cheMical and BIological\nNEtwork (COMBINE), a novel user-centric drug discovery platform, is described.\nUsing publicly available data, how COMBINE users capture insight and experience\nis explained, and how COMBINE users perform data organization, data sharing,\ndata analysis, and data visualization is illustrated.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 10:51:28 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Cho", "Sung Jin", ""]]}, {"id": "1711.04518", "submitter": "Marius St\\\"ark", "authors": "Marius St\\\"ark, Damian Backes, Christian Kehl", "title": "A Supervised Learning Concept for Reducing User Interaction in Passenger\n  Cars", "comments": "4 pages, 9 figures, concept only", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article an automation system for human-machine-interfaces (HMI) for\nsetpoint adjustment using supervised learning is presented. We use HMIs of\nmulti-modal thermal conditioning systems in passenger cars as example for a\ncomplex setpoint selection system. The goal is the reduction of interaction\ncomplexity up to full automation. The approach is not limited to climate\ncontrol applications but can be extended to other setpoint-based HMIs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 10:58:58 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["St\u00e4rk", "Marius", ""], ["Backes", "Damian", ""], ["Kehl", "Christian", ""]]}, {"id": "1711.04695", "submitter": "Rudy Arthur", "authors": "Rudy Arthur, Chris A. Boulton, Humphrey Shotton, Hywel T.P. Williams", "title": "Social Sensing of Floods in the UK", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0189327", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Social sensing\" is a form of crowd-sourcing that involves systematic\nanalysis of digital communications to detect real-world events. Here we\nconsider the use of social sensing for observing natural hazards. In\nparticular, we present a case study that uses data from a popular social media\nplatform (Twitter) to detect and locate flood events in the UK. In order to\nimprove data quality we apply a number of filters (timezone, simple text\nfilters and a naive Bayes `relevance' filter) to the data. We then use place\nnames in the user profile and message text to infer the location of the tweets.\nThese two steps remove most of the irrelevant tweets and yield orders of\nmagnitude more located tweets than we have by relying on geo-tagged data. We\ndemonstrate that high resolution social sensing of floods is feasible and we\ncan produce high-quality historical and real-time maps of floods using Twitter.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 16:42:55 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Arthur", "Rudy", ""], ["Boulton", "Chris A.", ""], ["Shotton", "Humphrey", ""], ["Williams", "Hywel T. P.", ""]]}, {"id": "1711.04971", "submitter": "Srikanta Bedathur", "authors": "Rema Ananthanarayanan and Pranay Kr. Lohia and Srikanta Bedathur", "title": "DataVizard: Recommending Visual Presentations for Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the appropriate visual presentation of the data such that it\npreserves the semantics of the underlying data and at the same time provides an\nintuitive summary of the data is an important, often the final step of data\nanalytics. Unfortunately, this is also a step involving significant human\neffort starting from selection of groups of columns in the structured results\nfrom analytics stages, to the selection of right visualization by experimenting\nwith various alternatives. In this paper, we describe our \\emph{DataVizard}\nsystem aimed at reducing this overhead by automatically recommending the most\nappropriate visual presentation for the structured result. Specifically, we\nconsider the following two scenarios: first, when one needs to visualize the\nresults of a structured query such as SQL; and the second, when one has\nacquired a data table with an associated short description (e.g., tables from\nthe Web). Using a corpus of real-world database queries (and their results) and\na number of statistical tables crawled from the Web, we show that DataVizard is\ncapable of recommending visual presentations with high accuracy. We also\npresent the results of a user survey that we conducted in order to assess user\nviews of the suitability of the presented charts vis-a-vis the plain text\ncaptions of the data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 06:43:30 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Ananthanarayanan", "Rema", ""], ["Lohia", "Pranay Kr.", ""], ["Bedathur", "Srikanta", ""]]}, {"id": "1711.05016", "submitter": "Morad Behandish", "authors": "Morad Behandish and Horea T. Ilies", "title": "Peg-in-Hole Revisited: A Generic Force Model for Haptic Assembly", "comments": "A shorter version was presented in ASME Computers and Information in\n  Engineering Conference (CIE'2014) (Best Paper Award)", "journal-ref": "ASME Transactions, Journal of ASME Transactions, Journal of\n  Computing and Information Science in Engineering, 15(4), p.041004, 2015", "doi": "10.1115/1.4030749", "report-no": "CDL-TR-15-08", "categories": "cs.HC cs.CG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of a generic and effective force model for semi-automatic or\nmanual virtual assembly with haptic support is not a trivial task, especially\nwhen the assembly constraints involve complex features of arbitrary shape. The\nprimary challenge lies in a proper formulation of the guidance forces and\ntorques that effectively assist the user in the exploration of the virtual\nenvironment (VE), from repulsing collisions to attracting proper contact. The\nsecondary difficulty is that of efficient implementation that maintains the\nstandard 1 kHz haptic refresh rate. We propose a purely geometric model for an\nartificial energy field that favors spatial relations leading to proper\nassembly, differentiated to obtain forces and torques for general motions. The\nenergy function is expressed in terms of a cross-correlation of shape-dependent\naffinity fields, precomputed offline separately for each object. We test the\neffectiveness of the method using familiar peg-in-hole examples. We show that\nthe proposed technique unifies the two phases of free motion and precise\ninsertion into a single interaction mode and provides a generic model to\nreplace the ad hoc mating constraints or virtual fixtures, with no restrictive\nassumption on the types of the involved assembly features.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 09:30:49 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Behandish", "Morad", ""], ["Ilies", "Horea T.", ""]]}, {"id": "1711.05017", "submitter": "Morad Behandish", "authors": "Morad Behandish and Horea T. Ilies", "title": "Haptic Assembly Using Skeletal Densities and Fourier Transforms", "comments": "A shorter version was presented in ASME Computers and Information in\n  Engineering Conference (CIE'2015) (Best Paper Award)", "journal-ref": "ASME Transactions, Journal of ASME Transactions, Journal of\n  Computing and Information Science in Engineering, 16(2), p.021002, 2016", "doi": "10.1115/1.4032696", "report-no": "CDL-TR-16-01", "categories": "cs.HC cs.CG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haptic-assisted virtual assembly and prototyping has seen significant\nattention over the past two decades. However, in spite of the appealing\nprospects, its adoption has been slower than expected. We identify the main\nroadblocks as the inherent geometric complexities faced when assembling objects\nof arbitrary shape, and the computation time limitation imposed by the\nnotorious 1 kHz haptic refresh rate. We addressed the first problem in a recent\nwork by introducing a generic energy model for geometric guidance and\nconstraints between features of arbitrary shape. In the present work, we\naddress the second challenge by leveraging Fourier transforms to compute the\nconstraint forces and torques. Our new concept of 'geometric energy' field is\ncomputed automatically from a cross-correlation of 'skeletal densities' in the\nfrequency domain, and serves as a generalization of the manually specified\nvirtual fixtures or heuristically identified mating constraints proposed in the\nliterature. The formulation of the energy field as a convolution enables\nefficient computation using fast Fourier transforms (FFT) on the graphics\nprocessing unit (GPU). We show that our method is effective for low-clearance\nassembly of objects of arbitrary geometric and syntactic complexity.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 09:30:56 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Behandish", "Morad", ""], ["Ilies", "Horea T.", ""]]}, {"id": "1711.05233", "submitter": "Nazmus Saquib", "authors": "Manash Kumar Mandal, Pinku Deb Nath, Arpeeta Shams Mizan, Nazmus\n  Saquib", "title": "A visual search engine for Bangladeshi laws", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World. Corresponding author: Nazmus Saquib", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Browsing and finding relevant information for Bangladeshi laws is a challenge\nfaced by all law students and researchers in Bangladesh, and by citizens who\nwant to learn about any legal procedure. Some law archives in Bangladesh are\ndigitized, but lack proper tools to organize the data meaningfully. We present\na text visualization tool that utilizes machine learning techniques to make the\nsearching of laws quicker and easier. Using Doc2Vec to layout law article\nnodes, link mining techniques to visualize relevant citation networks, and\nnamed entity recognition to quickly find relevant sections in long law\narticles, our tool provides a faster and better search experience to the users.\nQualitative feedback from law researchers, students, and government officials\nshow promise for visually intuitive search tools in the context of\ngovernmental, legal, and constitutional data in developing countries, where\ndigitized data does not necessarily pave the way towards an easy access to\ninformation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 18:15:11 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mandal", "Manash Kumar", ""], ["Nath", "Pinku Deb", ""], ["Mizan", "Arpeeta Shams", ""], ["Saquib", "Nazmus", ""]]}, {"id": "1711.05444", "submitter": "Nuri Murat Arar", "authors": "Nuri Murat Arar and Jean-Philippe Thiran", "title": "Robust Real-Time Multi-View Eye Tracking", "comments": "Organisational changes in the main msp and supplementary info.\n  Results unchanged. Main msp: 14 pages, 15 figures. Supplementary: 2 tables, 1\n  figure. Under review for an IEEE transactions publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant advances in improving the gaze tracking accuracy under\ncontrolled conditions, the tracking robustness under real-world conditions,\nsuch as large head pose and movements, use of eyeglasses, illumination and eye\ntype variations, remains a major challenge in eye tracking. In this paper, we\nrevisit this challenge and introduce a real-time multi-camera eye tracking\nframework to improve the tracking robustness. First, differently from previous\nwork, we design a multi-view tracking setup that allows for acquiring multiple\neye appearances simultaneously. Leveraging multi-view appearances enables to\nmore reliably detect gaze features under challenging conditions, particularly\nwhen they are obstructed in conventional single-view appearance due to large\nhead movements or eyewear effects. The features extracted on various\nappearances are then used for estimating multiple gaze outputs. Second, we\npropose to combine estimated gaze outputs through an adaptive fusion mechanism\nto compute user's overall point of regard. The proposed mechanism firstly\ndetermines the estimation reliability of each gaze output according to user's\nmomentary head pose and predicted gazing behavior, and then performs a\nreliability-based weighted fusion. We demonstrate the efficacy of our framework\nwith extensive simulations and user experiments on a collected dataset\nfeaturing 20 subjects. Our results show that in comparison with\nstate-of-the-art eye trackers, the proposed framework provides not only a\nsignificant enhancement in accuracy but also a notable robustness. Our\nprototype system runs at 30 frames-per-second (fps) and achieves 1 degree\naccuracy under challenging experimental scenarios, which makes it suitable for\napplications demanding high accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 08:23:06 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 16:10:53 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Arar", "Nuri Murat", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1711.05962", "submitter": "Leilani Battle", "authors": "Leilani Battle, Peitong Duan, Zachery Miranda, Dana Mukusheva, Remco\n  Chang, Michael Stonebraker", "title": "Beagle: Automated Extraction and Interpretation of Visualizations from\n  the Web", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"How common is interactive visualization on the web?\" \"What is the most\npopular visualization design?\" \"How prevalent are pie charts really?\" These\nquestions intimate the role of interactive visualization in the real (online)\nworld. In this paper, we present our approach (and findings) to answering these\nquestions. First, we introduce Beagle, which mines the web for SVG-based\nvisualizations and automatically classifies them by type (i.e., bar, pie,\netc.). With Beagle, we extract over 41,000 visualizations across five different\ntools and repositories, and classify them with 86% accuracy, across 24\nvisualization types. Given this visualization collection, we study usage across\ntools. We find that most visualizations fall under four types: bar charts, line\ncharts, scatter charts, and geographic maps. Though controversial, pie charts\nare relatively rare in practice. Our findings also indicate that users may\nprefer tools that emphasize a succinct set of visualization types, and provide\ndiverse expert visualization examples.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 07:10:14 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Battle", "Leilani", ""], ["Duan", "Peitong", ""], ["Miranda", "Zachery", ""], ["Mukusheva", "Dana", ""], ["Chang", "Remco", ""], ["Stonebraker", "Michael", ""]]}, {"id": "1711.06068", "submitter": "Joos Behncke", "authors": "Joos Behncke, Robin Tibor Schirrmeister, Wolfram Burgard and Tonio\n  Ball", "title": "The signature of robot action success in EEG signals of a human\n  observer: Decoding and visualization using deep convolutional neural networks", "comments": null, "journal-ref": null, "doi": "10.1109/IWW-BCI.2018.8311531", "report-no": null, "categories": "cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of robotic assistive devices grows in our work and everyday\nlife. Cooperative scenarios involving both robots and humans require safe\nhuman-robot interaction. One important aspect here is the management of robot\nerrors, including fast and accurate online robot-error detection and\ncorrection. Analysis of brain signals from a human interacting with a robot may\nhelp identifying robot errors, but accuracies of such analyses have still\nsubstantial space for improvement. In this paper we evaluate whether a novel\nframework based on deep convolutional neural networks (deep ConvNets) could\nimprove the accuracy of decoding robot errors from the EEG of a human observer,\nboth during an object grasping and a pouring task. We show that deep ConvNets\nreached significantly higher accuracies than both regularized Linear\nDiscriminant Analysis (rLDA) and filter bank common spatial patterns (FB-CSP)\ncombined with rLDA, both widely used EEG classifiers. Deep ConvNets reached\nmean accuracies of 75% +/- 9 %, rLDA 65% +/- 10% and FB-CSP + rLDA 63% +/- 6%\nfor decoding of erroneous vs. correct trials. Visualization of the time-domain\nEEG features learned by the ConvNets to decode errors revealed spatiotemporal\npatterns that reflected differences between the two experimental paradigms.\nAcross subjects, ConvNet decoding accuracies were significantly correlated with\nthose obtained with rLDA, but not CSP, indicating that in the present context\nConvNets behaved more 'rLDA-like' (but consistently better), while in a\nprevious decoding study with another task but the same ConvNet architecture, it\nwas found to behave more 'CSP-like'. Our findings thus provide further support\nfor the assumption that deep ConvNets are a versatile addition to the existing\ntoolbox of EEG decoding techniques, and we discuss steps how ConvNet EEG\ndecoding performance could be further optimized.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 12:59:25 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Behncke", "Joos", ""], ["Schirrmeister", "Robin Tibor", ""], ["Burgard", "Wolfram", ""], ["Ball", "Tonio", ""]]}, {"id": "1711.06116", "submitter": "Aaqib Saeed", "authors": "Aaqib Saeed and Stojan Trajanovski", "title": "Personalized Driver Stress Detection with Multi-task Neural Networks\n  using Physiological Signals", "comments": "6 pages, 1 figure, 2 tables, NIPS - Machine Learning for Health\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stress can be seen as a physiological response to everyday emotional, mental\nand physical challenges. A long-term exposure to stressful situations can have\nnegative health consequences, such as increased risk of cardiovascular diseases\nand immune system disorder. Therefore, a timely stress detection can lead to\nsystems for better management and prevention in future circumstances. In this\npaper, we suggest a multi-task learning based neural network approach (with\nhard parameter sharing of mutual representation and task-specific layers) for\npersonalized stress recognition using skin conductance and heart rate from\nwearable devices. The proposed method is tested on multi-modal physiological\nresponses collected during real-world and simulator driving tasks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 12:20:11 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Saeed", "Aaqib", ""], ["Trajanovski", "Stojan", ""]]}, {"id": "1711.06134", "submitter": "Pascal Budner", "authors": "Pascal Budner, Joscha Eirich, Peter A. Gloor", "title": "\"Making you happy makes me happy\" -- Measuring Individual Mood with\n  Smartwatches", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a system to measure individual happiness based on interpreting\nbody sensors on smartwatches. In our prototype system we use a Pebble\nsmartwatch to track activity, heartrate, light level, and GPS coordinates, and\nextend it with external information such as weather data, humidity, and day of\nthe week. Training our machine learning-based mood prediction system using\nrandom forests with data manually entered into the smartwatch, we achieve\nprediction accuracy of up to 94%. We find that besides body signals, the\nweather data exerts a strong influence on mood. In addition our system also\nallows us to identify friends who are indicators of our positive or negative\nmood.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 01:34:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Budner", "Pascal", ""], ["Eirich", "Joscha", ""], ["Gloor", "Peter A.", ""]]}, {"id": "1711.06149", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Lina Yao, Salil S. Kanhere, Yunhao Liu, Tao Gu, Kaixuan\n  Chen", "title": "MindID: Person Identification from Brain Waves through Attention-based\n  Recurrent Neural Network", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person identification technology recognizes individuals by exploiting their\nunique, measurable physiological and behavioral characteristics. However, the\nstate-of-the-art person identification systems have been shown to be\nvulnerable, e.g., contact lenses can trick iris recognition and fingerprint\nfilms can deceive fingerprint sensors. EEG (Electroencephalography)-based\nidentification, which utilizes the users brainwave signals for identification\nand offers a more resilient solution, draw a lot of attention recently.\nHowever, the accuracy still requires improvement and very little work is\nfocusing on the robustness and adaptability of the identification system. We\npropose MindID, an EEG-based biometric identification approach, achieves higher\naccuracy and better characteristics. At first, the EEG data patterns are\nanalyzed and the results show that the Delta pattern contains the most\ndistinctive information for user identification. Then the decomposed Delta\npattern is fed into an attention-based Encoder-Decoder RNNs (Recurrent Neural\nNetworks) structure which assigns varies attention weights to different EEG\nchannels based on the channels importance. The discriminative representations\nlearned from the attention-based RNN are used to recognize the user\nidentification through a boosting classifier. The proposed approach is\nevaluated over 3 datasets (two local and one public). One local dataset (EID-M)\nis used for performance assessment and the result illustrate that our model\nachieves the accuracy of 0.982 which outperforms the baselines and the\nstate-of-the-art. Another local dataset (EID-S) and a public dataset (EEG-S)\nare utilized to demonstrate the robustness and adaptability, respectively. The\nresults indicate that the proposed approach has the potential to be largely\ndeployment in practice environment.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 15:42:22 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Zhang", "Xiang", ""], ["Yao", "Lina", ""], ["Kanhere", "Salil S.", ""], ["Liu", "Yunhao", ""], ["Gu", "Tao", ""], ["Chen", "Kaixuan", ""]]}, {"id": "1711.06259", "submitter": "Olga Dergachyova", "authors": "Olga Dergachyova and Xavier Morandi and Pierre Jannin", "title": "Analyzing Before Solving: Which Parameters Influence Low-Level Surgical\n  Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic low-level surgical activity recognition is today well-known\ntechnical bottleneck for smart and situation-aware assistance for the operating\nroom of the future. Our study sought to discover which sensors and signals\ncould facilitate this recognition. Low-level surgical activity represents\nsemantic information about a surgical procedure that is usually expressed by\nthe following elements: an action verb, surgical instrument, and operated\nanatomical structure. We hypothesized that activity recognition does not\nrequire sensors for all three elements. We conducted a large-scale study using\ndeep learning on semantic data from 154 operations from four different\nsurgeries. The results demonstrated that the instrument and verb encode similar\ninformation, meaning only one needs to be tracked, preferably the instrument.\nThe anatomical structure, however, provides some unique cues, and it is thus\ncrucial to recognize it. For all the studied surgeries, a combination of two\nelements, always including the structure, proved sufficient to confidently\nrecognize the activities. We also found that in the presence of noise,\ncombining the information about the instrument, structure, and historical\ncontext produced better results than a simple composition of all three\nelements. Several relevant observations about surgical practices were also made\nin this paper. Such findings provide cues for designing a new generation of\noperating rooms.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 22:45:23 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Dergachyova", "Olga", ""], ["Morandi", "Xavier", ""], ["Jannin", "Pierre", ""]]}, {"id": "1711.06328", "submitter": "Sung Cho", "authors": "Sung Jin Cho", "title": "PRE-render Content Using Tiles (PRECUT). 1. Large-Scale Compound-Target\n  Relationship Analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing a complex network is computationally intensive process and\ndepends heavily on the number of components in the network. One way to solve\nthis problem is not to render the network in real time. PRE-render Content\nUsing Tiles (PRECUT) is a process to convert any complex network into a\npre-rendered network. Tiles are generated from pre-rendered images at different\nzoom levels, and navigating the network simply becomes delivering relevant\ntiles. PRECUT is exemplified by performing large-scale compound-target\nrelationship analyses. Matched molecular pair (MMP) networks were created using\ncompounds and the target class description found in the ChEMBL database. To\nvisualize MMP networks, the MMP network viewer has been implemented in COMBINE\nand as a web application, hosted at http://cheminformatic.com/mmpnet/.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 11:00:46 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Cho", "Sung Jin", ""]]}, {"id": "1711.06553", "submitter": "Gangli Liu", "authors": "Gangli Liu", "title": "Understanding Graph and Understanding Map and their Potential\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the previously proposed concept Understanding Tree, this paper\nintroduces two concepts: Understanding Graph and Understanding Map, and\nexplores their potential applications. Understanding Graph and Understanding\nMap can be deemed as special cases of mind map, semantic network, or concept\nmap. The two main differences are: Firstly, the data sources for constructing\nUnderstanding Map and Understanding Graph are distinctive and simple. Secondly,\nthe relations between concepts in Understanding Graph and Understanding Map are\nmonotonous. Based on their characteristics, applications of them include\nquantitatively measuring a concept's complexity degree, quantitatively\nmeasuring a concept's importance degree in a domain, and computing an optimized\nlearning sequence for comprehending a concept etc. Further study involves\nevaluating their performances in these applications.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 14:33:59 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Liu", "Gangli", ""]]}, {"id": "1711.06953", "submitter": "Andrew Anderson", "authors": "Jonathan Dodge, Sean Penney, Claudia Hilderbrand, Andrew Anderson, and\n  Margaret Burnett", "title": "How the Experts Do It: Assessing and Explaining Agent Behaviors in\n  Real-Time Strategy Games", "comments": "12 pages, 11 figures, submitted to CHI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  How should an AI-based explanation system explain an agent's complex behavior\nto ordinary end users who have no background in AI? Answering this question is\nan active research area, for if an AI-based explanation system could\neffectively explain intelligent agents' behavior, it could enable the end users\nto understand, assess, and appropriately trust (or distrust) the agents\nattempting to help them. To provide insights into this question, we turned to\nhuman expert explainers in the real-time strategy domain, \"shoutcaster\", to\nunderstand (1) how they foraged in an evolving strategy game in real time, (2)\nhow they assessed the players' behaviors, and (3) how they constructed\npertinent and timely explanations out of their insights and delivered them to\ntheir audience. The results provided insights into shoutcasters' foraging\nstrategies for gleaning information necessary to assess and explain the\nplayers; a characterization of the types of implicit questions shoutcasters\nanswered; and implications for creating explanations by using the patterns\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 01:57:55 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Dodge", "Jonathan", ""], ["Penney", "Sean", ""], ["Hilderbrand", "Claudia", ""], ["Anderson", "Andrew", ""], ["Burnett", "Margaret", ""]]}, {"id": "1711.06976", "submitter": "Lex Fridman", "authors": "Lex Fridman, Daniel E. Brown, Michael Glazer, William Angell, Spencer\n  Dodd, Benedikt Jenik, Jack Terwilliger, Aleksandr Patsekin, Julia\n  Kindelsberger, Li Ding, Sean Seaman, Alea Mehler, Andrew Sipperley, Anthony\n  Pettinato, Bobbie Seppelt, Linda Angell, Bruce Mehler, Bryan Reimer", "title": "MIT Advanced Vehicle Technology Study: Large-Scale Naturalistic Driving\n  Study of Driver Behavior and Interaction with Automation", "comments": null, "journal-ref": "IEEE Access, vol. 7, pp. 102021-102038, 2019", "doi": "10.1109/ACCESS.2019.2926040", "report-no": null, "categories": "cs.CY cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the foreseeble future, human beings will likely remain an integral part\nof the driving task, monitoring the AI system as it performs anywhere from just\nover 0% to just under 100% of the driving. The governing objectives of the MIT\nAutonomous Vehicle Technology (MIT-AVT) study are to (1) undertake large-scale\nreal-world driving data collection that includes high-definition video to fuel\nthe development of deep learning based internal and external perception\nsystems, (2) gain a holistic understanding of how human beings interact with\nvehicle automation technology by integrating video data with vehicle state\ndata, driver characteristics, mental models, and self-reported experiences with\ntechnology, and (3) identify how technology and other factors related to\nautomation adoption and use can be improved in ways that save lives. In\npursuing these objectives, we have instrumented 23 Tesla Model S and Model X\nvehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6\nvehicles for both long-term (over a year per driver) and medium term (one month\nper driver) naturalistic driving data collection. Furthermore, we are\ncontinually developing new methods for analysis of the massive-scale dataset\ncollected from the instrumented vehicle fleet. The recorded data streams\ninclude IMU, GPS, CAN messages, and high-definition video streams of the driver\nface, the driver cabin, the forward roadway, and the instrument cluster (on\nselect vehicles). The study is on-going and growing. To date, we have 122\nparticipants, 15,610 days of participation, 511,638 miles, and 7.1 billion\nvideo frames. This paper presents the design of the study, the data collection\nhardware, the processing of the data, and the computer vision algorithms\ncurrently being used to extract actionable knowledge from the data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 06:46:21 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 04:02:20 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 01:13:57 GMT"}, {"version": "v4", "created": "Wed, 14 Aug 2019 11:17:00 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Fridman", "Lex", ""], ["Brown", "Daniel E.", ""], ["Glazer", "Michael", ""], ["Angell", "William", ""], ["Dodd", "Spencer", ""], ["Jenik", "Benedikt", ""], ["Terwilliger", "Jack", ""], ["Patsekin", "Aleksandr", ""], ["Kindelsberger", "Julia", ""], ["Ding", "Li", ""], ["Seaman", "Sean", ""], ["Mehler", "Alea", ""], ["Sipperley", "Andrew", ""], ["Pettinato", "Anthony", ""], ["Seppelt", "Bobbie", ""], ["Angell", "Linda", ""], ["Mehler", "Bruce", ""], ["Reimer", "Bryan", ""]]}, {"id": "1711.07154", "submitter": "Ke Wang", "authors": "Ke Wang, Zhendong Su", "title": "Interactive, Intelligent Tutoring for Auxiliary Constructions in\n  Geometry Proofs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometry theorem proving forms a major and challenging component in the K-12\nmathematics curriculum. A particular difficult task is to add auxiliary\nconstructions (i.e, additional lines or points) to aid proof discovery.\nAlthough there exist many intelligent tutoring systems proposed for geometry\nproofs, few teach students how to find auxiliary constructions. And the few\nexceptions are all limited by their underlying reasoning processes for\nsupporting auxiliary constructions. This paper tackles these weaknesses of\nprior systems by introducing an interactive geometry tutor, the Advanced\nGeometry Proof Tutor (AGPT). It leverages a recent automated geometry prover to\nprovide combined benefits that any geometry theorem prover or intelligent\ntutoring system alone cannot accomplish. In particular, AGPT not only can\nautomatically process images of geometry problems directly, but also can\ninteractively train and guide students toward discovering auxiliary\nconstructions on their own. We have evaluated AGPT via a pilot study with 78\nhigh school students. The study results show that, on training students how to\nfind auxiliary constructions, there is no significant perceived difference\nbetween AGPT and human tutors, and AGPT is significantly more effective than\nthe state-of-the-art geometry solver that produces human-readable proofs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 05:39:58 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Wang", "Ke", ""], ["Su", "Zhendong", ""]]}, {"id": "1711.07462", "submitter": "Reza Abiri", "authors": "Reza Abiri, Soheil Borhani, Xiaopeng Zhao, Yang Jiang", "title": "Real-time brain machine interaction via social robot gesture control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-Machine Interaction (BMI) system motivates interesting and promising\nresults in forward/feedback control consistent with human intention. It holds\ngreat promise for advancements in patient care and applications to\nneurorehabilitation. Here, we propose a novel neurofeedback-based BCI robotic\nplatform using a personalized social robot in order to assist patients having\ncognitive deficits through bilateral rehabilitation and mental training. For\ninitial testing of the platform, electroencephalography (EEG) brainwaves of a\nhuman user were collected in real time during tasks of imaginary movements.\nFirst, the brainwaves associated with imagined body kinematics parameters were\ndecoded to control a cursor on a computer screen in training protocol. Then,\nthe experienced subject was able to interact with a social robot via our\nreal-time BMI robotic platform. Corresponding to subject's imagery performance,\nhe/she received specific gesture movements and eye color changes as\nneural-based feedback from the robot. This hands-free neurofeedback interaction\nnot only can be used for mind control of a social robot's movements, but also\nsets the stage for application to enhancing and recovering mental abilities\nsuch as attention via training in humans by providing real-time neurofeedback\nfrom a social robot.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 18:54:42 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Abiri", "Reza", ""], ["Borhani", "Soheil", ""], ["Zhao", "Xiaopeng", ""], ["Jiang", "Yang", ""]]}, {"id": "1711.07574", "submitter": "Omer Rana", "authors": "Matilda Rhode and Omer Rana and Tim Edwards", "title": "Data Capture & Analysis to Assess Impact of Carbon Credit Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data enables Non-Governmental Organisations (NGOs) to quantify the impact of\ntheir initiatives to themselves and to others. The increasing amount of data\nstored today can be seen as a direct consequence of the falling costs in\nobtaining it. Cheap data acquisition harnesses existing communications networks\nto collect information. Globally, more people are connected by the mobile phone\nnetwork than by the Internet. We worked with Vita, a development organisation\nimplementing green initiatives to develop an SMS-based data collection\napplication to collect social data surrounding the impacts of their\ninitiatives. We present our system design and lessons learned from\non-the-ground testing.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 23:11:17 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Rhode", "Matilda", ""], ["Rana", "Omer", ""], ["Edwards", "Tim", ""]]}, {"id": "1711.07661", "submitter": "Dalin Zhang", "authors": "Kaixuan Chen, Lina Yao, Tao Gu, Zhiwen Yu, Xianzhi Wang, Dalin Zhang", "title": "Fullie and Wiselie: A Dual-Stream Recurrent Convolutional Attention\n  Model for Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal features play a key role in wearable sensor based Human Activity\nRecognition (HAR). Selecting the most salient features adaptively is a\npromising way to maximize the effectiveness of multimodal sensor data. In this\nregard, we propose a \"collect fully and select wisely (Fullie and Wiselie)\"\nprinciple as well as a dual-stream recurrent convolutional attention model,\nRecurrent Attention and Activity Frame (RAAF), to improve the recognition\nperformance. We first collect modality features and the relations between each\npair of features to generate activity frames, and then introduce an attention\nmechanism to select the most prominent regions from activity frames precisely.\nThe selected frames not only maximize the utilization of valid features but\nalso reduce the number of features to be computed effectively. We further\nanalyze the hyper-parameters, accuracy, interpretability, and annotation\ndependency of the proposed model based on extensive experiments. The results\nshow that RAAF achieves competitive performance on two benchmarked datasets and\nworks well in real life scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:42:32 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Chen", "Kaixuan", ""], ["Yao", "Lina", ""], ["Gu", "Tao", ""], ["Yu", "Zhiwen", ""], ["Wang", "Xianzhi", ""], ["Zhang", "Dalin", ""]]}, {"id": "1711.08019", "submitter": "Andrew Anderson", "authors": "Sean Penney, Jonathan Dodge, Claudia Hilderbrand, Andrew Anderson,\n  Logan Simpson, and Margaret Burnett", "title": "Toward Foraging for Understanding of StarCraft Agents: An Empirical\n  Study", "comments": "13 pages, 10 figures, to appear in ACM IUI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Assessing and understanding intelligent agents is a difficult task for users\nthat lack an AI background. A relatively new area, called \"Explainable AI,\" is\nemerging to help address this problem, but little is known about how users\nwould forage through information an explanation system might offer. To inform\nthe development of Explainable AI systems, we conducted a formative study,\nusing the lens of Information Foraging Theory, into how experienced users\nforaged in the domain of StarCraft to assess an agent. Our results showed that\nparticipants faced difficult foraging problems. These foraging problems caused\nparticipants to entirely miss events that were important to them, reluctantly\nchoose to ignore actions they did not want to ignore, and bear high cognitive,\nnavigation, and information costs to access the information they needed.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 20:21:33 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 19:59:57 GMT"}, {"version": "v3", "created": "Tue, 26 Dec 2017 16:04:55 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Penney", "Sean", ""], ["Dodge", "Jonathan", ""], ["Hilderbrand", "Claudia", ""], ["Anderson", "Andrew", ""], ["Simpson", "Logan", ""], ["Burnett", "Margaret", ""]]}, {"id": "1711.08566", "submitter": "Samer Nashed", "authors": "Samer B. Nashed and Joydeep Biswas", "title": "Human-in-the-Loop SLAM", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building large-scale, globally consistent maps is a challenging problem, made\nmore difficult in environments with limited access, sparse features, or when\nusing data collected by novice users. For such scenarios, where\nstate-of-the-art mapping algorithms produce globally inconsistent maps, we\nintroduce a systematic approach to incorporating sparse human corrections,\nwhich we term Human-in-the-Loop Simultaneous Localization and Mapping\n(HitL-SLAM). Given an initial factor graph for pose graph SLAM, HitL-SLAM\naccepts approximate, potentially erroneous, and rank-deficient human input,\ninfers the intended correction via expectation maximization (EM),\nback-propagates the extracted corrections over the pose graph, and finally\njointly optimizes the factor graph including the human inputs as human\ncorrection factor terms, to yield globally consistent large-scale maps. We thus\ncontribute an EM formulation for inferring potentially rank-deficient human\ncorrections to mapping, and human correction factor extensions to the factor\ngraphs for pose graph SLAM that result in a principled approach to joint\noptimization of the pose graph while simultaneously accounting for multiple\nforms of human correction. We present empirical results showing the\neffectiveness of HitL-SLAM at generating globally accurate and consistent maps\neven when given poor initial estimates of the map.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 03:35:55 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Nashed", "Samer B.", ""], ["Biswas", "Joydeep", ""]]}, {"id": "1711.08992", "submitter": "Kalin Stefanov", "authors": "Kalin Stefanov, Jonas Beskow and Giampiero Salvi", "title": "Self-Supervised Vision-Based Detection of the Active Speaker as Support\n  for Socially-Aware Language Acquisition", "comments": "10 pages, IEEE Transactions on Cognitive and Developmental Systems", "journal-ref": null, "doi": "10.1109/TCDS.2019.2927941", "report-no": null, "categories": "cs.CV cs.CL cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a self-supervised method for visual detection of the\nactive speaker in a multi-person spoken interaction scenario. Active speaker\ndetection is a fundamental prerequisite for any artificial cognitive system\nattempting to acquire language in social settings. The proposed method is\nintended to complement the acoustic detection of the active speaker, thus\nimproving the system robustness in noisy conditions. The method can detect an\narbitrary number of possibly overlapping active speakers based exclusively on\nvisual information about their face. Furthermore, the method does not rely on\nexternal annotations, thus complying with cognitive development. Instead, the\nmethod uses information from the auditory modality to support learning in the\nvisual domain. This paper reports an extensive evaluation of the proposed\nmethod using a large multi-person face-to-face interaction dataset. The results\nshow good performance in a speaker dependent setting. However, in a speaker\nindependent setting the proposed method yields a significantly lower\nperformance. We believe that the proposed method represents an essential\ncomponent of any artificial cognitive system or robotic platform engaging in\nsocial interactions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 14:45:06 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 17:55:38 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Stefanov", "Kalin", ""], ["Beskow", "Jonas", ""], ["Salvi", "Giampiero", ""]]}, {"id": "1711.09206", "submitter": "Sherif Abdulatif", "authors": "Sherif Abdulatif (1), Bernhard Kleiner (1), Fady Aziz (1), Christopher\n  Riehs (1), Rory Cooper (2), Urs Schneider (1) ((1) Fraunhofer Institute for\n  Manufacturing Engineering and Automation IPA, (2) Human Engineering Research\n  Laboratories, University of Pittsburg)", "title": "Stairs Detection for Enhancing Wheelchair Capabilities Based on Radar\n  Sensors", "comments": "5 pages, Accepted and presented in 2017 IEEE 6th Global Conference on\n  Consumer Electronics (GCCE 2017)", "journal-ref": "IEEE 6th Global Conference on Consumer Electronics (GCCE) 2017 1 5", "doi": "10.1109/GCCE.2017.8229270", "report-no": null, "categories": "cs.HC cs.RO eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powered wheelchair users encounter barriers to their mobility everyday.\nEntering a building with non barrier-free areas can massively impact the user\nmobility related activities. There are a few commercial devices and some\nexperimental that can climb stairs using for instance adaptive wheels with\njoints or caterpillar drive. These systems rely on the use for sensing and\ncontrol. For safe automated obstacle crossing, a robust and environment\ninvariant detection of the surrounding is necessary. Radar may prove to be a\nsuitable sensor for its capability to handle harsh outdoor environmental\nconditions. In this paper, we introduce a mirror based two dimensional\nFrequency-Modulated Continuous-Wave (FMCW) radar scanner for stair detection. A\nradar image based stair dimensioning approach is presented and tested under\nlaboratory and realistic conditions.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 07:11:13 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Abdulatif", "Sherif", ""], ["Kleiner", "Bernhard", ""], ["Aziz", "Fady", ""], ["Riehs", "Christopher", ""], ["Cooper", "Rory", ""], ["Schneider", "Urs", ""]]}, {"id": "1711.09561", "submitter": "Emad Barsoum", "authors": "Emad Barsoum, John Kender and Zicheng Liu", "title": "HP-GAN: Probabilistic 3D human motion prediction via GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting and understanding human motion dynamics has many applications,\nsuch as motion synthesis, augmented reality, security, and autonomous vehicles.\nDue to the recent success of generative adversarial networks (GAN), there has\nbeen much interest in probabilistic estimation and synthetic data generation\nusing deep neural network architectures and learning algorithms.\n  We propose a novel sequence-to-sequence model for probabilistic human motion\nprediction, trained with a modified version of improved Wasserstein generative\nadversarial networks (WGAN-GP), in which we use a custom loss function designed\nfor human motion prediction. Our model, which we call HP-GAN, learns a\nprobability density function of future human poses conditioned on previous\nposes. It predicts multiple sequences of possible future human poses, each from\nthe same input sequence but a different vector z drawn from a random\ndistribution. Furthermore, to quantify the quality of the non-deterministic\npredictions, we simultaneously train a motion-quality-assessment model that\nlearns the probability that a given skeleton sequence is a real human motion.\n  We test our algorithm on two of the largest skeleton datasets: NTURGB-D and\nHuman3.6M. We train our model on both single and multiple action types. Its\npredictive power for long-term motion estimation is demonstrated by generating\nmultiple plausible futures of more than 30 frames from just 10 frames of input.\nWe show that most sequences generated from the same input have more than 50\\%\nprobabilities of being judged as a real human sequence. We will release all the\ncode used in this paper to Github.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 07:07:11 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Barsoum", "Emad", ""], ["Kender", "John", ""], ["Liu", "Zicheng", ""]]}, {"id": "1711.09562", "submitter": "Boris Ba\\v{c}i\\'c", "authors": "Boris Ba\\v{c}i\\'c and Patria Hume", "title": "Computational intelligence for qualitative coaching diagnostics:\n  Automated assessment of tennis swings to improve performance and safety", "comments": "TBA", "journal-ref": "Qualitative Coaching Diagnostics: Automated Assessment of Tennis\n  Swings to Improve Performance and Safety,\" Big Data, vol. 6, pp. 291-304,\n  2018", "doi": "10.1089/big.2018.0062", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Coaching technology, wearables and exergames can provide quantitative\nfeedback based on measured activity, but there is little evidence of\nqualitative feedback to aid technique improvement. To achieve personalised\nqualitative feedback, we demonstrated a proof-of-concept prototype combining\nkinesiology and computational intelligence that could help improving tennis\nswing technique utilising three-dimensional tennis motion data acquired from\nmulti-camera video. Expert data labelling relied on virtual 3D stick figure\nreplay. Diverse assessment criteria for novice to intermediate skill levels and\nconfigurable coaching scenarios matched with a variety of tennis swings (22\nbackhands and 21 forehands), included good technique and common errors. A set\nof selected coaching rules was transferred to adaptive assessment modules able\nto learn from data, evolve their internal structures and produce autonomous\npersonalised feedback including verbal cues over virtual camera 3D replay and\nan end-of-session progress report. The prototype demonstrated autonomous\nassessment on future data based on learning from prior examples, aligned with\nskill level, flexible coaching scenarios and coaching rules. The generated\nintuitive diagnostic feedback consisted of elements of safety and performance\nfor tennis swing technique, where each swing sample was compared with the\nexpert. For safety aspects of the relative swing width, the prototype showed\nimproved assessment ...\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 07:15:17 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 09:45:00 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Ba\u010di\u0107", "Boris", ""], ["Hume", "Patria", ""]]}, {"id": "1711.09700", "submitter": "Rudy Arthur", "authors": "Rudy Arthur, Hywel Williams", "title": "Scaling laws in geo-located Twitter data", "comments": "21 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe and report on a systematic relationship between population density\nand Twitter use. Number of tweets, number of users and population per unit area\nare related by power laws, with exponents greater than one, that are consistent\nwith each other and across a range of spatial scales. This implies that\npopulation density can accurately predict Twitter activity. Furthermore this\ntrend can be used to identify `anomalous' areas that deviate from the trend.\nAnalysis of geo-tagged and place-tagged tweets show that geo-tagged tweets are\ndifferent with respect to user type and content. Our findings have implications\nfor the spatial analysis of Twitter data and for understanding demographic\nbiases in the Twitter user base.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 14:19:40 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Arthur", "Rudy", ""], ["Williams", "Hywel", ""]]}, {"id": "1711.09714", "submitter": "Giampiero Salvi", "authors": "Giampiero Salvi, Luis Montesano, Alexandre Bernardino, Jos\\'e\n  Santos-Victor", "title": "Language Bootstrapping: Learning Word Meanings From Perception-Action\n  Association", "comments": "code available at\n  https://github.com/giampierosalvi/AffordancesAndSpeech", "journal-ref": "in IEEE Transactions on Systems, Man, and Cybernetics, Part B\n  (Cybernetics), Volume: 42 Issue: 3, year 2012, pages 660-671", "doi": "10.1109/TSMCB.2011.2172420", "report-no": null, "categories": "cs.RO cs.CL cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of bootstrapping language acquisition for an\nartificial system similarly to what is observed in experiments with human\ninfants. Our method works by associating meanings to words in manipulation\ntasks, as a robot interacts with objects and listens to verbal descriptions of\nthe interactions. The model is based on an affordance network, i.e., a mapping\nbetween robot actions, robot perceptions, and the perceived effects of these\nactions upon objects. We extend the affordance model to incorporate spoken\nwords, which allows us to ground the verbal symbols to the execution of actions\nand the perception of the environment. The model takes verbal descriptions of a\ntask as the input and uses temporal co-occurrence to create links between\nspeech utterances and the involved objects, actions, and effects. We show that\nthe robot is able form useful word-to-meaning associations, even without\nconsidering grammatical structure in the learning process and in the presence\nof recognition errors. These word-to-meaning associations are embedded in the\nrobot's own understanding of its actions. Thus, they can be directly used to\ninstruct the robot to perform tasks and also allow to incorporate context in\nthe speech recognition task. We believe that the encouraging results with our\napproach may afford robots with a capacity to acquire language descriptors in\ntheir operation's environment as well as to shed some light as to how this\nchallenging process develops with human infants.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 14:42:26 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Salvi", "Giampiero", ""], ["Montesano", "Luis", ""], ["Bernardino", "Alexandre", ""], ["Santos-Victor", "Jos\u00e9", ""]]}, {"id": "1711.09767", "submitter": "Matan Sela", "authors": "Matan Sela, Pingmei Xu, Junfeng He, Vidhya Navalpakkam and Dmitry\n  Lagun", "title": "GazeGAN - Unpaired Adversarial Image Generation for Gaze Estimation", "comments": "Project was done when the first author was at Google Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has demonstrated the ability to estimate gaze on mobile\ndevices by performing inference on the image from the phone's front-facing\ncamera, and without requiring specialized hardware. While this offers wide\npotential applications such as in human-computer interaction, medical diagnosis\nand accessibility (e.g., hands free gaze as input for patients with motor\ndisorders), current methods are limited as they rely on collecting data from\nreal users, which is a tedious and expensive process that is hard to scale\nacross devices. There have been some attempts to synthesize eye region data\nusing 3D models that can simulate various head poses and camera settings,\nhowever these lack in realism.\n  In this paper, we improve upon a recently suggested method, and propose a\ngenerative adversarial framework to generate a large dataset of high resolution\ncolorful images with high diversity (e.g., in subjects, head pose, camera\nsettings) and realism, while simultaneously preserving the accuracy of gaze\nlabels. The proposed approach operates on extended regions of the eye, and even\ncompletes missing parts of the image. Using this rich synthesized dataset, and\nwithout using any additional training data from real users, we demonstrate\nimprovements over state-of-the-art for estimating 2D gaze position on mobile\ndevices. We further demonstrate cross-device generalization of model\nperformance, as well as improved robustness to diverse head pose, blur and\ndistance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 15:32:36 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Sela", "Matan", ""], ["Xu", "Pingmei", ""], ["He", "Junfeng", ""], ["Navalpakkam", "Vidhya", ""], ["Lagun", "Dmitry", ""]]}, {"id": "1711.09918", "submitter": "Behzad Tabibian", "authors": "Jooyeon Kim, Behzad Tabibian, Alice Oh, Bernhard Schoelkopf, Manuel\n  Gomez-Rodriguez", "title": "Leveraging the Crowd to Detect and Reduce the Spread of Fake News and\n  Misinformation", "comments": "To appear at the 11th ACM International Conference on Web Search and\n  Data Mining (WSDM 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social networking sites are experimenting with the following\ncrowd-powered procedure to reduce the spread of fake news and misinformation:\nwhenever a user is exposed to a story through her feed, she can flag the story\nas misinformation and, if the story receives enough flags, it is sent to a\ntrusted third party for fact checking. If this party identifies the story as\nmisinformation, it is marked as disputed. However, given the uncertain number\nof exposures, the high cost of fact checking, and the trade-off between flags\nand exposures, the above mentioned procedure requires careful reasoning and\nsmart algorithms which, to the best of our knowledge, do not exist to date.\n  In this paper, we first introduce a flexible representation of the above\nprocedure using the framework of marked temporal point processes. Then, we\ndevelop a scalable online algorithm, Curb, to select which stories to send for\nfact checking and when to do so to efficiently reduce the spread of\nmisinformation with provable guarantees. In doing so, we need to solve a novel\nstochastic optimal control problem for stochastic differential equations with\njumps, which is of independent interest. Experiments on two real-world datasets\ngathered from Twitter and Weibo show that our algorithm may be able to\neffectively reduce the spread of fake news and misinformation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 19:00:08 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Kim", "Jooyeon", ""], ["Tabibian", "Behzad", ""], ["Oh", "Alice", ""], ["Schoelkopf", "Bernhard", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1711.10171", "submitter": "Abhinav Mehrotra", "authors": "Abhinav Mehrotra, Mirco Musolesi", "title": "Intelligent Notification Systems: A Survey of the State of the Art and\n  Research Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Notifications provide a unique mechanism for increasing the effectiveness of\nreal-time information delivery systems. However, notifications that demand\nusers' attention at inopportune moments are more likely to have adverse effects\nand might become a cause of potential disruption rather than proving beneficial\nto users. In order to address these challenges a variety of intelligent\nnotification mechanisms based on monitoring and learning users' behavior have\nbeen proposed. The goal of such mechanisms is maximizing users' receptivity to\nthe delivered information by automatically inferring the right time and the\nright context for sending a certain type of information.\n  This article provides an overview of the current state of the art in the area\nof intelligent notification mechanisms that relies on the awareness of users'\ncontext and preferences. More specifically, we first present a survey of\nstudies focusing on understanding and modeling users' interruptibility and\nreceptivity to notifications from desktops and mobile devices. Then, we discuss\nthe existing challenges and opportunities in developing mechanisms for\nintelligent notification systems in a variety of application scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 08:17:14 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 17:19:47 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Mehrotra", "Abhinav", ""], ["Musolesi", "Mirco", ""]]}, {"id": "1711.10746", "submitter": "Charles Martin", "authors": "Charles P. Martin and Jim Torresen", "title": "RoboJam: A Musical Mixture Density Network for Collaborative Touchscreen\n  Interaction", "comments": null, "journal-ref": "Computational Intelligence in Music, Sound, Art and Design.\n  EvoMUSART 2018. Lecture Notes in Computer Science, vol 10783", "doi": "10.1007/978-3-319-77583-8_11", "report-no": null, "categories": "cs.HC cs.NE cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RoboJam is a machine-learning system for generating music that assists users\nof a touchscreen music app by performing responses to their short\nimprovisations. This system uses a recurrent artificial neural network to\ngenerate sequences of touchscreen interactions and absolute timings, rather\nthan high-level musical notes. To accomplish this, RoboJam's network uses a\nmixture density layer to predict appropriate touch interaction locations in\nspace and time. In this paper, we describe the design and implementation of\nRoboJam's network and how it has been integrated into a touchscreen music app.\nA preliminary evaluation analyses the system in terms of training, musical\ngeneration and user interaction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 09:48:06 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Martin", "Charles P.", ""], ["Torresen", "Jim", ""]]}, {"id": "1711.11074", "submitter": "Rajiv Khadka", "authors": "Rajiv Khadka, James Money, Amy Banic", "title": "Towards Cross-Surface Immersion Using Low Cost Multi-Sensory Output Cues\n  to Support Proxemics and Kinesics Across Heterogeneous Systems", "comments": "7 pages, 7 figures, ISS Cross-Surface: Challenges and Opportunities\n  of Spatial and Proxemic Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaboration in immersive systems can be achieved by using an immersive\ndisplay system (i.e. CAVE and Head-Mounted Display), but how do we communicate\nimmersion cross-surface for low immersive displays, such as desktops, tablets,\nand smartphones? In this paper, we present a discussion of proxemics and\nkinesics to support based on observation of physical collaboration. We present\nour research agenda to investigate low-cost multi-sensory output cues to\ncommunicate proxemics and kinesics aspects cross-surface. Doing so may increase\nthe level of presence, co-presence, and immersion, and improve the\neffectiveness of collaboration cross-surface.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:36:10 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Khadka", "Rajiv", ""], ["Money", "James", ""], ["Banic", "Amy", ""]]}, {"id": "1711.11115", "submitter": "Naimul Khan", "authors": "Randy Tan, Naimul Khan, Ling Guan", "title": "Real-Time System for Human Activity Analysis", "comments": "Accepted at IEEE Symposium on Multimedia (ISM) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a real-time human activity analysis system, where a user's\nactivity can be quantiatively evaluated with respect to a ground truth\nrecording. We use two Kinects to solve the ptorblem of self-occlusion through\nextraction optimal joint positions using Singular Value Decomposition (SVD) and\nSequential Quadratic Programming (SQP). Incremental Dynamic Time Warping (IDTW)\nis used to compare the user and expert (ground truth) to quantiatively score\nthe user's performance. Furthermore, the user's performance is displayed\nthrough a visual feedback system, where colors on the skeleton represent the\nuser's score. Our experiements use a motion capture suit as ground truth to\ncompare our dual Kinect setup to a single Kinect. We also show that with out\nvisual feedback method, users gain statistically significant boost to learning\nas opposed to watching a simple video.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 21:35:56 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Tan", "Randy", ""], ["Khan", "Naimul", ""], ["Guan", "Ling", ""]]}, {"id": "1711.11123", "submitter": "Naimul Khan", "authors": "Naimul Khan, Riadh Ksantini, Ling Guan", "title": "A Novel Image-centric Approach Towards Direct Volume Rendering", "comments": "To appear in the ACM Transactions in Intelligent Systems and\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer Function (TF) generation is a fundamental problem in Direct Volume\nRendering (DVR). A TF maps voxels to color and opacity values to reveal inner\nstructures. Existing TF tools are complex and unintuitive for the users who are\nmore likely to be medical professionals than computer scientists. In this\npaper, we propose a novel image-centric method for TF generation where instead\nof complex tools, the user directly manipulates volume data to generate DVR.\nThe user's work is further simplified by presenting only the most informative\nvolume slices for selection. Based on the selected parts, the voxels are\nclassified using our novel Sparse Nonparametric Support Vector Machine\nclassifier, which combines both local and near-global distributional\ninformation of the training data. The voxel classes are mapped to aesthetically\npleasing and distinguishable color and opacity values using harmonic colors.\nExperimental results on several benchmark datasets and a detailed user survey\nshow the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 21:47:46 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Khan", "Naimul", ""], ["Ksantini", "Riadh", ""], ["Guan", "Ling", ""]]}, {"id": "1711.11141", "submitter": "Xiaofei Wang", "authors": "Xiaofei Wang and Yonghong Yan and Hynek Hermansky", "title": "Stream Attention for far-field multi-microphone ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stream attention framework has been applied to the posterior probabilities\nof the deep neural network (DNN) to improve the far-field automatic speech\nrecognition (ASR) performance in the multi-microphone configuration. The stream\nattention scheme has been realized through an attention vector, which is\nderived by predicting the ASR performance from the phoneme posterior\ndistribution of individual microphone stream, focusing the recognizer's\nattention to more reliable microphones. Investigation on the various ASR\nperformance measures has been carried out using the real recorded dataset.\nExperiments results show that the proposed framework has yielded substantial\nimprovements in word error rate (WER).\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 22:45:05 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Wang", "Xiaofei", ""], ["Yan", "Yonghong", ""], ["Hermansky", "Hynek", ""]]}, {"id": "1711.11205", "submitter": "Palash Thakur", "authors": "Anubhav Apurva, Palash Thakur, Anupam Misra", "title": "Aiding the Visually Impaired: Developing an efficient Braille Printer", "comments": "6 pages. IEEE accepted paper (not published yet) International\n  Conference on Advances in Computing, Communications and Informatics\n  (ICACCI-2017)", "journal-ref": null, "doi": "10.1109/ICACCI.2017.8126160", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the large number of partially or completely visually impaired persons in\nsociety, their integration as productive, educated and capable members of\nsociety is hampered heavily by a pervasively high level of braille illiteracy.\nThis problem is further compounded by the fact that braille printers are\nprohibitively expensive - generally starting from two thousand US dollars,\nbeyond the reach of the common man. Over the period of a year, the authors have\ntried to develop a Braille printer which attempts to overcome the problems\ninherent in commercial printers. The purpose of this paper, therefore, is to\nintroduce two prototypes - the first with an emphasis of cost-effectiveness,\nand the second prototype, which is more experimental and aims to eliminate\nseveral demerits of Braille printing. The first prototype has been constructed\nat a cost significantly less than the existing commercial braille printers.\nBoth the prototypes of the device have been constructed, which will be shown.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 03:30:13 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Apurva", "Anubhav", ""], ["Thakur", "Palash", ""], ["Misra", "Anupam", ""]]}, {"id": "1711.11243", "submitter": "Adam Ibrahim", "authors": "Adam Ibrahim, Brandon Huynh, Jonathan Downey, Tobias H\\\"ollerer,\n  Dorothy Chun, John O'Donovan", "title": "ARbis Pictus: A Study of Language Learning with Augmented Reality", "comments": "TVCG version", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics ( Volume:\n  24 , Issue: 11 , Nov. 2018 )", "doi": "10.1109/TVCG.2018.2868568", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes \"ARbis Pictus\" --a novel system for immersive language\nlearning through dynamic labeling of real-world objects in augmented reality.\nWe describe a within-subjects lab-based study (N=52) that explores the effect\nof our system on participants learning nouns in an unfamiliar foreign language,\ncompared to a traditional flashcard-based approach. Our results show that the\nimmersive experience of learning with virtual labels on real-world objects is\nboth more effective and more enjoyable for the majority of participants,\ncompared to flashcards. Specifically, when participants learned through\naugmented reality, they scored significantly better by 7% (p=0.011) on\nproductive recall tests performed same-day, and significantly better by 21%\n(p=0.001) on 4-day delayed productive recall post tests than when they learned\nusing the flashcard method. We believe this result is an indication of the\nstrong potential for language learning in augmented reality, particularly\nbecause of the improvement shown in sustained recall compared to the\ntraditional approach.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 05:46:01 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 18:41:34 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Ibrahim", "Adam", ""], ["Huynh", "Brandon", ""], ["Downey", "Jonathan", ""], ["H\u00f6llerer", "Tobias", ""], ["Chun", "Dorothy", ""], ["O'Donovan", "John", ""]]}, {"id": "1711.11319", "submitter": "Fabio Paolizzo", "authors": "Fabio Paolizzo, Colin G. Johnson", "title": "Creative Autonomy Through Salience and Multidominance in Interactive\n  Music Systems: Evaluating an Implementation", "comments": "23 pages, 5 figures, 2 tables, 2 supplement material (audio/video\n  links)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive music systems always exhibit some autonomy in the creative\nprocess. The capacity to generate novel material while retaining mutuality to\nthe interaction is proposed here as the bare minimum for creative autonomy in\nsuch systems. Video Interactive VST Orchestra is a system incorporating an\nadaptive technique based both on the concept of salience as a means for\nretaining mutuality to the interplay and on multidominance in the adaptive\ngeneration process as a means for introducing novelty. We call this property\nreflexive multidominance. A case study providing evidence of such creative\nautonomy in VIVO is presented.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 11:08:34 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 20:49:58 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Paolizzo", "Fabio", ""], ["Johnson", "Colin G.", ""]]}, {"id": "1711.11368", "submitter": "Paul Vickers", "authors": "Paul Vickers and Robert H\\\"oldrich", "title": "Direct Segmented Sonification of Characteristic Features of the Data\n  Domain", "comments": "10 pages, 12 figures, pre-print of submitted journal article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sonification and audification create auditory displays of datasets.\nAudification translates data points into digital audio samples and the auditory\ndisplay's duration is determined by the playback rate. Like audification,\nauditory graphs maintain the temporal relationships of data while using\nparameter mappings (typically data-to-frequency) to represent the ordinate\nvalues. Such direct approaches have the advantage of presenting the data stream\n`as is' without the imposed interpretations or accentuation of particular\nfeatures found in indirect approaches. However, datasets can often be\nsubdivided into short non-overlapping variable length segments that each\nencapsulate a discrete unit of domain-specific significant information and\ncurrent direct approaches cannot represent these. We present Direct Segmented\nSonification (DSSon) for highlighting the segments' data distributions as\nindividual sonic events. Using domain knowledge to segment data, DSSon presents\nsegments as discrete auditory gestalts while retaining the overall temporal\nregime and relationships of the dataset. The method's structural decoupling\nfrom the sound stream's formation means playback speed is independent of the\nindividual sonic event durations, thereby offering highly flexible time\ncompression/stretching to allow zooming into or out of the data. Demonstrated\nby three models applied to biomechanical data, DSSon displays high directness,\nletting the data `speak' for themselves.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 13:13:14 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 13:36:13 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Vickers", "Paul", ""], ["H\u00f6ldrich", "Robert", ""]]}, {"id": "1711.11460", "submitter": "Jianwei Qian", "authors": "Jianwei Qian, Haohua Du, Jiahui Hou, Linlin Chen, Taeho Jung,\n  Xiang-Yang Li, Yu Wang, Yanbo Deng", "title": "VoiceMask: Anonymize and Sanitize Voice Input on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice input has been tremendously improving the user experience of mobile\ndevices by freeing our hands from typing on the small screen. Speech\nrecognition is the key technology that powers voice input, and it is usually\noutsourced to the cloud for the best performance. However, the cloud might\ncompromise users' privacy by identifying their identities by voice, learning\ntheir sensitive input content via speech recognition, and then profiling the\nmobile users based on the content. In this paper, we design an intermediate\nbetween users and the cloud, named VoiceMask, to sanitize users' voice data\nbefore sending it to the cloud for speech recognition. We analyze the potential\nprivacy risks and aim to protect users' identities and sensitive input content\nfrom being disclosed to the cloud. VoiceMask adopts a carefully designed voice\nconversion mechanism that is resistant to several attacks. Meanwhile, it\nutilizes an evolution-based keyword substitution technique to sanitize the\nvoice input content. The two sanitization phases are all performed in the\nresource-limited mobile device while still maintaining the usability and\naccuracy of the cloud-supported speech recognition service. We implement the\nvoice sanitizer on Android systems and present extensive experimental results\nthat validate the effectiveness and efficiency of our app. It is demonstrated\nthat we are able to reduce the chance of a user's voice being identified from\n50 people by 84% while keeping the drop of speech recognition accuracy within\n14.2%.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 15:18:07 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Qian", "Jianwei", ""], ["Du", "Haohua", ""], ["Hou", "Jiahui", ""], ["Chen", "Linlin", ""], ["Jung", "Taeho", ""], ["Li", "Xiang-Yang", ""], ["Wang", "Yu", ""], ["Deng", "Yanbo", ""]]}, {"id": "1711.11502", "submitter": "Evgenia Paxinou", "authors": "Evgenia Paxinou, Vasilis Zafeiropoulos, Athanasios Sypsas, Chairi\n  Kiourt, and Dimitris Kalles", "title": "Assessing the Impact of Virtualizing Physical Labs", "comments": "4 pages, 5 figures, 1 table, Submitted in November 14, 2017 in\n  Bulleting of the Technical Committee on Learning Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual laboratories are the new online educational trend for communicating\nto students practical skills of science. In this paper we report on a\ncomparison of techniques for familiarizing distance learning students with a 3D\nvirtual biology laboratory, in order to prepare them for their microscopy\nexperiment in their physical wet lab. Initial training for these students was\nprovided at a distance, via Skype. Their progress was assessed through Pre and\nPost-tests and compared to those of students who opted to only prepare for\ntheir wet lab using the conventional face-to-face educational method, which was\nprovided for all students. Our results provide preliminary answers to questions\nsuch as whether the incorporation of a virtual lab in the educational process\nwill improve the quality of distance learning education and whether a virtual\nlab can be a valuable educational supplement to students enrolled in laboratory\ncourses on Biology.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 16:44:54 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Paxinou", "Evgenia", ""], ["Zafeiropoulos", "Vasilis", ""], ["Sypsas", "Athanasios", ""], ["Kiourt", "Chairi", ""], ["Kalles", "Dimitris", ""]]}]