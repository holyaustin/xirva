[{"id": "1912.00019", "submitter": "Deniz Ekiz", "authors": "Deniz Ekiz, Yekta Said Can, Cem Ersoy", "title": "Long Short-Term Network Based Unobtrusive Perceived Workload Monitoring\n  with Consumer Grade Smartwatches in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous high perceived workload has a negative impact on the individual's\nwell-being. Prior works focused on detecting the workload with medical-grade\nwearable systems in the restricted settings, and the effect of applying deep\nlearning techniques for perceived workload detection in the wild settings is\nnot investigated. We present an unobtrusive, comfortable, pervasive and\naffordable Long Short-Term Memory Network based continuous workload monitoring\nsystem based on a smartwatch application that monitors the perceived workload\nof individuals in the wild. We make use of modern consumer-grade smartwatches.\nWe have recorded physiological data from daily life with perceived workload\nquestionnaires from subjects in their real-life environments over a month. The\nmodel was trained and evaluated with the daily-life physiological data coming\nfrom different days which makes it robust to daily changes in the heart rate\nvariability, that we use with accelerometer features to asses low and high\nworkload. Our system has the capability of removing motion-related artifacts\nand detecting perceived workload by using traditional and deep classifiers. We\ndiscussed the problems related to in the wild applications with the\nconsumer-grade smartwatches. We showed that Long Short-Term Memory Network\noutperforms traditional classifiers on discrimination of low and high workload\nwith smartwatches in the wild.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 11:11:24 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ekiz", "Deniz", ""], ["Can", "Yekta Said", ""], ["Ersoy", "Cem", ""]]}, {"id": "1912.00045", "submitter": "Ramiro Velazquez", "authors": "Jonnatan Arroyo, Ramiro Velazquez", "title": "Mechanism for Embossing Braille Characters on Paper: Conceptual Design", "comments": "Presented at RESNA (Rehabilitation Engineering and Assistive\n  Technology Society of North America) Annual Conference, Baltimore, MD, USA,\n  2012", "journal-ref": "2012 RESNA Annual Conference, Baltimore, MD, USA,2012, pp. 1-3", "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the conceptual design of a low-cost simple printer head\nfor Braille embossers. Such device consists of a set of three rotary\ncam-follower mechanisms that, upon actuation, produce deformation on paper. The\nset of cam-followers is actuated by a single servomotor which rotation\ndetermines which cam-follower strikes the paper. Braille characters are quickly\nembossed by column using the proposed system. The aim of this research is to\nprovide new actuation ideas for making Braille embossers more affordable.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 19:23:55 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Arroyo", "Jonnatan", ""], ["Velazquez", "Ramiro", ""]]}, {"id": "1912.00124", "submitter": "Jihyeon Lee", "authors": "Jihyeon Lee, Sho Arora", "title": "A Free Lunch in Generating Datasets: Building a VQG and VQA System with\n  Attention and Humans in the Loop", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their importance in training artificial intelligence systems, large\ndatasets remain challenging to acquire. For example, the ImageNet dataset\nrequired fourteen million labels of basic human knowledge, such as whether an\nimage contains a chair. Unfortunately, this knowledge is so simple that it is\ntedious for human annotators but also tacit enough such that they are\nnecessary. However, human collaborative efforts for tasks like labeling massive\namounts of data are costly, inconsistent, and prone to failure, and this method\ndoes not resolve the issue of the resulting dataset being static in nature.\nWhat if we asked people questions they want to answer and collected their\nresponses as data? This would mean we could gather data at a much lower cost,\nand expanding a dataset would simply become a matter of asking more questions.\nWe focus on the task of Visual Question Answering (VQA) and propose a system\nthat uses Visual Question Generation (VQG) to produce questions, asks them to\nsocial media users, and collects their responses. We present two models that\ncan then parse clean answers from the noisy human responses significantly\nbetter than our baselines, with the goal of eventually incorporating the\nanswers into a Visual Question Answering (VQA) dataset. By demonstrating how\nour system can collect large amounts of data at little to no cost, we envision\nsimilar systems being used to improve performance on other tasks in the future.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 03:45:17 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 17:52:03 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Lee", "Jihyeon", ""], ["Arora", "Sho", ""]]}, {"id": "1912.00142", "submitter": "Christian Marzahl", "authors": "Christian Marzahl, Marc Aubreville, Christof A. Bertram, Stefan\n  Gerlach, Jennifer Maier, J\\\"orn Voigt, Jenny Hill, Robert Klopfleisch,\n  Andreas Maier", "title": "Fooling the Crowd with Deep Learning-based Methods", "comments": "6 pages, accepted at BVM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern, state-of-the-art deep learning approaches yield human like\nperformance in numerous object detection and classification tasks. The\nfoundation for their success is the availability of training datasets of\nsubstantially high quantity, which are expensive to create, especially in the\nfield of medical imaging. Recently, crowdsourcing has been applied to create\nlarge datasets for a broad range of disciplines. This study aims to explore the\nchallenges and opportunities of crowd-algorithm collaboration for the object\ndetection task of grading cytology whole slide images. We compared the\nclassical crowdsourcing performance of twenty participants with their results\nfrom crowd-algorithm collaboration. All participants performed both modes in\nrandom order on the same twenty images. Additionally, we introduced artificial\nsystematic flaws into the precomputed annotations to estimate a bias towards\naccepting precomputed annotations. We gathered 9524 annotations on 800 images\nfrom twenty participants organised into four groups in concordance to their\nlevel of expertise with cytology. The crowd-algorithm mode improved on average\nthe participants' classification accuracy by 7%, the mean average precision by\n8% and the inter-observer Fleiss' kappa score by 20%, and reduced the time\nspent by 31%. However, two thirds of the artificially modified false labels\nwere not recognised as such by the contributors. This study shows that\ncrowd-algorithm collaboration is a promising new approach to generate large\ndatasets when it is ensured that a carefully designed setup eliminates\npotential biases.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 06:39:41 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Marzahl", "Christian", ""], ["Aubreville", "Marc", ""], ["Bertram", "Christof A.", ""], ["Gerlach", "Stefan", ""], ["Maier", "Jennifer", ""], ["Voigt", "J\u00f6rn", ""], ["Hill", "Jenny", ""], ["Klopfleisch", "Robert", ""], ["Maier", "Andreas", ""]]}, {"id": "1912.00312", "submitter": "Bilge Mutlu PhD", "authors": "Eric Deng, Bilge Mutlu, Maja Mataric", "title": "Embodiment in Socially Interactive Robots", "comments": "The official publication is available from now publishers via\n  https://www.nowpublishers.com/article/Details/ROB-056", "journal-ref": "Foundations and Trends in Robotics: Vol. 7: No. 4, pp 251-356\n  (2019)", "doi": "10.1561/2300000056", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical embodiment is a required component for robots that are structurally\ncoupled with their real-world environments. However, most socially interactive\nrobots do not need to physically interact with their environments in order to\nperform their tasks. When and why should embodied robots be used instead of\nsimpler and cheaper virtual agents? This paper reviews the existing work that\nexplores the role of physical embodiment in socially interactive robots. This\nclass consists of robots that are not only capable of engaging in social\ninteraction with humans, but are using primarily their social capabilities to\nperform their desired functions. Socially interactive robots provide\nentertainment, information, and/or assistance; this last category is typically\nencompassed by socially assistive robotics. In all cases, such robots can\nachieve their primary functions without performing functional physical work. To\ncomprehensively evaluate the existing body of work on embodiment, we first\nreview work from established related fields including psychology, philosophy,\nand sociology. We then systematically review 65 studies evaluating aspects of\nembodiment published from 2003 to 2017 in major peer-reviewed robotics\npublication venues. We examine relevant aspects of the selected studies,\nfocusing on the embodiments compared, tasks evaluated, social roles of robots,\nand measurements. We introduce three taxonomies for the types of robot\nembodiment, robot social roles, and human-robot tasks. These taxonomies are\nused to deconstruct the design and interaction spaces of socially interactive\nrobots and facilitate analysis and discussion of the reviewed studies. We use\nthis newly-defined methodology to critically discuss existing works, revealing\ntopics within embodiment research for social interaction, assistive robotics,\nand service robotics.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 03:58:44 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Deng", "Eric", ""], ["Mutlu", "Bilge", ""], ["Mataric", "Maja", ""]]}, {"id": "1912.00317", "submitter": "Daniel Votipka", "authors": "Daniel Votipka and Seth M. Rabin and Kristopher Micinski and Jeffrey\n  S. Foster and Michelle L. Mazurek", "title": "An Observational Investigation of Reverse Engineers' Processes", "comments": "22 pages, 6 figures, to appear at the 2020 USENIX Security Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reverse engineering is a complex process essential to software-security tasks\nsuch as vulnerability discovery and malware analysis. Significant research and\nengineering effort has gone into developing tools to support reverse engineers.\nHowever, little work has been done to understand the way reverse engineers\nthink when analyzing programs, leaving tool developers to make interface design\ndecisions based only on intuition.\n  This paper takes a first step toward a better understanding of reverse\nengineers' processes, with the goal of producing insights for improving\ninteraction design for reverse engineering tools. We present the results of a\nsemi-structured, observational interview study of reverse engineers (N=16).\nEach observation investigated the questions reverse engineers ask as they probe\na program, how they answer these questions, and the decisions they make\nthroughout the reverse engineering process. From the interview responses, we\ndistill a model of the reverse engineering process, divided into three phases:\noverview, sub-component scanning, and focused experimentation. Each analysis\nphase's results feed the next as reverse engineers' mental representations\nbecome more concrete. We find that reverse engineers typically use static\nmethods in the first two phases, but dynamic methods in the final phase, with\nexperience playing large, but varying, roles in each phase. % and the role of\nexperience varies between phases. Based on these results, we provide five\ninteraction design guidelines for reverse engineering tools.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 04:41:37 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Votipka", "Daniel", ""], ["Rabin", "Seth M.", ""], ["Micinski", "Kristopher", ""], ["Foster", "Jeffrey S.", ""], ["Mazurek", "Michelle L.", ""]]}, {"id": "1912.00369", "submitter": "Roger Moore", "authors": "Roger K. Moore", "title": "Talking with Robots: Opportunities and Challenges", "comments": "Submitted for presentation at the UNESCO International Conference\n  Language Technologies for All (LT4All), Paris, 4-6 December 2019\n  (https://en.unesco.org/LT4All)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Notwithstanding the tremendous progress that is taking place in spoken\nlanguage technology, effective speech-based human-robot interaction still\nraises a number of important challenges. Not only do the fields of robotics and\nspoken language technology present their own special problems, but their\ncombination raises an additional set of issues. In particular, there is a large\ngap between the formulaic speech that typifies contemporary spoken dialogue\nsystems and the flexible nature of human-human conversation. It is pointed out\nthat grounded and situated speech-based human-robot interaction may lead to\ndeeper insights into the pragmatics of language usage, thereby overcoming the\ncurrent `habitability gap'.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 09:42:50 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Moore", "Roger K.", ""]]}, {"id": "1912.00581", "submitter": "William Paul Boyce", "authors": "W. Paul Boyce, Tony Lindsay, Arkady Zgonnikov, Ignacio Rano, and\n  KongFatt Wong-Lin", "title": "Optimality and limitations of audio-visual integration for cognitive\n  systems", "comments": "20 pages, 6 figures, 1 table 16/06/2020: Updated version includes\n  expanded discussion and addition of new references. Also updated author\n  affiliation information. This version has been accepted for publication with\n  Frontiers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal integration is an important process in perceptual decision-making.\nIn humans, this process has often been shown to be statistically optimal, or\nnear optimal: sensory information is combined in a fashion that minimises the\naverage error in perceptual representation of stimuli. However, sometimes there\nare costs that come with the optimization, manifesting as illusory percepts. We\nreview audio-visual facilitations and illusions that are products of\nmultisensory integration, and the computational models that account for these\nphenomena. In particular, the same optimal computational model can lead to\nillusory percepts, and we suggest that more studies should be needed to detect\nand mitigate these illusions, as artefacts in artificial cognitive systems. We\nprovide cautionary considerations when designing artificial cognitive systems\nwith the view of avoiding such artefacts. Finally, we suggest avenues of\nresearch towards solutions to potential pitfalls in system design. We conclude\nthat detailed understanding of multisensory integration and the mechanisms\nbehind audio-visual illusions can benefit the design of artificial cognitive\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 05:08:39 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 06:27:55 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 02:10:31 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Boyce", "W. Paul", ""], ["Lindsay", "Tony", ""], ["Zgonnikov", "Arkady", ""], ["Rano", "Ignacio", ""], ["Wong-Lin", "KongFatt", ""]]}, {"id": "1912.00669", "submitter": "Wenwu Qu", "authors": "Wenwu Qu and Xiaoyu Chi and Wei Zheng", "title": "KRM-based Dialogue Management", "comments": "9 pages, 4 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A KRM-based dialogue management (DM) is proposed using to implement\nhuman-computer dialogue system in complex scenarios. KRM-based DM has a well\ndescription ability and it can ensure the logic of the dialogue process. Then a\ncomplex application scenario in the Internet of Things (IOT) industry and a\ndialogue system implemented based on the KRM-based DM will be introduced, where\nthe system allows enterprise customers to customize topics and adapts\ncorresponding topics in the interaction process with users. The experimental\nresults show that the system can complete the interactive tasks well, and can\neffectively solve the problems of topic switching, information inheritance\nbetween topics, change of dominance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 10:21:34 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Qu", "Wenwu", ""], ["Chi", "Xiaoyu", ""], ["Zheng", "Wei", ""]]}, {"id": "1912.00719", "submitter": "Jules Wulms", "authors": "Jules Wulms, Juri Buchm\\\"uller, Wouter Meulemans, Kevin Verbeek,\n  Bettina Speckmann", "title": "Stable Visual Summaries for Trajectory Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of devices that track moving objects has led to an explosive\ngrowth in trajectory data. When exploring the resulting large trajectory\ncollections, visual summaries are a useful tool to identify time intervals of\ninterest. A typical approach is to represent the spatial positions of the\ntracked objects at each time step via a one-dimensional ordering;\nvisualizations of such orderings can then be placed in temporal order along a\ntime line. There are two main criteria to assess the quality of the resulting\nvisual summary: spatial quality -- how well does the ordering capture the\nstructure of the data at each time step, and stability -- how coherent are the\norderings over consecutive time steps or temporal ranges? In this paper we\nintroduce a new Stable Principal Component (SPC) method to compute such\norderings, which is explicitly parameterized for stability, allowing a\ntrade-off between the spatial quality and stability. We conduct extensive\ncomputational experiments that quantitatively compare the orderings produced by\nours and other stable dimensionality-reduction methods to various\nstate-of-the-art approaches using a set of well-established quality metrics\nthat capture spatial quality and stability. We conclude that stable\ndimensionality reduction outperforms existing methods on stability, without\nsacrificing spatial quality or efficiency; in particular, our new SPC method\ndoes so at a fraction of the computational costs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 12:39:26 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 21:08:07 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 13:22:18 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wulms", "Jules", ""], ["Buchm\u00fcller", "Juri", ""], ["Meulemans", "Wouter", ""], ["Verbeek", "Kevin", ""], ["Speckmann", "Bettina", ""]]}, {"id": "1912.00903", "submitter": "Nora Hollenstein", "authors": "Nora Hollenstein, Marius Troendle, Ce Zhang, Nicolas Langer", "title": "ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading\n  and Annotation", "comments": "Proceedings of the Language Resources and Evaluation Conference (LREC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recorded and preprocessed ZuCo 2.0, a new dataset of simultaneous\neye-tracking and electroencephalography during natural reading and during\nannotation. This corpus contains gaze and brain activity data of 739 sentences,\n349 in a normal reading paradigm and 390 in a task-specific paradigm, in which\nthe 18 participants actively search for a semantic relation type in the given\nsentences as a linguistic annotation task. This new dataset complements ZuCo\n1.0 by providing experiments designed to analyze the differences in cognitive\nprocessing between natural reading and annotation. The data is freely available\nhere: https://osf.io/2urht/.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 16:30:46 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 14:42:37 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 20:14:16 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Hollenstein", "Nora", ""], ["Troendle", "Marius", ""], ["Zhang", "Ce", ""], ["Langer", "Nicolas", ""]]}, {"id": "1912.01068", "submitter": "Bohao Wu", "authors": "Bohao Wu and Xue Bai", "title": "About how to make novel Visible by using Newly Translated Tale of Genji\n  as an example", "comments": "23 pages, in Chinese, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to make Tales of Genji visible by using natural language\nprocessing, mathematic analysis, emiton analysis. Based on novel, mining data\nfrom content of this novel at respect of information abstracting. Summing up\nthe fundamental method of novel visualization, our work are as follows: Based\non frequency analysis, we use tf-did to abstract keyword of Newly Translated\nTale of Genji, which means the most important word in each chapter. We\nrecognize the emotion of word to analysis the emotion of each chapter of Newly\nTranslated Tale of Genji. Next, we think about the connection between the\nresult of emotion analysis and literature analysis, showing we can get same\nresult by natural language processing. We build a network of all the word\napperanced in Newly Translated Tale of Genji. Make a study of relationships\nbetween words. Further, we search the writer of Uji Chapters.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 08:35:46 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Wu", "Bohao", ""], ["Bai", "Xue", ""]]}, {"id": "1912.01122", "submitter": "Zhou Yang", "authors": "Zhou Yang, Spencer Bradshaw, Rattikorn Hewett, and Fang Jin", "title": "Discovering Opioid Use Patterns from Social Media for Relapse Prevention", "comments": "7 pages, and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The United States is currently experiencing an unprecedented opioid crisis,\nand opioid overdose has become a leading cause of injury and death. Effective\nopioid addiction recovery calls for not only medical treatments, but also\nbehavioral interventions for impacted individuals. In this paper, we study\ncommunication and behavior patterns of patients with opioid use disorder (OUD)\nfrom social media, intending to demonstrate how existing information from\ncommon activities, such as online social networking, might lead to better\nprediction, evaluation, and ultimately prevention of relapses. Through a\nmulti-disciplinary and advanced novel analytic perspective, we characterize\nopioid addiction behavior patterns by analyzing opioid groups from Reddit.com -\nincluding modeling online discussion topics, analyzing text co-occurrence and\ncorrelations, and identifying emotional states of people with OUD. These\nquantitative analyses are of practical importance and demonstrate innovative\nways to use information from online social media, to create technology that can\nassist in relapse prevention.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 23:15:02 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Yang", "Zhou", ""], ["Bradshaw", "Spencer", ""], ["Hewett", "Rattikorn", ""], ["Jin", "Fang", ""]]}, {"id": "1912.01130", "submitter": "Zhou Yang", "authors": "Zhou Yang, Vinay Jayachandra Reddy, Rashmi Kesidi, Fang Jin", "title": "Addict Free -- A Smart and Connected Relapse Intervention Mobile App", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is widely acknowledged that addiction relapse is highly associated with\nspatial-temporal factors such as some specific places or time periods. Current\nstudies suggest that those factors can be utilized for better relapse\ninterventions, however, there is no relapse prevention application that makes\nuse of those factors. In this paper, we introduce a mobile app called \"Addict\nFree\", which records user profiles, tracks relapse history and summarizes\nrecovering statistics to help users better understand their recovering\nsituations. Also, this app builds a relapse recovering community, which allows\nusers to ask for advice and encouragement, and share relapse prevention\nexperience. Moreover, machine learning algorithms that ingest spatial and\ntemporal factors are utilized to predict relapse, based on which helpful\naddiction diversion activities are recommended by a recovering recommendation\nalgorithm. By interacting with users, this app targets at providing smart\nsuggestions that aim to stop relapse, especially for alcohol and tobacco\naddiction users.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 23:38:22 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Yang", "Zhou", ""], ["Reddy", "Vinay Jayachandra", ""], ["Kesidi", "Rashmi", ""], ["Jin", "Fang", ""]]}, {"id": "1912.01166", "submitter": "Dongrui Wu", "authors": "He He and Dongrui Wu", "title": "Different Set Domain Adaptation for Brain-Computer Interfaces: A Label\n  Alignment Approach", "comments": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, 2020", "journal-ref": "IEEE Trans. on Neural Systems and Rehabilitation Engineering,\n  28(5), pp. 1091-1108, 2020", "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain-computer interface (BCI) system usually needs a long calibration\nsession for each new subject/task to adjust its parameters, which impedes its\ntransition from the laboratory to real-world applications. Domain adaptation,\nwhich leverages labeled data from auxiliary subjects/tasks (source domains),\nhas demonstrated its effectiveness in reducing such calibration effort.\nCurrently, most domain adaptation approaches require the source domains to have\nthe same feature space and label space as the target domain, which limits their\napplications, as the auxiliary data may have different feature spaces and/or\ndifferent label spaces. This paper considers different set domain adaptation\nfor BCIs, i.e., the source and target domains have different label spaces. We\nintroduce a practical setting of different label sets for BCIs, and propose a\nnovel label alignment (LA) approach to align the source label space with the\ntarget label space. It has three desirable properties: 1) LA only needs as few\nas one labeled sample from each class of the target subject; 2) LA can be used\nas a preprocessing step before different feature extraction and classification\nalgorithms; and, 3) LA can be integrated with other domain adaptation\napproaches to achieve even better performance. Experiments on two motor imagery\ndatasets demonstrated the effectiveness of LA.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 02:46:56 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 18:05:56 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 21:11:36 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2020 02:51:55 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["He", "He", ""], ["Wu", "Dongrui", ""]]}, {"id": "1912.01171", "submitter": "Dongrui Wu", "authors": "Zihan Liu and Lubin Meng and Xiao Zhang and Weili Fang and Dongrui Wu", "title": "Universal Adversarial Perturbations for CNN Classifiers in EEG-Based\n  BCIs", "comments": null, "journal-ref": "Journal of Neural Engineering, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple convolutional neural network (CNN) classifiers have been proposed\nfor electroencephalogram (EEG) based brain-computer interfaces (BCIs). However,\nCNN models have been found vulnerable to universal adversarial perturbations\n(UAPs), which are small and example-independent, yet powerful enough to degrade\nthe performance of a CNN model, when added to a benign example. This paper\nproposes a novel total loss minimization (TLM) approach to generate UAPs for\nEEG-based BCIs. Experimental results demonstrated the effectiveness of TLM on\nthree popular CNN classifiers for both target and non-target attacks. We also\nverified the transferability of UAPs in EEG-based BCI systems. To our\nknowledge, this is the first study on UAPs of CNN classifiers in EEG-based\nBCIs. UAPs are easy to construct, and can attack BCIs in real-time, exposing a\npotentially critical security concern of BCIs.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 03:00:08 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 22:38:54 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2021 20:34:58 GMT"}, {"version": "v4", "created": "Thu, 18 Feb 2021 16:03:24 GMT"}, {"version": "v5", "created": "Thu, 24 Jun 2021 02:03:24 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Liu", "Zihan", ""], ["Meng", "Lubin", ""], ["Zhang", "Xiao", ""], ["Fang", "Weili", ""], ["Wu", "Dongrui", ""]]}, {"id": "1912.01177", "submitter": "Honggu Lee", "authors": "Jaehyun Nam, Hyesun Chung, Young ah Seong, Honggu Lee", "title": "A New Terrain in HCI: Emotion Recognition Interface using Biometric Data\n  for an Immersive VR Experience", "comments": "9 pages, 7 figures, chi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Emotion recognition technology is crucial in providing a personalized user\nexperience. It is especially important in virtual reality(VR) to assess the\nuser's emotions to enhance their sense of immersion. We propose an emotion\nrecognition interface that incorporates the user's biometric data with machine\nlearning technology for increasing user engagement in VR. Our key technologies\ninclude brainwave sensors and eye-tracking cameras embedded in a VR headset,\nwhich seamlessly acquire physiological signals, and secondly, an attractiveness\nrecognition algorithm that uses bio-signals to predict the user's attraction on\nvisual stimuli. We conducted experiments to test the performance of the system,\nand also interviewed experts and participants to acquire opinions on the\nsystem. This study demonstrated the technical feasibility of our system with\nhigh accuracy and usability. Interviewees expected that the interface will be\nactively used in the context of various applications. Our proposed interface\ncould contribute to an immersive VR experience design.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 03:23:25 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Nam", "Jaehyun", ""], ["Chung", "Hyesun", ""], ["Seong", "Young ah", ""], ["Lee", "Honggu", ""]]}, {"id": "1912.01218", "submitter": "Daan Van Esch", "authors": "Daan van Esch, Elnaz Sarbar, Tamar Lucassen, Jeremy O'Brien, Theresa\n  Breiner, Manasa Prasad, Evan Crew, Chieu Nguyen, Fran\\c{c}oise Beaufays", "title": "Writing Across the World's Languages: Deep Internationalization for\n  Gboard, the Google Keyboard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This technical report describes our deep internationalization program for\nGboard, the Google Keyboard. Today, Gboard supports 900+ language varieties\nacross 70+ writing systems, and this report describes how and why we have been\nadding support for hundreds of language varieties from around the globe. Many\nlanguages of the world are increasingly used in writing on an everyday basis,\nand we describe the trends we see. We cover technological and logistical\nchallenges in scaling up a language technology product like Gboard to hundreds\nof language varieties, and describe how we built systems and processes to\noperate at scale. Finally, we summarize the key take-aways from user studies we\nran with speakers of hundreds of languages from around the world.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 06:56:15 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["van Esch", "Daan", ""], ["Sarbar", "Elnaz", ""], ["Lucassen", "Tamar", ""], ["O'Brien", "Jeremy", ""], ["Breiner", "Theresa", ""], ["Prasad", "Manasa", ""], ["Crew", "Evan", ""], ["Nguyen", "Chieu", ""], ["Beaufays", "Fran\u00e7oise", ""]]}, {"id": "1912.01368", "submitter": "Akrivi Katifori", "authors": "Ektor Vrettakis, Vassilis Kourtis, Akrivi Katifori, Manos Karvounis,\n  Christos Lougiakis, Yannis Ioannidis", "title": "Narralive -- Creating and experiencing mobile digital storytelling in\n  cultural heritage", "comments": null, "journal-ref": "Digital Applications in Archaeology and Cultural Heritage, Volume\n  15, 2019, e00114,ISSN 2212-0548", "doi": "10.1016/j.daach.2019.e00114", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Storytelling has the potential to revolutionize the way we engage with\ncultural heritage and has been widely recognized as an important direction for\nattracting and satisfying the audience of museums and other cultural heritage\nsites. This approach has been investigated in various research projects, but\nits adoption outside research remains limited due to the challenges inherent in\nits creation. In this work, we present the web-based Narralive Storyboard\nEditor and the Narralive Mobile Player app, developed with the objective to\nassist the creative process and promote research on different aspects of the\napplication of mobile digital storytelling in cultural heritage settings. The\ntools have been applied and evaluated in a variety of contexts and sites, and\nthe main findings of this process are presented and discussed, concluding in\ngeneral findings about the authoring of digital storytelling experiences in\ncultural heritage.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 13:56:55 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Vrettakis", "Ektor", ""], ["Kourtis", "Vassilis", ""], ["Katifori", "Akrivi", ""], ["Karvounis", "Manos", ""], ["Lougiakis", "Christos", ""], ["Ioannidis", "Yannis", ""]]}, {"id": "1912.01947", "submitter": "Matti Pouke", "authors": "Matti Pouke, Katherine J. Mimnaugh, Timo Ojala, Steven M. LaValle", "title": "The Plausibility Paradox for Scaled-Down Users in Virtual Environments", "comments": "Accepted to the 27th IEEE Conference on Virtual Reality and 3D User\n  Interfaces (IEEEVR 2020). The title of the paper was changed among other\n  edits necessary for the accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper identifies a new phenomenon: when users interact with simulated\nobjects in a virtual environment where the user is much smaller than usual,\nthere is a mismatch between the object physics that they expect and the object\nphysics that would be correct at that scale. We report the findings of our\nstudy investigating the relationship between perceived realism and a physically\naccurate approximation of reality in a virtual reality experience in which the\nuser has been scaled down by a factor of ten. We conducted a within-subjects\nexperiment in which 44 subjects performed a simple interaction task with\nobjects under two different physics simulation conditions. In one condition,\nthe objects, when dropped and thrown, behaved accurately according to the\nphysics that would be correct at that reduced scale in the real world, our true\nphysics condition. In the other condition, the movie physics condition, the\nobjects behaved in a similar manner as they would if no scaling of the user had\noccurred. We found that a significant majority of the users considered the\nlatter condition to be the more realistic one. We argue that our findings have\nimplications for many virtual reality and telepresence applications involving\noperation with simulated or physical objects in small scales.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 12:18:53 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 08:48:17 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Pouke", "Matti", ""], ["Mimnaugh", "Katherine J.", ""], ["Ojala", "Timo", ""], ["LaValle", "Steven M.", ""]]}, {"id": "1912.01987", "submitter": "Edwin D. Simpson", "authors": "Edwin Simpson, Iryna Gurevych", "title": "Scalable Bayesian Preference Learning for Crowds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable Bayesian preference learning method for jointly\npredicting the preferences of individuals as well as the consensus of a crowd\nfrom pairwise labels. Peoples' opinions often differ greatly, making it\ndifficult to predict their preferences from small amounts of personal data.\nIndividual biases also make it harder to infer the consensus of a crowd when\nthere are few labels per item. We address these challenges by combining matrix\nfactorisation with Gaussian processes, using a Bayesian approach to account for\nuncertainty arising from noisy and sparse data. Our method exploits input\nfeatures, such as text embeddings and user metadata, to predict preferences for\nnew items and users that are not in the training set. As previous solutions\nbased on Gaussian processes do not scale to large numbers of users, items or\npairwise labels, we propose a stochastic variational inference approach that\nlimits computational and memory costs. Our experiments on a recommendation task\nshow that our method is competitive with previous approaches despite our\nscalable inference approximation. We demonstrate the method's scalability on a\nnatural language processing task with thousands of users and items, and show\nimprovements over the state of the art on this task. We make our software\npublicly available for future work.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 13:56:38 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 20:01:44 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Simpson", "Edwin", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1912.02083", "submitter": "Dillon Lohr", "authors": "Dillon J. Lohr, Lee Friedman, Oleg V. Komogortsev", "title": "Evaluating the Data Quality of Eye Tracking Signals from a Virtual\n  Reality System: Case Study using SMI's Eye-Tracking HTC Vive", "comments": "Data publicly available at https://doi.org/10.18738/T8/N8EIVG, 35\n  pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We evaluated the data quality of SMI's tethered eye-tracking head-mounted\ndisplay based on the HTC Vive (ET-HMD) during a random saccade task. We\nmeasured spatial accuracy, spatial precision, temporal precision, linearity,\nand crosstalk. We proposed the use of a non-parametric spatial precision\nmeasure based on the median absolute deviation (MAD). Our linearity analysis\nconsidered both the slope and adjusted R-squared of a best-fitting line. We\nwere the first to test for a quadratic component to crosstalk. We prepended a\ncalibration task to the random saccade task and evaluated 2 methods to employ\nthis user-supplied calibration. For this, we used a unique binning approach to\nchoose samples to be included in the recalibration analyses. We compared our\nquality measures between the ET-HMD and our EyeLink 1000 (SR-Research, Ottawa,\nOntario, CA). We found that the ET-HMD had significantly better spatial\naccuracy and linearity fit than our EyeLink, but both devices had similar\nspatial precision and linearity slope. We also found that, while the EyeLink\nhad no significant crosstalk, the ET-HMD generally exhibited quadratic\ncrosstalk. Fourier analysis revealed that the binocular signal was a low-pass\nfiltered version of the monocular signal. Such filtering resulted in the\nbinocular signal being useless for the study of high-frequency components such\nas saccade dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 16:22:56 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Lohr", "Dillon J.", ""], ["Friedman", "Lee", ""], ["Komogortsev", "Oleg V.", ""]]}, {"id": "1912.02182", "submitter": "Maurizio Tesconi", "authors": "Marco Avvenuti, Salvatore Bellomo, Stefano Cresci, Leonardo Nizzoli,\n  Maurizio Tesconi", "title": "Towards better social crisis data with HERMES: Hybrid sensing for\n  EmeRgency ManagEment System", "comments": null, "journal-ref": "Pervasive and Mobile Computing 67, 2020", "doi": "10.1016/j.pmcj.2020.101225", "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People involved in mass emergencies increasingly publish information-rich\ncontents in online social networks (OSNs), thus acting as a distributed and\nresilient network of human sensors. In this work, we present HERMES, a system\ndesigned to enrich the information spontaneously disclosed by OSN users in the\naftermath of disasters. HERMES leverages a mixed data collection strategy,\ncalled hybrid crowdsensing, and state-of-the-art AI techniques. Evaluated in\nreal-world emergencies, HERMES proved to increase: (i) the amount of the\navailable damage information; (ii) the density (up to 7x) and the variety (up\nto 18x) of the retrieved geographic information; (iii) the geographic coverage\n(up to 30%) and granularity.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 14:25:52 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Avvenuti", "Marco", ""], ["Bellomo", "Salvatore", ""], ["Cresci", "Stefano", ""], ["Nizzoli", "Leonardo", ""], ["Tesconi", "Maurizio", ""]]}, {"id": "1912.02241", "submitter": "Jing Bi", "authors": "Jing Bi, Vikas Dhiman, Tianyou Xiao, Chenliang Xu", "title": "Learning from Interventions using Hierarchical Policies for Safe\n  Learning", "comments": "Accepted for publication at the Thirty-Fourth AAAI Conference on\n  Artificial Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from Demonstrations (LfD) via Behavior Cloning (BC) works well on\nmultiple complex tasks. However, a limitation of the typical LfD approach is\nthat it requires expert demonstrations for all scenarios, including those in\nwhich the algorithm is already well-trained. The recently proposed Learning\nfrom Interventions (LfI) overcomes this limitation by using an expert overseer.\nThe expert overseer only intervenes when it suspects that an unsafe action is\nabout to be taken. Although LfI significantly improves over LfD, the\nstate-of-the-art LfI fails to account for delay caused by the expert's reaction\ntime and only learns short-term behavior. We address these limitations by 1)\ninterpolating the expert's interventions back in time, and 2) by splitting the\npolicy into two hierarchical levels, one that generates sub-goals for the\nfuture and another that generates actions to reach those desired sub-goals.\nThis sub-goal prediction forces the algorithm to learn long-term behavior while\nalso being robust to the expert's reaction time. Our experiments show that LfI\nusing sub-goals in a hierarchical policy framework trains faster and achieves\nbetter asymptotic performance than typical LfD.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 20:28:51 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Bi", "Jing", ""], ["Dhiman", "Vikas", ""], ["Xiao", "Tianyou", ""], ["Xu", "Chenliang", ""]]}, {"id": "1912.02596", "submitter": "Aida Todri-Sanial", "authors": "Abhishek Singh Dahiya (IES), Jerome Thireau (PhyMedExp), Jamila\n  Boudaden (Fraunhofer IZM), Swatchith Lal, Umair Gulzar (UCC), Yan Zhang\n  (UCC), Thierry Gil (LIRMM), Nadine Azemard (SmartIES), Peter Ramm (Fraunhofer\n  IZM), Tim Kiessling (Fraunhofer IZM), Cian O'Murchu, Fredrik Sebelius, Jonas\n  Tilly, Colm Glynn (ADI), Shane Geary (ADI), Colm O'Dwyer (UCC), Kafil Razeeb,\n  Alain Lacampagne (PhyMedExp), Benoit Charlot (IES), Aida Todri-Sanial\n  (SmartIES, LIRMM)", "title": "Energy Autonomous Wearable Sensors for Smart Healthcare: A Review", "comments": null, "journal-ref": "Journal of The Electrochemical Society, Electrochemical Society,\n  2019, JES Focus Issue on Sensor Reviews, 167 (3)", "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy Autonomous Wearable Sensors (EAWS) have attracted a large interest due\nto their potential to provide reliable measurements and continuous bioelectric\nsignals, which help to reduce health risk factors early on, ongoing assessment\nfor disease prevention, and maintaining optimum, lifelong health quality. This\nreview paper presents recent developments and state-of-the-art research related\nto three critical elements that enable an EAWS. The first element is wearable\nsensors, which monitor human body physiological signals and activities.\nEmphasis is given on explaining different types of transduction mechanisms\npresented, and emerging materials and fabrication techniques. The second\nelement is the flexible and wearable energy storage device to drive low-power\nelectronics and the software needed for automatic detection of unstable\nphysiological parameters. The third is the flexible and stretchable energy\nharvesting module to recharge batteries for continuous operation of wearable\nsensors. We conclude by discussing some of the technical challenges in\nrealizing energy-autonomous wearable sensing technologies and possible\nsolutions for overcoming them.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 14:35:11 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Dahiya", "Abhishek Singh", "", "IES"], ["Thireau", "Jerome", "", "PhyMedExp"], ["Boudaden", "Jamila", "", "Fraunhofer IZM"], ["Lal", "Swatchith", "", "UCC"], ["Gulzar", "Umair", "", "UCC"], ["Zhang", "Yan", "", "UCC"], ["Gil", "Thierry", "", "LIRMM"], ["Azemard", "Nadine", "", "SmartIES"], ["Ramm", "Peter", "", "Fraunhofer\n  IZM"], ["Kiessling", "Tim", "", "Fraunhofer IZM"], ["O'Murchu", "Cian", "", "ADI"], ["Sebelius", "Fredrik", "", "ADI"], ["Tilly", "Jonas", "", "ADI"], ["Glynn", "Colm", "", "ADI"], ["Geary", "Shane", "", "ADI"], ["O'Dwyer", "Colm", "", "UCC"], ["Razeeb", "Kafil", "", "PhyMedExp"], ["Lacampagne", "Alain", "", "PhyMedExp"], ["Charlot", "Benoit", "", "IES"], ["Todri-Sanial", "Aida", "", "SmartIES, LIRMM"]]}, {"id": "1912.02736", "submitter": "Amith Kamath  Belman", "authors": "Amith K. Belman, Li Wang, S. S. Iyengar, Pawel Sniatala, Robert\n  Wright, Robert Dora, Jacob Baldwin, Zhanpeng Jin, and Vir V. Phoha", "title": "Insights from BB-MAS -- A Large Dataset for Typing, Gait and Swipes of\n  the Same Person on Desktop, Tablet and Phone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavioral biometrics are key components in the landscape of research in\ncontinuous and active user authentication. However, there is a lack of large\ndatasets with multiple activities, such as typing, gait and swipe performed by\nthe same person. Furthermore, large datasets with multiple activities performed\non multiple devices by the same person are non-existent. The difficulties of\nprocuring devices, participants, designing protocol, secure storage and\non-field hindrances may have contributed to this scarcity. The availability of\nsuch a dataset is crucial to forward the research in behavioral biometrics as\nusage of multiple devices by a person is common nowadays. Through this paper,\nwe share our dataset, the details of its collection, features for each modality\nand our findings of how keystroke features vary across devices. We have\ncollected data from 117 subjects for typing (both fixed and free text), gait\n(walking, upstairs and downstairs) and touch on Desktop, Tablet and Phone. The\ndataset consists a total of about: 3.5 million keystroke events; 57.1 million\ndata-points for accelerometer and gyroscope each; 1.7 million data-points for\nswipes; and enables future research to explore previously unexplored directions\nin inter-device and inter-modality biometrics. Our analysis on keystrokes\nreveals that in most cases, keyhold times are smaller but inter-key latencies\nare larger, on hand-held devices when compared to desktop. We also present;\ndetailed comparison with related datasets; possible research directions with\nthe dataset; and lessons learnt from the data collection.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 15:24:06 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 18:40:28 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Belman", "Amith K.", ""], ["Wang", "Li", ""], ["Iyengar", "S. S.", ""], ["Sniatala", "Pawel", ""], ["Wright", "Robert", ""], ["Dora", "Robert", ""], ["Baldwin", "Jacob", ""], ["Jin", "Zhanpeng", ""], ["Phoha", "Vir V.", ""]]}, {"id": "1912.02744", "submitter": "Elia Onofri", "authors": "Pietro Centorrino, Alessandro Corbetta, Emiliano Cristiani, Elia\n  Onofri", "title": "Measurement and analysis of visitors' trajectories in crowded museums", "comments": "6 pages, 11 figures, International Conference on Metrology for\n  Archaeology and Cultural Heritage", "journal-ref": "2019 IMEKO TC-4 International Conference on Metrology for\n  Archaeology and Cultural Heritage, 2019", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the issue of measuring and analyzing the visitors' dynamics in\ncrowded museums. We propose an IoT-based system -- supported by artificial\nintelligence models -- to reconstruct the visitors' trajectories throughout the\nmuseum spaces. Thanks to this tool, we are able to gather wide ensembles of\nvisitors' trajectories, allowing useful insights for the facility management\nand the preservation of the art pieces. Our contribution comes with one\nsuccessful use case: the Galleria Borghese in Rome, Italy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 10:51:31 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 10:03:46 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Centorrino", "Pietro", ""], ["Corbetta", "Alessandro", ""], ["Cristiani", "Emiliano", ""], ["Onofri", "Elia", ""]]}, {"id": "1912.02913", "submitter": "Arian Mehrfard", "authors": "Arian Mehrfard, Javad Fotouhi, Giacomo Taylor, Tess Forster, Nassir\n  Navab, and Bernhard Fuerst", "title": "A Comparative Analysis of Virtual Reality Head-Mounted Display Systems", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advances of Virtual Reality (VR) technology, the deployment of\nsuch will dramatically increase in non-entertainment environments, such as\nprofessional education and training, manufacturing, service, or low\nfrequency/high risk scenarios. Clinical education is an area that especially\nstands to benefit from VR technology due to the complexity, high cost, and\ndifficult logistics. The effectiveness of the deployment of VR systems, is\nsubject to factors that may not be necessarily considered for devices targeting\nthe entertainment market. In this work, we systematically compare a wide range\nof VR Head-Mounted Displays (HMDs) technologies and designs by defining a new\nset of metrics that are 1) relevant to most generic VR solutions and 2) are of\nparamount importance for VR-based education and training. We evaluated ten HMDs\nbased on various criteria, including neck strain, heat development, and color\naccuracy. Other metrics such as text readability, comfort, and contrast\nperception were evaluated in a multi-user study on three selected HMDs, namely\nOculus Rift S, HTC Vive Pro and Samsung Odyssey+. Results indicate that the HTC\nVive Pro performs best with regards to comfort, display quality and\ncompatibility with glasses.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 23:01:33 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mehrfard", "Arian", ""], ["Fotouhi", "Javad", ""], ["Taylor", "Giacomo", ""], ["Forster", "Tess", ""], ["Navab", "Nassir", ""], ["Fuerst", "Bernhard", ""]]}, {"id": "1912.02943", "submitter": "P. M. Krafft", "authors": "Michael Katell, Meg Young, Bernease Herman, Dharma Dailey, Aaron Tam,\n  Vivian Guetler, Corinne Binz, Daniella Raz, P. M. Krafft", "title": "An Algorithmic Equity Toolkit for Technology Audits by Community\n  Advocates and Activists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wave of recent scholarship documenting the discriminatory harms of\nalgorithmic systems has spurred widespread interest in algorithmic\naccountability and regulation. Yet effective accountability and regulation is\nstymied by a persistent lack of resources supporting public understanding of\nalgorithms and artificial intelligence. Through interactions with a US-based\ncivil rights organization and their coalition of community organizations, we\nidentify a need for (i) heuristics that aid stakeholders in distinguishing\nbetween types of analytic and information systems in lay language, and (ii)\nrisk assessment tools for such systems that begin by making algorithms more\nlegible. The present work delivers a toolkit to achieve these aims. This paper\nboth presents the Algorithmic Equity Toolkit (AEKit) Equity as an artifact, and\ndetails how our participatory process shaped its design. Our work fits within\nhuman-computer interaction scholarship as a demonstration of the value of HCI\nmethods and approaches to problems in the area of algorithmic transparency and\naccountability.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 01:32:16 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Katell", "Michael", ""], ["Young", "Meg", ""], ["Herman", "Bernease", ""], ["Dailey", "Dharma", ""], ["Tam", "Aaron", ""], ["Guetler", "Vivian", ""], ["Binz", "Corinne", ""], ["Raz", "Daniella", ""], ["Krafft", "P. M.", ""]]}, {"id": "1912.03072", "submitter": "Youngnam Lee", "authors": "Youngduck Choi, Youngnam Lee, Dongmin Shin, Junghyun Cho, Seoyon Park,\n  Seewoo Lee, Jineon Baek, Chan Bae, Byungsoo Kim, Jaewe Heo", "title": "EdNet: A Large-Scale Hierarchical Dataset in Education", "comments": "AIED 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advances in Artificial Intelligence in Education (AIEd) and the\never-growing scale of Interactive Educational Systems (IESs), data-driven\napproach has become a common recipe for various tasks such as knowledge tracing\nand learning path recommendation. Unfortunately, collecting real students'\ninteraction data is often challenging, which results in the lack of public\nlarge-scale benchmark dataset reflecting a wide variety of student behaviors in\nmodern IESs. Although several datasets, such as ASSISTments, Junyi Academy,\nSynthetic and STATICS, are publicly available and widely used, they are not\nlarge enough to leverage the full potential of state-of-the-art data-driven\nmodels and limits the recorded behaviors to question-solving activities. To\nthis end, we introduce EdNet, a large-scale hierarchical dataset of diverse\nstudent activities collected by Santa, a multi-platform self-study solution\nequipped with artificial intelligence tutoring system. EdNet contains\n131,441,538 interactions from 784,309 students collected over more than 2\nyears, which is the largest among the ITS datasets released to the public so\nfar. Unlike existing datasets, EdNet provides a wide variety of student actions\nranging from question-solving to lecture consumption and item purchasing. Also,\nEdNet has a hierarchical structure where the student actions are divided into 4\ndifferent levels of abstractions. The features of EdNet are domain-agnostic,\nallowing EdNet to be extended to different domains easily. The dataset is\npublicly released under Creative Commons Attribution-NonCommercial 4.0\nInternational license for research purposes. We plan to host challenges in\nmultiple AIEd tasks with EdNet to provide a common ground for the fair\ncomparison between different state of the art models and encourage the\ndevelopment of practical and effective methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 11:46:18 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 09:03:18 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 10:09:08 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Choi", "Youngduck", ""], ["Lee", "Youngnam", ""], ["Shin", "Dongmin", ""], ["Cho", "Junghyun", ""], ["Park", "Seoyon", ""], ["Lee", "Seewoo", ""], ["Baek", "Jineon", ""], ["Bae", "Chan", ""], ["Kim", "Byungsoo", ""], ["Heo", "Jaewe", ""]]}, {"id": "1912.03125", "submitter": "Katerin Romeo-Pakker", "authors": "Katerine Romeo (LITIS), Edwige Pissaloux (ISIR), Fr\\'ed\\'eric Serin", "title": "Accessibility of websites for visually impaired persons", "comments": "in French. arXiv admin note: text overlap with arXiv:1911.06727", "journal-ref": "CNRIUT 2018, Institut Universitaire de Technologie, Aix Marseille\n  Universit{\\'e}, Jun 2018, Aix-en-Provence, France. pp.163-165", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accessibility of websites for visually impaired persons is mishandled by\nscreen readers which are not always adapted to interactivity needed by actual\nweb/multimedia technologies. This paper analyses the difficulties to access to\ninformation with the use of different technologies and the existing\nrecommendations for the conception of accessible websites for all. Preliminary\nresults with visually impaired persons on our ACCESSPACE project website are\npresented.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 14:39:23 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Romeo", "Katerine", "", "LITIS"], ["Pissaloux", "Edwige", "", "ISIR"], ["Serin", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1912.03863", "submitter": "Zhenyi He", "authors": "Zhenyi He, Ken Perlin", "title": "Exploring the Effectiveness of Face-to-face Mixed Reality for Teaching\n  with Chalktalk", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching that uses projected presentation media such as slide-shows lacks\nsupport for dynamic content whose form and behaviors require live changes\nduring a lecture. Recent software alternatives such as the Chalktalk software\nplatform allow the creation of interactive simulations in arbitrary sequences\nand combinations within presentations. These more dynamic solutions, however,\ndo not optimize for face-to-face interactions: eye-contact, gaze direction, and\nconcurrent awareness of another person's movements together with the presented\ncontent. To explore the extent to which these face-to-face interactions may\nimprove learning and engagement during a lecture, we propose a Mixed Reality\n(MR) platform that places Chalktalk's behaviors and simulations within a\nmirrored virtual world environment designed for face-to-face, one-on-one\ninteractions. We compare our system with projected Chalktalk to evaluate its\nrelative effectiveness for learning, retention, and level of engagement.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 05:54:59 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 20:35:34 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2019 20:35:00 GMT"}, {"version": "v4", "created": "Fri, 27 Dec 2019 00:30:39 GMT"}, {"version": "v5", "created": "Wed, 19 Aug 2020 06:13:41 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["He", "Zhenyi", ""], ["Perlin", "Ken", ""]]}, {"id": "1912.04235", "submitter": "David Schneider", "authors": "Tino Fuhrman, David Schneider, Felix Altenberg, Tung Nguyen, Simon\n  Blasen, Stefan Constantin, Alex Waibel", "title": "An Interactive Indoor Drone Assistant", "comments": "Presented at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid advance of sophisticated control algorithms, the capabilities\nof drones to stabilise, fly and manoeuvre autonomously have dramatically\nimproved, enabling us to pay greater attention to entire missions and the\ninteraction of a drone with humans and with its environment during the course\nof such a mission. In this paper, we present an indoor office drone assistant\nthat is tasked to run errands and carry out simple tasks at our laboratory,\nwhile given instructions from and interacting with humans in the space. To\naccomplish its mission, the system has to be able to understand verbal\ninstructions from humans, and perform subject to constraints from control and\nhardware limitations, uncertain localisation information, unpredictable and\nuncertain obstacles and environmental factors. We combine and evaluate the\ndialogue, navigation, flight control, depth perception and collision avoidance\ncomponents. We discuss performance and limitations of our assistant at the\ncomponent as well as the mission level. A 78% mission success rate was obtained\nover the course of 27 missions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:22:30 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Fuhrman", "Tino", ""], ["Schneider", "David", ""], ["Altenberg", "Felix", ""], ["Nguyen", "Tung", ""], ["Blasen", "Simon", ""], ["Constantin", "Stefan", ""], ["Waibel", "Alex", ""]]}, {"id": "1912.04356", "submitter": "Mengchen Wang", "authors": "Mengchen Wang, Nicolas Ferey, Patrick Bourdot, Frederic Magoules", "title": "Interactive 3D fluid simulation: steering the simulation in progress\n  using Lattice Boltzmann Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a work in progress about software and hardware\narchitecture to steer and control an ongoing fluid simulation in a context of a\nserious game application. We propose to use the Lattice Boltzmann Method as the\nsimulation approach considering that it can provide fully parallel algorithms\nto reach interactive time and because it is easier to change parameters while\nthe simulation is in progress remaining physically relevant than more classical\nsimulation approaches. We describe which parameters we can modify and how we\nsolve technical issues of interactive steering and we finally show an\napplication of our interactive fluid simulation approach of water dam\nphenomena.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 20:16:48 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Wang", "Mengchen", ""], ["Ferey", "Nicolas", ""], ["Bourdot", "Patrick", ""], ["Magoules", "Frederic", ""]]}, {"id": "1912.04444", "submitter": "Alexey Drutsa", "authors": "Alexey Drutsa, Viktoriya Farafonova, Valentina Fedorova, Olga\n  Megorskaya, Evfrosiniya Zerminova, Olga Zhilinskaya", "title": "Practice of Efficient Data Collection via Crowdsourcing at Large-Scale", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning algorithms need large datasets to be trained.\nCrowdsourcing has become a popular approach to label large datasets in a\nshorter time as well as at a lower cost comparing to that needed for a limited\nnumber of experts. However, as crowdsourcing performers are non-professional\nand vary in levels of expertise, such labels are much noisier than those\nobtained from experts. For this reason, in order to collect good quality data\nwithin a limited budget special techniques such as incremental relabelling,\naggregation and pricing need to be used. We make an introduction to data\nlabeling via public crowdsourcing marketplaces and present key components of\nefficient label collection. We show how to choose one of real label collection\ntasks, experiment with selecting settings for the labelling process, and launch\nlabel collection project at Yandex.Toloka, one of the largest crowdsourcing\nmarketplace. The projects will be run on real crowds. We also present main\nalgorithms for aggregation, incremental relabelling, and pricing in\ncrowdsourcing. In particular, we, first, discuss how to connect these three\ncomponents to build an efficient label collection process; and, second, share\nrich industrial experiences of applying these algorithms and constructing\nlarge-scale label collection pipelines (emphasizing best practices and common\npitfalls).\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 01:36:21 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Drutsa", "Alexey", ""], ["Farafonova", "Viktoriya", ""], ["Fedorova", "Valentina", ""], ["Megorskaya", "Olga", ""], ["Zerminova", "Evfrosiniya", ""], ["Zhilinskaya", "Olga", ""]]}, {"id": "1912.04464", "submitter": "Oswald Barral", "authors": "Cristina Conati, Oswald Barral, Vanessa Putnam, Lea Rieger", "title": "Toward Personalized XAI: A Case Study in Intelligent Tutoring Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research is a step toward ascertaining the need for personalization, in\nXAI, and we do so in the context of investigating the value of explanations of\nAI-driven hints and feedback are useful in Intelligent Tutoring Systems (ITS).\nWe added an explanation functionality for the adaptive hints provided by the\nAdaptive CSP (ACSP) applet, an interactive simulation that helps students learn\nan algorithm for constraint satisfaction problems by providing AI-driven hints\nadapted to their predicted level of learning. We present the design of the\nexplanation functionality and the results of a controlled study to evaluate its\nimpact on students' learning and perception of the ACPS hints. The study\nincludes an analysis of how these outcomes are modulated by several user\ncharacteristics such as personality traits and cognitive abilities, to asses if\nexplanations should be personalized to these characteristics. Our results\nindicate that providing explanations increase students' trust in the ACPS\nhints, perceived usefulness of the hints, and intention to use them again. In\naddition, we show that students' access of the explanation and learning gains\nare modulated by user characteristics, providing insights toward designing\npersonalized Explainable AI (XAI) for ITS.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 02:57:17 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 23:13:18 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 16:11:24 GMT"}, {"version": "v4", "created": "Wed, 12 Aug 2020 23:01:46 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Conati", "Cristina", ""], ["Barral", "Oswald", ""], ["Putnam", "Vanessa", ""], ["Rieger", "Lea", ""]]}, {"id": "1912.04711", "submitter": "Quim Comas Mart\\'inez", "authors": "Joaquim Comas, Decky Aspandi and Xavier Binefa", "title": "End-to-end facial and physiological model for Affective Computing and\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Affective Computing and its applications have become a\nfast-growing research topic. Furthermore, the rise of Deep Learning has\nintroduced significant improvements in the emotion recognition system compared\nto classical methods. In this work, we propose a multi-modal emotion\nrecognition model based on deep learning techniques using the combination of\nperipheral physiological signals and facial expressions. Moreover, we present\nan improvement to proposed models by introducing latent features extracted from\nour internal Bio Auto-Encoder (BAE). Both models are trained and evaluated on\nAMIGOS datasets reporting valence, arousal, and emotion state classification.\nFinally, to demonstrate a possible medical application in affective computing\nusing deep learning techniques, we applied the proposed method to the\nassessment of anxiety therapy. To this purpose, a reduced multi-modal database\nhas been collected by recording facial expressions and peripheral signals such\nas Electrocardiogram (ECG) and Galvanic Skin Response (GSR) of each patient.\nValence and arousal estimation was extracted using the proposed model from the\nbeginning until the end of the therapy, with successful evaluation to the\ndifferent emotional changes in the temporal domain.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 14:37:15 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 11:25:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Comas", "Joaquim", ""], ["Aspandi", "Decky", ""], ["Binefa", "Xavier", ""]]}, {"id": "1912.04719", "submitter": "Michael Coblenz", "authors": "Michael Coblenz, Gauri Kambhatla, Paulette Koronkevich, Jenna L. Wise,\n  Celeste Barnaby, Joshua Sunshine, Jonathan Aldrich, Brad A. Myers", "title": "PLIERS: A Process that Integrates User-Centered Methods into Programming\n  Language Design", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming language design requires making many usability-related design\ndecisions. However, existing HCI methods can be impractical to apply to\nprogramming languages: they have high iteration costs, programmers require\nsignificant learning time, and user performance has high variance. To address\nthese problems, we adapted both formative and summative HCI methods to make\nthem more suitable for programming language design. We integrated these methods\ninto a new process, PLIERS, for designing programming languages in a\nuser-centered way. We evaluated PLIERS by using it to design two new\nprogramming languages. Glacier extends Java to enable programmers to express\nimmutability properties effectively and easily. Obsidian is a language for\nblockchains that includes verification of critical safety properties. Summative\nusability studies showed that programmers were able to program effectively in\nboth languages after short training periods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 14:45:18 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 21:19:57 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2020 04:07:39 GMT"}, {"version": "v4", "created": "Tue, 25 Aug 2020 17:24:49 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Coblenz", "Michael", ""], ["Kambhatla", "Gauri", ""], ["Koronkevich", "Paulette", ""], ["Wise", "Jenna L.", ""], ["Barnaby", "Celeste", ""], ["Sunshine", "Joshua", ""], ["Aldrich", "Jonathan", ""], ["Myers", "Brad A.", ""]]}, {"id": "1912.04760", "submitter": "Deniz Ekiz", "authors": "Deniz Ekiz, Yekta Said Can, Yagmur Ceren Dardagan, Cem Ersoy", "title": "Is Your Smartband Smart Enough to Know Who You Are: Continuous\n  Physiological Authentication in The Wild", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.2982852", "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of cloud services that process privacy-sensitive information such as\ndigital banking, pervasive healthcare, smart home applications requires an\nimplicit continuous authentication solution which will make these systems less\nvulnerable to the spoofing attacks. Physiological signals can be used for\ncontinuous authentication due to their personal uniqueness. Ubiquitous\nwrist-worn wearable devices are equipped with photoplethysmogram sensors which\nenable to extract heart rate variability (HRV) features. In this study, we show\nthat these devices can be used for continuous physiological authentication, for\nenhancing the security of the cloud, edge services, and IoT devices. A system\nthat is suitable for the smartband framework comes with new challenges such as\nrelatively low signal quality and artifacts due to placement which were not\nencountered in full lead electrocardiogram systems. After the artifact removal,\ncleaned physiological signals are fed to the machine learning algorithms. In\norder to train our machine learning models, we collected physiological data\nusing off-the-shelf smartbands and smartwatches in a real-life event.\nPerformance evaluation of selected machine learning algorithms shows that HRV\nis a strong candidate for continuous unobtrusive implicit physiological\nauthentication.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:25:06 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 08:59:57 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Ekiz", "Deniz", ""], ["Can", "Yekta Said", ""], ["Dardagan", "Yagmur Ceren", ""], ["Ersoy", "Cem", ""]]}, {"id": "1912.04786", "submitter": "Javier Hernandez-Ortega", "authors": "Javier Hernandez-Ortega, Roberto Daza, Aythami Morales, Julian\n  Fierrez, Javier Ortega-Garcia", "title": "edBB: Biometrics and Behavior for Assessing Remote Education", "comments": "Preprint of the paper presented to the Workshop on Artificial\n  Intelligence for Education (AI4EDU) of AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a platform for student monitoring in remote education consisting\nof a collection of sensors and software that capture biometric and behavioral\ndata. We define a collection of tasks to acquire behavioral data that can be\nuseful for facing the existing challenges in automatic student monitoring\nduring remote evaluation. Additionally, we release an initial database\nincluding data from 20 different users completing these tasks with a set of\nbasic sensors: webcam, microphone, mouse, and keyboard; and also from more\nadvanced sensors: NIR camera, smartwatch, additional RGB cameras, and an EEG\nband. Information from the computer (e.g. system logs, MAC, IP, or web browsing\nhistory) is also stored. During each acquisition session each user completed\nthree different types of tasks generating data of different nature: mouse and\nkeystroke dynamics, face data, and audio data among others. The tasks have been\ndesigned with two main goals in mind: i) analyse the capacity of such biometric\nand behavioral data for detecting anomalies during remote evaluation, and ii)\nstudy the capability of these data, i.e. EEG, ECG, or NIR video, for estimating\nother information about the users such as their attention level, the presence\nof stress, or their pulse rate.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:55:11 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Hernandez-Ortega", "Javier", ""], ["Daza", "Roberto", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Ortega-Garcia", "Javier", ""]]}, {"id": "1912.04836", "submitter": "Chris Xiaoxuan Lu", "authors": "Chris Xiaoxuan Lu, Bowen Du, Hongkai Wen, Sen Wang, Andrew Markham,\n  Ivan Martinovic, Yiran Shen and Niki Trigoni", "title": "Snoopy: Sniffing Your Smartwatch Passwords via Deep Sequence Learning", "comments": "27 pages. Originally published at ACM UbiComp 2018. This version\n  corrects some errors in the original version and add the pointer to released\n  code & dataset", "journal-ref": null, "doi": "10.1145/3161196", "report-no": null, "categories": "cs.HC cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Demand for smartwatches has taken off in recent years with new models which\ncan run independently from smartphones and provide more useful features,\nbecoming first-class mobile platforms. One can access online banking or even\nmake payments on a smartwatch without a paired phone. This makes smartwatches\nmore attractive and vulnerable to malicious attacks, which to date have been\nlargely overlooked. In this paper, we demonstrate Snoopy, a password extraction\nand inference system which is able to accurately infer passwords entered on\nAndroid/Apple watches within 20 attempts, just by eavesdropping on motion\nsensors. Snoopy uses a uniform framework to extract the segments of motion data\nwhen passwords are entered, and uses novel deep neural networks to infer the\nactual passwords. We evaluate the proposed Snoopy system in the real-world with\ndata from 362 participants and show that our system offers a 3-fold improvement\nin the accuracy of inferring passwords compared to the state-of-the-art,\nwithout consuming excessive energy or computational resources. We also show\nthat Snoopy is very resilient to user and device heterogeneity: it can be\ntrained on crowd-sourced motion data (e.g. via Amazon Mechanical Turk), and\nthen used to attack passwords from a new user, even if they are wearing a\ndifferent model. This paper shows that, in the wrong hands, Snoopy can\npotentially cause serious leaks of sensitive information. By raising awareness,\nwe invite the community and manufacturers to revisit the risks of continuous\nmotion sensing on smart wearable devices.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:25:40 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 10:24:33 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Lu", "Chris Xiaoxuan", ""], ["Du", "Bowen", ""], ["Wen", "Hongkai", ""], ["Wang", "Sen", ""], ["Markham", "Andrew", ""], ["Martinovic", "Ivan", ""], ["Shen", "Yiran", ""], ["Trigoni", "Niki", ""]]}, {"id": "1912.04853", "submitter": "Brandon Carter", "authors": "Angie Boggust, Brandon Carter, Arvind Satyanarayan", "title": "Embedding Comparator: Visualizing Differences in Global Structure and\n  Local Neighborhoods via Small Multiples", "comments": "Equal contribution by first two authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings mapping high-dimensional discrete input to lower-dimensional\ncontinuous vector spaces have been widely adopted in machine learning\napplications as a way to capture domain semantics. Interviewing 13 embedding\nusers across disciplines, we find comparing embeddings is a key task for\ndeployment or downstream analysis but unfolds in a tedious fashion that poorly\nsupports systematic exploration. In response, we present the Embedding\nComparator, an interactive system that presents a global comparison of\nembedding spaces alongside fine-grained inspection of local neighborhoods. It\nsystematically surfaces points of comparison by computing the similarity of the\n$k$-nearest neighbors of every embedded object between a pair of spaces.\nThrough case studies, we demonstrate our system rapidly reveals insights, such\nas semantic changes following fine-tuning, language changes over time, and\ndifferences between seemingly similar models. In evaluations with 15\nparticipants, we find our system accelerates comparisons by shifting from\nlaborious manual specification to browsing and manipulating visualizations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:46:43 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 21:28:57 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Boggust", "Angie", ""], ["Carter", "Brandon", ""], ["Satyanarayan", "Arvind", ""]]}, {"id": "1912.04896", "submitter": "Damith Senanayake PhD", "authors": "Damith Senanayake, Wei Wang, Shalin H. Naik, Saman Halgamuge", "title": "Self Organizing Nebulous Growths for Robust and Incremental Data\n  Visualization", "comments": "in IEEE Transactions on Neural Networks and Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2020.3023941.", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-parametric dimensionality reduction techniques, such as t-SNE and UMAP,\nare proficient in providing visualizations for datasets of fixed sizes.\nHowever, they cannot incrementally map and insert new data points into an\nalready provided data visualization. We present Self-Organizing Nebulous\nGrowths (SONG), a parametric nonlinear dimensionality reduction technique that\nsupports incremental data visualization, i.e., incremental addition of new data\nwhile preserving the structure of the existing visualization. In addition, SONG\nis capable of handling new data increments, no matter whether they are similar\nor heterogeneous to the already observed data distribution. We test SONG on a\nvariety of real and simulated datasets. The results show that SONG is superior\nto Parametric t-SNE, t-SNE and UMAP in incremental data visualization.\nSpecifically, for heterogeneous increments, SONG improves over Parametric t-SNE\nby 14.98 % on the Fashion MNIST dataset and 49.73% on the MNIST dataset\nregarding the cluster quality measured by the Adjusted Mutual Information\nscores. On similar or homogeneous increments, the improvements are 8.36% and\n42.26% respectively. Furthermore, even when the above datasets are presented\nall at once, SONG performs better or comparable to UMAP, and superior to t-SNE.\nWe also demonstrate that the algorithmic foundations of SONG render it more\ntolerant to noise compared to UMAP and t-SNE, thus providing greater utility\nfor data with high variance, high mixing of clusters, or noise.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 22:11:51 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 21:59:58 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 01:18:23 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Senanayake", "Damith", ""], ["Wang", "Wei", ""], ["Naik", "Shalin H.", ""], ["Halgamuge", "Saman", ""]]}, {"id": "1912.05011", "submitter": "Felix Biessmann", "authors": "Felix Biessmann and Dionysius Irza Refiano", "title": "A psychophysics approach for quantitative comparison of interpretable\n  computer vision models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of transparent Machine Learning (ML) has contributed many novel\nmethods aiming at better interpretability for computer vision and ML models in\ngeneral. But how useful the explanations provided by transparent ML methods are\nfor humans remains difficult to assess. Most studies evaluate interpretability\nin qualitative comparisons, they use experimental paradigms that do not allow\nfor direct comparisons amongst methods or they report only offline experiments\nwith no humans in the loop. While there are clear advantages of evaluations\nwith no humans in the loop, such as scalability, reproducibility and less\nalgorithmic bias than with humans in the loop, these metrics are limited in\ntheir usefulness if we do not understand how they relate to other metrics that\ntake human cognition into account. Here we investigate the quality of\ninterpretable computer vision algorithms using techniques from psychophysics.\nIn crowdsourced annotation tasks we study the impact of different\ninterpretability approaches on annotation accuracy and task time. In order to\nrelate these findings to quality measures for interpretability without humans\nin the loop we compare quality metrics with and without humans in the loop. Our\nresults demonstrate that psychophysical experiments allow for robust quality\nassessment of transparency in machine learning. Interestingly the quality\nmetrics computed without humans in the loop did not provide a consistent\nranking of interpretability methods nor were they representative for how useful\nan explanation was for humans. These findings highlight the potential of\nmethods from classical psychophysics for modern machine learning applications.\nWe hope that our results provide convincing arguments for evaluating\ninterpretability in its natural habitat, human-ML interaction, if the goal is\nto obtain an authentic assessment of interpretability.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 16:51:20 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Biessmann", "Felix", ""], ["Refiano", "Dionysius Irza", ""]]}, {"id": "1912.05045", "submitter": "James Bagrow", "authors": "Abigail Hotaling and James P. Bagrow", "title": "Efficient crowdsourcing of crowd-generated microtasks", "comments": "12 pages, 5 figures", "journal-ref": "PLoS ONE 15(12): e0244245, 2020", "doi": "10.1371/journal.pone.0244245", "report-no": null, "categories": "cs.HC cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allowing members of the crowd to propose novel microtasks for one another is\nan effective way to combine the efficiencies of traditional microtask work with\nthe inventiveness and hypothesis generation potential of human workers.\nHowever, microtask proposal leads to a growing set of tasks that may overwhelm\nlimited crowdsourcer resources. Crowdsourcers can employ methods to utilize\ntheir resources efficiently, but algorithmic approaches to efficient\ncrowdsourcing generally require a fixed task set of known size. In this paper,\nwe introduce *cost forecasting* as a means for a crowdsourcer to use efficient\ncrowdsourcing algorithms with a growing set of microtasks. Cost forecasting\nallows the crowdsourcer to decide between eliciting new tasks from the crowd or\nreceiving responses to existing tasks based on whether or not new tasks will\ncost less to complete than existing tasks, efficiently balancing resources as\ncrowdsourcing occurs. Experiments with real and synthetic crowdsourcing data\nshow that cost forecasting leads to improved accuracy. Accuracy and efficiency\ngains for crowd-generated microtasks hold the promise to further leverage the\ncreativity and wisdom of the crowd, with applications such as generating more\ninformative and diverse training data for machine learning applications and\nimproving the performance of user-generated content and question-answering\nplatforms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 23:23:54 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 19:24:17 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Hotaling", "Abigail", ""], ["Bagrow", "James P.", ""]]}, {"id": "1912.05047", "submitter": "Yi Ren", "authors": "Namwoo Kang, Yi Ren, Fred Feinberg, and Panos Papalambros", "title": "Form + Function: Optimizing Aesthetic Product Design via Adaptive,\n  Geometrized Preference Elicitation", "comments": "submitted to Marketing Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual design is critical to product success, and the subject of intensive\nmarketing research effort. Yet visual elements, due to their holistic and\ninteractive nature, do not lend themselves well to optimization using extant\ndecompositional methods for preference elicitation. Here we present a\nsystematic methodology to incorporate interactive, 3D-rendered product\nconfigurations into a conjoint-like framework. The method relies on rapid,\nscalable machine learning algorithms to adaptively update product designs along\nwith standard information-oriented product attributes. At its heart is a\nparametric account of a product's geometry, along with a novel, adaptive\n\"bi-level\" query task that can estimate individuals' visual design form\npreferences and their trade-offs against such traditional elements as price and\nproduct features. We illustrate the method's performance through extensive\nsimulations and robustness checks, a formal proof of the bi-level query\nmethodology's domain of superiority, and a field test for the design of a\nmid-priced sedan, using real-time 3D rendering for an online panel. Results\nindicate not only substantially enhanced predictive accuracy, but two\nquantities beyond the reach of standard conjoint methods: trade-offs between\nform and function overall, and willingness-to-pay for specific design elements.\nMoreover -- and most critically for applications -- the method provides\n\"optimal\" visual designs for both individuals and model-derived or\nanalyst-supplied consumer groupings, as well as their sensitivities to form and\nfunctional elements.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 23:36:49 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Kang", "Namwoo", ""], ["Ren", "Yi", ""], ["Feinberg", "Fred", ""], ["Papalambros", "Panos", ""]]}, {"id": "1912.05284", "submitter": "Tomi Peltola", "authors": "Mustafa Mert \\c{C}elikok, Tomi Peltola, Pedram Daee, Samuel Kaski", "title": "Interactive AI with a Theory of Mind", "comments": "This is a slightly updated version of a manuscript that appeared in\n  ACM CHI 2019 Workshop: Computational Modeling in Human-Computer Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding each other is the key to success in collaboration. For humans,\nattributing mental states to others, the theory of mind, provides the crucial\nadvantage. We argue for formulating human--AI interaction as a multi-agent\nproblem, endowing AI with a computational theory of mind to understand and\nanticipate the user. To differentiate the approach from previous work, we\nintroduce a categorisation of user modelling approaches based on the level of\nagency learnt in the interaction. We describe our recent work in using nested\nmulti-agent modelling to formulate user models for multi-armed bandit based\ninteractive AI systems, including a proof-of-concept user study.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 19:26:48 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["\u00c7elikok", "Mustafa Mert", ""], ["Peltola", "Tomi", ""], ["Daee", "Pedram", ""], ["Kaski", "Samuel", ""]]}, {"id": "1912.05474", "submitter": "Vivek Singh", "authors": "Vivek Singh, Mary Chayko, Raj Inamdar, and Diana Floegel", "title": "Female Librarians and Male Computer Programmers? Gender Bias in\n  Occupational Images on Digital Media Platforms", "comments": "Article accepted for publication in The Journal of the Association\n  for Information Science and Technology (JASIST)", "journal-ref": null, "doi": "10.1002/ASI.24335", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media platforms, technological systems, and search engines act as conduits\nand gatekeepers for all kinds of information. They often influence, reflect,\nand reinforce gender stereotypes, including those that represent occupations.\nThis study examines the prevalence of gender stereotypes on digital media\nplatforms and considers how human efforts to create and curate messages\ndirectly may impact these stereotypes. While gender stereotyping in social\nmedia and algorithms has received some examination in recent literature, its\nprevalence in different types of platforms (e.g., wiki vs. news vs. social\nnetwork) and under differing conditions (e.g., degrees of human and machine led\ncontent creation and curation) has yet to be studied. This research explores\nthe extent to which stereotypes of certain strongly gendered professions\n(librarian, nurse, computer programmer, civil engineer) persist and may vary\nacross digital platforms (Twitter, the New York Times online, Wikipedia, and\nShutterstock). The results suggest that gender stereotypes are most likely to\nbe challenged when human beings act directly to create and curate content in\ndigital platforms, and that highly algorithmic approaches for curation showed\nlittle inclination towards breaking stereotypes. Implications for the more\ninclusive design and use of digital media platforms, particularly with regard\nto mediated occupational messaging, are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 17:10:23 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Singh", "Vivek", ""], ["Chayko", "Mary", ""], ["Inamdar", "Raj", ""], ["Floegel", "Diana", ""]]}, {"id": "1912.05538", "submitter": "Robert Soden", "authors": "Robert Soden, Dennis Wagenaar, Dave Luo, Annegien Tijssen", "title": "Taking Ethics, Fairness, and Bias Seriously in Machine Learning for\n  Disaster Risk Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper highlights an important, if under-examined, set of questions about\nthe deployment of machine learning technologies in the field of disaster risk\nmanagement (DRM). While emerging tools show promising capacity to support\nscientific efforts to better understand and mitigate the threats posed by\ndisasters and climate change, our field must undertake a much more careful\nassessment of the potential negative impacts that machine learning technologies\nmay create. We also argue that attention to these issues in the context of\nmachine learning affords the opportunity to have discussions about potential\nethics, bias, and fairness concerns within disaster data more broadly. In what\nfollows, we first describe some of the uses and potential benefits of\nmachine-learning technology in disaster risk management. We then draw on\nresearch from other fields to speculate about potential negative impacts.\nFinally, we outline a research agenda for how our disaster risk management can\nbegin to take these issues seriously and ensure that deployments of\nmachine-learning tools are conducted in a responsible and beneficial manner.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:37:45 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 18:17:41 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Soden", "Robert", ""], ["Wagenaar", "Dennis", ""], ["Luo", "Dave", ""], ["Tijssen", "Annegien", ""]]}, {"id": "1912.05662", "submitter": "Roger Immich", "authors": "Diego O. Rodrigues, Frances A. Santos, Geraldo P. Rocha Filho, Ademar\n  T. Akabane, Raquel Cabral, Roger Immich, Wellington L. Junior, Felipe D.\n  Cunha, Daniel L. Guidoni, Thiago H. Silva, Denis Ros\\'ario, Eduardo\n  Cerqueira, Antonio A. F. Loureiro, Leandro A. Villas", "title": "Computa\\c{c}\\~ao Urbana da Teoria \\`a Pr\\'atica: Fundamentos,\n  Aplica\\c{c}\\~oes e Desafios", "comments": "in Portuguese. Simp\\'osio Brasileiro de Redes de Computadores e\n  Sistemas Distribu\\'idos (SBRC) 2019 - Minicursos", "journal-ref": "Simposio Brasileiro de Redes de Computadores e Sistemas\n  Distribuidos (SBRC), 2019", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.DC cs.HC cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing of cities has resulted in innumerable technical and managerial\nchallenges for public administrators such as energy consumption, pollution,\nurban mobility and even supervision of private and public spaces in an\nappropriate way. Urban Computing emerges as a promising paradigm to solve such\nchallenges, through the extraction of knowledge, from a large amount of\nheterogeneous data existing in urban space. Moreover, Urban Computing\ncorrelates urban sensing, data management, and analysis to provide services\nthat have the potential to improve the quality of life of the citizens of large\nurban centers. Consider this context, this chapter aims to present the\nfundamentals of Urban Computing and the steps necessary to develop an\napplication in this area. To achieve this goal, the following questions will be\ninvestigated, namely: (i) What are the main research problems of Urban\nComputing?; (ii) What are the technological challenges for the implementation\nof services in Urban Computing?; (iii) What are the main methodologies used for\nthe development of services in Urban Computing?; and (iv) What are the\nrepresentative applications in this field?\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:01:58 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Rodrigues", "Diego O.", ""], ["Santos", "Frances A.", ""], ["Filho", "Geraldo P. Rocha", ""], ["Akabane", "Ademar T.", ""], ["Cabral", "Raquel", ""], ["Immich", "Roger", ""], ["Junior", "Wellington L.", ""], ["Cunha", "Felipe D.", ""], ["Guidoni", "Daniel L.", ""], ["Silva", "Thiago H.", ""], ["Ros\u00e1rio", "Denis", ""], ["Cerqueira", "Eduardo", ""], ["Loureiro", "Antonio A. F.", ""], ["Villas", "Leandro A.", ""]]}, {"id": "1912.05971", "submitter": "Yucheng Zhu", "authors": "Yucheng Zhu, Xiongkuo Min, DanDan Zhu, Ke Gu, Jiantao Zhou, Guangtao\n  Zhai, Xiaokang Yang, and Wenjun Zhang", "title": "Toward Better Understanding of Saliency Prediction in Augmented 360\n  Degree Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality (AR) overlays digital content onto the reality. In AR\nsystem, correct and precise estimations of user's visual fixations and head\nmovements can enhance the quality of experience by allocating more computation\nresources on the areas of interest. However, there is inadequate research about\nunderstanding the visual exploration of users when using an AR system or\nmodeling AR visual attention. To bridge the gap between the saliency prediction\non real-world scene and on scene augmented by virtual information, we construct\nthe ARVR saliency dataset with 12 diverse videos viewed by 20 people. The\nvirtual reality (VR) technique is employed to simulate the real-world.\nAnnotations of object recognition and tracking as augmented contents are\nblended into the omnidirectional videos. The saliency annotations of head and\neye movements for both original and augmented videos are collected and together\nconstitute the ARVR dataset. We also design a model which is capable of solving\nthe saliency prediction problem in AR. Local block images are extracted to\nsimulate the viewport and offset the projection distortion. Conspicuous visual\ncues in local viewports are extracted to constitute the spatial features. The\noptical flow information is estimated as the important temporal feature. We\nalso consider the interplay between virtual information and reality. The\ncomposition of the augmentation information is distinguished, and the joint\neffects of adversarial augmentation and complementary augmentation are\nestimated. We generate a graph by taking each block image as one node. Both the\nvisual saliency mechanism and the characteristics of viewing behaviors are\nconsidered in the computation of edge weights on the graph which are\ninterpreted as Markov chains. The fraction of the visual attention that is\ndiverted to each block image is estimated through equilibrium distribution on\nof this chain.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 14:16:05 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 13:54:08 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhu", "Yucheng", ""], ["Min", "Xiongkuo", ""], ["Zhu", "DanDan", ""], ["Gu", "Ke", ""], ["Zhou", "Jiantao", ""], ["Zhai", "Guangtao", ""], ["Yang", "Xiaokang", ""], ["Zhang", "Wenjun", ""]]}, {"id": "1912.06602", "submitter": "Rahul Shome", "authors": "Malihe Alikhani, Baber Khalid, Rahul Shome, Chaitanya Mitash, Kostas\n  Bekris, Matthew Stone", "title": "That and There: Judging the Intent of Pointing Actions with Robotic Arms", "comments": "Accepted to AAAI 2020, New York City", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative robotics requires effective communication between a robot and a\nhuman partner. This work proposes a set of interpretive principles for how a\nrobotic arm can use pointing actions to communicate task information to people\nby extending existing models from the related literature. These principles are\nevaluated through studies where English-speaking human subjects view animations\nof simulated robots instructing pick-and-place tasks. The evaluation\ndistinguishes two classes of pointing actions that arise in pick-and-place\ntasks: referential pointing (identifying objects) and locating pointing\n(identifying locations). The study indicates that human subjects show greater\nflexibility in interpreting the intent of referential pointing compared to\nlocating pointing, which needs to be more deliberate. The results also\ndemonstrate the effects of variation in the environment and task context on the\ninterpretation of pointing. Our corpus, experiments and design principles\nadvance models of context, common sense reasoning and communication in embodied\ncommunication.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 16:54:38 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Alikhani", "Malihe", ""], ["Khalid", "Baber", ""], ["Shome", "Rahul", ""], ["Mitash", "Chaitanya", ""], ["Bekris", "Kostas", ""], ["Stone", "Matthew", ""]]}, {"id": "1912.06723", "submitter": "Daniel Karl I. Weidele", "authors": "Daniel Karl I. Weidele, Justin D. Weisz, Eno Oduor, Michael Muller,\n  Josh Andres, Alexander Gray, Dakuo Wang", "title": "AutoAIViz: Opening the Blackbox of Automated Artificial Intelligence\n  with Conditional Parallel Coordinates", "comments": "5 pages, 1 figure, IUI2020", "journal-ref": null, "doi": "10.1145/3377325.3377538", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) can now automate the algorithm selection,\nfeature engineering, and hyperparameter tuning steps in a machine learning\nworkflow. Commonly known as AutoML or AutoAI, these technologies aim to relieve\ndata scientists from the tedious manual work. However, today's AutoAI systems\noften present only limited to no information about the process of how they\nselect and generate model results. Thus, users often do not understand the\nprocess, neither do they trust the outputs. In this short paper, we provide a\nfirst user evaluation by 10 data scientists of an experimental system,\nAutoAIViz, that aims to visualize AutoAI's model generation process. We find\nthat the proposed system helps users to complete the data science tasks, and\nincreases their understanding, toward the goal of increasing trust in the\nAutoAI system.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 21:53:01 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 16:32:18 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 15:51:23 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Weidele", "Daniel Karl I.", ""], ["Weisz", "Justin D.", ""], ["Oduor", "Eno", ""], ["Muller", "Michael", ""], ["Andres", "Josh", ""], ["Gray", "Alexander", ""], ["Wang", "Dakuo", ""]]}, {"id": "1912.06972", "submitter": "Wanshan Yang", "authors": "Wanshan Yang, Ting Huang, Junlin Zeng, Lijun Chen, Shivakant Mishra\n  and Youjian (Eugene) Liu", "title": "Utilizing Players' Playtime Records for Churn Prediction: Mining\n  Playtime Regularity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the free online game industry, churn prediction is an important research\ntopic. Reducing the churn rate of a game significantly helps with the success\nof the game. Churn prediction helps a game operator identify possible churning\nplayers and keep them engaged in the game via appropriate operational\nstrategies, marketing strategies, and/or incentives. Playtime related features\nare some of the widely used universal features for most churn prediction\nmodels. In this paper, we consider developing new universal features for churn\npredictions for long-term players based on players' playtime.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 04:51:15 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 11:40:06 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yang", "Wanshan", "", "Eugene"], ["Huang", "Ting", "", "Eugene"], ["Zeng", "Junlin", "", "Eugene"], ["Chen", "Lijun", "", "Eugene"], ["Mishra", "Shivakant", "", "Eugene"], ["Youjian", "", "", "Eugene"], ["Liu", "", ""]]}, {"id": "1912.06979", "submitter": "Jon Gillick", "authors": "Jon Gillick, David Bamman", "title": "Breaking Speech Recognizers to Imagine Lyrics", "comments": "3 pages", "journal-ref": "NeurIPS 2019 Workshop on Machine Learning for Creativity and\n  Design", "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for generating text, and in particular song lyrics,\nbased on the speech-like acoustic qualities of a given audio file. We repurpose\na vocal source separation algorithm and an acoustic model trained to recognize\nisolated speech, instead inputting instrumental music or environmental sounds.\nFeeding the \"mistakes\" of the vocal separator into the recognizer, we obtain a\ntranscription of words \\emph{imagined} to be spoken in the input audio. We\ndescribe the key components of our approach, present initial analysis, and\ndiscuss the potential of the method for machine-in-the-loop collaboration in\ncreative applications.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 05:34:45 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Gillick", "Jon", ""], ["Bamman", "David", ""]]}, {"id": "1912.07381", "submitter": "Q.Vera Liao", "authors": "Q. Vera Liao, Michael Muller", "title": "Enabling Value Sensitive AI Systems through Participatory Design\n  Fictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two general routes have been followed to develop artificial agents that are\nsensitive to human values---a top-down approach to encode values into the\nagents, and a bottom-up approach to learn from human actions, whether from\nreal-world interactions or stories. Although both approaches have made exciting\nscientific progress, they may face challenges when applied to the current\ndevelopment practices of AI systems, which require the under-standing of the\nspecific domains and specific stakeholders involved. In this work, we bring\ntogether perspectives from the human-computer interaction (HCI) community,\nwhere designing technologies sensitive to user values has been a longstanding\nfocus. We highlight several well-established areas focusing on developing\nempirical methods for inquiring user values. Based on these methods, we propose\nparticipatory design fictions to study user values involved in AI systems and\npresent preliminary results from a case study. With this paper, we invite the\nconsideration of user-centered value inquiry and value learning.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 01:16:03 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Liao", "Q. Vera", ""], ["Muller", "Michael", ""]]}, {"id": "1912.07416", "submitter": "Byung Hyung Kim", "authors": "Byung Hyung Kim, Seunghun Koh, Sejoon Huh, Sungho Jo, Sunghee Choi", "title": "Improved Explanatory Efficacy on Human Affect and Workload through\n  Interactive Process in Artificial Intelligence", "comments": null, "journal-ref": "IEEE Access, Vol.8, 2020", "doi": "10.1109/ACCESS.2020.3032056", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite recent advances in the field of explainable artificial intelligence\nsystems, a concrete quantitative measure for evaluating the usability of such\nsystems is nonexistent. Ensuring the success of an explanatory interface in\ninteracting with users requires a cyclic, symbiotic relationship between human\nand artificial intelligence. We, therefore, propose explanatory efficacy, a\nnovel metric for evaluating the strength of the cyclic relationship the\ninterface exhibits. Furthermore, in a user study, we evaluated the perceived\naffect and workload and recorded the EEG signals of our participants as they\ninteracted with our custom-built, iterative explanatory interface to build\npersonalized recommendation systems. We found that systems for perceptually\ndriven iterative tasks with greater explanatory efficacy are characterized by\nstatistically significant hemispheric differences in neural signals with 62.4%\naccuracy, indicating the feasibility of neural correlates as a measure of\nexplanatory efficacy. These findings are beneficial for researchers who aim to\nstudy the circular ecosystem of the human-artificial intelligence partnership.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 14:08:49 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 01:53:08 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Kim", "Byung Hyung", ""], ["Koh", "Seunghun", ""], ["Huh", "Sejoon", ""], ["Jo", "Sungho", ""], ["Choi", "Sunghee", ""]]}, {"id": "1912.07456", "submitter": "Audrey Girouard", "authors": "Audrey Girouard, Jon E. Froehlich, Regan Mandryk and Mark Hancock", "title": "Organizing Family Support Services at ACM Conferences", "comments": "This article is a complement to an abbreviated version published in\n  the April 2020 issue of the Communications of the ACM. This longer version\n  includes significantly more information on our approach to planning family\n  services at CHI2018, decision making, and our pre-conference and\n  post-conference assessments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reflects on our experiences providing family-support services to\na large, interdisciplinary ACM conference (CHI2018) including, the policy\ndecisions, the challenges, and the successes. The article incorporates\nempirical data collected from pre- and post-conference surveys, observed use of\nthe services, and aspirational aims for future conferences. We are discussing\nbest practices and recommendations to facilitate the implementation of child\nsupport services at other conferences. We believe our article will be of great\ninterest to both practitioners and academics in expanding the inclusivity and\nfamily support provided by ACM conferences and beyond.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 17:22:01 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Girouard", "Audrey", ""], ["Froehlich", "Jon E.", ""], ["Mandryk", "Regan", ""], ["Hancock", "Mark", ""]]}, {"id": "1912.07773", "submitter": "Sonia Baee", "authors": "Sonia Baee, Erfan Pakdamanian, Inki Kim, Lu Feng, Vicente Ordonez,\n  Laura Barnes", "title": "MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy\n  Deep Inverse Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by human visual attention, we introduce a Maximum Entropy Deep\nInverse Reinforcement Learning (MEDIRL) framework for modeling the visual\nattention allocation of drivers in imminent rear-end collisions. MEDIRL is\ncomposed of visual, driving, and attention modules. Given a front-view driving\nvideo and corresponding eye fixations from humans, the visual and driving\nmodules extract generic and driving-specific visual features, respectively.\nFinally, the attention module learns the intrinsic task-sensitive reward\nfunctions induced by eye fixation policies recorded from attentive drivers.\nMEDIRL uses the learned policies to predict visual attention allocation of\ndrivers. We also introduce EyeCar, a new driver visual attention dataset during\naccident-prone situations. We conduct comprehensive experiments and show that\nMEDIRL outperforms previous state-of-the-art methods on driving task-related\nvisual attention allocation on the following large-scale driving attention\nbenchmark datasets: DR(eye)VE, BDD-A, and DADA-2000. The code and dataset are\nprovided for reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 01:05:26 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 00:34:35 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 00:37:03 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Baee", "Sonia", ""], ["Pakdamanian", "Erfan", ""], ["Kim", "Inki", ""], ["Feng", "Lu", ""], ["Ordonez", "Vicente", ""], ["Barnes", "Laura", ""]]}, {"id": "1912.07827", "submitter": "Yue Jiang", "authors": "Yue Jiang, Ruofei Du, Christof Lutteroth, Wolfgang Stuerzlinger", "title": "ORC Layout: Adaptive GUI Layout with OR-Constraints", "comments": null, "journal-ref": null, "doi": "10.1145/3290605.3300643", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for constraint-based graphical user interface\n(GUI) layout based on OR-constraints (ORC) in standard soft/hard linear\nconstraint systems. ORC layout unifies grid layout and flow layout, supporting\nboth their features as well as cases where grid and flow layouts individually\nfail. We describe ORC design patterns that enable designers to safely create\nflexible layouts that work across different screen sizes and orientations. We\nalso present the ORC Editor, a GUI editor that enables designers to apply ORC\nin a safe and effective manner, mixing grid, flow and new ORC layout features\nas appropriate. We demonstrate that our prototype can adapt layouts to screens\nwith different aspect ratios with only a single layout specification, easing\nthe burden of GUI maintenance. Finally, we show that ORC specifications can be\nmodified interactively and solved efficiently at runtime.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 05:41:42 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Jiang", "Yue", ""], ["Du", "Ruofei", ""], ["Lutteroth", "Christof", ""], ["Stuerzlinger", "Wolfgang", ""]]}, {"id": "1912.07938", "submitter": "Galit Shmueli", "authors": "Travis Greene and Galit Shmueli", "title": "How Personal is Machine Learning Personalization?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though used extensively, the concept and process of machine learning (ML)\npersonalization have generally received little attention from academics,\npractitioners, and the general public. We describe the ML approach as relying\non the metaphor of the person as a feature vector and contrast this with\nhumanistic views of the person. In light of the recent calls by the IEEE to\nconsider the effects of ML on human well-being, we ask whether ML\npersonalization can be reconciled with these humanistic views of the person,\nwhich highlight the importance of moral and social identity. As human behavior\nincreasingly becomes digitized, analyzed, and predicted, to what extent do our\nsubsequent decisions about what to choose, buy, or do, made both by us and\nothers, reflect who we are as persons? This paper first explicates the term\npersonalization by considering ML personalization and highlights its relation\nto humanistic conceptions of the person, then proposes several dimensions for\nevaluating the degree of personalization of ML personalized scores. By doing\nso, we hope to contribute to current debate on the issues of algorithmic bias,\ntransparency, and fairness in machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 11:37:19 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 03:39:46 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Greene", "Travis", ""], ["Shmueli", "Galit", ""]]}, {"id": "1912.08097", "submitter": "Avinash Kumar Singh", "authors": "Avinash Kumar Singh, Kai-Florian Richter", "title": "Conflict Detection and Resolution in Table Top Scenarios for Human-Robot\n  Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As in any interaction process, misunderstandings, ambiguity, and failures to\ncorrectly understand the interaction partner are bound to happen in human-robot\ninteraction. We term these failures 'conflicts' and are interested in both\nconflict detection and conflict resolution. In that, we focus on the robot's\nperspective. For the robot, conflicts may occur because of errors in its\nperceptual processes or because of ambiguity stemming from human input. This\nposter presents a brief system overview, and details Here, we briefly outline\nthe project's motivation and setting, introduce the general processing\nframework, and then present two kinds of conflicts in some more detail: 1) a\nfailure to identify a relevant object at all; 2) ambiguity emerging from\nmultiple matches in scene perception.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 12:04:52 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 15:37:25 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Singh", "Avinash Kumar", ""], ["Richter", "Kai-Florian", ""]]}, {"id": "1912.08101", "submitter": "Christoph Kinkeldey", "authors": "Christoph Kinkeldey, Jean-Daniel Fekete, Tanja Blascheck and Petra\n  Isenberg", "title": "Visualizing and Analyzing Entity Activity on the Bitcoin Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BitConduite, a visual analytics tool for explorative analysis of\nfinancial activity within the Bitcoin network. Bitcoin is the largest\ncryptocurrency worldwide and a phenomenon that challenges the underpinnings of\ntraditional financial systems - its users can send money pseudo-anonymously\nwhile circumventing traditional banking systems. Yet, despite the fact that all\nfinancial transactions in Bitcoin are available in an openly accessible online\nledger - the blockchain - not much is known about how different types of actors\nin the network (we call them entities) actually use Bitcoin. BitConduite offers\nan entity-centered view on transactions, making the data accessible to\nnon-technical experts through a guided workflow for classification of entities\naccording to several activity metrics. Other novelties are the possibility to\ncluster entities by similarity and exploration of transaction data at different\nscales, from large groups of entities down to a single entity and the\nassociated transactions. Two use cases illustrate the workflow of the system\nand its analytic power. We report on feedback regarding the approach and the\nthe software tool gathered during a workshop with domain experts, and we\ndiscuss the potential of the approach based on our findings.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 15:47:57 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 12:14:06 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Kinkeldey", "Christoph", ""], ["Fekete", "Jean-Daniel", ""], ["Blascheck", "Tanja", ""], ["Isenberg", "Petra", ""]]}, {"id": "1912.08377", "submitter": "Heng Xu", "authors": "Heng Xu, Michael A. Peshkin, J. Edward Colgate", "title": "How the Mechanical Properties and Thickness of Glass Affect TPaD\n  Performance", "comments": "IEEE Transactions on Haptics", "journal-ref": null, "doi": "10.1109/TOH.2020.3013287", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One well-known class of surface haptic devices that we have called TPaDs\n(Tactile Pattern Displays) uses ultrasonic transverse vibrations of a touch\nsurface to modulate fingertip friction. This paper addresses the power\nconsumption of glass TPaDs, which is an important consideration in the context\nof mobile touchscreens. In particular, based on existing ultrasonic friction\nreduction models, we consider how the mechanical properties (density and\nYoung's modulus) and thickness of commonly-used glass formulations affect TPaD\nperformance, namely the relation between its friction reduction ability and its\nreal power consumption. Experiments performed with eight types of TPaDs and an\nelectromechanical model for the fingertip-TPaD system indicate: 1) TPaD\nperformance decreases as glass thickness increases; 2) TPaD performance\nincreases as the Young's modulus and density of glass decrease; 3) real power\nconsumption of a TPaD decreases as the contact force increases. Proper\napplications of these results can lead to significant increases in TPaD\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 04:47:30 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 00:08:54 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Xu", "Heng", ""], ["Peshkin", "Michael A.", ""], ["Colgate", "J. Edward", ""]]}, {"id": "1912.08381", "submitter": "Heng Xu", "authors": "Heng Xu, Roberta L. Klatzky, Michael A. Peshkin, J. Edward Colgate", "title": "Localizable Button Click Rendering via Active Lateral Force Feedback", "comments": "Accepted, IEEE Transactions on Haptics", "journal-ref": null, "doi": "10.1109/TOH.2020.2990947", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a novel button click rendering mechanism based on active\nlateral force feedback. The effect can be localized because electroadhesion\nbetween a finger and a surface can be localized. Psychophysical experiments\nwere conducted to evaluate the quality of a rendered button click, which\nsubjects judged to be acceptable. Both the experiment results and the subjects'\ncomments confirm that this button click rendering mechanism has the ability to\ngenerate a range of realistic button click sensations that could match\nsubjects' different preferences. We can thus generate a button click on a flat\nsurface without macroscopic motion of the surface in the lateral or normal\ndirection, and we can localize this haptic effect to an individual finger.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 05:06:40 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 06:26:47 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 03:39:34 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Xu", "Heng", ""], ["Klatzky", "Roberta L.", ""], ["Peshkin", "Michael A.", ""], ["Colgate", "J. Edward", ""]]}, {"id": "1912.08473", "submitter": "Daniel Graziotin", "authors": "Falko Koetter, Matthias Blohm, Jens Drawehn, Monika Kochanowski,\n  Joscha Goetzer, Daniel Graziotin, Stefan Wagner", "title": "Conversational Agents for Insurance Companies: From Theory to Practice", "comments": "26 pages, 7 figures. ICAART 2019 extension. Extension of\n  arXiv:1812.07339", "journal-ref": "Koetter F. et al. (2019) Conversational Agents for Insurance\n  Companies: From Theory to Practice. In: Agents and Artificial Intelligence.\n  ICAART 2019. LNCS, vol 11978. Springer, Cham", "doi": "10.1007/978-3-030-37494-5_17", "report-no": null, "categories": "cs.HC cs.AI cs.CY cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in artificial intelligence have renewed interest in conversational\nagents. Additionally to software developers, today all kinds of employees show\ninterest in new technologies and their possible applications for customers.\nGerman insurance companies generally are interested in improving their customer\nservice and digitizing their business processes. In this work we investigate\nthe potential use of conversational agents in insurance companies theoretically\nby determining which classes of agents exist which are of interest to insurance\ncompanies, finding relevant use cases and requirements. We add two practical\nparts: First we develop a showcase prototype for an exemplary insurance\nscenario in claim management. Additionally in a second step, we create a\nprototype focusing on customer service in a chatbot hackathon, fostering\ninnovation in interdisciplinary teams. In this work, we describe the results of\nboth prototypes in detail. We evaluate both chatbots defining criteria for both\nsettings in detail and compare the results and draw conclusions for the\nmaturity of chatbot technology for practical use, describing the opportunities\nand challenges companies, especially small and medium enterprises, face.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 09:26:55 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Koetter", "Falko", ""], ["Blohm", "Matthias", ""], ["Drawehn", "Jens", ""], ["Kochanowski", "Monika", ""], ["Goetzer", "Joscha", ""], ["Graziotin", "Daniel", ""], ["Wagner", "Stefan", ""]]}, {"id": "1912.08558", "submitter": "Christian Tominski", "authors": "Christian Eichner and Heidrun Schumann and Christian Tominski", "title": "Multi-display Visual Analysis: Model, Interface, and Layout Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern display environments offer great potential for involving multiple\nusers in presentations, discussions, and data analysis sessions. By showing\nmultiple views on multiple displays, information exchange can be improved,\nseveral perspectives on the data can be combined, and different analysis\nstrategies can be pursued.\n  In this report, we describe concepts to support display composition,\ninformation distribution, and analysis coordination for visual data analysis in\nmulti-display environments. In particular, a basic model for layout modeling is\nintroduced, a graphical interface for interactive generation of the model is\npresented, and a layout mechanism is described that arranges multiple views on\nmultiple displays automatically. Furthermore, approaches to meta-analysis will\nbe discussed. The developed approaches are demonstrated in a use case that\nfocuses on parameter space analysis for the segmentation of time series data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:21:01 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Eichner", "Christian", ""], ["Schumann", "Heidrun", ""], ["Tominski", "Christian", ""]]}, {"id": "1912.08609", "submitter": "Tim Ziemer", "authors": "Tim Ziemer and Nuttawut Nuchprayoon and Holger Schultheis", "title": "Psychoacoustic Sonification as User Interface for Human-Machine\n  Interaction", "comments": "14 pages, International Journal of Informatics Society", "journal-ref": "International Journal of Informatics Society 12(1), 2020, pp.\n  3-16, http://www.infsoc.org/journal/vol12/12-1", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When operating a machine, the operator needs to know some spatial relations,\nlike the relative location of the target or the nearest obstacle. Often,\nsensors are used to derive this spatial information, and visual displays are\ndeployed as interfaces to communicate this information to the operator. In this\npaper, we present psychoacoustic sonification as an alternative interface for\nhuman-machine interaction. Instead of visualizations, an interactive sound\nguides the operator to the desired target location, or helps her avoid\nobstacles in space. By considering psychoacoustics --- i.e., the relationship\nbetween the physical and the perceptual attributes of sound --- in the audio\nsignal processing, we can communicate precisely and unambiguously interpretable\ndirection and distance cues along three orthogonal axes to a user. We present\nexemplary use cases from various application areas where users can benefit from\npsychoacoustic sonification.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 13:51:28 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Ziemer", "Tim", ""], ["Nuchprayoon", "Nuttawut", ""], ["Schultheis", "Holger", ""]]}, {"id": "1912.08809", "submitter": "Joy Bose", "authors": "Joy Bose", "title": "Field Label Prediction for Autofill in Web Browsers", "comments": "3 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic form fill is an important productivity related feature present in\nmajor web browsers, which predicts the field labels of a web form and\nautomatically fills values in a new form based on the values previously filled\nfor the same field in other forms. This feature increases the convenience and\nefficiency of users who have to fill similar information in fields in multiple\nforms. In this paper we describe a machine learning solution for predicting the\nform field labels, implemented as a web service using Azure ML Studio.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 22:55:06 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Bose", "Joy", ""]]}, {"id": "1912.08904", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, Nick Craswell", "title": "Macaw: An Extensible Conversational Information Seeking Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational information seeking (CIS) has been recognized as a major\nemerging research area in information retrieval. Such research will require\ndata and tools, to allow the implementation and study of conversational\nsystems. This paper introduces Macaw, an open-source framework with a modular\narchitecture for CIS research. Macaw supports multi-turn, multi-modal, and\nmixed-initiative interactions, and enables research for tasks such as document\nretrieval, question answering, recommendation, and structured data exploration.\nIt has a modular design to encourage the study of new CIS algorithms, which can\nbe evaluated in batch mode. It can also integrate with a user interface, which\nallows user studies and data collection in an interactive mode, where the back\nend can be fully algorithmic or a wizard of oz setup. Macaw is distributed\nunder the MIT License.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 21:51:22 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Zamani", "Hamed", ""], ["Craswell", "Nick", ""]]}, {"id": "1912.08910", "submitter": "Nutta Homdee", "authors": "Nutta Homdee, Mehdi Boukhechba, Yixue W. Feng, Natalie Kramer, John\n  Lach, Laura E. Barnes", "title": "Enabling Smartphone-based Estimation of Heart Rate", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous, ubiquitous monitoring through wearable sensors has the potential\nto collect useful information about users' context. Heart rate is an important\nphysiologic measure used in a wide variety of applications, such as fitness\ntracking and health monitoring. However, wearable sensors that monitor heart\nrate, such as smartwatches and electrocardiogram (ECG) patches, can have gaps\nin their data streams because of technical issues (e.g., bad wireless channels,\nbattery depletion, etc.) or user-related reasons (e.g. motion artifacts, user\ncompliance, etc.). The ability to use other available sensor data (e.g.,\nsmartphone data) to estimate missing heart rate readings is useful to cope with\nany such gaps, thus improving data quality and continuity. In this paper, we\ntest the feasibility of estimating raw heart rate using smartphone sensor data.\nUsing data generated by 12 participants in a one-week study period, we were\nable to build both personalized and generalized models using regression, SVM,\nand random forest algorithms. All three algorithms outperformed the baseline\nmoving-average interpolation method for both personalized and generalized\nsettings. Moreover, our findings suggest that personalized models outperformed\nthe generalized models, which speaks to the importance of considering personal\nphysiology, behavior, and life style in the estimation of heart rate. The\npromising results provide preliminary evidence of the feasibility of combining\nsmartphone sensor data with wearable sensor data for continuous heart rate\nmonitoring.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 22:00:19 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Homdee", "Nutta", ""], ["Boukhechba", "Mehdi", ""], ["Feng", "Yixue W.", ""], ["Kramer", "Natalie", ""], ["Lach", "John", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1912.09007", "submitter": "Pedro Sequeira", "authors": "Pedro Sequeira and Melinda Gervasio", "title": "Interestingness Elements for Explainable Reinforcement Learning:\n  Understanding Agents' Capabilities and Limitations", "comments": "To appear in: Artificial Intelligence", "journal-ref": null, "doi": "10.1016/j.artint.2020.103367", "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an explainable reinforcement learning (XRL) framework that\nanalyzes an agent's history of interaction with the environment to extract\ninterestingness elements that help explain its behavior. The framework relies\non data readily available from standard RL algorithms, augmented with data that\ncan easily be collected by the agent while learning. We describe how to create\nvisual summaries of an agent's behavior in the form of short video-clips\nhighlighting key interaction moments, based on the proposed elements. We also\nreport on a user study where we evaluated the ability of humans to correctly\nperceive the aptitude of agents with different characteristics, including their\ncapabilities and limitations, given visual summaries automatically generated by\nour framework. The results show that the diversity of aspects captured by the\ndifferent interestingness elements is crucial to help humans correctly\nunderstand an agent's strengths and limitations in performing a task, and\ndetermine when it might need adjustments to improve its performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 03:46:22 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 03:25:14 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Sequeira", "Pedro", ""], ["Gervasio", "Melinda", ""]]}, {"id": "1912.09142", "submitter": "Davide De Tommaso", "authors": "Davide De Tommaso and Agnieszka Wykowska", "title": "TobiiGlassesPySuite: An open-source suite for using the Tobii Pro\n  Glasses 2 in eye-tracking studies", "comments": null, "journal-ref": null, "doi": "10.1145/3314111.3319828", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present the TobiiGlassesPySuite, an open-source suite we\nimplemented for using the Tobii Pro Glasses 2 wearable eye-tracker in custom\neye-tracking studies. We provide a platform-independent solution for\ncontrolling the device and for managing the recordings. The software consists\nof Python modules, integrated into a single package, accompanied by sample\nscripts and recordings. The proposed solution aims at providing additional\nmethods with respect to the manufacturer's software, for allowing the users to\nexploit more the device's capabilities and the existing software. Our suite is\navailable for download from the repository indicated in the paper and usable\naccording to the terms of the GNU GPL v3.0 license.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 11:46:57 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["De Tommaso", "Davide", ""], ["Wykowska", "Agnieszka", ""]]}, {"id": "1912.09148", "submitter": "Takashi Nose Prof.", "authors": "Keita Ishizuka, Takashi Nose", "title": "Developing a Multi-Platform Speech Recording System Toward Open Service\n  of Building Large-Scale Speech Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper briefly reports our ongoing attempt at the development of a\nmulti-platform browser-based speech recording system. We designed the system\ntoward a service of providing open service of building large-scale speech\ncorpora at a low-cost for any researchers and developers related to speech\nprocessing. The recent increase in the use of crowdsourcing services, e.g.,\nAmazon Mechanical Turk, enable us to reduce the cost of collecting speakers in\nthe web, and there have been many attempts to develop the automated speech\ncollecting platforms or application that is designed for the use the\ncrowdsourcing. However, one of the major problems in the previous studies and\ndevelopments for the attempts is that most of the systems are not a form of\ncommon service of speech recording and corpus building, and each corpus builder\nis necessary to develop the system in their own environment including a web\nserver. For this problem, we develope a new platform where both the corpus\nbuilders and recording participants can commonly use a single system and\nservice by creating their user accounts. A brief introduction of the system is\ngiven in this paper as the start of this challenge.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 11:57:22 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Ishizuka", "Keita", ""], ["Nose", "Takashi", ""]]}, {"id": "1912.09336", "submitter": "Nilavra Bhattacharya", "authors": "Nilavra Bhattacharya, Danna Gurari", "title": "VizWiz Dataset Browser: A Tool for Visualizing Machine Learning Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a visualization tool to exhaustively search and browse through a\nset of large-scale machine learning datasets. Built on the top of the VizWiz\ndataset, our dataset browser tool has the potential to support and enable a\nvariety of qualitative and quantitative research, and open new directions for\nvisualizing and researching with multimodal information. The tool is publicly\navailable at https://vizwiz.org/browse.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:18:34 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Bhattacharya", "Nilavra", ""], ["Gurari", "Danna", ""]]}, {"id": "1912.09380", "submitter": "Ulysse C\\^ot\\'e-Allard", "authors": "Ulysse C\\^ot\\'e-Allard, Gabriel Gagnon-Turcotte, Angkoon Phinyomark,\n  Kyrre Glette, Erik Scheme, Fran\\c{c}ois Laviolette, and Benoit Gosselin", "title": "A Transferable Adaptive Domain Adversarial Neural Network for Virtual\n  Reality Augmented EMG-Based Gesture Recognition", "comments": "10 Pages. The last three authors shared senior authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the field of electromyography-based (EMG) gesture recognition,\ndisparities exist between the offline accuracy reported in the literature and\nthe real-time usability of a classifier. This gap mainly stems from two\nfactors: 1) The absence of a controller, making the data collected dissimilar\nto actual control. 2) The difficulty of including the four main dynamic factors\n(gesture intensity, limb position, electrode shift, and transient changes in\nthe signal), as including their permutations drastically increases the amount\nof data to be recorded. Contrarily, online datasets are limited to the exact\nEMG-based controller used to record them, necessitating the recording of a new\ndataset for each control method or variant to be tested. Consequently, this\npaper proposes a new type of dataset to serve as an intermediate between\noffline and online datasets, by recording the data using a real-time\nexperimental protocol. The protocol, performed in virtual reality, includes the\nfour main dynamic factors and uses an EMG-independent controller to guide\nmovements. This EMG-independent feedback ensures that the user is in-the-loop\nduring recording, while enabling the resulting dynamic dataset to be used as an\nEMG-based benchmark. The dataset is comprised of 20 able-bodied participants\ncompleting three to four sessions over a period of 14 to 21 days. The ability\nof the dynamic dataset to serve as a benchmark is leveraged to evaluate the\nimpact of different recalibration techniques for long-term (across-day) gesture\nrecognition, including a novel algorithm, named TADANN. TADANN consistently and\nsignificantly (p<0.05) outperforms using fine-tuning as the recalibration\ntechnique.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:41:56 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 14:55:11 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["C\u00f4t\u00e9-Allard", "Ulysse", ""], ["Gagnon-Turcotte", "Gabriel", ""], ["Phinyomark", "Angkoon", ""], ["Glette", "Kyrre", ""], ["Scheme", "Erik", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Gosselin", "Benoit", ""]]}, {"id": "1912.09589", "submitter": "Denis Gudovskiy", "authors": "Denis Gudovskiy, Gyuri Han, Takuya Yamaguchi, Sotaro Tsukizawa", "title": "Smart Home Appliances: Chat with Your Fridge", "comments": "NeurIPS 2019 demo track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current home appliances are capable to execute a limited number of voice\ncommands such as turning devices on or off, adjusting music volume or light\nconditions. Recent progress in machine reasoning gives an opportunity to\ndevelop new types of conversational user interfaces for home appliances. In\nthis paper, we apply state-of-the-art visual reasoning model and demonstrate\nthat it is feasible to ask a smart fridge about its contents and various\nproperties of the food with close-to-natural conversation experience. Our\nvisual reasoning model answers user questions about existence, count, category\nand freshness of each product by analyzing photos made by the image sensor\ninside the smart fridge. Users may chat with their fridge using off-the-shelf\nphone messenger while being away from home, for example, when shopping in the\nsupermarket. We generate a visually realistic synthetic dataset to train\nmachine learning reasoning model that achieves 95% answer accuracy on test\ndata. We present the results of initial user tests and discuss how we modify\ndistribution of generated questions for model training based on\nhuman-in-the-loop guidance. We open source code for the whole system including\ndataset generation, reasoning model and demonstration scripts.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 23:12:25 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Gudovskiy", "Denis", ""], ["Han", "Gyuri", ""], ["Yamaguchi", "Takuya", ""], ["Tsukizawa", "Sotaro", ""]]}, {"id": "1912.10311", "submitter": "Daniel McDuff", "authors": "Daniel McDuff and Jonah Berger", "title": "Do Facial Expressions Predict Ad Sharing? A Large-Scale Observational\n  Study", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People often share news and information with their social connections, but\nwhy do some advertisements get shared more than others? A large-scale test\nexamines whether facial responses predict sharing. Facial expressions play a\nkey role in emotional expression. Using scalable automated facial coding\nalgorithms, we quantify the facial expressions of thousands of individuals in\nresponse to hundreds of advertisements. Results suggest that not all emotions\nexpressed during viewing increase sharing, and that the relationship between\nemotion and transmission is more complex than mere valence alone. Facial\nactions linked to positive emotions (i.e., smiles) were associated with\nincreased sharing. But while some actions associated with negative emotion\n(e.g., lip depressor, associated with sadness) were linked to decreased\nsharing, others (i.e., nose wrinkles, associated with disgust) were linked to\nincreased sharing. The ability to quickly collect facial responses at scale in\npeoples' natural environment has important implications for marketers and opens\nup a range of avenues for further research.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 18:25:57 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["McDuff", "Daniel", ""], ["Berger", "Jonah", ""]]}, {"id": "1912.10528", "submitter": "Stanis{\\l}aw Saganowski", "authors": "Stanis{\\l}aw Saganowski, Anna Dutkowiak, Adam Dziadek, Maciej\n  Dzie\\.zyc, Joanna Komoszy\\'nska, Weronika Michalska, Adam Polak, Micha{\\l}\n  Ujma, Przemys{\\l}aw Kazienko", "title": "Emotion Recognition Using Wearables: A Systematic Literature Review Work\n  in progress", "comments": "6 pages, accepted to the Emotion Aware 2020 workshop. Copyright 2019\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for all other uses, in any current or future media", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearables like smartwatches or wrist bands equipped with pervasive sensors\nenable us to monitor our physiological signals. In this study, we address the\nquestion whether they can help us to recognize our emotions in our everyday\nlife for ubiquitous computing. Using the systematic literature review, we\nidentified crucial research steps and discussed the main limitations and\nproblems in the domain.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 20:37:30 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 11:22:52 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Saganowski", "Stanis\u0142aw", ""], ["Dutkowiak", "Anna", ""], ["Dziadek", "Adam", ""], ["Dzie\u017cyc", "Maciej", ""], ["Komoszy\u0144ska", "Joanna", ""], ["Michalska", "Weronika", ""], ["Polak", "Adam", ""], ["Ujma", "Micha\u0142", ""], ["Kazienko", "Przemys\u0142aw", ""]]}, {"id": "1912.10554", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi, Manajit Chakraborty, Esteban Andr\\'es R\\'issola,\n  Fabio Crestani", "title": "Harnessing Evolution of Multi-Turn Conversations for Effective Answer\n  Retrieval", "comments": "To appear in ACM CHIIR 2020, Vancouver, BC, Canada", "journal-ref": null, "doi": "10.1145/3343413.3377968", "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the improvements in speech recognition and voice generation technologies\nover the last years, a lot of companies have sought to develop conversation\nunderstanding systems that run on mobile phones or smart home devices through\nnatural language interfaces. Conversational assistants, such as Google\nAssistant and Microsoft Cortana, can help users to complete various types of\ntasks. This requires an accurate understanding of the user's information need\nas the conversation evolves into multiple turns. Finding relevant context in a\nconversation's history is challenging because of the complexity of natural\nlanguage and the evolution of a user's information need. In this work, we\npresent an extensive analysis of language, relevance, dependency of user\nutterances in a multi-turn information-seeking conversation. To this aim, we\nhave annotated relevant utterances in the conversations released by the TREC\nCaST 2019 track. The annotation labels determine which of the previous\nutterances in a conversation can be used to improve the current one.\nFurthermore, we propose a neural utterance relevance model based on BERT\nfine-tuning, outperforming competitive baselines. We study and compare the\nperformance of multiple retrieval models, utilizing different strategies to\nincorporate the user's context. The experimental results on both classification\nand retrieval tasks show that our proposed approach can effectively identify\nand incorporate the conversation context. We show that processing the current\nutterance using the predicted relevant utterance leads to a 38% relative\nimprovement in terms of nDCG@20. Finally, to foster research in this area, we\nhave released the dataset of the annotations.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 22:39:04 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 15:41:03 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Chakraborty", "Manajit", ""], ["R\u00edssola", "Esteban Andr\u00e9s", ""], ["Crestani", "Fabio", ""]]}, {"id": "1912.10628", "submitter": "EPTCS", "authors": "Jan Bessai (Technical University of Dortmund), Moritz Roidl (Technical\n  University of Dortmund), Anna Vasileva (Technical University of Dortmund)", "title": "Experience Report: Towards Moving Things with Types -- Helping Logistics\n  Domain Experts to Control Cyber-Physical Systems with Type-Based Synthesis", "comments": "In Proceedings F-IDE 2019, arXiv:1912.09611", "journal-ref": "EPTCS 310, 2019, pp. 1-6", "doi": "10.4204/EPTCS.310.1", "report-no": null, "categories": "cs.HC cs.FL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the ultimate goals of software engineering is to leave virtual spaces\nand move real things. We take one step toward supporting users with this goal\nby connecting a type-based synthesis algorithm, (CL)S, and its IDE to a\nlogistics lab environment. The environment is built and used by domain experts,\nwho have little or no training in formal methods, and need to cope with large\nspaces of software, hardware and problem specific solution variability. It\nconsists of a number of Cyber-Physical Systems (CPS), including wheel-driven\nrobots as well as flying drones, and it has laser-based support to visualize\ntheir possible movements. Our work describes results on an experiment\nintegrating the latter with (CL)S. Possibilities and challenges of working in\nthe domain of logistics and in cooperation with its experts are outlined.\nFuture research plans are presented and an invitation is made to join the\neffort of building better, formally understood, development tools for\nCPS-enabled industrial environments.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 05:39:44 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Bessai", "Jan", "", "Technical University of Dortmund"], ["Roidl", "Moritz", "", "Technical\n  University of Dortmund"], ["Vasileva", "Anna", "", "Technical University of Dortmund"]]}, {"id": "1912.10635", "submitter": "EPTCS", "authors": "Eduard Kamburjan (Technische Universit\\\"at Darmstadt, Germany), Jonas\n  Stromberg (Technische Universit\\\"at Darmstadt, Germany)", "title": "Tool Support for Validation of Formal System Models: Interactive\n  Visualization and Requirements Traceability", "comments": "In Proceedings F-IDE 2019, arXiv:1912.09611", "journal-ref": "EPTCS 310, 2019, pp. 70-85", "doi": "10.4204/EPTCS.310.8", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development processes in various engineering disciplines are incorporating\nformal models to ensure safety properties of critical systems. The use of these\nformal models requires to reason about their adequacy, i.e., to validate that a\nmodel mirrors the structure of the system sufficiently that properties\nestablished for the model indeed carry over to the real system. Model\nvalidation itself is non-formal, as adequacy is not a formal (i.e.,\nmathematical) property. Instead it must be carried out by the modeler to\njustify the modeling to the certification agency or other stakeholders. In this\npaper we argue that model validation can be seen as a special form of\nrequirements engineering, and that interactive visualization and concepts from\nrequirements traceability can help to advance tool support for formal modeling\nby lowering the cognitive burden needed for validation. We present the\nVisualisierbaR tool, which supports the formal modeling of railway operations\nand describe how it uses interactive visualization and requirements\ntraceability concepts to validate a formal model.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 05:42:08 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kamburjan", "Eduard", "", "Technische Universit\u00e4t Darmstadt, Germany"], ["Stromberg", "Jonas", "", "Technische Universit\u00e4t Darmstadt, Germany"]]}, {"id": "1912.10637", "submitter": "Xiaowei Hu", "authors": "Xiao Tang, Xiaowei Hu, Chi-Wing Fu, Daniel Cohen-Or", "title": "GrabAR: Occlusion-aware Grabbing Virtual Objects in AR", "comments": "conditionally accepted to UIST 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing augmented reality (AR) applications often ignore occlusion between\nreal hands and virtual objects when incorporating virtual objects in our views.\nThe challenges come from the lack of accurate depth and mismatch between real\nand virtual depth. This paper presents GrabAR, a new approach that directly\npredicts the real-and-virtual occlusion, and bypasses the depth acquisition and\ninference. Our goal is to enhance AR applications with interactions between\nhand (real) and grabbable objects (virtual). With paired images of hand and\nobject as inputs, we formulate a neural network that learns to generate the\nocclusion mask. To train the network, we compile a synthetic dataset to\npre-train it and a real dataset to fine-tune it, thus reducing the burden of\nmanual labels and addressing the domain difference. Then, we embed the trained\nnetwork in a prototyping AR system that supports hand grabbing of various\nvirtual objects, demonstrate the system performance, both quantitatively and\nqualitatively, and showcase interaction scenarios, in which we can use bare\nhand to grab virtual objects and directly manipulate them.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 05:47:21 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 13:26:35 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 05:37:34 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tang", "Xiao", ""], ["Hu", "Xiaowei", ""], ["Fu", "Chi-Wing", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1912.10781", "submitter": "Radek O\\v{s}lej\\v{s}ek", "authors": "Radek O\\v{s}lej\\v{s}ek and V\\'it Rus\\v{n}\\'ak and Karol\\'ina Bursk\\'a\n  and Valdemar \\v{S}v\\'abensk\\'y and Jan Vykopal", "title": "Visual Feedback for Players of Multi-Level Capture the Flag Games: Field\n  Usability Study", "comments": "11 pages", "journal-ref": null, "doi": "10.1109/VizSec48167.2019.9161386", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capture the Flag games represent a popular method of cybersecurity training.\nProviding meaningful insight into the training progress is essential for\nincreasing learning impact and supporting participants' motivation, especially\nin advanced hands-on courses. In this paper, we investigate how to provide\nvaluable post-game feedback to players of serious cybersecurity games through\ninteractive visualizations. In collaboration with domain experts, we formulated\nuser requirements that cover three cognitive perspectives: gameplay overview,\nperson-centric view, and comparative feedback. Based on these requirements, we\ndesigned two interactive visualizations that provide complementary views on\ngame results. They combine a known clustering and time-based visual approaches\nto show game results in a way that is easy to decode for players. The\npurposefulness of our visual feedback was evaluated in a usability field study\nwith attendees of the Summer School in Cyber Security. The evaluation confirmed\nthe adequacy of the two visualizations for instant post-game feedback. Despite\nour initial expectations, there was no strong preference for neither of the\nvisualizations in solving different tasks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 13:09:07 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 11:32:56 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 14:06:35 GMT"}, {"version": "v4", "created": "Thu, 27 Aug 2020 20:52:03 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["O\u0161lej\u0161ek", "Radek", ""], ["Rus\u0148\u00e1k", "V\u00edt", ""], ["Bursk\u00e1", "Karol\u00edna", ""], ["\u0160v\u00e1bensk\u00fd", "Valdemar", ""], ["Vykopal", "Jan", ""]]}, {"id": "1912.11037", "submitter": "Ulysse C\\^ot\\'e-Allard", "authors": "Ulysse C\\^ot\\'e-Allard, Gabriel Gagnon-Turcotte, Angkoon Phinyomark,\n  Kyrre Glette, Erik Scheme, Fran\\c{c}ois Laviolette and Benoit Gosselin", "title": "Unsupervised Domain Adversarial Self-Calibration for\n  Electromyographic-based Gesture Recognition", "comments": "12 pages + 2 pages appendices. The last three authors shared senior\n  authorship", "journal-ref": "in IEEE Access, vol. 8, pp. 177941-177955, 2020", "doi": "10.1109/ACCESS.2020.3027497", "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface electromyography (sEMG) provides an intuitive and non-invasive\ninterface from which to control machines. However, preserving the myoelectric\ncontrol system's performance over multiple days is challenging, due to the\ntransient nature of the signals obtained with this recording technique. In\npractice, if the system is to remain usable, a time-consuming and periodic\nrecalibration is necessary. In the case where the sEMG interface is employed\nevery few days, the user might need to do this recalibration before every use.\nThus, severely limiting the practicality of such a control method.\nConsequently, this paper proposes tackling the especially challenging task of\nunsupervised adaptation of sEMG signals, when multiple days have elapsed\nbetween each recording, by introducing Self-Calibrating Asynchronous Domain\nAdversarial Neural Network (SCADANN). SCADANN is compared with two\nstate-of-the-art self-calibrating algorithms developed specifically for deep\nlearning within the context of EMG-based gesture recognition and three\nstate-of-the-art domain adversarial algorithms. The comparison is made both on\nan offline and a dynamic dataset (20 participants per dataset), using two\ndifferent deep network architectures with two different input modalities\n(temporal-spatial descriptors and spectrograms). Overall, SCADANN is shown to\nsubstantially and systematically improves classification performances over no\nrecalibration and obtains the highest average accuracy for all tested cases\nacross all methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 17:42:26 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 10:42:32 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2020 11:23:49 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 14:29:07 GMT"}, {"version": "v5", "created": "Fri, 9 Oct 2020 15:09:27 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["C\u00f4t\u00e9-Allard", "Ulysse", ""], ["Gagnon-Turcotte", "Gabriel", ""], ["Phinyomark", "Angkoon", ""], ["Glette", "Kyrre", ""], ["Scheme", "Erik", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Gosselin", "Benoit", ""]]}, {"id": "1912.11371", "submitter": "Amirmohammad Mijani", "authors": "S.A. Karimi, A.M.Mijani, M.T. Talebian and S. Mirzakuchaki", "title": "Comparison of the P300 detection accuracy related to the BCI speller and\n  image recognition scenarios", "comments": "8 pages, 3 figures, 2 tables, 24 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several protocols in the Electroencephalography (EEG) recording\nscenarios which produce various types of event-related potentials (ERP). P300\npattern is a well-known ERP which produced by auditory and visual oddball\nparadigm and BCI speller system. In this study, P300 and non-P300 separability\nare investigated in two scenarios including image recognition paradigm and BCI\nspeller. Image recognition scenario is an experiment that examines the\nparticipants, knowledge about an image that shown to them before by analyzing\nthe EEG signal recorded during the observing of that image as visual\nstimulation. To do this, three types of famous classifiers (SVM, Bayes LDA, and\nsparse logistic regression) were used to classify EEG recordings in six classes\nproblem. Filtered and down-sampled (temporal samples) of EEG recording were\nconsidered as features in classification P300 pattern. Also, different sets of\nEEG recording including 4, 8 and 16 channels and different trial numbers were\nused to considering various situations in comparison. The accuracy was\nincreased by increasing the number of trials and channels. The results prove\nthat better accuracy is observed in the case of the image recognition scenario\nfor the different sets of channels and by using the different number of trials.\nSo it can be concluded that P300 pattern which produced in image recognition\nparadigm is more separable than BCI (matrix speller).\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 14:04:24 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Karimi", "S. A.", ""], ["Mijani", "A. M.", ""], ["Talebian", "M. T.", ""], ["Mirzakuchaki", "S.", ""]]}, {"id": "1912.11474", "submitter": "Changan Chen", "authors": "Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vicenc Amengual\n  Gari, Ziad Al-Halah, Vamsi Krishna Ithapu, Philip Robinson, Kristen Grauman", "title": "SoundSpaces: Audio-Visual Navigation in 3D Environments", "comments": "Accepted to ECCV 2020 (Spotlight). Project page:\n  http://vision.cs.utexas.edu/projects/audio_visual_navigation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving around in the world is naturally a multisensory experience, but\ntoday's embodied agents are deaf---restricted to solely their visual perception\nof the environment. We introduce audio-visual navigation for complex,\nacoustically and visually realistic 3D environments. By both seeing and\nhearing, the agent must learn to navigate to a sounding object. We propose a\nmulti-modal deep reinforcement learning approach to train navigation policies\nend-to-end from a stream of egocentric audio-visual observations, allowing the\nagent to (1) discover elements of the geometry of the physical space indicated\nby the reverberating audio and (2) detect and follow sound-emitting targets. We\nfurther introduce SoundSpaces: a first-of-its-kind dataset of audio renderings\nbased on geometrical acoustic simulations for two sets of publicly available 3D\nenvironments (Matterport3D and Replica), and we instrument Habitat to support\nthe new sensor, making it possible to insert arbitrary sound sources in an\narray of real-world scanned environments. Our results show that audio greatly\nbenefits embodied visual navigation in 3D spaces, and our work lays groundwork\nfor new research in embodied AI with audio-visual perception.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 18:59:50 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 17:59:47 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 18:00:31 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chen", "Changan", ""], ["Jain", "Unnat", ""], ["Schissler", "Carl", ""], ["Gari", "Sebastia Vicenc Amengual", ""], ["Al-Halah", "Ziad", ""], ["Ithapu", "Vamsi Krishna", ""], ["Robinson", "Philip", ""], ["Grauman", "Kristen", ""]]}, {"id": "1912.11729", "submitter": "Ippei Suzuki", "authors": "Satoshi Hashizume, Ippei Suzuki, Kazuki Takazawa, Yoichi Ochiai", "title": "Discussion of Intelligent Electric Wheelchairs for Caregivers and Care\n  Recipients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to reduce the burden on caregivers, we developed an intelligent\nelectric wheelchair. We held workshops with caregivers, asked then regarding\nthe problems in caregiving, and developed problem-solving methods. In the\nworkshop, caregivers' physical fitness and psychology of the older adults were\nfound to be problems and a solution was proposed. We implemented a cooperative\noperation function for multiple electric wheelchairs based on the workshop and\ndemonstrated it at a nursing home. By listening to older adults, we obtained\nfeedback on the automatic driving electric wheelchair. From the results of this\nstudy, we discovered the issues and solutions to be applied to the intelligent\nelectric wheelchair.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 00:04:45 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 01:35:59 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Hashizume", "Satoshi", ""], ["Suzuki", "Ippei", ""], ["Takazawa", "Kazuki", ""], ["Ochiai", "Yoichi", ""]]}, {"id": "1912.11936", "submitter": "Yen-Chia Hsu", "authors": "Yen-Chia Hsu, Jennifer Cross, Paul Dille, Michael Tasota, Beatrice\n  Dias, Randy Sargent, Ting-Hao 'Kenneth' Huang, Illah Nourbakhsh", "title": "Smell Pittsburgh: Engaging Community Citizen Science for Air Quality", "comments": "Accepted by ACM Transactions on Interactive Intelligent Systems on\n  2020. This is an extended version of the arXiv:1810.11143, which was accepted\n  by the ACM IUI 2019 conference. arXiv admin note: substantial text overlap\n  with arXiv:1810.11143", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban air pollution has been linked to various human health concerns,\nincluding cardiopulmonary diseases. Communities who suffer from poor air\nquality often rely on experts to identify pollution sources due to the lack of\naccessible tools. Taking this into account, we developed Smell Pittsburgh, a\nsystem that enables community members to report odors and track where these\nodors are frequently concentrated. All smell report data are publicly\naccessible online. These reports are also sent to the local health department\nand visualized on a map along with air quality data from monitoring stations.\nThis visualization provides a comprehensive overview of the local pollution\nlandscape. Additionally, with these reports and air quality data, we developed\na model to predict upcoming smell events and send push notifications to inform\ncommunities. We also applied regression analysis to identify statistically\nsignificant effects of push notifications on user engagement. Our evaluation of\nthis system demonstrates that engaging residents in documenting their\nexperiences with pollution odors can help identify local air pollution\npatterns, and can empower communities to advocate for better air quality. All\ncitizen-contributed smell data are publicly accessible and can be downloaded\nfrom https://smellpgh.org.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 21:31:39 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 08:04:58 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 18:03:59 GMT"}, {"version": "v4", "created": "Fri, 20 Nov 2020 23:40:31 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Hsu", "Yen-Chia", ""], ["Cross", "Jennifer", ""], ["Dille", "Paul", ""], ["Tasota", "Michael", ""], ["Dias", "Beatrice", ""], ["Sargent", "Randy", ""], ["Huang", "Ting-Hao 'Kenneth'", ""], ["Nourbakhsh", "Illah", ""]]}, {"id": "1912.12526", "submitter": "Dohyun Kim", "authors": "Dohyun Kim, Joshua Gluck, Malcolm Hall, and Yuvraj Agarwal", "title": "Real World Longitudinal iOS App Usage Study at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the importance of understanding the interaction between mobile devices\nand their users, app usage patterns have been studied in various contexts.\nHowever, prior work has not fully investigated longitudinal changes to app\nusage behavior. In this paper, we present a longitudinal, large-scale study of\nmobile app usage based on a dataset collected from 162,006 iPhones and iPads\nover 4 years. We explore multiple dimensions of app usage pattern proving\nuseful insights on how app usage changes over time. Our key findings include\n(i) app usage pattern changes over time both at the individual app level and\nthe app category level (i.e. proportion of time a user spends using an app),\n(ii) users keep a small set of apps frequently launched (90% of iPhone users\nlaunch roughly 14-18 apps weekly), (iii) a small number of apps remain popular\nwhile some specific kinds of apps (e.g. Games) have a shorter life cycle\ncompared to other apps of different categories. Finally, we discuss our\nfindings and their implications, for example, a short-term study as an attempt\nto understand the general needs of mobile devices may not achieve useful\nresults for the long term.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 21:42:09 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Kim", "Dohyun", ""], ["Gluck", "Joshua", ""], ["Hall", "Malcolm", ""], ["Agarwal", "Yuvraj", ""]]}, {"id": "1912.12652", "submitter": "Supriya Sarker", "authors": "Supriya Sarker, Md. Shahraduan Mazumder, Md. Sajedur Rahman, Md. Anayt\n  Rabbi", "title": "An assistive HCI system based on block scanning objects using eye blinks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human-Computer Interaction (HCI) provides a new communication channel between\nhuman and the computer. We develop an assistive system based on block scanning\ntechniques using eye blinks that presents a hands-free interface between human\nand computer for people with motor impairments. The developed system has been\ntested by 12 users who performed 10 common in computer tasks using eye blinks\nwith scanning time 1.0 second. The performance of the proposed system has been\nevaluated by selection time, selection accuracy, false alarm rate and average\nsuccess rate. The success rate has found 98.1%.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 13:52:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Sarker", "Supriya", ""], ["Mazumder", "Md. Shahraduan", ""], ["Rahman", "Md. Sajedur", ""], ["Rabbi", "Md. Anayt", ""]]}, {"id": "1912.12712", "submitter": "Ludovic Saint-Bauzel", "authors": "Giovanni Pezzulo, Lucas Roche, Ludovic Saint-Bauzel", "title": "Haptic communication optimises joint decisions and affords implicit\n  confidence sharing", "comments": "13 pages, 3 figures, preprint", "journal-ref": "Scientific Reports (2021) Vol. 11", "doi": "10.1038/s41598-020-80041-6", "report-no": null, "categories": "cs.RO cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group decisions can outperform the choices of the best individual group\nmembers. Previous research suggested that optimal group decisions require\nindividuals to communicate explicitly (e.g., verbally) their confidence levels.\nOur study addresses the untested hypothesis that implicit communication using a\nsensorimotor channel -- haptic coupling -- may afford optimal group decisions,\ntoo. We report that haptically coupled dyads solve a perceptual discrimination\ntask more accurately than their best individual members; and five times faster\nthan dyads using explicit communication. Furthermore, our computational\nanalyses indicate that the haptic channel affords implicit confidence sharing.\nWe found that dyads take leadership over the choice and communicate their\nconfidence in it by modulating both the timing and the force of their\nmovements. Our findings may pave the way to negotiation technologies using fast\nsensorimotor communication to solve problems in groups.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 19:19:46 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 12:40:25 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Pezzulo", "Giovanni", ""], ["Roche", "Lucas", ""], ["Saint-Bauzel", "Ludovic", ""]]}, {"id": "1912.12914", "submitter": "Sonia Jawaid Shaikh", "authors": "Sonia Jawaid Shaikh and Ignacio Cruz", "title": "'Alexa, Do You Know Anything?' The Impact of an Intelligent Assistant on\n  Team Interactions and Creative Performance Under Time Scarcity", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-AI collaboration is on the rise with the deployment of AI-enabled\nintelligent assistants (e.g. Amazon Echo, Cortana, Siri, etc.) across\norganizational contexts. It is claimed that intelligent assistants can help\npeople achieve more in less time (Personal Digital Assistant - Cortana, n.d.).\nHowever, despite the increasing presence of intelligent assistants in\ncollaborative settings, there is a void in the literature on how the deployment\nof this technology intersects with time scarcity to impact team behaviors and\nperformance. To fill this gap in the literature, we collected behavioral data\nfrom 56 teams who participated in a between-subjects 2 (Intelligent Assistant:\nAvailable vs. Not Available) x 2 (Time: Scarce vs. Not Scarce/Control) lab\nexperiment. The results show that teams with an intelligent assistant had\nsignificantly fewer interactions between its members compared to teams without\nan intelligent assistant. Teams who faced time scarcity also used the\nintelligent assistant more often to seek its assistance during task completion\ncompared to those in the control condition. Lastly, teams with an intelligent\nassistant underperformed on a creative task compared to those without the\ndevice. We discuss implications of this technology from theoretical, empirical,\nand practical perspectives.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 13:08:12 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Shaikh", "Sonia Jawaid", ""], ["Cruz", "Ignacio", ""]]}, {"id": "1912.13273", "submitter": "Albrecht Kurze", "authors": "Albrecht Kurze, Arne Berger, Teresa Denefleh", "title": "From Ideation to Implications: Directions for the Internet of Things in\n  the Home", "comments": "Proceedings of the CHI 2019 Workshop on New Directions for the IoT:\n  Automate, Share, Build, and Care, (arXiv:1906.06089)", "journal-ref": null, "doi": null, "report-no": "IOTD/2019/05", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give a brief overview of our approaches and ongoing work for\nfuture directions of the Internet of Things (IoT) with a focus on the IoT in\nthe home. We highlight some of our activities including tools and methods for\nan ideation-driven approach as well as for an implications-driven approach. We\npoint to some findings of workshops and empirical field-studies. We show\nexamples for new classes of idiosyncratic IoT devices, how implications emerge\nby (mis)using sensor data and how users interacted with IoT systems in shared\nspaces.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 11:30:04 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Kurze", "Albrecht", ""], ["Berger", "Arne", ""], ["Denefleh", "Teresa", ""]]}]