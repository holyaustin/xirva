[{"id": "2103.00037", "submitter": "Nicolas LaLone", "authors": "John Dunham, Konstantinos Papangelis, Nicolas LaLone, Yihong Wang", "title": "Casual and Hardcore Player Traits and Gratifications of Pok\\'emon GO,\n  Harry Potter: Wizards Unite, Ingress", "comments": "39 Tables, 9 Figures, TOCHI Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Location-based games (LBG) impose virtual spaces on top of physical\nlocations. Studies have explored LBG from various perspectives. However, a\ncomprehensive study of who these players are, their traits, their\ngratifications, and the links between them is conspicuously absent from the\nliterature. In this paper, we aim to address this lacuna through a series of\nsurveys with 2390 active LBG players utilizing Tondello's Player Traits Model\nand Scale of Game playing Preferences, and Hamari's scale of LBG\ngratifications. Our findings (1) illustrate an association between player\nsatisfaction and social aspects of the studied games, (2) explicate how the\ncore-loops of the studied games impact the expressed gratifications and the\naffine traits of players, and (3) indicate a strong distinction between\nhardcore and casual players based on both traits and gratifications. Overall\nour findings shed light into the players of LBG, their traits, and\ngratifications they derive from playing LBGs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 20:09:08 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 02:55:41 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Dunham", "John", ""], ["Papangelis", "Konstantinos", ""], ["LaLone", "Nicolas", ""], ["Wang", "Yihong", ""]]}, {"id": "2103.00047", "submitter": "Abhijat Biswas", "authors": "Abhijat Biswas, Allan Wang, Gustavo Silvera, Aaron Steinfeld, Henny\n  Admoni", "title": "SocNavBench: A Grounded Simulation Testing Framework for Evaluating\n  Social Navigation", "comments": "Associated code and benchmark available at\n  https://github.com/CMU-TBD/SocNavBench ; Accepted to ACM Transactions on\n  Human-Robot Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human-robot interaction (HRI) community has developed many methods for\nrobots to navigate safely and socially alongside humans. However, experimental\nprocedures to evaluate these works are usually constructed on a per-method\nbasis. Such disparate evaluations make it difficult to compare the performance\nof such methods across the literature. To bridge this gap, we introduce\nSocNavBench, a simulation framework for evaluating social navigation\nalgorithms. SocNavBench comprises a simulator with photo-realistic capabilities\nand curated social navigation scenarios grounded in real-world pedestrian data.\nWe also provide an implementation of a suite of metrics to quantify the\nperformance of navigation algorithms on these scenarios. Altogether,\nSocNavBench provides a test framework for evaluating disparate social\nnavigation methods in a consistent and interpretable manner. To illustrate its\nuse, we demonstrate testing three existing social navigation methods and a\nbaseline method on SocNavBench, showing how the suite of metrics helps infer\ntheir performance trade-offs. Our code is open-source, allowing the addition of\nnew scenarios and metrics by the community to help evolve SocNavBench to\nreflect advancements in our understanding of social navigation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 20:40:25 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 05:33:32 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Biswas", "Abhijat", ""], ["Wang", "Allan", ""], ["Silvera", "Gustavo", ""], ["Steinfeld", "Aaron", ""], ["Admoni", "Henny", ""]]}, {"id": "2103.00064", "submitter": "Austin Hounsel", "authors": "J. Nathan Matias, Austin Hounsel, Nick Feamster", "title": "Software-Supported Audits of Decision-Making Systems: Testing Google and\n  Facebook's Political Advertising Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can society understand and hold accountable complex human and algorithmic\ndecision-making systems whose systematic errors are opaque to the outside?\nThese systems routinely make decisions on individual rights and well-being, and\non protecting society and the democratic process. Practical and statistical\nconstraints on external audits can lead researchers to miss important sources\nof error in these complex decision-making systems. In this paper, we design and\nimplement a software-supported approach to audit studies that auto-generates\naudit materials and coordinates volunteer activity. We implemented this\nsoftware in the case of political advertising policies enacted by Facebook and\nGoogle during the 2018 U.S. election. Guided by this software, a team of\nvolunteers posted 477 auto-generated ads and analyzed the companies' actions,\nfinding systematic errors in how companies enforced policies. We find that\nsoftware can overcome some common constraints of audit studies, within\nlimitations related to sample size and volunteer capacity.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 22:04:53 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Matias", "J. Nathan", ""], ["Hounsel", "Austin", ""], ["Feamster", "Nick", ""]]}, {"id": "2103.00097", "submitter": "Parth Sane", "authors": "Parth Sane", "title": "A Brief Survey of Current Software Engineering Practices in Continuous\n  Integration and Automated Accessibility Testing", "comments": "IEEE Conference WiSPNET 2021 Accepted Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It's long been accepted that continuous integration (CI) in software\nengineering increases the code quality of enterprise projects when adhered to\nby it's practitioners. But is any of that effort to increase code quality and\nvelocity directed towards improving software accessibility accommodations? What\nare the potential benefits quoted in literature? Does it fit with the modern\nagile way that teams operate in most enterprises? This paper attempts to map\nthe current scene of the software engineering effort spent on improving\naccessibility via continuous integration and it's hurdles to adoption as quoted\nby researchers. We also try to explore steps that agile teams may take to train\nmembers on how to implement accessibility testing and introduce key diagrams to\nvisualize processes to implement CI based accessibility testing procedures in\nthe software development lifecycle.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 01:13:43 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sane", "Parth", ""]]}, {"id": "2103.00127", "submitter": "Swaroop Panda", "authors": "Swaroop Panda, V. Namboodiri, S.T. Roy", "title": "Visualizing Music Genres using a Topic Model", "comments": "A version of this paper was published at the Sound and Music\n  Computing Conference 2019, Malaga", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music Genres serve as an important meta-data in the field of music\ninformation retrieval and have been widely used for music classification and\nanalysis tasks. Visualizing these music genres can thus be helpful for music\nexploration, archival and recommendation. Probabilistic topic models have been\nvery successful in modelling text documents. In this work, we visualize music\ngenres using a probabilistic topic model. Unlike text documents, audio is\ncontinuous and needs to be sliced into smaller segments. We use simple MFCC\nfeatures of these segments as musical words. We apply the topic model on the\ncorpus and subsequently use the genre annotations of the data to interpret and\nvisualize the latent space.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 04:46:36 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Panda", "Swaroop", ""], ["Namboodiri", "V.", ""], ["Roy", "S. T.", ""]]}, {"id": "2103.00129", "submitter": "Swaroop Panda", "authors": "Swaroop Panda, S.T.Roy", "title": "Music Genre Bars", "comments": "A version of this paper was presented at IEEE VIS 2020", "journal-ref": "IEEE VIS 2020 posters", "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music Genres, as a popular meta-data of music, are very useful to organize,\nexplore or search music datasets. Soft music genres are weighted multiple-genre\nannotations to songs. In this initial work, we propose horizontally stacked bar\ncharts to represent a music dataset annotated by these soft music genres. For\nthis purpose, we take an example of a toy dataset consisting of songs labelled\nwith help of three music genres; Blues, Jazz and Country. We demonstrate how\nsuch a stacked bar chart can be used as a slider for user-input in an\ninterface. We implement this by embedding this genre bar in a streaming\napplication prototype and show its utility in choosing playlists. We finally\nconclude by proposing further work and future explorations on our proposed\npreliminary research.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 05:05:12 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Panda", "Swaroop", ""], ["Roy", "S. T.", ""]]}, {"id": "2103.00268", "submitter": "Naoki Wake", "authors": "Naoki Wake, Daichi Saito, Kazuhiro Sasabuchi, Hideki Koike, Katsushi\n  Ikeuchi", "title": "Object affordance as a guide for grasp-type recognition", "comments": "12 pages, 11 figures. Last updated February 27th, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing human grasping strategies is an important factor in robot\nteaching as these strategies contain the implicit knowledge necessary to\nperform a series of manipulations smoothly. This study analyzed the effects of\nobject affordance-a prior distribution of grasp types for each object-on\nconvolutional neural network (CNN)-based grasp-type recognition. To this end,\nwe created datasets of first-person grasping-hand images labeled with grasp\ntypes and object names, and tested a recognition pipeline leveraging object\naffordance. We evaluated scenarios with real and illusory objects to be\ngrasped, to consider a teaching condition in mixed reality where the lack of\nvisual object information can make the CNN recognition challenging. The results\nshow that object affordance guided the CNN in both scenarios, increasing the\naccuracy by 1) excluding unlikely grasp types from the candidates and 2)\nenhancing likely grasp types. In addition, the \"enhancing effect\" was more\npronounced with high degrees of grasp-type heterogeneity. These results\nindicate the effectiveness of object affordance for guiding grasp-type\nrecognition in robot teaching applications.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 17:03:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wake", "Naoki", ""], ["Saito", "Daichi", ""], ["Sasabuchi", "Kazuhiro", ""], ["Koike", "Hideki", ""], ["Ikeuchi", "Katsushi", ""]]}, {"id": "2103.00370", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Scott Lundberg, Lei Zhang, Stephanie Fu, William T.\n  Freeman", "title": "Model-Agnostic Explainability for Visual Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  What makes two images similar? We propose new approaches to generate\nmodel-agnostic explanations for image similarity, search, and retrieval. In\nparticular, we extend Class Activation Maps (CAMs), Additive Shapley\nExplanations (SHAP), and Locally Interpretable Model-Agnostic Explanations\n(LIME) to the domain of image retrieval and search. These approaches enable\nblack and grey-box model introspection and can help diagnose errors and\nunderstand the rationale behind a model's similarity judgments. Furthermore, we\nextend these approaches to extract a full pairwise correspondence between the\nquery and retrieved image pixels, an approach we call \"joint interpretations\".\nFormally, we show joint search interpretations arise from projecting Harsanyi\ndividends, and that this approach generalizes Shapley Values and The\nShapley-Taylor indices. We introduce a fast kernel-based method for estimating\nShapley-Taylor indices and empirically show that these game-theoretic measures\nyield more consistent explanations for image similarity architectures.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 01:24:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hamilton", "Mark", ""], ["Lundberg", "Scott", ""], ["Zhang", "Lei", ""], ["Fu", "Stephanie", ""], ["Freeman", "William T.", ""]]}, {"id": "2103.00474", "submitter": "Jason R.C. Nurse Dr", "authors": "Jason R. C. Nurse", "title": "Cybersecurity Awareness", "comments": null, "journal-ref": "In: Jajodia S., Samarati P., Yung M. (eds) Encyclopedia of\n  Cryptography, Security and Privacy. Springer, Berlin, Heidelberg (2021)", "doi": "10.1007/978-3-642-27739-9_1596-1", "report-no": null, "categories": "cs.CR cs.CY cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybersecurity awareness can be viewed as the level of appreciation,\nunderstanding or knowledge of cybersecurity or information security aspects.\nSuch aspects include cognizance of cyber risks and threats, but also\nappropriate protection measures.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 11:54:58 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Nurse", "Jason R. C.", ""]]}, {"id": "2103.00536", "submitter": "Tanishq Chaudhary", "authors": "Tanishq Chaudhary, Mayank Goel, Radhika Mamidi", "title": "Towards Conversational Humor Analysis and Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Well-defined jokes can be divided neatly into a setup and a punchline. While\nmost works on humor today talk about a joke as a whole, the idea of generating\npunchlines to a setup has applications in conversational humor, where funny\nremarks usually occur with a non-funny context. Thus, this paper is based\naround two core concepts: Classification and the Generation of a punchline from\na particular setup based on the Incongruity Theory. We first implement a\nfeature-based machine learning model to classify humor. For humor generation,\nwe use a neural model, and then merge the classical rule-based approaches with\nthe neural approach to create a hybrid model. The idea behind being: combining\ninsights gained from other tasks with the setup-punchline model and thus\napplying it to existing text generation approaches. We then use and compare our\nmodel with human written jokes with the help of human evaluators in a\ndouble-blind study.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 15:22:57 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chaudhary", "Tanishq", ""], ["Goel", "Mayank", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2103.00616", "submitter": "Vignesh Prasad", "authors": "Vignesh Prasad, Ruth Stock-Homburg, Jan Peters", "title": "Learning Human-like Hand Reaching for Human-Robot Handshaking", "comments": "Accepted in ICRA'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the first and foremost non-verbal interactions that humans perform is\na handshake. It has an impact on first impressions as touch can convey complex\nemotions. This makes handshaking an important skill for the repertoire of a\nsocial robot. In this paper, we present a novel framework for learning reaching\nbehaviours for human-robot handshaking behaviours for humanoid robots solely\nusing third-person human-human interaction data. This is especially useful for\nnon-backdrivable robots that cannot be taught by demonstrations via kinesthetic\nteaching. Our approach can be easily executed on different humanoid robots.\nThis removes the need for re-training, which is especially tedious when\ntraining with human-interaction partners. We show this by applying the learnt\nbehaviours on two different humanoid robots with similar degrees of freedom but\ndifferent shapes and control limits.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 20:52:28 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 19:17:55 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Prasad", "Vignesh", ""], ["Stock-Homburg", "Ruth", ""], ["Peters", "Jan", ""]]}, {"id": "2103.00741", "submitter": "Linping Yuan", "authors": "Lin-Ping Yuan, Wei Zeng, Siwei Fu, Zhiliang Zeng, Haotian Li, Chi-Wing\n  Fu, Huamin Qu", "title": "Deep Colormap Extraction from Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work presents a new approach based on deep learning to automatically\nextract colormaps from visualizations. After summarizing colors in an input\nvisualization image as a Lab color histogram, we pass the histogram to a\npre-trained deep neural network, which learns to predict the colormap that\nproduces the visualization. To train the network, we create a new dataset of\n64K visualizations that cover a wide variety of data distributions, chart\ntypes, and colormaps. The network adopts an atrous spatial pyramid pooling\nmodule to capture color features at multiple scales in the input color\nhistograms. We then classify the predicted colormap as discrete or continuous\nand refine the predicted colormap based on its color histogram. Quantitative\ncomparisons to existing methods show the superior performance of our approach\non both synthetic and real-world visualizations. We further demonstrate the\nutility of our method with two use cases,i.e., color transfer and color\nremapping.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 04:15:37 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yuan", "Lin-Ping", ""], ["Zeng", "Wei", ""], ["Fu", "Siwei", ""], ["Zeng", "Zhiliang", ""], ["Li", "Haotian", ""], ["Fu", "Chi-Wing", ""], ["Qu", "Huamin", ""]]}, {"id": "2103.00912", "submitter": "Hai Dang", "authors": "Hai Dang, Daniel Buschek", "title": "GestureMap: Supporting Visual Analytics and Quantitative Analysis of\n  Motion Elicitation Data by Learning 2D Embeddings", "comments": "12 pages, 6 figures, 1 table, ACM CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445765", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents GestureMap, a visual analytics tool for gesture\nelicitation which directly visualises the space of gestures. Concretely, a\nVariational Autoencoder embeds gestures recorded as 3D skeletons on an\ninteractive 2D map. GestureMap further integrates three computational\ncapabilities to connect exploration to quantitative measures: Leveraging DTW\nBarycenter Averaging (DBA), we compute average gestures to 1) represent gesture\ngroups at a glance; 2) compute a new consensus measure (variance around average\ngesture); and 3) cluster gestures with k-means. We evaluate GestureMap and its\nconcepts with eight experts and an in-depth analysis of published data. Our\nfindings show how GestureMap facilitates exploring large datasets and helps\nresearchers to gain a visual understanding of elicited gesture spaces. It\nfurther opens new directions, such as comparing elicitations across studies. We\ndiscuss implications for elicitation studies and research, and opportunities to\nextend our approach to additional tasks in gesture elicitation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 11:00:23 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Dang", "Hai", ""], ["Buschek", "Daniel", ""]]}, {"id": "2103.00923", "submitter": "Sarah Janboecke", "authors": "Sarah Janboecke and Susanne Zajitschek", "title": "Anticipation Next -- System-sensitive technology development and\n  integration in work contexts", "comments": null, "journal-ref": "Information 2021, 12, 269", "doi": "10.3390/info12070269", "report-no": null, "categories": "cs.HC cs.AI cs.CY cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When discussing future concerns within socio-technical systems in work\ncontexts, we often find descriptions of missed technology development and\nintegration. The experience of technology that fails whilst being integrated is\noften rooted in dysfunctional epistemological approaches within the research\nand development process. Thus, ultimately leading to sustainable\ntechnology-distrust in work contexts. This is true for organizations that\nintegrate new technologies and for organizations that invent them.\nOrganizations in which we find failed technology development and integrations\nare, in their very nature, social systems. Nowadays, those complex social\nsystems act within an even more complex environment. This urges the development\nof new anticipation methods for technology development and integration.\nGathering of and dealing with complex information in the described context is\nwhat we call Anticipation Next. This explorative work uses existing literature\nfrom the adjoining research fields of system theory, organizational theory, and\nsocio-technical research to combine various concepts. We deliberately aim at a\nnetworked way of thinking in scientific contexts and thus combine\nmultidisciplinary subject areas in one paper to present an innovative way to\ndeal with multi-faceted problems in a human-centred way. We end with suggesting\na conceptual framework that should be used in the very early stages of\ntechnology development and integration in work contexts.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 11:27:19 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 07:38:29 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Janboecke", "Sarah", ""], ["Zajitschek", "Susanne", ""]]}, {"id": "2103.01022", "submitter": "Jun Yuan", "authors": "Jun Yuan, Oded Nov, Enrico Bertini", "title": "Visualizing Rule Sets: Exploration and Validation of a Design Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule sets are often used in Machine Learning (ML) as a way to communicate the\nmodel logic in settings where transparency and intelligibility are necessary.\nRule sets are typically presented as a text-based list of logical statements\n(rules). Surprisingly, to date there has been limited work on exploring visual\nalternatives for presenting rules. In this paper, we explore the idea of\ndesigning alternative representations of rules, focusing on a number of visual\nfactors we believe have a positive impact on rule readability and\nunderstanding. The paper presents an initial design space for visualizing rule\nsets and a user study exploring their impact. The results show that some design\nfactors have a strong impact on how efficiently readers can process the rules\nwhile having minimal impact on accuracy. This work can help practitioners\nemploy more effective solutions when using rules as a communication strategy to\nunderstand ML models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 14:19:22 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 14:57:10 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Yuan", "Jun", ""], ["Nov", "Oded", ""], ["Bertini", "Enrico", ""]]}, {"id": "2103.01078", "submitter": "Rodrigo Bonacin", "authors": "Renata de Podest\\'a Gaspar, Rodrigo Bonacin, Vin\\'icius Gon\\c{c}alves", "title": "Um Estudo sobre Atividades Participativas para Solu\\c{c}\\~oes IoT para o\n  Home care de Pessoas Idosas", "comments": "147 pages, in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population aging in Brazil and in the world occurs at the same time of\nadvances and evolutions in technology. Thus, opportunities for new solutions\narise for the elderly, such as innovations in Home Care. With the Internet of\nThings, it is possible to improve the elderly autonomy, safety and quality of\nlife. However, the design of IoT solutions for elderly Home Care poses new\nchallenges. In this context, this technical report aims to detail activities\ndeveloped as a case study to evaluate the IoT-PMHCS Method, which was developed\nin the context of the Master's program in Computer Science at UNIFACCAMP,\nBrazil. This report includes the planning and results of interviews,\nparticipatory workshops, validations, simulation of solutions, among other\nactivities. This document reports the practical experience of applying the\nIoT-PMHCS Method.\n  --\n  O envelhecimento populacional no Brasil e no mundo ocorre ao mesmo tempo que\nos avan\\c{c}os e evolu\\c{c}\\~oes na tecnologia. Desta forma, surgem\noportunidades de novas solu\\c{c}\\~oes para o p\\'ublico idoso, tais como\ninova\\c{c}\\~oes em Home Care. Com a Internet das Coisas \\'e poss\\'ivel promover\nmaior autonomia, seguran\\c{c}a e qualidade de vida aos idosos. Entretanto, o\ndesign de solu\\c{c}\\~oes de IoT para Home Care de pessoas idosas traz novos\ndesafios. Diante disto, este relat\\'orio t\\'ecnico tem o objetivo de detalhar\natividades desenvolvidas como estudo de caso para avalia\\c{c}\\~ao do M\\'etodo\nIoT-PMHCS, desenvolvido no contexto do programa de Mestrado em Ci\\^encia da\nComputa\\c{c}\\~ao da UNIFACCAMP, Brasil. O relat\\'orio inclui o planejamento e\nresultados de entrevistas, workshops participativos, pesquisas de\nvalida\\c{c}\\~ao, simula\\c{c}\\~ao de solu\\c{c}\\~oes, dentre outras atividades.\nEste documento relata a experi\\^encia pr\\'atica da aplica\\c{c}\\~ao do M\\'etodo\nIoT-PMHCS.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 15:43:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gaspar", "Renata de Podest\u00e1", ""], ["Bonacin", "Rodrigo", ""], ["Gon\u00e7alves", "Vin\u00edcius", ""]]}, {"id": "2103.01217", "submitter": "Burak Pak", "authors": "Gorsev Argin, Burak Pak, Handan Turkoglu", "title": "Between Post-Flaneur and Smartphone Zombie Smartphone Users Altering\n  Visual Attention and Walking Behavior in Public Space", "comments": null, "journal-ref": "2020 ISPRS International Journal of Geo-Information 9, 12, 700", "doi": "10.3390/ijgi9120700", "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The extensive use of smartphones in our everyday lives has created new modes\nof appropriation and behavior in public spaces. Recognition of these are\nessential for urban design and planning practices which help us to improve the\nrelationship between humans, technologies, and urban environment. This study\naims to research smartphone users in public space by observing their altering\nvisual attention and walking behavior, and, in this way, to reveal the emergent\nnew figures. For this purpose, Korenmarkt square in Ghent, Belgium, was\nobserved for seven days in 10-min time intervals. The gaze and walking behavior\nof smartphone users were encoded as geo-located and temporal data, analyzed and\nmapped using statistical and spatial analysis methods. Developing and\nimplementing new methods for identifying the characteristics of smartphone\nusers, this study resulted in a nuanced characterization of novel spatial\nappropriations. The findings led to a better understanding and knowledge of the\ndifferent behavior patterns of emergent figures such as post-flaneurs and\nsmartphone zombies while uncovering their altering visual interactions with and\nmovements in the public space. The results evoked questions on how researchers\nand designers can make use of spatial analysis methods and rethink the public\nspace of the future as a hybrid construct integrating the virtual and the\nphysical.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 14:53:45 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Argin", "Gorsev", ""], ["Pak", "Burak", ""], ["Turkoglu", "Handan", ""]]}, {"id": "2103.01347", "submitter": "Nahyun Kwon", "authors": "Nahyun Kwon, Yunjung Lee, Uran Oh", "title": "Supporting a Crowd-powered Accessible Online Art Gallery for People with\n  Visual Impairments: A Feasibility Study", "comments": "17 pages, 8 figures, UAIS, Universal Access and Information Society,\n  Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While people with visual impairments are interested in artwork as much as\ntheir sighted peers, their experience is limited to few selective artworks that\nare exhibited at certain museums. To enable people with visual impairments to\naccess and appreciate as many artworks as possible at ease, we propose an\nonline art gallery that allows users to explore different parts of a painting\ndisplayed on their touchscreen-based devices while listening to corresponding\nverbal descriptions of the touched part on the screen. To investigate the\nscalability of our approach, we first explored if anonymous crowd who may not\nhave expertise in art are capable of providing visual descriptions of artwork\nas a preliminary study. Then we conducted a user study with 9 participants with\nvisual impairments to explore the potential of our system for independent\nartwork appreciation by assessing if and how well the system supports 4 steps\nof Feldman Model of Criticism. The findings suggest that visual descriptions of\nartworks produced by an anonymous crowd are sufficient for people with visual\nimpairments to interpret and appreciate paintings with their own judgments\nwhich is different from existing approaches that focused on delivering\ndescriptions and opinions written by art experts. Based on the lessons learned\nfrom the study, we plan to collect visual descriptions of a greater number of\nartwork and distribute our online art gallery publicly to make more paintings\naccessible for people with visual impairments.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:15:12 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Kwon", "Nahyun", ""], ["Lee", "Yunjung", ""], ["Oh", "Uran", ""]]}, {"id": "2103.01371", "submitter": "Nalin Asanka Gamagedara Arachchilage", "authors": "Abdulrahman Alhazmi and Nalin Asanka Gamagedara Arachchilage", "title": "I'm all Ears! Listening to Software Developers on Putting GDPR\n  Principles into Software Development Practice", "comments": "18", "journal-ref": "Personal and Ubiquitous Computing, Springer, 2021", "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous research has been carried out to identify the impediments that\nprevent developers from incorporating privacy protocols into software\napplications. No research has been carried out to find out why developers are\nnot able to develop systems that preserve-privacy while specifically\nconsidering the General Data Protection Regulation principles (GDPR\nprinciples). Consequently, this paper aims to examine the issues, which prevent\ndevelopers from creating applications, which consider and include GDPR\nprinciples into their software systems. From our research findings, we\nidentified the lack of familiarity with GDPR principles by developers as one of\nthe obstacles that prevent GDPR onboarding. Those who were familiar with the\nprinciples did not have the requisite knowledge about the principles including\ntheir techniques. Developers focused on functional than on privacy\nrequirements. Unavailability of resourceful online tools and lack of support\nfrom institutions and clients were also identified as issues inimical to the\nonboarding of GDPR principles.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 00:12:02 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Alhazmi", "Abdulrahman", ""], ["Arachchilage", "Nalin Asanka Gamagedara", ""]]}, {"id": "2103.01518", "submitter": "Silvio Barra Dr", "authors": "Marco Grazioso, Alessandro Sebastian Podda, Silvio Barra and Francesco\n  Cutugno", "title": "Natural interaction with traffic control cameras through multimodal\n  interfaces", "comments": null, "journal-ref": "AI-HCI Conference 2021", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Computer Interfaces have always played a fundamental role in usability\nand commands' interpretability of the modern software systems. With the\nexplosion of the Artificial Intelligence concept, such interfaces have begun to\nfill the gap between the user and the system itself, further evolving in\nAdaptive User Interfaces (AUI). Meta Interfaces are a further step towards the\nuser, and they aim at supporting the human activities in an ambient interactive\nspace; in such a way, the user can control the surrounding space and interact\nwith it. This work aims at proposing a meta user interface that exploits the\nPut That There paradigm to enable the user to fast interaction by employing\nnatural language and gestures. The application scenario is a video surveillance\ncontrol room, in which the speed of actions and reactions is fundamental for\nurban safety and driver and pedestrian security. The interaction is oriented\ntowards three environments: the first is the control room itself, in which the\noperator can organize the views of the monitors related to the cameras on site\nby vocal commands and gestures, as well as conveying the audio on the headset\nor in the speakers of the room. The second one is related to the control of the\nvideo, in order to go back and forth to a particular scene showing specific\nevents, or zoom in/out a particular camera; the third allows the operator to\nsend rescue vehicle in a particular street, in case of need. The gestures data\nare acquired through a Microsoft Kinect 2 which captures pointing and gestures\nallowing the user to interact multimodally thus increasing the naturalness of\nthe interaction; the related module maps the movement information to a\nparticular instruction, also supported by vocal commands which enable its\nexecution. (cont...)\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:58:03 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Grazioso", "Marco", ""], ["Podda", "Alessandro Sebastian", ""], ["Barra", "Silvio", ""], ["Cutugno", "Francesco", ""]]}, {"id": "2103.01637", "submitter": "Alarith Uhde", "authors": "Alarith Uhde and Marc Hassenzahl", "title": "Towards a Better Understanding of Social Acceptability", "comments": "7 pages, 0 figures", "journal-ref": null, "doi": "10.1145/3411763.3451649", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social contexts play an important role in understanding acceptance and use of\ntechnology. However, current approaches used in HCI to describe contextual\ninfluence do not capture it appropriately. On the one hand, the often used\nTechnology Acceptance Model and related frameworks are too rigid to account for\nthe nuanced variations of social situations. On the other hand, Goffman's\ndramaturgical model of social interactions emphasizes interpersonal relations\nbut mostly overlooks the material (e.g., technology) that is central to HCI. As\nan alternative, we suggest an approach based on Social Practice Theory. We\nconceptualize social context as interactions between co-located social\npractices and acceptability as a matter of their (in)compatibilities. Finally,\nwe outline how this approach provides designers with a better understanding of\ndifferent types of social acceptability problems and helps finding appropriate\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:59:17 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 17:22:11 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Uhde", "Alarith", ""], ["Hassenzahl", "Marc", ""]]}, {"id": "2103.01701", "submitter": "Ab Mosca", "authors": "Ab Mosca, Alvitta Ottley, Remco Chang", "title": "Does Interaction Improve Bayesian Reasoning with Visualization?", "comments": "14 pages, 11 figures, To be published in 2021 ACM CHI Virtual\n  Conference on Human Factors in Computing Systems", "journal-ref": null, "doi": "10.1145/3411764.3445176", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interaction enables users to navigate large amounts of data effectively,\nsupports cognitive processing, and increases data representation methods.\nHowever, there have been few attempts to empirically demonstrate whether adding\ninteraction to a static visualization improves its function beyond popular\nbeliefs. In this paper, we address this gap. We use a classic Bayesian\nreasoning task as a testbed for evaluating whether allowing users to interact\nwith a static visualization can improve their reasoning. Through two\ncrowdsourced studies, we show that adding interaction to a static Bayesian\nreasoning visualization does not improve participants' accuracy on a Bayesian\nreasoning task. In some cases, it can significantly detract from it. Moreover,\nwe demonstrate that underlying visualization design modulates performance and\nthat people with high versus low spatial ability respond differently to\ndifferent interaction techniques and underlying base visualizations. Our work\nsuggests that interaction is not as unambiguously good as we often believe; a\nwell designed static visualization can be as, if not more, effective than an\ninteractive one.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:10:42 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 12:47:39 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Mosca", "Ab", ""], ["Ottley", "Alvitta", ""], ["Chang", "Remco", ""]]}, {"id": "2103.01771", "submitter": "Santiago Ontanon", "authors": "Santiago Onta\\~n\\'on, Jichen Zhu", "title": "The Personalization Paradox: the Conflict between Accurate User Models\n  and Personalized Adaptive Systems", "comments": "arXiv admin note: substantial text overlap with arXiv:2101.10020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized adaptation technology has been adopted in a wide range of\ndigital applications such as health, training and education, e-commerce and\nentertainment. Personalization systems typically build a user model, aiming to\ncharacterize the user at hand, and then use this model to personalize the\ninteraction. Personalization and user modeling, however, are often\nintrinsically at odds with each other (a fact some times referred to as the\npersonalization paradox). In this paper, we take a closer look at this\npersonalization paradox, and identify two ways in which it might manifest:\nfeedback loops and moving targets. To illustrate these issues, we report\nresults in the domain of personalized exergames (videogames for physical\nexercise), and describe our early steps to address some of the issues arisen by\nthe personalization paradox.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 14:44:15 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Onta\u00f1\u00f3n", "Santiago", ""], ["Zhu", "Jichen", ""]]}, {"id": "2103.02186", "submitter": "Zhen Fu", "authors": "Zhen Fu, Bo Wang, Fei Chen, Xihong Wu, Jing Chen", "title": "Eye-gaze Estimation with HEOG and Neck EMG using Deep Neural Networks", "comments": "5 pages, 5 figures, submitted to EUSIPCO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hearing-impaired listeners usually have troubles attending target talker in\nmulti-talker scenes, even with hearing aids (HAs). The problem can be solved\nwith eye-gaze steering HAs, which requires listeners eye-gazing on the target.\nIn a situation where head rotates, eye-gaze is subject to both behaviors of\nsaccade and head rotation. However, existing methods of eye-gaze estimation did\nnot work reliably, since the listener's strategy of eye-gaze varies and\nmeasurements of the two behaviors were not properly combined. Besides, existing\nmethods were based on hand-craft features, which could overlook some important\ninformation. In this paper, a head-fixed and a head-free experiments were\nconducted. We used horizontal electrooculography (HEOG) and neck\nelectromyography (NEMG), which separately measured saccade and head rotation to\ncommonly estimate eye-gaze. Besides traditional classifier and hand-craft\nfeatures, deep neural networks (DNN) were introduced to automatically extract\nfeatures from intact waveforms. Evaluation results showed that when the input\nwas HEOG with inertial measurement unit, the best performance of our proposed\nDNN classifiers achieved 93.3%; and when HEOG was with NEMG together, the\naccuracy reached 72.6%, higher than that with HEOG (about 71.0%) or NEMG (about\n35.7%) alone. These results indicated the feasibility to estimate eye-gaze with\nHEOG and NEMG.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 05:21:01 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Fu", "Zhen", ""], ["Wang", "Bo", ""], ["Chen", "Fei", ""], ["Wu", "Xihong", ""], ["Chen", "Jing", ""]]}, {"id": "2103.02197", "submitter": "Young-Eun Lee", "authors": "Young-Eun Lee, Seong-Whan Lee", "title": "Decoding Event-related Potential from Ear-EEG Signals based on Ensemble\n  Convolutional Neural Networks in Ambulatory Environment", "comments": "Submitted IEEE the 9th International Winter Conference on\n  Brain-Computer Interface. arXiv admin note: text overlap with\n  arXiv:2002.01085", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, practical brain-computer interface is actively carried out,\nespecially, in an ambulatory environment. However, the electroencephalography\n(EEG) signals are distorted by movement artifacts and electromyography signals\nwhen users are moving, which make hard to recognize human intention. In\naddition, as hardware issues are also challenging, ear-EEG has been developed\nfor practical brain-computer interface and has been widely used. In this paper,\nwe proposed ensemble-based convolutional neural networks in ambulatory\nenvironment and analyzed the visual event-related potential responses in scalp-\nand ear-EEG in terms of statistical analysis and brain-computer interface\nperformance. The brain-computer interface performance deteriorated as 3-14%\nwhen walking fast at 1.6 m/s. The proposed methods showed 0.728 in average of\nthe area under the curve. The proposed method shows robust to the ambulatory\nenvironment and imbalanced data as well.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 06:04:59 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Lee", "Young-Eun", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2103.02238", "submitter": "Imran Raza", "authors": "A. Shahid (COMSATS University Islamabad, Lahore Campus), I. Raza\n  (COMSATS University Islamabad, Lahore Campus), S. A. Hussain (COMSATS\n  University Islamabad, Lahore Campus)", "title": "EmoWrite: A Sentiment Analysis-Based Thought to Text Conversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain Computer Interface (BCI) helps in processing and extraction of useful\ninformation from the acquired brain signals having applications in diverse\nfields such as military, medicine, neuroscience, and rehabilitation. BCI has\nbeen used to support paralytic patients having speech impediments with severe\ndisabilities. To help paralytic patients communicate with ease, BCI based\nsystems convert silent speech (thoughts) to text. However, these systems have\nan inconvenient graphical user interface, high latency, limited typing speed,\nand low accuracy rate. Apart from these limitations, the existing systems do\nnot incorporate the inevitable factor of a patient's emotional states and\nsentiment analysis. The proposed system EmoWrite implements a dynamic keyboard\nwith contextualized appearance of characters reducing the traversal time and\nimproving the utilization of the screen space. The proposed system has been\nevaluated and compared with the existing systems for accuracy, convenience,\nsentimental analysis, and typing speed. This system results in 6.58 Words Per\nMinute (WPM) and 31.92 Characters Per Minute (CPM) with an accuracy of 90.36\npercent. EmoWrite also gives remarkable results when it comes to the\nintegration of emotional states. Its Information Transfer Rate (ITR) is also\nhigh as compared to other systems i.e., 87.55 bits per min with commands and\n72.52 bits per min for letters. Furthermore, it provides easy to use interface\nwith a latency of 2.685 sec.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:03:59 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Shahid", "A.", "", "COMSATS University Islamabad, Lahore Campus"], ["Raza", "I.", "", "COMSATS University Islamabad, Lahore Campus"], ["Hussain", "S. A.", "", "COMSATS\n  University Islamabad, Lahore Campus"]]}, {"id": "2103.02380", "submitter": "Bin Chen", "authors": "Ruizhen Hu, Bin Chen, Juzhan Xu, Oliver van Kaick, Oliver Deussen, Hui\n  Huang", "title": "Shape-driven Coordinate Ordering for Star Glyph Sets via Reinforcement\n  Learning", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 2021", "doi": "10.1109/TVCG.2021.3052167", "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural optimization model trained with reinforcement learning to\nsolve the coordinate ordering problem for sets of star glyphs. Given a set of\nstar glyphs associated to multiple class labels, we propose to use shape\ncontext descriptors to measure the perceptual distance between pairs of glyphs,\nand use the derived silhouette coefficient to measure the perception of class\nseparability within the entire set. To find the optimal coordinate order for\nthe given set, we train a neural network using reinforcement learning to reward\norderings with high silhouette coefficients. The network consists of an encoder\nand a decoder with an attention mechanism. The encoder employs a recurrent\nneural network (RNN) to encode input shape and class information, while the\ndecoder together with the attention mechanism employs another RNN to output a\nsequence with the new coordinate order. In addition, we introduce a neural\nnetwork to efficiently estimate the similarity between shape context\ndescriptors, which allows to speed up the computation of silhouette\ncoefficients and thus the training of the axis ordering network. Two user\nstudies demonstrate that the orders provided by our method are preferred by\nusers for perceiving class separation. We tested our model on different\nsettings to show its robustness and generalization abilities and demonstrate\nthat it allows to order input sets with unseen data size, data dimension, or\nnumber of classes. We also demonstrate that our model can be adapted to\ncoordinate ordering of other types of plots such as RadViz by replacing the\nproposed shape-aware silhouette coefficient with the corresponding quality\nmetric to guide network training.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:05:10 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hu", "Ruizhen", ""], ["Chen", "Bin", ""], ["Xu", "Juzhan", ""], ["van Kaick", "Oliver", ""], ["Deussen", "Oliver", ""], ["Huang", "Hui", ""]]}, {"id": "2103.02381", "submitter": "Saar Alon-Barkat", "authors": "Saar Alon-Barkat and Madalina Busuioc", "title": "Decision-makers Processing of AI Algorithmic Advice: Automation Bias\n  versus Selective Adherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial intelligence algorithms are increasingly adopted as decisional\naides by public organisations, with the promise of overcoming biases of human\ndecision-makers. At the same time, the use of algorithms may introduce new\nbiases in the human-algorithm interaction. A key concern emerging from\npsychology studies regards human overreliance on algorithmic advice even in the\nface of warning signals and contradictory information from other sources\n(automation bias). A second concern regards decision-makers inclination to\nselectively adopt algorithmic advice when it matches their pre-existing beliefs\nand stereotypes (selective adherence). To date, we lack rigorous empirical\nevidence about the prevalence of these biases in a public sector context. We\nassess these via two pre-registered experimental studies (N=1,509), simulating\nthe use of algorithmic advice in decisions pertaining to the employment of\nschool teachers in the Netherlands. In study 1, we test automation bias by\nexploring participants adherence to a prediction of teachers performance, which\ncontradicts additional evidence, while comparing between two types of\npredictions: algorithmic v. human-expert. We do not find evidence for\nautomation bias. In study 2, we replicate these findings, and we also test\nselective adherence by manipulating the teachers ethnic background. We find a\npropensity for adherence when the advice predicts low performance for a teacher\nof a negatively stereotyped ethnic minority, with no significant differences\nbetween algorithmic and human advice. Overall, our findings of selective,\nbiased adherence belie the promise of neutrality that has propelled algorithm\nuse in the public sector.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:10:50 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Alon-Barkat", "Saar", ""], ["Busuioc", "Madalina", ""]]}, {"id": "2103.02502", "submitter": "Min Chen", "authors": "Min Chen, Alfie Abdul-Rahman, Deborah Silver, and Mateu Sbert", "title": "A Bounded Measure for Estimating the Benefit of Visualization: Case\n  Studies and Empirical Evaluation", "comments": "Following the SciVis 2020 reviewers' request for more explanation and\n  clarification, the origianl article, \"A Bounded Measure for Estimating the\n  Benefit of Visualization, arxiv:2002.05282\", has been split into two\n  articles, on \"Theoretical Discourse and Conceptual Evaluation\" and \"Case\n  Studies and Empirical Evaluation\" respectively. This is the second article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many visual representations, such as volume-rendered images and metro maps,\nfeature a noticeable amount of information loss. At a glance, there seem to be\nnumerous opportunities for viewers to misinterpret the data being visualized,\nhence undermining the benefits of these visual representations. In practice,\nthere is little doubt that these visual representations are useful. The\nrecently-proposed information-theoretic measure for analyzing the cost-benefit\nratio of visualization processes can explain such usefulness experienced in\npractice, and postulate that the viewers' knowledge can reduce the potential\ndistortion (e.g., misinterpretation) due to information loss. This suggests\nthat viewers' knowledge can be estimated by comparing the potential distortion\nwithout any knowledge and the actual distortion with some knowledge. In this\npaper, we describe several case studies for collecting instances that can (i)\nsupport the evaluation of several candidate measures for estimating the\npotential distortion distortion in visualization, and (ii) demonstrate their\napplicability in practical scenarios. Because the theoretical discourse on\nchoosing an appropriate bounded measure for estimating the potential distortion\nis yet conclusive, it is the real world data about visualization further\ninforms the selection of a bounded measure, providing practical evidence to aid\na theoretical conclusion. Meanwhile, once we can measure the potential\ndistortion in a bounded manner, we can interpret the numerical values\ncharacterizing the benefit of visualization more intuitively.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:12:12 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Chen", "Min", ""], ["Abdul-Rahman", "Alfie", ""], ["Silver", "Deborah", ""], ["Sbert", "Mateu", ""]]}, {"id": "2103.02505", "submitter": "Min Chen", "authors": "Min Chen and Mateu Sbert", "title": "A Bounded Measure for Estimating the Benefit of Visualization:\n  Theoretical Discourse and Conceptual Evaluation", "comments": "Following the SciVis 2020 reviewers' request for more explanation and\n  clarification, the origianl article, \"A Bounded Measure for Estimating the\n  Benefit of Visualization, arxiv:2002.05282\", was split into two articles, on\n  \"Theoretical Discourse and Conceptual Evaluation\" and \"Case Studies and\n  Empirical Evaluation\" respectively. This is the first article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.GR cs.HC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theory can be used to analyze the cost-benefit of visualization\nprocesses. However, the current measure of benefit contains an unbounded term\nthat is neither easy to estimate nor intuitive to interpret. In this work, we\npropose to revise the existing cost-benefit measure by replacing the unbounded\nterm with a bounded one. We examine a number of bounded measures that include\nthe Jenson-Shannon divergence and a new divergence measure formulated as part\nof this work. We describe the rationale for proposing a new divergence measure.\nAs the first part of comparative evaluation, we use visual analysis to support\nthe multi-criteria comparison, narrowing the search down to several options\nwith better mathematical properties. The theoretical discourse and conceptual\nevaluation in this paper provide the basis for further comparative evaluation\nthrough synthetic and experimental case studies, which are to be reported in a\nseparate paper.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:14:10 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Chen", "Min", ""], ["Sbert", "Mateu", ""]]}, {"id": "2103.02524", "submitter": "Longqi Yang", "authors": "Jenna Butler, Mary Czerwinski, Shamsi Iqbal, Sonia Jaffe, Kate Nowak,\n  Emily Peloquin, Longqi Yang", "title": "Personal Productivity and Well-being -- Chapter 2 of the 2021 New Future\n  of Work Report", "comments": "In The New Future of Work: Research from Microsoft on the Impact of\n  the Pandemic on Work Practices, edited by Jaime Teevan, Brent Hecht, and\n  Sonia Jaffe, 1st ed. Microsoft, 2021. https://aka.ms/newfutureofwork", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.SE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We now turn to understanding the impact that COVID-19 had on the personal\nproductivity and well-being of information workers as their work practices were\nimpacted by remote work. This chapter overviews people's productivity,\nsatisfaction, and work patterns, and shows that the challenges and benefits of\nremote work are closely linked. Looking forward, the infrastructure surrounding\nwork will need to evolve to help people adapt to the challenges of remote and\nhybrid work.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:57:45 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Butler", "Jenna", ""], ["Czerwinski", "Mary", ""], ["Iqbal", "Shamsi", ""], ["Jaffe", "Sonia", ""], ["Nowak", "Kate", ""], ["Peloquin", "Emily", ""], ["Yang", "Longqi", ""]]}, {"id": "2103.02550", "submitter": "Marta Orduna", "authors": "Marta Orduna, Pablo P\\'erez, Jes\\'us Guti\\'errez and Narciso Garc\\'ia", "title": "Methodology to Assess Quality, Presence, Empathy, Attitude, and\n  Attention in Social VR: International Experiences Use Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper analyzes the joint assessment of quality, spatial and social\npresence, empathy, attitude, and attention in three conditions: (A)visualizing\nand rating the quality of contents in a Head-Mounted Display (HMD),\n(B)visualizing the contents in an HMD,and (C)visualizing the contents in an HMD\nwhere participants can see their hands and take notes. The experiment simulates\nan immersive communication where participants attend conversations of different\ngenres and from different acquisition perspectives in the context of\ninternational experiences. Video quality is evaluated with Single-Stimulus\nDiscrete Quality Evaluation (SSDQE) methodology. Spatial and social presence\nare evaluated with questionnaires adapted from the literature. Initial empathy\nis assessed with Interpersonal Reactivity Index(IRI) and a questionnaire is\ndesigned to evaluate attitude. Attention is evaluated with 3 questions that had\npass/fail answers. 54 participants were evenly distributed among A, B, and C\nconditions taking into account their international experience backgrounds,\nobtaining a diverse sample of participants. The results from the subjective\ntest validate the proposed methodology in VR communications, showing that video\nquality experiments can be adapted to conditions imposed by experiments focused\non the evaluation of socioemotional features in terms of contents of\nlong-duration, actor and observer acquisition perspectives, and genre. In\naddition, the positive results related to the sense of presence imply that\ntechnology can be relevant in the analyzed use case. The acquisition\nperspective greatly influences social presence and all the contents have a\npositive impact on all participants on their attitude towards international\nexperiences. The annotated dataset, Student Experiences Around the World\ndataset (SEAW-dataset), obtained from the experiment is made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 17:43:18 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Orduna", "Marta", ""], ["P\u00e9rez", "Pablo", ""], ["Guti\u00e9rrez", "Jes\u00fas", ""], ["Garc\u00eda", "Narciso", ""]]}, {"id": "2103.02727", "submitter": "Sydney Katz", "authors": "Sydney M. Katz, Amir Maleki, Erdem B{\\i}y{\\i}k, Mykel J. Kochenderfer", "title": "Preference-based Learning of Reward Function Features", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preference-based learning of reward functions, where the reward function is\nlearned using comparison data, has been well studied for complex robotic tasks\nsuch as autonomous driving. Existing algorithms have focused on learning reward\nfunctions that are linear in a set of trajectory features. The features are\ntypically hand-coded, and preference-based learning is used to determine a\nparticular user's relative weighting for each feature. Designing a\nrepresentative set of features to encode reward is challenging and can result\nin inaccurate models that fail to model the users' preferences or perform the\ntask properly. In this paper, we present a method to learn both the relative\nweighting among features as well as additional features that help encode a\nuser's reward function. The additional features are modeled as a neural network\nthat is trained on the data from pairwise comparison queries. We apply our\nmethods to a driving scenario used in previous work and compare the predictive\npower of our method to that of only hand-coded features. We perform additional\nanalysis to interpret the learned features and examine the optimal\ntrajectories. Our results show that adding an additional learned feature to the\nreward model enhances both its predictive power and expressiveness, producing\nunique results for each user.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 22:32:43 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Katz", "Sydney M.", ""], ["Maleki", "Amir", ""], ["B\u0131y\u0131k", "Erdem", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "2103.02831", "submitter": "Holger Regenbrecht", "authors": "Holger Regenbrecht and Thomas Schubert", "title": "Measuring Presence in Augmented Reality Environments: Design and a First\n  Test of a Questionnaire", "comments": "This paper was first presented in 2002: Regenbrecht, H. & Schubert,\n  T. (2002b). Measuring Presence in Augmented Reality Environments: Design and\n  a First Test of a Questionnaire. Proceedings of the Fifth Annual\n  International Workshop Presence 2002, Porto, Portugal - October 9-11. pp.\n  138-144", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality (AR) enriches a user's real environment by adding spatially\naligned virtual objects (3D models, 2D textures, textual annotations, etc) by\nmeans of special display technologies. These are either worn on the body or\nplaced in the working environment. From a technical point of view, AR faces\nthree major challenges: (1) to generate a high quality rendering, (2) to\nprecisely register (in position and orientation) the virtual objects (VOs) with\nthe real environment, and (3) to do so in interactive real-time (Regenbrecht,\nWagner, and Baratoff, 2002). The goal is to create the impression that the VOs\nare part of the real environment. Therefore, and similar to definitions of\nvirtual reality (Steuer, 1992), it makes sense to define AR from a\npsychological point of view: Augmented Reality conveys the impression that VOs\nare present in the real environment. In order to evaluate how well this goal is\nreached, a psychological measurement of this type of presence is necessary. In\nthe following, we will describe technological features of AR systems that make\na special questionnaire version necessary, describe our approach to the\nquestionnaire development, and the data collection strategy. Finally we will\npresent first results of the application of the questionnaire in a recent study\nwith 385 participants.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 04:46:19 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Regenbrecht", "Holger", ""], ["Schubert", "Thomas", ""]]}, {"id": "2103.02851", "submitter": "Byoung-Hee Kwon", "authors": "Byoung-Hee Kwon, Ji-Hoon Jeong, and Seong-Whan Lee", "title": "Visual Motion Imagery Classification with Deep Neural Network based on\n  Functional Connectivity", "comments": "10 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interfaces (BCIs) use brain signals such as\nelectroencephalography to reflect user intention and enable two-way\ncommunication between computers and users. BCI technology has recently received\nmuch attention in healthcare applications, such as neurorehabilitation and\ndiagnosis. BCI applications can also control external devices using only brain\nactivity, which can help people with physical or mental disabilities,\nespecially those suffering from neurological and neuromuscular diseases such as\nstroke and amyotrophic lateral sclerosis. Motor imagery (MI) has been widely\nused for BCI-based device control, but we adopted intuitive visual motion\nimagery to overcome the weakness of MI. In this study, we developed a\nthree-dimensional (3D) BCI training platform to induce users to imagine\nupper-limb movements used in real-life activities (picking up a cell phone,\npouring water, opening a door, and eating food). We collected intuitive visual\nmotion imagery data and proposed a deep learning network based on functional\nconnectivity as a mind-reading technique. As a result, the proposed network\nrecorded a high classification performance on average (71.05%). Furthermore, we\napplied the leave-one-subject-out approach to confirm the possibility of\nimprovements in subject-independent classification performance. This study will\ncontribute to the development of BCI-based healthcare applications for\nrehabilitation, such as robotic arms and wheelchairs, or assist daily life.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 06:27:43 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Kwon", "Byoung-Hee", ""], ["Jeong", "Ji-Hoon", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2103.02938", "submitter": "Silvio Barra Dr", "authors": "Silvio Barra, Salvatore M. Carta, Alessandro Giuliani, Alessia Pisu,\n  Alessandro Sebastian Podda and DanieleRiboni", "title": "FootApp: an AI-Powered System for Football Match Annotation", "comments": null, "journal-ref": "Pattern Recognition for Adaptive User Interfaces [1178] on\n  Multimedia Tools and Applications 2021", "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the last years, scientific and industrial research has experienced a\ngrowing interest in acquiring large annotated data sets to train artificial\nintelligence algorithms for tackling problems in different domains. In this\ncontext, we have observed that even the market for football data has\nsubstantially grown. The analysis of football matches relies on the annotation\nof both individual players' and team actions, as well as the athletic\nperformance of players. Consequently, annotating football events at a\nfine-grained level is a very expensive and error-prone task. Most existing\nsemi-automatic tools for football match annotation rely on cameras and computer\nvision. However, those tools fall short in capturing team dynamics, and in\nextracting data of players who are not visible in the camera frame. To address\nthese issues, in this manuscript we present FootApp, an AI-based system for\nfootball match annotation. First, our system relies on an advanced and mixed\nuser interface that exploits both vocal and touch interaction. Second, the\nmotor performance of players is captured and processed by applying machine\nlearning algorithms to data collected from inertial sensors worn by players.\nArtificial intelligence techniques are then used to check the consistency of\ngenerated labels, including those regarding the physical activity of players,\nto automatically recognize annotation errors. Notably, we implemented a full\nprototype of the proposed system, performing experiments to show its\neffectiveness in a real-world adoption scenario.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 10:38:46 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Barra", "Silvio", ""], ["Carta", "Salvatore M.", ""], ["Giuliani", "Alessandro", ""], ["Pisu", "Alessia", ""], ["Podda", "Alessandro Sebastian", ""], ["DanieleRiboni", "", ""]]}, {"id": "2103.03020", "submitter": "Manuel Guimar\\~aes", "authors": "Samuel Mascarenhas, Manuel Guimar\\~aes, Pedro A. Santos, Jo\\~ao Dias,\n  Rui Prada, Ana Paiva", "title": "FAtiMA Toolkit -- Toward an effective and accessible tool for the\n  development of intelligent virtual agents and social robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  More than a decade has passed since the development of FearNot!, an\napplication designed to help children deal with bullying through role-playing\nwith virtual characters. It was also the application that led to the creation\nof FAtiMA, an affective agent architecture for creating autonomous characters\nthat can evoke empathic responses. In this paper, we describe FAtiMA Toolkit, a\ncollection of open-source tools that is designed to help researchers, game\ndevelopers and roboticists incorporate a computational model of emotion and\ndecision-making in their work. The toolkit was developed with the goal of\nmaking FAtiMA more accessible, easier to incorporate into different projects\nand more flexible in its capabilities for human-agent interaction, based upon\nthe experience gathered over the years across different virtual environments\nand human-robot interaction scenarios. As a result, this work makes several\ndifferent contributions to the field of Agent-Based Architectures. More\nprecisely, FAtiMA Toolkit's library based design allows developers to easily\nintegrate it with other frameworks, its meta-cognitive model affords different\ninternal reasoners and affective components and its explicit dialogue structure\ngives control to the author even within highly complex scenarios. To\ndemonstrate the use of FAtiMA Toolkit, several different use cases where the\ntoolkit was successfully applied are described and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:30:59 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Mascarenhas", "Samuel", ""], ["Guimar\u00e3es", "Manuel", ""], ["Santos", "Pedro A.", ""], ["Dias", "Jo\u00e3o", ""], ["Prada", "Rui", ""], ["Paiva", "Ana", ""]]}, {"id": "2103.03072", "submitter": "Mahendran Subramanian", "authors": "Mahendran Subramanian, Suhyung Park, Pavel Orlov, Ali Shafti, A. Aldo\n  Faisal", "title": "Gaze-contingent decoding of human navigation intention on an autonomous\n  wheelchair platform", "comments": "Accepted manuscript IEEE/EMBS Neural Engineering (NER) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have pioneered the Where-You-Look-Is Where-You-Go approach to controlling\nmobility platforms by decoding how the user looks at the environment to\nunderstand where they want to navigate their mobility device. However, many\nnatural eye-movements are not relevant for action intention decoding, only some\nare, which places a challenge on decoding, the so-called Midas Touch Problem.\nHere, we present a new solution, consisting of 1. deep computer vision to\nunderstand what object a user is looking at in their field of view, with 2. an\nanalysis of where on the object's bounding box the user is looking, to 3. use a\nsimple machine learning classifier to determine whether the overt visual\nattention on the object is predictive of a navigation intention to that object.\nOur decoding system ultimately determines whether the user wants to drive to\ne.g., a door or just looks at it. Crucially, we find that when users look at an\nobject and imagine they were moving towards it, the resulting eye-movements\nfrom this motor imagery (akin to neural interfaces) remain decodable. Once a\ndriving intention and thus also the location is detected our system instructs\nour autonomous wheelchair platform, the A.Eye-Drive, to navigate to the desired\nobject while avoiding static and moving obstacles. Thus, for navigation\npurposes, we have realised a cognitive-level human interface, as it requires\nthe user only to cognitively interact with the desired goal, not to\ncontinuously steer their wheelchair to the target (low-level human\ninterfacing).\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:52:06 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Subramanian", "Mahendran", ""], ["Park", "Suhyung", ""], ["Orlov", "Pavel", ""], ["Shafti", "Ali", ""], ["Faisal", "A. Aldo", ""]]}, {"id": "2103.03125", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang and Hai Zhao", "title": "Advances in Multi-turn Dialogue Comprehension: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machines to understand natural language and interact with humans is\nan elusive and essential task in the field of artificial intelligence. In\nrecent years, a diversity of dialogue systems has been designed with the rapid\ndevelopment of deep learning researches, especially the recent pre-trained\nlanguage models. Among these studies, the fundamental yet challenging part is\ndialogue comprehension whose role is to teach the machines to read and\ncomprehend the dialogue context before responding. In this paper, we review the\nprevious methods from the perspective of dialogue modeling. We summarize the\ncharacteristics and challenges of dialogue comprehension in contrast to\nplain-text reading comprehension. Then, we discuss three typical patterns of\ndialogue modeling that are widely-used in dialogue comprehension tasks such as\nresponse selection and conversation question-answering, as well as\ndialogue-related language modeling techniques to enhance PrLMs in dialogue\nscenarios. Finally, we highlight the technical advances in recent years and\npoint out the lessons we can learn from the empirical analysis and the\nprospects towards a new frontier of researches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:50:17 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""]]}, {"id": "2103.03130", "submitter": "Ylva Ferstl", "authors": "Ylva Ferstl, Michael Neff, Rachel McDonnell", "title": "It's A Match! Gesture Generation Using Expressive Parameter Matching", "comments": "to be published in Proceedings of the 20th International Conference\n  on Autonomous Agents and Multiagent Systems (AAMAS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic gesture generation from speech generally relies on implicit\nmodelling of the nondeterministic speech-gesture relationship and can result in\naveraged motion lacking defined form. Here, we propose a database-driven\napproach of selecting gestures based on specific motion characteristics that\nhave been shown to be associated with the speech audio. We extend previous work\nthat identified expressive parameters of gesture motion that can both be\npredicted from speech and are perceptually important for a good speech-gesture\nmatch, such as gesture velocity and finger extension. A perceptual study was\nperformed to evaluate the appropriateness of the gestures selected with our\nmethod. We compare our method with two baseline selection methods. The first\nrespects timing, the desired onset and duration of a gesture, but does not\nmatch gesture form in other ways. The second baseline additionally disregards\nthe original gesture timing for selecting gestures. The gesture sequences from\nour method were rated as a significantly better match to the speech than\ngestures selected by either baseline method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 16:05:56 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ferstl", "Ylva", ""], ["Neff", "Michael", ""], ["McDonnell", "Rachel", ""]]}, {"id": "2103.03163", "submitter": "Wendy Ju", "authors": "Wendy Ju, Ilan Mandel, Kevin Weatherwax, Leila Takayama, Nikolas\n  Martelaro, Denis Willett", "title": "Remote Observation of Field Work on the Farm", "comments": "Presented at Microsoft Future of Work Symposium, August 3-5, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Travel restrictions and social distancing measures make it difficult to\nobserve, monitor or manage physical fieldwork. We describe research in progress\nthat applies technologies for real-time remote observation and conversation in\non-road vehicles to observe field work on a farm. We collaborated on a pilot\ndeployment of this project at Kreher Eggs in upstate New York. We instrumented\na tractor with equipment to remotely observe and interview farm workers\nperforming vehicle-related work. This work was initially undertaken to allow\nsustained observation of field work over longer periods of time from\ngeographically distant locales; given our current situation, this work provides\na case study in how to perform observational research when geographic and\nbodily distance have become the norm. We discuss our experiences and provide\nsome preliminary insights for others looking to conduct remote observational\nresearch in the field.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:10:09 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ju", "Wendy", ""], ["Mandel", "Ilan", ""], ["Weatherwax", "Kevin", ""], ["Takayama", "Leila", ""], ["Martelaro", "Nikolas", ""], ["Willett", "Denis", ""]]}, {"id": "2103.03296", "submitter": "Atharva Kulkarni", "authors": "Atharva Kulkarni, Sunanda Somwase, Shivam Rajput, and Manisha Marathe", "title": "PVG at WASSA 2021: A Multi-Input, Multi-Task, Transformer-Based\n  Architecture for Empathy and Distress Prediction", "comments": "Accepted at the 11th Workshop on Computational Approaches to\n  Subjectivity, Sentiment and Social Media Analysis (WASSA 2021), co-located\n  with EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Active research pertaining to the affective phenomenon of empathy and\ndistress is invaluable for improving human-machine interaction. Predicting\nintensities of such complex emotions from textual data is difficult, as these\nconstructs are deeply rooted in the psychological theory. Consequently, for\nbetter prediction, it becomes imperative to take into account ancillary factors\nsuch as the psychological test scores, demographic features, underlying latent\nprimitive emotions, along with the text's undertone and its psychological\ncomplexity. This paper proffers team PVG's solution to the WASSA 2021 Shared\nTask on Predicting Empathy and Emotion in Reaction to News Stories. Leveraging\nthe textual data, demographic features, psychological test score, and the\nintrinsic interdependencies of primitive emotions and empathy, we propose a\nmulti-input, multi-task framework for the task of empathy score prediction.\nHere, the empathy score prediction is considered the primary task, while\nemotion and empathy classification are considered secondary auxiliary tasks.\nFor the distress score prediction task, the system is further boosted by the\naddition of lexical features. Our submission ranked 1$^{st}$ based on the\naverage correlation (0.545) as well as the distress correlation (0.574), and\n2$^{nd}$ for the empathy Pearson correlation (0.517).\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:12:25 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kulkarni", "Atharva", ""], ["Somwase", "Sunanda", ""], ["Rajput", "Shivam", ""], ["Marathe", "Manisha", ""]]}, {"id": "2103.03413", "submitter": "Yi-Lin Tsai", "authors": "Yi-Lin Tsai (1), Chetanya Rastogi (2), Peter K. Kitanidis (1, 3, and\n  4), Christopher B. Field (3, 5, and 6) ((1) Department of Civil and\n  Environmental Engineering, Stanford University, Stanford, CA, USA, (2)\n  Department of Computer Science, Stanford University, Stanford, CA, USA, (3)\n  Woods Institute for the Environment, Stanford University, Stanford, CA, USA,\n  (4) Institute for Computational and Mathematical Engineering, Stanford\n  University, Stanford, CA, USA, (5) Department of Biology, Stanford\n  University, Stanford, CA, USA, (6) Department of Earth System Science,\n  Stanford University, Stanford, CA, USA)", "title": "Routing algorithms as tools for integrating social distancing with\n  emergency evacuation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the implications of integrating social distancing with emergency\nevacuation, as would be expected when a hurricane approaches a city during the\nCOVID-19 pandemic. Specifically, we compare DNN (Deep Neural Network)-based and\nnon-DNN methods for generating evacuation strategies that minimize evacuation\ntime while allowing for social distancing in emergency vehicles. A central\nquestion is whether a DNN-based method provides sufficient extra routing\nefficiency to accommodate increased social distancing in a time-constrained\nevacuation operation. We describe the problem as a Capacitated Vehicle Routing\nProblem and solve it using a non-DNN solution (Sweep Algorithm) and a DNN-based\nsolution (Deep Reinforcement Learning). The DNN-based solution can provide\ndecision-makers with more efficient routing than the typical non-DNN routing\nsolution. However, it does not come close to compensating for the extra time\nrequired for social distancing, and its advantage disappears as the emergency\nvehicle capacity approaches the number of people per household.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 01:12:31 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 22:43:07 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 02:26:53 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Tsai", "Yi-Lin", "", "1, 3, and\n  4"], ["Rastogi", "Chetanya", "", "1, 3, and\n  4"], ["Kitanidis", "Peter K.", "", "1, 3, and\n  4"], ["Field", "Christopher B.", "", "3, 5, and 6"]]}, {"id": "2103.03488", "submitter": "Daniel Leite", "authors": "Daniel Leite, Volnei Frigeri Jr., Rodrigo Medeiros", "title": "Adaptive Gaussian Fuzzy Classifier for Real-Time Emotion Recognition in\n  Computer Games", "comments": "7 pages, 6 figures, Fuzz-IEEE 2021, Luxembourg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Human emotion recognition has become a need for more realistic and\ninteractive machines and computer systems. The greatest challenge is the\navailability of high-performance algorithms to effectively manage individual\ndifferences and nonstationarities in physiological data streams, i.e.,\nalgorithms that self-customize to a user with no subject-specific calibration\ndata. We describe an evolving Gaussian Fuzzy Classifier (eGFC), which is\nsupported by an online semi-supervised learning algorithm to recognize emotion\npatterns from electroencephalogram (EEG) data streams. We extract features from\nthe Fourier spectrum of EEG data. The data are provided by 28 individuals\nplaying the games 'Train Sim World', 'Unravel', 'Slender The Arrival', and\n'Goat Simulator' - a public dataset. Different emotions prevail, namely,\nboredom, calmness, horror and joy. We analyze the effect of individual\nelectrodes, time window lengths, and frequency bands on the accuracy of\nuser-independent eGFCs. We conclude that both brain hemispheres may assist\nclassification, especially electrodes on the frontal (Af3-Af4), occipital\n(O1-O2), and temporal (T7-T8) areas. We observe that patterns may be eventually\nfound in any frequency band; however, the Alpha (8-13Hz), Delta (1-4Hz), and\nTheta (4-8Hz) bands, in this order, are the highest correlated with emotion\nclasses. eGFC has shown to be effective for real-time learning of EEG data. It\nreaches a 72.2% accuracy using a variable rule base, 10-second windows, and\n1.8ms/sample processing time in a highly-stochastic time-varying 4-class\nclassification problem.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 06:27:04 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Leite", "Daniel", ""], ["Frigeri", "Volnei", "Jr."], ["Medeiros", "Rodrigo", ""]]}, {"id": "2103.03580", "submitter": "Sara Durrani", "authors": "Sara Durrani, Muhammad Umair Arshad", "title": "Transfer Learning based Speech Affect Recognition in Urdu", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been established that Speech Affect Recognition for low resource\nlanguages is a difficult task. Here we present a Transfer learning based Speech\nAffect Recognition approach in which: we pre-train a model for high resource\nlanguage affect recognition task and fine tune the parameters for low resource\nlanguage using Deep Residual Network. Here we use standard four data sets to\ndemonstrate that transfer learning can solve the problem of data scarcity for\nAffect Recognition task. We demonstrate that our approach is efficient by\nachieving 74.7 percent UAR on RAVDESS as source and Urdu data set as a target.\nThrough an ablation study, we have identified that pre-trained model adds most\nof the features information, improvement in results and solves less data\nissues. Using this knowledge, we have also experimented on SAVEE and EMO-DB\ndata set by setting Urdu as target language where only 400 utterances of data\nis available. This approach achieves high Unweighted Average Recall (UAR) when\ncompared with existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 10:30:58 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Durrani", "Sara", ""], ["Arshad", "Muhammad Umair", ""]]}, {"id": "2103.03591", "submitter": "Marvin Wyrich", "authors": "Marvin Wyrich and Raoul Ghit and Tobias Haller and Christian M\\\"uller", "title": "Bots Don't Mind Waiting, Do They? Comparing the Interaction With\n  Automatically and Manually Created Pull Requests", "comments": "To be published in Proceedings of 2021 IEEE/ACM International\n  Workshop on Bots in Software Engineering (BotSE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a maintainer of an open source software project, you are usually happy\nabout contributions in the form of pull requests that bring the project a step\nforward. Past studies have shown that when reviewing a pull request, not only\nits content is taken into account, but also, for example, the social\ncharacteristics of the contributor. Whether a contribution is accepted and how\nlong this takes therefore depends not only on the content of the contribution.\nWhat we only have indications for so far, however, is that pull requests from\nbots may be prioritized lower, even if the bots are explicitly deployed by the\ndevelopment team and are considered useful.\n  One goal of the bot research and development community is to design helpful\nbots to effectively support software development in a variety of ways. To get\ncloser to this goal, in this GitHub mining study, we examine the measurable\ndifferences in how maintainers interact with manually created pull requests\nfrom humans compared to those created automatically by bots.\n  About one third of all pull requests on GitHub currently come from bots.\nWhile pull requests from humans are accepted and merged in 72.53% of all cases,\nthis applies to only 37.38% of bot pull requests. Furthermore, it takes\nsignificantly longer for a bot pull request to be interacted with and for it to\nbe merged, even though they contain fewer changes on average than human pull\nrequests. These results suggest that bots have yet to realize their full\npotential.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 10:55:35 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wyrich", "Marvin", ""], ["Ghit", "Raoul", ""], ["Haller", "Tobias", ""], ["M\u00fcller", "Christian", ""]]}, {"id": "2103.03598", "submitter": "Bhavya Ghai", "authors": "Bhavya Ghai, Md Naimul Hoque, Klaus Mueller", "title": "WordBias: An Interactive Visual Tool for Discovering Intersectional\n  Biases Encoded in Word Embeddings", "comments": "Accepted to ACM SIGCHI 2021 LBW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intersectional bias is a bias caused by an overlap of multiple social factors\nlike gender, sexuality, race, disability, religion, etc. A recent study has\nshown that word embedding models can be laden with biases against\nintersectional groups like African American females, etc. The first step\ntowards tackling such intersectional biases is to identify them. However,\ndiscovering biases against different intersectional groups remains a\nchallenging task. In this work, we present WordBias, an interactive visual tool\ndesigned to explore biases against intersectional groups encoded in static word\nembeddings. Given a pretrained static word embedding, WordBias computes the\nassociation of each word along different groups based on race, age, etc. and\nthen visualizes them using a novel interactive interface. Using a case study,\nwe demonstrate how WordBias can help uncover biases against intersectional\ngroups like Black Muslim Males, Poor Females, etc. encoded in word embedding.\nIn addition, we also evaluate our tool using qualitative feedback from expert\ninterviews. The source code for this tool can be publicly accessed for\nreproducibility at github.com/bhavyaghai/WordBias.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:04:35 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ghai", "Bhavya", ""], ["Hoque", "Md Naimul", ""], ["Mueller", "Klaus", ""]]}, {"id": "2103.03602", "submitter": "Ahmed Rasheed", "authors": "Ahmed Rasheed, Muhammad Shahzad Younis, Junaid Qadir and Muhammad\n  Bilal", "title": "Use of Transfer Learning and Wavelet Transform for Breast Cancer\n  Detection", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast cancer is one of the most common cause of deaths among women.\nMammography is a widely used imaging modality that can be used for cancer\ndetection in its early stages. Deep learning is widely used for the detection\nof cancerous masses in the images obtained via mammography. The need to improve\naccuracy remains constant due to the sensitive nature of the datasets so we\nintroduce segmentation and wavelet transform to enhance the important features\nin the image scans. Our proposed system aids the radiologist in the screening\nphase of cancer detection by using a combination of segmentation and wavelet\ntransforms as pre-processing augmentation that leads to transfer learning in\nneural networks. The proposed system with these pre-processing techniques\nsignificantly increases the accuracy of detection on Mini-MIAS.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:08:56 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Rasheed", "Ahmed", ""], ["Younis", "Muhammad Shahzad", ""], ["Qadir", "Junaid", ""], ["Bilal", "Muhammad", ""]]}, {"id": "2103.03621", "submitter": "Siqi Cai", "authors": "Siqi Cai, Pengcheng Sun, Tanja Schultz, and Haizhou Li", "title": "Low-latency auditory spatial attention detection based on\n  spectro-spatial features from EEG", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting auditory attention based on brain signals enables many everyday\napplications, and serves as part of the solution to the cocktail party effect\nin speech processing. Several studies leverage the correlation between brain\nsignals and auditory stimuli to detect the auditory attention of listeners.\nRecently, studies show that the alpha band (8-13 Hz) EEG signals enable the\nlocalization of auditory stimuli. We believe that it is possible to detect\nauditory spatial attention without the need of auditory stimuli as references.\nIn this work, we use alpha power signals for automatic auditory spatial\nattention detection. To the best of our knowledge, this is the first attempt to\ndetect spatial attention based on alpha power neural signals. We propose a\nspectro-spatial feature extraction technique to detect the auditory spatial\nattention (left/right) based on the topographic specificity of alpha power.\nExperiments show that the proposed neural approach achieves 81.7% and 94.6%\naccuracy for 1-second and 10-second decision windows, respectively. Our\ncomparative results show that this neural approach outperforms other\ncompetitive models by a large margin in all test cases.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:50:50 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 01:59:25 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Cai", "Siqi", ""], ["Sun", "Pengcheng", ""], ["Schultz", "Tanja", ""], ["Li", "Haizhou", ""]]}, {"id": "2103.03665", "submitter": "Shijun Cai Ms", "authors": "Shijun Cai, Seok-Hee Hong, Jialiang Shen, Tongliang Liu", "title": "A Machine Learning Approach for Predicting Human Preference for Graph\n  Layouts", "comments": "9 pages, 9 figures, PacificVis_Notes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding what graph layout human prefer and why they prefer is\nsignificant and challenging due to the highly complex visual perception and\ncognition system in human brain. In this paper, we present the first machine\nlearning approach for predicting human preference for graph layouts.\n  In general, the data sets with human preference labels are limited and\ninsufficient for training deep networks. To address this, we train our deep\nlearning model by employing the transfer learning method, e.g., exploiting the\nquality metrics, such as shape-based metrics, edge crossing and stress, which\nare shown to be correlated to human preference on graph layouts. Experimental\nresults using the ground truth human preference data sets show that our model\ncan successfully predict human preference for graph layouts. To our best\nknowledge, this is the first approach for predicting qualitative evaluation of\ngraph layouts using human preference experiment data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:54:54 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Cai", "Shijun", ""], ["Hong", "Seok-Hee", ""], ["Shen", "Jialiang", ""], ["Liu", "Tongliang", ""]]}, {"id": "2103.03756", "submitter": "Andrew Tristan", "authors": "Andrew Tristan, Vinicius Woloszyn and Ben Kaden", "title": "BOPI: A Programming Interface For Reuse Of Research Data Available On\n  DSpace Repositories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A recent study showed that more than 70% of researchers fail to reproduce\ntheir peers's experiments and more than half fail to reproduce their own\nexperiments. Obviously, from a perspective of scientific quality this is a more\nthan unsatisfying numbers. One approach to mitigate this flaw lies in the\ntransparent provision of relevant research data to increase the base of\navailable material to evaluate and possibly reconduct experiments. However,\nsuch data needs to be presented and accessed in a findable and purposefully\nusable way. In this work, we report the development of a programming interface\nto enhance findability and accessibility of research data (available in DSpace\nsystems) and hence reproducibility of scientific experiments with data. This\ninterface allows researchers to (i) find research data in multiples languages\ntrough automatic translation of metadata; (ii) display a preview of data\nwithout download it beforehand; (iii) provide a detailed statistics of the data\nwith interactive graphs for quality assessment; (iv) automatic download of data\ndirectly from Python-based experiments. Usability tests revealed that this\ninterface improves the effectiveness, efficiency and satisfaction during the\nreuse of research data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:32:51 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 09:02:12 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Tristan", "Andrew", ""], ["Woloszyn", "Vinicius", ""], ["Kaden", "Ben", ""]]}, {"id": "2103.03898", "submitter": "Jason McEwen", "authors": "Iqra Arshad, Paulo De Mello, Martin Ender, Jason D. McEwen, Elisa R.\n  Ferr\\'e", "title": "Reducing cybersickness in 360-degree virtual reality", "comments": "18 pages, 1 figure; Software available at\n  https://www.kagenova.com/products/copernic360/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the technological advancements in Virtual Reality (VR), users are\nconstantly combating feelings of nausea and disorientation, the so called\ncybersickness. Triggered by a sensory conflict between the visual and\nvestibular systems, cybersickness symptoms cause discomfort and hinder the\nimmersive VR experience. Here we investigated cybersickness in 360-degree VR.\nIn 360-degrees VR experiences, movement in the real world is not reflected in\nthe virtual world, and therefore self-motion information is not corroborated by\nmatching visual and vestibular cues, which may potentially induce\ncybersickness. We have evaluated whether an Artificial Intelligence (AI)\nsoftware designed to supplement the VR experience with artificial\n6-degree-of-freedom motion may reduce sensory conflict, and therefore\ncybersickness. Explicit (questionnaires) and implicit (physiological responses)\nmeasurements were used to measure cybersickness symptoms during and after VR\nexposure. Our results confirmed a reduction in feelings of nausea during the AI\nsupplemented 6-degree-of-freedom motion VR. Through improving the congruency\nbetween visual and vestibular cues, users can experience more engaging,\nimmersive and safe virtual reality, which is critical for the application of VR\nin educational, medical, cultural and entertainment settings.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 19:06:15 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Arshad", "Iqra", ""], ["De Mello", "Paulo", ""], ["Ender", "Martin", ""], ["McEwen", "Jason D.", ""], ["Ferr\u00e9", "Elisa R.", ""]]}, {"id": "2103.03996", "submitter": "Shenyu Xu", "authors": "Jian Zhao, Shenyu Xu, Senthil Chandrasegaran, Chris Bryan, Fan Du,\n  Aditi Mishra, Xin Qian, Yiran Li, Kwan-Liu Ma", "title": "ChartStory: Automated Partitioning, Layout, and Captioning of Charts\n  into Comic-Style Narratives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual data storytelling is gaining importance as a means of presenting\ndata-driven information or analysis results, especially to the general public.\nThis has resulted in design principles being proposed for data-driven\nstorytelling, and new authoring tools being created to aid such storytelling.\nHowever, data analysts typically lack sufficient background in design and\nstorytelling to make effective use of these principles and authoring tools. To\nassist this process, we present ChartStory for crafting data stories from a\ncollection of user-created charts, using a style akin to comic panels to imply\nthe underlying sequence and logic of data-driven narratives. Our approach is to\noperationalize established design principles into an advanced pipeline which\ncharacterizes charts by their properties and similarity, and recommends ways to\npartition, layout, and caption story pieces to serve a narrative. ChartStory\nalso augments this pipeline with intuitive user interactions for visual\nrefinement of generated data comics. We extensively and holistically evaluate\nChartStory via a trio of studies. We first assess how the tool supports data\ncomic creation in comparison to a manual baseline tool. Data comics from this\nstudy are subsequently compared and evaluated to ChartStory's automated\nrecommendations by a team of narrative visualization practitioners. This is\nfollowed by a pair of interview studies with data scientists using their own\ndatasets and charts who provide an additional assessment of the system. We find\nthat ChartStory provides cogent recommendations for narrative generation,\nresulting in data comics that compare favorably to manually-created ones.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 00:24:32 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 20:52:34 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhao", "Jian", ""], ["Xu", "Shenyu", ""], ["Chandrasegaran", "Senthil", ""], ["Bryan", "Chris", ""], ["Du", "Fan", ""], ["Mishra", "Aditi", ""], ["Qian", "Xin", ""], ["Li", "Yiran", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2103.04044", "submitter": "Zijie Wang", "authors": "Zijie J. Wang, Dongjin Choi, Shenyu Xu, Diyi Yang", "title": "Putting Humans in the Natural Language Processing Loop: A Survey", "comments": "The paper is accepted to the HCI+NLP workshop at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we design Natural Language Processing (NLP) systems that learn from\nhuman feedback? There is a growing research body of Human-in-the-loop (HITL)\nNLP frameworks that continuously integrate human feedback to improve the model\nitself. HITL NLP research is nascent but multifarious -- solving various NLP\nproblems, collecting diverse feedback from different people, and applying\ndifferent methods to learn from collected feedback. We present a survey of HITL\nNLP work from both Machine Learning (ML) and Human-Computer Interaction (HCI)\ncommunities that highlights its short yet inspiring history, and thoroughly\nsummarize recent frameworks focusing on their tasks, goals, human interactions,\nand feedback learning methods. Finally, we discuss future directions for\nintegrating human feedback in the NLP development loop.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 06:26:00 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wang", "Zijie J.", ""], ["Choi", "Dongjin", ""], ["Xu", "Shenyu", ""], ["Yang", "Diyi", ""]]}, {"id": "2103.04083", "submitter": "Muhao Chen", "authors": "Changping Meng, Muhao Chen, Jie Mao, Jennifer Neville", "title": "ReadNet: A Hierarchical Transformer Framework for Web Article\n  Readability Analysis", "comments": "ECIR 2020", "journal-ref": null, "doi": "10.1007/978-3-030-45439-5_3", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the readability of articles has been an important sociolinguistic\ntask. Addressing this task is necessary to the automatic recommendation of\nappropriate articles to readers with different comprehension abilities, and it\nfurther benefits education systems, web information systems, and digital\nlibraries. Current methods for assessing readability employ empirical measures\nor statistical learning techniques that are limited by their ability to\ncharacterize complex patterns such as article structures and semantic meanings\nof sentences. In this paper, we propose a new and comprehensive framework which\nuses a hierarchical self-attention model to analyze document readability. In\nthis model, measurements of sentence-level difficulty are captured along with\nthe semantic meanings of each sentence. Additionally, the sentence-level\nfeatures are incorporated to characterize the overall readability of an article\nwith consideration of article structures. We evaluate our proposed approach on\nthree widely-used benchmark datasets against several strong baseline\napproaches. Experimental results show that our proposed method achieves the\nstate-of-the-art performance on estimating the readability for various web\narticles and literature.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 09:42:23 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Meng", "Changping", ""], ["Chen", "Muhao", ""], ["Mao", "Jie", ""], ["Neville", "Jennifer", ""]]}, {"id": "2103.04097", "submitter": "No\\'e Tits", "authors": "No\\'e Tits, Kevin El Haddad and Thierry Dutoit", "title": "Analysis and Assessment of Controllability of an Expressive Deep\n  Learning-based TTS system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we study the controllability of an Expressive TTS system\ntrained on a dataset for a continuous control. The dataset is the Blizzard 2013\ndataset based on audiobooks read by a female speaker containing a great\nvariability in styles and expressiveness. Controllability is evaluated with\nboth an objective and a subjective experiment. The objective assessment is\nbased on a measure of correlation between acoustic features and the dimensions\nof the latent space representing expressiveness. The subjective assessment is\nbased on a perceptual experiment in which users are shown an interface for\nControllable Expressive TTS and asked to retrieve a synthetic utterance whose\nexpressiveness subjectively corresponds to that a reference utterance.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 11:06:13 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tits", "No\u00e9", ""], ["Haddad", "Kevin El", ""], ["Dutoit", "Thierry", ""]]}, {"id": "2103.04222", "submitter": "Adam Goodkind", "authors": "Adam Goodkind", "title": "TypeShift: A User Interface for Visualizing the Typing Production\n  Process", "comments": "7 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  TypeShift is a tool for visualizing linguistic patterns in the timing of\ntyping production. Language production is a complex process which draws on\nlinguistic, cognitive and motor skills. By visualizing holistic trends in the\ntyping process, TypeShift aims to elucidate the often noisy information signals\nthat are used to represent typing patterns, both at the word-level and\ncharacter-level. It accomplishes this by enabling a researcher to compare and\ncontrast specific linguistic phenomena, and compare an individual typing\nsession to multiple group averages. Finally, although TypeShift was originally\ndesigned for typing data, it can easy be adapted to accommodate speech data, as\nwell. A web demo is available at https://angoodkind.shinyapps.io/TypeShift/.\nThe source code can be accessed at https://github.com/angoodkind/TypeShift.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 00:59:31 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Goodkind", "Adam", ""]]}, {"id": "2103.04380", "submitter": "Leonard Yoon", "authors": "Leonard Yoon, Dongseok Yang, Choongho Chung, Sung-Hee Lee", "title": "A Full Body Avatar-Based Telepresence System for Dissimilar Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel mixed reality (MR) telepresence system enabling a local\nuser to interact with a remote user through full-body avatars in their own\nrooms. If the remote rooms have different sizes and furniture arrangements,\ndirectly applying a user's motion to an avatar leads to a mismatch of placement\nand deictic gesture. To overcome this problem, we retarget the placement, arm\ngesture, and head movement of a local user to an avatar in a remote room to\npreserve a local user's environment and interaction context. This allows\navatars to utilize real furniture and interact with a local user and shared\nobjects as if they were in the same room. This paper describes our system's\ndesign and implementation in detail and a set of example scenarios in the\nliving room and office room. A qualitative user study delves into a user\nexperience, challenges, and possible extensions of the proposed system.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 15:43:57 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Yoon", "Leonard", ""], ["Yang", "Dongseok", ""], ["Chung", "Choongho", ""], ["Lee", "Sung-Hee", ""]]}, {"id": "2103.04429", "submitter": "Anastasia Danilova", "authors": "Anastasia Danilova, Alena Naiakshina, Stefan Horstmann, Matthew Smith", "title": "Do you really code? Designing and Evaluating Screening Questions for\n  Online Surveys with Programmers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recruiting professional programmers in sufficient numbers for research\nstudies can be challenging because they often cannot spare the time, or due to\ntheir geographical distribution and potentially the cost involved. Online\nplatforms such as Clickworker or Qualtrics do provide options to recruit\nparticipants with programming skill; however, misunderstandings and fraud can\nbe an issue. This can result in participants without programming skill taking\npart in studies and surveys. If these participants are not detected, they can\ncause detrimental noise in the survey data. In this paper, we develop screener\nquestions that are easy and quick to answer for people with programming skill\nbut difficult to answer correctly for those without. In order to evaluate our\nquestionnaire for efficacy and efficiency, we recruited several batches of\nparticipants with and without programming skill and tested the questions. In\nour batch 42% of Clickworkers stating that they have programming skill did not\nmeet our criteria and we would recommend filtering these from studies. We also\nevaluated the questions in an adversarial setting. We conclude with a set of\nrecommended questions which researchers can use to recruit participants with\nprogramming skill from online platforms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 18:59:44 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Danilova", "Anastasia", ""], ["Naiakshina", "Alena", ""], ["Horstmann", "Stefan", ""], ["Smith", "Matthew", ""]]}, {"id": "2103.04437", "submitter": "Alex Brandsen", "authors": "Alex Brandsen, Suzan Verberne, Karsten Lambers, Milco Wansleeben", "title": "Usability Evaluation for Online Professional Search in the Dutch\n  Archaeology Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents AGNES, the first information retrieval system for\narchaeological grey literature, allowing full-text search of these long\narchaeological documents.\n  This search system has a web interface that allows archaeology professionals\nand scholars to search through a collection of over 60,000 Dutch excavation\nreports, totalling 361 million words. We conducted a user study for the\nevaluation of AGNES's search interface, with a small but diverse user group.\nThe evaluation was done by screen capturing and a think aloud protocol,\ncombined with a user interface feedback questionnaire. The evaluation covered\nboth controlled use (completion of a pre-defined task) as well as free use\n(completion of a freely chosen task). The free use allows us to study the\ninformation needs of archaeologists, as well as their interactions with the\nsearch system. We conclude that: (1) the information needs of archaeologists\nare typically recall-oriented, often requiring a list of items as answer; (2)\nthe users prefer the use of free-text queries over metadata filters, confirming\nthe value of a free-text search system; (3) the compilation of a diverse user\ngroup contributed to the collection of diverse issues as feedback for improving\nthe system. We are currently refining AGNES's user interface and improving its\nprecision for archaeological entities, so that AGNES will help archaeologists\nto answer their research questions more effectively and efficiently, leading to\na more coherent narrative of the past.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 19:48:35 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Brandsen", "Alex", ""], ["Verberne", "Suzan", ""], ["Lambers", "Karsten", ""], ["Wansleeben", "Milco", ""]]}, {"id": "2103.04439", "submitter": "Tianwei Ni", "authors": "Tianwei Ni, Huao Li, Siddharth Agrawal, Suhas Raja, Fan Jia, Yikang\n  Gui, Dana Hughes, Michael Lewis, Katia Sycara", "title": "Adaptive Agent Architecture for Real-time Human-Agent Teaming", "comments": "The first three authors contributed equally. In AAAI 2021 Workshop on\n  Plan, Activity, and Intent Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Teamwork is a set of interrelated reasoning, actions and behaviors of team\nmembers that facilitate common objectives. Teamwork theory and experiments have\nresulted in a set of states and processes for team effectiveness in both\nhuman-human and agent-agent teams. However, human-agent teaming is less well\nstudied because it is so new and involves asymmetry in policy and intent not\npresent in human teams. To optimize team performance in human-agent teaming, it\nis critical that agents infer human intent and adapt their polices for smooth\ncoordination. Most literature in human-agent teaming builds agents referencing\na learned human model. Though these agents are guaranteed to perform well with\nthe learned model, they lay heavy assumptions on human policy such as\noptimality and consistency, which is unlikely in many real-world scenarios. In\nthis paper, we propose a novel adaptive agent architecture in human-model-free\nsetting on a two-player cooperative game, namely Team Space Fortress (TSF).\nPrevious human-human team research have shown complementary policies in TSF\ngame and diversity in human players' skill, which encourages us to relax the\nassumptions on human policy. Therefore, we discard learning human models from\nhuman data, and instead use an adaptation strategy on a pre-trained library of\nexemplar policies composed of RL algorithms or rule-based methods with minimal\nassumptions of human behavior. The adaptation strategy relies on a novel\nsimilarity metric to infer human policy and then selects the most complementary\npolicy in our library to maximize the team performance. The adaptive agent\narchitecture can be deployed in real-time and generalize to any off-the-shelf\nstatic agents. We conducted human-agent experiments to evaluate the proposed\nadaptive agent framework, and demonstrated the suboptimality, diversity, and\nadaptability of human policies in human-agent teams.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 20:08:09 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ni", "Tianwei", ""], ["Li", "Huao", ""], ["Agrawal", "Siddharth", ""], ["Raja", "Suhas", ""], ["Jia", "Fan", ""], ["Gui", "Yikang", ""], ["Hughes", "Dana", ""], ["Lewis", "Michael", ""], ["Sycara", "Katia", ""]]}, {"id": "2103.04544", "submitter": "Toby Chong", "authors": "Toby Chong, Nolwenn Maudet, Katsuki Harima, Takeo Igarashi", "title": "Exploring a Makeup Support System for Transgender Passing based on\n  Automatic Gender Recognition", "comments": "Accepted to CHI2021. Project Page:\n  https://sites.google.com/view/flyingcolor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  How to handle gender with machine learning is a controversial topic. A\ngrowing critical body of research brought attention to the numerous issues\ntransgender communities face with the adoption of current automatic gender\nrecognition (AGR) systems. In contrast, we explore how such technologies could\npotentially be appropriated to support transgender practices and needs,\nespecially in non-Western contexts like Japan. We designed a virtual makeup\nprobe to assist transgender individuals with passing, that is to be perceived\nas the gender they identify as. To understand how such an application might\nsupport expressing transgender individuals gender identity or not, we\ninterviewed 15 individuals in Tokyo and found that in the right context and\nunder strict conditions, AGR based systems could assist transgender passing.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 04:43:10 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chong", "Toby", ""], ["Maudet", "Nolwenn", ""], ["Harima", "Katsuki", ""], ["Igarashi", "Takeo", ""]]}, {"id": "2103.04694", "submitter": "Changkun Ou", "authors": "Changkun Ou, Daniel Buschek, Malin Eiband, Andreas Butz", "title": "Modeling Web Browsing Behavior across Tabs and Websites with Tracking\n  and Prediction on the Client Side", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Clickstreams on individual websites have been studied for decades to gain\ninsights into user interests and to improve website experiences. This paper\nproposes and examines a novel sequence modeling approach for web clickstreams,\nthat also considers multi-tab branching and backtracking actions across\nwebsites to capture the full action sequence of a user while browsing. All of\nthis is done using machine learning on the client side to obtain a more\ncomprehensive view and at the same time preserve privacy. We evaluate our\nformalism with a model trained on data collected in a user study with three\ndifferent browsing tasks based on different human information seeking\nstrategies from psychological literature. Our results show that the model can\nsuccessfully distinguish between browsing behaviors and correctly predict\nfuture actions. A subsequent qualitative analysis identified five common web\nbrowsing patterns from our collected behavior data, which help to interpret the\nmodel. More generally, this illustrates the power of overparameterization in ML\nand offers a new way of modeling, reasoning with, and prediction of observable\nsequential human interaction behaviors.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:11:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ou", "Changkun", ""], ["Buschek", "Daniel", ""], ["Eiband", "Malin", ""], ["Butz", "Andreas", ""]]}, {"id": "2103.04725", "submitter": "Monica Agrawal", "authors": "Ariel Levy, Monica Agrawal, Arvind Satyanarayan, David Sontag", "title": "Assessing the Impact of Automated Suggestions on Decision Making: Domain\n  Experts Mediate Model Errors but Take Less Initiative", "comments": "Fixed minor formatting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated decision support can accelerate tedious tasks as users can focus\ntheir attention where it is needed most. However, a key concern is whether\nusers overly trust or cede agency to automation. In this paper, we investigate\nthe effects of introducing automation to annotating clinical texts--a\nmulti-step, error-prone task of identifying clinical concepts (e.g.,\nprocedures) in medical notes, and mapping them to labels in a large ontology.\nWe consider two forms of decision aid: recommending which labels to map\nconcepts to, and pre-populating annotation suggestions. Through laboratory\nstudies, we find that 18 clinicians generally build intuition of when to rely\non automation and when to exercise their own judgement. However, when presented\nwith fully pre-populated suggestions, these expert users exhibit less agency:\naccepting improper mentions, and taking less initiative in creating additional\nannotations. Our findings inform how systems and algorithms should be designed\nto mitigate the observed issues.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 13:01:02 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 16:45:50 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Levy", "Ariel", ""], ["Agrawal", "Monica", ""], ["Satyanarayan", "Arvind", ""], ["Sontag", "David", ""]]}, {"id": "2103.04757", "submitter": "Jakob Schoeffer", "authors": "Jakob Schoeffer, Yvette Machowski, Niklas Kuehl", "title": "A Study on Fairness and Trust Perceptions in Automated Decision Making", "comments": "Joint Proceedings of the ACM IUI 2021 Workshops, April 13--17, 2021,\n  College Station, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated decision systems are increasingly used for consequential decision\nmaking -- for a variety of reasons. These systems often rely on sophisticated\nyet opaque models, which do not (or hardly) allow for understanding how or why\na given decision was arrived at. This is not only problematic from a legal\nperspective, but non-transparent systems are also prone to yield undesirable\n(e.g., unfair) outcomes because their sanity is difficult to assess and\ncalibrate in the first place. In this work, we conduct a study to evaluate\ndifferent attempts of explaining such systems with respect to their effect on\npeople's perceptions of fairness and trustworthiness towards the underlying\nmechanisms. A pilot study revealed surprising qualitative insights as well as\npreliminary significant effects, which will have to be verified, extended and\nthoroughly discussed in the larger main study.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 13:57:31 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Schoeffer", "Jakob", ""], ["Machowski", "Yvette", ""], ["Kuehl", "Niklas", ""]]}, {"id": "2103.04899", "submitter": "Shawn Jones", "authors": "Shawn M. Jones and Michele C. Weigle and Martin Klein and Michael L.\n  Nelson", "title": "Automatically Selecting Striking Images for Social Cards", "comments": "10 pages, 5 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To allow previewing a web page, social media platforms have developed social\ncards: visualizations consisting of vital information about the underlying\nresource. At a minimum, social cards often include features such as the web\nresource's title, text summary, striking image, and domain name. News and\nscholarly articles on the web are frequently subject to social card creation\nwhen being shared on social media. However, we noticed that not all web\nresources offer sufficient metadata elements to enable appealing social cards.\nFor example, the COVID-19 emergency has made it clear that scholarly articles,\nin particular, are at an aesthetic disadvantage in social media platforms when\ncompared to their often more flashy disinformation rivals. Also, social cards\nare often not generated correctly for archived web resources, including pages\nthat lack or predate standards for specifying striking images. With these\nobservations, we are motivated to quantify the levels of inclusion of required\nmetadata in web resources, its evolution over time for archived resources, and\ncreate and evaluate an algorithm to automatically select a striking image for\nsocial cards. We find that more than 40% of archived news articles sampled from\nthe NEWSROOM dataset and 22% of scholarly articles sampled from the PubMed\nCentral dataset fail to supply striking images. We demonstrate that we can\nautomatically predict the striking image with a Precision@1 of 0.83 for news\narticles from NEWSROOM and 0.78 for scholarly articles from the open access\njournal PLOS ONE.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:01:45 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Jones", "Shawn M.", ""], ["Weigle", "Michele C.", ""], ["Klein", "Martin", ""], ["Nelson", "Michael L.", ""]]}, {"id": "2103.04924", "submitter": "Justas Brazauskas", "authors": "Justas Brazauskas, Rohit Verma, Vadim Safronov, Matthew Danish, Jorge\n  Merino, Xiang Xie, Ian Lewis, Richard Mortier", "title": "Data Management for Building Information Modelling in a Real-Time\n  Adaptive City Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legacy Building Information Modelling (BIM) systems are not designed to\nprocess the high-volume, high-velocity data emitted by in-building\nInternet-of-Things (IoT) sensors. Historical lack of consideration for the\nreal-time nature of such data means that outputs from such BIM systems\ntypically lack the timeliness necessary for enacting decisions as a result of\npatterns emerging in the sensor data. Similarly, as sensors are increasingly\ndeployed in buildings, antiquated Building Management Systems (BMSs) struggle\nto maintain functionality as interoperability challenges increase. In\ncombination these motivate the need to fill an important gap in smart buildings\nresearch, to enable faster adoption of these technologies, by combining BIM,\nBMS and sensor data. This paper describes the data architecture of the Adaptive\nCity Platform, designed to address these combined requirements by enabling\nintegrated BIM and real-time sensor data analysis across both time and space.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:36:54 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Brazauskas", "Justas", ""], ["Verma", "Rohit", ""], ["Safronov", "Vadim", ""], ["Danish", "Matthew", ""], ["Merino", "Jorge", ""], ["Xie", "Xiang", ""], ["Lewis", "Ian", ""], ["Mortier", "Richard", ""]]}, {"id": "2103.04940", "submitter": "Murtuza Shergadwala", "authors": "Murtuza N. Shergadwala and Magy Seif El-Nasr", "title": "Esports Agents with a Theory of Mind: Towards Better Engagement,\n  Education, and Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The role of AI in esports is shifting from leveraging games as a testbed for\nimproving AI algorithms to addressing the needs of the esports players such as\nenhancing their gaming experience, esports skills, and providing coaching. For\nAI to be able to effectively address such needs in esports, AI agents require a\ntheory of mind, that is, the ability to infer players' tactics and intents. To\nthat end, in this position paper, we argue for human-in-the-loop approaches for\nthe discovery and computational embedding of the theory of mind within\nbehavioral models of esports players. We discuss that such approaches can be\nenabled by player-centric investigations on situated cognition that will expand\nour understanding of the cognitive and other unobservable factors that\ninfluence esports players' behaviors. We conclude by discussing the\nimplications of such a research direction in esports as well as broader\nimplications in engineering design and design education.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:59:08 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Shergadwala", "Murtuza N.", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "2103.05154", "submitter": "Daniel Omeiza A", "authors": "Daniel Omeiza, Helena Webb, Marina Jirotka, Lars Kunze", "title": "Explanations in Autonomous Driving: A Survey", "comments": "18 pages, 5 Tables and 3 Figures. Submitted to the IEEE Transaction\n  on Intelligent Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The automotive industry is seen to have witnessed an increasing level of\ndevelopment in the past decades; from manufacturing manually operated vehicles\nto manufacturing vehicles with high level of automation. With the recent\ndevelopments in Artificial Intelligence (AI), automotive companies now employ\nhigh performance AI models to enable vehicles to perceive their environment and\nmake driving decisions with little or no influence from a human. With the hope\nto deploy autonomous vehicles (AV) on a commercial scale, the acceptance of AV\nby society becomes paramount and may largely depend on their degree of\ntransparency, trustworthiness, and compliance to regulations. The assessment of\nthese acceptance requirements can be facilitated through the provision of\nexplanations for AVs' behaviour. Explainability is therefore seen as an\nimportant requirement for AVs. AVs should be able to explain what they have\n'seen', done and might do in environments where they operate. In this paper, we\nprovide a comprehensive survey of the existing work in explainable autonomous\ndriving. First, we open by providing a motivation for explanations and\nexamining existing standards related to AVs. Second, we identify and categorise\nthe different stakeholders involved in the development, use, and regulation of\nAVs and show their perceived need for explanation. Third, we provide a taxonomy\nof explanations and reviewed previous work on explanation in the different AV\noperations. Finally, we draw a close by pointing out pertinent challenges and\nfuture research directions. This survey serves to provide fundamental knowledge\nrequired of researchers who are interested in explanation in autonomous\ndriving.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 00:31:30 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 15:51:59 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Omeiza", "Daniel", ""], ["Webb", "Helena", ""], ["Jirotka", "Marina", ""], ["Kunze", "Lars", ""]]}, {"id": "2103.05200", "submitter": "Rongkai Shi", "authors": "Rongkai Shi, Hai-Ning Liang, Yu Wu, Difeng Yu, Wenge Xu", "title": "Virtual Reality Sickness Mitigation Methods: A Comparative Study in a\n  Racing Game", "comments": null, "journal-ref": null, "doi": "10.1145/3451255", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using virtual reality (VR) head-mounted displays (HMDs) can induce VR\nsickness. VR sickness can cause strong discomfort, decrease users' presence and\nenjoyment, especially in games, shorten the duration of the VR experience, and\ncan even pose health risks. Previous research has explored different VR\nsickness mitigation methods by adding visual effects or elements. Field of View\n(FOV) reduction, Depth of Field (DOF) blurring, and adding a rest frame into\nthe virtual environment are examples of such methods. Although useful in some\ncases, they might result in information loss. This research is the first to\ncompare VR sickness, presence, workload to complete a search task, and\ninformation loss of these three VR sickness mitigation methods in a racing game\nwith two levels of control. To do this, we conducted a mixed factorial user\nstudy (N = 32) with degree of control as the between-subjects factor and the VR\nsickness mitigation techniques as the within-subjects factor. Participants were\nrequired to find targets with three difficulty levels while steering or not\nsteering a car in a virtual environment. Our results show that there are no\nsignificant differences in VR sickness, presence and workload among these\ntechniques under two levels of control in our VR racing game. We also found\nthat changing FOV dynamically or using DOF blur effects would result in\ninformation loss while adding a target reticule as a rest frame would not.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 03:31:12 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Shi", "Rongkai", ""], ["Liang", "Hai-Ning", ""], ["Wu", "Yu", ""], ["Yu", "Difeng", ""], ["Xu", "Wenge", ""]]}, {"id": "2103.05296", "submitter": "Gianluca Schiavo", "authors": "Gianluca Schiavo, Nadia Mana, Ornella Mich, Massimo Zancanaro, Remo\n  Job", "title": "Attention-driven read-aloud technology increases reading comprehension\n  in children with reading disabilities", "comments": null, "journal-ref": "J Comput Assist Learn. 2021 1-12", "doi": "10.1111/jcal.12530", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The paper presents the design of an assistive reading tool that integrates\nread-aloud technology with eye-tracking to regulate the speed of reading and\nsupport struggling readers in following the text while listening to it. The\npaper describes the design rationale of this approach, following the theory of\nauditory-visual integration, in terms of an automatic self-adaptable technique\nbased on the reader's gaze that provides an individualized interaction\nexperience. This tool has been assessed in a controlled experiment with 20\nchildren (aged 8-10 years) with a diagnosis of dyslexia and a control group of\n20 children with typical reading abilities. The results show that children with\nreading difficulties improved their comprehension scores by 24% measured on a\nstandardized instrument for the assessment of reading comprehension, and that\nchildren with more inaccurate reading (N=9) tended to benefit more. The\nfindings are discussed in terms of a better integration between audio and\nvisual text information, paving the way to improve standard read-aloud\ntechnology with gaze-contingency and self-adaptable techniques to personalize\nthe reading experience.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:52:41 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Schiavo", "Gianluca", ""], ["Mana", "Nadia", ""], ["Mich", "Ornella", ""], ["Zancanaro", "Massimo", ""], ["Job", "Remo", ""]]}, {"id": "2103.05309", "submitter": "Gianluca Schiavo", "authors": "Gianluca Schiavo, Ornella Mich, Michela Ferron, Nadia Mana", "title": "Trade-offs in the Design of Multimodal Interaction for Older Adults", "comments": null, "journal-ref": "Behaviour & Information Technology, 2020", "doi": "10.1080/0144929X.2020.1851768", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents key aspects and trade-offs that designers and\nHuman-Computer Interaction practitioners might encounter when designing\nmultimodal interaction for older adults. The paper gathers literature on\nmultimodal interaction and assistive technology, and describes a set of design\nchallenges specific for older users. Building on these main design challenges,\nfour trade-offs in the design of multimodal technology for this target group\nare presented and discussed. To highlight the relevance of the trade-offs in\nthe design process of multimodal technology for older adults, two of the four\nreported trade-offs are illustrated with two user studies that explored mid-air\nand speech-based interaction with a tablet device. The first study investigates\nthe design trade-offs related to redundant multimodal commands in older,\nmiddle-aged and younger adults, whereas the second one investigates the design\nchoices related to the definition of a set of mid-air one-hand gestures and\nvoice input commands. Further reflections highlight the design trade-offs that\nsuch considerations bring in the process, presenting an overview of the design\nchoices involved and of their potential consequences.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 09:12:16 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Schiavo", "Gianluca", ""], ["Mich", "Ornella", ""], ["Ferron", "Michela", ""], ["Mana", "Nadia", ""]]}, {"id": "2103.05560", "submitter": "Yan Feng", "authors": "Yan Feng, Dorine Duives, Serge Hoogendoorn", "title": "Development of a VR tool to study pedestrian route and exit choice\n  behaviour in a multi-story building", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Although route and exit choice in complex buildings are important aspects of\npedestrian behaviour, studies predominantly investigated pedestrian movement in\na single level. This paper presents an innovative VR tool that was designed to\ninvestigate pedestrian route and exit choice in a multi-story building. This\ntool supports free navigation and collects pedestrian walking trajectories,\nhead movements and gaze points automatically. An experiment was conducted to\nevaluate the VR tool from objective standpoints (i.e., pedestrian behaviour)\nand subjective standpoints (i.e., the feeling of presence, system usability,\nsimulation sickness). The results show that the VR tool allows for accurate\ncollection of pedestrian behavioural data in the complex building. Moreover,\nthe results of the questionnaire report high realism of the virtual\nenvironment, high immersive feeling, high usability, and low simulator\nsickness. This paper contributes by showcasing an innovative approach of\napplying VR technologies to study pedestrian behaviour in complex and realistic\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:49:46 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Feng", "Yan", ""], ["Duives", "Dorine", ""], ["Hoogendoorn", "Serge", ""]]}, {"id": "2103.05561", "submitter": "Biplav Srivastava", "authors": "Biplav Srivastava", "title": "Did Chatbots Miss Their 'Apollo Moment'? A Survey of the Potential, Gaps\n  and Lessons from Using Collaboration Assistants During COVID-19", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial Intelligence (AI) technologies have long been positioned as a tool\nto provide crucial data-driven decision support to people. In this survey\npaper, we look at how AI in general, and collaboration assistants (CAs or\nchatbots for short) in particular, have been used during a true global exigency\n- the COVID-19 pandemic. The key observation is that chatbots missed their\n\"Apollo moment\" when they could have really provided contextual, personalized,\nreliable decision support at scale that the state-of-the-art makes possible. We\nreview the existing capabilities that are feasible and methods, identify the\npotential that chatbots could have met, the use-cases they were deployed on,\nthe challenges they faced and gaps that persisted, and draw lessons that, if\nimplemented, would make them more relevant in future health emergencies.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 19:08:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Srivastava", "Biplav", ""]]}, {"id": "2103.05562", "submitter": "Samrat Kumar Dey", "authors": "Khandaker Mohammad Mohi Uddin, Samrat Kumar Dey, Gias Uddin Parvez,\n  Ayesha Siddika Mukta, and Uzzal Kumar Acharjee", "title": "MirrorME: Implementation of an IoT based Smart Mirror through Facial\n  Recognition and Personalized Information Recommendation Algorithm", "comments": "17 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We are living in the era of the fourth industrial revolution, which also\ntreated as 4IR or Industry 4.0. Generally, 4IR considered as the mixture of\nrobotics, artificial intelligence (AI), quantum computing, the Internet of\nThings (IoT) and other frontier technologies. It is obvious that nowadays a\nplethora of smart devices is providing services to make the daily life of human\neasier. However, in the morning most people around the globe use a traditional\nmirror while preparing themselves for daily task. The aim is to build a\nlow-cost intelligent mirror system that can display a variety of details based\non user recommendations. Therefore, in this article, Internet of Things (IoT)\nand AI-based smart mirror is introduced that will support the users to receive\nthe necessary daily update of weather information, date, time, calendar, to-do\nlist, updated news headlines, traffic updates, COVID-19 cases status and so on.\nMoreover, a face detection method also implemented with the smart mirror to\nconstruct the architecture more secure. Our proposed MirrorME application\nprovides a success rate of nearly 87% in interacting with the features of face\nrecognition and voice input. The mirror is capable of delivering multimedia\nfacilities while maintaining high levels of security within the device.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 20:19:34 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Uddin", "Khandaker Mohammad Mohi", ""], ["Dey", "Samrat Kumar", ""], ["Parvez", "Gias Uddin", ""], ["Mukta", "Ayesha Siddika", ""], ["Acharjee", "Uzzal Kumar", ""]]}, {"id": "2103.05563", "submitter": "Ahmet Orun", "authors": "Ahmet Orun", "title": "Low-level cognitive skill transfer between two individuals' minds via\n  computer game-based framework", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The novel technique introduced here aims to accomplish the first stage of\ntransferring low-level cognitive skills between two individuals (e.g. from\nexpert to learner) to ease the consecutive higher level declarative learning\nprocess for the target \"learner\" individual in a game environment. Such\nlow-level cognitive skill is associated with the procedural knowledge and\nestablished at low-level of mind which can be unveiled and transferred by only\na novel technique (rather than by a traditional educational environment ) like\na highly interactive computer game domain in which a user exposes his/her\nunconscious mind behaviors via the game-hero non-deliberately during the game\nsessions. The cognitive data exposed by the game-hero would be recorded, and\nthen be modelled by the artificial intelligence technique like Bayesian\nnetworks for an early stage of cognitive skill transfer and the cognitive\nstimuli are also generated to be used as game agents to train the learner.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 01:52:16 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Orun", "Ahmet", ""]]}, {"id": "2103.05668", "submitter": "Christoforos Mavrogiannis", "authors": "Christoforos Mavrogiannis, Francesca Baldini, Allan Wang, Dapeng Zhao,\n  Pete Trautman, Aaron Steinfeld, Jean Oh", "title": "Core Challenges of Social Robot Navigation: A Survey", "comments": "Minor formatting edits (36 pages, 3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot navigation in crowded public spaces is a complex task that requires\naddressing a variety of engineering and human factors challenges. These\nchallenges have motivated a great amount of research resulting in important\ndevelopments for the fields of robotics and human-robot interaction over the\npast three decades. Despite the significant progress and the massive recent\ninterest, we observe a number of significant remaining challenges that prohibit\nthe seamless deployment of autonomous robots in public pedestrian environments.\nIn this survey article, we organize existing challenges into a set of\ncategories related to broader open problems in motion planning, behavior\ndesign, and evaluation methodologies. Within these categories, we review past\nwork, and offer directions for future research. Our work builds upon and\nextends earlier survey efforts by a) taking a critical perspective and\ndiagnosing fundamental limitations of adopted practices in the field and b)\noffering constructive feedback and ideas that we aspire will drive research in\nthe field over the coming decade.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:13:09 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 00:24:33 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Mavrogiannis", "Christoforos", ""], ["Baldini", "Francesca", ""], ["Wang", "Allan", ""], ["Zhao", "Dapeng", ""], ["Trautman", "Pete", ""], ["Steinfeld", "Aaron", ""], ["Oh", "Jean", ""]]}, {"id": "2103.05682", "submitter": "Abhijeet Krishnan", "authors": "Abhijeet Krishnan, Aaron Williams, Chris Martens", "title": "Towards Action Model Learning for Player Modeling", "comments": "7 pages, 1 figure", "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment. Vol. 16. No. 1. 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Player modeling attempts to create a computational model which accurately\napproximates a player's behavior in a game. Most player modeling techniques\nrely on domain knowledge and are not transferable across games. Additionally,\nplayer models do not currently yield any explanatory insight about a player's\ncognitive processes, such as the creation and refinement of mental models. In\nthis paper, we present our findings with using action model learning (AML), in\nwhich an action model is learned given data in the form of a play trace, to\nlearn a player model in a domain-agnostic manner. We demonstrate the utility of\nthis model by introducing a technique to quantitatively estimate how well a\nplayer understands the mechanics of a game. We evaluate an existing AML\nalgorithm (FAMA) for player modeling and develop a novel algorithm called\nBlackout that is inspired by player cognition. We compare Blackout with FAMA\nusing the puzzle game Sokoban and show that Blackout generates better player\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:32:30 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Krishnan", "Abhijeet", ""], ["Williams", "Aaron", ""], ["Martens", "Chris", ""]]}, {"id": "2103.05704", "submitter": "Christiane Gresse Von Wangenheim", "authors": "Daniel Baul\\'e, Christiane Gresse von Wangenheim, Aldo von Wangenheim,\n  Jean C. R. Hauck, Edson C. Vargas J\\'unior", "title": "Automatic code generation from sketches of mobile applications in\n  end-user development using Deep Learning", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common need for mobile application development by end-users or in computing\neducation is to transform a sketch of a user interface into wireframe code\nusing App Inventor, a popular block-based programming environment. As this task\nis challenging and time-consuming, we present the Sketch2aia approach that\nautomates this process. Sketch2aia employs deep learning to detect the most\nfrequent user interface components and their position on a hand-drawn sketch\ncreating an intermediate representation of the user interface and then\nautomatically generates the App Inventor code of the wireframe. The approach\nachieves an average user interface component classification accuracy of 87,72%\nand results of a preliminary user evaluation indicate that it generates\nwireframes that closely mirror the sketches in terms of visual similarity. The\napproach has been implemented as a web tool and can be used to support the\nend-user development of mobile applications effectively and efficiently as well\nas the teaching of user interface design in K-12.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 20:32:20 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Baul\u00e9", "Daniel", ""], ["von Wangenheim", "Christiane Gresse", ""], ["von Wangenheim", "Aldo", ""], ["Hauck", "Jean C. R.", ""], ["J\u00fanior", "Edson C. Vargas", ""]]}, {"id": "2103.05746", "submitter": "Andrea Bajcsy", "authors": "Andrea Bajcsy, Anand Siththaranjan, Claire J. Tomlin, Anca D. Dragan", "title": "Analyzing Human Models that Adapt Online", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive human models often need to adapt their parameters online from\nhuman data. This raises previously ignored safety-related questions for robots\nrelying on these models such as what the model could learn online and how\nquickly could it learn it. For instance, when will the robot have a confident\nestimate in a nearby human's goal? Or, what parameter initializations guarantee\nthat the robot can learn the human's preferences in a finite number of\nobservations? To answer such analysis questions, our key idea is to model the\nrobot's learning algorithm as a dynamical system where the state is the current\nmodel parameter estimate and the control is the human data the robot observes.\nThis enables us to leverage tools from reachability analysis and optimal\ncontrol to compute the set of hypotheses the robot could learn in finite time,\nas well as the worst and best-case time it takes to learn them. We demonstrate\nthe utility of our analysis tool in four human-robot domains, including\nautonomous driving and indoor navigation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 22:38:46 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Bajcsy", "Andrea", ""], ["Siththaranjan", "Anand", ""], ["Tomlin", "Claire J.", ""], ["Dragan", "Anca D.", ""]]}, {"id": "2103.05823", "submitter": "Aysja Johnson", "authors": "Aysja Johnson, Wai Keen Vong, Brenden M. Lake, Todd M. Gureckis", "title": "Fast and flexible: Human program induction in abstract reasoning tasks", "comments": "7 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Abstraction and Reasoning Corpus (ARC) is a challenging program induction\ndataset that was recently proposed by Chollet (2019). Here, we report the first\nset of results collected from a behavioral study of humans solving a subset of\ntasks from ARC (40 out of 1000). Although this subset of tasks contains\nconsiderable variation, our results showed that humans were able to infer the\nunderlying program and generate the correct test output for a novel test input\nexample, with an average of 80% of tasks solved per participant, and with 65%\nof tasks being solved by more than 80% of participants. Additionally, we find\ninteresting patterns of behavioral consistency and variability within the\naction sequences during the generation process, the natural language\ndescriptions to describe the transformations for each task, and the errors\npeople made. Our findings suggest that people can quickly and reliably\ndetermine the relevant features and properties of a task to compose a correct\nsolution. Future modeling work could incorporate these findings, potentially by\nconnecting the natural language descriptions we collected here to the\nunderlying semantics of ARC.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 02:18:21 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Johnson", "Aysja", ""], ["Vong", "Wai Keen", ""], ["Lake", "Brenden M.", ""], ["Gureckis", "Todd M.", ""]]}, {"id": "2103.05858", "submitter": "Jisheng Li", "authors": "Jisheng Li, Ziyu Wen, Sihan Li, Yikai Zhao, Bichuan Guo, Jiangtao Wen", "title": "Novel tile segmentation scheme for omnidirectional video", "comments": "Published in 2016 IEEE International Conference on Image Processing\n  (ICIP)", "journal-ref": null, "doi": "10.1109/ICIP.2016.7532381", "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regular omnidirectional video encoding technics use map projection to flatten\na scene from a spherical shape into one or several 2D shapes. Common projection\nmethods including equirectangular and cubic projection have varying levels of\ninterpolation that create a large number of non-information-carrying pixels\nthat lead to wasted bitrate. In this paper, we propose a tile based\nomnidirectional video segmentation scheme which can save up to 28% of pixel\narea and 20% of BD-rate averagely compared to the traditional equirectangular\nprojection based approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:49:18 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Li", "Jisheng", ""], ["Wen", "Ziyu", ""], ["Li", "Sihan", ""], ["Zhao", "Yikai", ""], ["Guo", "Bichuan", ""], ["Wen", "Jiangtao", ""]]}, {"id": "2103.05862", "submitter": "Katie Seaborn", "authors": "Katie Seaborn", "title": "Removing Gamification: A Research Agenda", "comments": "Accepted at CHI EA 2021", "journal-ref": null, "doi": "10.1145/3411763.3451695", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The effect of removing gamification elements from interactive systems has\nbeen a long-standing question in gamification research. Early work and\nfoundational theories raised concerns about the endurance of positive effects\nand the emergence of negative ones. Yet, nearly a decade later, no work to date\nhas sought consensus on these matters. Here, I offer a rapid review on the\nstate of the art and what is known about the impact of removing gamification. A\nsmall corpus of 8 papers published between 2012 and 2020 were found. Findings\nsuggest a mix of positive and negative effects related to removing\ngamification. Significantly, insufficient reporting, methodological weaknesses,\nlimited measures, and superficial interpretations of \"negative\" results prevent\nfirm conclusions. I offer a research agenda towards better understanding the\nnature of gamification removal. I end with a call for empirical and theoretical\nwork on illuminating the effects that may linger after systems are un-gamified.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:59:46 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 00:36:35 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Seaborn", "Katie", ""]]}, {"id": "2103.05984", "submitter": "Jens Grubert", "authors": "Jens Grubert", "title": "Mixed Reality Interaction Techniques", "comments": "To appear in the Springer Handbook of Augmented Reality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter gives an overview of interaction techniques for mixed reality\nincluding augmented and virtual reality (AR/VR). Various modalities for input\nand output are discussed. Specifically, techniques for tangible and\nsurface-based interaction, gesture-based, pen-based, gaze-based, keyboard and\nmouse-based, as well as haptic interaction are discussed. Furthermore, the\ncombination of multiple modalities in multisensory and multimodal interaction,\nas well as interaction using multiple physical or virtual displays, are\npresented. Finally, interaction with intelligent virtual agents is considered.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 10:47:10 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Grubert", "Jens", ""]]}, {"id": "2103.06063", "submitter": "V\\'ictor Hugo Mas\\'ias H.", "authors": "V\\'ictor H. Mas\\'ias, Fernando Crespo, Pilar Navarro R., Razan Masood,\n  Nicole C. Kr\\\"amer, and H. Ulrich Hoppe", "title": "On spatial variation in the detectability and density of social media\n  user protest supporters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although much has been published regarding street protests on social media,\nfew works have attempted to characterize social media users' spatial behavior\nin such events. The research reported here uses spatial capture-recapture\nmethods to determine the influence of the built environment, physical proximity\nto protest location, and collective posting rhythm on variations in users'\nspatial detectability and density during a protest in Mexico City. The\nbest-obtained model, together with explaining the spatial density of users,\nshows that there is high variability in the detectability of social media user\nprotest supporters and that the collective posting rhythm and the day of\nobservation are significant explanatory factors. The implication is that\nstudies of collective spatial behavior would benefit by focussing on users'\nactivity centres and their urban environment, rather than their physical\nproximity to the protest location, the latter being unable to adequately\nexplain spatial variations in users' detectability and density during the\nprotest event.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:08:08 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Mas\u00edas", "V\u00edctor H.", ""], ["Crespo", "Fernando", ""], ["R.", "Pilar Navarro", ""], ["Masood", "Razan", ""], ["Kr\u00e4mer", "Nicole C.", ""], ["Hoppe", "H. Ulrich", ""]]}, {"id": "2103.06160", "submitter": "Chao Zhang", "authors": "Chao Zhang, Shihan Wang, Henk Aarts and Mehdi Dastani", "title": "Using Cognitive Models to Train Warm Start Reinforcement Learning Agents\n  for Human-Computer Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning (RL) agents in human-computer interactions\napplications require repeated user interactions before they can perform well.\nTo address this \"cold start\" problem, we propose a novel approach of using\ncognitive models to pre-train RL agents before they are applied to real users.\nAfter briefly reviewing relevant cognitive models, we present our general\nmethodological approach, followed by two case studies from our previous and\nongoing projects. We hope this position paper stimulates conversations between\nRL, HCI, and cognitive science researchers in order to explore the full\npotential of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:20:02 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Zhang", "Chao", ""], ["Wang", "Shihan", ""], ["Aarts", "Henk", ""], ["Dastani", "Mehdi", ""]]}, {"id": "2103.06171", "submitter": "Adina M. Panchea PhD", "authors": "Adina M. Panchea, Dominic L\\'etourneau, Simon Bri\\`ere, Mathieu Hamel,\n  Marc-Antoine Maheux, C\\'edric Godin, Michel Tousignant, Mathieu Labb\\'e,\n  Fran\\c{c}ois Ferland, Fran\\c{c}ois Grondin, Fran\\c{c}ois Michaud", "title": "OpenTera: A Microservice Architecture Solution for Rapid Prototyping of\n  Robotic Solutions to COVID-19 Challenges in Care Facilities", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As telecommunications technology progresses, telehealth frameworks are\nbecoming more widely adopted in the context of long-term care (LTC) for older\nadults, both in care facilities and in homes. Today, robots could assist\nhealthcare workers when they provide care to elderly patients, who constitute a\nparticularly vulnerable population during the COVID-19 pandemic. Previous work\non user-centered design of assistive technologies in LTC facilities for seniors\nhas identified positive impacts. The need to deal with the effects of the\nCOVID-19 pandemic emphasizes the benefits of this approach, but also highlights\nsome new challenges for which robots could be interesting solutions to be\ndeployed in LTC facilities. This requires customization of telecommunication\nand audio/video/data processing to address specific clinical requirements and\nneeds. This paper presents OpenTera, an open source telehealth framework,\naiming to facilitate prototyping of such solutions by software and robotic\ndesigners. Designed as a microservice-oriented platform, OpenTera is an\nend-to-end solution that employs a series of independent modules for tasks such\nas data and session management, telehealth, daily assistive tasks/actions,\ntogether with smart devices and environments, all connected through the\nframework. After explaining the framework, we illustrate how OpenTera can be\nused to implement robotic solutions for different applications identified in\nLTC facilities and homes, and we describe how we plan to validate them through\nfield trials.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:42:09 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Panchea", "Adina M.", ""], ["L\u00e9tourneau", "Dominic", ""], ["Bri\u00e8re", "Simon", ""], ["Hamel", "Mathieu", ""], ["Maheux", "Marc-Antoine", ""], ["Godin", "C\u00e9dric", ""], ["Tousignant", "Michel", ""], ["Labb\u00e9", "Mathieu", ""], ["Ferland", "Fran\u00e7ois", ""], ["Grondin", "Fran\u00e7ois", ""], ["Michaud", "Fran\u00e7ois", ""]]}, {"id": "2103.06181", "submitter": "Jess Hohenstein", "authors": "Jess Hohenstein, Bill Selman, Gemma Petrie, Jofish Kaye, Rebecca Weiss", "title": "\"This Browser is Lightning Fast\": The Effects of Message Content on\n  Perceived Performance", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With technical performance being similar for various web browsers, improving\nuser perceived performance is integral to optimizing browser quality. We\ninvestigated the importance of priming, which has a well-documented ability to\naffect people's beliefs, on users' perceptions of web browser performance. We\nstudied 1495 participants who read either an article about performance\nimprovements to Mozilla Firefox, an article about user interface updates to\nFirefox, or an article about self-driving cars, and then watched video clips of\nbrowser tasks. As the priming effect would suggest, we found that reading\narticles about Firefox increased participants' perceived performance of Firefox\nover the most widely used web browser, Google Chrome. In addition, we found\nthat article content mattered, as the article about performance improvements\nled to higher performance ratings than the article about UI updates. Our\nfindings demonstrate how perceived performance can be improved without making\ntechnical improvements and that designers and developers must consider a wider\npicture when trying to improve user attitudes about technology.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:59:46 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Hohenstein", "Jess", ""], ["Selman", "Bill", ""], ["Petrie", "Gemma", ""], ["Kaye", "Jofish", ""], ["Weiss", "Rebecca", ""]]}, {"id": "2103.06192", "submitter": "Tom Lotze", "authors": "Tom Lotze, Stefan Klut, Mohammad Aliannejadi, Evangelos Kanoulas", "title": "Ranking Clarifying Questions Based on Predicted User Engagement", "comments": "Appeared in MICROS Workshop, co-located with ECIR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve online search results, clarification questions can be used to\nelucidate the information need of the user. This research aims to predict the\nuser engagement with the clarification pane as an indicator of relevance based\non the lexical information: query, question, and answers. Subsequently, the\npredicted user engagement can be used as a feature to rank the clarification\npanes. Regression and classification are applied for predicting user engagement\nand compared to naive heuristic baselines (e.g. mean) on the new MIMICS dataset\n[20]. An ablation study is carried out using a RankNet model to determine\nwhether the predicted user engagement improves clarification pane ranking\nperformance. The prediction models were able to improve significantly upon the\nnaive baselines, and the predicted user engagement feature significantly\nimproved the RankNet results in terms of NDCG and MRR. This research\ndemonstrates the potential for ranking clarification panes based on lexical\ninformation only and can serve as a first neural baseline for future research\nto improve on. The code is available online.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:11:59 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 16:23:05 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 13:31:10 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Lotze", "Tom", ""], ["Klut", "Stefan", ""], ["Aliannejadi", "Mohammad", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "2103.06238", "submitter": "Benzar Glen Grepon", "authors": "Benzar Glen Grepon and Aldwin Lester Martinez", "title": "Architectural Visualization Using Virtual Reality: A User Experience in\n  Simulating Buildings of a Community College in Bukidnon, Philippines", "comments": "19 pages", "journal-ref": "International Journal of Computing Sciences Research, [S.l.], v.\n  5, n. 1, p. 644-662, Feb. 2021", "doi": "10.25147/ijcsr.2017.001.1.64", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study aims to design and develop a virtual structural design that\nsimulates the campus and its buildings of a community college in Bukidnon,\nPhilippines through Virtual Reality. With the immersion of technology, this\nproject represents the architectural design of the establishment with the use\nof Virtual Reality Technology. The project uses a modified Iterative\nDevelopment Model which is a guide for the design and development of the 3D\nModels and VR Application. TinkerCAD which is a web-based application has been\nused to design buildings on the other hand Unity is used to develop the\nstructural designs of the buildings. The respondents of this study are the\nGrade 12 Senior High students from the 4 schools which are geographically near\nto the college. With this study, the researchers were able to showcase its VR\nApplication to the students and later evaluated using a System Usability Scale,\na 10 item questionnaire measuring usability with an overall average of 90% or\nPoint Score of 4.5 which is interpreted as excellent in a Likert table for\ndescriptive interpretation. With the use of the VR application potential\nstudents of the college will be able to visualize and experience the present\nstructures of the college without being physically present in the area. In this\npaper, the buildings and structures of NBCC were designed and developed through\na Virtual Reality Platform allowing students from different secondary schools\nthat are geographically near to the college to experience the feeling to be in\nthe school without being able to set a step in physically.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 03:55:39 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Grepon", "Benzar Glen", ""], ["Martinez", "Aldwin Lester", ""]]}, {"id": "2103.06379", "submitter": "Taha Hassan", "authors": "Taha Hassan, Bob Edmison, Timothy Stelter, D. Scott McCrickard", "title": "Learning to Trust: Understanding Editorial Authority and Trust in\n  Recommender Systems for Education", "comments": "(UMAP '21) Proceedings of the 29th ACM Conference on User Modeling,\n  Adaptation and Personalization, June 21 - 25, 2021 (Utrecht, the Netherlands)", "journal-ref": null, "doi": "10.1145/3450613.3456811", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Trust in a recommendation system (RS) is often algorithmically incorporated\nusing implicit or explicit feedback of user-perceived trustworthy social\nneighbors, and evaluated using user-reported trustworthiness of recommended\nitems. However, real-life recommendation settings can feature group disparities\nin trust, power, and prerogatives. Our study examines a complementary view of\ntrust which relies on the editorial power relationships and attitudes of all\nstakeholders in the RS application domain. We devise a simple, first-principles\nmetric of editorial authority, i.e., user preferences for recommendation\nsourcing, veto power, and incorporating user feedback, such that one RS user\ngroup confers trust upon another by ceding or assigning editorial authority. In\na mixed-methods study at Virginia Tech, we surveyed faculty, teaching\nassistants, and students about their preferences of editorial authority, and\nhypothesis-tested its relationship with trust in algorithms for a hypothetical\n`Suggested Readings' RS. We discover that higher RS editorial authority\nassigned to students is linked to the relative trust the course staff allocates\nto RS algorithm and students. We also observe that course staff favors higher\ncontrol for the RS algorithm in sourcing and updating the recommendations\nlong-term. Using content analysis, we discuss frequent staff-recommended\nstudent editorial roles and highlight their frequent rationales, such as\nperceived expertise, scaling the learning environment, professional curriculum\nneeds, and learner disengagement. We argue that our analyses highlight critical\nuser preferences to help detect editorial power asymmetry and identify RS\nuse-cases for supporting teaching and research\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:57:39 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Hassan", "Taha", ""], ["Edmison", "Bob", ""], ["Stelter", "Timothy", ""], ["McCrickard", "D. Scott", ""]]}, {"id": "2103.06490", "submitter": "Parag Dutta", "authors": "Rishi Hazra, Parag Dutta, Shubham Gupta, Mohammed Abdul Qaathir,\n  Ambedkar Dukkipati", "title": "Active$^2$ Learning: Actively reducing redundancies in Active Learning\n  methods for Sequence Tagging and Machine Translation", "comments": "Two of the authors had published similar manuscripts on arXiv. So\n  withdrawing this one. All further updations will be reflected at\n  arXiv:1911.00234", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning is a powerful tool for natural language processing (NLP)\nproblems, successful solutions to these problems rely heavily on large amounts\nof annotated samples. However, manually annotating data is expensive and\ntime-consuming. Active Learning (AL) strategies reduce the need for huge\nvolumes of labeled data by iteratively selecting a small number of examples for\nmanual annotation based on their estimated utility in training the given model.\nIn this paper, we argue that since AL strategies choose examples independently,\nthey may potentially select similar examples, all of which may not contribute\nsignificantly to the learning process. Our proposed approach,\nActive$\\mathbf{^2}$ Learning (A$\\mathbf{^2}$L), actively adapts to the deep\nlearning model being trained to eliminate further such redundant examples\nchosen by an AL strategy. We show that A$\\mathbf{^2}$L is widely applicable by\nusing it in conjunction with several different AL strategies and NLP tasks. We\nempirically demonstrate that the proposed approach is further able to reduce\nthe data requirements of state-of-the-art AL strategies by an absolute\npercentage reduction of $\\approx\\mathbf{3-25\\%}$ on multiple NLP tasks while\nachieving the same performance with no additional computation overhead.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:27:31 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 13:49:59 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Hazra", "Rishi", ""], ["Dutta", "Parag", ""], ["Gupta", "Shubham", ""], ["Qaathir", "Mohammed Abdul", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "2103.06571", "submitter": "Anelia Kurteva", "authors": "Anelia Kurteva and H\\'el\\`ene De Ribaupierre", "title": "Interface to Query and Visualise Definitions from a Knowledge Base", "comments": "To be published in: Proceedings of the International Conference on\n  Web Engineering 2021 (ICWE2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The semantic linked data model is at the core of the Web due to its ability\nto model real world entities, connect them via relationships and provide\ncontext, which could help to transform data into information and information\ninto knowledge. Linked Data, in the form of ontologies and knowledge graphs\ncould be stored locally or could be made available to everyone online. For\nexample, the DBpedia knowledge base, which provides global and unified access\nto knowledge graphs is open access. However, both access and usage of Linked\nData require individuals to have expert knowledge in the field of the Semantic\nWeb. Many of the existing solutions that are powered by Linked Data are\ndeveloped for specific use cases such as building and exploring ontologies\nvisually and are aimed at researchers with knowledge of semantic technology.\nThe solutions that are aimed at non-experts are generic and, in most cases,\ninformation visualisation is not available. Instead, information is presented\nin textual format, which does not ease cognitive processes such as\ncomprehension and could lead to problems such as information overload. In this\npaper, we present a web application with a user interface (UI), which combines\nfeatures from applications for both experts and non-experts. The UI allows\nindividuals with no previous knowledge of the Semantic Web to query the DBpedia\nknowledge base for definitions of a specific word and to view a graphical\nvisualisation of the query results (the search keyword itself and concepts\nrelated to it).\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:57:36 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Kurteva", "Anelia", ""], ["De Ribaupierre", "H\u00e9l\u00e8ne", ""]]}, {"id": "2103.06700", "submitter": "Xiupeng Shi", "authors": "Ziyao Zhou, Chen Chai, Weiru Yin, Xiupeng Shi", "title": "Developing and evaluating an human-automation shared control takeover\n  strategy based on Human-in-the-loop driving simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to develop a shared control takeover strategy\nfor smooth and safety control transition from an automation driving system to\nthe human driver and to approve its positive impacts on drivers' behavior and\nattitudes. A \"human-in-the-loop\" driving simulator experiment was conducted to\nevaluate the impact of the proposed shared control takeover strategy under\ndifferent disengagement conditions. Results of thirty-two drivers showed shared\ncontrol takeover strategy could improve safety performance at the aggregated\nlevel, especially at non-driving related disengagements. For more urgent\ndisengagements caused by another vehicle's sudden brake, a shared control\nstrategy enlarges individual differences. The primary reason is that some\ndrivers had higher self-reported mental workloads in response to the shared\ncontrol takeover strategy. Therefore, shared control between driver and\nautomation can involve driver's training to avoid mental overload when\ndeveloping takeover strategies.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 12:59:54 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhou", "Ziyao", ""], ["Chai", "Chen", ""], ["Yin", "Weiru", ""], ["Shi", "Xiupeng", ""]]}, {"id": "2103.06807", "submitter": "Kashyap Todi", "authors": "Kashyap Todi, Gilles Bailly, Luis A. Leiva, Antti Oulasvirta", "title": "Adapting User Interfaces with Model-based Reinforcement Learning", "comments": "13 pages, 10 figures, ACM CHI 2021 Full Paper", "journal-ref": null, "doi": "10.1145/3411764.3445497", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adapting an interface requires taking into account both the positive and\nnegative effects that changes may have on the user. A carelessly picked\nadaptation may impose high costs to the user -- for example, due to surprise or\nrelearning effort -- or \"trap\" the process to a suboptimal design immaturely.\nHowever, effects on users are hard to predict as they depend on factors that\nare latent and evolve over the course of interaction. We propose a novel\napproach for adaptive user interfaces that yields a conservative adaptation\npolicy: It finds beneficial changes when there are such and avoids changes when\nthere are none. Our model-based reinforcement learning method plans sequences\nof adaptations and consults predictive HCI models to estimate their effects. We\npresent empirical and simulation results from the case of adaptive menus,\nshowing that the method outperforms both a non-adaptive and a frequency-based\npolicy.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 17:24:34 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Todi", "Kashyap", ""], ["Bailly", "Gilles", ""], ["Leiva", "Luis A.", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "2103.06938", "submitter": "Saba Kawas", "authors": "Saba Kawas, Nicole S. Kuhn, Kyle Sorstokke, Emily E. Bascom, Alexis\n  Hiniker, and Katie Davis", "title": "When Screen Time Is not Screen Time: Tensions and Needs Between Tweens\n  and Their Parents During Nature-Based Exploration", "comments": "21 pages, 1 figure, 1 table, Forthcoming at the CHI 2021 Conference", "journal-ref": null, "doi": "10.1145/3411764.3445142", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigated the experiences of 15 parents and their tween children (ages\n8-12, n=23) during nature explorations using the NatureCollections app, a\nmobile application that connects children with nature. Drawing on parent\ninterviews and in-app audio recordings from a 2-week deployment study, we found\nthat tweens experiences with the NatureCollections app were influenced by\ntensions surrounding how parents and tweens negotiate technology use more\nbroadly. Despite these tensions, the app succeeded in engaging tweens in\noutdoor nature explorations, and parents valued the shared family experiences\naround nature. Parents desired the app to support family bonding and inform\nthem about how their tween used the app. This work shows how applications\nintended to support enriching youth experiences are experienced in the context\nof screen time tensions between parents and tween during a transitional period\nof child development. We offer recommendations for designing digital\nexperiences to support family needs and reduce screen time tensions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 20:22:09 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Kawas", "Saba", ""], ["Kuhn", "Nicole S.", ""], ["Sorstokke", "Kyle", ""], ["Bascom", "Emily E.", ""], ["Hiniker", "Alexis", ""], ["Davis", "Katie", ""]]}, {"id": "2103.06989", "submitter": "Lucy Lu Wang", "authors": "Lucy Lu Wang, Kelly Mack, Emma McDonnell, Dhruv Jain, Leah Findlater,\n  Jon E. Froehlich", "title": "A bibliometric analysis of citation diversity in accessibility and HCI\n  research", "comments": "11 pages, 5 figures, 3 tables, 2 appendices; CHI LBW 2021; accessible\n  PDF available at https://makeabilitylab.cs.washington.edu/", "journal-ref": null, "doi": "10.1145/3411763.3451618", "report-no": null, "categories": "cs.DL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accessibility research sits at the junction of several disciplines, drawing\ninfluence from HCI, disability studies, psychology, education, and more. To\ncharacterize the influences and extensions of accessibility research, we\nundertake a study of citation trends for accessibility and related HCI\ncommunities. We assess the diversity of venues and fields of study represented\namong the referenced and citing papers of 836 accessibility research papers\nfrom ASSETS and CHI, finding that though publications in computer science\ndominate these citation relationships, the relative proportion of citations\nfrom papers on psychology and medicine has grown over time. Though ASSETS is a\nmore niche venue than CHI in terms of citational diversity, both conferences\ndisplay standard levels of diversity among their incoming and outgoing\ncitations when analyzed in the context of 53K papers from 13 accessibility and\nHCI conference venues.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 22:24:28 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Wang", "Lucy Lu", ""], ["Mack", "Kelly", ""], ["McDonnell", "Emma", ""], ["Jain", "Dhruv", ""], ["Findlater", "Leah", ""], ["Froehlich", "Jon E.", ""]]}, {"id": "2103.07108", "submitter": "Katie Seaborn", "authors": "Katie Seaborn and Jacqueline Urakami", "title": "Measuring Voice UX Quantitatively: A Rapid Review", "comments": "Accepted at CHI EA '21", "journal-ref": null, "doi": "10.1145/3411763.3451712", "report-no": null, "categories": "cs.HC cs.RO cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer voice is experiencing a renaissance through the growing popularity\nof voice-based interfaces, agents, and environments. Yet, how to measure the\nuser experience (UX) of voice-based systems remains an open and urgent\nquestion, especially given that their form factors and interaction styles tend\nto be non-visual, intangible, and often considered disembodied or \"body-less.\"\nAs a first step, we surveyed the ACM and IEEE literatures to determine which\nquantitative measures and measurements have been deemed important for voice UX.\nOur findings show that there is little consensus, even with similar situations\nand systems, as well as an overreliance on lab work and unvalidated scales. In\nresponse, we offer two high-level descriptive frameworks for guiding future\nresearch, developing standardized instruments, and informing ongoing review\nwork. Our work highlights the current strengths and weaknesses of voice UX\nresearch and charts a path towards measuring voice UX in a more comprehensive\nway.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 07:05:06 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Seaborn", "Katie", ""], ["Urakami", "Jacqueline", ""]]}, {"id": "2103.07169", "submitter": "Niyati Rawal", "authors": "Niyati Rawal and Ruth Maria Stock-Homburg", "title": "Facial emotion expressions in human-robot interaction: A survey", "comments": "Pre-print version. Accepted in International Journal of Social\n  Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Facial expressions are an ideal means of communicating one's emotions or\nintentions to others. This overview will focus on human facial expression\nrecognition as well as robotic facial expression generation. In case of human\nfacial expression recognition, both facial expression recognition on predefined\ndatasets as well as in real time will be covered. For robotic facial expression\ngeneration, hand coded and automated methods i.e., facial expressions of a\nrobot are generated by moving the features (eyes, mouth) of the robot by hand\ncoding or automatically using machine learning techniques, will also be\ncovered. There are already plenty of studies that achieve high accuracy for\nemotion expression recognition on predefined datasets, but the accuracy for\nfacial expression recognition in real time is comparatively lower. In case of\nexpression generation in robots, while most of the robots are capable of making\nbasic facial expressions, there are not many studies that enable robots to do\nso automatically.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 09:39:43 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Rawal", "Niyati", ""], ["Stock-Homburg", "Ruth Maria", ""]]}, {"id": "2103.07180", "submitter": "Enka Blanchard", "authors": "Enka Blanchard (LORIA), Ryan Robucci (UMBC), Ted Selker (UMBC), Alan\n  Sherman (UMBC)", "title": "Phrase-Verified Voting: Verifiable Low-Tech Remote Boardroom Voting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Phrase-Verified Voting, a voter-verifiable remote voting system\nassembled from commercial off-the-shelf software for small private elections.\nThe system is transparent and enables each voter to verify that the tally\nincludes their ballot selection without requiring any understanding of\ncryptography. This paper describes the system and its use in fall 2020, to vote\nremotely in promotion committees in a university. Each voter fills out a form\nin the cloud with their vote V (YES, NO, ABSTAIN) and a passphrase P-two words\nentered by the voter. The system generates a verification prompt of the (P,V)\npairs and a tally of the votes, organized to help visualize how the votes add\nup. After the polls close, each voter verifies that this table lists their\n(P,V) pair and that the tally is computed correctly. The system is especially\nappropriate for any small group making sensitive decisions. Because the system\nwould not prevent a coercer from demanding that their victim use a specified\npassphrase, it is not designed for applications where such malfeasance would be\nlikely or go undetected. Results from 43 voters show that the system was\nwell-accepted, performed effectively for its intended purpose, and introduced\nusers to the concept of voter-verified elections. Compared to the commonly-used\nalternatives of paper ballots or voting by email, voters found the system\neasier to use, and that it provided greater privacy and outcome integrity.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 09:59:55 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Blanchard", "Enka", "", "LORIA"], ["Robucci", "Ryan", "", "UMBC"], ["Selker", "Ted", "", "UMBC"], ["Sherman", "Alan", "", "UMBC"]]}, {"id": "2103.07710", "submitter": "Elias Fern\\'andez Domingos", "authors": "Elias Fern\\'andez Domingos, In\\^es Terrucha, R\\'emi Suchon, Jelena\n  Gruji\\'c, Juan C. Burguillo, Francisco C. Santos and Tom Lenaerts", "title": "Delegation to autonomous agents promotes cooperation in collective-risk\n  dilemmas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Home assistant chat-bots, self-driving cars, drones or automated negotiations\nare some of the several examples of autonomous (artificial) agents that have\npervaded our society. These agents enable the automation of multiple tasks,\nsaving time and (human) effort. However, their presence in social settings\nraises the need for a better understanding of their effect on social\ninteractions and how they may be used to enhance cooperation towards the public\ngood, instead of hindering it. To this end, we present an experimental study of\nhuman delegation to autonomous agents and hybrid human-agent interactions\ncentered on a public goods dilemma shaped by a collective risk. Our aim to\nunderstand experimentally whether the presence of autonomous agents has a\npositive or negative impact on social behaviour, fairness and cooperation in\nsuch a dilemma. Our results show that cooperation increases when participants\ndelegate their actions to an artificial agent that plays on their behalf. Yet,\nthis positive effect is reduced when humans interact in hybrid human-agent\ngroups. Finally, we show that humans are biased towards agent behaviour,\nassuming that they will contribute less to the collective effort.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 12:39:54 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Domingos", "Elias Fern\u00e1ndez", ""], ["Terrucha", "In\u00eas", ""], ["Suchon", "R\u00e9mi", ""], ["Gruji\u0107", "Jelena", ""], ["Burguillo", "Juan C.", ""], ["Santos", "Francisco C.", ""], ["Lenaerts", "Tom", ""]]}, {"id": "2103.07731", "submitter": "Matteo Macchini", "authors": "Matteo Macchini, Ludovic De Matte\\\"is, Fabrizio Schiano, and Dario\n  Floreano", "title": "Personalized Human-Swarm Interaction through Hand Motion", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The control of collective robotic systems, such as drone swarms, is often\ndelegated to autonomous navigation algorithms due to their high dimensionality.\nHowever, like other robotic entities, drone swarms can still benefit from being\nteleoperated by human operators, whose perception and decision-making\ncapabilities are still out of the reach of autonomous systems. Drone swarm\nteleoperation is only at its dawn, and a standard human-swarm interface (HRI)\nis missing to date. In this study, we analyzed the spontaneous interaction\nstrategies of naive users with a swarm of drones. We implemented a\nmachine-learning algorithm to define a personalized Body-Machine Interface\n(BoMI) based only on a short calibration procedure. During this procedure, the\nhuman operator is asked to move spontaneously as if they were in control of a\nsimulated drone swarm. We assessed that hands are the most commonly adopted\nbody segment, and thus we chose a LEAP Motion controller to track them to let\nthe users control the aerial drone swarm. This choice makes our interface\nportable since it does not rely on a centralized system for tracking the human\nbody. We validated our algorithm to define personalized HRIs for a set of\nparticipants in a realistic simulated environment, showing promising results in\nperformance and user experience. Our method leaves unprecedented freedom to the\nuser to choose between position and velocity control only based on their body\nmotion preferences.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 15:36:16 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Macchini", "Matteo", ""], ["De Matte\u00efs", "Ludovic", ""], ["Schiano", "Fabrizio", ""], ["Floreano", "Dario", ""]]}, {"id": "2103.07757", "submitter": "Yalda Ghasemi", "authors": "Yalda Ghasemi, Heejin Jeong", "title": "Model-based Task Analysis and Large-scale Video-based Remote Evaluation\n  Methods for Extended Reality Research", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce two remote extended reality (XR) research methods\nthat can overcome the limitations of lab-based controlled experiments,\nespecially during the COVID-19 pandemic: (1) a predictive model-based task\nanalysis and (2) a large-scale video-based remote evaluation. We used a box\nstacking task including three interaction modalities - two multimodal\ngaze-based interactions as well as a unimodal hand-based interaction which is\ndefined as our baseline. For the first evaluation, a GOMS-based task analysis\nwas performed by analyzing the tasks to understand human behaviors in XR and\npredict task execution times. For the second evaluation, an online survey was\nadministered using a series of the first-person point of view videos where a\nuser performs the corresponding task with three interaction modalities. A total\nof 118 participants were asked to compare the interaction modes based on their\njudgment. Two standard questionnaires were used to measure perceived workload\nand the usability of the modalities.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 17:36:55 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ghasemi", "Yalda", ""], ["Jeong", "Heejin", ""]]}, {"id": "2103.07805", "submitter": "Subhajit Das", "authors": "Subhajit Das and Alex Endert", "title": "CACTUS: Detecting and Resolving Conflicts in Objective Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models are constructed by expert ML practitioners using\nvarious coding languages, in which they tune and select models hyperparameters\nand learning algorithms for a given problem domain. They also carefully design\nan objective function or loss function (often with multiple objectives) that\ncaptures the desired output for a given ML task such as classification,\nregression, etc. In multi-objective optimization, conflicting objectives and\nconstraints is a major area of concern. In such problems, several competing\nobjectives are seen for which no single optimal solution is found that\nsatisfies all desired objectives simultaneously. In the past VA systems have\nallowed users to interactively construct objective functions for a classifier.\nIn this paper, we extend this line of work by prototyping a technique to\nvisualize multi-objective objective functions either defined in a Jupyter\nnotebook or defined using an interactive visual interface to help users to: (1)\nperceive and interpret complex mathematical terms in it and (2) detect and\nresolve conflicting objectives. Visualization of the objective function\nenlightens potentially conflicting objectives that obstructs selecting correct\nsolution(s) for the desired ML task or goal. We also present an enumeration of\npotential conflicts in objective specification in multi-objective objective\nfunctions for classifier selection. Furthermore, we demonstrate our approach in\na VA system that helps users in specifying meaningful objective functions to a\nclassifier by detecting and resolving conflicting objectives and constraints.\nThrough a within-subject quantitative and qualitative user study, we present\nresults showing that our technique helps users interactively specify meaningful\nobjective functions by resolving potential conflicts for a classification task.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 22:38:47 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Das", "Subhajit", ""], ["Endert", "Alex", ""]]}, {"id": "2103.07820", "submitter": "Asma Tabassum", "authors": "Asma Tabassum and He Bai", "title": "Dynamic Control Allocation between Onboard and Delayed Remote Control\n  for Unmanned Aircraft System Detect-and-Avoid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.HC cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops and evaluates the performance of an allocation agent to\nbe potentially integrated into the onboard Detect and Avoid (DAA) computer of\nan Unmanned Aircraft System (UAS). We consider a UAS that can be fully\ncontrolled by the onboard DAA system and by a remote human pilot. With a\ncommunication channel prone to latency, we consider a mixed initiative\ninteraction environment, where the control authority of the UAS is dynamically\nallocated by the allocation agent. In an encounter with a dynamic intruder, the\nprobability of collision may increase in the absence of pilot commands in the\npresence of latency. Moreover, a delayed pilot command may not result in safe\nresolution of the current scenario and need to be improvised. We design an\noptimization algorithm to reduce collision risk and refine delayed pilot\ncommands. Towards this end, a Markov Decision Process (MDP)and its solution are\nemployed to create a wait time map. The map consists of estimated times that\nthe UAS can wait for the remote pilot commands at each state. A command\nblending algorithm is designed to select an avoidance maneuver that prioritizes\nthe pilot intention extracted from the pilot commands. The wait time map and\nthe command blending algorithm are implemented and integrated into a\nclosed-loop simulator. We conduct ten thousands fast-time Monte Carlo\nsimulations and compare the performance of the integrated setup with a\nstandalone DAA setup. The simulation results show that the allocation agent\nenables the UAS to wait without inducing any near mid air collision (NMAC) and\nsevere loss of well clear (LoWC) while positively improve pilot involvement in\nthe encounter resolution.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 00:33:08 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Tabassum", "Asma", ""], ["Bai", "He", ""]]}, {"id": "2103.07964", "submitter": "Karol Chlasta", "authors": "Khanh-Duy Le, Tanh Quang Tran, Karol Chlasta, Krzysztof Krejtz, Morten\n  Fjeld, Andreas Kunz", "title": "VXSlate: Combining Head Movement and Mobile Touch for Large Virtual\n  Display Interaction", "comments": "The authors have not been asked to publish at arXiv", "journal-ref": "2021 IEEE Conference on Virtual Reality and 3D User Interfaces\n  Abstracts and Workshops (VRW)", "doi": "10.1109/VRW52623.2021.00146", "report-no": "pp. 528-529", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) headsets can open opportunities for users to accomplish\ncomplex tasks on large virtual displays, using compact setups. However,\ninteracting with large virtual displays using existing interaction techniques\nmight cause fatigue, especially for precise manipulations, due to the lack of\nphysical surfaces. We designed VXSlate, an interaction technique that uses a\nlarge virtual display, as an expansion of a tablet. VXSlate combines a user's\nheadmovement, as tracked by the VR headset, and touch interaction on the\ntablet. The user's headmovement position both a virtual representation of the\ntablet and of the user's hand on the large virtual display. The user's\nmulti-touch interactions perform finely-tuned content manipulations.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 16:18:01 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 01:51:12 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Le", "Khanh-Duy", ""], ["Tran", "Tanh Quang", ""], ["Chlasta", "Karol", ""], ["Krejtz", "Krzysztof", ""], ["Fjeld", "Morten", ""], ["Kunz", "Andreas", ""]]}, {"id": "2103.07987", "submitter": "Daniel McDuff", "authors": "Daniel McDuff and Ewa Nowara", "title": "\"Warm Bodies\": A Post-Processing Technique for Animating Dynamic Blood\n  Flow on Photos and Avatars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What breathes life into an embodied agent or avatar? While body motions such\nas facial expressions, speech and gestures have been well studied, relatively\nlittle attention has been applied to subtle changes due to underlying\nphysiology. We argue that subtle pulse signals are important for creating more\nlifelike and less disconcerting avatars. We propose a method for animating\nblood flow patterns, based on a data-driven physiological model that can be\nused to directly augment the appearance of synthetic avatars and\nphoto-realistic faces. While the changes are difficult for participants to\n\"see\", they significantly more frequently select faces with blood flow as more\nanthropomorphic and animated than faces without blood flow. Furthermore, by\nmanipulating the frequency of the heart rate in the underlying signal we can\nchange the perceived arousal of the character.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 18:09:42 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["McDuff", "Daniel", ""], ["Nowara", "Ewa", ""]]}, {"id": "2103.08079", "submitter": "Katie Seaborn", "authors": "Katie Seaborn, Peter Pennefather, Norihisa P. Miyake, Mihoko\n  Otake-Matsuura", "title": "Crossing the Tepper Line: An Emerging Ontology for Describing the\n  Dynamic Sociality of Embodied AI", "comments": "Accepted at CHI EA '21", "journal-ref": null, "doi": "10.1145/3411763.3451783", "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial intelligences (AI) are increasingly being embodied and embedded in\nthe world to carry out tasks and support decision-making with and for people.\nRobots, recommender systems, voice assistants, virtual humans - do these\ndisparate types of embodied AI have something in common? Here we show how they\ncan manifest as \"socially embodied AI.\" We define this as the state that\nembodied AI \"circumstantially\" take on within interactive contexts when\nperceived as both social and agentic by people. We offer a working ontology\nthat describes how embodied AI can dynamically transition into socially\nembodied AI. We propose an ontological heuristic for describing the threshold:\nthe Tepper line. We reinforce our theoretical work with expert insights from a\ncard sort workshop. We end with two case studies to illustrate the dynamic and\ncontextual nature of this heuristic.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 00:45:44 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Seaborn", "Katie", ""], ["Pennefather", "Peter", ""], ["Miyake", "Norihisa P.", ""], ["Otake-Matsuura", "Mihoko", ""]]}, {"id": "2103.08119", "submitter": "Guanhao Fu", "authors": "Guanhao Fu, Ehsan Azimi, Peter Kazanzides", "title": "Mobile Teleoperation: Evaluation of Wireless Wearable Sensing of the\n  Operator's Arm Motion", "comments": "6 pages, 11 figures. Submitted to 2021 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS), pending review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teleoperation platforms often require the user to be situated at a fixed\nlocation to both visualize and control the movement of the robot and thus do\nnot provide the operator with much mobility. One example of such systems is in\nexisting robotic surgery solutions that require the surgeons to be away from\nthe patient, attached to consoles where their heads must be fixed and their\narms can only move in a limited space. This creates a barrier between\nphysicians and patients that does not exist in normal surgery. To address this\nissue, we propose a mobile telesurgery solution where the surgeons are no\nlonger mechanically limited to control consoles and are able to teleoperate the\nrobots from the patient bedside, using their arms equipped with wireless\nsensors and viewing the endoscope video via optical see-through HMDs. We\nevaluate the feasibility and efficiency of our user interaction method with a\nstandard surgical robotic manipulator via two tasks with different levels of\nrequired dexterity. The results indicate that with sufficient training our\nproposed platform can attain similar efficiency while providing added mobility\nfor the operator.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 03:30:11 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Fu", "Guanhao", ""], ["Azimi", "Ehsan", ""], ["Kazanzides", "Peter", ""]]}, {"id": "2103.08324", "submitter": "Phil Heslop Dr", "authors": "Philip Heslop, Ahmed Kharrufa, Madeline Balaam, David Leat", "title": "Classroom Technology Deployment Matrix: A Planning, Monitoring,\n  Evaluating and Reporting Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present the Classroom Technology Deployment Matrix (CTDM), a tool for\nhigh-level Planning, Monitoring, Evaluating and Reporting of classroom\ndeployments of educational technologies, enabling researchers, teachers and\nschools to work together for successful deployments. The tool is de-rived from\na review of literature on technology adaptation (at the individual, process and\norganisation level), concluding that Normalization Process Theory, which seeks\nto explain the social processes that lead to the routine embedding of\ninnovative technology in an existing system, would a suitable foundation for\ndeveloping this matrix. This can be leveraged in the specific context of the\nclassroom, specifically including the Normal Desired State of teachers. We\nexplore this classroom context, and the developed CTDM, through look-ing at two\nseparate deployments (different schools and teachers) of the same technology\n(Collocated Collaborative Writing), observing how lessons learned from the\nfirst changed our approach to the second. The descriptive and an-alytical value\nof the tool is then demonstrated through map-ping these observation to the\nmatrix and can be applied to future deployments.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 12:17:14 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Heslop", "Philip", ""], ["Kharrufa", "Ahmed", ""], ["Balaam", "Madeline", ""], ["Leat", "David", ""]]}, {"id": "2103.08472", "submitter": "Ge Ren", "authors": "Ge Ren, Jun Wu, Gaolei Li, Shenghong Li", "title": "Automatically Lock Your Neural Networks When You're Away", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smartphone and laptop can be unlocked by face or fingerprint recognition,\nwhile neural networks which confront numerous requests every day have little\ncapability to distinguish between untrustworthy and credible users. It makes\nmodel risky to be traded as a commodity. Existed research either focuses on the\nintellectual property rights ownership of the commercialized model, or traces\nthe source of the leak after pirated models appear. Nevertheless, active\nidentifying users legitimacy before predicting output has not been considered\nyet. In this paper, we propose Model-Lock (M-LOCK) to realize an end-to-end\nneural network with local dynamic access control, which is similar to the\nautomatic locking function of the smartphone to prevent malicious attackers\nfrom obtaining available performance actively when you are away. Three kinds of\nmodel training strategy are essential to achieve the tremendous performance\ndivergence between certified and suspect input in one neural network. Extensive\nexperiments based on MNIST, FashionMNIST, CIFAR10, CIFAR100, SVHN and GTSRB\ndatasets demonstrated the feasibility and effectiveness of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:47:54 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ren", "Ge", ""], ["Wu", "Jun", ""], ["Li", "Gaolei", ""], ["Li", "Shenghong", ""]]}, {"id": "2103.08483", "submitter": "Arianna Rossi", "authors": "Arianna Rossi and Kerstin Bongard-Blanchy", "title": "All in one stroke? Intervention Spaces for Dark Patterns", "comments": "Position Paper at the Workshop \"What Can CHI Do About Dark Patterns?\"\n  at the CHI Conference on Human Factors in Computing Systems (CHI'21), May\n  8--13, 2021, Online Virtual Conference (originally Yokohama, Japan)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This position paper draws from the complexity of dark patterns to develop\narguments for differentiated interventions. We propose a matrix of\ninterventions with a \\textit{measure axis} (from user-directed to\nenvironment-directed) and a \\textit{scope axis} (from general to specific). We\nfurthermore discuss a set of interventions situated in different fields of the\nintervention spaces. The discussions at the 2021 CHI workshop \"What can CHI do\nabout dark patterns?\" should help hone the matrix structure and fill its fields\nwith specific intervention proposals.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:02:16 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 13:25:33 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Rossi", "Arianna", ""], ["Bongard-Blanchy", "Kerstin", ""]]}, {"id": "2103.08525", "submitter": "Tobias Kauer", "authors": "Tobias Kauer, Arran Ridley, Marian D\\\"ork, Benjamin Bach", "title": "The Public Life of Data: Investigating Reactions to Visualizations on\n  Reddit", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445720", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research investigates how people engage with data visualizations when\ncommenting on the social platform Reddit. There has been considerable research\non collaborative sensemaking with visualizations and the personal relation of\npeople with data. Yet, little is known about how public audiences without\nspecific expertise and shared incentives openly express their thoughts,\nfeelings, and insights in response to data visualizations. Motivated by the\nextensive social exchange around visualizations in online communities, this\nresearch examines characteristics and motivations of people's reactions to\nposts featuring visualizations. Following a Grounded Theory approach, we study\n475 reactions from the /r/dataisbeautiful community, identify ten\ndistinguishable reaction types, and consider their contribution to the\ndiscourse. A follow-up survey with 168 Reddit users clarified their intentions\nto react. Our results help understand the role of personal perspectives on data\nand inform future interfaces that integrate audience reactions into\nvisualizations to foster a public discourse about data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:47:54 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 08:13:19 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Kauer", "Tobias", ""], ["Ridley", "Arran", ""], ["D\u00f6rk", "Marian", ""], ["Bach", "Benjamin", ""]]}, {"id": "2103.08558", "submitter": "Jos\\'e Alberto \\'Alvarez Mart\\'in", "authors": "J. Alberto \\'Alvarez Mart\\'in, Henrik Gollee, J\\\"org M\\\"uller and\n  Roderick Murray-Smith", "title": "Intermittent control as a model of mouse movements", "comments": "42 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Intermittent Control (IC) models as a candidate framework for\nmodelling human input movements in Human--Computer Interaction (HCI). IC\ndiffers from continuous control in that users are not assumed to use feedback\nto adjust their movements continuously, but only when the difference between\nthe observed pointer position and predicted pointer positions become large. We\nuse a parameter optimisation approach to identify the parameters of an\nintermittent controller from experimental data, where users performed\none-dimensional mouse movements in a reciprocal pointing task. Compared to\nprevious published work with continuous control models, based on the\nKullback-Leibler divergence from the experimental observations, IC is better\nable to generatively reproduce the distinctive dynamical features and\nvariability of the pointing task across participants and over repeated tasks.\nIC is compatible with current physiological and psychological theory and\nprovides insight into the source of variability in HCI tasks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:23:57 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mart\u00edn", "J. Alberto \u00c1lvarez", ""], ["Gollee", "Henrik", ""], ["M\u00fcller", "J\u00f6rg", ""], ["Murray-Smith", "Roderick", ""]]}, {"id": "2103.08733", "submitter": "Nikolaos Kondylidis", "authors": "Nikolaos Kondylidis, Jie Zou and Evangelos Kanoulas", "title": "Category Aware Explainable Conversational Recommendation", "comments": "Workshop on Mixed-Initiative ConveRsatiOnal Systems (MICROS) @ECIR,\n  2021", "journal-ref": "Workshop on Mixed-Initiative ConveRsatiOnal Systems (MICROS)\n  @ECIR, 2021", "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most conversational recommendation approaches are either not explainable, or\nthey require external user's knowledge for explaining or their explanations\ncannot be applied in real time due to computational limitations. In this work,\nwe present a real time category based conversational recommendation approach,\nwhich can provide concise explanations without prior user knowledge being\nrequired. We first perform an explainable user model in the form of preferences\nover the items' categories, and then use the category preferences to recommend\nitems. The user model is performed by applying a BERT-based neural architecture\non the conversation. Then, we translate the user model into item recommendation\nscores using a Feed Forward Network. User preferences during the conversation\nin our approach are represented by category vectors which are directly\ninterpretable. The experimental results on the real conversational\nrecommendation dataset ReDial demonstrate comparable performance to the\nstate-of-the-art, while our approach is explainable. We also show the potential\npower of our framework by involving an oracle setting of category preference\nprediction.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 21:45:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kondylidis", "Nikolaos", ""], ["Zou", "Jie", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "2103.08786", "submitter": "Jessie J. Smith", "authors": "Nasim Sonboli and Jessie J. Smith, Florencia Cabral Berenfus, Robin\n  Burke, Casey Fiesler", "title": "Fairness and Transparency in Recommendation: The Users' Perspective", "comments": null, "journal-ref": null, "doi": "10.1145/3450613.3456835", "report-no": null, "categories": "cs.IR cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Though recommender systems are defined by personalization, recent work has\nshown the importance of additional, beyond-accuracy objectives, such as\nfairness. Because users often expect their recommendations to be purely\npersonalized, these new algorithmic objectives must be communicated\ntransparently in a fairness-aware recommender system. While explanation has a\nlong history in recommender systems research, there has been little work that\nattempts to explain systems that use a fairness objective. Even though the\nprevious work in other branches of AI has explored the use of explanations as a\ntool to increase fairness, this work has not been focused on recommendation.\nHere, we consider user perspectives of fairness-aware recommender systems and\ntechniques for enhancing their transparency. We describe the results of an\nexploratory interview study that investigates user perceptions of fairness,\nrecommender systems, and fairness-aware objectives. We propose three features\n-- informed by the needs of our participants -- that could improve user\nunderstanding of and trust in fairness-aware recommender systems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 00:42:09 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Sonboli", "Nasim", ""], ["Smith", "Jessie J.", ""], ["Berenfus", "Florencia Cabral", ""], ["Burke", "Robin", ""], ["Fiesler", "Casey", ""]]}, {"id": "2103.09058", "submitter": "Joon Sung Park", "authors": "Joon Sung Park, Michael S. Bernstein, Robin N. Brewer, Ece Kamar,\n  Meredith Ringel Morris", "title": "Understanding the Representation and Representativeness of Age in AI\n  Data Sets", "comments": "9 pages", "journal-ref": "In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and\n  Society (AIES '21)", "doi": "10.1145/3461702.3462590", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A diverse representation of different demographic groups in AI training data\nsets is important in ensuring that the models will work for a large range of\nusers. To this end, recent efforts in AI fairness and inclusion have advocated\nfor creating AI data sets that are well-balanced across race, gender,\nsocioeconomic status, and disability status. In this paper, we contribute to\nthis line of work by focusing on the representation of age by asking whether\nolder adults are represented proportionally to the population at large in AI\ndata sets. We examine publicly-available information about 92 face data sets to\nunderstand how they codify age as a case study to investigate how the subjects'\nages are recorded and whether older generations are represented. We find that\nolder adults are very under-represented; five data sets in the study that\nexplicitly documented the closed age intervals of their subjects included older\nadults (defined as older than 65 years), while only one included oldest-old\nadults (defined as older than 85 years). Additionally, we find that only 24 of\nthe data sets include any age-related information in their documentation or\nmetadata, and that there is no consistent method followed across these data\nsets to collect and record the subjects' ages. We recognize the unique\ndifficulties in creating representative data sets in terms of age, but raise it\nas an important dimension that researchers and engineers interested in\ninclusive AI should consider.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 12:26:22 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 04:30:40 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Park", "Joon Sung", ""], ["Bernstein", "Michael S.", ""], ["Brewer", "Robin N.", ""], ["Kamar", "Ece", ""], ["Morris", "Meredith Ringel", ""]]}, {"id": "2103.09228", "submitter": "Neelma Bhatti", "authors": "Neelma Bhatti, Timothy L. Stelter, D. Scott McCrickard", "title": "Conversational User Interfaces As Assistive interlocutors For Young\n  Children's Bilingual Language Acquisition", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Children in a large number of international and cross-cultural families in\nand outside of the US learn and speak more than one language. However, parents\noften struggle to acquaint their young children with their local language if\nthe child spends majority of time at home and with their spoken language if\nthey go to daycare or school. By reviewing relevant literature about the role\nof screen media content in young children's language learning, and interviewing\na subset of parents raising multilingual children, we explore the potential of\ndesigning conversational user interfaces which can double as an assistive\nlanguage aid.We present a preliminary list of objectives to guide the the\ndesign of conversational user interfaces dialogue for young children's\nbilingual language acquisition.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 17:57:10 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bhatti", "Neelma", ""], ["Stelter", "Timothy L.", ""], ["McCrickard", "D. Scott", ""]]}, {"id": "2103.09319", "submitter": "Samaneh Saadat", "authors": "Samaneh Saadat, Natalia Colmenares, Gita Sukthankar", "title": "Do Bots Modify the Workflow of GitHub Teams?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasing complexity of modern software engineering projects makes\nthe usage of automated assistants imperative. Bots can be used to complete\nrepetitive tasks during development and testing, as well as promoting\ncommunication between team members through issue reporting and documentation.\nAlthough the ultimate aim of these automated assistants is to speed taskwork\ncompletion, their inclusion into GitHub repositories may affect teamwork as\nwell. This paper studies the question of how bots modify the team workflow. We\nexamined the event sequences of repositories with bots and without bots using a\ncontrast motif discovery method to detect subsequences that are more prevalent\nin one set of event sequences vs. the other. Our study reveals that teams with\nbots are more likely to intersperse comments throughout their coding\nactivities, while not actually being more prolific commenters.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 20:50:29 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Saadat", "Samaneh", ""], ["Colmenares", "Natalia", ""], ["Sukthankar", "Gita", ""]]}, {"id": "2103.09856", "submitter": "Raz Saremi", "authors": "Razieh Saremi, Ye Yang, Gregg Vesonder, Guenther Ruhe, He Zhang", "title": "CrowdSim: A Hybrid Simulation Model for Failure Prediction in\n  Crowdsourced Software Development", "comments": "13 pages paper, 5 pages Appendix, 9 Figure, 6 Tables, 1 algorithm\n  This work has been submitted to the IEEE Transactions on Systems, Man, and\n  Cybernetics: Systems for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical crowdsourcing software development(CSD) marketplace consists of a\nlist of software tasks as service demands and a pool of freelancer developers\nas service suppliers. Highly dynamic and competitive CSD market places may\nresult in task failure due to unforeseen risks, such as increased competition\nover shared worker supply, or uncertainty associated with workers' experience\nand skills, and so on. To improve CSD effectiveness, it is essential to better\nunderstand and plan with respect to dynamic worker characteristics and risks\nassociated with CSD processes. In this paper, we present a hybrid simulation\nmodel, CrowdSim, to forecast crowdsourcing task failure risk in competitive CSD\nplatforms. CrowdSim is composed of three layered components: the macro-level\nreflects the overall crowdsourcing platform based on system dynamics,the\nmeso-level represents the task life cycle based on discrete event simulation,\nand the micro-level models the crowd workers' decision-making processes based\non agent-based simulation. CrowdSim is evaluated through three CSD decision\nscenarios to demonstrate its effectiveness, using a real-world historical\ndataset and the results demonstrate CrowdSim's potential in empowering\ncrowdsourcing managers to explore crowdsourcing outcomes with respect to\ndifferent task scheduling options.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 18:41:53 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Saremi", "Razieh", ""], ["Yang", "Ye", ""], ["Vesonder", "Gregg", ""], ["Ruhe", "Guenther", ""], ["Zhang", "He", ""]]}, {"id": "2103.09905", "submitter": "Mohammad Tahaei", "authors": "Mohammad Tahaei, Adam Jenkins, Kami Vaniea, Maria Wolters", "title": "\"I Don't Know Too Much About It\": On the Security Mindsets of Computer\n  Science Students", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-55958-8", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The security attitudes and approaches of software developers have a large\nimpact on the software they produce, yet we know very little about how and when\nthese views are constructed. This paper investigates the security and privacy\n(S&P) perceptions, experiences, and practices of current Computer Science\nstudents at the graduate and undergraduate level using semi-structured\ninterviews. We find that the attitudes of students already match many of those\nthat have been observed in professional level developers. Students have a range\nof hacker and attack mindsets, lack of experience with security APIs, a mixed\nview of who is in charge of S&P in the software life cycle, and a tendency to\ntrust other peoples' code as a convenient approach to rapidly build software.\nWe discuss the impact of our results on both curriculum development and support\nfor professional developers.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 12:07:48 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tahaei", "Mohammad", ""], ["Jenkins", "Adam", ""], ["Vaniea", "Kami", ""], ["Wolters", "Maria", ""]]}, {"id": "2103.10057", "submitter": "Shreyas Krishnaswamy", "authors": "P. Dayani, N. Orr, A. Thomopoulos, V. Saran, S. Krishnaswamy, E.\n  Zhang, N. Hu, D. McPherson, J. Menke, A. Yang, K. Vetter", "title": "Immersive Operation of a Semi-Autonomous Aerial Platform for Detecting\n  and Mapping Radiation", "comments": "3 pages, 2 figures. The first three authors contributed equally.\n  Accepted to the 2020 IEEE Nuclear Science Symposium & Medical Imaging\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in radiation detection and computer vision have enabled\nsmall unmanned aerial systems (sUASs) to produce 3D nuclear radiation maps in\nreal-time. Currently these state-of-the-art systems still require two\noperators: one to pilot the sUAS and another operator to monitor the detected\nradiation. In this work we present a system that integrates real-time 3D\nradiation visualization with semi-autonomous sUAS control. Our Virtual Reality\ninterface enables a single operator to define trajectories using waypoints to\nabstract complex flight control and utilize the semi-autonomous maneuvering\ncapabilities of the sUAS. The interface also displays a fused radiation\nvisualization and environment map, thereby enabling simultaneous remote\noperation and radiation monitoring by a single operator. This serves as the\nbasis for development of a single system that deploys and autonomously controls\nfleets of sUASs.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 07:38:37 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Dayani", "P.", ""], ["Orr", "N.", ""], ["Thomopoulos", "A.", ""], ["Saran", "V.", ""], ["Krishnaswamy", "S.", ""], ["Zhang", "E.", ""], ["Hu", "N.", ""], ["McPherson", "D.", ""], ["Menke", "J.", ""], ["Yang", "A.", ""], ["Vetter", "K.", ""]]}, {"id": "2103.10355", "submitter": "Raz Saremi", "authors": "Razieh Saremi, Marzieh Lotfalian Saremi, Sanam Jena, Robert Anzalone,\n  and Ahmed Bahabry", "title": "Impact of Task Cycle Pattern on Project Success in Software\n  Crowdsourcing", "comments": "11 pages, 5 figures, 1 table Accepted in HCI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is becoming an accepted method of software development for\ndifferent phases in the production lifecycle. Ideally, mass parallel production\nthrough Crowdsourcing could be an option for rapid acquisition in software\nengineering by leveraging infinite worker resource on the internet. It is\nimportant to understand the patterns and strategies of decomposing and\nuploading parallel tasks to maintain a stable worker supply as well as a\nsatisfactory task completion rate.\n  This research report is an empirical analysis of the available tasks'\nlifecycle patterns in crowdsourcing. Following the waterfall model in\nCrowdsourced Software Development (CSD), this research identified four patterns\nfor the sequence of task arrival per project: 1) Prior Cycle, 2) Current Cycle,\n3) Orbit Cycle, and 4) Fresh Cycle.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 16:21:46 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Saremi", "Razieh", ""], ["Saremi", "Marzieh Lotfalian", ""], ["Jena", "Sanam", ""], ["Anzalone", "Robert", ""], ["Bahabry", "Ahmed", ""]]}, {"id": "2103.10451", "submitter": "Dominik D\\\"urrschnabel", "authors": "Lena Stubbemann, Dominik D\\\"urrschnabel, Robert Refflinghaus", "title": "Neural Networks for Semantic Gaze Analysis in XR Settings", "comments": "16 pages, 6 figures, 1 table, Accepted to: ETRA2021, ACM Symposium on\n  Eye Tracking Research and Applications", "journal-ref": null, "doi": "10.1145/3448017.3457380", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual-reality (VR) and augmented-reality (AR) technology is increasingly\ncombined with eye-tracking. This combination broadens both fields and opens up\nnew areas of application, in which visual perception and related cognitive\nprocesses can be studied in interactive but still well controlled settings.\nHowever, performing a semantic gaze analysis of eye-tracking data from\ninteractive three-dimensional scenes is a resource-intense task, which so far\nhas been an obstacle to economic use. In this paper we present a novel approach\nwhich minimizes time and information necessary to annotate volumes of interest\n(VOIs) by using techniques from object recognition. To do so, we train\nconvolutional neural networks (CNNs) on synthetic data sets derived from\nvirtual models using image augmentation techniques. We evaluate our method in\nreal and virtual environments, showing that the method can compete with\nstate-of-the-art approaches, while not relying on additional markers or\npreexisting databases but instead offering cross-platform use.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:05:01 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Stubbemann", "Lena", ""], ["D\u00fcrrschnabel", "Dominik", ""], ["Refflinghaus", "Robert", ""]]}, {"id": "2103.10455", "submitter": "Chen Chen", "authors": "Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen,\n  Zhengming Ding", "title": "3D Human Pose Estimation with Spatial and Temporal Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer architectures have become the model of choice in natural language\nprocessing and are now being introduced into computer vision tasks such as\nimage classification, object detection, and semantic segmentation. However, in\nthe field of human pose estimation, convolutional architectures still remain\ndominant. In this work, we present PoseFormer, a purely transformer-based\napproach for 3D human pose estimation in videos without convolutional\narchitectures involved. Inspired by recent developments in vision transformers,\nwe design a spatial-temporal transformer structure to comprehensively model the\nhuman joint relations within each frame as well as the temporal correlations\nacross frames, then output an accurate 3D human pose of the center frame. We\nquantitatively and qualitatively evaluate our method on two popular and\nstandard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments\nshow that PoseFormer achieves state-of-the-art performance on both datasets.\nCode is available at \\url{https://github.com/zczcwh/PoseFormer}\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:14:37 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 16:54:14 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zheng", "Ce", ""], ["Zhu", "Sijie", ""], ["Mendieta", "Matias", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""], ["Ding", "Zhengming", ""]]}, {"id": "2103.10585", "submitter": "Benjamin Levy", "authors": "Benjamin Levy and Matthew Stewart", "title": "The evolving ecosystem of COVID-19 contact tracing applications", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the outbreak of the novel coronavirus, COVID-19, there has been\nincreased interest in the use of digital contact tracing as a means of stopping\nchains of viral transmission, provoking alarm from privacy advocates.\nConcerning the ethics of this technology, recent studies have predominantly\nfocused on (1) the formation of guidelines for ethical contact tracing, (2) the\nanalysis of specific implementations, or (3) the review of a select number of\ncontact tracing applications and their relevant privacy or ethical\nimplications. In this study, we provide a comprehensive survey of the evolving\necosystem of COVID-19 tracing applications, examining 152 contact tracing\napplications and assessing the extent to which they comply with existing\nguidelines for ethical contact tracing. The assessed criteria cover areas\nincluding data collection and storage, transparency and consent, and whether\nthe implementation is open source. We find that although many apps released\nearly in the pandemic fell short of best practices, apps released more\nrecently, following the publication of the Apple/Google exposure notification\nprotocol, have tended to be more closely aligned with ethical contact tracing\nprinciples. This dataset will be publicly available and may be updated as the\npandemic continues.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 01:38:19 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Levy", "Benjamin", ""], ["Stewart", "Matthew", ""]]}, {"id": "2103.10725", "submitter": "Thomas Mildner", "authors": "Thomas Mildner, Gian-Luca Savino", "title": "How Social Are Social Media The Dark Patterns In Facebook's Interface", "comments": "Position Paper at the Workshop \"What Can CHI Do About Dark Patterns?\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Many researchers have been concerned with social media and possible negative\nimpacts on the well-being of their audience. With the popularity of social\nnetworking sites (SNS) steadily increasing, psychological and social sciences\nhave shown great interest in their effects and consequences on humans.\nUnfortunately, it appears to be difficult to find correlations between SNS and\nthe results of their works. We, therefore, investigate Facebook using the tools\nof HCI to find connections between interface features and the concerns raised\nby these domains. With a nod towards Dark Patterns, we use an empirical design\nanalysis to identify interface interferences that impact users' online privacy.\nWe further discuss how HCI can help to work towards more ethical user\ninterfaces in the future.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 10:40:29 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Mildner", "Thomas", ""], ["Savino", "Gian-Luca", ""]]}, {"id": "2103.10769", "submitter": "Valentin Zieglmeier", "authors": "Valentin Zieglmeier and Alexander Pretschner", "title": "Trustworthy Transparency by Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals lack oversight over systems that process their data. This can\nlead to discrimination and hidden biases that are hard to uncover. Recent data\nprotection legislation tries to tackle these issues, but it is inadequate. It\ndoes not prevent data misusage while stifling sensible use cases for data. We\nthink the conflict between data protection and increasingly data-based systems\nshould be solved differently. When access to data is given, all usages should\nbe made transparent to the data subjects. This enables their data sovereignty,\nallowing individuals to benefit from sensible data usage while addressing\npotentially harmful consequences of data misusage. We contribute to this with a\ntechnical concept and an empirical evaluation. First, we conceptualize a\ntransparency framework for software design, incorporating research on user\ntrust and experience. Second, we instantiate and empirically evaluate the\nframework in a focus group study over three months, centering on the user\nperspective. Our transparency framework enables developing software that\nincorporates transparency in its design. The evaluation shows that it satisfies\nusability and trustworthiness requirements. The provided transparency is\nexperienced as beneficial and participants feel empowered by it. This shows\nthat our framework enables Trustworthy Transparency by Design.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 12:34:01 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zieglmeier", "Valentin", ""], ["Pretschner", "Alexander", ""]]}, {"id": "2103.10798", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Quanwei Huang, Youbao Tang, Xingxu Yao, Jufeng Yang,\n  Guiguang Ding, Bj\\\"orn W. Schuller", "title": "Computational Emotion Analysis From Images: Recent Advances and Future\n  Directions", "comments": "Accepted chapter in the book \"Human Perception of Visual Information\n  Psychological and Computational Perspective\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions are usually evoked in humans by images. Recently, extensive research\nefforts have been dedicated to understanding the emotions of images. In this\nchapter, we aim to introduce image emotion analysis (IEA) from a computational\nperspective with the focus on summarizing recent advances and suggesting future\ndirections. We begin with commonly used emotion representation models from\npsychology. We then define the key computational problems that the researchers\nhave been trying to solve and provide supervised frameworks that are generally\nused for different IEA tasks. After the introduction of major challenges in\nIEA, we present some representative methods on emotion feature extraction,\nsupervised classifier learning, and domain adaptation. Furthermore, we\nintroduce available datasets for evaluation and summarize some main results.\nFinally, we discuss some open questions and future directions that researchers\ncan pursue.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 13:33:34 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zhao", "Sicheng", ""], ["Huang", "Quanwei", ""], ["Tang", "Youbao", ""], ["Yao", "Xingxu", ""], ["Yang", "Jufeng", ""], ["Ding", "Guiguang", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2103.10804", "submitter": "Enes Yigitbas", "authors": "Enes Yigitbas, Kadiray Karakaya, Ivan Jovanovikj, Gregor Engels", "title": "Enhancing Human-in-the-Loop Adaptive Systems through Digital Twins and\n  VR Interfaces", "comments": "Submitted to SEAMS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-adaptation approaches usually rely on closed-loop controllers that avoid\nhuman intervention from adaptation. While such fully automated approaches have\nproven successful in many application domains, there are situations where human\ninvolvement in the adaptation process is beneficial or even necessary. For such\n\"human-in-the-loop\" adaptive systems, two major challenges, namely transparency\nand controllability, have to be addressed to include the human in the\nself-adaptation loop. Transparency means that relevant context information\nabout the adaptive systems and its context is represented based on a digital\ntwin enabling the human an immersive and realistic view. Concerning\ncontrollability, the decision-making and adaptation operations should be\nmanaged in a natural and interactive way. As existing human-in-the-loop\nadaptation approaches do not fully cover these aspects, we investigate\nalternative human-in-the-loop strategies by using a combination of digital\ntwins and virtual reality (VR) interfaces. Based on the concept of the digital\ntwin, we represent a self-adaptive system and its respective context in a\nvirtual environment. With the help of a VR interface, we support an immersive\nand realistic human involvement in the self-adaptation loop by mirroring the\nphysical entities of the real world to the VR interface. For integrating the\nhuman in the decision-making and adaptation process, we have implemented and\nanalyzed two different human-in-the-loop strategies in VR: a procedural control\nwhere the human can control the decision making-process and adaptations through\nVR interactions (human-controlled) and a declarative control where the human\nspecifies the goal state and the configuration is delegated to an AI planner\n(mixed-initiative). We illustrate and evaluate our approach based on an\nautonomic robot system that is accessible and controlled through a VR\ninterface.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 13:48:25 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Yigitbas", "Enes", ""], ["Karakaya", "Kadiray", ""], ["Jovanovikj", "Ivan", ""], ["Engels", "Gregor", ""]]}, {"id": "2103.10977", "submitter": "Tamer Olmez", "authors": "Zumray Dokur, Tamer Olmez", "title": "Classification of Motor Imagery EEG Signals by Using a Divergence Based\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks (DNNs) are observed to be successful in pattern\nclassification. However, high classification performances of DNNs are related\nto their large training sets. Unfortunately, in the literature, the datasets\nused to classify motor imagery (MI) electroencephalogram (EEG) signals contain\na small number of samples. To achieve high performances with small-sized\ndatasets, most of the studies have employed a transformation such as common\nspatial patterns (CSP) before the classification process. However, CSP is\ndependent on subjects and introduces computational load in real-time\napplications. It is observed in the literature that the augmentation process is\nnot applied for increasing the classification performance of EEG signals. In\nthis study, we have investigated the effect of the augmentation process on the\nclassification performance of MI EEG signals instead of using a preceding\ntransformation such as the CSP, and we have demonstrated that by resulting in\nhigh success rates for the classification of MI EEGs, the augmentation process\nis able to compete with the CSP. In addition to the augmentation process, we\nmodified the DNN structure to increase the classification performance, to\ndecrease the number of nodes in the structure, and to be used with less number\nof hyper parameters. A minimum distance network (MDN) following the last layer\nof the convolutional neural network (CNN) was used as the classifier instead of\na fully connected neural network (FCNN). By augmenting the EEG dataset and\nfocusing solely on CNN's training, the training algorithm of the proposed\nstructure is strengthened without applying any transformation. We tested these\nimprovements on brain-computer interface (BCI) competitions 2005 and 2008\ndatabases with two and four classes, and the high impact of the augmentation on\nthe average performances are demonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 18:27:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Dokur", "Zumray", ""], ["Olmez", "Tamer", ""]]}, {"id": "2103.11029", "submitter": "Denis Newman-Griffis", "authors": "Denis Newman-Griffis, Venkatesh Sivaraman, Adam Perer, Eric\n  Fosler-Lussier, Harry Hochheiser", "title": "TextEssence: A Tool for Interactive Analysis of Semantic Shifts Between\n  Corpora", "comments": "Accepted as a Systems Demonstration at NAACL-HLT 2021. Video\n  demonstration at https://youtu.be/1xEEfsMwL0k", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings of words and concepts capture syntactic and semantic regularities\nof language; however, they have seen limited use as tools to study\ncharacteristics of different corpora and how they relate to one another. We\nintroduce TextEssence, an interactive system designed to enable comparative\nanalysis of corpora using embeddings. TextEssence includes visual,\nneighbor-based, and similarity-based modes of embedding analysis in a\nlightweight, web-based interface. We further propose a new measure of embedding\nconfidence based on nearest neighborhood overlap, to assist in identifying\nhigh-quality embeddings for corpus analysis. A case study on COVID-19\nscientific literature illustrates the utility of the system. TextEssence is\navailable from https://github.com/drgriffis/text-essence.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 21:26:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Newman-Griffis", "Denis", ""], ["Sivaraman", "Venkatesh", ""], ["Perer", "Adam", ""], ["Fosler-Lussier", "Eric", ""], ["Hochheiser", "Harry", ""]]}, {"id": "2103.11104", "submitter": "Qingyang Li", "authors": "Qingyang Li, Zhiwen Yu, Lina Yao, Bin Guo", "title": "RLTIR: Activity-based Interactive Person Identification based on\n  Reinforcement Learning Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identity recognition plays an important role in ensuring security in our\ndaily life. Biometric-based (especially activity-based) approaches are favored\ndue to their fidelity, universality, and resilience. However, most existing\nmachine learning-based approaches rely on a traditional workflow where models\nare usually trained once for all, with limited involvement from end-users in\nthe process and neglecting the dynamic nature of the learning process. This\nmakes the models static and can not be updated in time, which usually leads to\nhigh false positive or false negative. Thus, in practice, an expert is desired\nto assist with providing high-quality observations and interpretation of model\noutputs. It is expedient to combine both advantages of human experts and the\ncomputational capability of computers to create a tight-coupling incremental\nlearning process for better performance. In this study, we develop RLTIR, an\ninteractive identity recognition approach based on reinforcement learning, to\nadjust the identification model by human guidance. We first build a base\ntree-structured identity recognition model. And an expert is introduced in the\nmodel for giving feedback upon model outputs. Then, the model is updated\naccording to strategies that are automatically learned under a designated\nreinforcement learning framework. To the best of our knowledge, it is the very\nfirst attempt to combine human expert knowledge with model learning in the area\nof identity recognition. The experimental results show that the reinforced\ninteractive identity recognition framework outperforms baseline methods with\nregard to recognition accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 05:48:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Li", "Qingyang", ""], ["Yu", "Zhiwen", ""], ["Yao", "Lina", ""], ["Guo", "Bin", ""]]}, {"id": "2103.11297", "submitter": "Ryan Rossi", "authors": "Camille Harris, Ryan A. Rossi, Sana Malik, Jane Hoffswell, Fan Du, Tak\n  Yeon Lee, Eunyee Koh, Handong Zhao", "title": "Insight-centric Visualization Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization recommendation systems simplify exploratory data analysis (EDA)\nand make understanding data more accessible to users of all skill levels by\nautomatically generating visualizations for users to explore. However, most\nexisting visualization recommendation systems focus on ranking all\nvisualizations into a single list or set of groups based on particular\nattributes or encodings. This global ranking makes it difficult and\ntime-consuming for users to find the most interesting or relevant insights. To\naddress these limitations, we introduce a novel class of visualization\nrecommendation systems that automatically rank and recommend both groups of\nrelated insights as well as the most important insights within each group. Our\nproposed approach combines results from many different learning-based methods\nto discover insights automatically. A key advantage is that this approach\ngeneralizes to a wide variety of attribute types such as categorical,\nnumerical, and temporal, as well as complex non-trivial combinations of these\ndifferent attribute types. To evaluate the effectiveness of our approach, we\nimplemented a new insight-centric visualization recommendation system,\nSpotLight, which generates and ranks annotated visualizations to explain each\ninsight. We conducted a user study with 12 participants and two datasets which\nshowed that users are able to quickly understand and find relevant insights in\nunfamiliar data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 03:30:22 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Harris", "Camille", ""], ["Rossi", "Ryan A.", ""], ["Malik", "Sana", ""], ["Hoffswell", "Jane", ""], ["Du", "Fan", ""], ["Lee", "Tak Yeon", ""], ["Koh", "Eunyee", ""], ["Zhao", "Handong", ""]]}, {"id": "2103.11726", "submitter": "Antonios Liapis", "authors": "Panagiotis Migkotzidis and Antonios Liapis", "title": "SuSketch: Surrogate Models of Gameplay as a Design Assistant", "comments": "To be published in IEEE Transactions on Games, 11 pages", "journal-ref": null, "doi": "10.1109/TG.2021.3068360", "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces SuSketch, a design tool for first person shooter\nlevels. SuSketch provides the designer with gameplay predictions for two\ncompeting players of specific character classes. The interface allows the\ndesigner to work side-by-side with an artificially intelligent creator and to\nreceive varied types of feedback such as path information, predicted balance\nbetween players in a complete playthrough, or a predicted heatmap of the\nlocations of player deaths. The system also proactively designs alternatives to\nthe level and class pairing, and presents them to the designer as suggestions\nthat improve the predicted balance of the game. SuSketch offers a new way of\nintegrating machine learning into mixed-initiative co-creation tools, as a\nsurrogate of human play trained on a large corpus of artificial playtraces. A\nuser study with 16 game developers indicated that the tool was easy to use, but\nalso highlighted a need to make SuSketch more accessible and more explainable.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 11:05:27 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Migkotzidis", "Panagiotis", ""], ["Liapis", "Antonios", ""]]}, {"id": "2103.12016", "submitter": "Christopher Starke", "authors": "Christopher Starke, Janine Baleis, Birte Keller, Frank Marcinkowski", "title": "Fairness Perceptions of Algorithmic Decision-Making: A Systematic Review\n  of the Empirical Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Algorithmic decision-making (ADM) increasingly shapes people's daily lives.\nGiven that such autonomous systems can cause severe harm to individuals and\nsocial groups, fairness concerns have arisen. A human-centric approach demanded\nby scholars and policymakers requires taking people's fairness perceptions into\naccount when designing and implementing ADM. We provide a comprehensive,\nsystematic literature review synthesizing the existing empirical insights on\nperceptions of algorithmic fairness from 39 empirical studies spanning multiple\ndomains and scientific disciplines. Through thorough coding, we systemize the\ncurrent empirical literature along four dimensions: (a) algorithmic predictors,\n(b) human predictors, (c) comparative effects (human decision-making vs.\nalgorithmic decision-making), and (d) consequences of ADM. While we identify\nmuch heterogeneity around the theoretical concepts and empirical measurements\nof algorithmic fairness, the insights come almost exclusively from\nWestern-democratic contexts. By advocating for more interdisciplinary research\nadopting a society-in-the-loop framework, we hope our work will contribute to\nfairer and more responsible ADM.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:12:45 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Starke", "Christopher", ""], ["Baleis", "Janine", ""], ["Keller", "Birte", ""], ["Marcinkowski", "Frank", ""]]}, {"id": "2103.12063", "submitter": "Muhammad E. H. Chowdhury", "authors": "Muhammad E. H. Chowdhury, Nabil Ibtehaz, Tawsifur Rahman, Yosra Magdi\n  Salih Mekki, Yazan Qibalwey, Sakib Mahmud, Maymouna Ezeddin, Susu Zughaier,\n  Sumaya Ali S A Al-Maadeed", "title": "QUCoughScope: An Artificially Intelligent Mobile Application to Detect\n  Asymptomatic COVID-19 Patients using Cough and Breathing Sounds", "comments": "6 page, Table 4, Figure 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.HC cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the break of COVID-19 pandemic, mass testing has become essential to\nreduce the spread of the virus. Several recent studies suggest that a\nsignificant number of COVID-19 patients display no physical symptoms\nwhatsoever. Therefore, it is unlikely that these patients will undergo COVID-19\ntest, which increases their chances of unintentionally spreading the virus.\nCurrently, the primary diagnostic tool to detect COVID-19 is RT-PCR test on\ncollected respiratory specimens from the suspected case. This requires patients\nto travel to a laboratory facility to be tested, thereby potentially infecting\nothers along the way.It is evident from recent researches that asymptomatic\nCOVID-19 patients cough and breath in a different way than the healthy people.\nSeveral research groups have created mobile and web-platform for crowdsourcing\nthe symptoms, cough and breathing sounds from healthy, COVID-19 and Non-COVID\npatients. Some of these data repositories were made public. We have received\nsuch a repository from Cambridge University team under data-sharing agreement,\nwhere we have cough and breathing sound samples for 582 and 141 healthy and\nCOVID-19 patients, respectively. 87 COVID-19 patients were asymptomatic, while\nrest of them have cough. We have developed an Android application to\nautomatically screen COVID-19 from the comfort of people homes. Test subjects\ncan simply download a mobile application, enter their symptoms, record an audio\nclip of their cough and breath, and upload the data anonymously to our servers.\nOur backend server converts the audio clip to spectrogram and then apply our\nstate-of-the-art machine learning model to classify between cough sounds\nproduced by COVID-19 patients, as opposed to healthy subjects or those with\nother respiratory conditions. The system can detect asymptomatic COVID-19\npatients with a sensitivity more than 91%.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 18:26:39 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Chowdhury", "Muhammad E. H.", ""], ["Ibtehaz", "Nabil", ""], ["Rahman", "Tawsifur", ""], ["Mekki", "Yosra Magdi Salih", ""], ["Qibalwey", "Yazan", ""], ["Mahmud", "Sakib", ""], ["Ezeddin", "Maymouna", ""], ["Zughaier", "Susu", ""], ["Al-Maadeed", "Sumaya Ali S A", ""]]}, {"id": "2103.12233", "submitter": "Alvaro Leandro Cavalcante Carneiro", "authors": "Alvaro Leandro Cavalcante Carneiro, Lucas de Brito Silva, Denis\n  Henrique Pinheiro Salvadeo", "title": "Efficient sign language recognition system and dataset creation method\n  based on deep learning and image processing", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New deep-learning architectures are created every year, achieving\nstate-of-the-art results in image recognition and leading to the belief that,\nin a few years, complex tasks such as sign language translation will be\nconsiderably easier, serving as a communication tool for the hearing-impaired\ncommunity. On the other hand, these algorithms still need a lot of data to be\ntrained and the dataset creation process is expensive, time-consuming, and\nslow. Thereby, this work aims to investigate techniques of digital image\nprocessing and machine learning that can be used to create a sign language\ndataset effectively. We argue about data acquisition, such as the frames per\nsecond rate to capture or subsample the videos, the background type,\npreprocessing, and data augmentation, using convolutional neural networks and\nobject detection to create an image classifier and comparing the results based\non statistical tests. Different datasets were created to test the hypotheses,\ncontaining 14 words used daily and recorded by different smartphones in the RGB\ncolor system. We achieved an accuracy of 96.38% on the test set and 81.36% on\nthe validation set containing more challenging conditions, showing that 30 FPS\nis the best frame rate subsample to train the classifier, geometric\ntransformations work better than intensity transformations, and artificial\nbackground creation is not effective to model generalization. These trade-offs\nshould be considered in future work as a cost-benefit guideline between\ncomputational cost and accuracy gain when creating a dataset and training a\nsign recognition model.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 23:36:49 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 22:36:18 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Carneiro", "Alvaro Leandro Cavalcante", ""], ["Silva", "Lucas de Brito", ""], ["Salvadeo", "Denis Henrique Pinheiro", ""]]}, {"id": "2103.12505", "submitter": "Subhabrata Majumdar", "authors": "Christina Last, Prithviraj Pramanik, Nikita Saini, Akash Smaran\n  Majety, Do-Hyung Kim, Manuel Garc\\'ia-Herranz, Subhabrata Majumdar", "title": "Towards an Open Global Air Quality Monitoring Platform to Assess\n  Children's Exposure to Air Pollutants in the Light of COVID-19 Lockdowns", "comments": "Accepted as CHI-2021 Late-Breaking Work", "journal-ref": "Extended Abstracts of the 2021 CHI Conference on Human Factors in\n  Computing Systems, article 434", "doi": "10.1145/3411763.3451768", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This ongoing work attempts to understand and address the requirements of\nUNICEF, a leading organization working in children's welfare, where they aim to\ntackle the problem of air quality for children at a global level. We are\nmotivated by the lack of a proper model to account for heavily fluctuating air\nquality levels across the world in the wake of the COVID-19 pandemic, leading\nto uncertainty among public health professionals on the exact levels of\nchildren's exposure to air pollutants. We create an initial model as per the\nagency's requirement to generate insights through a combination of virtual\nmeetups and online presentations. Our research team comprised of UNICEF's\nresearchers and a group of volunteer data scientists. The presentations were\ndelivered to a number of scientists and domain experts from UNICEF and\ncommunity champions working with open data. We highlight their feedback and\npossible avenues to develop this research further.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 16:02:28 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Last", "Christina", ""], ["Pramanik", "Prithviraj", ""], ["Saini", "Nikita", ""], ["Majety", "Akash Smaran", ""], ["Kim", "Do-Hyung", ""], ["Garc\u00eda-Herranz", "Manuel", ""], ["Majumdar", "Subhabrata", ""]]}, {"id": "2103.12645", "submitter": "Humphrey Yang", "authors": "Humphrey Yang, Zeyu Yan, Danli Luo, Lining Yao", "title": "FoamFactor: Hydrogel-Foam Composite with Tunable Stiffness and\n  Compressibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents FoamFactor, a novel material with tunable stiffness and\ncompressibility between hydration states, and a tailored pipeline to design and\nfabricate artifacts consisting of it. This technique compounds hydrogel with\nopen-cell foams via additive manufacturing to produce a water-responsive\ncomposite material. Enabled by the large volumetric changes of hydrogel\ndispersions, the material is soft and compressible when dehydrated and becomes\nstiffer and rather incompressible when hydrated. Leveraging this material\nproperty transition, we explore its design space in various aspects pertaining\nto the transition of hydration states, including multi-functional shoes,\namphibious cars, mechanical transmission systems, and self-deploying robotic\ngrippers.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 15:56:27 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Yang", "Humphrey", ""], ["Yan", "Zeyu", ""], ["Luo", "Danli", ""], ["Yao", "Lining", ""]]}, {"id": "2103.12702", "submitter": "Eleftherios Triantafyllidis Mr.", "authors": "Eleftherios Triantafyllidis and Zhibin Li", "title": "Considerations and Challenges of Measuring Operator Performance in\n  Telepresence and Teleoperation Entailing Mixed Reality Technologies", "comments": "Accepted at ACM CHI 2021 Conference on Human Factors in Computing\n  Systems Workshop CHI' 21 (Evaluating User Experiences in Mixed Reality)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing human performance in robotic scenarios such as those seen in\ntelepresence and teleoperation has always been a challenging task. With the\nrecent spike in mixed reality technologies and the subsequent focus by\nresearchers, new pathways have opened in elucidating human perception and\nmaximising overall immersion. Yet with the multitude of different assessment\nmethods in evaluating operator performance in virtual environments within the\nfield of HCI and HRI, inter-study comparability and transferability are\nlimited. In this short paper, we present a brief overview of existing methods\nin assessing operator performance including subjective and objective approaches\nwhile also attempting to capture future technical challenges and frontiers. The\nultimate goal is to assist and pinpoint readers towards potentially important\ndirections with the future hope of providing a unified immersion framework for\nteleoperation and telepresence by standardizing a set of guidelines and\nevaluation methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:24:09 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 12:42:57 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Triantafyllidis", "Eleftherios", ""], ["Li", "Zhibin", ""]]}, {"id": "2103.12832", "submitter": "Nicholas Dacre PhD", "authors": "Nicholas Dacre, Panos Constantinides, Joe Nandhakumar", "title": "How to Motivate and Engage Generation Clash of Clans at Work? Emergent\n  Properties of Business Gamification Elements in the Digital Economy", "comments": "International Gamification for Business Conference", "journal-ref": null, "doi": "10.2139/ssrn.3809398", "report-no": null, "categories": "econ.GN cs.HC q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Organisations are currently lacking in developing and implementing business\nsystems in meaningful ways to motivate and engage their staff. This is\nparticularly salient as the average employee spends eleven cumulative years of\ntheir life at work, however less than one third of the workforce are actually\nengaged in their duties throughout their career. Such low levels of engagement\nare particularly prominent with younger employees, referred to as Generation Y\n(GenY), who are the least engaged of all groups at work. However, they will\ndedicate around five cumulative years of their life immersed playing video\ngames such as Clash of Clans, whether for social, competitive, extrinsic, or\nintrinsic motivational factors. Using behavioural concepts derived from video\ngames, and applying game design elements in business systems to motivate\nemployees in the digital economy, is a concept which has come to be recognised\nas Business Gamification. Thus, the purpose of this research paper is to\nfurther our understanding of game design elements for business, and investigate\ntheir properties from design to implementation in gamified systems. Following a\ntwo-year ethnographic style study with both a system development, and a\ncommunication agency largely staffed with GenY employees, findings suggest\nproperties in game design elements are emergent and temporal in their\ninstantiations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 20:52:19 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 12:04:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dacre", "Nicholas", ""], ["Constantinides", "Panos", ""], ["Nandhakumar", "Joe", ""]]}, {"id": "2103.12910", "submitter": "Dongyu Liu", "authors": "Dongyu Liu, Kalyan Veeramachaneni, Alexander Geiger, Victor O.K. Li,\n  Huamin Qu", "title": "AQEyes: Visual Analytics for Anomaly Detection and Examination of Air\n  Quality Data", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection plays a key role in air quality analysis by enhancing\nsituational awareness and alerting users to potential hazards. However,\nexisting anomaly detection approaches for air quality analysis have their own\nlimitations regarding parameter selection (e.g., need for extensive domain\nknowledge), computational expense, general applicability (e.g., require labeled\ndata), interpretability, and the efficiency of analysis. Furthermore, the poor\nquality of collected air quality data (inconsistently formatted and sometimes\nmissing) also increases the difficulty of analysis substantially. In this\npaper, we systematically formulate design requirements for a system that can\nsolve these limitations and then propose AQEyes, an integrated visual analytics\nsystem for efficiently monitoring, detecting, and examining anomalies in air\nquality data. In particular, we propose a unified end-to-end tunable machine\nlearning pipeline that includes several data pre-processors and featurizers to\ndeal with data quality issues. The pipeline integrates an efficient\nunsupervised anomaly detection method that works without the use of labeled\ndata and overcomes the limitations of existing approaches. Further, we develop\nan interactive visualization system to visualize the outputs from the pipeline.\nThe system incorporates a set of novel visualization and interaction designs,\nallowing analysts to visually examine air quality dynamics and anomalous events\nin multiple scales and from multiple facets. We demonstrate the performance of\nthis pipeline through a quantitative evaluation and show the effectiveness of\nthe visualization system using qualitative case studies on real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 01:13:20 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Liu", "Dongyu", ""], ["Veeramachaneni", "Kalyan", ""], ["Geiger", "Alexander", ""], ["Li", "Victor O. K.", ""], ["Qu", "Huamin", ""]]}, {"id": "2103.12940", "submitter": "Alexander Sutherland", "authors": "Sascha Griffiths, Tayfun Alpay, Alexander Sutherland, Matthias Kerzel,\n  Manfred Eppe, Erik Strahl, Stefan Wermter", "title": "Exercise with Social Robots: Companion or Coach?", "comments": "6 pages, 5 figures, Found in Proceedings of Workshop on Personal\n  Robots for Exercising and Coaching at the HRI 2018 (HRI2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we investigate the roles that social robots can take in\nphysical exercise with human partners. In related work, robots or virtual\nintelligent agents take the role of a coach or instructor whereas in other\napproaches they are used as motivational aids. These are two \"paradigms\", so to\nspeak, within the small but growing area of robots for social exercise. We\ndesigned an online questionnaire to test whether the preferred role in which\npeople want to see robots would be the companion or the coach. The\nquestionnaire asks people to imagine working out with a robot with the help of\nthree utilized questionnaires: (1) CART-Q which is used for judging\ncoach-athlete relationships, (2) the mind perception questionnaire and (3) the\nSystem Usability Scale (SUS). We present the methodology, some preliminary\nresults as well as our intended future work on personal robots for coaching.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 02:25:05 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Griffiths", "Sascha", ""], ["Alpay", "Tayfun", ""], ["Sutherland", "Alexander", ""], ["Kerzel", "Matthias", ""], ["Eppe", "Manfred", ""], ["Strahl", "Erik", ""], ["Wermter", "Stefan", ""]]}, {"id": "2103.13198", "submitter": "Dror Feitelson", "authors": "Guy Amir, Ayala Prusak, Tal Reiss, Nir Zabari, Dror G. Feitelson", "title": "Use and Perceptions of Multi-Monitor Workstations: A Natural Experiment", "comments": "9 pages, 16 figures. Accepted to 8th International Workshop on\n  Software Engineering Research and Industrial Practice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using multiple monitors is commonly thought to improve productivity, but this\nis hard to check experimentally. We use a survey, taken by 101 practitioners of\nwhich 80% have coded professionally for at least 2 years, to assess subjective\nperspectives based on experience. To improve validity, we compare situations in\nwhich developers naturally use different setups -- the difference between\nworking at home or at the office, and how things changed when developers were\nforced to work from home due to the Covid-19 pandemic. The results indicate\nthat using multiple monitors is indeed perceived as beneficial and desirable.\n19% of the respondents reported adding a monitor to their home setup in\nresponse to the Covid-19 situation. At the same time, the single most\ninfluential factor cited as affecting productivity was not the physical setup\nbut interactions with co-workers -- both reduced productivity due to lack of\nconnections available at work, and improved productivity due to reduced\ninterruptions from co-workers. A central implication of our work is that\nempirical research on software development should be conducted in settings\nsimilar to those actually used by practitioners, and in particular using\nworkstations configured with multiple monitors.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 20:57:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Amir", "Guy", ""], ["Prusak", "Ayala", ""], ["Reiss", "Tal", ""], ["Zabari", "Nir", ""], ["Feitelson", "Dror G.", ""]]}, {"id": "2103.13287", "submitter": "Tobias Fiebig", "authors": "Mannat Kaur, Michel van Eeten, Marijn Janssen, Kevin Borgolte, and\n  Tobias Fiebig", "title": "Human Factors in Security Research: Lessons Learned from 2008-2018", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instead of only considering technology, computer security research now\nstrives to also take into account the human factor by studying regular users\nand, to a lesser extent, experts like operators and developers of systems. We\nfocus our analysis on the research on the crucial population of experts, whose\nhuman errors can impact many systems at once, and compare it to research on\nregular users. To understand how far we advanced in the area of human factors,\nhow the field can further mature, and to provide a point of reference for\nresearchers new to this field, we analyzed the past decade of human factors\nresearch in security and privacy, identifying 557 relevant publications. Of\nthese, we found 48 publications focused on expert users and analyzed all in\ndepth. For additional insights, we compare them to a stratified sample of 48\nend-user studies.\n  In this paper we investigate:\n  (i) The perspective on human factors, and how we can learn from safety\nscience (ii) How and who are the participants recruited, and how this -- as we\nfind -- creates a western-centric perspective (iii) Research objectives, and\nhow to align these with the chosen research methods (iv) How theories can be\nused to increase rigor in the communities scientific work, including\nlimitations to the use of Grounded Theory, which is often incompletely applied\n(v) How researchers handle ethical implications, and what we can do to account\nfor them more consistently\n  Although our literature review has limitations, new insights were revealed\nand avenues for further research identified.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:58:05 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Kaur", "Mannat", ""], ["van Eeten", "Michel", ""], ["Janssen", "Marijn", ""], ["Borgolte", "Kevin", ""], ["Fiebig", "Tobias", ""]]}, {"id": "2103.13452", "submitter": "Anh Tuan Nguyen", "authors": "Anh Tuan Nguyen, Markus W. Drealan, Diu Khue Luu, Ming Jiang, Jian Xu,\n  Jonathan Cheng, Qi Zhao, Edward W. Keefer, Zhi Yang", "title": "A Portable, Self-Contained Neuroprosthetic Hand with Deep Learning-Based\n  Finger Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective: Deep learning-based neural decoders have emerged as the prominent\napproach to enable dexterous and intuitive control of neuroprosthetic hands.\nYet few studies have materialized the use of deep learning in clinical settings\ndue to its high computational requirements. Methods: Recent advancements of\nedge computing devices bring the potential to alleviate this problem. Here we\npresent the implementation of a neuroprosthetic hand with embedded deep\nlearning-based control. The neural decoder is designed based on the recurrent\nneural network (RNN) architecture and deployed on the NVIDIA Jetson Nano - a\ncompacted yet powerful edge computing platform for deep learning inference.\nThis enables the implementation of the neuroprosthetic hand as a portable and\nself-contained unit with real-time control of individual finger movements.\nResults: The proposed system is evaluated on a transradial amputee using\nperipheral nerve signals (ENG) with implanted intrafascicular microelectrodes.\nThe experiment results demonstrate the system's capabilities of providing\nrobust, high-accuracy (95-99%) and low-latency (50-120 msec) control of\nindividual finger movements in various laboratory and real-world environments.\nConclusion: Modern edge computing platforms enable the effective use of deep\nlearning-based neural decoders for neuroprosthesis control as an autonomous\nsystem. Significance: This work helps pioneer the deployment of deep neural\nnetworks in clinical applications underlying a new class of wearable biomedical\ndevices with embedded artificial intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 19:11:58 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Nguyen", "Anh Tuan", ""], ["Drealan", "Markus W.", ""], ["Luu", "Diu Khue", ""], ["Jiang", "Ming", ""], ["Xu", "Jian", ""], ["Cheng", "Jonathan", ""], ["Zhao", "Qi", ""], ["Keefer", "Edward W.", ""], ["Yang", "Zhi", ""]]}, {"id": "2103.14160", "submitter": "Benoit Ozell", "authors": "Dany Naser Addin and Benoit Ozell", "title": "Design and Test of an adaptive augmented reality interface to manage\n  systems to assist critical missions", "comments": "For associated mpeg files, see\n  https://www.polymtl.ca/rv/Activites/Drones/arxiv/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a user interface (UI) based on augmented reality (AR) with\nhead-mounted display (HMD) for improving situational awareness during critical\noperation and improve human efficiency on operations. The UI displays\ncontextual information as well as accepts orders given from the headset to\ncontrol unmanned aerial vehicles (UAVs) for assisting the rescue team. We\nestablished experiments where people had been put in a stressful situation and\nare asked to resolve a complex mission using a headset and a computer.\nComparing both technologies, our results show that augmented reality has the\npotential to be an important tool to help those involved in the emergency\nsituation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 22:28:37 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Addin", "Dany Naser", ""], ["Ozell", "Benoit", ""]]}, {"id": "2103.14400", "submitter": "Michael Salvato", "authors": "M. Salvato, Sophia R. Williams, Cara M. Nunez, Xin Zhu, Ali Israr,\n  Frances Lau, Keith Klumb, Freddy Abnousi, Allison M. Okamura, Heather\n  Culbertson", "title": "Data-driven sparse skin stimulation can convey social touch information\n  to humans", "comments": "Under review for IEEE Transactions on Haptics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During social interactions, people use auditory, visual, and haptic cues to\nconvey their thoughts, emotions, and intentions. Due to weight, energy, and\nother hardware constraints, it is difficult to create devices that completely\ncapture the complexity of human touch. Here we explore whether a sparse\nrepresentation of human touch is sufficient to convey social touch signals. To\ntest this we collected a dataset of social touch interactions using a soft\nwearable pressure sensor array, developed an algorithm to map recorded data to\nan array of actuators, then applied our algorithm to create signals that drive\nan array of normal indentation actuators placed on the arm. Using this\nwearable, low-resolution, low-force device, we find that users are able to\ndistinguish the intended social meaning, and compare performance to results\nbased on direct human touch. As online communication becomes more prevalent,\nsuch systems to convey haptic signals could allow for improved distant\nsocializing and empathetic remote human-human interaction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 11:10:51 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Salvato", "M.", ""], ["Williams", "Sophia R.", ""], ["Nunez", "Cara M.", ""], ["Zhu", "Xin", ""], ["Israr", "Ali", ""], ["Lau", "Frances", ""], ["Klumb", "Keith", ""], ["Abnousi", "Freddy", ""], ["Okamura", "Allison M.", ""], ["Culbertson", "Heather", ""]]}, {"id": "2103.14491", "submitter": "Yi-Hao Peng", "authors": "Yi-Hao Peng, JiWoong Jang, Jeffrey P. Bigham, Amy Pavel", "title": "Say It All: Feedback for Improving Non-Visual Presentation Accessibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Presenters commonly use slides as visual aids for informative talks. When\npresenters fail to verbally describe the content on their slides, blind and\nvisually impaired audience members lose access to necessary content, making the\npresentation difficult to follow. Our analysis of 90 presentation videos\nrevealed that 72% of 610 visual elements (e.g., images, text) were\ninsufficiently described. To help presenters create accessible presentations,\nwe introduce Presentation A11y, a system that provides real-time and\npost-presentation accessibility feedback. Our system analyzes visual elements\non the slide and the transcript of the verbal presentation to provide\nelement-level feedback on what visual content needs to be further described or\neven removed. Presenters using our system with their own slide-based\npresentations described more of the content on their slides, and identified\n3.26 times more accessibility problems to fix after the talk than when using a\ntraditional slide-based presentation interface. Integrating accessibility\nfeedback into content creation tools will improve the accessibility of\ninformational content for all.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 14:27:35 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Peng", "Yi-Hao", ""], ["Jang", "JiWoong", ""], ["Bigham", "Jeffrey P.", ""], ["Pavel", "Amy", ""]]}, {"id": "2103.14539", "submitter": "Angelos Chatzimparmpas", "authors": "Angelos Chatzimparmpas, Rafael M. Martins, Kostiantyn Kucher, Andreas\n  Kerren", "title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise\n  Selection and Semi-Automatic Extraction Approaches", "comments": "This manuscript is currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning (ML) life cycle involves a series of iterative steps,\nfrom the effective gathering and preparation of the data, including complex\nfeature engineering processes, to the presentation and improvement of results,\nwith various algorithms to choose from in every step. Feature engineering in\nparticular can be very beneficial for ML, leading to numerous improvements such\nas boosting the predictive results, decreasing computational times, reducing\nexcessive noise, and increasing the transparency behind the decisions taken\nduring the training. Despite that, while several visual analytics tools exist\nto monitor and control the different stages of the ML life cycle (especially\nthose related to data and algorithms), feature engineering support remains\ninadequate. In this paper, we present FeatureEnVi, a visual analytics system\nspecifically designed to assist with the feature engineering process. Our\nproposed system helps users to choose the most important feature, to transform\nthe original features into powerful alternatives, and to experiment with\ndifferent feature generation combinations. Additionally, data space slicing\nallows users to explore the impact of features on both local and global scales.\nFeatureEnVi utilizes multiple automatic feature selection techniques;\nfurthermore, it visually guides users with statistical evidence about the\ninfluence of each feature (or subsets of features). The final outcome is the\nextraction of heavily engineered features, evaluated by multiple validation\nmetrics. The usefulness and applicability of FeatureEnVi are demonstrated with\ntwo use cases, using a popular red wine quality data set and publicly available\ndata related to vehicle recognition from their silhouettes. We also report\nfeedback from interviews with ML experts and a visualization researcher who\nassessed the effectiveness of our system.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:45:19 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Chatzimparmpas", "Angelos", ""], ["Martins", "Rafael M.", ""], ["Kucher", "Kostiantyn", ""], ["Kerren", "Andreas", ""]]}, {"id": "2103.14625", "submitter": "Zijie Wang", "authors": "Zijie J. Wang, Robert Turko, Duen Horng Chau", "title": "Dodrio: Exploring Transformer Models with Interactive Visualization", "comments": "10 pages, 8 figures, Accepted to ACL 2021. For a demo video, see\n  https://youtu.be/qB-T9j7UTgE . For a live demo, see\n  https://poloclub.github.io/dodrio/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why do large pre-trained transformer-based models perform so well across a\nwide variety of NLP tasks? Recent research suggests the key may lie in\nmulti-headed attention mechanism's ability to learn and represent linguistic\ninformation. Understanding how these models represent both syntactic and\nsemantic knowledge is vital to investigate why they succeed and fail, what they\nhave learned, and how they can improve. We present Dodrio, an open-source\ninteractive visualization tool to help NLP researchers and practitioners\nanalyze attention mechanisms in transformer-based models with linguistic\nknowledge. Dodrio tightly integrates an overview that summarizes the roles of\ndifferent attention heads, and detailed views that help users compare attention\nweights with the syntactic structure and semantic information in the input\ntext. To facilitate the visual comparison of attention weights and linguistic\nknowledge, Dodrio applies different graph visualization techniques to represent\nattention weights scalable to longer input text. Case studies highlight how\nDodrio provides insights into understanding the attention mechanism in\ntransformer-based models. Dodrio is available at\nhttps://poloclub.github.io/dodrio/.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:39:37 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 17:42:50 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 14:51:10 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Zijie J.", ""], ["Turko", "Robert", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2103.14627", "submitter": "Marco Cavallo", "authors": "Marco Cavallo", "title": "Higher Dimensional Graphics: Conceiving Worlds in Four Spatial\n  Dimensions and Beyond", "comments": "Eurographics 2021 / Computer Graphics Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the interpretation of high-dimensional datasets has become a necessity\nin most industries, and is supported by continuous advances in data science and\nmachine learning, the spatial visualization of higher-dimensional geometry has\nmostly remained a niche research topic for mathematicians and physicists.\nIntermittent contributions to this field date back more than a century, and\nhave had a non-negligible influence on contemporary art and philosophy.\nHowever, most contributions have focused on the understanding of specific\nmathematical shapes, with few concrete applications. In this work, we attempt\nto revive the community's interest in visualizing higher dimensional geometry\nby shifting the focus from the visualization of abstract shapes to the design\nof a broader hyper-universe concept, wherein 3D and 4D objects can coexist and\ninteract with each other. Specifically, we discuss the content definition,\nauthoring patterns, and technical implementations associated with the process\nof extending standard 3D applications as to support 4D mechanics. We\noperationalize our ideas through the introduction of a new hybrid 3D/4D\nvideogame called Across Dimensions, which we developed in Unity3D through the\nintegration of our own 4D plugin.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:41:25 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Cavallo", "Marco", ""]]}, {"id": "2103.14712", "submitter": "Arijit Ray", "authors": "Arijit Ray, Michael Cogswell, Xiao Lin, Kamran Alipour, Ajay\n  Divakaran, Yi Yao, Giedrius Burachas", "title": "Knowing What VQA Does Not: Pointing to Error-Inducing Regions to Improve\n  Explanation Helpfulness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention maps, a popular heatmap-based explanation method for Visual\nQuestion Answering (VQA), are supposed to help users understand the model by\nhighlighting portions of the image/question used by the model to infer answers.\nHowever, we see that users are often misled by current attention map\nvisualizations that point to relevant regions despite the model producing an\nincorrect answer. Hence, we propose Error Maps that clarify the error by\nhighlighting image regions where the model is prone to err. Error maps can\nindicate when a correctly attended region may be processed incorrectly leading\nto an incorrect answer, and hence, improve users' understanding of those cases.\nTo evaluate our new explanations, we further introduce a metric that simulates\nusers' interpretation of explanations to evaluate their potential helpfulness\nto understand model correctness. We finally conduct user studies to see that\nour new explanations help users understand model correctness better than\nbaselines by an expected 30% and that our proxy helpfulness metrics correlate\nstrongly ($\\rho$>0.97) with how well users can predict model correctness.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 19:52:32 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 21:15:40 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Ray", "Arijit", ""], ["Cogswell", "Michael", ""], ["Lin", "Xiao", ""], ["Alipour", "Kamran", ""], ["Divakaran", "Ajay", ""], ["Yao", "Yi", ""], ["Burachas", "Giedrius", ""]]}, {"id": "2103.14792", "submitter": "Feng Zhou", "authors": "Feng Zhou, X. Jessie Yang, Joost de Winter", "title": "Using Eye-tracking Data to Predict Situation Awareness in Real Time\n  during Takeover Transitions in Conditionally Automated Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Situation awareness (SA) is critical to improving takeover performance during\nthe transition period from automated driving to manual driving. Although many\nstudies measured SA during or after the driving task, few studies have\nattempted to predict SA in real time in automated driving. In this work, we\npropose to predict SA during the takeover transition period in conditionally\nautomated driving using eye-tracking and self-reported data. First, a tree\nensemble machine learning model, named LightGBM (Light Gradient Boosting\nMachine), was used to predict SA. Second, in order to understand what factors\ninfluenced SA and how, SHAP (SHapley Additive exPlanations) values of\nindividual predictor variables in the LightGBM model were calculated. These\nSHAP values explained the prediction model by identifying the most important\nfactors and their effects on SA, which further improved the model performance\nof LightGBM through feature selection. We standardized SA between 0 and 1 by\naggregating three performance measures (i.e., placement, distance, and speed\nestimation of vehicles with regard to the ego-vehicle) of SA in recreating\nsimulated driving scenarios, after 33 participants viewed 32 videos with six\nlengths between 1 and 20 s. Using only eye-tracking data, our proposed model\noutperformed other selected machine learning models, having a root-mean-squared\nerror (RMSE) of 0.121, a mean absolute error (MAE) of 0.096, and a 0.719\ncorrelation coefficient between the predicted SA and the ground truth. The code\nis available at https://github.com/refengchou/Situation-awareness-prediction.\nOur proposed model provided important implications on how to monitor and\npredict SA in real time in automated driving using eye-tracking data.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 02:42:41 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhou", "Feng", ""], ["Yang", "X. Jessie", ""], ["de Winter", "Joost", ""]]}, {"id": "2103.14852", "submitter": "Ilhan Aslan", "authors": "Muhammad Mehran Sunny, Moritz Berghofer, Ilhan Aslan", "title": "Towards Tool-Support for Interactive-Machine Learning Applications in\n  the Android Ecosystem", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer applications are becoming increasingly smarter and most of them have\nto run on device ecosystems. Potential benefits are for example enabling\ncross-device interaction and seamless user experiences. Essential for today's\nsmart solutions with high performance are machine learning models. However,\nthese models are often developed separately by AI engineers for one specific\ndevice and do not consider the challenges and potentials associated with a\ndevice ecosystem in which their models have to run. We believe that there is a\nneed for tool-support for AI engineers to address the challenges of\nimplementing, testing, and deploying machine learning models for a next\ngeneration of smart interactive consumer applications. This paper presents\npreliminary results of a series of inquiries, including interviews with AI\nengineers and experiments for an interactive machine learning use case with a\nSmartwatch and Smartphone. We identified the themes through interviews and\nhands-on experience working on our use case and proposed features, such as data\ncollection from sensors and easy testing of the resources consumption of\nrunning pre-processing code on the target device, which will serve as\ntool-support for AI engineers.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 09:28:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sunny", "Muhammad Mehran", ""], ["Berghofer", "Moritz", ""], ["Aslan", "Ilhan", ""]]}, {"id": "2103.14853", "submitter": "Gavin Buckingham", "authors": "Gavin Buckingham", "title": "Hand tracking for immersive virtual reality: opportunities and\n  challenges", "comments": "8 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hand tracking has become an integral feature of recent generations of\nimmersive virtual reality head-mounted displays. With the widespread adoption\nof this feature, hardware engineers and software developers are faced with an\nexciting array of opportunities and a number of challenges, mostly in relation\nto the human user. In this article, I outline what I see as the main\npossibilities for hand tracking to add value to immersive virtual reality as\nwell as some of the potential challenges in the context of the psychology and\nneuroscience of the human user. It is hoped that this paper serves as a roadmap\nfor the development of best practices in the field for the development of\nsubsequent generations of hand tracking and virtual reality technologies.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 09:28:47 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Buckingham", "Gavin", ""]]}, {"id": "2103.14956", "submitter": "Philip Hausner", "authors": "Philip Hausner and Michael Gertz", "title": "Dark Patterns in the Interaction with Cookie Banners", "comments": "5 pages, 3 figures, Position Paper at the Workshop \"What Can CHI Do\n  About Dark Patterns?\" at the CHI Conference on Human Factors in Computing\n  Systems (CHI 2021), May 8-13, 2021, Yokohama, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dark patterns are interface designs that nudge users towards behavior that is\nagainst their best interests. Since humans are often not even aware that they\nare influenced by these malicious patterns, research has to identify ways to\nprotect web users against them. One approach to this is the automatic detection\nof dark patterns which enables the development of tools that are able to\nprotect users by proactively warning them in cases where they face a dark\npattern. In this paper, we present ongoing work in the direction of automatic\ndetection of dark patterns, and outline an example to detect malicious patterns\nwithin the domain of cookie banners.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 18:08:29 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Hausner", "Philip", ""], ["Gertz", "Michael", ""]]}, {"id": "2103.14973", "submitter": "Hua Shen", "authors": "Hua Shen, Ting-Hao 'Kenneth' Huang", "title": "Explaining the Road Not Taken", "comments": "Accepted by The 2021 ACM CHI Workshop on Operationalizing\n  Human-Centered Perspectives in Explainable AI (CHI 2021 HCXAI Workshop). For\n  associated website, see https://human-centered-exnlp.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is unclear if existing interpretations of deep neural network models\nrespond effectively to the needs of users. This paper summarizes the common\nforms of explanations (such as feature attribution, decision rules, or probes)\nused in over 200 recent papers about natural language processing (NLP), and\ncompares them against user questions collected in the XAI Question Bank. We\nfound that although users are interested in explanations for the road not taken\n-- namely, why the model chose one result and not a well-defined, seemly\nsimilar legitimate counterpart -- most model interpretations cannot answer\nthese questions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 19:47:06 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 04:51:50 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shen", "Hua", ""], ["Huang", "Ting-Hao 'Kenneth'", ""]]}, {"id": "2103.14976", "submitter": "Ana Lucic", "authors": "Ana Lucic, Madhulika Srikumar, Umang Bhatt, Alice Xiang, Ankur Taly,\n  Q. Vera Liao, Maarten de Rijke", "title": "A Multistakeholder Approach Towards Evaluating AI Transparency\n  Mechanisms", "comments": "Accepted to CHI 2021 Workshop on Operationalizing Human-Centered\n  Perspectives in Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given that there are a variety of stakeholders involved in, and affected by,\ndecisions from machine learning (ML) models, it is important to consider that\ndifferent stakeholders have different transparency needs. Previous work found\nthat the majority of deployed transparency mechanisms primarily serve technical\nstakeholders. In our work, we want to investigate how well transparency\nmechanisms might work in practice for a more diverse set of stakeholders by\nconducting a large-scale, mixed-methods user study across a range of\norganizations, within a particular industry such as health care, criminal\njustice, or content moderation. In this paper, we outline the setup for our\nstudy.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 19:57:20 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 10:29:44 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Lucic", "Ana", ""], ["Srikumar", "Madhulika", ""], ["Bhatt", "Umang", ""], ["Xiang", "Alice", ""], ["Taly", "Ankur", ""], ["Liao", "Q. Vera", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2103.15004", "submitter": "Carolin Wienrich Prof. Dr.", "authors": "Carolin Wienrich and Marc Erich Latoschik", "title": "eXtended Artificial Intelligence: New Prospects of Human-AI Interaction\n  Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial Intelligence (AI) covers a broad spectrum of computational\nproblems and use cases. Many of those implicate profound and sometimes\nintricate questions of how humans interact or should interact with AIs.\nMoreover, many users or future users do have abstract ideas of what AI is,\nsignificantly depending on the specific embodiment of AI applications.\nHuman-centered-design approaches would suggest evaluating the impact of\ndifferent embodiments on human perception of and interaction with AI. An\napproach that is difficult to realize due to the sheer complexity of\napplication fields and embodiments in reality. However, here XR opens new\npossibilities to research human-AI interactions. The article's contribution is\ntwofold: First, it provides a theoretical treatment and model of human-AI\ninteraction based on an XR-AI continuum as a framework for and a perspective of\ndifferent approaches of XR-AI combinations. It motivates XR-AI combinations as\na method to learn about the effects of prospective human-AI interfaces and\nshows why the combination of XR and AI fruitfully contributes to a valid and\nsystematic investigation of human-AI interactions and interfaces. Second, the\narticle provides two exemplary experiments investigating the aforementioned\napproach for two distinct AI-systems. The first experiment reveals an\ninteresting gender effect in human-robot interaction, while the second\nexperiment reveals an Eliza effect of a recommender system. Here the article\nintroduces two paradigmatic implementations of the proposed XR testbed for\nhuman-AI interactions and interfaces and shows how a valid and systematic\ninvestigation can be conducted. In sum, the article opens new perspectives on\nhow XR benefits human-centered AI design and development.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 22:12:06 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 11:18:14 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 16:10:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wienrich", "Carolin", ""], ["Latoschik", "Marc Erich", ""]]}, {"id": "2103.15048", "submitter": "Mustaffa Alfatlawi", "authors": "Mustaffa Alfatlawi", "title": "An Affective Approach for Behavioral Performance Estimation and\n  Induction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotions have a major interactive role in defining how humans interact with\ntheir environment by encoding their perception to external events and\naccordingly, influencing their cognition and decision-making process.\nTherefore, increasing attention has been directed toward integrating human\naffective states into system design in order to optimize the quality of task\nperformance. In this work, we seize on the significant correlation between\nemotions and behavioral performance that is reported in several psychological\nstudies and develop an online closed-loop design framework for Human-Robot\nInteraction (HRI). The proposed approach monitors the behavioral performance\nbased on the levels of Pleasure, Arousal, and Dominance (PAD) states for the\nhuman operator and when required, applies an external stimulus which is\nselected to induce an improvement in performance. The framework is implemented\non an HRI task involving a human operator teleoperating an articulated robotic\nmanipulator. Our statistical analysis shows a significant decrease in pleasure,\narousal, and dominance states as the behavioral performance deteriorates $(p <\n0.05)$. Our closed-loop experiment that uses an audio stimulus to improve\nemotional state shows a significant improvement in the behavioral performance\nof certain subjects.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 04:41:21 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Alfatlawi", "Mustaffa", ""]]}, {"id": "2103.15113", "submitter": "Min Chen", "authors": "Min Chen", "title": "A Short Introduction to Information-Theoretic Cost-Benefit Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:2103.02502;\n  text overlap with arXiv:2103.02505", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.HC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This arXiv report provides a short introduction to the information-theoretic\nmeasure proposed by Chen and Golan in 2016 for analyzing machine- and\nhuman-centric processes in data intelligence workflows. This introduction was\ncompiled based on several appendices written to accompany a few research papers\non topics of data visualization and visual analytics. Although the original\n2016 paper and the follow-on papers were mostly published in the field of\nvisualization and visual analytics, the cost-benefit measure can help explain\nthe informative trade-off in a wide range of data intelligence phenomena\nincluding machine learning, human cognition, language development, and so on.\nMeanwhile, there is an ongoing effort to improve its mathematical properties in\norder to make it more intuitive and usable in practical applications as a\nmeasurement tool.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 12:25:56 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Min", ""]]}, {"id": "2103.15256", "submitter": "Zhonghao Shi", "authors": "Zhonghao Shi, Manwei Cao, Sophia Pei, Xiaoyang Qiao, Thomas R\n  Groechel, Maja J Matari\\'c", "title": "Personalized Affect-Aware Socially Assistive Robot Tutors Aimed at\n  Fostering Social Grit in Children with Autism", "comments": "Accepted to ACM/IEEE International Conference on Human-Robot\n  Interaction Workshop on Child-Robot Interaction and Child's Fundamental\n  Rights", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affect-aware socially assistive robotics (SAR) tutors have great potential to\naugment and democratize professional therapeutic interventions for children\nwith autism spectrum disorders (ASD) from different socioeconomic backgrounds.\nHowever, the majority of research on SAR for ASD has been on teaching cognitive\nand/or social skills, not on addressing users' emotional needs for real-world\nsocial situations. To bridge that gap, this work aims to develop personalized\naffect-aware SAR tutors to help alleviate social anxiety and foster social\ngrit-the growth mindset for social skill development-in children with ASD. We\npropose a novel paradigm to incorporate clinically validated Acceptance and\nCommitment Training (ACT) with personalized SAR interventions. This work paves\nthe way toward developing personalized affect-aware SAR interventions to\nsupport the unique and diverse socio-emotional needs and challenges of children\nwith ASD.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 00:46:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Shi", "Zhonghao", ""], ["Cao", "Manwei", ""], ["Pei", "Sophia", ""], ["Qiao", "Xiaoyang", ""], ["Groechel", "Thomas R", ""], ["Matari\u0107", "Maja J", ""]]}, {"id": "2103.15307", "submitter": "Dingwen Zhang", "authors": "Dingwen Zhang, Bo Wang, Gerong Wang, Qiang Zhang, Jiajia Zhang,\n  Jungong Han, Zheng You", "title": "Onfocus Detection: Identifying Individual-Camera Eye Contact from\n  Unconstrained Images", "comments": null, "journal-ref": "SCIENCE CHINA Information Sciences, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Onfocus detection aims at identifying whether the focus of the individual\ncaptured by a camera is on the camera or not. Based on the behavioral research,\nthe focus of an individual during face-to-camera communication leads to a\nspecial type of eye contact, i.e., the individual-camera eye contact, which is\na powerful signal in social communication and plays a crucial role in\nrecognizing irregular individual status (e.g., lying or suffering mental\ndisease) and special purposes (e.g., seeking help or attracting fans). Thus,\ndeveloping effective onfocus detection algorithms is of significance for\nassisting the criminal investigation, disease discovery, and social behavior\nanalysis. However, the review of the literature shows that very few efforts\nhave been made toward the development of onfocus detector due to the lack of\nlarge-scale public available datasets as well as the challenging nature of this\ntask. To this end, this paper engages in the onfocus detection research by\naddressing the above two issues. Firstly, we build a large-scale onfocus\ndetection dataset, named as the OnFocus Detection In the Wild (OFDIW). It\nconsists of 20,623 images in unconstrained capture conditions (thus called ``in\nthe wild'') and contains individuals with diverse emotions, ages, facial\ncharacteristics, and rich interactions with surrounding objects and background\nscenes. On top of that, we propose a novel end-to-end deep model, i.e., the\neye-context interaction inferring network (ECIIN), for onfocus detection, which\nexplores eye-context interaction via dynamic capsule routing. Finally,\ncomprehensive experiments are conducted on the proposed OFDIW dataset to\nbenchmark the existing learning models and demonstrate the effectiveness of the\nproposed ECIIN. The project (containing both datasets and codes) is at\nhttps://github.com/wintercho/focus.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 03:29:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Dingwen", ""], ["Wang", "Bo", ""], ["Wang", "Gerong", ""], ["Zhang", "Qiang", ""], ["Zhang", "Jiajia", ""], ["Han", "Jungong", ""], ["You", "Zheng", ""]]}, {"id": "2103.15462", "submitter": "Claudia M\\\"uller-Birn", "authors": "Claudia M\\\"uller-Birn (1), Katrin Glinka (1), Peter S\\\"orries (1),\n  Michael Tebbe (1) and Susanne Michl (2) ((1) Freie Universit\\\"at Berlin,\n  Human-Centered Computing, (2) Charit\\'e Medical Humanities and Ethics in\n  Medicine)", "title": "Situated Case Studies for a Human-Centered Design of Explanation User\n  Interfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Researchers and practitioners increasingly consider a human-centered\nperspective in the design of machine learning-based applications, especially in\nthe context of Explainable Artificial Intelligence (XAI). However, clear\nmethodological guidance in this context is still missing because each new\nsituation seems to require a new setup, which also creates different\nmethodological challenges. Existing case study collections in XAI inspired us;\ntherefore, we propose a similar collection of case studies for human-centered\nXAI that can provide methodological guidance or inspiration for others. We want\nto showcase our idea in this workshop by describing three case studies from our\nresearch. These case studies are selected to highlight how apparently small\ndifferences require a different set of methods and considerations. With this\nworkshop contribution, we would like to engage in a discussion on how such a\ncollection of case studies can provide a methodological guidance and critical\nreflection.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:55:43 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["M\u00fcller-Birn", "Claudia", ""], ["Glinka", "Katrin", ""], ["S\u00f6rries", "Peter", ""], ["Tebbe", "Michael", ""], ["Michl", "Susanne", ""]]}, {"id": "2103.15721", "submitter": "Kushal Chawla", "authors": "Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale Lucas, Jonathan May,\n  Jonathan Gratch", "title": "CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic\n  Negotiation Systems", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated systems that negotiate with humans have broad applications in\npedagogy and conversational AI. To advance the development of practical\nnegotiation systems, we present CaSiNo: a novel corpus of over a thousand\nnegotiation dialogues in English. Participants take the role of campsite\nneighbors and negotiate for food, water, and firewood packages for their\nupcoming trip. Our design results in diverse and linguistically rich\nnegotiations while maintaining a tractable, closed-domain environment. Inspired\nby the literature in human-human negotiations, we annotate persuasion\nstrategies and perform correlation analysis to understand how the dialogue\nbehaviors are associated with the negotiation performance. We further propose\nand evaluate a multi-task framework to recognize these strategies in a given\nutterance. We find that multi-task learning substantially improves the\nperformance for all strategy labels, especially for the ones that are the most\nskewed. We release the dataset, annotations, and the code to propel future work\nin human-machine negotiations: https://github.com/kushalchawla/CaSiNo\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:07:25 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 02:36:51 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chawla", "Kushal", ""], ["Ramirez", "Jaysa", ""], ["Clever", "Rene", ""], ["Lucas", "Gale", ""], ["May", "Jonathan", ""], ["Gratch", "Jonathan", ""]]}, {"id": "2103.15781", "submitter": "Bereket Abera Yilma Mr.", "authors": "Bereket Abera Yilma, Yannick Naudet and Herv\\'e Panetto", "title": "Towards a Personalisation Framework for Cyber-Physical-Social System\n  (CPSS)", "comments": "Accepted at the 17th IFAC Symposium on Information Control Problems\n  in Manufacturing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Cyber-Physical-Social System (CPSS) is an emerging paradigm often\nunderstood as a physical and virtual space of interaction which is cohabited by\nhumans and sensor-enabled smart devices. In such settings, human interaction\nbehaviour is often different from person to person and is guided by complex\nenvironmental and natural factors that are not yet fully explored. Thus,\nensuring a seamless human-machine interaction in CPSS calls for efficient means\nof handling human dynamics and bringing interaction experience to a personal\nlevel. To this end in this paper, we propose a personalisation framework to\nsupport the design of CPSS in recognising and addressing human/social aspects.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 10:15:16 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 07:22:58 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yilma", "Bereket Abera", ""], ["Naudet", "Yannick", ""], ["Panetto", "Herv\u00e9", ""]]}, {"id": "2103.15787", "submitter": "Micah Smith", "authors": "Micah J. Smith, J\\\"urgen Cito, Kalyan Veeramachaneni", "title": "Meeting in the notebook: a notebook-based environment for\n  micro-submissions in data science collaborations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Developers in data science and other domains frequently use computational\nnotebooks to create exploratory analyses and prototype models. However, they\noften struggle to incorporate existing software engineering tooling into these\nnotebook-based workflows, leading to fragile development processes. We\nintroduce Assembl\\'{e}, a new development environment for collaborative data\nscience projects, in which promising code fragments of data science pipelines\ncan be contributed as pull requests to an upstream repository entirely from\nwithin JupyterLab, abstracting away low-level version control tool usage. We\ndescribe the design and implementation of Assembl\\'{e} and report on a user\nstudy of 23 data scientists.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:31:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Smith", "Micah J.", ""], ["Cito", "J\u00fcrgen", ""], ["Veeramachaneni", "Kalyan", ""]]}, {"id": "2103.15990", "submitter": "Albara Ramli", "authors": "Rex Liu, Albara Ah Ramli, Huanle Zhang, Esha Datta, Erik Henricson,\n  Xin Liu", "title": "An Overview of Human Activity Recognition Using Wearable Sensors:\n  Healthcare and Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of the internet of things (IoT) and artificial\nintelligence (AI) technologies, human activity recognition (HAR) has been\napplied in a variety of domains such as security and surveillance, human-robot\ninteraction, and entertainment. Even though a number of surveys and review\npapers have been published, there is a lack of HAR overview papers focusing on\nhealthcare applications that use wearable sensors. Therefore, we fill in the\ngap by presenting this overview paper. In particular, we present our projects\nto illustrate the system design of HAR applications for healthcare. Our\nprojects include early mobility identification of human activities for\nintensive care unit (ICU) patients and gait analysis of Duchenne muscular\ndystrophy (DMD) patients. We cover essential components of designing HAR\nsystems including sensor factors (e.g., type, number, and placement location),\nAI model selection (e.g., classical machine learning models versus deep\nlearning models), and feature engineering. In addition, we highlight the\nchallenges of such healthcare-oriented HAR systems and propose several research\nopportunities for both the medical and the computer science community.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 23:48:51 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 06:53:45 GMT"}, {"version": "v3", "created": "Sun, 4 Jul 2021 12:41:11 GMT"}, {"version": "v4", "created": "Wed, 28 Jul 2021 04:39:20 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Liu", "Rex", ""], ["Ramli", "Albara Ah", ""], ["Zhang", "Huanle", ""], ["Datta", "Esha", ""], ["Henricson", "Erik", ""], ["Liu", "Xin", ""]]}, {"id": "2103.16060", "submitter": "Scott Davidoff", "authors": "Connie Ye, Lukas Hermann, Nur Yildirim, Shravya Bhat, Dominik Moritz,\n  Scott Davidoff", "title": "PIXLISE-C: Exploring The Data Analysis Needs of NASA Scientists for\n  Mineral Identification", "comments": "5 pages, 6 figures, ACM Conference on Human Factors in Computing\n  Systems Workshop on Human-Computer Interaction for Space Exploration\n  (SpaceCHI 2021), https://spacechi.media.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR physics.comp-ph physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NASA JPL scientists working on the micro x-ray fluorescence (microXRF)\nspectroscopy data collected from Mars surface perform data analysis to look for\nsigns of past microbial life on Mars. Their data analysis workflow mainly\ninvolves identifying mineral compounds through the element abundance in\nspatially distributed data points. Working with the NASA JPL team, we\nidentified pain points and needs to further develop their existing data\nvisualization and analysis tool. Specifically, the team desired improvements\nfor the process of creating and interpreting mineral composition groups. To\naddress this problem, we developed an interactive tool that enables scientists\nto (1) cluster the data using either manual lasso-tool selection or through\nvarious machine learning clustering algorithms, and (2) compare the clusters\nand individual data points to make informed decisions about mineral\ncompositions. Our preliminary tool supports a hybrid data analysis workflow\nwhere the user can manually refine the machine-generated clusters.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 04:11:22 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ye", "Connie", ""], ["Hermann", "Lukas", ""], ["Yildirim", "Nur", ""], ["Bhat", "Shravya", ""], ["Moritz", "Dominik", ""], ["Davidoff", "Scott", ""]]}, {"id": "2103.16153", "submitter": "Sangsun Han", "authors": "Hojun Aan, Sangsun Han, Hyeonkyu Kim, Jimoon Kim, Pilhyoun Yoon, Kibum\n  Kim", "title": "Remote Virtual Showdown: A Collaborative Virtual Reality Game for People\n  with Visual Impairments", "comments": "31pages, 7 figures, 5 Table, submitted to CSCW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many researchers have developed VR systems for people with visual impairments\nby using various audio feedback techniques. However, there has been much less\nstudy of collaborative VR systems in which people with visual impairments and\npeople with able-body can participate together. Therefore, we developed a VR\nshowdown game which is similar to a real Showdown game in which two players can\nplay together in the same virtual environment. We incorporate auditory distance\nperception using the HRTF (Head Related Transform Function) based on a spatial\nposition in VR. We developed two modes in the showdown game. One is the PVA\n(Player vs. Agent) mode in which people with visual impairments can play alone\nand the PVP (Player vs. Player) mode in which people with visual impairments\ncan play with another player in the network environment. We conducted our user\nstudies by comparing the performances of people with visual impairments and\npeople with able-body. The user study results show that people with visual\nimpairments won 67.6% of the games when competing against people with\nable-body. This paper reports an example of a collaborative VR system for\npeople with visual impairments and also design guideline for developing VR\nsystems for people with visual impairments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:20:54 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 03:25:34 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Aan", "Hojun", ""], ["Han", "Sangsun", ""], ["Kim", "Hyeonkyu", ""], ["Kim", "Jimoon", ""], ["Yoon", "Pilhyoun", ""], ["Kim", "Kibum", ""]]}, {"id": "2103.16168", "submitter": "Jichen Zhu", "authors": "Jennifer Villareale, Jichen Zhu", "title": "Understanding Mental Models of AI through Player-AI Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Designing human-centered AI-driven applications require deep understandings\nof how people develop mental models of AI. Currently, we have little knowledge\nof this process and limited tools to study it. This paper presents the position\nthat AI-based games, particularly the player-AI interaction component, offer an\nideal domain to study the process in which mental models evolve. We present a\ncase study to illustrate the benefits of our approach for explainable AI.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:49:45 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Villareale", "Jennifer", ""], ["Zhu", "Jichen", ""]]}, {"id": "2103.16269", "submitter": "Chenglin Xu", "authors": "Chenglin Xu and Wei Rao and Jibin Wu and Haizhou Li", "title": "Target Speaker Verification with Selective Auditory Attention for Single\n  and Multi-talker Speech", "comments": "13 pages, submitted to IEEE/ACM transaction on Audio, Speech and\n  Language on 10 Jan. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.HC cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker verification has been studied mostly under the single-talker\ncondition. It is adversely affected in the presence of interference speakers.\nInspired by the study on target speaker extraction, e.g., SpEx, we propose a\nunified speaker verification framework for both single- and multi-talker\nspeech, that is able to pay selective auditory attention to the target speaker.\nThis target speaker verification (tSV) framework jointly optimizes a speaker\nattention module and a speaker representation module via multi-task learning.\nWe study four different target speaker embedding schemes under the tSV\nframework. The experimental results show that all four target speaker embedding\nschemes significantly outperform other competitive solutions for multi-talker\nspeech. Notably, the best tSV speaker embedding scheme achieves 76.0% and 55.3%\nrelative improvements over the baseline system on the WSJ0-2mix-extr and\nLibri2Mix corpora in terms of equal-error-rate for 2-talker speech, while the\nperformance of tSV for single-talker speech is on par with that of traditional\nspeaker verification system, that is trained and evaluated under the same\nsingle-talker condition.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 11:40:35 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 08:29:56 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Xu", "Chenglin", ""], ["Rao", "Wei", ""], ["Wu", "Jibin", ""], ["Li", "Haizhou", ""]]}, {"id": "2103.16399", "submitter": "Pawe{\\l} W. Wo\\'zniak", "authors": "Marit Bentvelzen and Jasmin Niess and Miko{\\l}aj P. Wo\\'zniak and\n  Pawe{\\l} W. Wo\\'zniak", "title": "The Development and Validation of the Technology-Supported Reflection\n  Inventory", "comments": "CHI Conference on Human Factors in Computing Systems (CHI '21), May\n  8--13, 2021, Yokohama, Japan", "journal-ref": null, "doi": "10.1145/3411764.3445673", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflection is an often addressed design goal in Human-Computer Interaction\n(HCI) research. An increasing number of artefacts for reflection have been\ndeveloped in recent years. However, evaluating if and how an interactive\ntechnology helps a user reflect is still complex. This makes it difficult to\ncompare artefacts (or prototypes) for reflection, impeding future design\nefforts. To address this issue, we developed the \\emph{Technology-Supported\nReflection Inventory} (TSRI), which is a scale that evaluates how effectively a\nsystem supports reflection. We first created a list of possible scale items\nbased on past work in defining reflection. The items were then reviewed by\nexperts. Next, we performed exploratory factor analysis to reduce the scale to\nits final length of nine items. Subsequently, we confirmed test-retest validity\nof our instrument, as well as its construct validity. The TSRI enables\nresearchers and practitioners to compare prototypes designed to support\nreflection.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:48:42 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Bentvelzen", "Marit", ""], ["Niess", "Jasmin", ""], ["Wo\u017aniak", "Miko\u0142aj P.", ""], ["Wo\u017aniak", "Pawe\u0142 W.", ""]]}, {"id": "2103.16427", "submitter": "Zhao Han", "authors": "Zhao Han, Adam Norton, Eric McCann, Lisa Baraniecki, Will Ober, Dave\n  Shane, Anna Skinner, Holly A. Yanco", "title": "Investigation of Multiple Resource Theory Design Principles on Robot\n  Teleoperation and Workload Management", "comments": "7 pages, 13 figures, ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot interfaces often only use the visual channel. Inspired by Wickens'\nMultiple Resource Theory, we investigated if the addition of audio elements\nwould reduce cognitive workload and improve performance. Specifically, we\ndesigned a search and threat-defusal task (primary) with a memory test task\n(secondary). Eleven participants - predominantly first responders - were\nrecruited to control a robot to clear all threats in a combination of four\nconditions of primary and secondary tasks in visual and auditory channels. We\ndid not find any statistically significant differences in performance or\nworkload across subjects, making it questionable that Multiple Resource Theory\ncould shorten longer-term task completion time and reduce workload. Our results\nsuggest that considering individual differences for splitting interface\nmodalities across multiple channels requires further investigation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:23:15 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 01:43:57 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Han", "Zhao", ""], ["Norton", "Adam", ""], ["McCann", "Eric", ""], ["Baraniecki", "Lisa", ""], ["Ober", "Will", ""], ["Shane", "Dave", ""], ["Skinner", "Anna", ""], ["Yanco", "Holly A.", ""]]}, {"id": "2103.16429", "submitter": "Hsuan Su", "authors": "Hsuan Su, Jiun-Hao Jhan, Fan-yun Sun, Saurav Sahay, Hung-yi Lee", "title": "Put Chatbot into Its Interlocutor's Shoes: New Framework to Learn\n  Chatbot Responding with Intention", "comments": "Accepted at NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most chatbot literature that focuses on improving the fluency and coherence\nof a chatbot, is dedicated to making chatbots more human-like. However, very\nlittle work delves into what really separates humans from chatbots -- humans\nintrinsically understand the effect their responses have on the interlocutor\nand often respond with an intention such as proposing an optimistic view to\nmake the interlocutor feel better. This paper proposes an innovative framework\nto train chatbots to possess human-like intentions. Our framework includes a\nguiding chatbot and an interlocutor model that plays the role of humans. The\nguiding chatbot is assigned an intention and learns to induce the interlocutor\nto reply with responses matching the intention, for example, long responses,\njoyful responses, responses with specific words, etc. We examined our framework\nusing three experimental setups and evaluated the guiding chatbot with four\ndifferent metrics to demonstrate flexibility and performance advantages.\nAdditionally, we performed trials with human interlocutors to substantiate the\nguiding chatbot's effectiveness in influencing the responses of humans to a\ncertain extent. Code will be made available to the public.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:24:37 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 17:39:23 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 03:42:16 GMT"}, {"version": "v4", "created": "Mon, 12 Apr 2021 15:58:42 GMT"}, {"version": "v5", "created": "Fri, 23 Apr 2021 14:45:14 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Su", "Hsuan", ""], ["Jhan", "Jiun-Hao", ""], ["Sun", "Fan-yun", ""], ["Sahay", "Saurav", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2103.16434", "submitter": "Georgios Papadopoulos Th.", "authors": "Georgios Th. Papadopoulos, Asterios Leonidis, Margherita Antona,\n  Constantine Stephanidis", "title": "User profile-driven large-scale multi-agent learning from demonstration\n  in federated human-robot collaborative environments", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.08174", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning from Demonstration (LfD) has been established as the dominant\nparadigm for efficiently transferring skills from human teachers to robots. In\nthis context, the Federated Learning (FL) conceptualization has very recently\nbeen introduced for developing large-scale human-robot collaborative\nenvironments, targeting to robustly address, among others, the critical\nchallenges of multi-agent learning and long-term autonomy. In the current work,\nthe latter scheme is further extended and enhanced, by designing and\nintegrating a novel user profile formulation for providing a fine-grained\nrepresentation of the exhibited human behavior, adopting a Deep Learning\n(DL)-based formalism. In particular, a hierarchically organized set of key\ninformation sources is considered, including: a) User attributes (e.g.\ndemographic, anthropomorphic, educational, etc.), b) User state (e.g. fatigue\ndetection, stress detection, emotion recognition, etc.) and c)\nPsychophysiological measurements (e.g. gaze, electrodermal activity, heart\nrate, etc.) related data. Then, a combination of Long Short-Term Memory (LSTM)\nand stacked autoencoders, with appropriately defined neural network\narchitectures, is employed for the modelling step. The overall designed scheme\nenables both short- and long-term analysis/interpretation of the human behavior\n(as observed during the feedback capturing sessions), so as to adaptively\nadjust the importance of the collected feedback samples when aggregating\ninformation originating from the same and different human teachers,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:33:21 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Papadopoulos", "Georgios Th.", ""], ["Leonidis", "Asterios", ""], ["Antona", "Margherita", ""], ["Stephanidis", "Constantine", ""]]}, {"id": "2103.16435", "submitter": "Omar Shaikh", "authors": "Omar Shaikh, Jon Saad-Falcon, Austin P Wright, Nilaksh Das, Scott\n  Freitas, Omar Isaac Asensio, Duen Horng Chau", "title": "EnergyVis: Interactively Tracking and Exploring Energy Consumption for\n  ML Models", "comments": "7 pages, 5 figures; CHI 2021 Extended Abstracts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of larger machine learning (ML) models have improved\nstate-of-the-art (SOTA) performance in various modeling tasks, ranging from\ncomputer vision to natural language. As ML models continue increasing in size,\nso does their respective energy consumption and computational requirements.\nHowever, the methods for tracking, reporting, and comparing energy consumption\nremain limited. We presentEnergyVis, an interactive energy consumption tracker\nfor ML models. Consisting of multiple coordinated views, EnergyVis enables\nresearchers to interactively track, visualize and compare model energy\nconsumption across key energy consumption and carbon footprint metrics (kWh and\nCO2), helping users explore alternative deployment locations and hardware that\nmay reduce carbon footprints. EnergyVis aims to raise awareness concerning\ncomputational sustainability by interactively highlighting excessive energy\nusage during model training; and by providing alternative training options to\nreduce energy usage.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:33:43 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shaikh", "Omar", ""], ["Saad-Falcon", "Jon", ""], ["Wright", "Austin P", ""], ["Das", "Nilaksh", ""], ["Freitas", "Scott", ""], ["Asensio", "Omar Isaac", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2103.16443", "submitter": "Frances Cleary Ms", "authors": "Frances Cleary, David Henshall, Sasitharan Balasubramaniam", "title": "On-body Edge Computing through E-Textile Programmable Logic Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  E-textiles has received tremendous attention in recent years due to the\ncapability of integrating sensors into a garment to provide high precision\nsensing of the human body. Besides sensing, a number of solutions for e-textile\ngarments have also integrated wireless interfaces allowing these sensing data\nto be transmitted and also sensors that allow users to provide instructions\nthrough touching. While this has provided a new level of sensing that can\nresult in unprecedented applications, there has been little attention placed on\non-body computing for e-textiles. Facilitating computing on e-textiles can\nresult in a new form of On-body Edge Computing, where sensor information are\nprocessed very close to the body before being transmitted to an external device\nor wireless access point. This form of computing can provide new security and\ndata privacy capabilities and at the same time provide opportunities for new\nenergy harvesting mechanisms to process the data through the garment. This\npaper proposes this concept through embroidered Programmable Logic Array (PLA)\nintegrated into e-textiles. In the way that PLAs have programmable logic\ncircuits by interconnecting different AND, NOT and OR gates, we propose\ne-textile based gates that are sewn into a garment and connected through\nconductive thread stitching. Two designs are proposed and this includes Single\nand Multi-Layered PLA. Experimental validations have been conducted at the\nindividual gates as well as the entire PLA circuits to determine the voltage\nutilization as well as logic computing reliability. Our proposed approach can\nusher in a new form of On-Body Edge Computing for e-textile garments for future\nwearable technologies\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:42:54 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Cleary", "Frances", ""], ["Henshall", "David", ""], ["Balasubramaniam", "Sasitharan", ""]]}, {"id": "2103.16489", "submitter": "Cagatay Basdogan", "authors": "Idil Ozdamar, M.Reza Alipour, Benoit P. Delhaye, Philippe Lef`evre,\n  Cagatay Basdogan", "title": "Step-Change in Friction under Electrovibration", "comments": null, "journal-ref": "IEEE Transactions on Haptics, 2020, Vol. 13, No. 1, pp. 137-143", "doi": "10.1109/TOH.2020.2966992", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rendering tactile effects on a touch screen via electrovibration has many\npotential applications. However, our knowledge on tactile perception of change\nin friction and the underlying contact mechanics are both very limited. In this\nstudy, we investigate the tactile perception and the contact mechanics for a\nstep change in friction under electrovibration during a relative sliding\nbetween finger and the surface of a capacitive touchscreen. First, we conduct\nmagnitude estimation experiments to investigate the role of normal force and\nsliding velocity on the perceived tactile intensity for a step increase and\ndecrease in friction, called as rising friction (RF) and falling friction (FF).\nTo investigate the contact mechanics involved in RF and FF, we then measure the\nfrictional force, the apparent contact area, and the strains acting on the\nfingerpad during sliding at a constant velocity under three different normal\nloads using a custom-made experimental set-up. The results show that the\nparticipants perceived RF stronger than FF, and both the normal force and\nsliding velocity significantly influenced their perception. These results are\nsupported by our mechanical measurements; the relative change in friction, the\napparent contact area, and the strain in the sliding direction were all higher\nfor RF than those for FF, especially for low normal forces. Taken together, our\nresults suggest that different contact mechanics take place during RF and FF\ndue to the viscoelastic behavior of fingerpad skin, and those differences\ninfluence our tactile perception of a step change in friction.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 16:45:27 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ozdamar", "Idil", ""], ["Alipour", "M. Reza", ""], ["Delhaye", "Benoit P.", ""], ["Lef`evre", "Philippe", ""], ["Basdogan", "Cagatay", ""]]}, {"id": "2103.16510", "submitter": "Cagatay Basdogan", "authors": "Senem Ezgi Emgin, Amirreza Aghakhani, T. Metin Sezgin, and Cagatay\n  Basdogan", "title": "HapTable: An Interactive Tabletop Providing Online Haptic Feedback for\n  Touch Gestures", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2019,\n  Vol. 25, No. 9, pp. 2749-2762", "doi": "10.1109/TVCG.2018.2855154", "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HapTable; a multimodal interactive tabletop that allows users to\ninteract with digital images and objects through natural touch gestures, and\nreceive visual and haptic feedback accordingly. In our system, hand pose is\nregistered by an infrared camera and hand gestures are classified using a\nSupport Vector Machine (SVM) classifier. To display a rich set of haptic\neffects for both static and dynamic gestures, we integrated electromechanical\nand electrostatic actuation techniques effectively on tabletop surface of\nHapTable, which is a surface capacitive touch screen. We attached four piezo\npatches to the edges of tabletop to display vibrotactile feedback for static\ngestures. For this purpose, the vibration response of the tabletop, in the form\nof frequency response functions (FRFs), was obtained by a laser Doppler\nvibrometer for 84 grid points on its surface. Using these FRFs, it is possible\nto display localized vibrotactile feedback on the surface for static gestures.\nFor dynamic gestures, we utilize the electrostatic actuation technique to\nmodulate the frictional forces between finger skin and tabletop surface by\napplying voltage to its conductive layer. Here, we present two examples of such\napplications, one for static and one for dynamic gestures, along with detailed\nuser studies. In the first one, user detects the direction of a virtual flow,\nsuch as that of wind or water, by putting their hand on the tabletop surface\nand feeling a vibrotactile stimulus traveling underneath it. In the second\nexample, user rotates a virtual knob on the tabletop surface to select an item\nfrom a menu while feeling the knob's detents and resistance to rotation in the\nform of frictional haptic feedback.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:12:10 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Emgin", "Senem Ezgi", ""], ["Aghakhani", "Amirreza", ""], ["Sezgin", "T. Metin", ""], ["Basdogan", "Cagatay", ""]]}, {"id": "2103.16518", "submitter": "Cagatay Basdogan", "authors": "Bushra Sadia, Senem Ezgi Emgin, T. Metin Sezgin, Cagatay Basdogan", "title": "Data-Driven Vibrotactile Rendering of Digital Buttons on Touchscreens", "comments": null, "journal-ref": "International Journal of Human-Computer Studies, 2020, Vol. 135,\n  102363", "doi": "10.1016/j.ijhcs.2019.09.005", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although mobile phones incorporate low-cost vibration motors to enhance\ntouch-based interactions, it is not possible to generate complex tactile\neffects on their touchscreens. It is also difficult to relate the limited\nvibrotactile feedback generated by these motors to different types of physical\nbuttons. In this study, we focus on creating vibrotactile feedback on a\ntouchscreen that simulates the feeling of physical buttons using piezo\nactuators attached to it. We first recorded and analyzed the force,\nacceleration, and voltage data from twelve participants interacting with three\ndifferent physical buttons: latch, toggle, and push buttons. Then, a\nbutton-specific vibrotactile stimulus was generated for each button based on\nthe recorded data. Finally, we conducted a threealternative forced choice\n(3AFC) experiment with twenty participants to explore whether the resultant\nstimulus is distinct and realistic. In our experiment, participants were able\nto match the three digital buttons with their physical counterparts with a\nsuccess rate of 83%. In addition, we harvested seven adjective pairs from the\nparticipants expressing their perceptual feeling of pressing the physical\nbuttons. All twenty participants rated the degree of their subjective feelings\nassociated with each adjective for all the physical and digital buttons\ninvestigated in this study. Our statistical analysis showed that there exist at\nleast three adjective pairs for which participants have rated two out of three\ndigital buttons similar to their physical counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:21:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Sadia", "Bushra", ""], ["Emgin", "Senem Ezgi", ""], ["Sezgin", "T. Metin", ""], ["Basdogan", "Cagatay", ""]]}, {"id": "2103.16637", "submitter": "Roman Grigorii", "authors": "Roman V. Grigorii, Yifei Li, Michael A. Peshkin, J. Edward Colgate", "title": "Comparison of wide-band vibrotactile and friction modulation surface\n  gratings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study seeks to understand conditions under which virtual gratings\nproduced via vibrotaction and friction modulation are perceived as similar and\nto find physical origins in the results. To accomplish this, we developed two\nsingle-axis devices, one based on electroadhesion and one based on out-of-plane\nvibration. The two devices had identical touch surfaces, and the vibrotactile\ndevice used a novel closed-loop controller to achieve precise control of\nout-of-plane plate displacement under varying load conditions across a wide\nrange of frequencies. A first study measured the perceptual intensity\nequivalence curve of gratings generated under electroadhesion and vibrotaction\nacross the 20-400Hz frequency range. A second study assessed the perceptual\nsimilarity between two forms of skin excitation given the same driving\nfrequency and same perceived intensity. Our results indicate that it is largely\nthe out-of-plane velocity that predicts vibrotactile intensity relative to\nshear forces generated by friction modulation. A high degree of perceptual\nsimilarity between gratings generated through friction modulation and through\nvibrotaction is apparent and tends to scale with actuation frequency suggesting\nperceptual indifference to the manner of fingerpad actuation in the upper\nfrequency range.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:21:34 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Grigorii", "Roman V.", ""], ["Li", "Yifei", ""], ["Peshkin", "Michael A.", ""], ["Colgate", "J. Edward", ""]]}, {"id": "2103.16674", "submitter": "Pu Wang", "authors": "Pu Wang, Hugo Van hamme", "title": "Pre-training for low resource speech-to-intent applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.HC cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a speech-to-intent (S2I) agent which maps the users' spoken\ncommands to the agents' desired task actions can be challenging due to the\ndiverse grammatical and lexical preference of different users. As a remedy, we\ndiscuss a user-taught S2I system in this paper. The user-taught system learns\nfrom scratch from the users' spoken input with action demonstration, which\nensure it is fully matched to the users' way of formulating intents and their\narticulation habits. The main issue is the scarce training data due to the user\neffort involved. Existing state-of-art approaches in this setting are based on\nnon-negative matrix factorization (NMF) and capsule networks. In this paper we\ncombine the encoder of an end-to-end ASR system with the prior NMF/capsule\nnetwork-based user-taught decoder, and investigate whether pre-training\nmethodology can reduce training data requirements for the NMF and capsule\nnetwork. Experimental results show the pre-trained ASR-NMF framework\nsignificantly outperforms other models, and also, we discuss limitations of\npre-training with different types of command-and-control(C&C) applications.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 20:44:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Pu", ""], ["Van hamme", "Hugo", ""]]}, {"id": "2103.16705", "submitter": "Ivan Sysoev", "authors": "Ivan Sysoev, James H. Gray, Susan Fine, Deb Roy", "title": "Designing Building Blocks for Open-Ended Early Literacy Software", "comments": "This is a published manuscript for the paper published in the\n  International Journal of Child-Computer Interaction. Sharing on ArXiv is in\n  accordance with Elsevier sharing policy", "journal-ref": "International Journal of Child-Computer Interaction. Article\n  100273 (2021)", "doi": "10.1016/j.ijcci.2021.100273", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  English has a convoluted relationship between its pronunciation and spelling,\nwhich obscures its phonological structure for early literacy learners. This\nconvoluted relationship has implications for early literacy software,\nparticularly for open-ended, child-driven designs. A tempting way to bypass\nthis issue is to use manipulables (blocks) that are directly tied to phonemes.\nHowever, creating phoneme-based blocks leads to two design challenges: (a) how\nto represent phonemes visually in a child-accessible way and (b) how to account\nfor context-dependent spelling. In the present work, we approached these\nchallenges by developing a set of animated, onomatopoeia-based mnemonic\ncharacters, one per phoneme, that can take the shape of different graphemes.We\napplied the characters to a construction-based literacy app to simplify\nindependent word-building for literacy beginners. We tested the app during a\n13-week-long period with 4- to 5-year-olds in kindergarten classrooms. Children\nshowed visible interest in the characters and properly grasped the principles\nof their functioning. However, the blocks were not sufficient to scaffold\nindependent word building, leading children to rely on other scaffolding\nmechanisms. To test the characters' efficiency as mnemonics, we evaluated their\neffect on the speed and accuracy of finding phonemes on a keyboard. The results\nsuggest that there were both children who benefitted from the characters in\nthis task and those who performed better without them. The factors that\ndifferentiated these two categories are currently unclear. To help further\nresearch on phonetic mnemonics in literacy learning software, we are making the\ncharacters available to the research community.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 22:16:02 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Sysoev", "Ivan", ""], ["Gray", "James H.", ""], ["Fine", "Susan", ""], ["Roy", "Deb", ""]]}, {"id": "2103.17023", "submitter": "Georgios Mylonas", "authors": "Dimitrios Amaxilatis, Evangelos Lagoudianakis, Georgios Mylonas,\n  Evangelos Theodoridis", "title": "Managing smartphone crowdsensing campaigns through the Organicity smart\n  city platform", "comments": "Preprint submitted to UbiComp/ISWC '16 Adjunct, September 12-16,\n  2016, Heidelberg, Germany", "journal-ref": null, "doi": "10.1145/2968219.2968588", "report-no": null, "categories": "cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We briefly present the design and architecture of a system that aims to\nsimplify the process of organizing, executing and administering crowdsensing\ncampaigns in a smart city context over smartphones volunteered by citizens. We\nbuilt our system on top of an Android app substrate on the end-user level,\nwhich enables us to utilize smartphone resources. Our system allows researchers\nand other developers to manage and distribute their \"mini\" smart city\napplications, gather data and publish their results through the Organicity\nsmart city platform. We believe this is the first time such a tool is paired\nwith a large scale IoT infrastructure, to enable truly city-scale IoT and smart\ncity experimentation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 12:13:59 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Amaxilatis", "Dimitrios", ""], ["Lagoudianakis", "Evangelos", ""], ["Mylonas", "Georgios", ""], ["Theodoridis", "Evangelos", ""]]}, {"id": "2103.17095", "submitter": "Romy M\\\"uller", "authors": "Romy M\\\"uller, Franziska Kessler, David W. Humphrey, and Julian Rahm", "title": "Data in context: How digital transformation can support human reasoning\n  in cyber-physical production systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In traditional production plants, current technologies do not provide\nsufficient context to support information integration and interpretation.\nDigital transformation technologies have the potential to support\ncontextualization, but it is unclear how this can be achieved. The present\narticle reviews psychological literature in four areas relevant to\ncontextualization: information sampling, integration, categorization, and\ncausal reasoning. Characteristic biases and limitations of human information\nprocessing are discussed. Based on this literature, we derive functional\nrequirements for digital transformation technologies, focusing on the cognitive\nactivities they should support. We then present a selection of technologies\nthat have the potential to foster contextualization. These technologies enable\nthe modelling of system relations, the integration of data from different\nsources, and the connection of the present situation with historical data. We\nillustrate how these technologies can support contextual reasoning and\nhighlight challenges that should be addressed when designing human-technology\ncooperation in cyber-physical production systems.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:04:47 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["M\u00fcller", "Romy", ""], ["Kessler", "Franziska", ""], ["Humphrey", "David W.", ""], ["Rahm", "Julian", ""]]}, {"id": "2103.17100", "submitter": "Jethro Shell Dr", "authors": "Jethro Shell", "title": "What Do We See: An Investigation Into the Representation of Disability\n  in Video Games", "comments": "Journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There has been a large body of research focused on the representation of\ngender in video games. Disproportionately, there has been very little research\nin respect to the representation of disability. This research was aimed at\nexamining the representation of disabled characters through a method of content\nanalysis of trailers combined with a survey of video gamers. The overall\nresults showed that disabled characters were under-represented in videogames\ntrailers, and respondents to the survey viewed disabled characters as the least\nrepresented group. Both methods of research concluded that the representation\nof disabled characters was low. Additionally, the characters represented were\npredominantly secondary, non-playable characters not primary. However, the\nresearch found that the defined character type was a mixture of protagonists\nand antagonists, bucking the standard view of disabled characters in video\ngames.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 12:34:02 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Shell", "Jethro", ""]]}, {"id": "2103.17261", "submitter": "Aayush Bansal", "authors": "Kevin Wang and Deva Ramanan and Aayush Bansal", "title": "Video Exploration via Video-Specific Autoencoders", "comments": "Project Page: https://www.cs.cmu.edu/~aayushb/Video-ViSA/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present simple video-specific autoencoders that enables human-controllable\nvideo exploration. This includes a wide variety of analytic tasks such as (but\nnot limited to) spatial and temporal super-resolution, spatial and temporal\nediting, object removal, video textures, average video exploration, and\ncorrespondence estimation within and across videos. Prior work has\nindependently looked at each of these problems and proposed different\nformulations. In this work, we observe that a simple autoencoder trained (from\nscratch) on multiple frames of a specific video enables one to perform a large\nvariety of video processing and editing tasks. Our tasks are enabled by two key\nobservations: (1) latent codes learned by the autoencoder capture spatial and\ntemporal properties of that video and (2) autoencoders can project\nout-of-sample inputs onto the video-specific manifold. For e.g. (1)\ninterpolating latent codes enables temporal super-resolution and\nuser-controllable video textures; (2) manifold reprojection enables spatial\nsuper-resolution, object removal, and denoising without training for any of the\ntasks. Importantly, a two-dimensional visualization of latent codes via\nprincipal component analysis acts as a tool for users to both visualize and\nintuitively control video edits. Finally, we quantitatively contrast our\napproach with the prior art and found that without any supervision and\ntask-specific knowledge, our approach can perform comparably to supervised\napproaches specifically trained for a task.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:56:13 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Kevin", ""], ["Ramanan", "Deva", ""], ["Bansal", "Aayush", ""]]}]