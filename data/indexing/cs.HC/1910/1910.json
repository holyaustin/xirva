[{"id": "1910.00087", "submitter": "Longsheng Jiang", "authors": "Longsheng Jiang, Yue Wang", "title": "Respect Your Emotion: Human-Multi-Robot Teaming based on Regret Decision\n  Model", "comments": "8 pages, 4 figures, conference", "journal-ref": "IEEE 15th International Conference on Automation Science and\n  Engineering (CASE), Vancouver, BC, Canada, August 22-26, 2019", "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Often, when modeling human decision-making behaviors in the context of\nhuman-robot teaming, the emotion aspect of human is ignored. Nevertheless, the\ninfluence of emotion, in some cases, is not only undeniable but beneficial.\nThis work studies the human-like characteristics brought by regret emotion in\none-human-multi-robot teaming for the application of domain search. In such\napplication, the task management load is outsourced to the robots to reduce the\nhuman's workload, freeing the human to do more important work. The regret\ndecision model is first used by each robot for deciding whether to request\nhuman service, then is extended for optimally queuing the requests from\nmultiple robots. For the movement of the robots in the domain search, we\ndesigned a path planning algorithm based on dynamic programming for each robot.\nThe simulation shows that the human-like characteristics, namely, risk-seeking\nand risk-aversion, indeed bring some appealing effects for balancing the\nworkload and performance in the human-multi-robot team.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 18:21:42 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Jiang", "Longsheng", ""], ["Wang", "Yue", ""]]}, {"id": "1910.00106", "submitter": "Joe Rexwinkle", "authors": "Joe T. Rexwinkle, Gregory Lieberman, Matthew Jaswa, Brent J. Lance", "title": "Development of a Game with a Purpose for Acquisition of Brain-Computer\n  Interface Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interfaces (BCIs) have the potential to significantly change\nthe ways in which humans interact with technology, the environment, and even\neach other. Unfortunately, BCI technologies are seldom robust enough for use in\nreal-world applications, in part due to the large amount of data that must be\ncollected, processed, and classified in order to develop models of task-related\nneural activity that account for two of the most important and least-understood\ndrivers of BCI illiteracy: individual differences in neural signals and\nintra-individual differences across interdependent, time-varying neural states.\nThis paper describes the feasibility of using a game with a purpose (GWAP) as a\nviable instrument for collecting data from BCI-relevant research tasks. By\nleveraging game-related reward processes to maintain participant interest and\nengagement, this approach will enable large amounts of BCI data to be acquired,\nboth across many individuals and longitudinally from specific individuals as\nneural states vary naturally over time. Pilot and technical testing results are\npresented here to demonstrate that the BCI-relevant tasks embedded within the\nresearch game elicit neural signals similar to those that would be expected\nfrom more traditional BCI tasks. These preliminary data provide support and\nvalidation of the use of GWAPs as promising tools to enable long-term\ncollection of BCI-relevant data in an engaging environment.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 21:03:18 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Rexwinkle", "Joe T.", ""], ["Lieberman", "Gregory", ""], ["Jaswa", "Matthew", ""], ["Lance", "Brent J.", ""]]}, {"id": "1910.00340", "submitter": "Bernd Kiefer", "authors": "Bernd Kiefer and Anna Welker and Christophe Biwer", "title": "VOnDA: A Framework for Ontology-Based Dialogue Management", "comments": "Presented at the Tenth International Workshop on Spoken Dialogue\n  Systems Technology (IWSDS), April 24-26, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present VOnDA, a framework to implement the dialogue management\nfunctionality in dialogue systems. Although domain-independent, VOnDA is\ntailored towards dialogue systems with a focus on social communication, which\nimplies the need of long-term memory and high user adaptivity. For these\nsystems, which are used in health environments or elderly care, margin of error\nis very low and control over the dialogue process is of topmost importance. The\nsame holds for commercial applications, where customer trust is at risk.\nVOnDA's specification and memory layer relies upon (extended) RDF/OWL, which\nprovides a universal and uniform representation, and facilitates\ninteroperability with external data sources, e.g., from physical sensors.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 12:33:16 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Kiefer", "Bernd", ""], ["Welker", "Anna", ""], ["Biwer", "Christophe", ""]]}, {"id": "1910.00444", "submitter": "Siddharth Siddharth", "authors": "Siddharth Siddharth and Mohan M. Trivedi", "title": "On Assessing Driver Awareness of Situational Criticalities: Multi-modal\n  Bio-sensing and Vision-based Analysis, Evaluations, and Insights", "comments": "Journal article published in MDPI Brain Sciences. arXiv admin note:\n  text overlap with arXiv:1905.00503", "journal-ref": null, "doi": "10.3390/brainsci10010046", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automobiles for our roadways are increasingly using advanced driver\nassistance systems. The adoption of such new technologies requires us to\ndevelop novel perception systems not only for accurately understanding the\nsituational context of these vehicles, but also to infer the driver's awareness\nin differentiating between safe and critical situations. This manuscript\nfocuses on the specific problem of inferring driver awareness in the context of\nattention analysis and hazardous incident activity. Even after the development\nof wearable and compact multi-modal bio-sensing systems in recent years, their\napplication in driver awareness context has been scarcely explored.\nThe~capability of simultaneously recording different kinds of bio-sensing data\nin addition to traditionally employed computer vision systems provides exciting\nopportunities to explore the limitations of these sensor modalities. In this\nwork, we explore the applications of three different bio-sensing modalities\nnamely electroencephalogram (EEG), photoplethysmogram (PPG) and galvanic skin\nresponse (GSR) along with a camera-based vision system in driver awareness\ncontext. We assess the information from these sensors independently and\ntogether using both signal processing- and deep learning-based tools. We show\nthat our methods outperform previously reported studies to classify driver\nattention and detecting hazardous/non-hazardous situations for short time\nscales of two seconds. We use EEG and vision data for high resolution temporal\nclassification (two seconds) while additionally also employing PPG and GSR over\nlonger time periods. We evaluate our methods by collecting user data on twelve\nsubjects for two real-world driving datasets among which one is publicly\navailable (KITTI dataset) while the other was collected by us (LISA dataset)\nwith the vehicle ....\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 16:30:26 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 00:43:12 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 13:16:47 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Siddharth", "Siddharth", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1910.01191", "submitter": "Yu Liang", "authors": "Dakila Ledesma, Yu Liang, and Dalei Wu", "title": "Adaptive Generation of Phantom Limbs Using Visible Hierarchical\n  Autoencoders", "comments": "arXiv admin note: text overlap with arXiv:1612.06336 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposed a hierarchical visible autoencoder in the adaptive\nphantom limbs generation according to the kinetic behavior of functional\nbody-parts, which are measured by heterogeneous kinetic sensors. The proposed\nvisible hierarchical autoencoder consists of interpretable and multi-correlated\nautoencoder pipelines, which is directly derived from the hierarchical network\ndescribed in forest data-structure. According to specified kinetic script\n(e.g., dancing, running, etc.) and users' physical conditions, hierarchical\nnetwork is extracted from human musculoskeletal network, which is fabricated by\nmultiple body components (e.g., muscle, bone, and joints, etc.) that are\nbio-mechanically, functionally, or nervously correlated with each other and\nexhibit mostly non-divergent kinetic behaviors. Multi-layer perceptron (MLP)\nregressor models, as well as several variations of autoencoder models, are\ninvestigated for the sequential generation of missing or dysfunctional limbs.\nThe resulting kinematic behavior of phantom limbs will be constructed using\nvirtual reality and augmented reality (VR/AR), actuators, and potentially\ncontroller for a prosthesis (an artificial device that replaces a missing body\npart). The addressed work aims to develop practical innovative exercise methods\nthat (1) engage individuals at all ages, including those with a chronic health\ncondition(s) and/or disability, in regular physical activities, (2) accelerate\nthe rehabilitation of patients, and (3) release users' phantom limb pain. The\nphysiological and psychological impact of the addressed work will critically be\nassessed in future work.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 19:54:19 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Ledesma", "Dakila", ""], ["Liang", "Yu", ""], ["Wu", "Dalei", ""]]}, {"id": "1910.01216", "submitter": "Matt Higger", "authors": "Matt Higger, Fernando Quivira, Deniz Erdogmus", "title": "User-Adaptive Text Entry for Augmentative and Alternative Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The viability of an Augmentative and Alternative Communication device often\ndepends on its ability to adapt to an individual user's unique abilities.\nThough human input can be noisy, there is often structure to our errors. For\nexample, keyboard keys adjacent to a target may be more likely to be pressed in\nerror. Furthermore, there can be structure in the input message itself (e.g.\n`u' is likely to follow `q'). In a previous work, `Recursive Bayesian Coding\nfor BCIs' (IEEE Transactions on Neural Systems and Rehabilitation Engineering,\n2016), a query strategy considers these structures to offer an error-adaptive\nsingle-character text entry scheme. However, constraining ourselves to\nsingle-character entry limits performance. A single user input may be able to\nresolve more uncertainty than the next character has. In this work, we extend\nthe previous framework to incorporate multi-character querying similar to word\ncompletion. During simulated spelling, our method requires $20\\%$ fewer queries\ncompared to single-character querying with no accuracy penalty. Most\nsignificantly, we show that this multi-character querying scheme converges to\nthe information theoretic capacity of the discrete, memoryless user input\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 20:50:24 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Higger", "Matt", ""], ["Quivira", "Fernando", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "1910.01459", "submitter": "Changkun Ou", "authors": "Changkun Ou, Yifei Zhan, Yaxi Chen", "title": "Identifying Malicious Players in GWAP-based Disaster Monitoring\n  Crowdsourcing System", "comments": null, "journal-ref": "In IEEE ICAIBD' 19: Proceedings of the 2nd International\n  Conference on Artificial Intelligence and Big Data. Chengdu, Sichuan, China,\n  May 25-28, 2019", "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Disaster monitoring is challenging due to the lake of infrastructures in\nmonitoring areas. Based on the theory of Game-With-A-Purpose (GWAP), this paper\ncontributes to a novel large-scale crowdsourcing disaster monitoring system.\nThe system analyzes tagged satellite pictures from anonymous players, and then\nreports aggregated and evaluated monitoring results to its stakeholders. An\nalgorithm based on directed graph centralities is presented to address the core\nissues of malicious user detection and disaster level calculation. Our method\ncan be easily applied in other human computation systems. In the end, some\nissues with possible solutions are discussed for our future work.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 14:49:11 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Ou", "Changkun", ""], ["Zhan", "Yifei", ""], ["Chen", "Yaxi", ""]]}, {"id": "1910.01546", "submitter": "Chih Han Chung", "authors": "Yi-Ting Chen, Chi-Hsuan Hsu, Chih-Han Chung, Yu-Shuen Wang, Sabarish\n  V. Babu", "title": "iVRNote: Design, Creation and Evaluation of an Interactive Note-Taking\n  Interface for Study and Reflection in VR Learning Environments", "comments": "9 pages, IEEE VR 2019,\n  https://people.cs.nctu.edu.tw/~yushuen/data/iVRNote19.mp4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we design, implement and evaluate the pedagogical\nbenefits of a novel interactive note taking interface (iVRNote) in VR for the\npurpose of learning and reflection lectures. In future VR learning\nenvironments, students would have challenges in taking notes when they wear a\nhead mounted display (HMD). To solve this problem, we installed a digital\ntablet on the desk and provided several tools in VR to facilitate the learning\nexperience. Specifically, we track the stylus position and orientation in the\nphysical world and then render a virtual stylus in VR. In other words, when\nstudents see a virtual stylus somewhere on the desk, they can reach out with\ntheir hand for the physical stylus. The information provided will also enable\nthem to know where they will draw or write before the stylus touches the\ntablet. Since the presented iVRNote featuring our note taking system is a\ndigital environment, we also enable students save efforts in taking extensive\nnotes by providing several functions, such as post-editing and picture taking,\nso that they can pay more attention to lectures in VR. We also record the time\nof each stroke on the note to help students review a lecture. They can select a\npart of their note to revisit the corresponding segment in a virtual online\nlecture. Figures and the accompanying video demonstrate the feasibility of the\npresented iVRNote system. To evaluate the system, we conducted a user study\nwith 20 participants to assess the preference and pedagogical benefits of the\niVRNote interface.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 15:11:32 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Chen", "Yi-Ting", ""], ["Hsu", "Chi-Hsuan", ""], ["Chung", "Chih-Han", ""], ["Wang", "Yu-Shuen", ""], ["Babu", "Sabarish V.", ""]]}, {"id": "1910.01586", "submitter": "David Murphy", "authors": "David Murphy and Conor Higgins", "title": "Secondary Inputs for Measuring User Engagement in Immersive VR Education\n  Environments", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an experiment to assess the feasibility of using\nsecondary input data as a method of determining user engagement in immersive\nvirtual reality (VR). The work investigates whether secondary data (biosignals)\nacquired from users are useful as a method of detecting levels of\nconcentration, stress, relaxation etc. in immersive environments, and if they\ncould be used to create an affective feedback loop in immersive VR\nenvironments, including educational contexts. A VR Experience was developed in\nthe Unity game engine, with three different levels, each designed to expose the\nuser in one of three different states (relaxation, concentration, stress).\nWhile in the VR Experience users physiological responses were measured using\nECG and EEG sensors. After the experience users completed questionnaires to\nestablish their perceived state during the levels, and to established the\nusability of the system. Next a comparison between the reported levels of\nemotion and the measured signals is presented, which show a strong\ncorrespondence between the two measures indicating that biosignals are a useful\nindicator of emotional state while in VR. Finally we make some recommendations\non the practicalities of using biosensors, and design considerations for their\nincorporation in to a VR system, with particular focus on their integration in\nto task-based training and educational virtual environments.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 16:39:28 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Murphy", "David", ""], ["Higgins", "Conor", ""]]}, {"id": "1910.01770", "submitter": "Kizito Nkurikiyeyezu", "authors": "Kizito Nkurikiyeyezu, Anna Yokokubo, Guillaume Lopez", "title": "The Effect of Person-Specific Biometrics in Improving Generic Stress\n  Predictive Models", "comments": "Journal of Sensors and Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because stress is subjective and is expressed differently from one person to\nanother, generic stress prediction models (i.e., models that predict the stress\nof any person) perform crudely. Only person-specific ones (i.e., models that\npredict the stress of a preordained person) yield reliable predictions, but\nthey are not adaptable and costly to deploy in real-world environments. For\nillustration, in an office environment, a stress monitoring system that uses\nperson-specific models would require collecting new data and training a new\nmodel for every employee. Moreover, once deployed, the models would deteriorate\nand need expensive periodic upgrades because stress is dynamic and depends on\nunforeseeable factors. We propose a simple, yet practical and cost effective\ncalibration technique that derives an accurate and personalized stress\nprediction model from physiological samples collected from a large population.\nWe validate our approach on two stress datasets. The results show that our\ntechnique performs much better than a generic model. For instance, a generic\nmodel achieved only a 42.5% accuracy. However, with only 100 calibration\nsamples, we raised its accuracy to 95.2% We also propose a blueprint for a\nstress monitoring system based on our strategy, and we debate its merits and\nlimitation. Finally, we made public our source code and the relevant datasets\nto allow other researchers to replicate our findings.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 01:06:20 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 08:13:55 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Nkurikiyeyezu", "Kizito", ""], ["Yokokubo", "Anna", ""], ["Lopez", "Guillaume", ""]]}, {"id": "1910.01918", "submitter": "Mohsen Jafarzadeh", "authors": "Mohsen Jafarzadeh, Yonas Tadesse", "title": "Convolutional Neural Networks for Speech Controlled Prosthetic Hands", "comments": "2019 First International Conference on Transdisciplinary AI\n  (TransAI), Laguna Hills, California, USA, 2019, pp. 35-42", "journal-ref": "2019 First International Conference on Transdisciplinary AI\n  (TransAI)", "doi": "10.1109/TransAI46475.2019.00014", "report-no": null, "categories": "eess.SY cs.HC cs.LG cs.RO cs.SD cs.SY eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech recognition is one of the key topics in artificial intelligence, as it\nis one of the most common forms of communication in humans. Researchers have\ndeveloped many speech-controlled prosthetic hands in the past decades,\nutilizing conventional speech recognition systems that use a combination of\nneural network and hidden Markov model. Recent advancements in general-purpose\ngraphics processing units (GPGPUs) enable intelligent devices to run deep\nneural networks in real-time. Thus, state-of-the-art speech recognition systems\nhave rapidly shifted from the paradigm of composite subsystems optimization to\nthe paradigm of end-to-end optimization. However, a low-power embedded GPGPU\ncannot run these speech recognition systems in real-time. In this paper, we\nshow the development of deep convolutional neural networks (CNN) for speech\ncontrol of prosthetic hands that run in real-time on a NVIDIA Jetson TX2\ndeveloper kit. First, the device captures and converts speech into 2D features\n(like spectrogram). The CNN receives the 2D features and classifies the hand\ngestures. Finally, the hand gesture classes are sent to the prosthetic hand\nmotion control system. The whole system is written in Python with Keras, a deep\nlearning library that has a TensorFlow backend. Our experiments on the CNN\ndemonstrate the 91% accuracy and 2ms running time of hand gestures (text\noutput) from speech commands, which can be used to control the prosthetic hands\nin real-time.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 04:47:40 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Jafarzadeh", "Mohsen", ""], ["Tadesse", "Yonas", ""]]}, {"id": "1910.01970", "submitter": "Jana Korunovska", "authors": "Jana Korunovska and Sarah Spiekermann", "title": "The Effects of Information and Communication Technology Use on Human\n  Energy and Fatigue: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information and communication technologies (ICTs) are generally assumed to\nsave time and energy, yet user fatigue due to ICT use is assumed to be on the\nrise. The question about the effects of ICT use on human energy has sparked\nincreased research interest in recent years. however, the course is complicated\nby the fact that the conceptualization of human energy is extremely diverse.\nThe aim of this paper is therefore twofold. First, we provide a conceptual\nframework and classification for subjective energy concepts and reflect on the\ntheoretical embedding of technology within the theories on subjective energy.\nSecond, we review the leading empirical literature on the relationship between\nICT use and eight different subjective energy concepts prominent in different\ndisciplines. We also include the new phenomena of social networking sites (SNS)\nexhaustion and SNS fatigue. With this, we aim to consolidate the existing\nresearch, illuminate the gaps and provide a conceptual baseline for future\nresearch on the relationship between ICT use and subjective energy of ICT\nusers. We show that ICT use has predominantly negative effect on users' energy,\nespecially in organizational contexts, and show the main patterns and\nmechanisms through which technology drains as well as energizes users.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 14:38:44 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 07:58:48 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Korunovska", "Jana", ""], ["Spiekermann", "Sarah", ""]]}, {"id": "1910.01973", "submitter": "Martin Johannes Kraemer", "authors": "Martin J Kraemer, William Seymour, Reuben Binns, Max Van Kleek, Ivan\n  Flechais", "title": "Informing The Future of Data Protection in Smart Homes", "comments": "Proceedings of the CHI 2019 Workshop on New Directions for the IoT:\n  Automate, Share, Build, and Care, (arXiv:1906.06089)", "journal-ref": null, "doi": null, "report-no": "IOTD/2019/12", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent changes to data protection regulation, particularly in Europe, are\nchanging the design landscape for smart devices, requiring new design\ntechniques to ensure that devices are able to adequately protect users' data. A\nparticularly interesting space in which to explore and address these challenges\nis the smart home, which presents a multitude of difficult social and technical\nproblems in an intimate and highly private context. This position paper\noutlines the motivation and research approach of a new project aiming to inform\nthe future of data protection by design and by default in smart homes through a\ncombination of ethnography and speculative design.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 09:57:41 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Kraemer", "Martin J", ""], ["Seymour", "William", ""], ["Binns", "Reuben", ""], ["Van Kleek", "Max", ""], ["Flechais", "Ivan", ""]]}, {"id": "1910.01974", "submitter": "Raphael Kim Mr", "authors": "Raphael Kim, Stefan Poslad", "title": "The Thing With E.coli: Highlighting Opportunities and Challenges of\n  Integrating Bacteria in IoT and HCI", "comments": "Proceedings of the CHI 2019 Workshop on New Directions for the IoT:\n  Automate, Share, Build, and Care (arXiv:1906.06089)", "journal-ref": null, "doi": null, "report-no": "IOTD/2019/07", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With advances in nano- and biotechnology, bacteria are receiving increasing\nattention in scientific research as a potential substrate for Internet of\nBio-Nano Things (IoBNT), which involve networking and communication through\nnanoscale and biological entities. Harnessing the special features of bacteria,\nincluding an ability to become autonomous - helped by an embedded, natural\npropeller motor - the microbes show promising array of application in\nhealthcare and environmental health. In this paper, we briefly outline\nsignificant features of bacteria that allow analogies between them and\ntraditional computerized IoT device to be made. We argue that such comparisons\nare critical in terms of helping researchers to explore human-bacteria\ninteraction in the context of IoT and HCI. Furthermore, we highlight the\ncurrent lack of tangible infrastructure for researchers in IoT and HCI to\naccess and experiment with bacteria. As a potential solution, we propose to\nutilize the DIY biology movement and gamification techniques to leverage user\nengagement and introduction to bacteria.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 18:35:11 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Kim", "Raphael", ""], ["Poslad", "Stefan", ""]]}, {"id": "1910.01975", "submitter": "EunJeong Cheon", "authors": "EunJeong Cheon", "title": "Alternative Vision of Living with IoT", "comments": "Proceedings of the CHI 2019 Workshop on New Directions for the IoT:\n  Automate, Share, Build, and Care, (arXiv:1906.06089)", "journal-ref": null, "doi": null, "report-no": "IOTD/2019/08", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this submission, I discuss my research on values, norms and practices of\nsubcultures formed as an \"alternative\" to the dominant way of life. In\nparticular, I explore how the Internet of Things (IoT) or intelligent agents\nrelates to alternative forms of interaction or be understood and reconstructed\nthrough alternative concepts or frameworks. For the past three years I have\nbeen conducting fieldwork on communities pursuing alternative lifestyles. This\nwork considers how those alternative lifestyles may contribute to an\nunderstanding of objects, spaces in future smart home. Through my fieldwork and\nresearch through design, I hope to offer an alternative vision to living with\nIoT and envision future domesticity in a unique and even groundbreaking way.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 14:32:24 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Cheon", "EunJeong", ""]]}, {"id": "1910.01976", "submitter": "Mirzel Avdic", "authors": "Mirzel Avdic, Jo Vermeulen", "title": "Studying Breakdowns in Interactions with Smart Speakers", "comments": "Proceedings of the CHI 2019 Workshop on New Directions for the IoT:\n  Automate, Share, Build, and Care, (arXiv:1906.06089)", "journal-ref": null, "doi": null, "report-no": "IOTD/2019/04", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of voice-controlled smart speakers with intelligent personal\nassistants (IPAs) like the Amazon Echo and their increasing use as an interface\nfor other Internet of Things (IoT) technologies in the home provides\nopportunities to study smart speakers as an emerging and ubiquitous IoT device.\nPrior research has studied how smart speaker usage has unfolded in homes and\nhow the devices have been integrated into people's daily routines. In this\ncontext, researchers have also observed instances of smart speakers' 'black\nbox' behaviour. In this position paper, we present findings from a study we\nconducted to specifically investigate such challenges people experience around\nintelligibility and control of their smart speakers, for instance, when the\nsmart speaker interfaces with other IoT systems. Reflecting on our findings, we\ndiscuss new possible directions for smart speakers including physical\nintelligibility, situational physical interaction, and providing access to\nalternative interpretations in shared and non-shared contexts.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 12:17:39 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Avdic", "Mirzel", ""], ["Vermeulen", "Jo", ""]]}, {"id": "1910.02015", "submitter": "Janis Stolzenwald", "authors": "Janis Stolzenwald and Walterio W. Mayol-Cuevas", "title": "Reach Out and Help: Assisted Remote Collaboration through a Handheld\n  Robot", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A remote collaboration setup allows an expert to instruct a remotely located\nnovice user to help them with a task. Advanced solutions exist in the field of\nremote guidance and telemanipulation, however, we lack a device that combines\nthese two aspects to grant the expert physical access to the work site. We\nexplore a setup that involves three parties: a local worker, a remote helper\nand a handheld robot carried by the local worker. Using remote maintenance as\nan example task, the remote user assists the local user through diagnosis,\nguidance and physical interaction. We introduce a remote collaboration system\nwith a handheld robot that provides task knowledge and enhanced motion and\naccuracy capabilities. We assess the proposed system in two different\nconfigurations: with and without the robot's assistance features enabled. We\nshow that the handheld robot can mediate the helper's instructions and remote\nobject interactions while the robot's semi-autonomous features improve task\nperformance by 37%, reduce the workload for the remote user and decrease\nrequired verbal communication bandwidth. Our results demonstrate the\neffectiveness of task delegation with the remote user choosing an action and\nthe robot autonomously completing it at the local level. This study is a first\nattempt to evaluate how this new type of collaborative robot works in a remote\nassistance scenario. We believe that the proposed setup and task-delegation\nconcepts help to leverage current constrains in telemanipulation systems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:16:42 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 14:21:02 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Stolzenwald", "Janis", ""], ["Mayol-Cuevas", "Walterio W.", ""]]}, {"id": "1910.02020", "submitter": "Himanshu Goyal", "authors": "Himanshu Goyal, Pramit Saha, Bryan Gick, and Sidney Fels", "title": "EEG-to-F0: Establishing artificial neuro-muscular pathway for\n  kinematics-based fundamental frequency control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental frequency (F0) of human voice is generally controlled by\nchanging the vocal fold parameters (including tension, length and mass), which\nin turn is manipulated by the muscle exciters, activated by the neural\nsynergies. In order to begin investigating the neuromuscular F0 control\npathway, we simulate a simple biomechanical arm prototype (instead of an\nartificial vocal tract) that tends to control F0 of an artificial sound\nsynthesiser based on the elbow movements. The intended arm movements are\ndecoded from the EEG signal inputs (collected simultaneously with the kinematic\nhand data of the participant) through a combined machine learning and\nbiomechanical modeling strategy. The machine learning model is employed to\nidentify the muscle activation of a single-muscle arm model in ArtiSynth (from\ninput brain signal), in order to match the actual kinematic (elbow joint angle)\ndata . The biomechanical model utilises this estimated muscle excitation to\nproduce corresponding changes in elbow angle, which is then linearly mapped to\nF0 of a vocal sound synthesiser. We use the F0 value mapped from the actual\nkinematic hand data (via same function) as the ground truth and compare the F0\nestimated from brain signal. A detailed qualitative and quantitative\nperformance comparison shows that the proposed neuromuscular pathway can indeed\nbe utilised to accurately control the vocal fundamental frequency, thereby\ndemonstrating the success of our closed loop neuro-biomechanical control\nscheme.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 02:49:12 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Goyal", "Himanshu", ""], ["Saha", "Pramit", ""], ["Gick", "Bryan", ""], ["Fels", "Sidney", ""]]}, {"id": "1910.02201", "submitter": "Jun Miura", "authors": "Motoki Kojima and Jun Miura", "title": "Early Estimation of User's Intention of Tele-Operation Using Object\n  Affordance and Hand Motion in a Dual First-Person Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method of estimating the intention of a user's motion\nin a robot tele-operation scenario. One of the issues in tele-operation is\nlatency, which occurs due to various reasons such as a slow robot motion and a\nnarrow communication channel. An effective way of reducing the latency is to\nestimate the human intention of motions and to move the robot proactively. To\nenable a reliable early intention estimation, we use both hand motion and\nobject affordances in a dual first-person vision (robot and user) with an HMD.\nExperimental results in an object pickup scenario show the effectiveness of the\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 03:21:54 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Kojima", "Motoki", ""], ["Miura", "Jun", ""]]}, {"id": "1910.02368", "submitter": "Sang Won Lee", "authors": "Sang Won Lee", "title": "Computer-mediated Empathy", "comments": null, "journal-ref": "Virginia Tech Workshop on the Future of Human-Computer\n  Interaction, 2019", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While novel social networks and emerging technologies help us transcend the\nspatial and temporal constraints inherent to in-person communication, the\ntrade-off is a loss of natural expressivity. While empathetic interaction is\nalready challenging in in-person communication, computer-mediated communication\nmakes such empathetically rich communication even more difficult. Are\ntechnology and intelligent systems opportunities or threats to more empathic\ninterpersonal communication? Realizing empathy is suggested not only as a way\nto communicate with others but also to design products for users and facilitate\ncreativity. In this position paper, I suggest a framework to breakdown empathy,\nintroduce each element, and show how computing, technologies, and algorithms\ncan support (or hinder) certain elements of the empathy framework.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 04:25:51 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Lee", "Sang Won", ""]]}, {"id": "1910.02377", "submitter": "Sang Won Lee", "authors": "Sang Won Lee", "title": "Liveness in Interactive Systems", "comments": null, "journal-ref": "the CSCW 2018 workshop on Hybrid Events (CSCW) the CSCW 2018\n  workshop on Hybrid Events (CSCW) , 2018", "doi": "10.5281/zenodo.1471026", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Creating an artifact in front of public offers an opportunity to involve\nspectators in the creation process. For example, in a live music concert,\naudience members can clap, stomp and sing with the musicians to be part of the\nmusic piece. Live creation can facilitate collaboration with the spectators.\nThe questions I set out to answer are what does it mean to have liveness in\ninteractive systems to support large-scale hybrid events that involve audience\nparticipation. The notion of liveness is subtle in human-computer interaction.\nIn this paper, I revisit the notion of liveness and provide definitions of both\nlive and liveness from the perspective of designing interactive systems. In\naddition, I discuss why liveness matters in facilitating hybrid events and\nsuggest future research works\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 05:21:04 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Lee", "Sang Won", ""]]}, {"id": "1910.02440", "submitter": "Umut Caliskan", "authors": "Umut Caliskan and Volkan Patoglu", "title": "Efficacy of Haptic Pedal Feel Compensation on Driving with Regenerative\n  Braking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the efficacy of haptic pedal feel compensation on driving safety and\nperformance during regenerative braking. In particular, we evaluate the\neffectiveness of the preservation of the natural brake pedal feel under\ntwo-pedal cooperative braking and one-pedal driving scenarios, through human\nsubject experiments in a simulated vehicle pursuit task. The experimental\nresults indicate that pedal feel compensation can significantly decrease the\nhard braking instances, improving safety for both two-pedal cooperative braking\nand one-pedal driving. Volunteers also strongly prefer compensation, while they\nequally prefer and can effectively utilize both two-pedal and one-pedal driving\nconditions. Furthermore, the beneficial effects of haptic pedal feel\ncompensation is larger for the two-pedal cooperative braking case.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 12:49:44 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Caliskan", "Umut", ""], ["Patoglu", "Volkan", ""]]}, {"id": "1910.02445", "submitter": "Christoph Heindl", "authors": "Christoph Heindl, Markus Ikeda, Gernot St\\\"ubl, Andreas Pichler, Josef\n  Scharinger", "title": "Enhanced Human-Machine Interaction by Combining Proximity Sensing with\n  Global Perception", "comments": "IROS 2019 / 2nd Workshop on Proximity Perception", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The raise of collaborative robotics has led to wide range of sensor\ntechnologies to detect human-machine interactions: at short distances,\nproximity sensors detect nontactile gestures virtually occlusion-free, while at\nmedium distances, active depth sensors are frequently used to infer human\nintentions. We describe an optical system for large workspaces to capture human\npose based on a single panoramic color camera. Despite the two-dimensional\ninput, our system is able to predict metric 3D pose information over larger\nfield of views than would be possible with active depth measurement cameras. We\nmerge posture context with proximity perception to reduce occlusions and\nimprove accuracy at long distances. We demonstrate the capabilities of our\nsystem in two use cases involving multiple humans and robots.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 13:17:57 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 10:48:31 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 05:49:43 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Heindl", "Christoph", ""], ["Ikeda", "Markus", ""], ["St\u00fcbl", "Gernot", ""], ["Pichler", "Andreas", ""], ["Scharinger", "Josef", ""]]}, {"id": "1910.02923", "submitter": "Samuel Budd", "authors": "Samuel Budd, Emma C Robinson, Bernhard Kainz", "title": "A Survey on Active Learning and Human-in-the-Loop Deep Learning for\n  Medical Image Analysis", "comments": "Medical Image Analysis Volume 71 2021\n  https://doi.org/10.1016/j.media.2021.102062", "journal-ref": "Medical Image Analysis, Volume 71, 2021, 102062, ISSN 1361-8415", "doi": "10.1016/j.media.2021.102062", "report-no": null, "categories": "cs.LG cs.CV cs.HC eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fully automatic deep learning has become the state-of-the-art technique for\nmany tasks including image acquisition, analysis and interpretation, and for\nthe extraction of clinically useful information for computer-aided detection,\ndiagnosis, treatment planning, intervention and therapy. However, the unique\nchallenges posed by medical image analysis suggest that retaining a human end\nuser in any deep learning enabled system will be beneficial. In this review we\ninvestigate the role that humans might play in the development and deployment\nof deep learning enabled diagnostic applications and focus on techniques that\nwill retain a significant input from a human end user. Human-in-the-Loop\ncomputing is an area that we see as increasingly important in future research\ndue to the safety-critical nature of working in the medical domain. We evaluate\nfour key areas that we consider vital for deep learning in the clinical\npractice: (1) Active Learning to choose the best data to annotate for optimal\nmodel performance; (2) Interaction with model outputs - using iterative\nfeedback to steer models to optima for a given prediction and offering\nmeaningful ways to interpret and respond to predictions; (3) Practical\nconsiderations - developing full scale applications and the key considerations\nthat need to be made before deployment; (4) Future Prospective and Unanswered\nQuestions - knowledge gaps and related research fields that will benefit\nhuman-in-the-loop computing as they evolve. We offer our opinions on the most\npromising directions of research and how various aspects of each area might be\nunified towards common goals.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 17:24:33 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 12:07:48 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Budd", "Samuel", ""], ["Robinson", "Emma C", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1910.03042", "submitter": "Dian Yu", "authors": "Dian Yu, Michelle Cohn, Yi Mang Yang, Chun-Yen Chen, Weiming Wen,\n  Jiaping Zhang, Mingyang Zhou, Kevin Jesse, Austin Chau, Antara Bhowmick,\n  Shreenath Iyer, Giritheja Sreenivasulu, Sam Davidson, Ashwin Bhandare, Zhou\n  Yu", "title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gunrock is the winner of the 2018 Amazon Alexa Prize, as evaluated by\ncoherence and engagement from both real users and Amazon-selected expert\nconversationalists. We focus on understanding complex sentences and having\nin-depth conversations in open domains. In this paper, we introduce some\ninnovative system designs and related validation analysis. Overall, we found\nthat users produce longer sentences to Gunrock, which are directly related to\nusers' engagement (e.g., ratings, number of turns). Additionally, users'\nbackstory queries about Gunrock are positively correlated to user satisfaction.\nFinally, we found dialog flows that interleave facts and personal opinions and\nstories lead to better user satisfaction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 19:24:36 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Yu", "Dian", ""], ["Cohn", "Michelle", ""], ["Yang", "Yi Mang", ""], ["Chen", "Chun-Yen", ""], ["Wen", "Weiming", ""], ["Zhang", "Jiaping", ""], ["Zhou", "Mingyang", ""], ["Jesse", "Kevin", ""], ["Chau", "Austin", ""], ["Bhowmick", "Antara", ""], ["Iyer", "Shreenath", ""], ["Sreenivasulu", "Giritheja", ""], ["Davidson", "Sam", ""], ["Bhandare", "Ashwin", ""], ["Yu", "Zhou", ""]]}, {"id": "1910.03061", "submitter": "Zhiwei Steven Wu", "authors": "Bowen Yu, Ye Yuan, Loren Terveen, Zhiwei Steven Wu, Jodi Forlizzi,\n  Haiyi Zhu", "title": "Keeping Designers in the Loop: Communicating Inherent Algorithmic\n  Trade-offs Across Multiple Objectives", "comments": "Paper appeared at Proceedings of The 2020 ACM conference on Designing\n  Interactive Systems (DIS'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence algorithms have been used to enhance a wide variety\nof products and services, including assisting human decision making in\nhigh-stakes contexts. However, these algorithms are complex and have\ntrade-offs, notably between prediction accuracy and fairness to population\nsubgroups. This makes it hard for designers to understand algorithms and design\nproducts or services in a way that respects users' goals, values, and needs. We\nproposed a method to help designers and users explore algorithms, visualize\ntheir trade-offs, and select algorithms with trade-offs consistent with their\ngoals and needs. We evaluated our method on the problem of predicting criminal\ndefendants' likelihood to re-offend through (i) a large-scale Amazon Mechanical\nTurk experiment, and (ii) in-depth interviews with domain experts. Our\nevaluations show that our method can help designers and users of these systems\nbetter understand and navigate algorithmic trade-offs. This paper contributes a\nnew way of providing designers the ability to understand and control the\noutcomes of algorithmic systems they are creating.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 20:08:58 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 14:55:05 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 03:48:05 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Yu", "Bowen", ""], ["Yuan", "Ye", ""], ["Terveen", "Loren", ""], ["Wu", "Zhiwei Steven", ""], ["Forlizzi", "Jodi", ""], ["Zhu", "Haiyi", ""]]}, {"id": "1910.03239", "submitter": "Christoph Heindl", "authors": "Christoph Heindl, Markus Ikeda, Gernot St\\\"ubl, Andreas Pichler, Josef\n  Scharinger", "title": "Metric Pose Estimation for Human-Machine Interaction Using Monocular\n  Vision", "comments": "IROS 2019, Factory of the Future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of collaborative robotics in production requires new\nautomation technologies that take human and machine equally into account. In\nthis work, we describe a monocular camera based system to detect human-machine\ninteractions from a bird's-eye perspective. Our system predicts poses of humans\nand robots from a single wide-angle color image. Even though our approach works\non 2D color input, we lift the majority of detections to a metric 3D space. Our\nsystem merges pose information with predefined virtual sensors to coordinate\nhuman-machine interactions. We demonstrate the advantages of our system in\nthree use cases.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 07:00:05 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Heindl", "Christoph", ""], ["Ikeda", "Markus", ""], ["St\u00fcbl", "Gernot", ""], ["Pichler", "Andreas", ""], ["Scharinger", "Josef", ""]]}, {"id": "1910.03380", "submitter": "Mauricio Sousa", "authors": "Maur\\'icio Sousa, Daniel Mendes, Rafael Kuffner dos Anjos, Daniel\n  Sim\\~oes Lopes, and Joaquim Jorge", "title": "Negative Space: Workspace Awareness in 3D Face-to-Face Remote\n  Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face-to-face telepresence promotes the sense of \"being there\" and can improve\ncollaboration by allowing immediate understanding of remote people's nonverbal\ncues. Several approaches successfully explored interactions with 2D content\nusing a see-through whiteboard metaphor. However, with 3D content, there is a\ndecrease in awareness due to ambiguities originated by participants' opposing\npoints-of-view. In this paper, we investigate how people and content should be\npresented for discussing 3D renderings within face-to-face collaborative\nsessions. To this end, we performed a user evaluation to compare four different\nconditions, in which we varied reflections of both workspace and remote people\nrepresentation. Results suggest potentially more benefits to remote\ncollaboration from workspace consistency rather than people's representation\nfidelity. We contribute a novel design space, the Negative Space, for remote\nface-to-face collaboration focusing on 3D content.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 13:18:52 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Sousa", "Maur\u00edcio", ""], ["Mendes", "Daniel", ""], ["Anjos", "Rafael Kuffner dos", ""], ["Lopes", "Daniel Sim\u00f5es", ""], ["Jorge", "Joaquim", ""]]}, {"id": "1910.03515", "submitter": "Carol Smith", "authors": "Carol J. Smith", "title": "Designing Trustworthy AI: A Human-Machine Teaming Framework to Guide\n  Development", "comments": "Presented at AAAI FSS-19: Artificial Intelligence in Government and\n  Public Sector, Arlington, Virginia, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial intelligence (AI) holds great promise to empower us with knowledge\nand augment our effectiveness. We can -- and must -- ensure that we keep humans\nsafe and in control, particularly with regard to government and public sector\napplications that affect broad populations. How can AI development teams\nharness the power of AI systems and design them to be valuable to humans?\nDiverse teams are needed to build trustworthy artificial intelligent systems,\nand those teams need to coalesce around a shared set of ethics. There are many\ndiscussions in the AI field about ethics and trust, but there are few\nframeworks available for people to use as guidance when creating these systems.\nThe Human-Machine Teaming (HMT) Framework for Designing Ethical AI Experiences\ndescribed in this paper, when used with a set of technical ethics, will guide\nAI development teams to create AI systems that are accountable, de-risked,\nrespectful, secure, honest, and usable. To support the team's efforts,\nactivities to understand people's needs and concerns will be introduced along\nwith the themes to support the team's efforts. For example, usability testing\ncan help determine if the audience understands how the AI system works and\ncomplies with the HMT Framework. The HMT Framework is based on reviews of\nexisting ethical codes and best practices in human-computer interaction and\nsoftware development. Human-machine teams are strongest when human users can\ntrust AI systems to behave as expected, safely, securely, and understandably.\nUsing the HMT Framework to design trustworthy AI systems will provide support\nto teams in identifying potential issues ahead of time and making great\nexperiences for humans.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 16:19:49 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Smith", "Carol J.", ""]]}, {"id": "1910.03622", "submitter": "Anas Mahmoud", "authors": "Fahimeh Ebrahimi, Miroslav Tushev, Anas Mahmoud", "title": "Mobile App Privacy in Software Engineering Research: A Systematic\n  Mapping Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile applications (apps) have become deeply personal, constantly demanding\naccess to privacy-sensitive information in exchange for more personalized user\nexperiences. Such privacy-invading practices have generated major\nmultidimensional and unconventional privacy concerns among app users. To\naddress these concerns, the research on mobile app privacy has experienced\nrapid growth over the past decade. In general, this line of research is aimed\nat systematically exposing the privacy practices of apps and proposing\nsolutions to protect the privacy of mobile app users. In this survey paper, we\nconduct a systematic mapping study of 54 Software Engineering (SE) primary\nstudies on mobile app privacy. Our objectives are to a) explore trends in SE\napp privacy research, b) categorize existing evidence, and c) identify\npotential directions for future research. Our results show that existing\nliterature can be divided into four main categories: privacy policy,\nrequirements, user perspective, and leak detection. Furthermore, our survey\nreveals an imbalance between these categories; majority of existing research\nfocuses on proposing tools for detecting privacy leaks, with less studies\ntargeting privacy requirements and policy and even less on user perspective.\nFinally, our survey exposes several gaps in existing research and suggests\nareas for improvement.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:14:54 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Ebrahimi", "Fahimeh", ""], ["Tushev", "Miroslav", ""], ["Mahmoud", "Anas", ""]]}, {"id": "1910.03641", "submitter": "Haoqi Li", "authors": "Haoqi Li, Brian Baucom and Panayiotis Georgiou", "title": "Linking emotions to behaviors through deep transfer learning", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human behavior refers to the way humans act and interact. Understanding human\nbehavior is a cornerstone of observational practice, especially in\npsychotherapy. An important cue of behavior analysis is the dynamical changes\nof emotions during the conversation. Domain experts integrate emotional\ninformation in a highly nonlinear manner, thus, it is challenging to explicitly\nquantify the relationship between emotions and behaviors. In this work, we\nemploy deep transfer learning to analyze their inferential capacity and\ncontextual importance. We first train a network to quantify emotions from\nacoustic signals and then use information from the emotion recognition network\nas features for behavior recognition. We treat this emotion-related information\nas behavioral primitives and further train higher level layers towards behavior\nquantification. Through our analysis, we find that emotion-related information\nis an important cue for behavior recognition. Further, we investigate the\nimportance of emotional-context in the expression of behavior by constraining\n(or not) the neural networks' contextual view of the data. This demonstrates\nthat the sequence of emotions is critical in behavior expression. To achieve\nthese frameworks we employ hybrid architectures of convolutional networks and\nrecurrent networks to extract emotion-related behavior primitives and\nfacilitate automatic behavior recognition from speech.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:55:08 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Li", "Haoqi", ""], ["Baucom", "Brian", ""], ["Georgiou", "Panayiotis", ""]]}, {"id": "1910.03779", "submitter": "Juntao Wang Mr", "authors": "Juntao Wang, Yang Liu, Yiling Chen", "title": "Forecast Aggregation via Peer Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.HC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is a popular paradigm for soliciting forecasts on future\nevents. As people may have different forecasts, how to aggregate solicited\nforecasts into a single accurate prediction remains to be an important\nchallenge, especially when no historical accuracy information is available for\nidentifying experts. In this paper, we borrow ideas from the peer prediction\nliterature and assess the prediction accuracy of participants using solely the\ncollected forecasts. This approach leverages the correlations among peer\nreports to cross-validate each participant's forecasts and allows us to assign\na \"peer assessment score (PAS)\" for each agent as a proxy for the agent's\nprediction accuracy. We identify several empirically effective methods to\ngenerate PAS and propose an aggregation framework that uses PAS to identify\nexperts and to boost existing aggregators' prediction accuracy. We evaluate our\nmethods over 14 real-world datasets and show that i) PAS generated from peer\nprediction methods can approximately reflect the prediction accuracy of agents,\nand ii) our aggregation framework demonstrates consistent and significant\nimprovement in the prediction accuracy over existing aggregators for both\nbinary and multi-choice questions under three popular accuracy measures: Brier\nscore (mean square error), log score (cross-entropy loss) and AUC-ROC.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 04:07:13 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 00:27:10 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 19:39:40 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2021 05:03:42 GMT"}, {"version": "v5", "created": "Thu, 4 Mar 2021 07:28:11 GMT"}, {"version": "v6", "created": "Tue, 27 Apr 2021 21:11:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Wang", "Juntao", ""], ["Liu", "Yang", ""], ["Chen", "Yiling", ""]]}, {"id": "1910.03970", "submitter": "Marcin Straczkiewicz", "authors": "Marcin Straczkiewicz, Peter James, Jukka-Pekka Onnela", "title": "A systematic review of smartphone-based human activity recognition for\n  health research", "comments": "47 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Smartphones are now nearly ubiquitous; their numerous built-in\nsensors enable continuous measurement of activities of daily living, making\nthem especially well-suited for health research. Researchers have proposed\nvarious human activity recognition (HAR) systems aimed at translating\nmeasurements from smartphones into various types of physical activity. In this\nreview, we summarize the existing approaches to smartphone-based HAR. Methods:\nWe systematically searched Scopus, PubMed, and Web of Science for peer-reviewed\narticles published up to December 2020 on the use of smartphones for HAR. We\nextracted information on smartphone body location, sensors, and physical\nactivity types studied and the data transformation techniques and\nclassification schemes used for activity recognition. Results: We identified\n108 articles and described the various approaches used for data acquisition,\ndata preprocessing, feature extraction, and activity classification,\nidentifying the most common practices and their alternatives. Conclusions:\nSmartphones are well-suited for HAR research in the health sciences. For\npopulation-level impact, future studies should focus on improving quality of\ncollected data, address missing data, incorporate more diverse participants and\nactivities, relax requirements about phone placement, provide more complete\ndocumentation on study participants, and share the source code of the\nimplemented methods and algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:45:10 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 04:21:41 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Straczkiewicz", "Marcin", ""], ["James", "Peter", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1910.04273", "submitter": "Ayush Kumar", "authors": "Ayush Kumar, Rudolf Netzel, Michael Burch, Daniel Weiskopf, and Klaus\n  Mueller", "title": "Visual Multi-Metric Grouping of Eye-Tracking Data", "comments": "17 pages, 15 figures", "journal-ref": "Journal of Eye Movement Research, 2018", "doi": "10.16910/jemr.10.5.10", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithmic and visual grouping of participants and\neye-tracking metrics derived from recorded eye-tracking data. Our method\nutilizes two well-established visualization concepts. First, parallel\ncoordinates are used to provide an overview of the used metrics, their\ninteractions, and similarities, which helps select suitable metrics that\ndescribe characteristics of the eye-tracking data. Furthermore, parallel\ncoordinates plots enable an analyst to test the effects of creating a\ncombination of a subset of metrics resulting in a newly derived eye-tracking\nmetric. Second, a similarity matrix visualization is used to visually represent\nthe affine combination of metrics utilizing an algorithmic grouping of subjects\nthat leads to distinct visual groups of similar behavior. To keep the diagrams\nof the matrix visualization simple and understandable, we visually encode our\neye-tracking data into the cells of a similarity matrix of participants. The\nalgorithmic grouping is performed with a clustering based on the affine\ncombination of metrics, which is also the basis for the similarity value\ncomputation of the similarity matrix. To illustrate the usefulness of our\nvisualization, we applied it to an eye-tracking data set involving the reading\nbehavior of metro maps of up to 40 participants. Finally, we discuss\nlimitations and scalability issues of the approach focusing on visual and\nperceptual issues.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 21:52:20 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Kumar", "Ayush", ""], ["Netzel", "Rudolf", ""], ["Burch", "Michael", ""], ["Weiskopf", "Daniel", ""], ["Mueller", "Klaus", ""]]}, {"id": "1910.04357", "submitter": "Wei Xu", "authors": "Xinyi Huang, Suphanut Jamonnak, Ye Zhao, Boyu Wang, Minh Hoai, Kevin\n  Yager, Wei Xu", "title": "Visual Understanding of Multiple Attributes Learning Model of X-Ray\n  Scattering Images", "comments": "5 pages, 2 figures, ICCV conference co-held XAIC workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This extended abstract presents a visualization system, which is designed for\ndomain scientists to visually understand their deep learning model of\nextracting multiple attributes in x-ray scattering images. The system focuses\non studying the model behaviors related to multiple structural attributes. It\nallows users to explore the images in the feature space, the classification\noutput of different attributes, with respect to the actual attributes labelled\nby domain scientists. Abundant interactions allow users to flexibly select\ninstance images, their clusters, and compare them visually in details. Two\npreliminary case studies demonstrate its functionalities and usefulness.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 03:51:58 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Huang", "Xinyi", ""], ["Jamonnak", "Suphanut", ""], ["Zhao", "Ye", ""], ["Wang", "Boyu", ""], ["Hoai", "Minh", ""], ["Yager", "Kevin", ""], ["Xu", "Wei", ""]]}, {"id": "1910.04386", "submitter": "Thomas Kerdreux", "authors": "Vivien Cabannes and Thomas Kerdreux and Louis Thiry and Tina Campana\n  and Charly Ferrandes", "title": "Dialog on a canvas with a machine", "comments": "Accepted for poster at creativity workshop NeurIPS 2019", "journal-ref": "creativity workshop NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new form of human-machine interaction. It is a pictorial game\nconsisting of interactive rounds of creation between artists and a machine.\nThey repetitively paint one after the other. At its rounds, the computer\npartially completes the drawing using machine learning algorithms, and projects\nits additions directly on the canvas, which the artists are free to insert or\nmodify. Alongside fostering creativity, the process is designed to question the\ngrowing interaction between humans and machines.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 06:33:28 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 15:40:26 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Cabannes", "Vivien", ""], ["Kerdreux", "Thomas", ""], ["Thiry", "Louis", ""], ["Campana", "Tina", ""], ["Ferrandes", "Charly", ""]]}, {"id": "1910.04424", "submitter": "Boris Ruf", "authors": "Boris Ruf, Matteo Sammarco, Marcin Detyniecki", "title": "Contract Statements Knowledge Service for Chatbots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Towards conversational agents that are capable of handling more complex\nquestions on contractual conditions, formalizing contract statements in a\nmachine readable way is crucial. However, constructing a formal model which\ncaptures the full scope of a contract proves difficult due to the overall\ncomplexity its set of rules represent. Instead, this paper presents a top-down\napproach to the problem. After identifying the most relevant contract\nstatements, we model their underlying rules in a novel knowledge engineering\nmethod. A user-friendly tool we developed for this purpose allows to do so\neasily and at scale. Then, we expose the statements as service so they can get\nsmoothly integrated in any chatbot framework.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 08:25:42 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Ruf", "Boris", ""], ["Sammarco", "Matteo", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "1910.04695", "submitter": "Ethan Shaotran", "authors": "Ethan Shaotran, Jonathan J. Cruz, Vijay Janapa Reddi", "title": "GLADAS: Gesture Learning for Advanced Driver Assistance Systems", "comments": "9 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-computer interaction (HCI) is crucial for the safety of lives as\nautonomous vehicles (AVs) become commonplace. Yet, little effort has been put\ntoward ensuring that AVs understand humans on the road. In this paper, we\npresent GLADAS, a simulator-based research platform designed to teach AVs to\nunderstand pedestrian hand gestures. GLADAS supports the training, testing, and\nvalidation of deep learning-based self-driving car gesture recognition systems.\nWe focus on gestures as they are a primordial (i.e, natural and common) way to\ninteract with cars. To the best of our knowledge, GLADAS is the first system of\nits kind designed to provide an infrastructure for further research into\nhuman-AV interaction. We also develop a hand gesture recognition algorithm for\nself-driving cars, using GLADAS to evaluate its performance. Our results show\nthat an AV understands human gestures 85.91% of the time, reinforcing the need\nfor further research into human-AV interaction.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 03:55:45 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Shaotran", "Ethan", ""], ["Cruz", "Jonathan J.", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "1910.04696", "submitter": "Diego Seco", "authors": "Miguel \\'Angel Bernab\\'e, Jacinto Estima, Mar\\'ia Ester Gonz\\'alez,\n  Carlos Granell, Carlos L\\'opez-V\\'azquez, Miguel R. Luaces, Bruno Martins,\n  Daniela Moctezuma, Diego Seco", "title": "IDEAIS: Smart Voice Assistants to Improve Interaction with SDIs", "comments": "This research has received funding from CYTED, Ibero-American Program\n  of Science and Technology for Development, GA No. 519RT0579", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical goal, is that organizations and citizens can easily access the\ngeographic information required for good governance. However, despite the\ncostly efforts of governments to create and implement Spatial Data\nInfrastructures (SDIs), this goal is far from being achieved. This is partly\ndue to the lack of usability of the geoportals through which the geographic\ninformation is accessed. In this position paper, we present IDEAIS, a research\nnetwork composed of multiple Ibero-American partners to address this usability\nissue through the use of Intelligent Systems, in particular Smart Voice\nAssistants, to efficiently recover and access geographic information.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 13:42:59 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Bernab\u00e9", "Miguel \u00c1ngel", ""], ["Estima", "Jacinto", ""], ["Gonz\u00e1lez", "Mar\u00eda Ester", ""], ["Granell", "Carlos", ""], ["L\u00f3pez-V\u00e1zquez", "Carlos", ""], ["Luaces", "Miguel R.", ""], ["Martins", "Bruno", ""], ["Moctezuma", "Daniela", ""], ["Seco", "Diego", ""]]}, {"id": "1910.04697", "submitter": "Tommy Nilsson", "authors": "Glyn Lawson, Emily Shaw, Tessa Roper, Tommy Nilsson, Laura\n  Bajorunaite, Ayesha Batool", "title": "Immersive virtual worlds: Multi-sensory virtual environments for health\n  and safety training", "comments": "IOSH, (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual environments (VEs) offer potential benefits to health and safety\ntraining: exposure to dangerous (virtual) environments; the opportunity for\nexperiential learning; and a high level of control over the training, in that\naspects can be repeated or reviewed based on the trainee's performance.\nHowever, VEs are typically presented as audiovisual (AV) systems, whereas\nengagement of other senses could increase the immersion in the virtual\nexperience. Moreover, other senses play a key role in certain health and safety\ncontexts, for example the feel of heat and smell in a fire or smell in a fuel\nleak. A multisensory (MS) VE was developed, which provided simulated heat and\nsmell in accordance with events in a virtual world. As users approached a\nvirtual fire, they felt heat from three 2 kW heaters and smelled smoke from a\nscent diffuser. Behaviours in the MS VE demonstrated higher validity than those\nin a comparable AV VE, which ratings and verbatim responses indicated was down\nto a greater belief that participants were in a real fire. However, a study of\nthe effectiveness of the MS VE as a training tool demonstrated that it did not\noffer benefits over AV as measured by a written knowledge test and subjective\nratings of engagement, attitude towards health and safety and desire to repeat.\nHowever, the study found further evidence for the use of AV VEs in health and\nsafety training, particularly as the subjective ratings were generally better\nthan for PowerPoint based training. Despite the lack of evidence for MS\nsimulation on traditional measures of training, the different attitudes and\nexperiences of users suggest that it may have value as a system for changing\ntrainees' attitudes towards their personal safety and awareness. This view was\nsupported by feedback from industrial partners.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 18:26:33 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Lawson", "Glyn", ""], ["Shaw", "Emily", ""], ["Roper", "Tessa", ""], ["Nilsson", "Tommy", ""], ["Bajorunaite", "Laura", ""], ["Batool", "Ayesha", ""]]}, {"id": "1910.04698", "submitter": "Prithaj Jana", "authors": "Prithaj Jana and Emil Joswin", "title": "Brown Ring Experiment in Virtual Reality", "comments": "3 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brown Ring Experiment is a very popular test to detect the presence of\nNitrate in salts commonly performed in chemical laboratories with supplies of\nrequired chemicals. Our work clears out the need for a chemical laboratory and\nchemicals in order to understand the experiment practically. We have used the\ntechnology of Virtual Reality to fulfill this requirement. Our research work\ncan be extensively utilized to create virtual environments for conducting other\nchemical processes in a virtual environment hence, eliminating the need for a\nchemical laboratory. This can help students in remote areas with minimal\nresources to fill in the void of practical experiments they have in their\nlearning process due to space constraints.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 21:10:19 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Jana", "Prithaj", ""], ["Joswin", "Emil", ""]]}, {"id": "1910.04699", "submitter": "Weston Aenchbacher", "authors": "Martin Alain, Weston Aenchbacher, Aljosa Smolic", "title": "Interactive Light Field Tilt-Shift Refocus with Generalized\n  Shift-and-Sum", "comments": "4 pages, 5 figures, to be published in Proceedings of the European\n  Light field Imaging Workshop 2019, authors Martin Alain and Weston\n  Aenchbacher contributed equally to this publication, additional results can\n  be found at https://v-sense.scss.tcd.ie/research/tilt-shift/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Since their introduction more than two decades ago, light fields have gained\nconsiderable interest in graphics and vision communities due to their ability\nto provide the user with interactive visual content. One of the earliest and\nmost common light field operations is digital refocus, enabling the user to\nchoose the focus and depth-of-field for the image after capture. A common\ninteractive method for such an operation utilizes disparity estimations,\nreadily available from the light field, to allow the user to point-and-click on\nthe image to chose the location of the refocus plane.\n  In this paper, we address the interactivity of a lesser-known light field\noperation: refocus to a non-frontoparallel plane, simulating the result of\ntraditional tilt-shift photography. For this purpose we introduce a generalized\nshift-and-sum framework. Further, we show that the inclusion of depth\ninformation allows for intuitive interactive methods for placement of the\nrefocus plane. In addition to refocusing, light fields also enable the user to\ninteract with the viewpoint, which can be easily included in the proposed\ngeneralized shift-and-sum framework.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 16:56:52 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Alain", "Martin", ""], ["Aenchbacher", "Weston", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1910.04703", "submitter": "Gregory Gutmann PhD", "authors": "Gregory Gutmann, Akihiko Konagaya", "title": "Predictive Simulation: Using Regression and Artificial Neural Networks\n  to Negate Latency in Networked Interactive Virtual Reality", "comments": "Accompanying materials:\n  https://codingbyexample.com/2019/10/02/predictive-simulation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current virtual reality systems are typically limited by performance/cost,\nusability (size), or a combination of both. By using a networked client/server\nenvironment, we have solved these limitations for the client. However, in doing\nso we have introduced a new problem, namely increased latency. Interactive\nnetworked virtual environments such as games and simulations have existed for\nnearly as long as the Internet and have consistently faced latency issues. We\npropose a solution for negating the effects of latency for interactive\nnetworked virtual environments with lightweight clients, with respect to the\nserver being used. The proposed method extrapolates future client states to be\nincorporated in the server's updates, which helps to synchronize actions on the\nclient-side and the results coming from the server. We refer to this approach\nas predictive simulation. In addition to describing our method, in this paper,\nwe look at extrapolation methods because the success of our predictive\nsimulation method is dependent on strong predictions. We focus on regression\nmethods and briefly examine the use of artificial neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 06:29:39 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Gutmann", "Gregory", ""], ["Konagaya", "Akihiko", ""]]}, {"id": "1910.04836", "submitter": "Shiwali Mohan", "authors": "Shiwali Mohan and Anusha Venkatakrishnan and Andrea Hartzler", "title": "Designing an AI Health Coach and Studying its Utility in Promoting\n  Regular Aerobic Exercise", "comments": null, "journal-ref": "ACM Transactions of Interactive Intelligent Syststems 10, 2,\n  Article 14 (May 2020), 30 pages", "doi": "10.1145/3366501", "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research aims to develop interactive, social agents that can coach people\nto learn new tasks, skills, and habits. In this paper, we focus on coaching\nsedentary, overweight individuals (i.e., trainees) to exercise regularly. We\nemploy adaptive goal setting in which the intelligent health coach generates,\ntracks, and revises personalized exercise goals for a trainee. The goals become\nincrementally more difficult as the trainee progresses through the training\nprogram. Our approach is model-based - the coach maintains a parameterized\nmodel of the trainee's aerobic capability that drives its expectation of the\ntrainee's performance. The model is continually revised based on trainee-coach\ninteractions. The coach is embodied in a smartphone application, NutriWalking,\nwhich serves as a medium for coach-trainee interaction. We adopt a task-centric\nevaluation approach for studying the utility of the proposed algorithm in\npromoting regular aerobic exercise. We show that our approach can adapt the\ntrainee program not only to several trainees with different capabilities, but\nalso to how a trainee's capability improves as they begin to exercise more.\nExperts rate the goals selected by the coach better than other plausible goals,\ndemonstrating that our approach is consistent with clinical recommendations.\nFurther, in a 6-week observational study with sedentary participants, we show\nthat the proposed approach helps increase exercise volume performed each week.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 20:07:15 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 17:58:21 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Mohan", "Shiwali", ""], ["Venkatakrishnan", "Anusha", ""], ["Hartzler", "Andrea", ""]]}, {"id": "1910.04855", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Stefanos Zafeiriou", "title": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task\n  Learning and ArcFace", "comments": "oral presentation in BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective computing has been largely limited in terms of available data\nresources. The need to collect and annotate diverse in-the-wild datasets has\nbecome apparent with the rise of deep learning models, as the default approach\nto address any computer vision task. Some in-the-wild databases have been\nrecently proposed. However: i) their size is small, ii) they are not\naudiovisual, iii) only a small part is manually annotated, iv) they contain a\nsmall number of subjects, or v) they are not annotated for all main behavior\ntasks (valence-arousal estimation, action unit detection and basic expression\nclassification). To address these, we substantially extend the largest\navailable in-the-wild database (Aff-Wild) to study continuous emotions such as\nvalence and arousal. Furthermore, we annotate parts of the database with basic\nexpressions and action units. As a consequence, for the first time, this allows\nthe joint study of all three types of behavior states. We call this database\nAff-Wild2. We conduct extensive experiments with CNN and CNN-RNN architectures\nthat use visual and audio modalities; these networks are trained on Aff-Wild2\nand their performance is then evaluated on 10 publicly available emotion\ndatabases. We show that the networks achieve state-of-the-art performance for\nthe emotion recognition tasks. Additionally, we adapt the ArcFace loss function\nin the emotion recognition context and use it for training two new networks on\nAff-Wild2 and then re-train them in a variety of diverse expression recognition\ndatabases. The networks are shown to improve the existing state-of-the-art. The\ndatabase, emotion recognition models and source code are available at\nhttp://ibug.doc.ic.ac.uk/resources/aff-wild2.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 22:45:18 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1910.04863", "submitter": "Hamed Nikbakht", "authors": "Hamed Nikbakht", "title": "Merging Virtual and Real Environments for Visualizing Seismic Hazards\n  and Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earthquake research in the last few decades has led to considerable advances\nin seismic hazard and risk modeling across academia, industry, and government.\nTechnological advances such as high performance computing and visualization can\nfurther facilitate earthquake hazard and risk research. This work utilizes the\nCAVE of the Marquette Visualization Laboratory to visualize seismic hazards and\nrisk by integrating hazard characterization, structural modeling, and emergency\nresponse. Building upon the framework of performance-based earthquake\nengineering, site-specific ground motions, which link seismic hazards to\nstructural responses, serve as loading inputs to structural models. The\nresulting structural responses can then be translated into damage states of\nbuilding elements in the immediate room environment based on fragility\nfunctions. To illustrate, we display a map of the Los Angeles region with\nground motions for the Mw7.8 ShakeOut scenario, create a virtual room in a\nresidential building subjected to such earthquake shaking, and simulate\nemergency response in this immersive environment. The illustrative\nvisualization can be extended to various scenarios and help communicate site-\nand structure-specific hazards and risk to the general public.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 22:15:48 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 00:43:25 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Nikbakht", "Hamed", ""]]}, {"id": "1910.05243", "submitter": "Md. Mirajul Islam", "authors": "Sharmin Akther Purabi, Rayhan Rashed, Md. Mirajul Islam, Md. Nahiyan\n  Uddin, Mahmuda Naznin, and A. B. M. Alim Al Islam", "title": "As You Are, So Shall You Move Your Head: A System-Level Analysis between\n  Head Movements and Corresponding Traits and Emotions", "comments": "9 pages, 7 figures, NSysS 2019", "journal-ref": null, "doi": "10.1145/3362966.3362985", "report-no": null, "categories": "cs.HC cs.IR cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying physical traits and emotions based on system-sensed physical\nactivities is a challenging problem in the realm of human-computer interaction.\nOur work contributes in this context by investigating an underlying connection\nbetween head movements and corresponding traits and emotions. To do so, we\nutilize a head movement measuring device called eSense, which gives\nacceleration and rotation of a head. Here, first, we conduct a thorough study\nover head movement data collected from 46 persons using eSense while inducing\nfive different emotional states over them in isolation. Our analysis reveals\nseveral new head movement based findings, which in turn, leads us to a novel\nunified solution for identifying different human traits and emotions through\nexploiting machine learning techniques over head movement data. Our analysis\nconfirms that the proposed solution can result in high accuracy over the\ncollected data. Accordingly, we develop an integrated unified solution for\nreal-time emotion and trait identification using head movement data leveraging\noutcomes of our analysis.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 15:22:37 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Purabi", "Sharmin Akther", ""], ["Rashed", "Rayhan", ""], ["Islam", "Md. Mirajul", ""], ["Uddin", "Md. Nahiyan", ""], ["Naznin", "Mahmuda", ""], ["Islam", "A. B. M. Alim Al", ""]]}, {"id": "1910.05265", "submitter": "Chris Norval", "authors": "Chris Norval, Tristan Henderson", "title": "Automating dynamic consent decisions for the processing of social media\n  data in health research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media have become a rich source of data, particularly in health\nresearch. Yet, the use of such data raises significant ethical questions about\nthe need for the informed consent of those being studied. Consent mechanisms,\nif even obtained, are typically broad and inflexible, or place a significant\nburden on the participant. Machine learning algorithms show much promise for\nfacilitating a 'middle ground' approach: using trained models to predict and\nautomate granular consent decisions. Such techniques, however, raise a myriad\nof follow-on ethical and technical considerations. In this paper, we present an\nexploratory user study (n = 67) in which we find that we can predict the\nappropriate flow of health-related social media data with reasonable accuracy,\nwhile minimising undesired data leaks. We then attempt to deconstruct the\nfindings of this study, identifying and discussing a number of real-world\nimplications if such a technique were put into practice.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 15:57:00 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Norval", "Chris", ""], ["Henderson", "Tristan", ""]]}, {"id": "1910.05327", "submitter": "Andreas Mallas Mr.", "authors": "Andreas Mallas and Michalis Xenos", "title": "Gamification of In-Classroom Diagram Design for Science Students", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Merging the content of learning with the motivation of games can be a\nsuccessful combination, if done properly and supported by the appropriate tool.\nTowards this goal, we developed Diagramatic an environment used to gamify the\nin-classroom activity of designing diagrams during a lecture. Using Diagramatic\nthe professor, instead of lecturing about diagrams or showing examples of such\ndiagrams, can design short games where the students could play by competing\nduring the lecture. Diagramatic is a complete environment offering to the\nprofessor a design application to create games and a management application.\nThe management application is used for monitoring the games while students\nplay, as well as to present the results to the students after the end of each\ngame, or to evaluate these results after classroom time. The students may use\nthe mobile application on their mobiles to practice by designing diagrams\noutside of the classroom, as well as to play a game during classroom time, but\nonly after the professor starts this game. The environment handles the\ncommunication from students' mobiles to the professor's applications and vice\nversa, while the students submit their diagrams or receive the correct ones, so\nto proceed to follow up games. The current version of Diagramatic is tailored\nfor designing flow graphs used for path testing into a higher education\nsoftware engineering course, but the environment can be used in any similar\ncase requiring the design of diagrams (e.g. math, physics, chemistry).\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 17:43:21 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Mallas", "Andreas", ""], ["Xenos", "Michalis", ""]]}, {"id": "1910.05376", "submitter": "Dimitrios Kollias", "authors": "Alvertos Benroumpi and Dimitrios Kollias", "title": "AffWild Net and Aff-Wild Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions recognition is the task of recognizing people's emotions. Usually it\nis achieved by analyzing expression of peoples faces. There are two ways for\nrepresenting emotions: The categorical approach and the dimensional approach by\nusing valence and arousal values. Valence shows how negative or positive an\nemotion is and arousal shows how much it is activated. Recent deep learning\nmodels, that have to do with emotions recognition, are using the second\napproach, valence and arousal. Moreover, a more interesting concept, which is\nuseful in real life is the \"in the wild\" emotions recognition. \"In the wild\"\nmeans that the images analyzed for the recognition task, come from from real\nlife sources(online videos, online photos, etc.) and not from staged\nexperiments. So, they introduce unpredictable situations in the images, that\nhave to be modeled. The purpose of this project is to study the previous work\nthat was done for the \"in the wild\" emotions recognition concept, design a new\ndataset which has as a standard the \"Aff-wild\" database, implement new deep\nlearning models and evaluate the results. First, already existing databases and\ndeep learning models are presented. Then, inspired by them a new database is\ncreated which includes 507.208 frames in total from 106 videos, which were\ngathered from online sources. Then, the data are tested in a CNN model based on\nCNN-M architecture, in order to be sure about their usability. Next, the main\nmodel of this project is implemented. That is a Regression GAN which can\nexecute unsupervised and supervised learning at the same time. More\nspecifically, it keeps the main functionality of GANs, which is to produce fake\nimages that look as good as the real ones, while it can also predict valence\nand arousal values for both real and fake images. Finally, the database created\nearlier is applied to this model and the results are presented and evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 18:57:18 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 22:58:20 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Benroumpi", "Alvertos", ""], ["Kollias", "Dimitrios", ""]]}, {"id": "1910.05441", "submitter": "Morteza Karimzadeh", "authors": "Morteza Karimzadeh, Luke S. Snyder, David S. Ebert", "title": "Geovisual Analytics and Interactive Machine Learning for Situational\n  Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": "2019 US National Report, International Cartographic Association", "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first responder community has traditionally relied on calls from the\npublic, officially-provided geographic information and maps for coordinating\nactions on the ground. The ubiquity of social media platforms created an\nopportunity for near real-time sensing of the situation (e.g. unfolding weather\nevents or crises) through volunteered geographic information. In this article,\nwe provide an overview of the design process and features of the Social Media\nAnalytics Reporting Toolkit (SMART), a visual analytics platform developed at\nPurdue University for providing first responders with real-time situational\nawareness. We attribute its successful adoption by many first responders to its\nuser-centered design, interactive (geo)visualizations and interactive machine\nlearning, giving users control over analysis.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 23:33:55 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Karimzadeh", "Morteza", ""], ["Snyder", "Luke S.", ""], ["Ebert", "David S.", ""]]}, {"id": "1910.05514", "submitter": "Hassan Khosravi", "authors": "Kendra M.L. Cooper, Hassan Khosravi", "title": "Multilevel Visualisation of Topic Dependency Models for Assessment\n  Design and Delivery: A Hypergraph Based Approach", "comments": "Published in the proceedings of the 25th International DMS Conference\n  on Visualization and Visual Languages", "journal-ref": null, "doi": "10.18293/DMSVIVA2019-018", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effective design and delivery of assessments in a wide variety of\nevolving educational environments remains a challenging problem. Proposals have\nincluded the use of learning dashboards, peer learning environments, and\ngrading support systems; these embrace visualisations to summarise and\ncommunicate results. In an on-going project, the investigation of graph based\nvisualisation models for assessment design and delivery has yielded promising\nresults. Here, an alternative graph foundation, a two-weighted hypergraph, is\nconsidered to represent the assessment material (e.g., questions) and their\nexplicit mapping to one or more learning objective topics. The visualisation\napproach considers the hypergraph as a collection of levels; the content of\nthese levels can be customized and presented according to user preferences. A\ncase study on generating hypergraph models using commonly available assessment\ndata and a flexible visualisation approach using historical data from an\nintroductory programming course is presented\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 07:15:55 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Cooper", "Kendra M. L.", ""], ["Khosravi", "Hassan", ""]]}, {"id": "1910.05522", "submitter": "Hassan Khosravi", "authors": "Hassan Khosravi, Kirsty Kitto, Joseph Jay Williams", "title": "RiPPLE: A Crowdsourced Adaptive Platform for Recommendation of Learning\n  Activities", "comments": "To be published by the Journal of Learning Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a platform called RiPPLE (Recommendation in Personalised\nPeer-Learning Environments) that recommends personalized learning activities to\nstudents based on their knowledge state from a pool of crowdsourced learning\nactivities that are generated by educators and the students themselves. RiPPLE\nintegrates insights from crowdsourcing, learning sciences, and adaptive\nlearning, aiming to narrow the gap between these large bodies of research while\nproviding a practical platform-based implementation that instructors can easily\nuse in their courses. This paper provides a design overview of RiPPLE, which\ncan be employed as a standalone tool or embedded into any learning management\nsystem (LMS) or online platform that supports the Learning Tools\nInteroperability (LTI) standard. The platform has been evaluated based on a\npilot in an introductory course with 453 students at The University of\nQueensland. Initial results suggest that the use of the \\name platform led to\nmeasurable learning gains and that students perceived the platform as\nbeneficially supporting their learning.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 07:42:52 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Khosravi", "Hassan", ""], ["Kitto", "Kirsty", ""], ["Williams", "Joseph Jay", ""]]}, {"id": "1910.05536", "submitter": "Xuanwu Yue", "authors": "Xuanwu Yue, Jiaxin Bai, Qinhan Liu, Yiyang Tang, Abishek Puri, Ke Li,\n  Huamin Qu", "title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934660", "report-no": null, "categories": "cs.HC q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative Investment, built on the solid foundation of robust financial\ntheories, is at the center stage in investment industry today. The essence of\nquantitative investment is the multi-factor model, which explains the\nrelationship between the risk and return of equities. However, the multi-factor\nmodel generates enormous quantities of factor data, through which even\nexperienced portfolio managers find it difficult to navigate. This has led to\nportfolio analysis and factor research being limited by a lack of intuitive\nvisual analytics tools. Previous portfolio visualization systems have mainly\nfocused on the relationship between the portfolio return and stock holdings,\nwhich is insufficient for making actionable insights or understanding market\ntrends. In this paper, we present sPortfolio, which, to the best of our\nknowledge, is the first visualization that attempts to explore the factor\ninvestment area. In particular, sPortfolio provides a holistic overview of the\nfactor data and aims to facilitate the analysis at three different levels: a\nRisk-Factor level, for a general market situation analysis; a\nMultiple-Portfolio level, for understanding the portfolio strategies; and a\nSingle-Portfolio level, for investigating detailed operations. The system's\neffectiveness and usability are demonstrated through three case studies. The\nsystem has passed its pilot study and is soon to be deployed in industry.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 09:26:11 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Yue", "Xuanwu", ""], ["Bai", "Jiaxin", ""], ["Liu", "Qinhan", ""], ["Tang", "Yiyang", ""], ["Puri", "Abishek", ""], ["Li", "Ke", ""], ["Qu", "Huamin", ""]]}, {"id": "1910.05624", "submitter": "Matthew Marge", "authors": "Matthew Marge, Stephen Nogar, Cory J. Hayes, Stephanie M. Lukin, Jesse\n  Bloecker, Eric Holder, Clare Voss", "title": "A Research Platform for Multi-Robot Dialogue with Humans", "comments": "Accepted for publication at NAACL 2019; also presented at AI-HRI 2019\n  (arXiv:1909.04812)", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/05", "categories": "cs.RO cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a research platform that supports spoken dialogue\ninteraction with multiple robots. The demonstration showcases our crafted\nMultiBot testing scenario in which users can verbally issue search, navigate,\nand follow instructions to two robotic teammates: a simulated ground robot and\nan aerial robot. This flexible language and robotic platform takes advantage of\nexisting tools for speech recognition and dialogue management that are\ncompatible with new domains, and implements an inter-agent communication\nprotocol (tactical behavior specification), where verbal instructions are\nencoded for tasks assigned to the appropriate robot.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 18:59:50 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Marge", "Matthew", ""], ["Nogar", "Stephen", ""], ["Hayes", "Cory J.", ""], ["Lukin", "Stephanie M.", ""], ["Bloecker", "Jesse", ""], ["Holder", "Eric", ""], ["Voss", "Clare", ""]]}, {"id": "1910.05789", "submitter": "Micah Carroll", "authors": "Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A.\n  Seshia, Pieter Abbeel, Anca Dragan", "title": "On the Utility of Learning about Humans for Human-AI Coordination", "comments": "Published at NeurIPS 2019\n  (http://papers.nips.cc/paper/8760-on-the-utility-of-learning-about-humans-for-human-ai-coordination)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While we would like agents that can coordinate with humans, current\nalgorithms such as self-play and population-based training create agents that\ncan coordinate with themselves. Agents that assume their partner to be optimal\nor similar to them can converge to coordination protocols that fail to\nunderstand and be understood by humans. To demonstrate this, we introduce a\nsimple environment that requires challenging coordination, based on the popular\ngame Overcooked, and learn a simple model that mimics human play. We evaluate\nthe performance of agents trained via self-play and population-based training.\nThese agents perform very well when paired with themselves, but when paired\nwith our human model, they are significantly worse than agents designed to play\nwith the human model. An experiment with a planning algorithm yields the same\nconclusion, though only when the human-aware planner is given the exact human\nmodel that it is playing with. A user study with real humans shows this pattern\nas well, though less strongly. Qualitatively, we find that the gains come from\nhaving the agent adapt to the human's gameplay. Given this result, we suggest\nseveral approaches for designing agents that learn about humans in order to\nbetter coordinate with them. Code is available at\nhttps://github.com/HumanCompatibleAI/overcooked_ai.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 17:17:52 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 00:51:44 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Carroll", "Micah", ""], ["Shah", "Rohin", ""], ["Ho", "Mark K.", ""], ["Griffiths", "Thomas L.", ""], ["Seshia", "Sanjit A.", ""], ["Abbeel", "Pieter", ""], ["Dragan", "Anca", ""]]}, {"id": "1910.05878", "submitter": "Dongrui Wu", "authors": "Wen Zhang and Dongrui Wu", "title": "Manifold Embedded Knowledge Transfer for Brain-Computer Interfaces", "comments": null, "journal-ref": "IEEE Trans. on Neural Systems and Rehabilitation Engineering,\n  28(5), pp. 1117-1127, 2020", "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning makes use of data or knowledge in one problem to help solve\na different, yet related, problem. It is particularly useful in brain-computer\ninterfaces (BCIs), for coping with variations among different subjects and/or\ntasks. This paper considers offline unsupervised cross-subject\nelectroencephalogram (EEG) classification, i.e., we have labeled EEG trials\nfrom one or more source subjects, but only unlabeled EEG trials from the target\nsubject. We propose a novel manifold embedded knowledge transfer (MEKT)\napproach, which first aligns the covariance matrices of the EEG trials in the\nRiemannian manifold, extracts features in the tangent space, and then performs\ndomain adaptation by minimizing the joint probability distribution shift\nbetween the source and the target domains, while preserving their geometric\nstructures. MEKT can cope with one or multiple source domains, and can be\ncomputed efficiently. We also propose a domain transferability estimation (DTE)\napproach to identify the most beneficial source domains, in case there are a\nlarge number of source domains. Experiments on four EEG datasets from two\ndifferent BCI paradigms demonstrated that MEKT outperformed several\nstate-of-the-art transfer learning approaches, and DTE can reduce more than\nhalf of the computational cost when the number of source subjects is large,\nwith little sacrifice of classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 01:33:33 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 19:38:50 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhang", "Wen", ""], ["Wu", "Dongrui", ""]]}, {"id": "1910.05998", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Allen Y. Yang, Woojin Ko, Luisa Caldas", "title": "Optimization and Manipulation of Contextual Mutual Spaces for Multi-User\n  Virtual and Augmented Reality Interaction", "comments": "Accepted at 2020 IEEE Conference on Virtual Reality and 3D User\n  Interfaces (VR)", "journal-ref": null, "doi": "10.1109/VR46266.2020.00055", "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial computing experiences are physically constrained by the geometry and\nsemantics of the local user environment. This limitation is elevated in remote\nmulti-user interaction scenarios, where finding a common virtual ground\nphysically accessible for all participants becomes challenging. Locating a\ncommon accessible virtual ground is difficult for the users themselves,\nparticularly if they are not aware of the spatial properties of other\nparticipants. In this paper, we introduce a framework to generate an optimal\nmutual virtual space for a multi-user interaction setting where remote users'\nroom spaces can have different layout and sizes. The framework further\nrecommends movement of surrounding furniture objects that expand the size of\nthe mutual space with minimal physical effort. Finally, we demonstrate the\nperformance of our solution on real-world datasets and also a real HoloLens\napplication. Results show the proposed algorithm can effectively discover\noptimal shareable space for multi-user virtual interaction and hence facilitate\nremote spatial computing communication in various collaborative workflows.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:10:54 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 05:36:47 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Yang", "Allen Y.", ""], ["Ko", "Woojin", ""], ["Caldas", "Luisa", ""]]}, {"id": "1910.06234", "submitter": "No\\'e Tits", "authors": "No\\'e Tits, Kevin El Haddad, Thierry Dutoit", "title": "The Theory behind Controllable Expressive Speech Synthesis: a\n  Cross-disciplinary Approach", "comments": "19 pages, 6 figures. To be published in the book \"Human Computer\n  Interaction\" edited by Prof. Yves Rybarczyk, published by IntechOpen", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.HC cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of the Human-Computer Interaction field, Expressive speech synthesis\nis a very rich domain as it requires knowledge in areas such as machine\nlearning, signal processing, sociology, psychology. In this Chapter, we will\nfocus mostly on the technical side. From the recording of expressive speech to\nits modeling, the reader will have an overview of the main paradigms used in\nthis field, through some of the most prominent systems and methods. We explain\nhow speech can be represented and encoded with audio features. We present a\nhistory of the main methods of Text-to-Speech synthesis: concatenative,\nparametric and statistical parametric speech synthesis. Finally, we focus on\nthe last one, with the last techniques modeling Text-to-Speech synthesis as a\nsequence-to-sequence problem. This enables the use of Deep Learning blocks such\nas Convolutional and Recurrent Neural Networks as well as Attention Mechanism.\nThe last part of the Chapter intends to assemble the different aspects of the\ntheory and summarize the concepts.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:08:33 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Tits", "No\u00e9", ""], ["Haddad", "Kevin El", ""], ["Dutoit", "Thierry", ""]]}, {"id": "1910.06380", "submitter": "Colin Ife", "authors": "Colin C. Ife, Toby Davies, Steven J. Murdoch, and Gianluca Stringhini", "title": "Bridging Information Security and Environmental Criminology Research to\n  Better Mitigate Cybercrime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybercrime is a complex phenomenon that spans both technical and human\naspects. As such, two disjoint areas have been studying the problem from\nseparate angles: the information security community and the environmental\ncriminology one. Despite the large body of work produced by these communities\nin the past years, the two research efforts have largely remained disjoint,\nwith researchers on one side not benefitting from the advancements proposed by\nthe other. In this paper, we argue that it would be beneficial for the\ninformation security community to look at the theories and systematic\nframeworks developed in environmental criminology to develop better mitigations\nagainst cybercrime. To this end, we provide an overview of the research from\nenvironmental criminology and how it has been applied to cybercrime. We then\nsurvey some of the research proposed in the information security domain,\ndrawing explicit parallels between the proposed mitigations and environmental\ncriminology theories, and presenting some examples of new mitigations against\ncybercrime. Finally, we discuss the concept of cyberplaces and propose a\nframework in order to define them. We discuss this as a potential research\ndirection, taking into account both fields of research, in the hope of\nbroadening interdisciplinary efforts in cybercrime research\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 18:53:40 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Ife", "Colin C.", ""], ["Davies", "Toby", ""], ["Murdoch", "Steven J.", ""], ["Stringhini", "Gianluca", ""]]}, {"id": "1910.06566", "submitter": "Joachim Meyer", "authors": "Nir Douer, Meirav Redlich, Joachim Meyer", "title": "Objective and Subjective Responsibility of a Control-Room Worker", "comments": null, "journal-ref": "Proceedings of the 64th Annual Meeting of the Human Factors and\n  Ergonomics Society, 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working with AI and advanced automation, human responsibility for\noutcomes becomes equivocal. We applied a newly developed responsibility\nquantification model (ResQu) to the real world setting of a control room in a\ndairy factory to calculate workers' objective responsibility in a common fault\nscenario. We compared the results to the subjective assessments made by\ndifferent functions in the diary. The capabilities of the automation greatly\nexceeded those of the human, and the optimal operator should have fully\ncomplied with the indications of the automation. Thus, in this case, the\noperator had no unique contribution, and the objective causal human\nresponsibility was zero. However, outside observers, such as managers, tended\nto assign much higher responsibility to the operator, in a manner that\nresembled aspects of the \"fundamental attribution error\". This, in turn, may\nlead to unjustifiably holding operators responsible for adverse outcomes in\nsituations in which they rightly trusted the automation, and acted accordingly.\nWe demonstrate the use of the ResQu model for the analysis of human causal\nresponsibility in intelligent systems. The model can help calibrate exogenous\nsubjective responsibility attributions, aid system design, and guide policy and\nlegal decisions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 07:33:56 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Douer", "Nir", ""], ["Redlich", "Meirav", ""], ["Meyer", "Joachim", ""]]}, {"id": "1910.06824", "submitter": "Kizito Nkurikiyeyezu", "authors": "Kizito Nkurikiyeyezu, Anna Yokokubo, Guillaume Lopez", "title": "Affect-aware thermal comfort provision in intelligent buildings", "comments": null, "journal-ref": null, "doi": "10.1109/ACIIW.2019.8925184", "report-no": null, "categories": "cs.HC eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predominant thermal comfort provision technologies are energy-hungry, and yet\nthey perform crudely because they overlook the requisite precursors to thermal\ncomfort. They also fail to exclusively cool or heat the parts of the body\n(e.g., the wrist, the feet, and the head) that influence the most a person's\nthermal comfort satisfaction. Instead, they waste energy by heating or cooling\nthe whole room. This research investigates the influence of neck-coolers on\npeople's thermal comfort perception and proposes an effective method that\ndelivers thermal comfort depending on people's heart rate variability (HRV).\nMoreover, because thermal comfort is idiosyncratic and depends on unforeseeable\ncircumstances, only person-specific thermal comfort models are adequate for\nthis task. Unfortunately, using person-specific models would be costly and\ninflexible for deployment in, e.g., a smart building because a system that uses\nperson-specific models would require collecting extensive training data from\neach person in the building. As a compromise, we devise a hybrid,\ncost-effective, yet satisfactory technique that derives a personalized\nperson-specific-like model from samples collected from a large population. For\nexample, it was possible to double the accuracy of a generic model (from 47.77%\nto 96.11%) using only 400 person-specific calibration samples. Finally, we\npropose a practical implementation of a real-time thermal comfort provision\nsystem that uses this strategy and highlighted its advantages and limitations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 13:46:17 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 08:20:58 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Nkurikiyeyezu", "Kizito", ""], ["Yokokubo", "Anna", ""], ["Lopez", "Guillaume", ""]]}, {"id": "1910.06825", "submitter": "Alessio Arleo PhD", "authors": "Johannes Sorger, Manuela Waldner, Wolfgang Knecht, and Alessio Arleo", "title": "Immersive Analytics of Large Dynamic Networks via Overview and Detail\n  Navigation", "comments": null, "journal-ref": "2019 IEEE International Conference on Artificial Intelligence and\n  Virtual Reality (AIVR), 2019, pp. 144-1447", "doi": "10.1109/AIVR46125.2019.00030", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of large dynamic networks is a thriving research field, typically\nrelying on 2D graph representations. The advent of affordable head mounted\ndisplays however, sparked new interest in the potential of 3D visualization for\nimmersive network analytics. Nevertheless, most solutions do not scale well\nwith the number of nodes and edges and rely on conventional fly- or\nwalk-through navigation. In this paper, we present a novel approach for the\nexploration of large dynamic graphs in virtual reality that interweaves two\nnavigation metaphors: overview exploration and immersive detail analysis. We\nthereby use the potential of state-of-the-art VR headsets, coupled with a\nweb-based 3D rendering engine that supports heterogeneous input modalities to\nenable ad-hoc immersive network analytics. We validate our approach through a\nperformance evaluation and a case study with experts analyzing a co-morbidity\nnetwork.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 14:42:55 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 09:20:24 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Sorger", "Johannes", ""], ["Waldner", "Manuela", ""], ["Knecht", "Wolfgang", ""], ["Arleo", "Alessio", ""]]}, {"id": "1910.07058", "submitter": "Lauren Linkous", "authors": "Lauren Linkous, Nasibeh Zohrabi, Sherif Abdelwahed", "title": "Health Monitoring in Smart Homes Utilizing Internet of Things", "comments": "6 pages, 2 figures. Submitted to MCPS workshop in CHASE 2019\n  conference in Washington D.C", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the concept of the Internet of Things (IoT) has evolved to\nconnect commercial gadgets together with the medical field to facilitate an\nunprecedented range of accessibility. The development of medical devices\nconnected to internet of things has been praised for the potential of\nalleviating the strain on the modern healthcare system by giving users the\nopportunity to reside in the home during treatment or recovery. With the IoT\nbecoming more prevalent and available at a commercial level, there exists room\nfor integration into emerging, intelligent environments such as smart homes.\nWhen used in tandem with conventional healthcare, the IoT offers a vast range\nof custom-tailored treatment options. This paper studies recent\nstate-of-the-art research on the field of IoT for health monitoring and smart\nhomes, examines several potential use-cases of blending the technology, and\nproposes integration with an existing smart home testbed for further study.\nChallenges of adoption and future research on the topic are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 21:22:19 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Linkous", "Lauren", ""], ["Zohrabi", "Nasibeh", ""], ["Abdelwahed", "Sherif", ""]]}, {"id": "1910.07083", "submitter": "Utku Kose", "authors": "Utku Kose", "title": "Occurence of A Cyber Security Eco-System: A Nature Oriented Project and\n  Evaluation of An Indirect Social Experiment", "comments": "International Scientific and Vocational Studies Congress -\n  Engineering 2019, pp. 505-513, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Because of todays technological developments and the influence of digital\nsystems into every aspect of our lives, importance of cyber security improves\nmore and more day-by-day. Projects, educational processes and seminars realized\nfor this aim create and improve awareness among individuals and provide useful\ntools for growing equipped generations. The aim of this study is to focus on a\ncyber security eco-system, which was self-occurred within the interactive\neducational environment designed under the scope of TUBITAK 4004 Nature\nEducation and Science Schools Projects (with the name of A Cyber Security\nAdventure) with the use of important technologies such as virtual reality,\naugmented reality, and artificial intelligence. The eco-system occurred within\nthe interactive educational process where high school students took place\ncaused both students and the project team to experience an indirect social\nexperiment environment. In this sense, it is thought that the findings and\ncomments presented in the study will give important ideas to everyone involved\nin cyber security education, life-long learning processes, and the technology\nuse in software oriented educational tools.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 22:04:19 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Kose", "Utku", ""]]}, {"id": "1910.07089", "submitter": "Subbarao Kambhampati", "authors": "Subbarao Kambhampati", "title": "Challenges of Human-Aware AI Systems", "comments": "To appear in AI Magazine (Written version of AAAI 2018 Presidential\n  Address. Video and slides at http://bit.ly/2tHyzAh )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From its inception, AI has had a rather ambivalent relationship to\nhumans---swinging between their augmentation and replacement. Now, as AI\ntechnologies enter our everyday lives at an ever increasing pace, there is a\ngreater need for AI systems to work synergistically with humans. To do this\neffectively, AI systems must pay more attention to aspects of intelligence that\nhelped humans work with each other---including social intelligence. I will\ndiscuss the research challenges in designing such human-aware AI systems,\nincluding modeling the mental states of humans in the loop, recognizing their\ndesires and intentions, providing proactive support, exhibiting explicable\nbehavior, giving cogent explanations on demand, and engendering trust. I will\nsurvey the progress made so far on these challenges, and highlight some\npromising directions. I will also touch on the additional ethical quandaries\nthat such systems pose. I will end by arguing that the quest for human-aware AI\nsystems broadens the scope of AI enterprise, necessitates and facilitates true\ninter-disciplinary collaborations, and can go a long way towards increasing\npublic acceptance of AI technologies.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 22:34:50 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Kambhampati", "Subbarao", ""]]}, {"id": "1910.07269", "submitter": "Karoline Busse", "authors": "Karoline Busse and Sabrina Amft and Daniel Hecker and Emanuel von\n  Zezschwitz", "title": "\"Get a Free Item Pack with Every Activation!\" -- Do Incentives Increase\n  the Adoption Rates of Two-Factor Authentication?", "comments": "Accepted in Journal of Interactive Media (i-com), Special Issue on\n  Usable Security and Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Account security is an ongoing issue in practice. Two-Factor Authentication\n(2FA) is a mechanism which could help mitigate this problem, however adoption\nis not very high in most domains. Online gaming has adopted an interesting\napproach to drive adoption: Games offer small rewards such as visual\nmodifications to the player's avatar's appearance, if players utilize 2FA. In\nthis paper, we evaluate the effectiveness of these incentives and investigate\nhow they can be applied to non-gaming contexts. We conducted two surveys, one\nrecruiting gamers and one recruiting from a general population. In addition, we\nconducted three focus group interviews to evaluate various incentive designs\nfor both, the gaming context and the non-gaming context. We found that visual\nmodifications, which are the most popular type of gaming-related incentives,\nare not as popular in non-gaming contexts. However, our design explorations\nindicate that well-chosen incentives have the potential to lead to more users\nadopting 2FA, even outside of the gaming context.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 10:39:54 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Busse", "Karoline", ""], ["Amft", "Sabrina", ""], ["Hecker", "Daniel", ""], ["von Zezschwitz", "Emanuel", ""]]}, {"id": "1910.07381", "submitter": "Irene-Angelica Chounta", "authors": "Irene-Angelica Chounta", "title": "Using learning analytics to provide personalized recommendations for\n  finding peers", "comments": "4 pages, 1 figure, conference, CollabTech", "journal-ref": null, "doi": "10.13140/RG.2.2.36616.78081", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to propose a method to support students in finding appropriate\npeers in collaborative and blended learning settings. The main goal of this\nresearch is to bridge the gap between pedagogical theory and data driven\npractice to provide personalized and adaptive guidance to students who engage\nin computer supported learning activities. The research hypothesis is that we\ncan use Learning Analytics to model students' cognitive state and to assess\nwhether the student is in the Zone of Proximal Development. Based on this\nassessment, we can plan how to provide scaffolding based on the principles of\nContingent Tutoring and how to form study groups based on the principles of the\nZone of Proximal Development.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 14:40:27 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Chounta", "Irene-Angelica", ""]]}, {"id": "1910.07428", "submitter": "Weixing Chen", "authors": "W.X. Chen, X.Y. Cui, J. Zheng, J.M. Zhang, S. Chen and Y.D. Yao", "title": "Gaze Gestures and Their Applications in human-computer interaction with\n  a head-mounted display", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A head-mounted display (HMD) is a portable and interactive display device.\nWith the development of 5G technology, it may become a general-purpose\ncomputing platform in the future. Human-computer interaction (HCI) technology\nfor HMDs has also been of significant interest in recent years. In addition to\ntracking gestures and speech, tracking human eyes as a means of interaction is\nhighly effective. In this paper, we propose two UnityEyes-based convolutional\nneural network models, UEGazeNet and UEGazeNet*, which can be used for input\nimages with low resolution and high resolution, respectively. These models can\nperform rapid interactions by classifying gaze trajectories (GTs), and a\nGTgestures dataset containing data for 10,200 \"eye-painting gestures\" collected\nfrom 15 individuals is established with our gaze-tracking method. We evaluated\nthe performance both indoors and outdoors and the UEGazeNet can obtaine results\n52\\% and 67\\% better than those of state-of-the-art networks. The\ngeneralizability of our GTgestures dataset using a variety of gaze-tracking\nmodels is evaluated, and an average recognition rate of 96.71\\% is obtained by\nour method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 15:42:10 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Chen", "W. X.", ""], ["Cui", "X. Y.", ""], ["Zheng", "J.", ""], ["Zhang", "J. M.", ""], ["Chen", "S.", ""], ["Yao", "Y. D.", ""]]}, {"id": "1910.07514", "submitter": "Deepali Aneja", "authors": "Deepali Aneja, Rens Hoegen, Daniel McDuff, and Mary Czerwinski", "title": "Designing Style Matching Conversational Agents", "comments": "Conversational Agents: Acting on the Wave of Research and\n  Development, CHI 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in machine intelligence have enabled conversational interfaces that\nhave the potential to radically change the way humans interact with machines.\nHowever, even with the progress in the abilities of these agents, there remain\ncritical gaps in their capacity for natural interactions. One limitation is\nthat the agents are often monotonic in behavior and do not adapt to their\npartner. We built two end-to-end conversational agents: a voice-based agent\nthat can engage in naturalistic, multi-turn dialogue and align with the\ninterlocutor's conversational style, and a 2nd, expressive, embodied\nconversational agent (ECA) that can recognize human behavior during open-ended\nconversations and automatically align its responses to the visual and\nconversational style of the other party. The embodied conversational agent\nleverages multimodal inputs to produce rich and perceptually valid vocal and\nfacial responses (e.g., lip syncing and expressions) during the conversation.\nBased on empirical results from a set of user studies, we highlight several\nsignificant challenges in building such systems and provide design guidelines\nfor multi-turn dialogue interactions using style adaptation for future\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:58:36 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Aneja", "Deepali", ""], ["Hoegen", "Rens", ""], ["McDuff", "Daniel", ""], ["Czerwinski", "Mary", ""]]}, {"id": "1910.07563", "submitter": "Alun Preece", "authors": "Alun Preece, Dave Braines, Federico Cerutti, Tien Pham", "title": "Explainable AI for Intelligence Augmentation in Multi-Domain Operations", "comments": "Presented at AAAI FSS-19: Artificial Intelligence in Government and\n  Public Sector, Arlington, Virginia, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Central to the concept of multi-domain operations (MDO) is the utilization of\nan intelligence, surveillance, and reconnaissance (ISR) network consisting of\noverlapping systems of remote and autonomous sensors, and human intelligence,\ndistributed among multiple partners. Realising this concept requires\nadvancement in both artificial intelligence (AI) for improved distributed data\nanalytics and intelligence augmentation (IA) for improved human-machine\ncognition. The contribution of this paper is threefold: (1) we map the\ncoalition situational understanding (CSU) concept to MDO ISR requirements,\npaying particular attention to the need for assured and explainable AI to allow\nrobust human-machine decision-making where assets are distributed among\nmultiple partners; (2) we present illustrative vignettes for AI and IA in MDO\nISR, including human-machine teaming, dense urban terrain analysis, and\nenhanced asset interoperability; (3) we appraise the state-of-the-art in\nexplainable AI in relation to the vignettes with a focus on human-machine\ncollaboration to achieve more rapid and agile coalition decision-making. The\nunion of these three elements is intended to show the potential value of a CSU\napproach in the context of MDO ISR, grounded in three distinct use cases,\nhighlighting how the need for explainability in the multi-partner coalition\nsetting is key.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 18:23:49 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Preece", "Alun", ""], ["Braines", "Dave", ""], ["Cerutti", "Federico", ""], ["Pham", "Tien", ""]]}, {"id": "1910.07728", "submitter": "Shiwali Mohan", "authors": "Shiwali Mohan", "title": "Exploring the Role of Common Model of Cognition in Designing Adaptive\n  Coaching Interactions for Health Behavior Change", "comments": "Accepted for publication in the ACM Transactions on Interactive\n  Intelligent Systems - https://dl.acm.org/journal/tiis", "journal-ref": "ACM Transactions of Interactive Intelligent Syststems 11, 1,\n  Article 1 (April 2021), 30 pages", "doi": "10.1145/3375790", "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research aims to develop intelligent collaborative agents that are\nhuman-aware - they can model, learn, and reason about their human partner's\nphysiological, cognitive, and affective states. In this paper, we study how\nadaptive coaching interactions can be designed to help people develop\nsustainable healthy behaviors. We leverage the common model of cognition - CMC\n[26] - as a framework for unifying several behavior change theories that are\nknown to be useful in human-human coaching. We motivate a set of interactive\nsystem desiderata based on the CMC-based view of behavior change. Then, we\npropose PARCoach - an interactive system that addresses the desiderata.\nPARCoach helps a trainee pick a relevant health goal, set an implementation\nintention, and track their behavior. During this process, the trainee\nidentifies a specific goal-directed behavior as well as the situational context\nin which they will perform it. PARCcoach uses this information to send\nnotifications to the trainee, reminding them of their chosen behavior and the\ncontext. We report the results from a 4-week deployment with 60 participants.\nOur results support the CMC-based view of behavior change and demonstrate that\nthe desiderata for proposed interactive system design is useful in producing\nbehavior change.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 06:18:37 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 20:23:14 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 00:40:35 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Mohan", "Shiwali", ""]]}, {"id": "1910.07747", "submitter": "Eunjin Jeon", "authors": "Eunjin Jeon, Wonjun Ko, Jee Seok Yoon, Heung-Il Suk", "title": "Mutual Information-driven Subject-invariant and Class-relevant Deep\n  Representation Learning in BCI", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning-based feature representation methods have\nshown a promising impact in electroencephalography (EEG)-based brain-computer\ninterface (BCI). Nonetheless, owing to high intra- and inter-subject\nvariabilities, many studies on decoding EEG were designed in a subject-specific\nmanner by using calibration samples, with no concern of its practical use,\nhampered by time-consuming steps and a large data requirement. To this end,\nrecent studies adopted a transfer learning strategy, especially domain\nadaptation techniques. Among those, to our knowledge, an adversarial learning\nhas shown its potential in BCIs. In the meantime, it is known that adversarial\nlearning-based domain adaptation methods are prone to negative transfer that\ndisrupts learning generalized feature representations, applicable to diverse\ndomains, e.g., subjects or sessions in BCIs. In this paper, we propose a novel\nframework that learns class-relevant and subject-invariant feature\nrepresentations in an information-theoretic manner, without using adversarial\nlearning. To be specific, we devise two operational components in a deep\nnetwork that explicitly estimate mutual information between feature\nrepresentations; (1) to decompose features in an intermediate layer into\nclass-relevant and class-irrelevant ones, (2) to enrich class-discriminative\nfeature representation. On two large EEG datasets, we validated the\neffectiveness of our proposed framework by comparing with several comparative\nmethods in performance. Further, we conducted rigorous analyses by performing\nan ablation study in regard to the components in our network, explaining our\nmodel's decision on input EEG signals via layer-wise relevance propagation, and\nvisualizing the distribution of learned features via t-SNE.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 07:32:40 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 09:24:45 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 01:27:47 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 06:32:26 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Jeon", "Eunjin", ""], ["Ko", "Wonjun", ""], ["Yoon", "Jee Seok", ""], ["Suk", "Heung-Il", ""]]}, {"id": "1910.07784", "submitter": "Deepanwita Datta", "authors": "Deepanwita Datta", "title": "Indoor Information Retrieval using Lifelog Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying human behaviour through lifelogging has seen an increase in\nattention from researchers over the past decade. The opportunities that\nlifelogging offers are based on the fact that a lifelog, as a \"black box\" of\nour lives, offers rich contextual information, which has been an Achilles heel\nof information discovery. While lifelog data has been put to use in various\ncontexts, its application to indoor environment scenario remains unexplored. In\nthis proposal, I plan to design a method that enables us to capture and record\nindoor lifelog data of a person's life in order to facilitate healthcare\nsystems, emergency response, item tracking etc. To this end, we aim to build an\nIndoor Information Retrieval system that can be queried with natural language\nqueries over lifelog data. Judicious use of the lifelog data for the indoor\napplication may enable us to solve very fundamental but non-avoidable problems\nof our daily life. Analysis of lifelog data coupled with Information Retrieval\nis not only a promising research topic, but the possibility of its indoor\napplication especially for healthcare, lost-item tracking would be an\ninnovative research idea to the best of our knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:28:39 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Datta", "Deepanwita", ""]]}, {"id": "1910.07799", "submitter": "Guangping Li", "authors": "Fabian Klute, Guangping Li, Raphael L\\\"offler, Martin N\\\"ollenburg,\n  Manuela Schmidt", "title": "Exploring Semi-Automatic Map Labeling", "comments": "Extended version of a paper appearing in SIGSPATIAL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label placement in maps is a very challenging task that is critical for the\noverall map quality. Most previous work focused on designing and implementing\nfully automatic solutions, but the resulting visual and aesthetic quality has\nnot reached the same level of sophistication that skilled human cartographers\nachieve. We investigate a different strategy that combines the strengths of\nhumans and algorithms. In our proposed method, first an initial labeling is\ncomputed that has many well-placed labels but is not claiming to be perfect.\nInstead it serves as a starting point for an expert user who can then\ninteractively and locally modify the labeling where necessary. In an iterative\nhuman-in-the-loop process alternating between user modifications and local\nalgorithmic updates and refinements the labeling can be tuned to the user's\nneeds. We demonstrate our approach by performing different possible\nmodification steps in a sample workflow with a prototypical interactive\nlabeling editor. Further, we report computational performance results from a\nsimulation experiment in QGIS, which investigates the differences between exact\nand heuristic algorithms for semi-automatic map labeling. To that end, we\ncompare several alternatives for recomputing the labeling after local\nmodifications and updates, as a major ingredient for an interactive labeling\neditor.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:57:45 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Klute", "Fabian", ""], ["Li", "Guangping", ""], ["L\u00f6ffler", "Raphael", ""], ["N\u00f6llenburg", "Martin", ""], ["Schmidt", "Manuela", ""]]}, {"id": "1910.07809", "submitter": "Yustinus Soelistio Eko", "authors": "Yustinus Eko Soelistio", "title": "Do you see what I see? Taking perspective of others using facial images", "comments": "6 pages, 3 figures, In 2018 4th International Conference on Science\n  and Technology (ICST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Albeit many HCI / emotion recognition studies use facial expressive images,\nfew scrutinize the accuracies of the people (experimenters and participants) in\nperceiving the expressions representing the intended emotions. The\nmisinterpretation of the expression will put bias in the data and introduce\nquestions on the validity of the studies. The possibility of misinterpretation\nof the expressions will be the focus of the experiment conducted in this study.\nThe experiment will evaluate the ability of people in taking the perspective of\nothers in spite of their current emotions and gender, and whether the\nexpressions can be universally perceived. This study find that it is relatively\nsafe to use facial expressive images for research as long as the emotions are\nexclusively within the six basic emotions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 10:18:42 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Soelistio", "Yustinus Eko", ""]]}, {"id": "1910.08006", "submitter": "Lilac Atassi", "authors": "Lilac Atassi", "title": "Body as controller", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the process of developing a new digital music interface, the author faced\nthree questions that have attracted little to no attention in the literature.\nBy tracking body joints, a performer can use body parts to directly control a\ndigital music instrument. An immediate question that follows asks which limb(s)\nis more effective for the instrument. The next question asks that movement\nshould be measured relative to a particular reference point. And the last\nquestion asks about the mathematical form of the mapping function from the\nmovement feature to the sound parameters. This paper attempts to discuss why\nfinding an answer to these questions is worthwhile and to provide possible\nsolutions that require further investigation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:09:03 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Atassi", "Lilac", ""]]}, {"id": "1910.08534", "submitter": "Vivian Lai", "authors": "Vivian Lai, Jon Z. Cai, Chenhao Tan", "title": "Many Faces of Feature Importance: Comparing Built-in and Post-hoc\n  Feature Importance in Text Classification", "comments": "17 pages, 18 figures, EMNLP 2019, the code is available at\n  https://vivlai.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature importance is commonly used to explain machine predictions. While\nfeature importance can be derived from a machine learning model with a variety\nof methods, the consistency of feature importance via different methods remains\nunderstudied. In this work, we systematically compare feature importance from\nbuilt-in mechanisms in a model such as attention values and post-hoc methods\nthat approximate model behavior such as LIME. Using text classification as a\ntestbed, we find that 1) no matter which method we use, important features from\ntraditional models such as SVM and XGBoost are more similar with each other,\nthan with deep learning models; 2) post-hoc methods tend to generate more\nsimilar important features for two models than built-in methods. We further\ndemonstrate how such similarity varies across instances. Notably, important\nfeatures do not always resemble each other better when two models agree on the\npredicted label than when they disagree.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:59:59 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Lai", "Vivian", ""], ["Cai", "Jon Z.", ""], ["Tan", "Chenhao", ""]]}, {"id": "1910.08549", "submitter": "Achim Rettinger", "authors": "Achim Rettinger, Viktoria Bogdanova, Philipp Niemann", "title": "Towards Learning Cross-Modal Perception-Trace Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is a key element of state-of-the-art deep learning\napproaches. It enables to transform raw data into structured vector space\nembeddings. Such embeddings are able to capture the distributional semantics of\ntheir context, e.g. by word windows on natural language sentences, graph walks\non knowledge graphs or convolutions on images. So far, this context is manually\ndefined, resulting in heuristics which are solely optimized for computational\nperformance on certain tasks like link-prediction. However, such heuristic\nmodels of context are fundamentally different to how humans capture\ninformation. For instance, when reading a multi-modal webpage (i) humans do not\nperceive all parts of a document equally: Some words and parts of images are\nskipped, others are revisited several times which makes the perception trace\nhighly non-sequential; (ii) humans construct meaning from a document's content\nby shifting their attention between text and image, among other things, guided\nby layout and design elements. In this paper we empirically investigate the\ndifference between human perception and context heuristics of basic embedding\nmodels. We conduct eye tracking experiments to capture the underlying\ncharacteristics of human perception of media documents containing a mixture of\ntext and images. Based on that, we devise a prototypical computational\nperception-trace model, called CMPM. We evaluate empirically how CMPM can\nimprove a basic skip-gram embedding approach. Our results suggest, that even\nwith a basic human-inspired computational perception model, there is a huge\npotential for improving embeddings since such a model does inherently capture\nmultiple modalities, as well as layout and design elements.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:20:38 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Rettinger", "Achim", ""], ["Bogdanova", "Viktoria", ""], ["Niemann", "Philipp", ""]]}, {"id": "1910.08595", "submitter": "Brett Mullins", "authors": "Brett Mullins", "title": "Identifying the Most Explainable Classifier", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of pointwise coverage to measure the explainability\nproperties of machine learning classifiers. An explanation for a prediction is\na definably simple region of the feature space sharing the same label as the\nprediction, and the coverage of an explanation measures its size or\ngeneralizability. With this notion of explanation, we investigate whether or\nnot there is a natural characterization of the most explainable classifier.\nAccording with our intuitions, we prove that the binary linear classifier is\nuniquely the most explainable classifier up to negligible sets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 19:24:38 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 22:05:02 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Mullins", "Brett", ""]]}, {"id": "1910.08685", "submitter": "Deepali Aneja", "authors": "Deepali Aneja and Wilmot Li", "title": "Real-Time Lip Sync for Live 2D Animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of commercial tools for real-time performance-based 2D\nanimation has enabled 2D characters to appear on live broadcasts and streaming\nplatforms. A key requirement for live animation is fast and accurate lip sync\nthat allows characters to respond naturally to other actors or the audience\nthrough the voice of a human performer. In this work, we present a deep\nlearning based interactive system that automatically generates live lip sync\nfor layered 2D characters using a Long Short Term Memory (LSTM) model. Our\nsystem takes streaming audio as input and produces viseme sequences with less\nthan 200ms of latency (including processing time). Our contributions include\nspecific design decisions for our feature definition and LSTM configuration\nthat provide a small but useful amount of lookahead to produce accurate lip\nsync. We also describe a data augmentation procedure that allows us to achieve\ngood results with a very small amount of hand-animated training data (13-20\nminutes). Extensive human judgement experiments show that our results are\npreferred over several competing methods, including those that only support\noffline (non-live) processing. Video summary and supplementary results at\nGitHub link: https://github.com/deepalianeja/CharacterLipSync2D\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 03:12:26 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Aneja", "Deepali", ""], ["Li", "Wilmot", ""]]}, {"id": "1910.08814", "submitter": "Ting-Hao Huang", "authors": "Patricia Simon, Suchitra Krishnan-Sarin, Ting-Hao 'Kenneth' Huang", "title": "On Using Chatbots to Promote Smoking Cessation Among Adolescents of Low\n  Socioeconomic Status", "comments": "Selected for round-table discussion in Artificial Intelligence and\n  Work: AAAI 2019 Fall Symposium (AAAI FSS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing youth tobacco use is critical for improving child health since\ntobacco use is associated with respiratory problems, and nicotine may interfere\nwith healthy brain development. While tobacco regulation has contributed to\ndeclines in cigarette use among youth, these declines have occurred more\nquickly for youth of high socioeconomic status (SES) compared to youth of low\nSES. A major barrier to smoking cessation for adolescents of low SES is\ncoordination of access and transportation to in-person treatment sessions.\nLow-SES youth may have family obligations that limit their ability to access\nin-person treatment. At the same time, mobile use among adolescents is high:\n85% have smartphones. Additionally, adolescents engage in texting at high\nrates, suggesting that they are well-suited for mobile instant messaging\ninterventions. Mobile interventions have shown promise for youth, but their use\nremains low. Thus, more research is needed to develop effective and engaging\nmobile interventions to increase quit rates. In this paper, we provide a brief\nreview of approaches to adolescent smoking cessation and describe the promise\nof chatbots for smoking cessation.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 18:34:56 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Simon", "Patricia", ""], ["Krishnan-Sarin", "Suchitra", ""], ["Huang", "Ting-Hao 'Kenneth'", ""]]}, {"id": "1910.08865", "submitter": "Yang Wang", "authors": "Yang Wang", "title": "Deck.gl: Large-scale Web-based Visual Analytics Made Easy", "comments": "The IEEE Workshop on Visualization in Practice, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate how deck.gl, an open-source project born out of\ndata-heavy visual analytics applications, has grown into the robust\nvisualization framework it is today. We begin by explaining why we built\nanother data visualization framework in the first place. Then, we summarize our\ndesign goals (distilled from our interactions with users) and discuss how they\nguided the development of the framework's main features. We use two real-world\napplications of deck.gl to showcase how it can be applied to simplify the\ncreation of data-heavy visualizations. We also discuss our lessons learned as\nwe continue to improve the framework for the larger visualization community.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 01:01:42 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Wang", "Yang", ""]]}, {"id": "1910.09137", "submitter": "Qian Yang", "authors": "Qian Yang", "title": "Two Case Studies of Experience Prototyping Machine Learning Systems in\n  the Wild", "comments": "This is an accepted position paper for the ACM CHI'19 Workshop\n  <Emerging Perspectives in Human-Centered Machine Learning>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the course of my Ph.D., I have been designing the user experience\n(UX) of various machine learning (ML) systems. In this workshop, I share two\nprojects as case studies in which people engage with ML in much more\ncomplicated and nuanced ways than the technical HCML work might assume. The\nfirst case study describes how cardiology teams in three hospitals used a\nclinical decision-support system that helps them decide whether and when to\nimplant an artificial heart to a heart failure patient. I demonstrate that\nphysicians cannot draw on their decision-making experience by seeing only\npatient data on paper. They are also confused by some fundamental premises upon\nwhich ML operates. For example, physicians asked: Are ML predictions made based\non clinicians' best efforts? Is it ethical to make decisions based on previous\npatients' collective outcomes? In the second case study, my collaborators and I\ndesigned an intelligent text editor, with the goal of improving authors'\nwriting experience with NLP (Natural Language Processing) technologies. We\nprototyped a number of generative functionalities where the system provides\nphrase-or-sentence-level writing suggestions upon user request. When writing\nwith the prototype, however, authors shared that they need to \"see where the\nsentence is going two paragraphs later\" in order to decide whether the\nsuggestion aligns with their writing; Some even considered adopting machine\nsuggestions as plagiarism, therefore \"is simply wrong\".\n  By sharing these unexpected and intriguing responses from these real-world ML\nusers, I hope to start a discussion about such previously-unknown complexities\nand nuances of -- as the workshop proposal states -- \"putting ML at the service\nof people in a way that is accessible, useful, and trustworthy to all\".\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 03:43:12 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Yang", "Qian", ""]]}, {"id": "1910.09358", "submitter": "Homayun Afrabandpey", "authors": "Homayun Afrabandpey, Tomi Peltola, Juho Piironen, Aki Vehtari and\n  Samuel Kaski", "title": "A Decision-Theoretic Approach for Model Interpretability in Bayesian\n  Framework", "comments": "This version contains more experiments including a comparison with\n  baseline methods from the literature and complemented some of the existing\n  results in the previous version", "journal-ref": "Machine Learning (2020)", "doi": "10.1007/s10994-020-05901-8", "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A salient approach to interpretable machine learning is to restrict modeling\nto simple models. In the Bayesian framework, this can be pursued by restricting\nthe model structure and prior to favor interpretable models. Fundamentally,\nhowever, interpretability is about users' preferences, not the data generation\nmechanism; it is more natural to formulate interpretability as a utility\nfunction. In this work, we propose an interpretability utility, which\nexplicates the trade-off between explanation fidelity and interpretability in\nthe Bayesian framework. The method consists of two steps. First, a reference\nmodel, possibly a black-box Bayesian predictive model which does not compromise\naccuracy, is fitted to the training data. Second, a proxy model from an\ninterpretable model family that best mimics the predictive behaviour of the\nreference model is found by optimizing the interpretability utility function.\nThe approach is model agnostic -- neither the interpretable model nor the\nreference model are restricted to a certain class of models -- and the\noptimization problem can be solved using standard tools. Through experiments on\nreal-word data sets, using decision trees as interpretable models and Bayesian\nadditive regression models as reference models, we show that for the same level\nof interpretability, our approach generates more accurate models than the\nalternative of restricting the prior. We also propose a systematic way to\nmeasure stability of interpretabile models constructed by different\ninterpretability approaches and show that our proposed approach generates more\nstable models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:22:44 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 21:34:35 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Afrabandpey", "Homayun", ""], ["Peltola", "Tomi", ""], ["Piironen", "Juho", ""], ["Vehtari", "Aki", ""], ["Kaski", "Samuel", ""]]}, {"id": "1910.09382", "submitter": "Olivier Pons", "authors": "Jean-Ferdy Susini (CEDRIC), Olivier Pons (CEDRIC), Nolwenn Guedin\n  (FPSE), Catherine Thevenot (UNIL)", "title": "Danse-doigts, a Fine Motor Game", "comments": null, "journal-ref": "Modelling, measurement and control C, AMSE, 2016", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design, implementation and testing of\n\"Danse-doigts\", an edutainment therapeutic application for hemiplegic children.\nThe objective of this program is twofold. Firstly, to allow them to train their\nfine motor skills on tablet. Secondly, to study the effect of this training on\ntheir numerical performance (counting, calculation...). The target population\nand the objective of evaluating numerical skills influenced the design. The\nsoftware was developed using standard web technologies but is based on a new\nparallel programming library written in JavaScript. Applications and libraries\nare free of charge and easy to install on most tablets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:56:08 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Susini", "Jean-Ferdy", "", "CEDRIC"], ["Pons", "Olivier", "", "CEDRIC"], ["Guedin", "Nolwenn", "", "FPSE"], ["Thevenot", "Catherine", "", "UNIL"]]}, {"id": "1910.09477", "submitter": "Romain Bourqui", "authors": "L. Giovannangeli and R. Bourqui and R. Giot and D. Auber", "title": "Toward automatic comparison of visualization techniques: Application to\n  graph visualization", "comments": "35 pages, 6 figures, 4 tables", "journal-ref": "Visual Informatics (2020)", "doi": "10.1016/j.visinf.2020.04.002", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many end-user evaluations of data visualization techniques have been run\nduring the last decades. Their results are cornerstones to build efficient\nvisualization systems. However, designing such an evaluation is always complex\nand time-consuming and may end in a lack of statistical evidence and\nreproducibility. We believe that modern and efficient computer vision\ntechniques, such as deep convolutional neural networks (CNNs), may help\nvisualization researchers to build and/or adjust their evaluation hypothesis.\nThe basis of our idea is to train machine learning models on several\nvisualization techniques to solve a specific task. Our assumption is that it is\npossible to compare the efficiency of visualization techniques based on the\nperformance of their corresponding model. As current machine learning models\nare not able to strictly reflect human capabilities, including their\nimperfections, such results should be interpreted with caution. However, we\nthink that using machine learning-based pre-evaluation, as a pre-process of\nstandard user evaluations, should help researchers to perform a more exhaustive\nstudy of their design space. Thus, it should improve their final user\nevaluation by providing it better test cases. In this paper, we present the\nresults of two experiments we have conducted to assess how correlated the\nperformance of users and computer vision techniques can be. That study compares\ntwo mainstream graph visualization techniques: node-link (\\NL) and\nadjacency-matrix (\\MD) diagrams. Using two well-known deep convolutional neural\nnetworks, we partially reproduced user evaluations from Ghoniem \\textit{et al.}\nand from Okoe \\textit{et al.}. These experiments showed that some user\nevaluation results can be reproduced automatically.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:09:32 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 14:48:07 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Giovannangeli", "L.", ""], ["Bourqui", "R.", ""], ["Giot", "R.", ""], ["Auber", "D.", ""]]}, {"id": "1910.09621", "submitter": "Ting-Hao Huang", "authors": "Ting-Hao 'Kenneth' Huang", "title": "On Automating Conversations", "comments": "An invited position paper at the \"Artificial Intelligence and Work:\n  AAAI 2019 Fall Symposium\" (AAAI-FSS 2019), Washington, DC, November 7-9, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From 2016 to 2018, we developed and deployed Chorus, a system that blends\nreal-time human computation with artificial intelligence (AI) and has\nreal-world, open conversations with users. We took a top-down approach that\nstarted with a working crowd-powered system, Chorus, and then created a\nframework, Evorus, that enables Chorus to automate itself over time. Over our\ntwo-year deployment, more than 420 users talked with Chorus, having over 2,200\nconversation sessions. This line of work demonstrated how a crowd-powered\nconversational assistant can be automated over time, and more importantly, how\nsuch a system can be deployed to talk with real users to help them with their\neveryday tasks. This position paper discusses two sets of challenges that we\nexplored during the development and deployment of Chorus and Evorus: the\nchallenges that come from being an \"agent\" and those that arise from the subset\nof conversations that are more difficult to automate.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 19:29:06 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 17:59:09 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 20:51:33 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Huang", "Ting-Hao 'Kenneth'", ""]]}, {"id": "1910.09636", "submitter": "Karin De Langis", "authors": "Karin de Langis and Junaed Sattar", "title": "Real-Time Multi-Diver Tracking and Re-identification for Underwater\n  Human-Robot Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous underwater robots working with teams of human divers may need to\ndistinguish between different divers, e.g. to recognize a lead diver or to\nfollow a specific team member. This paper describes a technique that enables\nautonomous underwater robots to track divers in real time as well as to\nreidentify them. The approach is an extension of Simple Online Realtime\nTracking (SORT) with an appearance metric (deep SORT). Initial diver detection\nis performed with a custom CNN designed for realtime diver detection, and\nappearance features are subsequently extracted for each detected diver. Next,\nrealtime tracking-by-detection is performed with an extension of the deep SORT\nalgorithm. We evaluate this technique on a series of videos of divers\nperforming human-robot collaborative tasks and show that our methods result in\nmore divers being accurately identified during tracking. We also discuss the\npractical considerations of applying multi-person tracking to on-board\nautonomous robot operations, and we consider how failure cases can be addressed\nduring on-board tracking.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 20:26:20 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["de Langis", "Karin", ""], ["Sattar", "Junaed", ""]]}, {"id": "1910.09719", "submitter": "Boonserm Kijsirikul", "authors": "Panayu Keelawat, Nattapong Thammasan, Masayuki Numao, and Boonserm\n  Kijsirikul", "title": "Spatiotemporal Emotion Recognition using Deep CNN Based on EEG during\n  Music Listening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition based on EEG has become an active research area. As one\nof the machine learning models, CNN has been utilized to solve diverse problems\nincluding issues in this domain. In this work, a study of CNN and its\nspatiotemporal feature extraction has been conducted in order to explore\ncapabilities of the model in varied window sizes and electrode orders. Our\ninvestigation was conducted in subject-independent fashion. Results have shown\nthat temporal information in distinct window sizes significantly affects\nrecognition performance in both 10-fold and leave-one-subject-out cross\nvalidation. Spatial information from varying electrode order has modicum effect\non classification. SVM classifier depending on spatiotemporal knowledge on the\nsame dataset was previously employed and compared to these empirical results.\nEven though CNN and SVM have a homologous trend in window size effect, CNN\noutperformed SVM using leave-one-subject-out cross validation. This could be\ncaused by different extracted features in the elicitation process.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 01:30:47 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Keelawat", "Panayu", ""], ["Thammasan", "Nattapong", ""], ["Numao", "Masayuki", ""], ["Kijsirikul", "Boonserm", ""]]}, {"id": "1910.09725", "submitter": "Melanie Bancilhon", "authors": "Melanie Bancilhon, Zhengliang Liu, Alvitta Ottley", "title": "Let's Gamble: Uncovering the Impact of Visualization on Risk Perception\n  and Decision-Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualizations are standard tools for assessing and communicating risks.\nHowever, it is not always clear which designs are optimal or how encoding\nchoices might influence risk perception and decision-making. In this paper, we\nreport the findings of a large-scale gambling game that immersed participants\nin an environment where their actions impacted their bonuses. Participants\nchose to either enter a draw or receive guaranteed monetary gains based on five\ncommon visualization designs. By measuring risk perception and observing\ndecision-making, we showed that icon arrays tended to elicit economically sound\nbehavior. We also found that people were more likely to gamble when presented\narea proportioned triangle and circle designs. Using our results, we model risk\nperception and decisions for each visualization and provide a ranking to\nimprove visualization selection.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 02:06:43 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Bancilhon", "Melanie", ""], ["Liu", "Zhengliang", ""], ["Ottley", "Alvitta", ""]]}, {"id": "1910.09800", "submitter": "Pranay Seshadri", "authors": "Slawomir Konrad Tadeja, Pranay Seshadri, Per Ola Kristensson", "title": "AeroVR: Immersive Visualization System for Aerospace Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of today's most propitious immersive technologies is virtual reality\n(VR). This term is colloquially associated with headsets that transport users\nto a bespoke, built-for-purpose immersive 3D virtual environment. It has given\nrise to the field of immersive analytics---a new field of research that aims to\nuse immersive technologies for enhancing and empowering data analytics.\nHowever, in developing such a new set of tools, one has to ask whether the move\nfrom standard hardware setup to a fully immersive 3D environment is\njustified---both in terms of efficiency and development costs. To this end, in\nthis paper, we present the AeroVR--an immersive aerospace design environment\nwith the objective of aiding the component aerodynamic design process by\ninteractively visualizing performance and geometry. We decompose the design of\nsuch an environment into function structures, identify the primary and\nsecondary tasks, present an implementation of the system, and verify the\ninterface in terms of usability and expressiveness. We deploy AeroVR on a\nprototypical design study of a compressor blade for an engine.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 07:22:29 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Tadeja", "Slawomir Konrad", ""], ["Seshadri", "Pranay", ""], ["Kristensson", "Per Ola", ""]]}, {"id": "1910.09895", "submitter": "Quang Vinh Dang", "authors": "Claudia-Lavinia Ignat (COAST), Quang-Vinh Dang (COAST), Valerie Shalin", "title": "The Influence of Trust Score on Cooperative Behavior", "comments": null, "journal-ref": "ACM Transactions on Internet Technology, Association for Computing\n  Machinery, 2019, 19 (4), pp.1-22", "doi": "10.1145/3329250", "report-no": null, "categories": "cs.GT cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assessment of trust between users is essential for collaboration. General\nreputation and ID mechanisms may support users' trust assessment. However,\nthese mechanisms lack sensitivity to pairwise interactions and specific\nexperience such as betrayal over time. Moreover, they place an interpretation\nburden that does not scale to dynamic, large-scale systems. While several\npairwise trust mechanisms have been proposed, no empirical research examines\ntrust score influence on participant behavior. We study the influence of\nshowing a partner trust score and/or ID on participants' behavior in a\nsmall-group collaborative laboratory experiment based on the trust game. We\nshow that trust score availability has the same effect as an ID to improve\ncooperation as measured by sending behavior and receiver response. Excellent\nmodels based on the trust score predict sender behavior and document\nparticipant sensitivity to the provision of partner information. Models based\non the trust score for recipient behavior have some predictive ability\nregarding trustworthiness, but suggest the need for more complex functions\nrelating experience to participant response. We conclude that the parameters of\na trust score, including pairwise interactions and betrayal, influence the\ndifferent roles of participants in the trust game differently, but complement\ntraditional ID and have the advantage of scalability.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 11:17:50 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Ignat", "Claudia-Lavinia", "", "COAST"], ["Dang", "Quang-Vinh", "", "COAST"], ["Shalin", "Valerie", ""]]}, {"id": "1910.09911", "submitter": "Nada Alhirabi Mrs", "authors": "Nada Alhirabi, Omer Rana, Charith Perera", "title": "Designing Security and Privacy Requirements in Internet of Things: A\n  Survey", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design and development process for the Internet of Things (IoT)\napplications is more complicated than that for desktop, mobile, or web\napplications. First, IoT applications require both software and hardware to\nwork together across different nodes with different capabilities under\ndifferent conditions. Secondly, IoT application development involves different\nsoftware engineers such as desktop, web, embedded and mobile to cooperate. In\naddition, the development process required different software\\hardware stacks\nto integrated together. Due to above complexities, more often non-functional\nrequirements (such as security and privacy) tend to get ignored in IoT\napplication development process.\n  In this paper, we have reviewed techniques, methods and tools that are being\ndeveloped to support incorporating security and privacy requirements into\ntraditional application designs. By doing so, we aim to explore how those\ntechniques could be applicable to the IoT domain.\n  In this paper, we primarily focused on two different aspects: (1) design\nnotations, models, and languages that facilitate capturing non-functional\nrequirements (i.e., security and privacy), and (2) proactive and reactive\ninteraction techniques that can be used to support and augment the IoT\napplication design process. Our goal is not only to analyse past research work\nbut also to discuss their applicability towards the IoT.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 12:04:07 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Alhirabi", "Nada", ""], ["Rana", "Omer", ""], ["Perera", "Charith", ""]]}, {"id": "1910.10019", "submitter": "Paul Zikas", "authors": "St\\'ephanie Bertrand, Martha Vassiliadi, Paul Zikas, Efstratios\n  Geronikolakis, George Papagiannakis", "title": "From Readership to Usership and Education, Entertainment, Consumption to\n  Valuation: Embodiment and Aesthetic Experience in Literature-based MR\n  Presence", "comments": "This is a preprint of a chapter for a planned book that was initiated\n  by \"Virtual Multimodal Museum\" and that is expected to be published by\n  Springer. The final book chapter will differ from this preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter will extend its preliminary scope by examining how literary\ntransportation further amplifies presence and affects user response vis-\\'a-vis\nvirtual heritage by focusing on embodiment and aesthetic experience. To do so,\nit will draw on recent findings emerging from the fields of applied psychology,\nneuroaesthetics and cognitive literary studies; and consider a case study\nadvancing the use of literary travel narratives in the design of DCH\napplications for Antiquities - in this case the well-known ancient Greek\nmonument of Acropolis. Subsequently, the chapter will discuss how\nLiterary-based MR Presence shifts public reception from an\neducation-entertainment-touristic consumption paradigm to a response predicated\non valuation. It will show that this type of public engagement is more closely\naligned both with MR applications' default mode of usership, and with newly\nemerging conceptions of a user-centered museum (e.g., the Museum 3.0), thus\nproviding a Virtual Museum model expressly suited to cultural heritage.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 11:55:49 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Bertrand", "St\u00e9phanie", ""], ["Vassiliadi", "Martha", ""], ["Zikas", "Paul", ""], ["Geronikolakis", "Efstratios", ""], ["Papagiannakis", "George", ""]]}, {"id": "1910.10140", "submitter": "Naveen Madapana", "authors": "Naveen Madapana, Glebys Gonzalez and Juan Wachs", "title": "Gesture Agreement Assessment Using Description Vectors", "comments": "5 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Participatory design is a popular design technique that involves the end\nusers in the early stages of the design process to obtain user-friendly\ngestural interfaces. Guessability studies followed by agreement analyses are\noften used to elicit and comprehend the preferences (or gestures/proposals) of\nthe participants. Previous approaches to assess agreement, grouped the gestures\ninto equivalence classes and ignored the integral properties that are shared\nbetween them. In this work, we represent the gestures using binary description\nvectors to allow them to be partially similar. In this context, we introduce a\nnew metric referred to as soft agreement rate (SAR) to quantify the level of\nconsensus between the participants. In addition, we performed computational\nexperiments to study the behavior of our partial agreement formula and\nmathematically show that existing agreement metrics are a special case of our\napproach. Our methodology was evaluated through a gesture elicitation study\nconducted with a group of neurosurgeons. Nevertheless, our formulation can be\napplied to any other user-elicitation study. Results show that the level of\nagreement obtained by SAR metric is 2.64 times higher than the existing\nmetrics. In addition to the mostly agreed gesture, SAR formulation also\nprovides the mostly agreed descriptors which can potentially help the designers\nto come up with a final gesture set.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 17:50:35 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Madapana", "Naveen", ""], ["Gonzalez", "Glebys", ""], ["Wachs", "Juan", ""]]}, {"id": "1910.10251", "submitter": "Ali Ayub", "authors": "Ali Ayub, Aldo Morales and Amit Banerjee", "title": "Using Markov Decision Process to Model Deception for Robotic and\n  Interactive Game Applications", "comments": "Accepted at IEEE International Conference on Consumer Electronics\n  (ICCE) 2021", "journal-ref": "2021 IEEE International Conference on Consumer Electronics (ICCE)", "doi": "10.1109/ICCE50685.2021.9427633", "report-no": null, "categories": "cs.HC cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates deception in the context of motion using a simulated\nmobile robot. We analyze some previously designed deceptive strategies on a\nmobile robot simulator. We then present a novel approach to adaptively choose\ntarget-oriented deceptive trajectories to deceive humans for multiple\ninteractions. Additionally, we propose a new metric to evaluate deception on\ndata collected from the users when interacting with the mobile robot simulator.\nWe performed a user study to test our proposed adaptive deceptive algorithm,\nwhich shows that our algorithm deceives humans even for multiple interactions\nand it is more effective than random choice of deceptive strategies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 22:13:25 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 01:12:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ayub", "Ali", ""], ["Morales", "Aldo", ""], ["Banerjee", "Amit", ""]]}, {"id": "1910.10335", "submitter": "Amila Silva", "authors": "Amila Silva, Shanika Karunasekera, Christopher Leckie and Ling Luo", "title": "USTAR: Online Multimodal Embedding for Modeling User-Guided\n  Spatiotemporal Activity", "comments": "10 pages, IEEE International Conference on Big Data 2019 (IEEE Big\n  Data 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building spatiotemporal activity models for people's activities in urban\nspaces is important for understanding the ever-increasing complexity of urban\ndynamics. With the emergence of Geo-Tagged Social Media (GTSM) records,\nprevious studies demonstrate the potential of GTSM records for spatiotemporal\nactivity modeling. State-of-the-art methods for this task embed different\nmodalities (location, time, and text) of GTSM records into a single embedding\nspace. However, they ignore Non-GeoTagged Social Media (NGTSM) records, which\ngenerally account for the majority of posts (e.g., more than 95\\% in Twitter),\nand could represent a great source of information to alleviate the sparsity of\nGTSM records. Furthermore, in the current spatiotemporal embedding techniques,\nless focus has been given to the users, who exhibit spatially motivated\nbehaviors. To bridge this research gap, this work proposes USTAR, a novel\nonline learning method for User-guided SpatioTemporal Activity Representation,\nwhich (1) embeds locations, time, and text along with users into the same\nembedding space to capture their correlations; (2) uses a novel collaborative\nfiltering approach based on two different empirically studied user behaviors to\nincorporate both NGTSM and GTSM records in learning; and (3) introduces a novel\nsampling technique to learn spatiotemporal representations in an online fashion\nto accommodate recent information into the embedding space, while avoiding\noverfitting to recent records and frequently appearing units in social media\nstreams. Our results show that USTAR substantially improves the\nstate-of-the-art for region retrieval and keyword retrieval and its potential\nto be applied to other downstream applications such as local event detection.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 04:02:20 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Silva", "Amila", ""], ["Karunasekera", "Shanika", ""], ["Leckie", "Christopher", ""], ["Luo", "Ling", ""]]}, {"id": "1910.10481", "submitter": "Christian Huyck", "authors": "Dan Diaper and Chris Huyck", "title": "The Task Analysis Cell Assembly Perspective", "comments": "39 Pages; 94 with two appendices; third appendix is a linked dynamic\n  file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An entirely novel synthesis combines the applied cognitive psychology of a\ntask analytic approach with a neural cell assembly perspective that models both\nbrain and mind function during task performance; similar cell assemblies could\nbe implemented as an artificially intelligent neural network. A simplified cell\nassembly model is introduced and this leads to several new representational\nformats that, in combination, are demonstrated as suitable for analysing tasks.\nThe advantages of using neural models are exposed and compared with previous\nresearch that has used symbolic artificial intelligence production systems,\nwhich make no attempt to model neurophysiology. For cognitive scientists, the\napproach provides an easy and practical introduction to thinking about brains,\nminds and artificial intelligence in terms of cell assemblies. In the future,\nsubsequent developments have the potential to lead to a new, general theory of\npsychology and neurophysiology, supported by cell assembly based artificial\nintelligences.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 11:51:13 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 08:24:20 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Diaper", "Dan", ""], ["Huyck", "Chris", ""]]}, {"id": "1910.10489", "submitter": "Mohsen Annabestani", "authors": "Fatemeh Hasanzadeh, Mohsen Annabestani, Sahar Moghimi", "title": "Continuous Emotion Recognition during Music Listening Using EEG Signals:\n  A Fuzzy Parallel Cascades Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A controversial issue in artificial intelligence is human emotion\nrecognition. This paper presents a fuzzy parallel cascades (FPC) model for\npredicting the continuous subjective appraisal of the emotional content of\nmusic by time-varying spectral content of EEG signals. The EEG, along with an\nemotional appraisal of 15 subjects, was recorded during listening to seven\nmusical excerpts. The emotional appraisement was recorded along the valence and\narousal emotional axes as a continuous signal. The FPC model was composed of\nparallel cascades with each cascade containing a fuzzy logic-based system. The\nFPC model performance was evaluated by comparing with linear regression (LR),\nsupport vector regression (SVR) and Long Short Term Memory recurrent neural\nnetwork (LSTM RNN) models. The RMSE of the FPC was lower than other models for\nthe estimation of both valence and arousal of all musical excerpts. The lowest\nRMSE was 0.089 which was obtained in estimation of the valence of MS4 by the\nFPC model. The analysis of MI of frontal EEG with the valence confirms the role\nof frontal channels in theta frequency band in emotion recognition. Considering\nthe dynamic variations of musical features during songs, employing a modeling\napproach to predict dynamic variations of the emotional appraisal can be a\nplausible substitute for the classification of musical excerpts into predefined\nlabels.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 10:43:05 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Hasanzadeh", "Fatemeh", ""], ["Annabestani", "Mohsen", ""], ["Moghimi", "Sahar", ""]]}, {"id": "1910.10758", "submitter": "Arijit Das", "authors": "Arijit Das, Jaydeep Mandal, Zargham Danial, Alok Ranjan Pal, Diganta\n  Saha", "title": "A Novel Approach for Automatic Bengali Question Answering System using\n  Semantic Similarity Analysis", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the semantically accurate answer is one of the key challenges in\nadvanced searching. In contrast to keyword-based searching, the meaning of a\nquestion or query is important here and answers are ranked according to\nrelevance. It is very natural that there is almost no common word between the\nquestion sentence and the answer sentence. In this paper, an approach is\ndescribed to find out the semantically relevant answers in the Bengali dataset.\nIn the first part of the algorithm, a set of statistical parameters like\nfrequency, index, part-of-speech (POS), etc. is matched between a question and\nthe probable answers. In the second phase, entropy and similarity are\ncalculated in different modules. Finally, a sense score is generated to rank\nthe answers. The algorithm is tested on a repository containing a total of\n275000 sentences. This Bengali repository is a product of Technology\nDevelopment for Indian Languages (TDIL) project sponsored by Govt. of India and\nprovided by the Language Research Unit of Indian Statistical Institute,\nKolkata. The shallow parser, developed by the LTRC group of IIIT Hyderabad is\nused for POS tagging. The actual answer is ranked as 1st in 82.3% cases. The\nactual answer is ranked within 1st to 5th in 90.0% cases. The accuracy of the\nsystem is coming as 97.32% and precision of the system is coming as 98.14%\nusing confusion matrix. The challenges and pitfalls of the work are reported at\nlast in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 18:24:55 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Das", "Arijit", ""], ["Mandal", "Jaydeep", ""], ["Danial", "Zargham", ""], ["Pal", "Alok Ranjan", ""], ["Saha", "Diganta", ""]]}, {"id": "1910.10854", "submitter": "Ursula Laa", "authors": "Ursula Laa, Dianne Cook, German Valencia", "title": "A slice tour for finding hollowness in high-dimensional data", "comments": "13 pages, 6 figures", "journal-ref": "Journal of Computational and Graphical Statistics 29 (2020)\n  681-687", "doi": "10.1080/10618600.2020.1777140", "report-no": null, "categories": "stat.CO cs.HC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking projections of high-dimensional data is a common analytical and\nvisualisation technique in statistics for working with high-dimensional\nproblems. Sectioning, or slicing, through high dimensions is less common, but\ncan be useful for visualising data with concavities, or non-linear structure.\nIt is associated with conditional distributions in statistics, and also linked\nbrushing between plots in interactive data visualisation. This short technical\nnote describes a simple approach for slicing in the orthogonal space of\nprojections obtained when running a tour, thus presenting the viewer with an\ninterpolated sequence of sliced projections. The method has been implemented in\nR as an extension to the tourr package, and can be used to explore for concave\nand non-linear structures in multivariate distributions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:27:27 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Laa", "Ursula", ""], ["Cook", "Dianne", ""], ["Valencia", "German", ""]]}, {"id": "1910.11006", "submitter": "Dongxu Li", "authors": "Dongxu Li, Cristian Rodriguez Opazo, Xin Yu, Hongdong Li", "title": "Word-level Deep Sign Language Recognition from Video: A New Large-scale\n  Dataset and Methods Comparison", "comments": "Accepted by WACV2020, First Round, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based sign language recognition aims at helping deaf people to\ncommunicate with others. However, most existing sign language datasets are\nlimited to a small number of words. Due to the limited vocabulary size, models\nlearned from those datasets cannot be applied in practice. In this paper, we\nintroduce a new large-scale Word-Level American Sign Language (WLASL) video\ndataset, containing more than 2000 words performed by over 100 signers. This\ndataset will be made publicly available to the research community. To our\nknowledge, it is by far the largest public ASL dataset to facilitate word-level\nsign recognition research.\n  Based on this new large-scale dataset, we are able to experiment with several\ndeep learning methods for word-level sign recognition and evaluate their\nperformances in large scale scenarios. Specifically we implement and compare\ntwo different models,i.e., (i) holistic visual appearance-based approach, and\n(ii) 2D human pose based approach. Both models are valuable baselines that will\nbenefit the community for method benchmarking. Moreover, we also propose a\nnovel pose-based temporal graph convolution networks (Pose-TGCN) that models\nspatial and temporal dependencies in human pose trajectories simultaneously,\nwhich has further boosted the performance of the pose-based method. Our results\nshow that pose-based and appearance-based models achieve comparable\nperformances up to 66% at top-10 accuracy on 2,000 words/glosses, demonstrating\nthe validity and challenges of our dataset. Our dataset and baseline deep\nmodels are available at \\url{https://dxli94.github.io/WLASL/}.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 10:04:29 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 00:24:44 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Li", "Dongxu", ""], ["Opazo", "Cristian Rodriguez", ""], ["Yu", "Xin", ""], ["Li", "Hongdong", ""]]}, {"id": "1910.11059", "submitter": "Zhiwei Han", "authors": "Zhiwei Han, Thomas Weber, Stefan Matthes, Yuanting Liu, Hao Shen", "title": "Interactive Image Restoration", "comments": "Human-centric Machine Learning Workshop, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and many of its applications are considered hard to approach\ndue to their complexity and lack of transparency. One mission of human-centric\nmachine learning is to improve algorithm transparency and user satisfaction\nwhile ensuring an acceptable task accuracy. In this work, we present an\ninteractive image restoration framework, which exploits both image prior and\nhuman painting knowledge in an iterative manner such that they can boost on\neach other. Additionally, in this system users can repeatedly get feedback of\ntheir interactions from the restoration progress. This informs the users about\ntheir impact on the restoration results, which leads to better sense of\ncontrol, which can lead to greater trust and approachability. The positive\nresults of both objective and subjective evaluation indicate that, our\ninteractive approach positively contributes to the approachability of\nrestoration algorithms in terms of algorithm performance and user experience.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 12:40:53 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Han", "Zhiwei", ""], ["Weber", "Thomas", ""], ["Matthes", "Stefan", ""], ["Liu", "Yuanting", ""], ["Shen", "Hao", ""]]}, {"id": "1910.11111", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Viktoriia Sharmanska and Stefanos Zafeiriou", "title": "Face Behavior a la carte: Expressions, Affect and Action Units in a\n  Single Network", "comments": "filed as a patent", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic facial behavior analysis has a long history of studies in the\nintersection of computer vision, physiology and psychology. However it is only\nrecently, with the collection of large-scale datasets and powerful machine\nlearning methods such as deep neural networks, that automatic facial behavior\nanalysis started to thrive. Three of its iconic tasks are automatic recognition\nof basic expressions (e.g. happy, sad, surprised), estimation of continuous\nemotions (e.g., valence and arousal), and detection of facial action units\n(activations of e.g. upper/inner eyebrows, nose wrinkles). Up until now these\ntasks have been mostly studied independently collecting a dataset for the task.\nWe present the first and the largest study of all facial behaviour tasks\nlearned jointly in a single multi-task, multi-domain and multi-label network,\nwhich we call FaceBehaviorNet. For this we utilize all publicly available\ndatasets in the community (around 5M images) that study facial behaviour tasks\nin-the-wild. We demonstrate that training jointly an end-to-end network for all\ntasks has consistently better performance than training each of the single-task\nnetworks. Furthermore, we propose two simple strategies for coupling the tasks\nduring training, co-annotation and distribution matching, and show the\nadvantages of this approach. Finally we show that FaceBehaviorNet has learned\nfeatures that encapsulate all aspects of facial behaviour, and can be\nsuccessfully applied to perform tasks (compound emotion recognition) beyond the\nones that it has been trained in a zero- and few-shot learning setting.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:45:41 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 23:35:29 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 02:35:49 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Sharmanska", "Viktoriia", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1910.11386", "submitter": "Shahan Ali Memon", "authors": "Shahan Ali Memon, Hira Dhamyal, Oren Wright, Daniel Justice,\n  Vijaykumar Palat, William Boler, Bhiksha Raj, Rita Singh", "title": "Detecting gender differences in perception of emotion in crowdsourced\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do men and women perceive emotions differently? Popular convictions place\nwomen as more emotionally perceptive than men. Empirical findings, however,\nremain inconclusive. Most prior studies focus on visual modalities. In\naddition, almost all of the studies are limited to experiments within\ncontrolled environments. Generalizability and scalability of these studies has\nnot been sufficiently established. In this paper, we study the differences in\nperception of emotion between genders from speech data in the wild, annotated\nthrough crowdsourcing. While we limit ourselves to a single modality (i.e.\nspeech), our framework is applicable to studies of emotion perception from all\nsuch loosely annotated data in general. Our paper addresses multiple serious\nchallenges related to making statistically viable conclusions from crowdsourced\ndata. Overall, the contributions of this paper are two fold: a reliable novel\nframework for perceptual studies from crowdsourced data; and the demonstration\nof statistically significant differences in speech-based emotion perception\nbetween genders.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 19:13:21 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 18:23:50 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 13:28:06 GMT"}, {"version": "v4", "created": "Mon, 4 Nov 2019 14:33:39 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Memon", "Shahan Ali", ""], ["Dhamyal", "Hira", ""], ["Wright", "Oren", ""], ["Justice", "Daniel", ""], ["Palat", "Vijaykumar", ""], ["Boler", "William", ""], ["Raj", "Bhiksha", ""], ["Singh", "Rita", ""]]}, {"id": "1910.11437", "submitter": "Saptarshi Purkayastha", "authors": "Bhanu Teja Yandrapalli, Josette Jones, Saptarshi Purkayastha", "title": "Development and Implementation of a Dashboard for Diabetes Care\n  Management in OpenMRS", "comments": "MS in HI thesis", "journal-ref": null, "doi": null, "report-no": "Fall 2018", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A clinical dashboard for a patient's diabetes condition helps physicians to\nmake better decisions based on readily available information. OpenMRS is a\nwidely used open-source electronic health records system but does not provide a\ndisease-specific dashboard. This project implemented a dashboard for displaying\nall diabetes-related lab measures at one place, when a physician accesses a\npatient record in OpenMRS. It summarizes a list of diabetes-related clinical\nmeasures through an intuitive, chart-based, customizable user experience. Gauge\ncharts are used to display the most important lab values for Glucose, Renal\nFunction, and Lipid Profile tests. Data Tables are used to display data of the\nlab values from the past and current visit in the table, including the ability\nto search for a specific visit date. Interactive line charts are used to\ndisplay the trends of lab measures. Diabetes Dashboard may help physicians to\nmake quicker decisions through this snapshot view. We took data for a few\npatients and demonstrated this to clinicians as a proof of concept, without\nperforming a full-fledged user evaluation. Future work involves integrating\nthis dashboard with clinical practice guidelines and alerting when measures are\noutside the guidelines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 21:46:25 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Yandrapalli", "Bhanu Teja", ""], ["Jones", "Josette", ""], ["Purkayastha", "Saptarshi", ""]]}, {"id": "1910.11459", "submitter": "Aaron M. Roth", "authors": "Aaron M. Roth, Samantha Reig, Umang Bhatt, Jonathan Shulgach, Tamara\n  Amin, Afsaneh Doryab, Fei Fang, Manuela Veloso", "title": "A Robot's Expressive Language Affects Human Strategy and Perceptions in\n  a Competitive Game", "comments": "RO-MAN 2019; 8 pages, 4 figures, 1 table", "journal-ref": "Proceedings of the 28th IEEE International Conference on Robot\n  Human Interactive Communication, New Delhi, India, October 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots are increasingly endowed with social and communicative\ncapabilities, they will interact with humans in more settings, both\ncollaborative and competitive. We explore human-robot relationships in the\ncontext of a competitive Stackelberg Security Game. We vary humanoid robot\nexpressive language (in the form of \"encouraging\" or \"discouraging\" verbal\ncommentary) and measure the impact on participants' rationality, strategy\nprioritization, mood, and perceptions of the robot. We learn that a robot\nopponent that makes discouraging comments causes a human to play a game less\nrationally and to perceive the robot more negatively. We also contribute a\nsimple open source Natural Language Processing framework for generating\nexpressive sentences, which was used to generate the speech of our autonomous\nsocial robot.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 23:39:44 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Roth", "Aaron M.", ""], ["Reig", "Samantha", ""], ["Bhatt", "Umang", ""], ["Shulgach", "Jonathan", ""], ["Amin", "Tamara", ""], ["Doryab", "Afsaneh", ""], ["Fang", "Fei", ""], ["Veloso", "Manuela", ""]]}, {"id": "1910.11637", "submitter": "Daniele Giunchi", "authors": "Daniele Giunchi, Stuart james, Donald Degraen, Anthony Steed", "title": "Mixing realities for sketch retrieval in Virtual Reality", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing tools for Virtual Reality (VR) enable users to model 3D designs from\nwithin the virtual environment itself. These tools employ sketching and\nsculpting techniques known from desktop-based interfaces and apply them to\nhand-based controller interaction. While these techniques allow for mid-air\nsketching of basic shapes, it remains difficult for users to create detailed\nand comprehensive 3D models. In our work, we focus on supporting the user in\ndesigning the virtual environment around them by enhancing sketch-based\ninterfaces with a supporting system for interactive model retrieval. Through\nsketching, an immersed user can query a database containing detailed 3D models\nand replace them into the virtual environment. To understand supportive\nsketching within a virtual environment, we compare different methods of sketch\ninteraction, i.e., 3D mid-air sketching, 2D sketching on a virtual tablet, 2D\nsketching on a fixed virtual whiteboard, and 2D sketching on a real tablet.\n%using a 2D physical tablet, a 2D virtual tablet, a 2D virtual whiteboard, and\n3D mid-air sketching. Our results show that 3D mid-air sketching is considered\nto be a more intuitive method to search a collection of models while the\naddition of physical devices creates confusion due to the complications of\ntheir inclusion within a virtual environment. While we pose our work as a\nretrieval problem for 3D models of chairs, our results can be extrapolated to\nother sketching tasks for virtual environments.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 11:52:25 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 09:58:24 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Giunchi", "Daniele", ""], ["james", "Stuart", ""], ["Degraen", "Donald", ""], ["Steed", "Anthony", ""]]}, {"id": "1910.11734", "submitter": "Eerik Mantere", "authors": "Eerik Mantere (CED)", "title": "What Smartphones, Ethnomethodology, and Bystander Inaccessibility Can\n  Teach Us About Better Design?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones, the ubiquitous mobile screens now normal parts of everyday\nsocial situations, have created a kind of ongoing natural experiment for social\nscientists. According to Garfinkel's ethnomethodology social action gets its\nmeaning not only from its content but also through its context. Mobility, small\nscreen size, and the habitual way of using smartphones ensure that, while\noffering the biggest variety of activities for the user, in comparison to other\neveryday items, smartphones offer the least cues to bystanders on what the user\nis actually doing and how long it might take. This 'bystander inaccessibility'\nhandicaps shared understanding of the social context that the user and\ncollocated others find themselves in. Added considerations and interactive\neffort in managing the situation is therefore required. Future design needs to\nrelate to this basic building block of collocated interaction to not be met\nwith discontent.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:02:32 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Mantere", "Eerik", "", "CED"]]}, {"id": "1910.12193", "submitter": "Marco Cavallo", "authors": "Marco Cavallo, Mishal Dholakia, Matous Havlena, Kenneth Ocheltree,\n  Mark Podlaseck", "title": "Immersive Insights: A Hybrid Analytics System for Collaborative\n  Exploratory Data Analysis", "comments": "VRST 2019", "journal-ref": null, "doi": "10.1145/3359996.3364242", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, augmented reality (AR) and virtual reality (VR)\ntechnologies have experienced terrific improvements in both accessibility and\nhardware capabilities, encouraging the application of these devices across\nvarious domains. While researchers have demonstrated the possible advantages of\nAR and VR for certain data science tasks, it is still unclear how these\ntechnologies would perform in the context of exploratory data analysis (EDA) at\nlarge. In particular, we believe it is important to better understand which\nlevel of immersion EDA would concretely benefit from, and to quantify the\ncontribution of AR and VR with respect to standard analysis workflows.\n  In this work, we leverage a Dataspace reconfigurable hybrid reality\nenvironment to study how data scientists might perform EDA in a co-located,\ncollaborative context. Specifically, we propose the design and implementation\nof Immersive Insights, a hybrid analytics system combining high-resolution\ndisplays, table projections, and augmented reality (AR) visualizations of the\ndata.\n  We conducted a two-part user study with twelve data scientists, in which we\nevaluated how different levels of data immersion affect the EDA process and\ncompared the performance of Immersive Insights with a state-of-the-art,\nnon-immersive data analysis system.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 06:44:30 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Cavallo", "Marco", ""], ["Dholakia", "Mishal", ""], ["Havlena", "Matous", ""], ["Ocheltree", "Kenneth", ""], ["Podlaseck", "Mark", ""]]}, {"id": "1910.12444", "submitter": "Sarah Preum", "authors": "Sarah Masud Preum, Kate Clark, Ashley Davis, Konstantine\n  Khutsishvilli, Rupa S Valdez", "title": "Information Seeking and Information Processing Behaviors Among Type 2\n  Diabetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective patient education is critical for managing Type 2 Diabetes Mellitus\n(T2DM), one of the most common chronic diseases in the United States. While\nsome studies focus on the information-seeking behavior of T2DM patients, other\nself-education behaviors including information processing and utilization are\nrarely explored in the context of T2DM. This study sought to assess two\nself-education behaviors of type 2 diabetics, namely, information seeking and\ninformation processing, to understand more about how these behaviors affect the\nself-management of this common chronic disease. Semi-structured interviews were\nconducted with 8 English speaking T2DM patients and qualitative content\nanalysis techniques were performed to analyze their responses. The information\nseeking and processing behaviors vary across individuals based on their\nprognosis of T2DM, information needs, and personal preferences. Patients are\noften dissatisfied with information from official sources, have difficulty\nevaluating the trustworthiness of information sources, and desire information\nthat is more personally relevant to them. Several participants identified a\nlack of personalized information as a key factor in the inability to adhere to\nT2DM management guidelines, which led them to experience increased glucose\nlevels, difficulty managing A1C levels, frustration, and anxiety. They\nmentioned that they followed trial and error based approaches to tailor\ninformation according to their needs and physiological conditions. Many\nparticipants identified conflicting or inconsistent information from different\nsources as a major barrier to information processing. The results of this study\nindicate a need for authentic, consistent, and individualized information for\ntype 2 diabetics.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 04:59:10 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Preum", "Sarah Masud", ""], ["Clark", "Kate", ""], ["Davis", "Ashley", ""], ["Khutsishvilli", "Konstantine", ""], ["Valdez", "Rupa S", ""]]}, {"id": "1910.12544", "submitter": "Yi-Ching Huang", "authors": "Yi-Ching Huang and Yu-Ting Cheng and Lin-Lin Chen and Jane Yung-jen\n  Hsu", "title": "Human-AI Co-Learning for Data-Driven AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human and AI are increasingly interacting and collaborating to accomplish\nvarious complex tasks in the context of diverse application domains (e.g.,\nhealthcare, transportation, and creative design). Two dynamic, learning\nentities (AI and human) have distinct mental model, expertise, and ability;\nsuch fundamental difference/mismatch offers opportunities for bringing new\nperspectives to achieve better results. However, this mismatch can cause\nunexpected failure and result in serious consequences. While recent research\nhas paid much attention to enhancing interpretability or explainability to\nallow machine to explain how it makes a decision for supporting humans, this\nresearch argues that there is urging the need for both human and AI should\ndevelop specific, corresponding ability to interact and collaborate with each\nother to form a human-AI team to accomplish superior results. This research\nintroduces a conceptual framework called \"Co-Learning,\" in which people can\nlearn with/from and grow with AI partners over time. We characterize three key\nconcepts of co-learning: \"mutual understanding,\" \"mutual benefits,\" and \"mutual\ngrowth\" for facilitating human-AI collaboration on complex problem solving. We\nwill present proof-of-concepts to investigate whether and how our approach can\nhelp human-AI team to understand and benefit each other, and ultimately improve\nproductivity and creativity on creative problem domains. The insights will\ncontribute to the design of Human-AI collaboration.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 10:40:15 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Huang", "Yi-Ching", ""], ["Cheng", "Yu-Ting", ""], ["Chen", "Lin-Lin", ""], ["Hsu", "Jane Yung-jen", ""]]}, {"id": "1910.12581", "submitter": "Hassan Khosravi", "authors": "Solmaz Abdi, Hassan Khosravi, Shazia Sadiq, Dragan Gasevic", "title": "A Multivariate Elo-based Learner Model for Adaptive Educational Systems", "comments": "Published in the Proceedings of the 12th International Conference on\n  Educational Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Elo rating system has been recognised as an effective method for\nmodelling students and items within adaptive educational systems. The existing\nElo-based models have the limiting assumption that items are only tagged with a\nsingle concept and are mainly studied in the context of adaptive testing\nsystems. In this paper, we introduce a multivariate Elo-based learner model\nthat is suitable for the domains where learning items can be tagged with\nmultiple concepts, and investigate its fit in the context of adaptive learning.\nTo evaluate the model, we first compare the predictive performance of the\nproposed model against the standard Elo-based model using synthetic and public\ndata sets. Our results from this study indicate that our proposed model has\nsuperior predictive performance compared to the standard Elo-based model, but\nthe difference is rather small. We then investigate the fit of the proposed\nmultivariate Elo-based model by integrating it into an adaptive learning system\nwhich incorporates the principles of open learner models (OLMs). The results\nfrom this study suggest that the availability of additional parameters derived\nfrom multivariate Elo-based models have two further advantages: guiding\nadaptive behaviour for the system and providing additional insight for students\nand instructors.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 02:47:44 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Abdi", "Solmaz", ""], ["Khosravi", "Hassan", ""], ["Sadiq", "Shazia", ""], ["Gasevic", "Dragan", ""]]}, {"id": "1910.13028", "submitter": "Heinrich Dinkel", "authors": "Heinrich Dinkel, Pingyue Zhang, Mengyue Wu, Kai Yu", "title": "Depa: Self-supervised audio embedding for depression detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depression detection research has increased over the last few decades as this\ndisease is becoming a socially-centered problem. One major bottleneck for\ndeveloping automatic depression detection methods lies in the limited data\navailability. Recently, pretrained text-embeddings have seen success in sparse\ndata scenarios, while pretrained audio embeddings are rarely investigated. This\npaper proposes DEPA, a self-supervised, Word2Vec like pretrained depression\naudio embedding method for depression detection. An encoder-decoder network is\nused to extract DEPA on sparse-data in-domain (DAIC) and large-data out-domain\n(switchboard, Alzheimer's) datasets. With DEPA as the audio embedding,\nperformance significantly outperforms traditional audio features regarding both\nclassification and regression metrics. Moreover, we show that large-data\nout-domain pretraining is beneficial to depression detection performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 01:11:58 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 05:10:06 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Dinkel", "Heinrich", ""], ["Zhang", "Pingyue", ""], ["Wu", "Mengyue", ""], ["Yu", "Kai", ""]]}, {"id": "1910.13122", "submitter": "Araz Taeihagh", "authors": "Hazel Si Min Lim, and Araz Taeihagh", "title": "Algorithmic decision-making in AVs: Understanding ethical and technical\n  concerns for smart cities", "comments": null, "journal-ref": "Sustainability, 2019, 11(20), 5791", "doi": "10.3390/su11205791", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous Vehicles (AVs) are increasingly embraced around the world to\nadvance smart mobility and more broadly, smart, and sustainable cities.\nAlgorithms form the basis of decision-making in AVs, allowing them to perform\ndriving tasks autonomously, efficiently, and more safely than human drivers and\noffering various economic, social, and environmental benefits. However,\nalgorithmic decision-making in AVs can also introduce new issues that create\nnew safety risks and perpetuate discrimination. We identify bias, ethics, and\nperverse incentives as key ethical issues in the AV algorithms' decision-making\nthat can create new safety risks and discriminatory outcomes. Technical issues\nin the AVs' perception, decision-making and control algorithms, limitations of\nexisting AV testing and verification methods, and cybersecurity vulnerabilities\ncan also undermine the performance of the AV system. This article investigates\nthe ethical and technical concerns surrounding algorithmic decision-making in\nAVs by exploring how driving decisions can perpetuate discrimination and create\nnew safety risks for the public. We discuss steps taken to address these\nissues, highlight the existing research gaps and the need to mitigate these\nissues through the design of AV's algorithms and of policies and regulations to\nfully realise AVs' benefits for smart and sustainable cities.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 07:50:02 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Lim", "Hazel Si Min", ""], ["Taeihagh", "Araz", ""]]}, {"id": "1910.13166", "submitter": "Johanne Trippas R.", "authors": "Johanne R. Trippas, Damiano Spina, Paul Thomas, Mark Sanderson, Hideo\n  Joho, Lawrence Cavedon", "title": "Towards a Model for Spoken Conversational Search", "comments": "Paper accepted at Information Processing & Management on October 29,\n  2019, Spoken Conversational Search, Information Seeking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversation is the natural mode for information exchange in daily life, a\nspoken conversational interaction for search input and output is a logical\nformat for information seeking. However, the conceptualisation of user-system\ninteractions or information exchange in spoken conversational search (SCS) has\nnot been explored. The first step in conceptualising SCS is to understand the\nconversational moves used in an audio-only communication channel for search.\nThis paper explores conversational actions for the task of search. We define a\nqualitative methodology for creating conversational datasets, propose analysis\nprotocols, and develop the SCSdata. Furthermore, we use the SCSdata to create\nthe first annotation schema for SCS: the SCoSAS, enabling us to investigate\ninteractivity in SCS. We further establish that SCS needs to incorporate\ninteractivity and pro-activity to overcome the complexity that the information\nseeking process in an audio-only channel poses. In summary, this exploratory\nstudy unpacks the breadth of SCS. Our results highlight the need for\nintegrating discourse in future SCS models and contributes the advancement in\nthe formalisation of SCS models and the design of SCS systems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 10:22:48 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 01:10:10 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Trippas", "Johanne R.", ""], ["Spina", "Damiano", ""], ["Thomas", "Paul", ""], ["Sanderson", "Mark", ""], ["Joho", "Hideo", ""], ["Cavedon", "Lawrence", ""]]}, {"id": "1910.13175", "submitter": "Eerik Mantere", "authors": "Sanna Raudaskoski, Eerik Mantere (CED), Satu Valkonen", "title": "Smartphone and the changing practices of face-to-face interaction", "comments": "in Finnish", "journal-ref": "Sosiologia, Westermarck-seura, 2019", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphone use has grown rapidly, but the ways it shapes concurrent\nface-to-face interaction remains scarcely studied. In our research we have\nformulated two new concepts to depict this: 1) Sticky media device illustrates\nsituations in which a person using a screen media device is difficult to get\nfully involved with ongoing face-to-face conversation. Their attention is not\neasily removed from the \"sticky\" device or returns to it quickly even if it is\nmomentarily removed. This article adds to the theoretical underpinnings of the\nconcept that we previously described mainly empirically. By 2) bystander\ninaccessibility we mean the difficulty of a bystander to a smartphone user to\nbe aware of what kind of action the user is undertaking with the device and\nwhat the phase of the activity is. Our research is based on the theory of\nethnomethodology. In addition to ethnomethodological analysis of interaction,\nwe also apply other reseach methods. We illustrate the phenomena of sticky\nmedia device and bystander inaccessibility by analyzing 1) naturalistic video\ndata, 2) written role playing materials and 3) quantitative data, all of which\nconcentrate on the overlapping of smartphone use and face-to-face conversation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 10:28:45 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Raudaskoski", "Sanna", "", "CED"], ["Mantere", "Eerik", "", "CED"], ["Valkonen", "Satu", ""]]}, {"id": "1910.13249", "submitter": "Florian Golemo", "authors": "Martin Weiss, Simon Chamorro, Roger Girgis, Margaux Luck, Samira E.\n  Kahou, Joseph P. Cohen, Derek Nowrouzezahrai, Doina Precup, Florian Golemo,\n  Chris Pal", "title": "Navigation Agents for the Visually Impaired: A Sidewalk Simulator and\n  Experiments", "comments": "Accepted at CoRL2019. Code & video available at\n  https://mweiss17.github.io/SEVN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of blind and visually-impaired (BVI) people navigate urban\nenvironments every day, using smartphones for high-level path-planning and\nwhite canes or guide dogs for local information. However, many BVI people still\nstruggle to travel to new places. In our endeavor to create a navigation\nassistant for the BVI, we found that existing Reinforcement Learning (RL)\nenvironments were unsuitable for the task. This work introduces SEVN, a\nsidewalk simulation environment and a neural network-based approach to creating\na navigation agent. SEVN contains panoramic images with labels for house\nnumbers, doors, and street name signs, and formulations for several navigation\ntasks. We study the performance of an RL algorithm (PPO) in this setting. Our\npolicy model fuses multi-modal observations in the form of variable resolution\nimages, visible text, and simulated GPS data to navigate to a goal door. We\nhope that this dataset, simulator, and experimental results will provide a\nfoundation for further research into the creation of agents that can assist\nmembers of the BVI community with outdoor navigation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 13:23:02 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Weiss", "Martin", ""], ["Chamorro", "Simon", ""], ["Girgis", "Roger", ""], ["Luck", "Margaux", ""], ["Kahou", "Samira E.", ""], ["Cohen", "Joseph P.", ""], ["Nowrouzezahrai", "Derek", ""], ["Precup", "Doina", ""], ["Golemo", "Florian", ""], ["Pal", "Chris", ""]]}, {"id": "1910.13602", "submitter": "Yang Liu", "authors": "Yang Liu, Tim Althoff, Jeffrey Heer", "title": "Paths Explored, Paths Omitted, Paths Obscured: Decision Points &\n  Selective Reporting in End-to-End Data Analysis", "comments": null, "journal-ref": null, "doi": "10.1145/3313831.3376533", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing reliable inferences from data involves many, sometimes arbitrary,\ndecisions across phases of data collection, wrangling, and modeling. As\ndifferent choices can lead to diverging conclusions, understanding how\nresearchers make analytic decisions is important for supporting robust and\nreplicable analysis. In this study, we pore over nine published research\nstudies and conduct semi-structured interviews with their authors. We observe\nthat researchers often base their decisions on methodological or theoretical\nconcerns, but subject to constraints arising from the data, expertise, or\nperceived interpretability. We confirm that researchers may experiment with\nchoices in search of desirable results, but also identify other reasons why\nresearchers explore alternatives yet omit findings. In concert with our\ninterviews, we also contribute visualizations for communicating decision\nprocesses throughout an analysis. Based on our results, we identify design\nopportunities for strengthening end-to-end analysis, for instance via tracking\nand meta-analysis of multiple decision paths.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 00:41:56 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 05:02:56 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 23:56:04 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Liu", "Yang", ""], ["Althoff", "Tim", ""], ["Heer", "Jeffrey", ""]]}, {"id": "1910.13607", "submitter": "Atoosa Kasirzadeh", "authors": "Atoosa Kasirzadeh", "title": "Mathematical decisions and non-causal elements of explainable AI", "comments": "A shorter version of this paper was presented at the NeurIPS 2019,\n  Human-Centric Machine Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The social implications of algorithmic decision-making in sensitive contexts\nhave generated lively debates among multiple stakeholders, such as moral and\npolitical philosophers, computer scientists, and the public. Yet, the lack of a\ncommon language and a conceptual framework for an appropriate bridging of the\nmoral, technical, and political aspects of the debate prevents the discussion\nto be as effective as it can be. Social scientists and psychologists are\ncontributing to this debate by gathering a wealth of empirical data, yet a\nphilosophical analysis of the social implications of algorithmic\ndecision-making remains comparatively impoverished. In attempting to address\nthis lacuna, this paper argues that a hierarchy of different types of\nexplanations for why and how an algorithmic decision outcome is achieved can\nestablish the relevant connection between the moral and technical aspects of\nalgorithmic decision-making. In particular, I offer a multi-faceted conceptual\nframework for the explanations and the interpretations of algorithmic\ndecisions, and I claim that this framework can lay the groundwork for a focused\ndiscussion among multiple stakeholders about the social implications of\nalgorithmic decision-making, as well as AI governance and ethics more\ngenerally.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 00:58:44 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 07:08:33 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Kasirzadeh", "Atoosa", ""]]}, {"id": "1910.13641", "submitter": "EPTCS", "authors": "Georgiana Caltais (Konstanz University), Jean Krivine (CNRS)", "title": "Proceedings of the 4th Workshop on Formal Reasoning about Causation,\n  Responsibility, and Explanations in Science and Technology", "comments": null, "journal-ref": "EPTCS 308, 2019", "doi": "10.4204/EPTCS.308", "report-no": null, "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fourth edition of the international workshop on Causation, Responsibility\nand Explanation took place in Prague (Czech Republic) as part of ETAPS 2019.\nThe program consisted in 5 invited speakers and 4 regular papers, whose\nselection was based on a careful reviewing process and that are included in\nthese proceedings.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 02:58:45 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Caltais", "Georgiana", "", "Konstanz University"], ["Krivine", "Jean", "", "CNRS"]]}, {"id": "1910.13656", "submitter": "Van Vung Pham", "authors": "Vung Pham and Tommy Dang", "title": "Outliagnostics: Visualizing Temporal Discrepancy in Outlying Signatures\n  of Data Entries", "comments": "in IEEE Visualization in Data Science (IEEE VDS) (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to analyzing two-dimensional temporal\ndatasets focusing on identifying observations that are significant in\ncalculating the outliers of a scatterplot. We also propose a prototype, called\nOutliagnostics, to guide users when interactively exploring abnormalities in\nlarge time series. Instead of focusing on detecting outliers at each time\npoint, we monitor and display the discrepant temporal signatures of each data\nentry concerning the overall distributions. Our prototype is designed to handle\nthese tasks in parallel to improve performance. To highlight the benefits and\nperformance of our approach, we illustrate and validate the use of\nOutliagnostics on real-world datasets of various sizes in different parallelism\nconfigurations. This work also discusses how to extend these ideas to handle\ntime series with a higher number of dimensions and provides a prototype for\nthis type of datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 04:24:38 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Pham", "Vung", ""], ["Dang", "Tommy", ""]]}, {"id": "1910.13872", "submitter": "Omiros Pantazis", "authors": "Hesham Dar, James Kwan, Yang Liu, Omiros Pantazis, Robert Sharp", "title": "The Game Performance Index for Mobile Phones", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent increase in the quantity of high fidelity games appearing on\nmobile devices and the recent trend of gaming focused mobile devices, there is\na new requirement for a clear and comprehensive measure of the quality of\ngaming performance on the mobile device platform. This paper proposes a\nconceptual framework for a user-experience and user-perception based set of\nperformance measures for mobile devices. This paper presents a specific\nimplementation and measurement use case which has been beneficial to Samsung\nElectronics when applied to our own product range, allowing us to better\nunderstand and quantify device performance. We believe that the methods\noutlined are potentially useful to the consumer, by providing an understandable\npublic facing score for device performance to guide consumers with purchasing\ndecisions. The methods may be useful to game developers and could better enable\nthe developer to add new richer game features based on the performance of the\ndevice.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 14:19:26 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Dar", "Hesham", ""], ["Kwan", "James", ""], ["Liu", "Yang", ""], ["Pantazis", "Omiros", ""], ["Sharp", "Robert", ""]]}, {"id": "1910.14084", "submitter": "Sahisnu Mazumder", "authors": "Sahisnu Mazumder, Bing Liu, Shuai Wang, Sepideh Esmaeilpour", "title": "Building an Application Independent Natural Language Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to building natural language (NL) interfaces typically\nuse a semantic parser to parse the user command and convert it to a logical\nform, which is then translated to an executable action in an application.\nHowever, it is still challenging for a semantic parser to correctly parse\nnatural language. For a different domain, the parser may need to be retrained\nor tuned, and a new translator also needs to be written to convert the logical\nforms to executable actions. In this work, we propose a novel and application\nindependent approach to building NL interfaces that does not need a semantic\nparser or a translator. It is based on natural language to natural language\nmatching and learning, where the representation of each action and each user\ncommand are both in natural language. To perform a user intended action, the\nsystem only needs to match the user command with the correct action\nrepresentation, and then execute the corresponding action. The system also\ninteractively learns new (paraphrased) commands for actions to expand the\naction representations over time. Our experimental results show the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 18:57:28 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Mazumder", "Sahisnu", ""], ["Liu", "Bing", ""], ["Wang", "Shuai", ""], ["Esmaeilpour", "Sepideh", ""]]}, {"id": "1910.14112", "submitter": "Danny Yuxing Huang", "authors": "David J. Major, Danny Yuxing Huang, Marshini Chetty, Nick Feamster", "title": "Alexa, Who Am I Speaking To? Understanding Users' Ability to Identify\n  Third-Party Apps on Amazon Alexa", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Internet of Things (IoT) devices have voice user interfaces (VUIs). One\nof the most popular VUIs is Amazon's Alexa, which supports more than 47,000\nthird-party applications (\"skills\"). We study how Alexa's integration of these\nskills may confuse users. Our survey of 237 participants found that users do\nnot understand that skills are often operated by third parties, that they often\nconfuse third-party skills with native Alexa functions, and that they are\nunaware of the functions that the native Alexa system supports. Surprisingly,\nusers who interact with Alexa more frequently are more likely to conclude that\na third-party skill is native Alexa functionality. The potential for\nmisunderstanding creates new security and privacy risks: attackers can develop\nthird-party skills that operate without users' knowledge or masquerade as\nnative Alexa functions. To mitigate this threat, we make design recommendations\nto help users distinguish native and third-party skills.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 20:07:01 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Major", "David J.", ""], ["Huang", "Danny Yuxing", ""], ["Chetty", "Marshini", ""], ["Feamster", "Nick", ""]]}]