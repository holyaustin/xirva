[{"id": "2009.00001", "submitter": "Victoria Lin", "authors": "Victoria Lin, Jeffrey M. Girard, Michael A. Sayette, Louis-Philippe\n  Morency", "title": "Toward Multimodal Modeling of Emotional Expressiveness", "comments": "V. Lin and J.M. Girard contributed equally to this research. This\n  paper was accepted to ICMI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional expressiveness captures the extent to which a person tends to\noutwardly display their emotions through behavior. Due to the close\nrelationship between emotional expressiveness and behavioral health, as well as\nthe crucial role that it plays in social interaction, the ability to\nautomatically predict emotional expressiveness stands to spur advances in\nscience, medicine, and industry. In this paper, we explore three related\nresearch questions. First, how well can emotional expressiveness be predicted\nfrom visual, linguistic, and multimodal behavioral signals? Second, which\nbehavioral modalities are uniquely important to the prediction of emotional\nexpressiveness? Third, which behavioral signals are reliably related to\nemotional expressiveness? To answer these questions, we add highly reliable\ntranscripts and human ratings of perceived emotional expressiveness to an\nexisting video database and use this data to train, validate, and test\npredictive models. Our best model shows promising predictive performance on\nthis dataset (RMSE=0.65, R^2=0.45, r=0.74). Multimodal models tend to perform\nbest overall, and models trained on the linguistic modality tend to outperform\nmodels trained on the visual modality. Finally, examination of our\ninterpretable models' coefficients reveals a number of visual and linguistic\nbehavioral signals--such as facial action unit intensity, overall word count,\nand use of words related to social processes--that reliably predict emotional\nexpressiveness.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 19:17:26 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Lin", "Victoria", ""], ["Girard", "Jeffrey M.", ""], ["Sayette", "Michael A.", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2009.00050", "submitter": "Benjamin Lee", "authors": "Benjamin Lee, Xiaoyun Hu, Maxime Cordeil, Arnaud Prouzeau, Bernhard\n  Jenny and Tim Dwyer", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a\n  Co-located Immersive Environment", "comments": "Presented at IEEE Conference on Information Visualization (InfoVis\n  2020)", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030450", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersive technologies offer new opportunities to support collaborative\nvisual data analysis by providing each collaborator a personal, high-resolution\nview of a flexible shared visualisation space through a head mounted display.\nHowever, most prior studies of collaborative immersive analytics have focused\non how groups interact with surface interfaces such as tabletops and wall\ndisplays. This paper reports on a study in which teams of three co-located\nparticipants are given flexible visualisation authoring tools to allow a great\ndeal of control in how they structure their shared workspace. They do so using\na prototype system we call FIESTA: the Free-roaming Immersive Environment to\nSupport Team-based Analysis. Unlike traditional visualisation tools, FIESTA\nallows users to freely position authoring interfaces and visualisation\nartefacts anywhere in the virtual environment, either on virtual surfaces or\nsuspended within the interaction space. Our participants solved visual\nanalytics tasks on a multivariate data set, doing so individually and\ncollaboratively by creating a large number of 2D and 3D visualisations. Their\nbehaviours suggest that the usage of surfaces is coupled with the type of\nvisualisation used, often using walls to organise 2D visualisations, but\npositioning 3D visualisations in the space around them. Outside of\ntightly-coupled collaboration, participants followed social protocols and did\nnot interact with visualisations that did not belong to them even if outside of\nits owner's personal workspace.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 18:32:35 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 11:03:32 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Lee", "Benjamin", ""], ["Hu", "Xiaoyun", ""], ["Cordeil", "Maxime", ""], ["Prouzeau", "Arnaud", ""], ["Jenny", "Bernhard", ""], ["Dwyer", "Tim", ""]]}, {"id": "2009.00059", "submitter": "Benjamin Lee", "authors": "Benjamin Lee, Dave Brown, Bongshin Lee, Christophe Hurter, Steven\n  Drucker, and Tim Dwyer", "title": "Data Visceralization: Enabling Deeper Understanding of Data Using\n  Virtual Reality", "comments": "Presented at IEEE Conference on Information Visualization (InfoVis\n  2020), InfoVis Honourable Mention Award", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030435", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental part of data visualization is transforming data to map abstract\ninformation onto visual attributes. While this abstraction is a powerful basis\nfor data visualization, the connection between the representation and the\noriginal underlying data (i.e., what the quantities and measurements actually\ncorrespond with in reality) can be lost. On the other hand, virtual reality\n(VR) is being increasingly used to represent real and abstract models as\nnatural experiences to users. In this work, we explore the potential of using\nVR to help restore the basic understanding of units and measures that are often\nabstracted away in data visualization in an approach we call data\nvisceralization. By building VR prototypes as design probes, we identify key\nthemes and factors for data visceralization. We do this first through a\ncritical reflection by the authors, then by involving external participants. We\nfind that data visceralization is an engaging way of understanding the\nqualitative aspects of physical measures and their real-life form, which\ncomplements analytical and quantitative understanding commonly gained from data\nvisualization. However, data visceralization is most effective when there is a\none-to-one mapping between data and representation, with transformations such\nas scaling affecting this understanding. We conclude with a discussion of\nfuture directions for data visceralization.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 18:55:28 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 11:09:37 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Lee", "Benjamin", ""], ["Brown", "Dave", ""], ["Lee", "Bongshin", ""], ["Hurter", "Christophe", ""], ["Drucker", "Steven", ""], ["Dwyer", "Tim", ""]]}, {"id": "2009.00091", "submitter": "Jon Saad-Falcon", "authors": "Jon Saad-Falcon, Omar Shaikh, Zijie J. Wang, Austin P. Wright, Sasha\n  Richardson, and Duen Horng Chau", "title": "Mapping Researchers with PeopleMap", "comments": "2020 IEEE Visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering research expertise at universities can be a difficult task.\nDirectories routinely become outdated, and few help in visually summarizing\nresearchers' work or supporting the exploration of shared interests among\nresearchers. This results in lost opportunities for both internal and external\nentities to discover new connections, nurture research collaboration, and\nexplore the diversity of research. To address this problem, at Georgia Tech, we\nhave been developing PeopleMap, an open-source interactive web-based tool that\nuses natural language processing (NLP) to create visual maps for researchers\nbased on their research interests and publications. Requiring only the\nresearchers' Google Scholar profiles as input, PeopleMap generates and\nvisualizes embeddings for the researchers, significantly reducing the need for\nmanual curation of publication information. To encourage and facilitate easy\nadoption and extension of PeopleMap, we have open-sourced it under the\npermissive MIT license at https://github.com/poloclub/people-map. PeopleMap has\nreceived positive feedback and enthusiasm for expanding its adoption across\nGeorgia Tech.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 20:46:27 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Saad-Falcon", "Jon", ""], ["Shaikh", "Omar", ""], ["Wang", "Zijie J.", ""], ["Wright", "Austin P.", ""], ["Richardson", "Sasha", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2009.00192", "submitter": "Sehi L'Yi", "authors": "Sehi L'Yi, Jaemin Jo, Jinwook Seo", "title": "Comparative Layouts Revisited: Design Space, Guidelines, and Future\n  Directions", "comments": "IEEE VIS InfoVis 2020, ACM 2012 CCS - Human-centered computing,\n  Visualization, Visualization theory, concepts, and paradigms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a systematic review on three comparative layouts (i.e.,\njuxtaposition, superposition, and explicit-encoding) which are information\nvisualization (InfoVis) layouts designed to support comparison tasks. For the\nlast decade, these layouts have served as fundamental idioms in designing many\nvisualization systems. However, we found that the layouts have been used with\ninconsistent terms and confusion, and the lessons from previous studies are\nfragmented. The goal of our research is to distill the results from previous\nstudies into a consistent and reusable framework. We review 127 research\npapers, including 15 papers with quantitative user studies, which employed\ncomparative layouts. We first alleviate the ambiguous boundaries in the design\nspace of comparative layouts by suggesting lucid terminology (e.g., chart-wise\nand item-wise juxtaposition). We then identify the diverse aspects of\ncomparative layouts, such as the advantages and concerns of using each layout\nin the real-world scenarios and researchers' approaches to overcome the\nconcerns. Building our knowledge on top of the initial insights gained from the\nGleicher et al.'s survey, we elaborate on relevant empirical evidence that we\ndistilled from our survey (e.g., the actual effectiveness of the layouts in\ndifferent study settings) and identify novel facets that the original work did\nnot cover (e.g., the familiarity of the layouts to people). Finally, we show\nthe consistent and contradictory results on the performance of comparative\nlayouts and offer practical implications for using the layouts by suggesting\ntrade-offs and seven actionable guidelines.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 02:45:48 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["L'Yi", "Sehi", ""], ["Jo", "Jaemin", ""], ["Seo", "Jinwook", ""]]}, {"id": "2009.00219", "submitter": "Zhuochen Jin", "authors": "Zhuochen Jin, Shunan Guo, Nan Chen, Daniel Weiskopf, David Gotz, Nan\n  Cao", "title": "Visual Causality Analysis of Event Sequence Data", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2020.3030465", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality is crucial to understanding the mechanisms behind complex systems\nand making decisions that lead to intended outcomes. Event sequence data is\nwidely collected from many real-world processes, such as electronic health\nrecords, web clickstreams, and financial transactions, which transmit a great\ndeal of information reflecting the causal relations among event types.\nUnfortunately, recovering causalities from observational event sequences is\nchallenging, as the heterogeneous and high-dimensional event variables are\noften connected to rather complex underlying event excitation mechanisms that\nare hard to infer from limited observations. Many existing automated causal\nanalysis techniques suffer from poor explainability and fail to include an\nadequate amount of human knowledge. In this paper, we introduce a visual\nanalytics method for recovering causalities in event sequence data. We extend\nthe Granger causality analysis algorithm on Hawkes processes to incorporate\nuser feedback into causal model refinement. The visualization system includes\nan interactive causal analysis framework that supports bottom-up causal\nexploration, iterative causal verification and refinement, and causal\ncomparison through a set of novel visualizations and interactions. We report\ntwo forms of evaluation: a quantitative evaluation of the model improvements\nresulting from the user-feedback mechanism, and a qualitative evaluation\nthrough case studies in different application domains to demonstrate the\nusefulness of the system.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 04:28:28 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Jin", "Zhuochen", ""], ["Guo", "Shunan", ""], ["Chen", "Nan", ""], ["Weiskopf", "Daniel", ""], ["Gotz", "David", ""], ["Cao", "Nan", ""]]}, {"id": "2009.00224", "submitter": "Samuel Reinders", "authors": "Samuel Reinders, Matthew Butler, Kim Marriott", "title": "\"Hey Model!\" -- Natural User Interactions and Agency in Accessible\n  Interactive 3D Models", "comments": "Paper presented at ACM CHI 2020: Proceedings of the 2020 CHI\n  Conference on Human Factors in Computing Systems, ACM, New York, April 2020;\n  Replacement: typos corrected", "journal-ref": null, "doi": "10.1145/3313831.3376145", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While developments in 3D printing have opened up opportunities for improved\naccess to graphical information for people who are blind or have low vision\n(BLV), they can provide only limited detailed and contextual information.\nInteractive 3D printed models (I3Ms) that provide audio labels and/or a\nconversational agent interface potentially overcome this limitation. We\nconducted a Wizard-of-Oz exploratory study to uncover the multi-modal\ninteraction techniques that BLV people would like to use when exploring I3Ms,\nand investigated their attitudes towards different levels of model agency.\nThese findings informed the creation of an I3M prototype of the solar system. A\nsecond user study with this model revealed a hierarchy of interaction, with BLV\nusers preferring tactile exploration, followed by touch gestures to trigger\naudio labels, and then natural language to fill in knowledge gaps and confirm\nunderstanding.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 04:49:31 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 03:14:39 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Reinders", "Samuel", ""], ["Butler", "Matthew", ""], ["Marriott", "Kim", ""]]}, {"id": "2009.00249", "submitter": "Tan Tang", "authors": "Tan Tang, Renzhong Li, Xinke Wu, Shuhan Liu, Johannes Knittel, Steffen\n  Koch, Thomas Ertl, Lingyun Yu, Peiran Ren, and Yingcai Wu", "title": "PlotThread: Creating Expressive Storyline Visualizations using\n  Reinforcement Learning", "comments": "to be published in IEEE VIS InfoVis 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storyline visualizations are an effective means to present the evolution of\nplots and reveal the scenic interactions among characters. However, the design\nof storyline visualizations is a difficult task as users need to balance\nbetween aesthetic goals and narrative constraints. Despite that the\noptimization-based methods have been improved significantly in terms of\nproducing aesthetic and legible layouts, the existing (semi-) automatic methods\nare still limited regarding 1) efficient exploration of the storyline design\nspace and 2) flexible customization of storyline layouts. In this work, we\npropose a reinforcement learning framework to train an AI agent that assists\nusers in exploring the design space efficiently and generating well-optimized\nstorylines. Based on the framework, we introduce PlotThread, an authoring tool\nthat integrates a set of flexible interactions to support easy customization of\nstoryline visualizations. To seamlessly integrate the AI agent into the\nauthoring process, we employ a mixed-initiative approach where both the agent\nand designers work on the same canvas to boost the collaborative design of\nstorylines. We evaluate the reinforcement learning model through qualitative\nand quantitative experiments and demonstrate the usage of PlotThread using a\ncollection of use cases.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 06:01:54 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Tang", "Tan", ""], ["Li", "Renzhong", ""], ["Wu", "Xinke", ""], ["Liu", "Shuhan", ""], ["Knittel", "Johannes", ""], ["Koch", "Steffen", ""], ["Ertl", "Thomas", ""], ["Yu", "Lingyun", ""], ["Ren", "Peiran", ""], ["Wu", "Yingcai", ""]]}, {"id": "2009.00260", "submitter": "Von Ralph Dane Herbuela", "authors": "Von Ralph Dane Marquez Herbuela, Tomonori Karita, Yoshiya Furukawa,\n  Yoshinori Wada, Shuichiro Senba, Eiko Onishi, Tatsuo Saeki", "title": "Children with PIMD/SMID expressive behaviors: Development and testing of\n  ChildSIDE app, the first step for independent communication and mobility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Children with profound intellectual and multiple disabilities or severe motor\nand intellectual disabilities only communicate through movements,\nvocalizations, body postures, muscle tensions, or facial expressions on a pre-\nor protosymbolic level. Yet, to the best of our knowledge, hardly any system\nhas been developed to interpret their expressive behaviors. This paper\ndescribes the design, development, and testing of ChildSIDE in collecting\nbehaviors of children and transmitting location and environmental data to the\ndatabase. The movements associated with each behavior were also identified for\nfuture system development. ChildSIDE app was pilot tested among purposively\nrecruited child-caregiver dyads. ChildSIDE was more likely to collect correct\nbehavior data than paper-based method and it had >93% in detecting and\ntransmitting location and environment data except for iBeacon data. Behaviors\nwere manifested mainly through hand and body movements and vocalizations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 06:26:02 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Herbuela", "Von Ralph Dane Marquez", ""], ["Karita", "Tomonori", ""], ["Furukawa", "Yoshiya", ""], ["Wada", "Yoshinori", ""], ["Senba", "Shuichiro", ""], ["Onishi", "Eiko", ""], ["Saeki", "Tatsuo", ""]]}, {"id": "2009.00261", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Clayton Hutson, Chin-Yi Cheng, Mehdi Nourbakhsh,\n  Michael Bergin, Mohammad Rahmani Asl", "title": "SketchOpt: Sketch-based Parametric Model Retrieval for Generative Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing fully parametric building models for performance-based generative\ndesign tasks often requires proficiency in many advanced 3D modeling and visual\nprogramming, limiting its use for many building designers. Moreover, iterations\nof such models can be time-consuming tasks and sometimes limiting, as major\nchanges in the layout design may result in remodeling the entire parametric\ndefinition. To address these challenges, we introduce a novel automated\ngenerative design system, which takes a basic floor plan sketch as an input and\nprovides a parametric model prepared for multi-objective building optimization\nas output. Furthermore, the user-designer can assign various design variables\nfor its desired building elements by using simple annotations in the drawing.\nThe system would recognize the corresponding element and define variable\nconstraints to prepare for a multi-objective optimization problem.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 06:28:09 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Hutson", "Clayton", ""], ["Cheng", "Chin-Yi", ""], ["Nourbakhsh", "Mehdi", ""], ["Bergin", "Michael", ""], ["Asl", "Mohammad Rahmani", ""]]}, {"id": "2009.00406", "submitter": "Shivam Agarwal", "authors": "Shivam Agarwal, Shahid Latif, and Fabian Beck", "title": "How Visualization PhD Students Cope with Paper Rejections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We conducted a questionnaire study aimed towards PhD students in the field of\nvisualization research to understand how they cope with paper rejections. We\ncollected responses from 24 participants and performed a qualitative analysis\nof the data in relation to the provided support by collaborators, resubmission\nstrategies, handling multiple rejects, and personal impression of the reviews.\nThe results indicate that the PhD students in the visualization community\ngenerally cope well with the negative reviews and, with experience, learn how\nto act accordingly to improve and resubmit their work. Our results reveal the\nmain coping strategies that can be applied for constructively handling rejected\nvisualization papers. The most prominent strategies include: discussing reviews\nwith collaborators and making a resubmission plan, doing a major revision to\nimprove the work, shortening the work, and seeing rejection as a positive\nlearning experience.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 13:17:23 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 11:01:58 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2020 17:15:03 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2020 15:42:40 GMT"}, {"version": "v5", "created": "Fri, 25 Sep 2020 21:33:27 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Agarwal", "Shivam", ""], ["Latif", "Shahid", ""], ["Beck", "Fabian", ""]]}, {"id": "2009.00449", "submitter": "Sungsoo (Ray) Hong", "authors": "Sungsoo Ray Hong, Sonia Castelo, Vito D'Orazio, Christopher Benthune,\n  Aecio Santos, Scott Langevin, David Jonker, Enrico Bertini, Juliana Freire", "title": "Towards Evaluating Exploratory Model Building Process with AutoML\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The use of Automated Machine Learning (AutoML) systems are highly open-ended\nand exploratory. While rigorously evaluating how end-users interact with AutoML\nis crucial, establishing a robust evaluation methodology for such exploratory\nsystems is challenging. First, AutoML is complex, including multiple\nsub-components that support a variety of sub-tasks for synthesizing ML\npipelines, such as data preparation, problem specification, and model\ngeneration, making it difficult to yield insights that tell us which components\nwere successful or not. Second, because the usage pattern of AutoML is highly\nexploratory, it is not possible to rely solely on widely used task efficiency\nand effectiveness metrics as success metrics. To tackle the challenges in\nevaluation, we propose an evaluation methodology that (1) guides AutoML\nbuilders to divide their AutoML system into multiple sub-system components, and\n(2) helps them reason about each component through visualization of end-users'\nbehavioral patterns and attitudinal data. We conducted a study to understand\nwhen, how, why, and applying our methodology can help builders to better\nunderstand their systems and end-users. We recruited 3 teams of professional\nAutoML builders. The teams prepared their own systems and let 41 end-users use\nthe systems. Using our methodology, we visualized end-users' behavioral and\nattitudinal data and distributed the results to the teams. We analyzed the\nresults in two directions: what types of novel insights the AutoML builders\nlearned from end-users, and (2) how the evaluation methodology helped the\nbuilders to understand workflows and the effectiveness of their systems. Our\nfindings suggest new insights explaining future design opportunities in the\nAutoML domain as well as how using our methodology helped the builders to\ndetermine insights and let them draw concrete directions for improving their\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 14:09:29 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Hong", "Sungsoo Ray", ""], ["Castelo", "Sonia", ""], ["D'Orazio", "Vito", ""], ["Benthune", "Christopher", ""], ["Santos", "Aecio", ""], ["Langevin", "Scott", ""], ["Jonker", "David", ""], ["Bertini", "Enrico", ""], ["Freire", "Juliana", ""]]}, {"id": "2009.00548", "submitter": "Philipp Meschenmoser", "authors": "Philipp Meschenmoser, Juri F. Buchm\\\"uller, Daniel Seebacher, Martin\n  Wikelski and Daniel A. Keim", "title": "MultiSegVA: Using Visual Analytics to Segment Biologging Time Series on\n  Multiple Scales", "comments": "IEEE VAST 2020 - Proceedings of IEEE Conference on Visual Analytics\n  Science and Technology (VAST), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting biologging time series of animals on multiple temporal scales is\nan essential step that requires complex techniques with careful\nparameterization and possibly cross-domain expertise. Yet, there is a lack of\nvisual-interactive tools that strongly support such multi-scale segmentation.\nTo close this gap, we present our MultiSegVA platform for interactively\ndefining segmentation techniques and parameters on multiple temporal scales.\nMultiSegVA primarily contributes tailored, visual-interactive means and visual\nanalytics paradigms for segmenting unlabeled time series on multiple scales.\nFurther, to flexibly compose the multi-scale segmentation, the platform\ncontributes a new visual query language that links a variety of segmentation\ntechniques. To illustrate our approach, we present a domain-oriented set of\nsegmentation techniques derived in collaboration with movement ecologists. We\ndemonstrate the applicability and usefulness of MultiSegVA in two real-world\nuse cases from movement ecology, related to behavior analysis after\nenvironment-aware segmentation, and after progressive clustering. Expert\nfeedback from movement ecologists shows the effectiveness of tailored\nvisual-interactive means and visual analytics paradigms at segmenting\nmulti-scale data, enabling them to perform semantically meaningful analyses. A\nthird use case demonstrates that MultiSegVA is generalizable to other domains.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 16:27:08 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 08:22:29 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Meschenmoser", "Philipp", ""], ["Buchm\u00fcller", "Juri F.", ""], ["Seebacher", "Daniel", ""], ["Wikelski", "Martin", ""], ["Keim", "Daniel A.", ""]]}, {"id": "2009.00717", "submitter": "Zhi Wang", "authors": "Zhi Wang, Xueying Tang, Jingchen Liu and Zhiliang Ying", "title": "Subtask Analysis of Process Data Through a Predictive Model", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response process data collected from human-computer interactive items contain\nrich information about respondents' behavioral patterns and cognitive\nprocesses. Their irregular formats as well as their large sizes make standard\nstatistical tools difficult to apply. This paper develops a computationally\nefficient method for exploratory analysis of such process data. The new\napproach segments a lengthy individual process into a sequence of short\nsubprocesses to achieve complexity reduction, easy clustering and meaningful\ninterpretation. Each subprocess is considered a subtask. The segmentation is\nbased on sequential action predictability using a parsimonious predictive model\ncombined with the Shannon entropy. Simulation studies are conducted to assess\nperformance of the new methods. We use the process data from PIAAC 2012 to\ndemonstrate how exploratory analysis of process data can be done with the new\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 21:11:01 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Wang", "Zhi", ""], ["Tang", "Xueying", ""], ["Liu", "Jingchen", ""], ["Ying", "Zhiliang", ""]]}, {"id": "2009.00722", "submitter": "Krist Wongsuphasawat", "authors": "Krist Wongsuphasawat", "title": "Encodable: Configurable Grammar for Visualization Components", "comments": "Will be published at IEEE VIS (InfoVis/VAST/SciVis) 2020. ACM 2012\n  CCS - Human-centered computing, Visualization, Visualization design and\n  evaluation methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are so many libraries of visualization components nowadays with their\nAPIs often different from one another. Could these components be more similar,\nboth in terms of the APIs and common functionalities? For someone who is\ndeveloping a new visualization component, how should the API look like? This\nwork drew inspiration from visualization grammar, decoupled the grammar from\nits rendering engine and adapted it into a configurable grammar for individual\ncomponents called Encodable. Encodable helps component authors define grammar\nfor their components, and parse encoding specifications from users into utility\nfunctions for the implementation. This paper explains the grammar design and\ndemonstrates how to build components with it.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 21:41:01 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Wongsuphasawat", "Krist", ""]]}, {"id": "2009.00750", "submitter": "Alaul Islam", "authors": "Alaul Islam (1), Anastasia Bezerianos (1), Bongshin Lee (2), Tanja\n  Blascheck (3), Petra Isenberg (1) ((1) Universit\\'e Paris-Saclay, CNRS,\n  Inria, LRI, (2) Microsoft Research, (3) University of Stuttgart)", "title": "Visualizing information on watch faces: A survey with smartwatch users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People increasingly wear smartwatches that can track a wide variety of data.\nHowever, it is currently unknown which data people consume and how it is\nvisualized. To better ground research on smartwatch visualization, it is\nimportant to understand the current use of these representation types on\nsmartwatches, and to identify missed visualization opportunities. We present\nthe findings of a survey with 237 smartwatch wearers, and assess the types of\ndata and representations commonly displayed on watch faces. We found a\npredominant display of health & fitness data, with icons accompanied by text\nbeing the most frequent representation type. Combining these results with a\nfurther analysis of online searches of watch faces and the data tracked on\nsmartwatches that are not commonly visualized, we discuss opportunities for\nvisualization research.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 23:45:22 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 11:23:18 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Islam", "Alaul", ""], ["Bezerianos", "Anastasia", ""], ["Lee", "Bongshin", ""], ["Blascheck", "Tanja", ""], ["Isenberg", "Petra", ""]]}, {"id": "2009.00956", "submitter": "Sabin Devkota", "authors": "Sabin Devkota, Pascal Aschwanden, Adam Kunen, Matthew Legendre, and\n  Katherine E. Isaacs", "title": "CcNav: Understanding Compiler Optimizations in Binary Code", "comments": "IEEE VIS VAST 2020", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030357", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program developers spend significant time on optimizing and tuning programs.\nDuring this iterative process, they apply optimizations, analyze the resulting\ncode, and modify the compilation until they are satisfied. Understanding what\nthe compiler did with the code is crucial to this process but is very\ntime-consuming and labor-intensive. Users need to navigate through thousands of\nlines of binary code and correlate it to source code concepts to understand the\nresults of the compilation and to identify optimizations. We present a design\nstudy in collaboration with program developers and performance analysts. Our\ncollaborators work with various artifacts related to the program such as binary\ncode, source code, control flow graphs, and call graphs. Through interviews,\nfeedback, and pair-analytics sessions, we analyzed their tasks and workflow.\nBased on this task analysis and through a human-centric design process, we\ndesigned a visual analytics system Compilation Navigator (CcNav) to aid\nexploration of the effects of compiler optimizations on the program. CcNav\nprovides a streamlined workflow and a unified context that integrates disparate\nartifacts. CcNav supports consistent interactions across all the artifacts\nmaking it easy to correlate binary code with source code concepts. CcNav\nenables users to navigate and filter large binary code to identify and\nsummarize optimizations such as inlining, vectorization, loop unrolling, and\ncode hoisting. We evaluate CcNav through guided sessions and semi-structured\ninterviews. We reflect on our design process, particularly the immersive\nelements, and on the transferability of design studies through our experience\nwith a previous design study on program analysis.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 11:24:19 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 08:18:45 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Devkota", "Sabin", ""], ["Aschwanden", "Pascal", ""], ["Kunen", "Adam", ""], ["Legendre", "Matthew", ""], ["Isaacs", "Katherine E.", ""]]}, {"id": "2009.01215", "submitter": "Michael Lyons", "authors": "Michael J. Lyons", "title": "Excavating \"Excavating AI\": The Elephant in the Gallery", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": "10.5281/zenodo.4037538", "report-no": null, "categories": "cs.CY cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two art exhibitions, \"Training Humans\" and \"Making Faces,\" and the\naccompanying essay \"Excavating AI: The politics of images in machine learning\ntraining sets\" by Kate Crawford and Trevor Paglen, are making substantial\nimpact on discourse taking place in the social and mass media networks, and\nsome scholarly circles. Critical scrutiny reveals, however, a\nself-contradictory stance regarding informed consent for the use of facial\nimages, as well as serious flaws in their critique of ML training sets. Our\nanalysis underlines the non-negotiability of informed consent when using human\ndata in artistic and other contexts, and clarifies issues relating to the\ndescription of ML training sets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 17:42:06 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 17:07:10 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 01:27:53 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Lyons", "Michael J.", ""]]}, {"id": "2009.01224", "submitter": "Sevgi Zubeyde Gurbuz", "authors": "Sevgi Z. Gurbuz, Ali C. Gurbuz, Evie A. Malaia, Darrin J. Griffin,\n  Chris Crawford, M. Mahbubur Rahman, Emre Kurtoglu, Ridvan Aksu, Trevor Macks,\n  Robiulhossain Mdrafi", "title": "American Sign Language Recognition Using RF Sensing", "comments": "submitted", "journal-ref": "IEEE Sensors Journal, August 2020", "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many technologies for human-computer interaction have been designed for\nhearing individuals and depend upon vocalized speech, precluding users of\nAmerican Sign Language (ASL) in the Deaf community from benefiting from these\nadvancements. While great strides have been made in ASL recognition with video\nor wearable gloves, the use of video in homes has raised privacy concerns,\nwhile wearable gloves severely restrict movement and infringe on daily life.\nMethods: This paper proposes the use of RF sensors for HCI applications serving\nthe Deaf community. A multi-frequency RF sensor network is used to acquire\nnon-invasive, non-contact measurements of ASL signing irrespective of lighting\nconditions. The unique patterns of motion present in the RF data due to the\nmicro-Doppler effect are revealed using time-frequency analysis with the\nShort-Time Fourier Transform. Linguistic properties of RF ASL data are\ninvestigated using machine learning (ML). Results: The information content,\nmeasured by fractal complexity, of ASL signing is shown to be greater than that\nof other upper body activities encountered in daily living. This can be used to\ndifferentiate daily activities from signing, while features from RF data show\nthat imitation signing by non-signers is 99\\% differentiable from native ASL\nsigning. Feature-level fusion of RF sensor network data is used to achieve\n72.5\\% accuracy in classification of 20 native ASL signs. Implications: RF\nsensing can be used to study dynamic linguistic properties of ASL and design\nDeaf-centric smart environments for non-invasive, remote recognition of ASL. ML\nalgorithms should be benchmarked on native, not imitation, ASL data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 17:56:38 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Gurbuz", "Sevgi Z.", ""], ["Gurbuz", "Ali C.", ""], ["Malaia", "Evie A.", ""], ["Griffin", "Darrin J.", ""], ["Crawford", "Chris", ""], ["Rahman", "M. Mahbubur", ""], ["Kurtoglu", "Emre", ""], ["Aksu", "Ridvan", ""], ["Macks", "Trevor", ""], ["Mdrafi", "Robiulhossain", ""]]}, {"id": "2009.01270", "submitter": "Amogh Gudi", "authors": "Amogh Gudi, Xin Li, Jan van Gemert", "title": "Efficiency in Real-time Webcam Gaze Tracking", "comments": "Awarded Best Paper at European Conference on Computer Vision (ECCV)\n  Workshop on Eye Gaze in AR, VR, and in the Wild (OpenEyes) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiency and ease of use are essential for practical applications of camera\nbased eye/gaze-tracking. Gaze tracking involves estimating where a person is\nlooking on a screen based on face images from a computer-facing camera. In this\npaper we investigate two complementary forms of efficiency in gaze tracking: 1.\nThe computational efficiency of the system which is dominated by the inference\nspeed of a CNN predicting gaze-vectors; 2. The usability efficiency which is\ndetermined by the tediousness of the mandatory calibration of the gaze-vector\nto a computer screen. To do so, we evaluate the computational speed/accuracy\ntrade-off for the CNN and the calibration effort/accuracy trade-off for screen\ncalibration. For the CNN, we evaluate the full face, two-eyes, and single eye\ninput. For screen calibration, we measure the number of calibration points\nneeded and evaluate three types of calibration: 1. pure geometry, 2. pure\nmachine learning, and 3. hybrid geometric regression. Results suggest that a\nsingle eye input and geometric regression calibration achieve the best\ntrade-off.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 18:07:41 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Gudi", "Amogh", ""], ["Li", "Xin", ""], ["van Gemert", "Jan", ""]]}, {"id": "2009.01282", "submitter": "Jeremy E. Block", "authors": "Jeremy E. Block, Eric D. Ragan", "title": "Micro-entries: Encouraging Deeper Evaluation of Mental Models Over Time\n  for Interactive Data Systems", "comments": "10 pages, submitted to BELIV 2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many interactive data systems combine visual representations of data with\nembedded algorithmic support for automation and data exploration. To\neffectively support transparent and explainable data systems, it is important\nfor researchers and designers to know how users understand the system. We\ndiscuss the evaluation of users' mental models of system logic. Mental models\nare challenging to capture and analyze. While common evaluation methods aim to\napproximate the user's final mental model after a period of system usage, user\nunderstanding continuously evolves as users interact with a system over time.\nIn this paper, we review many common mental model measurement techniques,\ndiscuss tradeoffs, and recommend methods for deeper, more meaningful evaluation\nof mental models when using interactive data analysis and visualization\nsystems. We present guidelines for evaluating mental models over time that\nreveal the evolution of specific model updates and how they may map to the\nparticular use of interface features and data queries. By asking users to\ndescribe what they know and how they know it, researchers can collect\nstructured, time-ordered insight into a user's conceptualization process while\nalso helping guide users to their own discoveries.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 18:27:04 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Block", "Jeremy E.", ""], ["Ragan", "Eric D.", ""]]}, {"id": "2009.01399", "submitter": "Jianping Kelvin Li", "authors": "Jianping Kelvin Li and Kwan-Liu Ma", "title": "P6: A Declarative Language for Integrating Machine Learning in Visual\n  Analytics", "comments": "Accepted for presentation at IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present P6, a declarative language for building high performance visual\nanalytics systems through its support for specifying and integrating machine\nlearning and interactive visualization methods. As data analysis methods based\non machine learning and artificial intelligence continue to advance, a visual\nanalytics solution can leverage these methods for better exploiting large and\ncomplex data. However, integrating machine learning methods with interactive\nvisual analysis is challenging. Existing declarative programming libraries and\ntoolkits for visualization lack support for coupling machine learning methods.\nBy providing a declarative language for visual analytics, P6 can empower more\ndevelopers to create visual analytics applications that combine machine\nlearning and visualization methods for data analysis and problem solving.\nThrough a variety of example applications, we demonstrate P6's capabilities and\nshow the benefits of using declarative specifications to build visual analytics\nsystems. We also identify and discuss the research opportunities and challenges\nfor declarative visual analytics.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 00:58:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Li", "Jianping Kelvin", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2009.01429", "submitter": "Younghoon Kim", "authors": "Younghoon Kim, Jeffrey Heer", "title": "Gemini: A Grammar and Recommender System for AnimatedTransitions in\n  Statistical Graphics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animated transitions help viewers follow changes between related\nvisualizations. Specifying effective animations demands significant effort:\nauthors must select the elements and properties to animate, provide transition\nparameters, and coordinate the timing of stages. To facilitate this process, we\npresent Gemini, a declarative grammar and recommendation system for animated\ntransitions between single-view statistical graphics. Gemini specifications\ndefine transition \"steps\" in terms of high-level visual components (marks,\naxes, legends) and composition rules to synchronize and concatenate steps. With\nthis grammar, Gemini can recommend animation designs to augment and accelerate\ndesigners' work. Gemini enumerates staged animation designs for given start and\nend states, and ranks those designs using a cost function informed by prior\nperceptual studies. To evaluate Gemini, we conduct both a formative study on\nMechanical Turk to assess and tune our ranking function, and a summative study\nin which 8 experienced visualization developers implement animations in D3 that\nwe then compare to Gemini's suggestions. We find that most designs (9/11) are\nexactly replicable in Gemini, with many (8/11) achievable via edits to\nsuggestions, and that Gemini suggestions avoid multiple participant errors.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 03:18:42 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Kim", "Younghoon", ""], ["Heer", "Jeffrey", ""]]}, {"id": "2009.01444", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Sara Evensen and Chang Ge and Dongjin Choi and \\c{C}a\\u{g}atay\n  Demiralp", "title": "Data Programming by Demonstration: A Framework for Interactively\n  Learning Labeling Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DB cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data programming is a programmatic weak supervision approach to efficiently\ncurate large-scale labeled training data. Writing data programs (labeling\nfunctions) requires, however, both programming literacy and domain expertise.\nMany subject matter experts have neither programming proficiency nor time to\neffectively write data programs. Furthermore, regardless of one's expertise in\ncoding or machine learning, transferring domain expertise into labeling\nfunctions by enumerating rules and thresholds is not only time consuming but\nalso inherently difficult. Here we propose a new framework, data programming by\ndemonstration (DPBD), to generate labeling rules using interactive\ndemonstrations of users. DPBD aims to relieve the burden of writing labeling\nfunctions from users, enabling them to focus on higher-level semantics such as\nidentifying relevant signals for labeling tasks. We operationalize our\nframework with Ruler, an interactive system that synthesizes labeling rules for\ndocument classification by using span-level annotations of users on document\nexamples. We compare Ruler with conventional data programming through a user\nstudy conducted with 10 data scientists creating labeling functions for\nsentiment and spam classification tasks. We find that Ruler is easier to use\nand learn and offers higher overall satisfaction, while providing\ndiscriminative model performances comparable to ones achieved by conventional\ndata programming.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 04:25:08 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 01:44:22 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 22:44:04 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Evensen", "Sara", ""], ["Ge", "Chang", ""], ["Choi", "Dongjin", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "2009.01465", "submitter": "Damla Cay", "authors": "Damla \\c{C}ay, Till Nagel, As{\\i}m Evren Yanta\\c{c}", "title": "Understanding User Experience of COVID-19 Maps through Remote\n  Elicitation Interviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the coronavirus pandemic, visualizations gained a new level of\npopularity and meaning for a wider audience. People were bombarded with a wide\nset of public health visualizations ranging from simple graphs to complex\ninteractive dashboards. In a pandemic setting, where large amounts of the world\npopulation are socially distancing themselves, it becomes an urgent need to\nrefine existing user experience evaluation methods for remote settings to\nunderstand how people make sense out of COVID-19 related visualizations. When\nevaluating visualizations aimed towards the general public with vastly\ndifferent socio-demographic backgrounds and varying levels of technical\nsavviness and data literacy, it is important to understand user feedback beyond\naspects such as speed, task accuracy, or usability problems. As a part of this\nwider evaluation perspective, micro-phenomenology has been used to evaluate\nstatic and narrative visualizations to reveal the lived experience in a\ndetailed way. Building upon these studies, we conducted a user study to\nunderstand how to employ Elicitation (aka Micro-phenomenological) interviews in\nremote settings. In a case study, we investigated what experiences the\nparticipants had with map-based interactive visualizations. Our findings reveal\npositive and negative aspects of conducting Elicitation interviews remotely.\nOur results can inform the process of planning and executing remote Elicitation\ninterviews to evaluate interactive visualizations. In addition, we share\nrecommendations regarding visualization techniques and interaction design about\npublic health data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 06:12:09 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["\u00c7ay", "Damla", ""], ["Nagel", "Till", ""], ["Yanta\u00e7", "As\u0131m Evren", ""]]}, {"id": "2009.01512", "submitter": "Julien Tierny", "authors": "Harish Doraiswamy and Julien Tierny and Paulo J. S. Silva and Luis\n  Gustavo Nonato and Claudio Silva", "title": "TopoMap: A 0-dimensional Homology Preserving Projection of\n  High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional Projection is a fundamental tool for high-dimensional data\nanalytics and visualization. With very few exceptions, projection techniques\nare designed to map data from a high-dimensional space to a visual space so as\nto preserve some dissimilarity (similarity) measure, such as the Euclidean\ndistance for example. In fact, although adopting distinct mathematical\nformulations designed to favor different aspects of the data, most\nmultidimensional projection methods strive to preserve dissimilarity measures\nthat encapsulate geometric properties such as distances or the proximity\nrelation between data objects. However, geometric relations are not the only\ninteresting property to be preserved in a projection. For instance, the\nanalysis of particular structures such as clusters and outliers could be more\nreliably performed if the mapping process gives some guarantee as to\ntopological invariants such as connected components and loops. This paper\nintroduces TopoMap, a novel projection technique which provides topological\nguarantees during the mapping process. In particular, the proposed method\nperforms the mapping from a high-dimensional space to a visual space, while\npreserving the 0-dimensional persistence diagram of the Rips filtration of the\nhigh-dimensional data, ensuring that the filtrations generate the same\nconnected components when applied to the original as well as projected data.\nThe presented case studies show that the topological guarantee provided by\nTopoMap not only brings confidence to the visual analytic process but also can\nbe used to assist in the assessment of other projection methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 08:30:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Doraiswamy", "Harish", ""], ["Tierny", "Julien", ""], ["Silva", "Paulo J. S.", ""], ["Nonato", "Luis Gustavo", ""], ["Silva", "Claudio", ""]]}, {"id": "2009.01649", "submitter": "Anav Mehta", "authors": "Anav Mehta", "title": "Augmented Reality Chess Analyzer (ARChessAnalyzer): In-Device Inference\n  of Physical Chess Game Positions through Board Segmentation and Piece\n  Recognition using Convolutional Neural Network", "comments": "arXiv admin note: text overlap with arXiv:1807.04355 by other authors", "journal-ref": "Emerging Investigators Journal, 2020\n  https://tinyurl.com/anavmehtaarchess", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chess game position analysis is important in improving ones game. It requires\nentry of moves into a chess engine which is, cumbersome and error prone. We\npresent ARChessAnalyzer, a complete pipeline from live image capture of a\nphysical chess game, to board and piece recognition, to move analysis and\nfinally to Augmented Reality (AR) overlay of the chess diagram position and\nmove on the physical board. ARChessAnalyzer is like a scene analyzer - it uses\nan ensemble of traditional image and vision techniques to segment the scene (ie\nthe chess game) and uses Convolution Neural Networks (CNNs) to predict the\nsegmented pieces and combine it together to analyze the game. This paper\nadvances the state of the art in the first of its kind end to end integration\nof robust detection and segmentation of the board, chess piece detection using\nthe fine-tuned AlexNet CNN and chess engine analyzer in a handheld device app.\nThe accuracy of the entire chess position prediction pipeline is 93.45\\% and\ntakes 3-4.5sec from live capture to AR overlay. We also validated our\nhypothesis that ARChessAnalyzer, is faster at analysis than manual entry for\nall board positions for valid outcomes. Our hope is that the instantaneous\nfeedback this app provides will help chess learners worldwide at all levels\nimprove their game.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 20:05:06 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Mehta", "Anav", ""]]}, {"id": "2009.01697", "submitter": "Roza G. Bayrak", "authors": "Roza G. Bayrak, Nhung Hoang, Colin B. Hansen, Catie Chang, Matthew\n  Berger", "title": "PRAGMA: Interactively Constructing Functional Brain Parcellations", "comments": "IEEE VIS 2020 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prominent goal of neuroimaging studies is mapping the human brain, in order\nto identify and delineate functionally-meaningful regions and elucidate their\nroles in cognitive behaviors. These brain regions are typically represented by\natlases that capture general trends over large populations. Despite being\nindispensable to neuroimaging experts, population-level atlases do not capture\nindividual differences in functional organization. In this work, we present an\ninteractive visualization method, PRAGMA, that allows domain experts to derive\nscan-specific parcellations from established atlases. PRAGMA features a\nuser-driven, hierarchical clustering scheme for defining temporally correlated\nparcels in varying granularity. The visualization design supports the user in\nmaking decisions on how to perform clustering, namely when to expand, collapse,\nor merge parcels. This is accomplished through a set of linked and coordinated\nviews for understanding the user's current hierarchy, assessing intra-cluster\nvariation, and relating parcellations to an established atlas. We assess the\neffectiveness of PRAGMA through a user study with four neuroimaging domain\nexperts, where our results show that PRAGMA shows the potential to enable\nexploration of individualized and state-specific brain parcellations and to\noffer interesting insights into functional brain networks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 14:22:37 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Bayrak", "Roza G.", ""], ["Hoang", "Nhung", ""], ["Hansen", "Colin B.", ""], ["Chang", "Catie", ""], ["Berger", "Matthew", ""]]}, {"id": "2009.01698", "submitter": "Radek O\\v{s}lej\\v{s}ek", "authors": "Michal Beran and Frantisek Hrdina and Daniel Kouril and Radek Oslejsek\n  and Kristina Zakopcanova", "title": "Exploratory Analysis of File System Metadata for Rapid Investigation of\n  Security Incidents", "comments": null, "journal-ref": null, "doi": "10.1109/VizSec51108.2020.00008", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Investigating cybersecurity incidents requires in-depth knowledge from the\nanalyst. Moreover, the whole process is demanding due to the vast data volumes\nthat need to be analyzed. While various techniques exist nowadays to help with\nparticular tasks of the analysis, the process as a whole still requires a lot\nof manual activities and expert skills. We propose an approach that allows the\nanalysis of disk snapshots more efficiently and with lower demands on expert\nknowledge. Following a user-centered design methodology, we implemented an\nanalytical tool to guide analysts during security incident investigations. The\nviability of the solution was validated by an evaluation conducted with members\nof different security teams.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 14:23:22 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 08:11:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Beran", "Michal", ""], ["Hrdina", "Frantisek", ""], ["Kouril", "Daniel", ""], ["Oslejsek", "Radek", ""], ["Zakopcanova", "Kristina", ""]]}, {"id": "2009.01747", "submitter": "Alyxander Burns", "authors": "Alyxander Burns, Cindy Xiong, Steven Franconeri, Alberto Cairo, Narges\n  Mahyar", "title": "How to evaluate data visualizations across different levels of\n  understanding", "comments": "8 pages, 3 figures, accepted for presentation at BELIV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding a visualization is a multi-level process. A reader must extract\nand extrapolate from numeric facts, understand how those facts apply to both\nthe context of the data and other potential contexts, and draw or evaluate\nconclusions from the data. A well-designed visualization should support each of\nthese levels of understanding. We diagnose levels of understanding of\nvisualized data by adapting Bloom's taxonomy, a common framework from the\neducation literature. We describe each level of the framework and provide\nexamples for how it can be applied to evaluate the efficacy of data\nvisualizations along six levels of knowledge acquisition - knowledge,\ncomprehension, application, analysis, synthesis, and evaluation. We present\nthree case studies showing that this framework expands on existing methods to\ncomprehensively measure how a visualization design facilitates a viewer's\nunderstanding of visualizations. Although Bloom's original taxonomy suggests a\nstrong hierarchical structure for some domains, we found few examples of\ndependent relationships between performance at different levels for our three\ncase studies. If this level-independence holds across new tested\nvisualizations, the taxonomy could serve to inspire more targeted evaluations\nof levels of understanding that are relevant to a communication goal.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 15:21:23 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Burns", "Alyxander", ""], ["Xiong", "Cindy", ""], ["Franconeri", "Steven", ""], ["Cairo", "Alberto", ""], ["Mahyar", "Narges", ""]]}, {"id": "2009.01785", "submitter": "Michael Oppermann", "authors": "Michael Oppermann and Tamara Munzner", "title": "Data-First Visualization Design Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of a data-first design study which is triggered by\nthe acquisition of real-world data instead of specific stakeholder analysis\nquestions. We propose an adaptation of the design study methodology framework\nto provide practical guidance and to aid transferability to other data-first\ndesign processes. We discuss opportunities and risks by reflecting on two of\nour own data-first design studies. We review 64 previous design studies and\nidentify 16 of them as edge cases with characteristics that may indicate a\ndata-first design process in action.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 16:50:25 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Oppermann", "Michael", ""], ["Munzner", "Tamara", ""]]}, {"id": "2009.01818", "submitter": "Matej Hoffmann", "authors": "Hagen Lehmann and Adam Rojik and Matej Hoffmann", "title": "Should a small robot have a small personal space? Investigating personal\n  spatial zones and proxemic behavior in human-robot interaction", "comments": "8 pages, 6 figures", "journal-ref": "CognitIve RobotiCs for intEraction (CIRCE) Workshop at IEEE\n  International Conference On Robot and Human Interactive Communication\n  (RO-MAN) 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first study in a series of proxemics experiments\nconcerned with the role of personal spatial zones in human-robot interaction.\nIn the study 40 participants approached a NAO robot positioned approximately at\nparticipants' eye level and entered different social zones around the robot\n(personal and intimate space). When the robot perceived the approaching person\nentering its personal space, it started gazing at the participant, and upon the\nintrusion of its intimate space it leaned back. Our research questions were:\n(1) given the small size of the robot (58 cm tall), will people expect its\nsocial zones to shrink by its size? (2) Will the robot behaviors be interpreted\nas appropriate social behaviors? We found that the average approach distance of\nthe participants was 48 cm, which represents the inner limit of the human-size\npersonal zone (45-120 cm), but is outside of the personal zone scaled to robot\nsize (16-42 cm). This suggests that most participants did not (fully) scale\ndown the extent of these zones to the robot size. We also found that the\nleaning back behavior of the robot was correctly interpreted by most\nparticipants as the robot's reaction to the intrusion of its personal space;\nhowever, our implementation of the behavior was often perceived as\n\"unfriendly\". We will discuss this and other limitations of the study in\ndetail. Additionally we found positive correlations between participants'\npersonality traits, Godspeed Questionnaire subscales, and the average approach\ndistance. The technical contribution of this work is the real-time perception\nof 25 keypoints on the human body using a single compact RGB-D camera and the\nuse of these points for accurate interpersonal distance estimation and as\ngazing targets for the robot.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 17:34:35 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Lehmann", "Hagen", ""], ["Rojik", "Adam", ""], ["Hoffmann", "Matej", ""]]}, {"id": "2009.01921", "submitter": "Suyun Bae", "authors": "Suyun Bae, Federico Rossi, Joshua Vander Hook, Scott Davidoff, and\n  Kwan-Liu Ma", "title": "A Visual Analytics Approach to Debugging Cooperative, Autonomous\n  Multi-Robot Systems' Worldviews", "comments": "To appear in IEEE Conference on Visual Analytics Science and\n  Technology (VAST) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous multi-robot systems, where a team of robots shares information to\nperform tasks that are beyond an individual robot's abilities, hold great\npromise for a number of applications, such as planetary exploration missions.\nEach robot in a multi-robot system that uses the shared-world coordination\nparadigm autonomously schedules which robot should perform a given task, and\nwhen, using its worldview--the robot's internal representation of its belief\nabout both its own state, and other robots' states. A key problem for operators\nis that robots' worldviews can fall out of sync (often due to weak\ncommunication links), leading to desynchronization of the robots' scheduling\ndecisions and inconsistent emergent behavior (e.g., tasks not performed, or\nperformed by multiple robots). Operators face the time-consuming and difficult\ntask of making sense of the robots' scheduling decisions, detecting\nde-synchronizations, and pinpointing the cause by comparing every robot's\nworldview. To address these challenges, we introduce MOSAIC Viewer, a visual\nanalytics system that helps operators (i) make sense of the robots' schedules\nand (ii) detect and conduct a root cause analysis of the robots' desynchronized\nworldviews. Over a year-long partnership with roboticists at the NASA Jet\nPropulsion Laboratory, we conduct a formative study to identify the necessary\nsystem design requirements and a qualitative evaluation with 12 roboticists. We\nfind that MOSAIC Viewer is faster- and easier-to-use than the users' current\napproaches, and it allows them to stitch low-level details to formulate a\nhigh-level understanding of the robots' schedules and detect and pinpoint the\ncause of the desynchronized worldviews.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 21:01:02 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Bae", "Suyun", ""], ["Rossi", "Federico", ""], ["Hook", "Joshua Vander", ""], ["Davidoff", "Scott", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2009.01966", "submitter": "Akira Matsui", "authors": "Akira Matsui, Emilio Ferrara, Fred Morstatter, Andres Abeliuk, Aram\n  Galstyan", "title": "Leveraging Clickstream Trajectories to Reveal Low-Quality Workers in\n  Crowdsourced Forecasting Platforms", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdwork often entails tackling cognitively-demanding and time-consuming\ntasks. Crowdsourcing can be used for complex annotation tasks, from medical\nimaging to geospatial data, and such data powers sensitive applications, such\nas health diagnostics or autonomous driving. However, the existence and\nprevalence of underperforming crowdworkers is well-recognized, and can pose a\nthreat to the validity of crowdsourcing. In this study, we propose the use of a\ncomputational framework to identify clusters of underperforming workers using\nclickstream trajectories. We focus on crowdsourced geopolitical forecasting.\nThe framework can reveal different types of underperformers, such as workers\nwith forecasts whose accuracy is far from the consensus of the crowd, those who\nprovide low-quality explanations for their forecasts, and those who simply\ncopy-paste their forecasts from other users. Our study suggests that\nclickstream clustering and analysis are fundamental tools to diagnose the\nperformance of crowdworkers in platforms leveraging the wisdom of crowds.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 00:26:38 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Matsui", "Akira", ""], ["Ferrara", "Emilio", ""], ["Morstatter", "Fred", ""], ["Abeliuk", "Andres", ""], ["Galstyan", "Aram", ""]]}, {"id": "2009.02005", "submitter": "Tarik Crnovrsanin", "authors": "Tarik Crnovrsanin, Shilpika, Senthil Chandrasegaran, and Kwan-Liu Ma", "title": "Staged Animation Strategies for Online Dynamic Networks", "comments": "IEEE VIS InfoVis 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic networks -- networks that change over time -- can be categorized into\ntwo types: offline dynamic networks, where all states of the network are known,\nand online dynamic networks, where only the past states of the network are\nknown. Research on staging animated transitions in dynamic networks has focused\nmore on offline data, where rendering strategies can take into account past and\nfuture states of the network. Rendering online dynamic networks is a more\nchallenging problem since it requires a balance between timeliness for\nmonitoring tasks -- so that the animations do not lag too far behind the events\n-- and clarity for comprehension tasks -- to minimize simultaneous changes that\nmay be difficult to follow. To illustrate the challenges placed by these\nrequirements, we explore three strategies to stage animations for online\ndynamic networks: time-based, event-based, and a new hybrid approach that we\nintroduce by combining the advantages of the first two. We illustrate the\nadvantages and disadvantages of each strategy in representing low- and\nhigh-throughput data and conduct a user study involving monitoring and\ncomprehension of dynamic networks. We also conduct a follow-up, a think-aloud\nstudy combining monitoring and comprehension with experts in dynamic network\nvisualization. Our findings show that animation staging strategies that\nemphasize comprehension do better for participant response times and accuracy.\nHowever, the notion of ``comprehension'' is not always clear when it comes to\ncomplex changes in highly dynamic networks, requiring some iteration in staging\nthat the hybrid approach affords. Based on our results, we make recommendations\nfor balancing event-based and time-based parameters for our hybrid approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 04:26:05 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Crnovrsanin", "Tarik", ""], ["Shilpika", "", ""], ["Chandrasegaran", "Senthil", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2009.02057", "submitter": "Matthias Miller", "authors": "Daniel F\\\"urst, Matthias Miller, Daniel Keim, Alexandra Bonnici, Hanna\n  Sch\\\"afer, Mennatallah El-Assady", "title": "Augmenting Sheet Music with Rhythmic Fingerprints", "comments": "6 pages, 1 page references, 3 pages appendix, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we bridge the gap between visualization and musicology by\nfocusing on rhythm analysis tasks, which are tedious due to the complex visual\nencoding of the well-established Common Music Notation (CMN). Instead of\nreplacing the CMN, we augment sheet music with rhythmic fingerprints to\nmitigate the complexity originating from the simultaneous encoding of musical\nfeatures. The proposed visual design exploits music theory concepts such as the\nrhythm tree to facilitate the understanding of rhythmic information.\nJuxtaposing sheet music and the rhythmic fingerprints maintains the connection\nto the familiar representation. To investigate the usefulness of the rhythmic\nfingerprint design for identifying and comparing rhythmic patterns, we\nconducted a controlled user study with four experts and four novices. The\nresults show that the rhythmic fingerprints enable novice users to recognize\nrhythmic patterns that only experts can identify using non-augmented sheet\nmusic.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 08:24:22 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["F\u00fcrst", "Daniel", ""], ["Miller", "Matthias", ""], ["Keim", "Daniel", ""], ["Bonnici", "Alexandra", ""], ["Sch\u00e4fer", "Hanna", ""], ["El-Assady", "Mennatallah", ""]]}, {"id": "2009.02063", "submitter": "Oren Mishali", "authors": "Moshe Schorr, Oren Mishali, Benny Kimelfeld, Ophir M\\\"unz-Manor", "title": "ViS-\\'A-ViS : Detecting Similar Patterns in Annotated Literary Text", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a web-based system called ViS-\\'A-ViS aiming to assist literary\nscholars in detecting repetitive patterns in an annotated textual corpus.\nPattern detection is made possible using distant reading visualizations that\nhighlight potentially interesting patterns. In addition, the system uses\ntime-series alignment algorithms, and in particular, dynamic time warping\n(DTW), to detect patterns automatically. We present a case-study where an\nancient Hebrew poetry corpus was manually annotated with figurative language\ndevices as metaphors and similes and then loaded into the system. Preliminary\nresults confirm the effectiveness of the system in analyzing the annotated data\nand in detecting literary patterns and similarities.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 08:33:38 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Schorr", "Moshe", ""], ["Mishali", "Oren", ""], ["Kimelfeld", "Benny", ""], ["M\u00fcnz-Manor", "Ophir", ""]]}, {"id": "2009.02078", "submitter": "Heungseok Park", "authors": "Heungseok Park, Yoonsoo Nam, Ji-Hoon Kim, Jaegul Choo", "title": "HyperTendril: Visual Analytics for User-Driven Hyperparameter\n  Optimization of Deep Neural Networks", "comments": "Will be presented at IEEE VAST 2020 and published in IEEE\n  Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030380", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mitigate the pain of manually tuning hyperparameters of deep neural\nnetworks, automated machine learning (AutoML) methods have been developed to\nsearch for an optimal set of hyperparameters in large combinatorial search\nspaces. However, the search results of AutoML methods significantly depend on\ninitial configurations, making it a non-trivial task to find a proper\nconfiguration. Therefore, human intervention via a visual analytic approach\nbears huge potential in this task. In response, we propose HyperTendril, a\nweb-based visual analytics system that supports user-driven hyperparameter\ntuning processes in a model-agnostic environment. HyperTendril takes a novel\napproach to effectively steering hyperparameter optimization through an\niterative, interactive tuning procedure that allows users to refine the search\nspaces and the configuration of the AutoML method based on their own insights\nfrom given results. Using HyperTendril, users can obtain insights into the\ncomplex behaviors of various hyperparameter search algorithms and diagnose\ntheir configurations. In addition, HyperTendril supports variable importance\nanalysis to help the users refine their search spaces based on the analysis of\nrelative importance of different hyperparameters and their interaction effects.\nWe present the evaluation demonstrating how HyperTendril helps users steer\ntheir tuning processes via a longitudinal user study based on the analysis of\ninteraction logs and in-depth interviews while we deploy our system in a\nprofessional industrial environment.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 09:11:08 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 08:29:56 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Park", "Heungseok", ""], ["Nam", "Yoonsoo", ""], ["Kim", "Ji-Hoon", ""], ["Choo", "Jaegul", ""]]}, {"id": "2009.02094", "submitter": "Alejandro Benito-Santos", "authors": "Alejandro Benito-Santos and Roberto Ther\\'on", "title": "GlassViz: Visualizing Automatically-Extracted Entry Points for Exploring\n  Scientific Corpora in Problem-Driven Visualization Research", "comments": "Accepted to IEEEVIS 2020 short papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report the development of a model and a proof-of-concept\nvisual text analytics (VTA) tool to enhance documentdiscovery in a\nproblem-driven visualization research (PDVR) con-text. The proposed model\ncaptures the cognitive model followed bydomain and visualization experts by\nanalyzing the interdisciplinarycommunication channel as represented by keywords\nfound in twodisjoint collections of research papers. High distributional\ninter-collection similarities are employed to build informative\nkeywordassociations that serve as entry points to drive the exploration of\nalarge document corpus. Our approach is demonstrated in the contextof research\non visualization for the digital humanities.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 10:21:23 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 21:38:55 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Benito-Santos", "Alejandro", ""], ["Ther\u00f3n", "Roberto", ""]]}, {"id": "2009.02110", "submitter": "Jose A. Gonzalez-Lopez", "authors": "Jose A. Gonzalez-Lopez, Alejandro Gomez-Alanis, Juan M.\n  Mart\\'in-Do\\~nas, Jos\\'e L. P\\'erez-C\\'ordoba, Angel M. Gomez", "title": "Silent Speech Interfaces for Speech Restoration: A Review", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3026579", "report-no": null, "categories": "eess.AS cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This review summarises the status of silent speech interface (SSI) research.\nSSIs rely on non-acoustic biosignals generated by the human body during speech\nproduction to enable communication whenever normal verbal communication is not\npossible or not desirable. In this review, we focus on the first case and\npresent latest SSI research aimed at providing new alternative and augmentative\ncommunication methods for persons with severe speech disorders. SSIs can employ\na variety of biosignals to enable silent communication, such as\nelectrophysiological recordings of neural activity, electromyographic (EMG)\nrecordings of vocal tract movements or the direct tracking of articulator\nmovements using imaging techniques. Depending on the disorder, some sensing\ntechniques may be better suited than others to capture speech-related\ninformation. For instance, EMG and imaging techniques are well suited for\nlaryngectomised patients, whose vocal tract remains almost intact but are\nunable to speak after the removal of the vocal folds, but fail for severely\nparalysed individuals. From the biosignals, SSIs decode the intended message,\nusing automatic speech recognition or speech synthesis algorithms. Despite\nconsiderable advances in recent years, most present-day SSIs have only been\nvalidated in laboratory settings for healthy users. Thus, as discussed in this\npaper, a number of challenges remain to be addressed in future research before\nSSIs can be promoted to real-world applications. If these issues can be\naddressed successfully, future SSIs will improve the lives of persons with\nsevere speech impairments by restoring their communication capabilities.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:05:50 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 08:43:47 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 08:50:17 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Gonzalez-Lopez", "Jose A.", ""], ["Gomez-Alanis", "Alejandro", ""], ["Mart\u00edn-Do\u00f1as", "Juan M.", ""], ["P\u00e9rez-C\u00f3rdoba", "Jos\u00e9 L.", ""], ["Gomez", "Angel M.", ""]]}, {"id": "2009.02119", "submitter": "Youngwoo Yoon", "authors": "Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee,\n  Jaehong Kim, Geehyuk Lee", "title": "Speech Gesture Generation from the Trimodal Context of Text, Audio, and\n  Speaker Identity", "comments": "16 pages; ACM Transactions on Graphics (SIGGRAPH Asia 2020)", "journal-ref": null, "doi": "10.1145/3414685.3417838", "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For human-like agents, including virtual avatars and social robots, making\nproper gestures while speaking is crucial in human--agent interaction.\nCo-speech gestures enhance interaction experiences and make the agents look\nalive. However, it is difficult to generate human-like gestures due to the lack\nof understanding of how people gesture. Data-driven approaches attempt to learn\ngesticulation skills from human demonstrations, but the ambiguous and\nindividual nature of gestures hinders learning. In this paper, we present an\nautomatic gesture generation model that uses the multimodal context of speech\ntext, audio, and speaker identity to reliably generate gestures. By\nincorporating a multimodal context and an adversarial training scheme, the\nproposed model outputs gestures that are human-like and that match with speech\ncontent and rhythm. We also introduce a new quantitative evaluation metric for\ngesture generation models. Experiments with the introduced metric and\nsubjective human evaluation showed that the proposed gesture generation model\nis better than existing end-to-end generation models. We further confirm that\nour model is able to work with synthesized audio in a scenario where contexts\nare constrained, and show that different gesture styles can be generated for\nthe same speech by specifying different speaker identities in the style\nembedding space that is learned from videos of various speakers. All the code\nand data is available at\nhttps://github.com/ai4r/Gesture-Generation-from-Trimodal-Context.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:42:45 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Yoon", "Youngwoo", ""], ["Cha", "Bok", ""], ["Lee", "Joo-Haeng", ""], ["Jang", "Minsu", ""], ["Lee", "Jaeyeon", ""], ["Kim", "Jaehong", ""], ["Lee", "Geehyuk", ""]]}, {"id": "2009.02132", "submitter": "Adam Pantanowitz", "authors": "Adam Pantanowitz, Kimoon Kim, Chelsey Chewins, Isabel N. K. Tollman,\n  David M. Rubin", "title": "Addressing the eye-fixation problem in gaze tracking for human computer\n  interface using the Vestibulo-ocular Reflex", "comments": "16 pages, 4 figures, 2 tables", "journal-ref": null, "doi": "10.1016/j.imu.2020.100488", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A custom head-mounted system to track smooth eye movements for control of a\nmouse cursor is implemented and evaluated. The system comprises a head-mounted\ninfrared camera, an infrared light source, and a computer. Software-based image\nprocessing techniques, implemented in Microsoft Visual Studio, OpenCV, and\nPupil, detect the pupil position and direction of pupil movement in near\nreal-time. The identified direction is used to determine the desired\npositioning of the cursor, and the cursor moves towards the target. Two users\nparticipated in three tests to quantify the differences between incremental\ntracking of smooth eye movement resulting from the Vestibulo-ocular Reflex\nversus step-change tracking of saccadic eye movement. Tracking smooth eye\nmovements was four times more accurate than tracking saccadic eye movements,\nwith an average position resolution of 0.80 cm away from the target. In\ncontrast, tracking saccadic eye movements was measured with an average position\nresolution of 3.21 cm. Using the incremental tracking of smooth eye movements,\nthe user was able to place the cursor within a target as small as a 9 x 9 pixel\nsquare 90 % of the time. However, when using the step change tracking of\nsaccadic eye movements, the user was unable to position the cursor within the 9\nx 9 pixel target. The average time for the incremental tracking of smooth eye\nmovements to track a target was 6.45 s, whereas for the step change tracking of\nsaccadic eye movements, it was 2.61 s.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 19:52:30 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 11:46:34 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 09:21:25 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Pantanowitz", "Adam", ""], ["Kim", "Kimoon", ""], ["Chewins", "Chelsey", ""], ["Tollman", "Isabel N. K.", ""], ["Rubin", "David M.", ""]]}, {"id": "2009.02242", "submitter": "Taylor Arnold", "authors": "Taylor Arnold, Nathaniel Ayers, Justin Madron, Robert Nelson, Lauren\n  Tilton", "title": "Visualizing a Large Spatiotemporal Collection of Historic Photography\n  with a Generous Interface", "comments": "Presented at 5th Workshop on Visualization for the Digital Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Museums, libraries, and other cultural institutions continue to prioritize\nand build web-based visualization systems that increase access and discovery to\ndigitized archives. Prominent examples exist that illustrate impressive\nvisualizations of a particular feature of a collection. For example,\ninteractive maps showing geographic spread or timelines capturing the temporal\naspects of collections. By way of a case study, this paper presents a new\nweb-based visualization system that allows users to simultaneously explore a\nlarge collection of images along several different dimensions---spatial,\ntemporal, visual, textual, and through additional metadata fields including the\nphotographer name---guided by the concept of generous interfaces. The case\nstudy is a complete redesign of a previously released digital, public\nhumanities project called Photogrammar (2014). The paper highlights the\nredesign's interactive visualizations that are now possible by the affordances\nof newly available software. All of the code is open-source in order to allow\nfor re-use of the codebase to other collections with a similar structure.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 15:19:54 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Arnold", "Taylor", ""], ["Ayers", "Nathaniel", ""], ["Madron", "Justin", ""], ["Nelson", "Robert", ""], ["Tilton", "Lauren", ""]]}, {"id": "2009.02288", "submitter": "Tomas Vancisin", "authors": "Tomas Vancisin, Mary Orr, Uta Hinrichs", "title": "Externalizing Transformations of Historical Documents: Opportunities for\n  Provenance-Driven Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcription, annotation, digitization and/or visualization are common\ntransformations that historical documents such as national records, birth/death\nregisters, university records, letters or books undergo. Reasons for those\ntransformations span from the (physical) protection of the original materials\nto disclosure of 'hidden' information or patterns within the documents. Even\nthough such transformations bring new insights and perspectives on the\ndocuments, they also modify the documents' content, structure, and/or\nartifactual form and thus, occlude prior knowledge and interpretation. When it\ncomes to visualization as a means to transform historical documents from\nwritten to abstract visual form, there is typically little acknowledgment or\neven understanding of the previous transformation steps these documents have\ngone through. The 'tremendous rhetorical force' of visualization, we argue,\nshould not be at the expense of the multiple pasts, contexts, and curators that\nare inherent in historical record collections. Rather, the urgent question for\nthe fields of visualization and the (digital) humanities is how to better\nsupport awareness of these multiple layers of interpretation and the people\nbehind them when representing historical documents. We begin to address this\nquestion based on a collection of historical university records by (a)\ninvestigating common transformation processes of historical documents, and (b)\ndiscussing opportunities and challenges for making such transformations\ntransparent through what we call 'provenance-driven visualization'; the idea\nfor a visualization that makes visible the layers of transformation (including\ninterpretation, re-structuring, and curation) inherent in historical documents.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 16:28:31 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Vancisin", "Tomas", ""], ["Orr", "Mary", ""], ["Hinrichs", "Uta", ""]]}, {"id": "2009.02306", "submitter": "S{\\o}ren Knudsen", "authors": "Tatiana Losev, Sarah Storteboom, Sheelagh Carpendale, S{\\o}ren Knudsen", "title": "Distributed Synchronous Visualization Design: Challenges and Strategies", "comments": "Final published version", "journal-ref": "2020 IEEE Workshop on Evaluation and Beyond - Methodological\n  Approaches to Visualization (BELIV), Salt Lake City, UT, USA, 2020, pp. 1-10", "doi": "10.1109/BELIV51497.2020.00008", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reflect on our experiences as designers of COVID-19 data visualizations\nworking in a distributed synchronous design space during the pandemic. This is\nespecially relevant as the pandemic posed new challenges to distributed\ncollaboration amidst civic lockdown measures and an increased dependency on\nspatially distributed teamwork across almost all sectors. Working from home\nbeing 'the new normal', we explored potential solutions for collaborating and\nprototyping remotely from our own homes using the existing tools at our\ndisposal. Since members of our cross-disciplinary team had different technical\nskills, we used a range of synchronous remote design tools and methods. We\naimed to preserve the richness of co-located collaboration such as face-to-face\nphysical presence, body gestures, facial expressions, and the making and\nsharing of physical artifacts. While meeting over Zoom, we sketched on paper\nand used digital collaboration tools, such as Miro and Google Docs. Using an\nauto-ethnographic approach, we articulate our challenges and strategies\nthroughout the process, providing useful insights about synchronous distributed\ncollaboration.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 17:12:40 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 14:25:39 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 18:52:36 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Losev", "Tatiana", ""], ["Storteboom", "Sarah", ""], ["Carpendale", "Sheelagh", ""], ["Knudsen", "S\u00f8ren", ""]]}, {"id": "2009.02348", "submitter": "Alejandro Benito-Santos", "authors": "Alejandro Benito-Santos and Roberto Ther\\'on", "title": "Pilaster: A Collection of Citation Metadata Extracted From Publications\n  on Visualization for the Digital Humanities", "comments": "Accepted to VIS4DH Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Pilaster (https://visusal.github.io/pilaster/), a\ncollection of citation metadata extracted from publications in visualization\nfor the digital humanities. The collection is generated from a seed set of\nrelevant publications from which we extracted cited works, including journal\nand conference papers, books, theses, or blog posts, among other resources. The\nmain aim of this work revolves around three main points: first, the collection\nmay serve as an entry point to the discipline for digital humanists and\nvisualization scholars without previous experience in the field. Second,\nPilaster can be regarded as a meeting point for more established visualization\nor humanities scholars seeking to collaborate in the development of novel\nresearch ideas and related visualization design studies in the context of the\nhumanities. Third, and given the large amount of visualization design spaces\nthat were captured, we believe the dataset has the potential to become the\nstarting point for future studies aimed at understanding the particularities of\nproblem-driven visualization research in this and other contexts.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 18:17:13 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Benito-Santos", "Alejandro", ""], ["Ther\u00f3n", "Roberto", ""]]}, {"id": "2009.02373", "submitter": "Stephen Kasica", "authors": "Stephen Kasica, Charles Berret, and Tamara Munzner", "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling\n  From An Artifact Study of Computational Journalism", "comments": "To appear, IEEE TVCG (Proc. InfoVis 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the many journalists who use data and computation to report the news,\ndata wrangling is an integral part of their work.Despite an abundance of\nliterature on data wrangling in the context of enterprise data analysis, little\nis known about the specific operations, processes, and pain points journalists\nencounter while performing this tedious, time-consuming task. To better\nunderstand the needs of this user group, we conduct a technical observation\nstudy of 50 public repositories of data and analysis code authored by 33\nprofessional journalists at 26 news organizations. We develop two detailed and\ncross-cutting taxonomies of data wrangling in computational journalism, for\nactions and for processes. We observe the extensive use of multiple tables, a\nnotable gap in previous wrangling analyses. We develop a concise, actionable\nframework for general multi-table data wrangling that includes wrangling\noperations documented in our taxonomy that are without clear parallels in other\nwork. This framework, the first to incorporate tablesas first-class objects,\nwill support future interactive wrangling tools for both computational\njournalism and general-purpose use. We assess the generative and descriptive\npower of our framework through discussion of its relationship to our set of\ntaxonomies.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 19:34:09 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 23:14:24 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Kasica", "Stephen", ""], ["Berret", "Charles", ""], ["Munzner", "Tamara", ""]]}, {"id": "2009.02374", "submitter": "Richard Brath", "authors": "Richard Brath", "title": "Literal Encoding: Text is a first-class data encoding", "comments": "6 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital humanities are rooted in text analysis. However, most visualization\nparadigms use only categoric, ordered or quantitative data. Literal text must\nbe considered a base data type to encode into visualizations. Literal text\noffers functional, perceptual, cognitive, semantic and operational benefits.\nThese are briefly illustrated with a subset of sample visualizations focused on\nsemantic word sequences, indicating benefits over standard graphs, maps,\ntreemaps, bar charts and narrative layouts.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 19:42:40 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Brath", "Richard", ""]]}, {"id": "2009.02384", "submitter": "Andrew McNutt", "authors": "Andrew McNutt, Agatha Kim, Sergio Elahi, Kazutaka Takahashi", "title": "Supporting Expert Close Analysis of Historical Scientific Writings: A\n  Case Study for Near-by Reading", "comments": "6 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant reading methodologies make use of computational processes to aid in\nthe analysis of large text corpora which might not be pliable to traditional\nmethods of scholarly analysis due to their volume. While these methods have\nbeen applied effectively to a variety of types of texts and contexts, they can\nleave unaddressed the needs of scholars in the humanities disciplines like\nhistory, who often engage in close reading of sources. Complementing the close\nanalysis of texts with some of the tools of distant reading, such as\nvisualization, can resolve some of the issues. We focus on a particular\ncategory of this intersection---which we refer to as near-by reading---wherein\nan expert engages in a computer-mediated analysis of a text with which they are\nfamiliar. We provide an example of this approach by developing a visual\nanalysis application for the near-by reading of 19th-century scientific\nwritings by J. W. von Goethe and A. P. de Candolle. We show that even the most\nformal and public texts, such as scientific treatises, can reveal unexpressed\npersonal biases and philosophies that the authors themselves might not have\nrecognized.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 20:26:07 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["McNutt", "Andrew", ""], ["Kim", "Agatha", ""], ["Elahi", "Sergio", ""], ["Takahashi", "Kazutaka", ""]]}, {"id": "2009.02397", "submitter": "Javad Rahimipour Anaraki", "authors": "Javad Rahimipour Anaraki, Silvia Orlandi, Tom Chau", "title": "A Deep Learning Approach to Tongue Detection for Pediatric Population", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Children with severe disabilities and complex communication needs face\nlimitations in the usage of access technology (AT) devices. Conventional ATs\n(e.g., mechanical switches) can be insufficient for nonverbal children and\nthose with limited voluntary motion control. Automatic techniques for the\ndetection of tongue gestures represent a promising pathway. Previous studies\nhave shown the robustness of tongue detection algorithms on adult participants,\nbut further research is needed to use these methods with children. In this\nstudy, a network architecture for tongue-out gesture recognition was\nimplemented and evaluated on videos recorded in a naturalistic setting when\nchildren were playing a video-game. A cascade object detector algorithm was\nused to detect the participants' faces, and an automated classification scheme\nfor tongue gesture detection was developed using a convolutional neural network\n(CNN). In evaluation experiments conducted, the network was trained using\nadults and children's images. The network classification accuracy was evaluated\nusing leave-one-subject-out cross-validation. Preliminary classification\nresults obtained from the analysis of videos of five typically developing\nchildren showed an accuracy of up to 99% in predicting tongue-out gestures.\nMoreover, we demonstrated that using only children data for training the\nclassifier yielded better performance than adult's one supporting the need for\npediatric tongue gesture datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 21:04:57 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 22:41:39 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 19:43:29 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Anaraki", "Javad Rahimipour", ""], ["Orlandi", "Silvia", ""], ["Chau", "Tom", ""]]}, {"id": "2009.02441", "submitter": "Angus Forbes", "authors": "Oskar Elek, Joseph N. Burchett, J. Xavier Prochaska, Angus G. Forbes", "title": "Polyphorm: Structural Analysis of Cosmological Datasets via Interactive\n  Physarum Polycephalum Visualization", "comments": "Accepted to IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Polyphorm, an interactive visualization and model\nfitting tool that provides a novel approach for investigating cosmological\ndatasets. Through a fast computational simulation method inspired by the\nbehavior of Physarum polycephalum, an unicellular slime mold organism that\nefficiently forages for nutrients, astrophysicists are able to extrapolate from\nsparse datasets, such as galaxy maps archived in the Sloan Digital Sky Survey,\nand then use these extrapolations to inform analyses of a wide range of other\ndata, such as spectroscopic observations captured by the Hubble Space\nTelescope. Researchers can interactively update the simulation by adjusting\nmodel parameters, and then investigate the resulting visual output to form\nhypotheses about the data. We describe details of Polyphorm's simulation model\nand its interaction and visualization modalities, and we evaluate Polyphorm\nthrough three scientific use cases that demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 02:45:49 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Elek", "Oskar", ""], ["Burchett", "Joseph N.", ""], ["Prochaska", "J. Xavier", ""], ["Forbes", "Angus G.", ""]]}, {"id": "2009.02458", "submitter": "Xiao Xie", "authors": "Xiao Xie, Fan Du, Yingcai Wu", "title": "A Visual Analytics Approach for Exploratory Causal Analysis:\n  Exploration, Validation, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using causal relations to guide decision making has become an essential\nanalytical task across various domains, from marketing and medicine to\neducation and social science. While powerful statistical models have been\ndeveloped for inferring causal relations from data, domain practitioners still\nlack effective visual interface for interpreting the causal relations and\napplying them in their decision-making process. Through interview studies with\ndomain experts, we characterize their current decision-making workflows,\nchallenges, and needs. Through an iterative design process, we developed a\nvisualization tool that allows analysts to explore, validate, and apply causal\nrelations in real-world decision-making scenarios. The tool provides an\nuncertainty-aware causal graph visualization for presenting a large set of\ncausal relations inferred from high-dimensional data. On top of the causal\ngraph, it supports a set of intuitive user controls for performing what-if\nanalyses and making action plans. We report on two case studies in marketing\nand student advising to demonstrate that users can effectively explore causal\nrelations and design action plans for reaching their goals.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 04:35:36 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Xie", "Xiao", ""], ["Du", "Fan", ""], ["Wu", "Yingcai", ""]]}, {"id": "2009.02459", "submitter": "Hongwei Zhou", "authors": "Hongwei (Henry) Zhou, Oskar Elek, Pranav Anand, Angus G. Forbes", "title": "Bio-inspired Structure Identification in Language Embeddings", "comments": "7 pages, 8 figures, 2 tables, Visualisation for the Digital\n  Humanities 2020. Comments: Fixed white spaces in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are a popular way to improve downstream performances in\ncontemporary language modeling. However, the underlying geometric structure of\nthe embedding space is not well understood. We present a series of explorations\nusing bio-inspired methodology to traverse and visualize word embeddings,\ndemonstrating evidence of discernible structure. Moreover, our model also\nproduces word similarity rankings that are plausible yet very different from\ncommon similarity metrics, mainly cosine similarity and Euclidean distance. We\nshow that our bio-inspired model can be used to investigate how different word\nembedding techniques result in different semantic outputs, which can emphasize\nor obscure particular interpretations in textual data.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 04:44:15 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 23:59:06 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Hongwei", "", "", "Henry"], ["Zhou", "", ""], ["Elek", "Oskar", ""], ["Anand", "Pranav", ""], ["Forbes", "Angus G.", ""]]}, {"id": "2009.02464", "submitter": "Xiao Xie", "authors": "Xiao Xie, Jiachen Wang, Hongye Liang, Dazhen Deng, Shoubin Cheng, Hui\n  Zhang, Wei Chen, Yingcai Wu", "title": "PassVizor: Toward Better Understanding of the Dynamics of Soccer Passes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In soccer, passing is the most frequent interaction between players and plays\na significant role in creating scoring chances. Experts are interested in\nanalyzing players' passing behavior to learn passing tactics, i.e., how players\nbuild up an attack with passing. Various approaches have been proposed to\nfacilitate the analysis of passing tactics. However, the dynamic changes of a\nteam's employed tactics over a match have not been comprehensively\ninvestigated. To address the problem, we closely collaborate with domain\nexperts and characterize requirements to analyze the dynamic changes of a\nteam's passing tactics. To characterize the passing tactic employed for each\nattack, we propose a topic-based approach that provides a high-level\nabstraction of complex passing behaviors. Based on the model, we propose a\nglyph-based design to reveal the multi-variate information of passing tactics\nwithin different phases of attacks, including player identity, spatial context,\nand formation. We further design and develop PassVizor, a visual analytics\nsystem, to support the comprehensive analysis of passing dynamics. With the\nsystem, users can detect the changing patterns of passing tactics and examine\nthe detailed passing process for evaluating passing tactics. We invite experts\nto conduct analysis with PassVizor and demonstrate the usability of the system\nthrough an expert interview.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 05:12:50 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Xie", "Xiao", ""], ["Wang", "Jiachen", ""], ["Liang", "Hongye", ""], ["Deng", "Dazhen", ""], ["Cheng", "Shoubin", ""], ["Zhang", "Hui", ""], ["Chen", "Wei", ""], ["Wu", "Yingcai", ""]]}, {"id": "2009.02476", "submitter": "Yun-Shiuan Chuang", "authors": "Yun-Shiuan Chuang, Xuezhou Zhang, Yuzhe Ma, Mark K. Ho, Joseph L.\n  Austerweil, Xiaojin Zhu", "title": "Using Machine Teaching to Investigate Human Assumptions when Teaching\n  Reinforcement Learners", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful teaching requires an assumption of how the learner learns - how\nthe learner uses experiences from the world to update their internal states. We\ninvestigate what expectations people have about a learner when they teach them\nin an online manner using rewards and punishment. We focus on a common\nreinforcement learning method, Q-learning, and examine what assumptions people\nhave using a behavioral experiment. To do so, we first establish a normative\nstandard, by formulating the problem as a machine teaching optimization\nproblem. To solve the machine teaching optimization problem, we use a deep\nlearning approximation method which simulates learners in the environment and\nlearns to predict how feedback affects the learner's internal states. What do\npeople assume about a learner's learning and discount rates when they teach\nthem an idealized exploration-exploitation task? In a behavioral experiment, we\nfind that people can teach the task to Q-learners in a relatively efficient and\neffective manner when the learner uses a small value for its discounting rate\nand a large value for its learning rate. However, they still are suboptimal. We\nalso find that providing people with real-time updates of how possible feedback\nwould affect the Q-learner's internal states weakly helps them teach. Our\nresults reveal how people teach using evaluative feedback and provide guidance\nfor how engineers should design machine agents in a manner that is intuitive\nfor people.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 06:32:38 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 21:54:58 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Chuang", "Yun-Shiuan", ""], ["Zhang", "Xuezhou", ""], ["Ma", "Yuzhe", ""], ["Ho", "Mark K.", ""], ["Austerweil", "Joseph L.", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "2009.02531", "submitter": "Quan Li", "authors": "Quan Li, Zhenhui Peng, Haipeng Zeng, Qiaoan Chen, Lingling Yi, Ziming\n  Wu, Xiaojuan Ma and Tianjian Chen", "title": "Friend Network as Gatekeeper: A Study of WeChat Users' Consumption of\n  Friend-Curated Contents", "comments": null, "journal-ref": "Chinese CHI 2020, April 26, 2020, Honolulu, HI, USA", "doi": "10.1145/3403676.3403679", "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media enables users to publish, disseminate, and access information\neasily. The downside is that it has fewer gatekeepers of what content is\nallowed to enter public circulation than the traditional media. In this paper,\nwe present preliminary empirical findings from WeChat, a popular messaging app\nof the Chinese, indicating that social media users leverage their friend\nnetworks collectively as latent, dynamic gatekeepers for content consumption.\nTaking a mixed-methods approach, we analyze over seven million users'\ninformation consumption behaviors on WeChat and conduct an online survey of\n$216$ users. Both quantitative and qualitative evidence suggests that friend\nnetwork indeed acts as a gatekeeper in social media. Shifting from what should\nbe produced that gatekeepers used to decide, friend network helps separate the\nworthy from the unworthy for individual information consumption, and its\nstructure and dynamics that play an important role in gatekeeping may inspire\nthe future design of socio-technical systems.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 13:15:44 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Li", "Quan", ""], ["Peng", "Zhenhui", ""], ["Zeng", "Haipeng", ""], ["Chen", "Qiaoan", ""], ["Yi", "Lingling", ""], ["Wu", "Ziming", ""], ["Ma", "Xiaojuan", ""], ["Chen", "Tianjian", ""]]}, {"id": "2009.02538", "submitter": "Quan Li", "authors": "Qiangqiang Liu, Quan Li, Chunfeng Tang, Huanbin Lin, Xiaojuan Ma and\n  Tianjian Chen", "title": "A Visual Analytics Approach to Scheduling Customized Shuttle Buses via\n  Perceiving Passengers' Travel Demands", "comments": null, "journal-ref": "IEEE VIS 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shuttle buses have been a popular means to move commuters sharing similar\norigins and destinations during periods of high travel demand. However,\nplanning and deploying reasonable, customized service bus systems becomes\nchallenging when the commute demand is rather dynamic. It is difficult, if not\nimpossible to form a reliable, unbiased estimation of user needs in such a case\nusing traditional modeling methods. We propose a visual analytics approach to\nfacilitating assessment of actual, varying travel demands and planning of night\ncustomized shuttle systems. A preliminary case study verifies the efficacy of\nour approach.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 14:22:31 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Liu", "Qiangqiang", ""], ["Li", "Quan", ""], ["Tang", "Chunfeng", ""], ["Lin", "Huanbin", ""], ["Ma", "Xiaojuan", ""], ["Chen", "Tianjian", ""]]}, {"id": "2009.02554", "submitter": "Matthew Berger", "authors": "Matthew Berger", "title": "Visually Analyzing Contextualized Embeddings", "comments": "IEEE Vis 2020, Observable notebook demo at\n  https://observablehq.com/@mattberger/visually-analyzing-contextualized-embeddings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a method for visually analyzing contextualized\nembeddings produced by deep neural network-based language models. Our approach\nis inspired by linguistic probes for natural language processing, where tasks\nare designed to probe language models for linguistic structure, such as\nparts-of-speech and named entities. These approaches are largely confirmatory,\nhowever, only enabling a user to test for information known a priori. In this\nwork, we eschew supervised probing tasks, and advocate for unsupervised probes,\ncoupled with visual exploration techniques, to assess what is learned by\nlanguage models. Specifically, we cluster contextualized embeddings produced\nfrom a large text corpus, and introduce a visualization design based on this\nclustering and textual structure - cluster co-occurrences, cluster spans, and\ncluster-word membership - to help elicit the functionality of, and relationship\nbetween, individual clusters. User feedback highlights the benefits of our\ndesign in discovering different types of linguistic structures.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 15:40:51 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Berger", "Matthew", ""]]}, {"id": "2009.02587", "submitter": "Rupayan Neogy", "authors": "Rupayan Neogy, Jonathan Zong, Arvind Satyanarayan", "title": "Representing Real-Time Multi-User Collaboration in Visualizations", "comments": "To be published IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing common ground and maintaining shared awareness amongst\nparticipants is a key challenge in collaborative visualization. For real-time\ncollaboration, existing work has primarily focused on synchronizing constituent\nvisualizations - an approach that makes it difficult for users to work\nindependently, or selectively attend to their collaborators' activity. To\naddress this gap, we introduce a design space for representing synchronous\nmulti-user collaboration in visualizations defined by two orthogonal axes:\nsituatedness, or whether collaborators' interactions are overlaid on or shown\noutside of a user's view, and specificity, or whether collaborators are\ndepicted through abstract, generic representations or through specific means\ncustomized for the given visualization. We populate this design space with a\nvariety of examples including generic and custom synchronized cursors, and user\nlegends that collect these cursors together or reproduce collaborators' views\nas thumbnails. To build common ground, users can interact with these\nrepresentations by peeking to take a quick look at a collaborator's view,\ntracking to follow along with a collaborator in real-time, and forking to\nindependently explore the visualization based on a collaborator's work. We\npresent a reference implementation of a wrapper library that converts\ninteractive Vega-Lite charts into collaborative visualizations. We find that\nour approach affords synchronous collaboration across an expressive range of\nvisual designs and interaction techniques.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 19:20:39 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Neogy", "Rupayan", ""], ["Zong", "Jonathan", ""], ["Satyanarayan", "Arvind", ""]]}, {"id": "2009.02628", "submitter": "Paul Parsons", "authors": "Paul Parsons and Colin M. Gray and Ali Baigelenov and Ian Carr", "title": "Design Judgment in Data Visualization Practice", "comments": "IEEE 2020 Visualization Conference (VIS), short papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualization is becoming an increasingly popular field of design\npractice. Although many studies have highlighted the knowledge required for\neffective data visualization design, their focus has largely been on formal\nknowledge and logical decision-making processes that can be abstracted and\ncodified. Less attention has been paid to the more situated and personal ways\nof knowing that are prevalent in all design activity. In this study, we\nconducted semi-structured interviews with data visualization practitioners\nduring which they were asked to describe the practical and situated aspects of\ntheir design processes. Using a philosophical framework of design judgment from\nNelson and Stolterman [23], we analyzed the transcripts to describe the volume\nand complex layering of design judgments that are used by data visualization\npractitioners as they describe and interrogate their work. We identify aspects\nof data visualization practice that require further investigation beyond\nnotions of rational, model- or principle-directed decision-making processes.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 02:04:41 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Parsons", "Paul", ""], ["Gray", "Colin M.", ""], ["Baigelenov", "Ali", ""], ["Carr", "Ian", ""]]}, {"id": "2009.02634", "submitter": "Paul Parsons", "authors": "Paul Parsons and Prakash Shukla", "title": "Data Visualization Practitioners' Perspectives on Chartjunk", "comments": "IEEE 2020 Visualization Conference (VIS), short papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chartjunk is a popular yet contentious topic. Previous studies have shown\nthat extreme minimalism is not always best, and that visual embellishments can\nbe useful depending on the context. While more knowledge is being developed\nregarding the effects of embellishments on users, less attention has been given\nto the perspectives of practitioners regarding how they design with\nembellishments. We conducted semi-structured interviews with 20 data\nvisualization practitioners, investigating how they understand chartjunk and\nthe factors that influence how and when they make use of embellishments. Our\ninvestigation uncovers a broad and pluralistic understanding of chartjunk among\npractitioners, and foregrounds a variety of personal and situated factors that\ninfluence the use of chartjunk beyond context. We highlight the personal nature\nof design practice, and discuss the need for more practice-led research to\nbetter understand the ways in which concepts like chartjunk are interpreted and\nused by practitioners.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 02:43:51 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Parsons", "Paul", ""], ["Shukla", "Prakash", ""]]}, {"id": "2009.02650", "submitter": "Chaoxing Huang", "authors": "Chaoxing Huang, Xuanying Zhu, Tom Gedeon", "title": "A Genetic Feature Selection Based Two-stream Neural Network for Anger\n  Veracity Recognition", "comments": "This paper has been accepted by the 27th International Conference on\n  Neural Information Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People can manipulate emotion expressions when interacting with others. For\nexample, acted anger can be expressed when stimuli is not genuinely angry with\nan aim to manipulate the observer. In this paper, we aim to examine if the\nveracity of anger can be recognized with observers' pupillary data with\ncomputational approaches. We use Genetic-based Feature Selection (GFS) methods\nto select time-series pupillary features of of observers who observe acted and\ngenuine anger of the video stimuli. We then use the selected features to train\na simple fully connected neural work and a two-stream neural network. Our\nresults show that the two-stream architecture is able to achieve a promising\nrecognition result with an accuracy of 93.58% when the pupillary responses from\nboth eyes are available. It also shows that genetic algorithm based feature\nselection method can effectively improve the classification accuracy by 3.07%.\nWe hope our work could help daily research such as human machine interaction\nand psychology studies that require emotion recognition .\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 05:52:41 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 12:43:56 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 03:03:43 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Huang", "Chaoxing", ""], ["Zhu", "Xuanying", ""], ["Gedeon", "Tom", ""]]}, {"id": "2009.02673", "submitter": "Praveen Damacharla", "authors": "Parashar Dhakal, Praveen Damacharla, Ahmad Y. Javaid, Hari K. Vege and\n  Vijay K. Devabhaktuni", "title": "IVACS: Intelligent Voice Assistant for Coronavirus Disease (COVID-19)\n  Self-Assessment", "comments": null, "journal-ref": "1st International Conference on Artificial Intelligence & Modern\n  Assistive Technology (ICAIMAT), Riyadh Saudi Arabia, November, 24-26, 2020", "doi": "10.1109/ICAIMAT51101.2020.9308013", "report-no": "9308013", "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the time of writing this paper, the world has around eleven million cases\nof COVID-19, scientifically known as severe acute respiratory syndrome\ncorona-virus 2 (SARS-COV-2). One of the popular critical steps various health\norganizations are advocating to prevent the spread of this contagious disease\nis self-assessment of symptoms. Multiple organizations have already pioneered\nmobile and web-based applications for self-assessment of COVID-19 to reduce\nthis global pandemic's spread. We propose an intelligent voice-based assistant\nfor COVID-19 self-assessment (IVACS). This interactive assistant has been built\nto diagnose the symptoms related to COVID-19 using the guidelines provided by\nthe Centers for Disease Control and Prevention (CDC) and the World Health\nOrganization (WHO). The empirical testing of the application has been performed\nwith 22 human subjects, all volunteers, using the NASA Task Load Index (TLX),\nand subjects performance accuracy has been measured. The results indicate that\nthe IVACS is beneficial to users. However, it still needs additional research\nand development to promote its widespread application.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 08:48:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Dhakal", "Parashar", ""], ["Damacharla", "Praveen", ""], ["Javaid", "Ahmad Y.", ""], ["Vege", "Hari K.", ""], ["Devabhaktuni", "Vijay K.", ""]]}, {"id": "2009.02752", "submitter": "Guohao Lan", "authors": "Dong Ma, Guohao Lan, Weitao Xu, Mahbub Hassan, Wen Hu", "title": "Simultaneous Energy Harvesting and Gait Recognition using Piezoelectric\n  Energy Harvester", "comments": "13 pages, 17 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Piezoelectric energy harvester, which generates electricity from stress or\nvibrations, is gaining increasing attention as a viable solution to extend\nbattery life in wearables. Recent research further reveals that, besides\ngenerating energy, PEH can also serve as a passive sensor to detect human gait\npower-efficiently because its stress or vibration patterns are significantly\ninfluenced by the gait. However, as PEHs are not designed for precise\nmeasurement of motion, achievable gait recognition accuracy remains low with\nconventional classification algorithms. The accuracy deteriorates further when\nthe generated electricity is stored simultaneously. To classify gait reliably\nwhile simultaneously storing generated energy, we make two distinct\ncontributions. First, we propose a preprocessing algorithm to filter out the\neffect of energy storage on PEH electricity signal. Second, we propose a long\nshort-term memory (LSTM) network-based classifier to accurately capture\ntemporal information in gait-induced electricity generation. We prototype the\nproposed gait recognition architecture in the form factor of an insole and\nevaluate its gait recognition as well as energy harvesting performance with 20\nsubjects. Our results show that the proposed architecture detects human gait\nwith 12% higher recall and harvests up to 127% more energy while consuming 38%\nless power compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 15:12:33 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Ma", "Dong", ""], ["Lan", "Guohao", ""], ["Xu", "Weitao", ""], ["Hassan", "Mahbub", ""], ["Hu", "Wen", ""]]}, {"id": "2009.02800", "submitter": "Stanislaw Nowak", "authors": "Stan Nowak, Lyn Bartram and Pascal Haegeli", "title": "Designing for Ambiguity: Visual Analytics in Avalanche Forecasting", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambiguity, an information state where multiple interpretations are plausible,\nis a common challenge in visual analytics (VA) systems. We discuss lessons\nlearned from a case study designing VA tools for Canadian avalanche\nforecasters. Avalanche forecasting is a complex and collaborative risk-based\ndecision-making and analysis domain, demanding experience and knowledge-based\ninterpretation of human reported and uncertain data. Differences in reporting\npractices, organizational contexts, and the particularities of individual\nreports result in a variety of potential interpretations that have to be\nnegotiated as part of the forecaster's sensemaking processes. We describe our\npreliminary research using glyphs to support sensemaking under ambiguity.\nAmbiguity is not unique to public avalanche forecasting. There are many other\ndomains where the way data are measured and reported vary in ways not accounted\nexplicitly in the data and require analysts to negotiate multiple potential\nmeanings. We argue that ambiguity is under-served by visualization research and\nwould benefit from more explicit VA support.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 19:08:34 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Nowak", "Stan", ""], ["Bartram", "Lyn", ""], ["Haegeli", "Pascal", ""]]}, {"id": "2009.02807", "submitter": "Kourosh Darvish", "authors": "Kourosh Darvish, Enrico Simetti, Fulvio Mastrogiovanni, Giuseppe\n  Casalino", "title": "A Hierarchical Architecture for Human-Robot Cooperation Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose FlexHRC+, a hierarchical human-robot cooperation\narchitecture designed to provide collaborative robots with an extended degree\nof autonomy when supporting human operators in high-variability shop-floor\ntasks. The architecture encompasses three levels, namely for perception,\nrepresentation, and action. Building up on previous work, here we focus on (i)\nan in-the-loop decision making process for the operations of collaborative\nrobots coping with the variability of actions carried out by human operators,\nand (ii) the representation level, integrating a hierarchical AND/OR graph\nwhose online behaviour is formally specified using First Order Logic. The\narchitecture is accompanied by experiments including collaborative furniture\nassembly and object positioning tasks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 19:55:32 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Darvish", "Kourosh", ""], ["Simetti", "Enrico", ""], ["Mastrogiovanni", "Fulvio", ""], ["Casalino", "Giuseppe", ""]]}, {"id": "2009.02927", "submitter": "Jens Grubert", "authors": "Jens Grubert and Eyal Ofek and Michel Pahud and Per Ola Kristensson", "title": "Back to the Future: Revisiting Mouse and Keyboard Interaction for\n  HMD-based Immersive Analytics", "comments": null, "journal-ref": "In ACM CHI 2020 4th Workshop on Immersive Analytics: Envisioning\n  Future Productivity for Immersive Analytics", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of natural user interfaces, immersive analytics applications\noften focus on novel forms of interaction modalities such as mid-air gestures,\ngaze or tangible interaction utilizing input devices such as depth-sensors,\ntouch screens and eye-trackers. At the same time, traditional input devices\nsuch as the physical keyboard and mouse are used to a lesser extent. We argue,\nthat for certain work scenarios, such as conducting analytic tasks at\nstationary desktop settings, it can be valuable to combine the benefits of\nnovel and established input devices as well as input modalities to create\nproductive immersive analytics environments.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 07:58:33 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Grubert", "Jens", ""], ["Ofek", "Eyal", ""], ["Pahud", "Michel", ""], ["Kristensson", "Per Ola", ""]]}, {"id": "2009.02947", "submitter": "Jens Grubert", "authors": "Eyal Ofek and Jens Grubert and Michel Pahud and Mark Phillips and Per\n  Ola Kristensson", "title": "Towards a Practical Virtual Office for Mobile Knowledge Workers", "comments": "https://www.microsoft.com/en-us/research/event/new-future-of-work/#!publications", "journal-ref": "Microsoft New Future of Work 2020 Symposium", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more people work from home or during travel, new opportunities and\nchallenges arise around mobile office work. On one hand, people may work at\nflexible hours, independent of traffic limitations, but on the other hand, they\nmay need to work at makeshift spaces, with less than optimal working conditions\nand decoupled from co-workers. Virtual Reality (VR) has the potential to change\nthe way information workers work: it enables personal bespoke working\nenvironments even on the go and allows new collaboration approaches that can\nhelp mitigate the effects of physical distance. In this paper, we investigate\nopportunities and challenges for realizing a mobile VR offices environments and\ndiscuss implications from recent findings of mixing standard off-the-shelf\nequipment, such as tablets, laptops or desktops, with VR to enable effective,\nefficient, ergonomic, and rewarding mobile knowledge work. Further, we\ninvestigate the role of conceptual and physical spaces in a mobile VR office.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 08:53:04 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Ofek", "Eyal", ""], ["Grubert", "Jens", ""], ["Pahud", "Michel", ""], ["Phillips", "Mark", ""], ["Kristensson", "Per Ola", ""]]}, {"id": "2009.02998", "submitter": "Marija Schufrin", "authors": "Marija Schufrin, Steven Lamarr Reynolds, Arjan Kuijper and J\\\"orn\n  Kohlhammer", "title": "A Visualization Interface to Improve the Transparency of Collected\n  Personal Data on the Internet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online services are used for all kinds of activities, like news,\nentertainment, publishing content or connecting with others. But information\ntechnology enables new threats to privacy by means of global mass surveillance,\nvast databases and fast distribution networks. Current news are full of misuses\nand data leakages. In most cases, users are powerless in such situations and\ndevelop an attitude of neglect for their online behaviour. On the other hand,\nthe GDPR (General Data Protection Regulation) gives users the right to request\na copy of all their personal data stored by a particular service, but the\nreceived data is hard to understand or analyze by the common internet user.\nThis paper presents TransparencyVis - a web-based interface to support the\nvisual and interactive exploration of data exports from different online\nservices. With this approach, we aim at increasing the awareness of personal\ndata stored by such online services and the effects of online behaviour. This\ndesign study provides an online accessible prototype and a best practice to\nunify data exports from different sources.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 10:30:26 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Schufrin", "Marija", ""], ["Reynolds", "Steven Lamarr", ""], ["Kuijper", "Arjan", ""], ["Kohlhammer", "J\u00f6rn", ""]]}, {"id": "2009.03002", "submitter": "Mai Elshehaly", "authors": "Mai Elshehaly, Rebecca Randell, Matthew Brehmer, Lynn McVey, Natasha\n  Alvarado, Chris P. Gale, Roy A. Ruddle", "title": "QualDash: Adaptable Generation of Visualisation Dashboards for\n  Healthcare Quality Improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adapting dashboard design to different contexts of use is an open question in\nvisualisation research. Dashboard designers often seek to strike a balance\nbetween dashboard adaptability and ease-of-use, and in hospitals challenges\narise from the vast diversity of key metrics, data models and users involved at\ndifferent organizational levels. In this design study, we present QualDash, a\ndashboard generation engine that allows for the dynamic configuration and\ndeployment of visualisation dashboards for healthcare quality improvement (QI).\nWe present a rigorous task analysis based on interviews with healthcare\nprofessionals, a co-design workshop and a series of one-on-one meetings with\nfront line analysts. From these activities we define a metric card metaphor as\na unit of visual analysis in healthcare QI, using this concept as a building\nblock for generating highly adaptable dashboards, and leading to the design of\na Metric Specification Structure (MSS). Each MSS is a JSON structure which\nenables dashboard authors to concisely configure unit-specific variants of a\nmetric card, while offloading common patterns that are shared across cards to\nbe preset by the engine. We reflect on deploying and iterating the design of\nQualDash in cardiology wards and pediatric intensive care units of five NHS\nhospitals. Finally, we report evaluation results that demonstrate the\nadaptability, ease-of-use and usefulness of QualDash in a real-world scenario.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 10:33:43 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Elshehaly", "Mai", ""], ["Randell", "Rebecca", ""], ["Brehmer", "Matthew", ""], ["McVey", "Lynn", ""], ["Alvarado", "Natasha", ""], ["Gale", "Chris P.", ""], ["Ruddle", "Roy A.", ""]]}, {"id": "2009.03115", "submitter": "Youngtaek Kim", "authors": "Youngtaek Kim, Jaeyoung Kim, Hyeon Jeon, Young-Ho Kim, Hyunjoo Song,\n  Bohyoung Kim, Jinwook Seo", "title": "Githru: Visual Analytics for Understanding Software Development History\n  Through Git Metadata Analysis", "comments": "IEEE VIS 2020 (VAST), ACM 2012 CCS - Human-centered computing,\n  Visualization", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics (TVCG)\n  Feb. 2021, pp. 656-666, vol. 27", "doi": "10.1109/TVCG.2020.3030414", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Git metadata contains rich information for developers to understand the\noverall context of a large software development project. Thus it can help new\ndevelopers, managers, and testers understand the history of development without\nneeding to dig into a large pile of unfamiliar source code. However, the\ncurrent tools for Git visualization are not adequate to analyze and explore the\nmetadata: They focus mainly on improving the usability of Git commands instead\nof on helping users understand the development history. Furthermore, they do\nnot scale for large and complex Git commit graphs, which can play an important\nrole in understanding the overall development history. In this paper, we\npresent Githru, an interactive visual analytics system that enables developers\nto effectively understand the context of development history through the\ninteractive exploration of Git metadata. We design an interactive visual\nencoding idiom to represent a large Git graph in a scalable manner while\npreserving the topological structures in the Git graph. To enable scalable\nexploration of a large Git commit graph, we propose novel techniques (graph\nreconstruction, clustering, and Context-Preserving Squash Merge (CSM) methods)\nto abstract a large-scale Git commit graph. Based on these Git commit graph\nabstraction techniques, Githru provides an interactive summary view to help\nusers gain an overview of the development history and a comparison view in\nwhich users can compare different clusters of commits. The efficacy of Githru\nhas been demonstrated by case studies with domain experts using real-world,\nin-house datasets from a large software development team at a major\ninternational IT company. A controlled user study with 12 developers comparing\nGithru to previous tools also confirms the effectiveness of Githru in terms of\ntask completion time.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 14:06:59 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 04:15:19 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Kim", "Youngtaek", ""], ["Kim", "Jaeyoung", ""], ["Jeon", "Hyeon", ""], ["Kim", "Young-Ho", ""], ["Song", "Hyunjoo", ""], ["Kim", "Bohyoung", ""], ["Seo", "Jinwook", ""]]}, {"id": "2009.03143", "submitter": "Ishan Ranasinghe", "authors": "Srikanth Jonnada, Ram Dantu, Ishan Ranasinghe, Logan Widick, Mark\n  Thompson and Janice A. Hauge (University of North Texas)", "title": "Cyber-Human System for Remote Collaborators", "comments": "36 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing ubiquity of technology in our daily lives, the complexity\nof our environment and the mechanisms required to function have also increased\nexponentially. Failure of any of the mechanical and digital devices that we\nrely on can be extremely disruptive. At times, the presence of an expert is\nneeded to analyze, troubleshoot, and fix the problem. The increased demand and\nrapidly evolving mechanisms have led to an insufficient amount of skilled\nworkers, thus resulting in long waiting times for consumers, and\ncorrespondingly high prices for expert services. We assert that performing a\nrepair task with the guidance of experts from any geographical location\nprovides an appropriate solution to the growing demand for handyman skills.\nThis paper proposes an innovative mechanism for two geographically separated\npeople to collaborate on a physical task. It also offers novel methods to\nanalyze the efficiency of a collaboration system and a collaboration protocol\nthrough complexity indices. Using the innovative Collaborative Appliance for\nRemote-help (CARE) and with the support of a remote expert, fifty-nine subjects\nwith minimal or no prior mechanical knowledge were able to elevate a car for\nreplacing a tire; in a second experiment, thirty subjects with minimal or no\nprior plumbing knowledge were able to change the cartridge of a faucet. In both\ncases, average times were close to standard average repair times, and more\nimportantly, both tasks were completed with total accuracy. Our experiments and\nresults show that one can use the developed mechanism and methods for expanding\nthe protocols for a variety of home, vehicle, and appliance repairs and\ninstallations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 14:56:09 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Jonnada", "Srikanth", "", "University of North Texas"], ["Dantu", "Ram", "", "University of North Texas"], ["Ranasinghe", "Ishan", "", "University of North Texas"], ["Widick", "Logan", "", "University of North Texas"], ["Thompson", "Mark", "", "University of North Texas"], ["Hauge", "Janice A.", "", "University of North Texas"]]}, {"id": "2009.03163", "submitter": "Jie Liu", "authors": "Jie Liu, Tim Dwyer, Guido Tack, Samuel Gratzl, Kim Marriott", "title": "Supporting the Problem-Solving Loop: Designing Highly Interactive\n  Optimisation Systems", "comments": "9+2 pages, 6 figures, VAST 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient optimisation algorithms have become important tools for finding\nhigh-quality solutions to hard, real-world problems such as production\nscheduling, timetabling, or vehicle routing. These algorithms are typically\n\"black boxes\" that work on mathematical models of the problem to solve.\nHowever, many problems are difficult to fully specify, and require a \"human in\nthe loop\" who collaborates with the algorithm by refining the model and guiding\nthe search to produce acceptable solutions. Recently, the Problem-Solving Loop\nwas introduced as a high-level model of such interactive optimisation. Here, we\npresent and evaluate nine recommendations for the design of interactive\nvisualisation tools supporting the Problem-Solving Loop. They range from the\nchoice of visual representation for solutions and constraints to the use of a\nsolution gallery to support exploration of alternate solutions. We first\nexamined the applicability of the recommendations by investigating how well\nthey had been supported in previous interactive optimisation tools. We then\nevaluated the recommendations in the context of the vehicle routing problem\nwith time windows (VRPTW). To do so we built a sophisticated interactive visual\nsystem for solving VRPTW that was informed by the recommendations. Ten\nparticipants then used this system to solve a variety of routing problems. We\nreport on participant comments and interaction patterns with the tool. These\nshowed the tool was regarded as highly usable and the results generally\nsupported the usefulness of the underlying recommendations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 15:27:29 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Liu", "Jie", ""], ["Dwyer", "Tim", ""], ["Tack", "Guido", ""], ["Gratzl", "Samuel", ""], ["Marriott", "Kim", ""]]}, {"id": "2009.03171", "submitter": "Laurent Lessard", "authors": "Karen B. Schloss, Zachary Leggon, Laurent Lessard", "title": "Semantic Discriminability for Visual Communication", "comments": "To Appear in IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To interpret information visualizations, observers must determine how visual\nfeatures map onto concepts. First and foremost, this ability depends on\nperceptual discriminability; e.g., observers must be able to see the difference\nbetween different colors for those colors to communicate different meanings.\nHowever, the ability to interpret visualizations also depends on semantic\ndiscriminability, the degree to which observers can infer a unique mapping\nbetween visual features and concepts, based on the visual features and concepts\nalone (i.e., without help from verbal cues such as legends or labels). Previous\nevidence suggested that observers were better at interpreting encoding systems\nthat maximized semantic discriminability (maximizing association strength\nbetween assigned colors and concepts while minimizing association strength\nbetween unassigned colors and concepts), compared to a system that only\nmaximized color-concept association strength. However, increasing semantic\ndiscriminability also resulted in increased perceptual distance, so it is\nunclear which factor was responsible for improved performance. In the present\nstudy, we conducted two experiments that tested for independent effects of\nsemantic distance and perceptual distance on semantic discriminability of bar\ngraph data visualizations. Perceptual distance was large enough to ensure\ncolors were more than just noticeably different. We found that increasing\nsemantic distance improved performance, independent of variation in perceptual\ndistance, and when these two factors were uncorrelated, responses were\ndominated by semantic distance. These results have implications for navigating\ntrade-offs in color palette design optimization for visual communication.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 15:37:34 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Schloss", "Karen B.", ""], ["Leggon", "Zachary", ""], ["Lessard", "Laurent", ""]]}, {"id": "2009.03179", "submitter": "Yating Lin", "authors": "Yating Lin, Kamkwai Wong, Yong Wang, Rong Zhang, Bo Dong, Huamin Qu,\n  Qinghua Zheng", "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion\n  Group", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tax evasion is a serious economic problem for many countries, as it can\nundermine the government' s tax system and lead to an unfair business\ncompetition environment. Recent research has applied data analytics techniques\nto analyze and detect tax evasion behaviors of individual taxpayers. However,\nthey failed to support the analysis and exploration of the uprising related\nparty transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where\na group of taxpayers is involved. In this paper, we present TaxThemis, an\ninteractive visual analytics system to help tax officers mine and explore\nsuspicious tax evasion groups through analyzing heterogeneous tax-related data.\nA taxpayer network is constructed and fused with the trade network to detect\nsuspicious RPTTE groups. Rich visualizations are designed to facilitate the\nexploration and investigation of suspicious transactions between related\ntaxpayers with profit and topological data analysis. Specifically, we propose a\ncalendar heatmap with a carefully-designed encoding scheme to intuitively show\nthe evidence of transferring revenue through related party transactions. We\ndemonstrate the usefulness and effectiveness of TaxThemis through two case\nstudies on real-world tax-related data, and interviews with domain experts.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 15:46:43 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Lin", "Yating", ""], ["Wong", "Kamkwai", ""], ["Wang", "Yong", ""], ["Zhang", "Rong", ""], ["Dong", "Bo", ""], ["Qu", "Huamin", ""], ["Zheng", "Qinghua", ""]]}, {"id": "2009.03237", "submitter": "Tamara Flemisch", "authors": "Patrick Reipschl\\\"ager, Tamara Flemisch, Raimund Dachselt", "title": "Personal Augmented Reality for Information Visualization on Large\n  Interactive Displays", "comments": "This work has been accepted for the IEEE VIS 2020 conference and will\n  be published in the IEEE Transactions on Visualization and Computer Graphics\n  journal. ACM 2012 CCS - Human-centered computing, Visualization,\n  Visualization design and evaluation methods", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030460", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose the combination of large interactive displays with\npersonal head-mounted Augmented Reality (AR) for information visualization to\nfacilitate data exploration and analysis. Even though large displays provide\nmore display space, they are challenging with regard to perception, effective\nmulti-user support, and managing data density and complexity. To address these\nissues and illustrate our proposed setup, we contribute an extensive design\nspace comprising first, the spatial alignment of display, visualizations, and\nobjects in AR space. Next, we discuss which parts of a visualization can be\naugmented. Finally, we analyze how AR can be used to display personal views in\norder to show additional information and to minimize the mutual disturbance of\ndata analysts. Based on this conceptual foundation, we present a number of\nexemplary techniques for extending visualizations with AR and discuss their\nrelation to our design space. We further describe how these techniques address\ntypical visualization problems that we have identified during our literature\nresearch. To examine our concepts, we introduce a generic AR visualization\nframework as well as a prototype implementing several example techniques. In\norder to demonstrate their potential, we further present a use case walkthrough\nin which we analyze a movie data set. From these experiences, we conclude that\nthe contributed techniques can be useful in exploring and understanding\nmultivariate data. We are convinced that the extension of large displays with\nAR for information visualization has a great potential for data analysis and\nsense-making.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:08:15 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 16:29:44 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 14:48:58 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Reipschl\u00e4ger", "Patrick", ""], ["Flemisch", "Tamara", ""], ["Dachselt", "Raimund", ""]]}, {"id": "2009.03385", "submitter": "Christian Tominski", "authors": "Tom Horak, Philip Berger, Heidrun Schumann, Raimund Dachselt,\n  Christian Tominski", "title": "Responsive Matrix Cells: A Focus+Context Approach for Exploring and\n  Editing Multivariate Graphs", "comments": "Tom Horak and Philip Berger are joint first authors", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030371", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix visualizations are a useful tool to provide a general overview of a\ngraph's structure. For multivariate graphs, a remaining challenge is to cope\nwith the attributes that are associated with nodes and edges. Addressing this\nchallenge, we propose responsive matrix cells as a focus+context approach for\nembedding additional interactive views into a matrix. Responsive matrix cells\nare local zoomable regions of interest that provide auxiliary data exploration\nand editing facilities for multivariate graphs. They behave responsively by\nadapting their visual contents to the cell location, the available display\nspace, and the user task. Responsive matrix cells enable users to reveal\ndetails about the graph, compare node and edge attributes, and edit data values\ndirectly in a matrix without resorting to external views or tools. We report\nthe general design considerations for responsive matrix cells covering the\nvisual and interactive means necessary to support a seamless data exploration\nand editing. Responsive matrix cells have been implemented in a web-based\nprototype based on which we demonstrate the utility of our approach. We\ndescribe a walk-through for the use case of analyzing a graph of soccer players\nand report on insights from a preliminary user feedback session.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 19:30:01 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Horak", "Tom", ""], ["Berger", "Philip", ""], ["Schumann", "Heidrun", ""], ["Dachselt", "Raimund", ""], ["Tominski", "Christian", ""]]}, {"id": "2009.03390", "submitter": "Laura Tateosian", "authors": "Alexander Yoshizumi, Megan M. Coffer, Elyssa L. Collins, Mollie D.\n  Gaines, Xiaojie Gao, Kate Jones, Ian R. McGregor, Katie A. McQuillan,\n  Vinicius Perin, Laura M. Tomkins, Thom Worm, Laura Tateosian", "title": "A Review of Geospatial Content in IEEE Visualization Publications", "comments": "5 pages, 4 figures, IEEE VIS Short Paper Proceedings 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geospatial analysis is crucial for addressing many of the world's most\npressing challenges. Given this, there is immense value in improving and\nexpanding the visualization techniques used to communicate geospatial data. In\nthis work, we explore this important intersection -- between geospatial\nanalytics and visualization -- by examining a set of recent IEEE VIS Conference\npapers (a selection from 2017-2019) to assess the inclusion of geospatial data\nand geospatial analyses within these papers. After removing the papers with no\ngeospatial data, we organize the remaining literature into geospatial data\ndomain categories and provide insight into how these categories relate to VIS\nConference paper types. We also contextualize our results by investigating the\nuse of geospatial terms in IEEE Visualization publications over the last 30\nyears. Our work provides an understanding of the quantity and role of\ngeospatial subject matter in recent IEEE VIS publications and supplies a\nfoundation for future meta-analytical work around geospatial analytics and\ngeovisualization that may shed light on opportunities for innovation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 19:42:56 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Yoshizumi", "Alexander", ""], ["Coffer", "Megan M.", ""], ["Collins", "Elyssa L.", ""], ["Gaines", "Mollie D.", ""], ["Gao", "Xiaojie", ""], ["Jones", "Kate", ""], ["McGregor", "Ian R.", ""], ["McQuillan", "Katie A.", ""], ["Perin", "Vinicius", ""], ["Tomkins", "Laura M.", ""], ["Worm", "Thom", ""], ["Tateosian", "Laura", ""]]}, {"id": "2009.03432", "submitter": "Gizem Sogancioglu", "authors": "Gizem So\\u{g}anc{\\i}o\\u{g}lu, Oxana Verkholyak, Heysem Kaya, Dmitrii\n  Fedotov, Tobias Cad\\`ee, Albert Ali Salah, Alexey Karpov", "title": "Is Everything Fine, Grandma? Acoustic and Linguistic Modeling for Robust\n  Elderly Speech Emotion Recognition", "comments": "5 pages, 1 figure, Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Acoustic and linguistic analysis for elderly emotion recognition is an\nunder-studied and challenging research direction, but essential for the\ncreation of digital assistants for the elderly, as well as unobtrusive\ntelemonitoring of elderly in their residences for mental healthcare purposes.\nThis paper presents our contribution to the INTERSPEECH 2020 Computational\nParalinguistics Challenge (ComParE) - Elderly Emotion Sub-Challenge, which is\ncomprised of two ternary classification tasks for arousal and valence\nrecognition. We propose a bi-modal framework, where these tasks are modeled\nusing state-of-the-art acoustic and linguistic features, respectively. In this\nstudy, we demonstrate that exploiting task-specific dictionaries and resources\ncan boost the performance of linguistic models, when the amount of labeled data\nis small. Observing a high mismatch between development and test set\nperformances of various models, we also propose alternative training and\ndecision fusion strategies to better estimate and improve the generalization\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 21:19:16 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["So\u011fanc\u0131o\u011flu", "Gizem", ""], ["Verkholyak", "Oxana", ""], ["Kaya", "Heysem", ""], ["Fedotov", "Dmitrii", ""], ["Cad\u00e8e", "Tobias", ""], ["Salah", "Albert Ali", ""], ["Karpov", "Alexey", ""]]}, {"id": "2009.03454", "submitter": "Xavier Tricoche", "authors": "Xavier M. Tricoche, Wayne R. Schlei, and Kathleen C. Howell", "title": "Extraction and Visualization of Poincar\\'e Map Topology for Spacecraft\n  Trajectory Design", "comments": "11 pages, 13 figures, to appear at IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mission designers must study many dynamical models to plan a low-cost\nspacecraft trajectory that satisfies mission constraints. They routinely use\nPoincar\\'e maps to search for a suitable path through the interconnected web of\nperiodic orbits and invariant manifolds found in multi-body gravitational\nsystems. This paper is concerned with the extraction and interactive visual\nexploration of this structural landscape to assist spacecraft trajectory\nplanning. We propose algorithmic solutions that address the specific challenges\nposed by the characterization of the topology in astrodynamics problems and\nallow for an effective visual analysis of the resulting information. This\nvisualization framework is applied to the circular restricted three-body\nproblem (CR3BP), where it reveals novel periodic orbits with their relevant\ninvariant manifolds in a suitable format for interactive transfer selection.\nRepresentative design problems illustrate how spacecraft path planners can\nleverage our topology visualization to fully exploit the natural dynamics\npathways for energy-efficient trajectory designs.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 23:10:14 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Tricoche", "Xavier M.", ""], ["Schlei", "Wayne R.", ""], ["Howell", "Kathleen C.", ""]]}, {"id": "2009.03520", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Sajjadur Rahman and Peter Griggs and \\c{C}a\\u{g}atay Demiralp", "title": "Leam: An Interactive System for In-situ Visual Text Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in scale and availability of digital text generated on the\nweb, enterprises such as online retailers and aggregators often use text\nanalytics to mine and analyze the data to improve their services and products\nalike. Text data analysis is an iterative, non-linear process with diverse\nworkflows spanning multiple stages, from data cleaning to visualization.\nExisting text analytics systems usually accommodate a subset of these stages\nand often fail to address challenges related to data heterogeneity, provenance,\nworkflow reusability and reproducibility, and compatibility with established\npractices. Based on a set of design considerations we derive from these\nchallenges, we propose Leam, a system that treats the text analysis process as\na single continuum by combining advantages of computational notebooks,\nspreadsheets, and visualization tools. Leam features an interactive user\ninterface for running text analysis workflows, a new data model for managing\nmultiple atomic and composite data types, and an expressive algebra that\ncaptures diverse sets of operations representing various stages of text\nanalysis and enables coordination among different components of the system,\nincluding data, code, and visualizations. We report our current progress in\nLeam development while demonstrating its usefulness with usage examples.\nFinally, we outline a number of enhancements to Leam and identify several\nresearch directions for developing an interactive visual text analysis system.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 05:18:29 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Rahman", "Sajjadur", ""], ["Griggs", "Peter", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "2009.03784", "submitter": "Wenchao Li", "authors": "Wenchao Li, Yun Wang, Haidong Zhang, Huamin Qu", "title": "Improving Engagement of Animated Visualization with Visual Foreshadowing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animated visualization is becoming increasingly popular as a compelling way\nto illustrate changes in time series data. However, maintaining the viewer's\nfocus throughout the entire animation is difficult because of its\ntime-consuming nature. Viewers are likely to become bored and distracted during\nthe ever-changing animated visualization. Informed by the role of foreshadowing\nthat builds the expectation in film and literature, we introduce visual\nforeshadowing to improve the engagement of animated visualizations. In\nspecific, we propose designs of visual foreshadowing that engage the audience\nwhile watching the animation. To demonstrate our approach, we built a\nproof-of-concept animated visualization authoring tool that incorporates visual\nforeshadowing techniques with various styles. Our user study indicates the\neffectiveness of our foreshadowing techniques on improving engagement for\nanimated visualization.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 14:25:16 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Li", "Wenchao", ""], ["Wang", "Yun", ""], ["Zhang", "Haidong", ""], ["Qu", "Huamin", ""]]}, {"id": "2009.03817", "submitter": "Chenhui Li", "authors": "Peiying Zhang, Chenhui Li, Changbo Wang", "title": "VisCode: Embedding Information in Visualization Images using\n  Encoder-Decoder Network", "comments": "11 pages, 16 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach called VisCode for embedding information into\nvisualization images. This technology can implicitly embed data information\nspecified by the user into a visualization while ensuring that the encoded\nvisualization image is not distorted. The VisCode framework is based on a deep\nneural network. We propose to use visualization images and QR codes data as\ntraining data and design a robust deep encoder-decoder network. The designed\nmodel considers the salient features of visualization images to reduce the\nexplicit visual loss caused by encoding. To further support large-scale\nencoding and decoding, we consider the characteristics of information\nvisualization and propose a saliency-based QR code layout algorithm. We present\na variety of practical applications of VisCode in the context of information\nvisualization and conduct a comprehensive evaluation of the perceptual quality\nof encoding, decoding success rate, anti-attack capability, time performance,\netc. The evaluation results demonstrate the effectiveness of VisCode.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 15:48:48 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Zhang", "Peiying", ""], ["Li", "Chenhui", ""], ["Wang", "Changbo", ""]]}, {"id": "2009.03979", "submitter": "Hengrui Luo", "authors": "Leland Wilkinson, Hengrui Luo", "title": "A Distance-preserving Matrix Sketch", "comments": "46 pages, 11 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing very large matrices involves many formidable problems. Various\npopular solutions to these problems involve sampling, clustering, projection,\nor feature selection to reduce the size and complexity of the original task. An\nimportant aspect of these methods is how to preserve relative distances between\npoints in the higher-dimensional space after reducing rows and columns to fit\nin a lower dimensional space. This aspect is important because conclusions\nbased on faulty visual reasoning can be harmful. Judging dissimilar points as\nsimilar or similar points as dissimilar on the basis of a visualization can\nlead to false conclusions. To ameliorate this bias and to make visualizations\nof very large datasets feasible, we introduce two new algorithms that\nrespectively select a subset of rows and columns of a rectangular matrix. This\nselection is designed to preserve relative distances as closely as possible. We\ncompare our matrix sketch to more traditional alternatives on a variety of\nartificial and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 20:15:14 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 19:51:23 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wilkinson", "Leland", ""], ["Luo", "Hengrui", ""]]}, {"id": "2009.03988", "submitter": "Sai Charan Bodda Mr", "authors": "Sai Charan Bodda, Palki Gupta, Gaurav Joshi, Ayush Chaturvedi", "title": "A new architecture for hand-worn Sign language to Speech translator", "comments": "9 pages, 5 figures, research done as part of Samsung electronics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People with speech and hearing impairments often rely on sign language to\ncommunicate with others but most of the general population cannot understand\nsign language and sign language itself is a difficult language to learn, so\nthere is a definite need for technologies to translate sign language to speech.\nIn this paper, we describe the design and implementation of Smart glove, a\nhand-worn hardware device capable of translating American Sign Language\ngestures into English speech by tracking the finger's orientation, gestures and\nhand motion. It uses hardware sensors like Flex, Accelerometer and gyroscope\nand intelligent software to capture and translate the gestures into speech.\nThis paper explains the translation of both Alphabet and Word gestures. New\napproaches and algorithms are proposed and implemented to address\nhardware-dependent issues in existing glove based designs. The whole device is\ndesigned to be modular with distributed processing units to encourage modular\nenhancement, reducing complexity, and interrelation between subsystems.Decision\nTrees are used in gesture recognition and error correction. We hope that the\nhenceforth mentioned design and architecture would be the basis for the\nadvancement in research related to sensor-based sign language translation along\nwith research for smart glove and cybernetic accessories.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 20:37:34 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Bodda", "Sai Charan", ""], ["Gupta", "Palki", ""], ["Joshi", "Gaurav", ""], ["Chaturvedi", "Ayush", ""]]}, {"id": "2009.04035", "submitter": "Teruaki Hayashi", "authors": "Teruaki Hayashi, Nao Uehara, Daisuke Hase, Yukio Ohsawa", "title": "Data Requests and Scenarios for Data Design of Unobserved Events in\n  Corona-related Confusion Using TEEDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the global violence of the novel coronavirus, various industries have\nbeen affected and the breakdown between systems has been apparent. To\nunderstand and overcome the phenomenon related to this unprecedented crisis\ncaused by the coronavirus infectious disease (COVID-19), the importance of data\nexchange and sharing across fields has gained social attention. In this study,\nwe use the interactive platform called treasuring every encounter of data\naffairs (TEEDA) to externalize data requests from data users, which is a tool\nto exchange not only the information on data that can be provided but also the\ncall for data, what data users want and for what purpose. Further, we analyze\nthe characteristics of missing data in the corona-related confusion stemming\nfrom both the data requests and the providable data obtained in the workshop.\nWe also create three scenarios for the data design of unobserved events\nfocusing on variables.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 23:40:26 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Hayashi", "Teruaki", ""], ["Uehara", "Nao", ""], ["Hase", "Daisuke", ""], ["Ohsawa", "Yukio", ""]]}, {"id": "2009.04100", "submitter": "Zheng Wang", "authors": "Zheng Wang, Satoshi Suga, Edric John Cruz Nacpil, Zhanhong Yan, and\n  Kimihiko Nakano", "title": "Adaptive driver-automation shared steering control via forearm surface\n  electromyography measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared steering control has been developed to reduce driver workload while\nkeeping the driver in the control loop. A driver could integrate visual sensory\ninformation from the road ahead and haptic sensory information from the\nsteering wheel to achieve better driving performance. Previous studies suggest\nthat, compared with adaptive automation authority, fixed automation authority\nis not always appropriate with respect to human factors. This paper focuses on\ndesigning an adaptive shared steering control system via sEMG (surface\nelectromyography) measurement from the forearm of the driver, and evaluates the\neffect of the system on driver behavior during a double lane change task. The\nshared steering control was achieved through a haptic guidance system which\nprovided active assistance torque on the steering wheel. Ten subjects\nparticipated in a high-fidelity driving simulator experiment. Two types of\nadaptive algorithms were investigated: haptic guidance decreases when driver\ngrip strength increases (HG-Decrease), and haptic guidance increases when\ndriver grip strength increases (HG-Increase). These two algorithms were\ncompared to manual driving and two levels of fixed authority haptic guidance,\nfor a total of five experimental conditions. Evaluation of the driving systems\nwas based on two sets of dependent variables: objective measures of driver\nbehavior and subjective measures of driver workload. The results indicate that\nthe adaptive authority of HG-Decrease yielded lower driver workload and reduced\nthe lane departure risk compared to manual driving and fixed authority haptic\nguidance.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 04:51:33 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Wang", "Zheng", ""], ["Suga", "Satoshi", ""], ["Nacpil", "Edric John Cruz", ""], ["Yan", "Zhanhong", ""], ["Nakano", "Kimihiko", ""]]}, {"id": "2009.04192", "submitter": "Pawe{\\l} W. Wo\\'zniak", "authors": "Pawe{\\l} W. Wo\\'zniak, Lex Dekker, Francisco Kiss, Ella Velner, Andrea\n  Kuijt and Stella Donker", "title": "Brotate and Tribike: Designing Smartphone Control for Cycling", "comments": "22nd International Conference on Human-Computer Interaction with\n  Mobile Devices and Services (MobileHCI '20), October 5--8, 2020, Oldenburg,\n  Germany", "journal-ref": null, "doi": "10.1145/3379503.3405660", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The more people commute by bicycle, the higher is the number of cyclists\nusing their smartphones while cycling and compromising traffic safety. We have\ndesigned, implemented and evaluated two prototypes for smartphone control\ndevices that do not require the cyclists to remove their hands from the\nhandlebars - the three-button device Tribike and the rotation-controlled\nBrotate. The devices were the result of a user-centred design process where we\nidentified the key features needed for a on-bike smartphone control device. We\nevaluated the devices in a biking exercise with 19 participants, where users\ncompleted a series of common smartphone tasks. The study showed that Brotate\nallowed for significantly more lateral control of the bicycle and both devices\nreduced the cognitive load required to use the smartphone. Our work contributes\ninsights into designing interfaces for cycling.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 10:06:54 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Wo\u017aniak", "Pawe\u0142 W.", ""], ["Dekker", "Lex", ""], ["Kiss", "Francisco", ""], ["Velner", "Ella", ""], ["Kuijt", "Andrea", ""], ["Donker", "Stella", ""]]}, {"id": "2009.04272", "submitter": "Andrey Krekhov", "authors": "Andrey Krekhov", "title": "How to Improve Your Virtual Experience -- Exploring the Obstacles of\n  Mainstream VR", "comments": "78 pages, 24 figures, PhD thesis", "journal-ref": null, "doi": "10.17185/duepublico/71124", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is Virtual Reality? A professional tool, made to facilitate our everyday\ntasks? A conceptual mistake, accompanied by cybersickness and unsolved\nlocomotion issues since the very beginning? Or just another source of\nentertainment that helps us escape from our deteriorating world? The public and\nscientific opinions in this respect are diverse. Furthermore, as researchers,\nwe sometimes ask ourselves whether our work in this area is really \"worth it\",\ngiven the ambiguous prognosis regarding the future of VR. To tackle this\nquestion, we explore three different areas of VR research in this dissertation,\nnamely locomotion, interaction, and perception. We begin our journey by\nstructuring VR locomotion and by introducing a novel locomotion concept for\nlarge distance traveling via virtual body resizing. In the second part, we\nfocus on our interaction possibilities in VR. We learn how to represent virtual\nobjects via self-transforming controllers and how to store our items in VR\ninventories. We design comprehensive 3D gestures for the audience and provide\nan I/O abstraction layer to facilitate the realization and usage of such\ndiverse interaction modalities. The third part is dedicated to the exploration\nof perceptual phenomena in VR. In contrast to locomotion and interaction, our\ncontributions in the field of perception emphasize the strong points of\nimmersive setups. We utilize VR to transfer the illusion of virtual body\nownership to nonhumanoid avatars and exploit this phenomenon for novel gaming\nexperiences with animals in the leading role. As one of our contributions, we\ndemonstrate how to repurpose the dichoptic presentation capability of immersive\nsetups for preattentive zero-overhead highlighting in visualizations. We round\noff the dissertation by coming back to VR research in general, providing a\ncritical assessment of our contributions and sharing our lessons learned along\nthe way.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 12:52:27 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Krekhov", "Andrey", ""]]}, {"id": "2009.04383", "submitter": "Venkata Sriram Siddhardh Nadendla", "authors": "Mukund Telukunta and Venkata Sriram Siddhardh Nadendla", "title": "On the Identification of Fair Auditors to Evaluate Recommender Systems\n  based on a Novel Non-Comparative Fairness Notion", "comments": "10 pages, Accepted to FAccTRec-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-support systems are information systems that offer support to\npeople's decisions in various applications such as judiciary, real-estate and\nbanking sectors. Lately, these support systems have been found to be\ndiscriminatory in the context of many practical deployments. In an attempt to\nevaluate and mitigate these biases, algorithmic fairness literature has been\nnurtured using notions of comparative justice, which relies primarily on\ncomparing two/more individuals or groups within the society that is supported\nby such systems. However, such a fairness notion is not very useful in the\nidentification of fair auditors who are hired to evaluate latent biases within\ndecision-support systems. As a solution, we introduce a paradigm shift in\nalgorithmic fairness via proposing a new fairness notion based on the principle\nof non-comparative justice. Assuming that the auditor makes fairness\nevaluations based on some (potentially unknown) desired properties of the\ndecision-support system, the proposed fairness notion compares the system's\noutcome with that of the auditor's desired outcome. We show that the proposed\nfairness notion also provides guarantees in terms of comparative fairness\nnotions by proving that any system can be deemed fair from the perspective of\ncomparative fairness (e.g. individual fairness and statistical parity) if it is\nnon-comparatively fair with respect to an auditor who has been deemed fair with\nrespect to the same fairness notions. We also show that the converse holds true\nin the context of individual fairness. A brief discussion is also presented\nregarding how our fairness notion can be used to identify fair and reliable\nauditors, and how we can use them to quantify biases in decision-support\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 16:04:41 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Telukunta", "Mukund", ""], ["Nadendla", "Venkata Sriram Siddhardh", ""]]}, {"id": "2009.04508", "submitter": "Brian Felipe Keith Norambuena", "authors": "Brian Keith and Tanushree Mitra", "title": "Narrative Maps: An Algorithmic Approach to Represent and Extract\n  Information Narratives", "comments": "33 pages, 15 figures, CSCW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Narratives are fundamental to our perception of the world and are pervasive\nin all activities that involve the representation of events in time. Yet,\nmodern online information systems do not incorporate narratives in their\nrepresentation of events occurring over time. This article aims to bridge this\ngap, combining the theory of narrative representations with the data from\nmodern online systems. We make three key contributions: a theory-driven\ncomputational representation of narratives, a novel extraction algorithm to\nobtain these representations from data, and an evaluation of our approach. In\nparticular, given the effectiveness of visual metaphors, we employ a route map\nmetaphor to design a narrative map representation. The narrative map\nrepresentation illustrates the events and stories in the narrative as a series\nof landmarks and routes on the map. Each element of our representation is\nbacked by a corresponding element from formal narrative theory, thus providing\na solid theoretical background to our method. Our approach extracts the\nunderlying graph structure of the narrative map using a novel optimization\ntechnique focused on maximizing coherence while respecting structural and\ncoverage constraints. We showcase the effectiveness of our approach by\nperforming a user evaluation to assess the quality of the representation,\nmetaphor, and visualization. Evaluation results indicate that the Narrative Map\nrepresentation is a powerful method to communicate complex narratives to\nindividuals. Our findings have implications for intelligence analysts,\ncomputational journalists, and misinformation researchers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 18:30:44 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 14:38:00 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Keith", "Brian", ""], ["Mitra", "Tanushree", ""]]}, {"id": "2009.04568", "submitter": "Bhavya Ghai", "authors": "Bhavya Ghai, Q. Vera Liao, Yunfeng Zhang, Klaus Mueller", "title": "Active Learning++: Incorporating Annotator's Rationale using Local Model\n  Explanation", "comments": "Accepted at Workshop on Data Science with Human in the Loop (DaSH) @\n  ACM SIGKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new active learning (AL) framework, Active Learning++, which can\nutilize an annotator's labels as well as its rationale. Annotators can provide\ntheir rationale for choosing a label by ranking input features based on their\nimportance for a given query. To incorporate this additional input, we modified\nthe disagreement measure for a bagging-based Query by Committee (QBC) sampling\nstrategy. Instead of weighing all committee models equally to select the next\ninstance, we assign higher weight to the committee model with higher agreement\nwith the annotator's ranking. Specifically, we generated a feature\nimportance-based local explanation for each committee model. The similarity\nscore between feature rankings provided by the annotator and the local model\nexplanation is used to assign a weight to each corresponding committee model.\nThis approach is applicable to any kind of ML model using model-agnostic\ntechniques to generate local explanation such as LIME. With a simulation study,\nwe show that our framework significantly outperforms a QBC based vanilla AL\nframework.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 08:07:33 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Ghai", "Bhavya", ""], ["Liao", "Q. Vera", ""], ["Zhang", "Yunfeng", ""], ["Mueller", "Klaus", ""]]}, {"id": "2009.04645", "submitter": "Hoyeon Ahn", "authors": "Hoyeon Ahn", "title": "Non-contact Real time Eye Gaze Mapping System Based on Deep\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Computer Interaction(HCI) is a field that studies interactions between\nhuman users and computer systems. With the development of HCI, individuals or\ngroups of people can use various digital technologies to achieve the optimal\nuser experience. Human visual attention and visual intelligence are related to\ncognitive science, psychology, and marketing informatics, and are used in\nvarious applications of HCI. Gaze recognition is closely related to the HCI\nfield because it is meaningful in that it can enhance understanding of basic\nhuman behavior. We can obtain reliable visual attention by the Gaze Matching\nmethod that finds the area the user is staring at. In the previous methods, the\nuser wears a glasses-type device which in the form of glasses equipped with a\ngaze tracking function and performs gaze tracking within a limited monitor\narea. Also, the gaze estimation within a limited range is performed while the\nuser's posture is fixed. We overcome the physical limitations of the previous\nmethod in this paper and propose a non-contact gaze mapping system applicable\nin real-world environments. In addition, we introduce the GIST Gaze Mapping\n(GGM) dataset, a Gaze mapping dataset created to learn and evaluate gaze\nmapping.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 02:37:37 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Ahn", "Hoyeon", ""]]}, {"id": "2009.04792", "submitter": "Mengdie Zhuang", "authors": "Mengdie Zhuang, Dave Concannon, and Ed Manley", "title": "A Framework for Evaluating Dashboards in Healthcare", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the era of \"information overload\", effective information provision is\nessential for enabling rapid response and critical decision making. In making\nsense of diverse information sources, data dashboards have become an\nindispensable tool, providing fast, effective, adaptable, and personalized\naccess to information for professionals and the general public alike. However,\nthese objectives place a heavy requirement on dashboards as information\nsystems, resulting in poor usability and ineffective design. Understanding\nthese shortfalls is a challenge given the absence of a consistent and\ncomprehensive approach to dashboard evaluation. In this paper we systematically\nreview literature on dashboard implementation in the healthcare domain, a field\nwhere dashboards have been employed widely, and in which there is widespread\ninterest for improving the current state of the art, and subsequently analyse\napproaches taken towards evaluation. We draw upon consolidated dashboard\nliterature and our own observations to introduce a general definition of\ndashboards which is more relevant to current trends, together with a dashboard\ntask-based classification, which underpin our subsequent analysis. From a total\nof 81 papers we derive seven evaluation scenarios - task performance, behaviour\nchange, interaction workflow, perceived engagement, potential utility,\nalgorithm performance and system implementation. These scenarios distinguish\ndifferent evaluation purposes which we illustrate through measurements, example\nstudies, and common challenges in evaluation study design. We provide a\nbreakdown of each evaluation scenario, and highlight some of the subtle and\nless well posed questions. We conclude by outlining a number of active\ndiscussion points and a set of dashboard evaluation best practices for the\nacademic, clinical and software development communities alike.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 11:50:29 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Zhuang", "Mengdie", ""], ["Concannon", "Dave", ""], ["Manley", "Ed", ""]]}, {"id": "2009.04927", "submitter": "Changjian Li", "authors": "Changjian Li, Hao Pan, Adrien Bousseau and Niloy J. Mitra", "title": "Sketch2CAD: Sequential CAD Modeling by Sketching in Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sketch-based CAD modeling system, where users create objects\nincrementally by sketching the desired shape edits, which our system\nautomatically translates to CAD operations. Our approach is motivated by the\nclose similarities between the steps industrial designers follow to draw 3D\nshapes, and the operations CAD modeling systems offer to create similar shapes.\nTo overcome the strong ambiguity with parsing 2D sketches, we observe that in a\nsketching sequence, each step makes sense and can be interpreted in the\n\\emph{context} of what has been drawn before. In our system, this context\ncorresponds to a partial CAD model, inferred in the previous steps, which we\nfeed along with the input sketch to a deep neural network in charge of\ninterpreting how the model should be modified by that sketch. Our deep network\narchitecture then recognizes the intended CAD operation and segments the sketch\naccordingly, such that a subsequent optimization estimates the parameters of\nthe operation that best fit the segmented sketch strokes. Since there exists no\ndatasets of paired sketching and CAD modeling sequences, we train our system by\ngenerating synthetic sequences of CAD operations that we render as line\ndrawings. We present a proof of concept realization of our algorithm supporting\nfour frequently used CAD operations. Using our system, participants are able to\nquickly model a large and diverse set of objects, demonstrating Sketch2CAD to\nbe an alternate way of interacting with current CAD modeling systems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 15:11:56 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Li", "Changjian", ""], ["Pan", "Hao", ""], ["Bousseau", "Adrien", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2009.05105", "submitter": "Ali Ayub", "authors": "Ali Ayub, Alan R. Wagner", "title": "What am I allowed to do here?: Online Learning of Context-Specific Norms\n  by Pepper", "comments": "The final authenticated publication is available online at\n  https://doi.org/10.1007/978-3-030-62056-1_19", "journal-ref": "International Conference on Social Robotics (ICSR), 2020", "doi": "10.1007/978-3-030-62056-1_19", "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social norms support coordination and cooperation in society. With social\nrobots becoming increasingly involved in our society, they also need to follow\nthe social norms of the society. This paper presents a computational framework\nfor learning contexts and the social norms present in a context in an online\nmanner on a robot. The paper utilizes a recent state-of-the-art approach for\nincremental learning and adapts it for online learning of scenes (contexts).\nThe paper further utilizes Dempster-Schafer theory to model context-specific\nnorms. After learning the scenes (contexts), we use active learning to learn\nrelated norms. We test our approach on the Pepper robot by taking it through\ndifferent scene locations. Our results show that Pepper can learn different\nscenes and related norms simply by communicating with a human partner in an\nonline manner.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 07:27:02 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 06:57:12 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2009.05233", "submitter": "Junxiu Tang", "authors": "Junxiu Tang, Lingyun Yu, Tan Tang, Xinhuan Shu, Lu Ying, Yuhua Zhou,\n  Peiran Ren, and Yingcai Wu", "title": "Narrative Transitions in Data Videos", "comments": "To be published in the Proceedings of IEEE VIS 2020 Short Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transitions are widely used in data videos to seamlessly connect data-driven\ncharts or connect visualizations and non-data-driven motion graphics. To inform\nthe transition designs in data videos, we conduct a content analysis based on\nmore than 3500 clips extracted from 284 data videos. We annotate visualization\ntypes and transition designs on these segments, and examine how these\ntransitions help make connections between contexts. We propose a taxonomy of\ntransitions in data videos, where two transition categories are defined in\nbuilding fluent narratives by using visual variables.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 04:56:07 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Tang", "Junxiu", ""], ["Yu", "Lingyun", ""], ["Tang", "Tan", ""], ["Shu", "Xinhuan", ""], ["Ying", "Lu", ""], ["Zhou", "Yuhua", ""], ["Ren", "Peiran", ""], ["Wu", "Yingcai", ""]]}, {"id": "2009.05254", "submitter": "Saroj Sahoo", "authors": "Saroj Sahoo and Matthew Berger", "title": "Visually Analyzing and Steering Zero Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a visual analytics system to help a user analyze and steer\nzero-shot learning models. Zero-shot learning has emerged as a viable scenario\nfor categorizing data that consists of no labeled examples, and thus a\npromising approach to minimize data annotation from humans. However, it is\nchallenging to understand where zero-shot learning fails, the cause of such\nfailures, and how a user can modify the model to prevent such failures. Our\nvisualization system is designed to help users diagnose and understand\nmispredictions in such models, so that they may gain insight on the behavior of\na model when applied to data associated with categories not seen during\ntraining. Through usage scenarios, we highlight how our system can help a user\nimprove performance in zero-shot learning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 06:58:13 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Sahoo", "Saroj", ""], ["Berger", "Matthew", ""]]}, {"id": "2009.05349", "submitter": "Cigdem Turan Ms.", "authors": "Cigdem Turan, Patrick Schramowski, Constantin Rothkopf, Kristian\n  Kersting", "title": "Alfie: An Interactive Robot with a Moral Compass", "comments": null, "journal-ref": null, "doi": "10.1145/3382507.3421163", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces Alfie, an interactive robot that is capable of answering\nmoral (deontological) questions of a user. The interaction of Alfie is designed\nin a way in which the user can offer an alternative answer when the user\ndisagrees with the given answer so that Alfie can learn from its interactions.\nAlfie's answers are based on a sentence embedding model that uses\nstate-of-the-art language models, e.g. Universal Sentence Encoder and BERT.\nAlfie is implemented on a Furhat Robot, which provides a customizable user\ninterface to design a social robot.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 11:28:53 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Turan", "Cigdem", ""], ["Schramowski", "Patrick", ""], ["Rothkopf", "Constantin", ""], ["Kersting", "Kristian", ""]]}, {"id": "2009.05488", "submitter": "Daniel Diethei", "authors": "Daniel Diethei, Ashley Colley, Matilda Kalving, Tarja Salmela, Jonna\n  H\\\"akkil\\\"a, Johannes Sch\\\"oning", "title": "Medical Selfies: Emotional Impacts and Practical Challenges", "comments": null, "journal-ref": "In 22nd International Conference on Human-Computer Interaction\n  with Mobile Devices and Services (MobileHCI 2020), October 5-8, 2020,\n  Oldenburg, Germany. ACM, New York, NY, USA, 20 pages", "doi": "10.1145/3379503.3403555", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images taken with mobile phones by patients, i.e. medical selfies,\nallow screening, monitoring and diagnosis of skin lesions. While mobile\nteledermatology can provide good diagnostic accuracy for skin tumours, there is\nlittle research about emotional and physical aspects when taking medical\nselfies of body parts. We conducted a survey with 100 participants and a\nqualitative study with twelve participants, in which they took images of eight\nbody parts including intimate areas. Participants had difficulties taking\nmedical selfies of their shoulder blades and buttocks. For the genitals, they\nprefer to visit a doctor rather than sending images. Taking the images\ntriggered privacy concerns, memories of past experiences with body parts and\nraised awareness of the bodily medical state. We present recommendations for\nthe design of mobile apps to address the usability and emotional impacts of\ntaking medical selfies.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 15:09:45 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 07:30:15 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Diethei", "Daniel", ""], ["Colley", "Ashley", ""], ["Kalving", "Matilda", ""], ["Salmela", "Tarja", ""], ["H\u00e4kkil\u00e4", "Jonna", ""], ["Sch\u00f6ning", "Johannes", ""]]}, {"id": "2009.05502", "submitter": "Johannes Knittel", "authors": "Johannes Knittel, Andres Lalama, Steffen Koch, and Thomas Ertl", "title": "Visual Neural Decomposition to Explain Multivariate Data Sets", "comments": "To appear in IEEE Transactions on Visualization and Computer Graphics\n  and IEEE VIS 2020 (VAST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating relationships between variables in multi-dimensional data sets\nis a common task for data analysts and engineers. More specifically, it is\noften valuable to understand which ranges of which input variables lead to\nparticular values of a given target variable. Unfortunately, with an increasing\nnumber of independent variables, this process may become cumbersome and\ntime-consuming due to the many possible combinations that have to be explored.\nIn this paper, we propose a novel approach to visualize correlations between\ninput variables and a target output variable that scales to hundreds of\nvariables. We developed a visual model based on neural networks that can be\nexplored in a guided way to help analysts find and understand such\ncorrelations. First, we train a neural network to predict the target from the\ninput variables. Then, we visualize the inner workings of the resulting model\nto help understand relations within the data set. We further introduce a new\nregularization term for the backpropagation algorithm that encourages the\nneural network to learn representations that are easier to interpret visually.\nWe apply our method to artificial and real-world data sets to show its utility.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 15:53:37 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Knittel", "Johannes", ""], ["Lalama", "Andres", ""], ["Koch", "Steffen", ""], ["Ertl", "Thomas", ""]]}, {"id": "2009.05605", "submitter": "Jichen Zhu", "authors": "Chelsea M. Myers, Jiachi Xie, Jichen Zhu", "title": "A Game-Based Approach for Helping Designers Learn Machine Learning\n  Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) is becoming more prevalent in the systems we use daily.\nYet designers of these systems are under-equipped to design with these\ntechnologies. Recently, interactive visualizations have been used to present ML\nconcepts to non-experts. However, little research exists evaluating how\ndesigners build an understanding of ML in these environments or how to instead\ndesign interfaces that guide their learning. In a user study (n=21), we observe\nhow designers interact with our interactive visualizer, \\textit{QUBE}, focusing\non visualizing Q-Learning through a game metaphor. We analyze how designers\napproach interactive visualizations and game metaphors to form an understanding\nof ML concepts and the challenges they face along the way. We found the\ninteractive visualization significantly improved participants' high-level\nunderstanding of ML concepts. However, it did not support their ability to\ndesign with these concepts. We present themes on the challenges our\nparticipants faced when learning an ML concept and their self-guided learning\nbehaviors. Our findings suggest design recommendations for supporting an\nunderstanding of ML concepts through guided learning interfaces and game\nmetaphors.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 18:38:41 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Myers", "Chelsea M.", ""], ["Xie", "Jiachi", ""], ["Zhu", "Jichen", ""]]}, {"id": "2009.05704", "submitter": "Palakorn Achananuparp", "authors": "Philips Kokoh Prasetyo, Palakorn Achananuparp, Ee-Peng Lim", "title": "Foodbot: A Goal-Oriented Just-in-Time Healthy Eating Interventions\n  Chatbot", "comments": "Accepted to EAI '20", "journal-ref": null, "doi": "10.1145/3421937.3421960", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has identified a few design flaws in popular mobile health\n(mHealth) applications for promoting healthy eating lifestyle, such as mobile\nfood journals. These include tediousness of manual food logging, inadequate\nfood database coverage, and a lack of healthy dietary goal setting. To address\nthese issues, we present Foodbot, a chatbot-based mHealth application for\ngoal-oriented just-in-time (JIT) healthy eating interventions. Powered by a\nlarge-scale food knowledge graph, Foodbot utilizes automatic speech recognition\nand mobile messaging interface to record food intake. Moreover, Foodbot allows\nusers to set goals and guides their behavior toward the goals via JIT\nnotification prompts, interactive dialogues, and personalized recommendation.\nAltogether, the Foodbot framework demonstrates the use of open-source data,\ntools, and platforms to build a practical mHealth solution for supporting\nhealthy eating lifestyle in the general population.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 02:24:07 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Prasetyo", "Philips Kokoh", ""], ["Achananuparp", "Palakorn", ""], ["Lim", "Ee-Peng", ""]]}, {"id": "2009.05714", "submitter": "Nalin Asanka Gamagedara Arachchilage", "authors": "Nalin Asanka Gamagedara Arachchilage and Mumtaz Abdul Hameed", "title": "Designing a Serious Game: Teaching Developers to Embed Privacy into\n  Software Systems", "comments": "6", "journal-ref": "35th IEEE/ACM International Conference on Automated Software\n  Engineering Workshops (ASEW '20), September 25, 2020, Virtual Event,\n  Australia. ACM, New York, NY, USA", "doi": "10.1145/3417113.3422149", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software applications continue to challenge user privacy when users interact\nwith them. Privacy practices (e.g. Data Minimisation (DM), Privacy by Design\n(PbD) or General Data Protection Regulation (GDPR)) and related \"privacy\nengineering\" methodologies exist and provide clear instructions for developers\nto implement privacy into software systems they develop that preserve user\nprivacy. However, those practices and methodologies are not yet a common\npractice in the software development community. There has been no previous\nresearch focused on developing \"educational\" interventions such as serious\ngames to enhance software developers' coding behaviour. Therefore, this\nresearch proposes a game design framework as an educational tool for software\ndevelopers to improve (secure) coding behaviour, so they can develop\nprivacy-preserving software applications that people can use. The elements of\nthe proposed framework were incorporated into a gaming application scenario\nthat enhances the software developers' coding behaviour through their\nmotivation. The proposed work not only enables the development of\nprivacy-preserving software systems but also helping the software development\ncommunity to put privacy guidelines and engineering methodologies into\npractice.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 03:18:11 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Arachchilage", "Nalin Asanka Gamagedara", ""], ["Hameed", "Mumtaz Abdul", ""]]}, {"id": "2009.05791", "submitter": "Petar Radanliev", "authors": "Petar Radanliev, David De Roure, Rob Walton, Max Van Kleek, Rafael\n  Mantilla Montalvo, Omar Santos, LaTreall Maddox, Stacy Cannady", "title": "COVID-19 what have we learned? The rise of social machines and connected\n  devices in pandemic management following the concepts of predictive,\n  preventive and personalised medicine", "comments": null, "journal-ref": "EPMA Journal 11, 311 332 (2020)", "doi": "10.1007/s13167-020-00218-x", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive bibliographic review with R statistical methods of the COVID\npandemic in PubMed literature and Web of Science Core Collection, supported\nwith Google Scholar search. In addition, a case study review of emerging new\napproaches in different regions, using medical literature, academic literature,\nnews articles and other reliable data sources. Public responses of mistrust\nabout privacy data misuse differ across countries, depending on the chosen\npublic communication strategy.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 13:26:54 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 21:26:57 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Radanliev", "Petar", ""], ["De Roure", "David", ""], ["Walton", "Rob", ""], ["Van Kleek", "Max", ""], ["Montalvo", "Rafael Mantilla", ""], ["Santos", "Omar", ""], ["Maddox", "LaTreall", ""], ["Cannady", "Stacy", ""]]}, {"id": "2009.05802", "submitter": "Ashad Kabir", "authors": "Anik Das, Sumaiya Amin, Muhammad Ashad Kabir, Md. Sabir Hossain,\n  Mohammad Mainul Islam", "title": "Learning Daily Calorie Intake Standard using a Mobile Game", "comments": null, "journal-ref": "International Journal of Game-Based Learning, 2021", "doi": "10.4018/IJGBL.2021040104", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile games can contribute to learning at greater success. In this paper, we\nhave developed and evaluated a novel educational game, named FoodCalorie, to\nlearn the food calorie intake standards. Our game is aimed to learn the calorie\nvalues of various traditional Bangladeshi foods and the calorie intake standard\nthat varies with age and gender. Our study confirms the finding of existing\nstudies that game-based learning can enhance the learning experience.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 14:32:00 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 02:48:53 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 03:49:57 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Das", "Anik", ""], ["Amin", "Sumaiya", ""], ["Kabir", "Muhammad Ashad", ""], ["Hossain", "Md. Sabir", ""], ["Islam", "Mohammad Mainul", ""]]}, {"id": "2009.05936", "submitter": "Mohammad Hadi", "authors": "Mohammad Abdul Hadi, Fatemeh H Fard, Irene Vrbik", "title": "Geo-Spatial Data Visualization and Critical Metrics Predictions for\n  Canadian Elections", "comments": "7 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open data published by various organizations is intended to make the data\navailable to the public. All over the world, numerous organizations maintain a\nconsiderable number of open databases containing a lot of facts and numbers.\nHowever, most of them do not offer a concise and insightful data interpretation\nor visualization tool, which can help users to process all of the information\nin a consistently comparable way. Canadian Federal and Provincial Elections is\nan example of these databases. This information exists in numerous websites, as\nseparate tables so that the user needs to traverse through a tree structure of\nscattered information on the site, and the user is left with the comparison,\nwithout providing proper tools, data-interpretation or visualizations. In this\npaper, we provide technical details of addressing this problem, by using the\nCanadian Elections data (since 1867) as a specific case study as it has\nnumerous technical challenges. We hope that the methodology used here can help\nin developing similar tools to achieve some of the goals of publicly available\ndatasets. The developed tool contains data visualization, trend analysis, and\nprediction components. The visualization enables the users to interact with the\ndata through various techniques, including Geospatial visualization. To\nreproduce the results, we have open-sourced the tool.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 06:54:33 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Hadi", "Mohammad Abdul", ""], ["Fard", "Fatemeh H", ""], ["Vrbik", "Irene", ""]]}, {"id": "2009.05938", "submitter": "Michael Lyons", "authors": "Michael J. Lyons, Miyuki Kamachi, Jiro Gyoba", "title": "Coding Facial Expressions with Gabor Wavelets (IVC Special Issue)", "comments": "13 pages, 7 figures, 23 references", "journal-ref": null, "doi": "10.5281/zenodo.4029679", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for extracting information about facial expressions from\ndigital images. The method codes facial expression images using a\nmulti-orientation, multi-resolution set of Gabor filters that are\ntopographically ordered and approximately aligned with the face. A similarity\nspace derived from this code is compared with one derived from semantic ratings\nof the images by human observers. Interestingly the low-dimensional structure\nof the image-derived similarity space shares organizational features with the\ncircumplex model of affect, suggesting a bridge between categorical and\ndimensional representations of facial expression. Our results also indicate\nthat it would be possible to construct a facial expression classifier based on\na topographically-linked multi-orientation, multi-resolution Gabor coding of\nthe facial images at the input stage. The significant degree of psychological\nplausibility exhibited by the proposed code may also be useful in the design of\nhuman-computer interfaces.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 07:01:16 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Lyons", "Michael J.", ""], ["Kamachi", "Miyuki", ""], ["Gyoba", "Jiro", ""]]}, {"id": "2009.06019", "submitter": "Sunwoo Ha", "authors": "Sunwoo Ha, Adam Kern, Melanie Bancilhon, and Alvitta Ottley", "title": "Expectation Versus Reality: The Failed Evaluation of a Mixed-Initiative\n  Visualization System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our research aimed to present the design and evaluation of a mixed-initiative\nsystem that aids the user in handling complex datasets and dense visualization\nsystems. We attempted to demonstrate this system with two trials of an online\nbetween-groups, two-by-two study, measuring the effects of this\nmixed-initiative system on user interactions and system usability. However, due\nto flaws in the interface design and the expectations that we put on users, we\nwere unable to show that the adaptive system had an impact on user interactions\nor system usability. In this paper, we discuss the unexpected findings that we\nfound from our \"failed\" experiments and examine how we can learn from our\nfailures to improve further research.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 15:19:38 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ha", "Sunwoo", ""], ["Kern", "Adam", ""], ["Bancilhon", "Melanie", ""], ["Ottley", "Alvitta", ""]]}, {"id": "2009.06042", "submitter": "Shayan Monadjemi", "authors": "Shayan Monadjemi, Roman Garnett, Alvitta Ottley", "title": "Competing Models: Inferring Exploration Patterns and Information\n  Relevance via Bayesian Model Selection", "comments": "To appear in IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030430", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing interaction data provides an opportunity to learn about users,\nuncover their underlying goals, and create intelligent visualization systems.\nThe first step for intelligent response in visualizations is to enable\ncomputers to infer user goals and strategies through observing their\ninteractions with a system. Researchers have proposed multiple techniques to\nmodel users, however, their frameworks often depend on the visualization\ndesign, interaction space, and dataset. Due to these dependencies, many\ntechniques do not provide a general algorithmic solution to user exploration\nmodeling. In this paper, we construct a series of models based on the dataset\nand pose user exploration modeling as a Bayesian model selection problem where\nwe maintain a belief over numerous competing models that could explain user\ninteractions. Each of these competing models represent an exploration strategy\nthe user could adopt during a session. The goal of our technique is to make\nhigh-level and in-depth inferences about the user by observing their low-level\ninteractions. Although our proposed idea is applicable to various probabilistic\nmodel spaces, we demonstrate a specific instance of encoding exploration\npatterns as competing models to infer information relevance. We validate our\ntechnique's ability to infer exploration bias, predict future interactions, and\nsummarize an analytic session using user study datasets. Our results indicate\nthat depending on the application, our method outperforms established baselines\nfor bias detection and future interaction prediction. Finally, we discuss\nfuture research directions based on our proposed modeling paradigm and suggest\nhow practitioners can use this method to build intelligent visualization\nsystems that understand users' goals and adapt to improve the exploration\nprocess.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 16:52:40 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 03:53:28 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Monadjemi", "Shayan", ""], ["Garnett", "Roman", ""], ["Ottley", "Alvitta", ""]]}, {"id": "2009.06062", "submitter": "Shayan Shokri", "authors": "Shayan Shokri, Shane Ward, Pierre-Amaury M. Anton, Paolo Siffredi,\n  Guglielmo Papetti", "title": "Recent Advances in Wearable Sensors with Application in Rehabilitation\n  Motion Analysis", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase in world elderly population has significantly underlined the\nneed for continuous health care measurement, specifically in rehabilitation\nmonitoring. The new technologies has enabled people to have in home healthcare\nservices, meanwhile, motion analysis methods are widely used for human activity\nmonitoring as a remote healthcare service. Wearable sensors have indicated\npromising results both in convenience and technical performance. These sensors\nare extensively used in human motion analysis and advancement of wireless\ncommunications has intensively contributed to this field. Exploiting wireless\ntechnology and wearable sensors contributes to more effective help in emergency\ncases and has significantly decreased the hospitalization time. This paper\nreviews the most recent advances in wearable sensors used in motion analysis,\nspecifically in the field of rehabilitation. Firstly, common wearable sensor\ntechnologies are introduced and then wearable sensors deploying Carbon Nano\nTubes (CNT) are specifically reviewed. The next section is dedicated to sensor\nfusion in which possibility and performance of integration of new technologies\nare reviewed. This technique has been widely exploited to bring forth certainty\nin clinical results. Lastly, the challenges and future possibilities for\nadvancement in motion analysis sensors is discussed.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 18:48:50 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 14:37:42 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Shokri", "Shayan", ""], ["Ward", "Shane", ""], ["Anton", "Pierre-Amaury M.", ""], ["Siffredi", "Paolo", ""], ["Papetti", "Guglielmo", ""]]}, {"id": "2009.06155", "submitter": "Laura Perovich", "authors": "Laura J. Perovich, Sara Ann Wylie, Roseann Bongiovanni", "title": "Chemicals in the Creek: designing a situated data physicalization of\n  open government data with the community", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Over the last decade growing amounts of government data have been made\navailable in an attempt to increase transparency and civic participation, but\nit is unclear if this data serves non-expert communities due to gaps in access\nand the technical knowledge needed to interpret this \"open\" data. We conducted\na two-year design study focused on the creation of a community-based data\ndisplay using the United States Environmental Protection Agency data on water\npermit violations by oil storage facilities on the Chelsea Creek in\nMassachusetts to explore whether situated data physicalization and\nParticipatory Action Research could support meaningful engagement with open\ndata. We selected this data as it is of interest to local groups and available\nonline, yet remains largely invisible and inaccessible to the Chelsea\ncommunity. The resulting installation, Chemicals in the Creek, responds to the\ncall for community-engaged visualization processes and provides an application\nof situated methods of data representation. It proposes event-centered and\npower-aware modes of engagement using contextual and embodied data\nrepresentations. The design of Chemicals in the Creek is grounded in\ninteractive workshops and we analyze it through event observation, interviews,\nand community outcomes. We reflect on the role of community engaged research in\nthe Information Visualization community relative to recent conversations on new\napproaches to design studies and evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 02:17:08 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Perovich", "Laura J.", ""], ["Wylie", "Sara Ann", ""], ["Bongiovanni", "Roseann", ""]]}, {"id": "2009.06171", "submitter": "Henry Griffith", "authors": "Henry Griffith, Dillon Lohr, Evgeny Abdulin, Oleg Komogortsev", "title": "GazeBase: A Large-Scale, Multi-Stimulus, Longitudinal Eye Movement\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This manuscript presents GazeBase, a large-scale longitudinal dataset\ncontaining 12,334 monocular eye-movement recordings captured from 322\ncollege-aged subjects. Subjects completed a battery of seven tasks in two\ncontiguous sessions during each round of recording, including a - 1) fixation\ntask, 2) horizontal saccade task, 3) random oblique saccade task, 4) reading\ntask, 5/6) free viewing of cinematic video task, and 7) gaze-driven gaming\ntask. A total of nine rounds of recording were conducted over a 37 month\nperiod, with subjects in each subsequent round recruited exclusively from the\nprior round. All data was collected using an EyeLink 1000 eye tracker at a\n1,000 Hz sampling rate, with a calibration and validation protocol performed\nbefore each task to ensure data quality. Due to its large number of subjects\nand longitudinal nature, GazeBase is well suited for exploring research\nhypotheses in eye movement biometrics, along with other emerging applications\napplying machine learning techniques to eye movement signal analysis.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 03:19:53 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 11:52:45 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 00:37:20 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Griffith", "Henry", ""], ["Lohr", "Dillon", ""], ["Abdulin", "Evgeny", ""], ["Komogortsev", "Oleg", ""]]}, {"id": "2009.06272", "submitter": "Lucas Morillo", "authors": "L. Morillo-Mendez", "title": "Implications of ageing for the design of cognitive interaction systems", "comments": "Ageing in a changing society: Interdisciplinary popular science\n  contributions from the Newbreed research school, E. Kristoffersson and T.\n  Strandberg (eds)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We are living longer in times of the biggest technological revolution\nhumanity had ever seen before. Trying to understand how these two facts\ninteract with each other, or more specifically, trying to maximise the benefits\nthat new developments could potentially offer for the enhancement of the\nquality of life of older adults, is a task on which we have already begun to\nwork. In particular, the rapid growth of cognitive interaction systems (CISs),\ntechnologies that learn and interact with humans to extend what human and\nmachine could do on their own, offers a promising landscape of possibilities.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 08:56:55 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Morillo-Mendez", "L.", ""]]}, {"id": "2009.06279", "submitter": "Andrew Tzer-Yeu Chen", "authors": "Andrew Tzer-Yeu Chen", "title": "How Fragmentation Can Undermine the Public Health Response to COVID-19", "comments": "7 pages, 5 figures, published by ACM Interactions (online at\n  https://interactions.acm.org/blog/view/how-fragmentation-can-undermine-the-public-health-response-to-covid-19,\n  in magazine in 2021)", "journal-ref": null, "doi": "10.1145/3448413", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Responses to COVID-19 have largely been led by local, national, and\ninternational public health agencies, who have activated their pandemic plans\nand opened the epidemiological toolkit of modelling, testing, isolation and\nmovement restrictions, surveillance, and contact tracing. In the contemporary\ntech-heavy world, many assumed that the common manual process of human\ninvestigators and phone calls could or should be replaced by digital solutions.\nBut it's not as simple as \"add more technology\" - the complex way in which\nusers and societies interact with the technology has significant impacts on\neffectiveness. When efforts are not well co-ordinated, fragmentation in system\ndesign and user experience can negatively impact the public health response.\nThis article briefly covers the journey of how contact tracing registers and\ndigital diaries evolved in New Zealand during the COVID-19 pandemic, the\ninitial poor outcomes caused by the lack of central co-ordination, and the\nlater improvement.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 09:16:12 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 23:16:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chen", "Andrew Tzer-Yeu", ""]]}, {"id": "2009.06308", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Paula Delgado-Santos, Andres Perez-Uribe, Ruben\n  Vera-Rodriguez, Julian Fierrez, Aythami Morales", "title": "DeepWriteSYN: On-Line Handwriting Synthesis via Deep Short-Term\n  Representations", "comments": null, "journal-ref": "Proc. 35th AAAI Conference on Artificial Intelligence, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This study proposes DeepWriteSYN, a novel on-line handwriting synthesis\napproach via deep short-term representations. It comprises two modules: i) an\noptional and interchangeable temporal segmentation, which divides the\nhandwriting into short-time segments consisting of individual or multiple\nconcatenated strokes; and ii) the on-line synthesis of those short-time\nhandwriting segments, which is based on a sequence-to-sequence Variational\nAutoencoder (VAE). The main advantages of the proposed approach are that the\nsynthesis is carried out in short-time segments (that can run from a character\nfraction to full characters) and that the VAE can be trained on a configurable\nhandwriting dataset. These two properties give a lot of flexibility to our\nsynthesiser, e.g., as shown in our experiments, DeepWriteSYN can generate\nrealistic handwriting variations of a given handwritten structure corresponding\nto the natural variation within a given population or a given subject. These\ntwo cases are developed experimentally for individual digits and handwriting\nsignatures, respectively, achieving in both cases remarkable results.\n  Also, we provide experimental results for the task of on-line signature\nverification showing the high potential of DeepWriteSYN to improve\nsignificantly one-shot learning scenarios. To the best of our knowledge, this\nis the first synthesis approach capable of generating realistic on-line\nhandwriting in the short term (including handwritten signatures) via deep\nlearning. This can be very useful as a module toward long-term realistic\nhandwriting generation either completely synthetic or as natural variation of\ngiven handwriting samples.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 10:17:55 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 11:03:11 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Tolosana", "Ruben", ""], ["Delgado-Santos", "Paula", ""], ["Perez-Uribe", "Andres", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""], ["Morales", "Aythami", ""]]}, {"id": "2009.06349", "submitter": "Mark Keane", "authors": "Courtney Ford and Eoin M. Kenny and Mark T. Keane", "title": "Play MNIST For Me! User Studies on the Effects of Post-Hoc,\n  Example-Based Explanations & Error Rates on Debugging a Deep Learning,\n  Black-Box Classifier", "comments": "2 Figures, 1 Table, 8 pages", "journal-ref": "IJCAI-20 Workshop on Explainable AI (XAI), September 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports two experiments (N=349) on the impact of post hoc\nexplanations by example and error rates on peoples perceptions of a black box\nclassifier. Both experiments show that when people are given case based\nexplanations, from an implemented ANN CBR twin system, they perceive miss\nclassifications to be more correct. They also show that as error rates increase\nabove 4%, people trust the classifier less and view it as being less correct,\nless reasonable and less trustworthy. The implications of these results for XAI\nare discussed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 14:55:52 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ford", "Courtney", ""], ["Kenny", "Eoin M.", ""], ["Keane", "Mark T.", ""]]}, {"id": "2009.06433", "submitter": "Fabian Sperrle", "authors": "Fabian Sperrle, Mennatallah El-Assady, Grace Guo, Duen Horng Chau,\n  Alex Endert, Daniel Keim", "title": "Should We Trust (X)AI? Design Dimensions for Structured Experimental\n  Evaluations", "comments": "18pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper systematically derives design dimensions for the structured\nevaluation of explainable artificial intelligence (XAI) approaches. These\ndimensions enable a descriptive characterization, facilitating comparisons\nbetween different study designs. They further structure the design space of\nXAI, converging towards a precise terminology required for a rigorous study of\nXAI. Our literature review differentiates between comparative studies and\napplication papers, revealing methodological differences between the fields of\nmachine learning, human-computer interaction, and visual analytics. Generally,\neach of these disciplines targets specific parts of the XAI process. Bridging\nthe resulting gaps enables a holistic evaluation of XAI in real-world\nscenarios, as proposed by our conceptual model characterizing bias sources and\ntrust-building. Furthermore, we identify and discuss the potential for future\nwork based on observed research gaps that should lead to better coverage of the\nproposed model.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 13:40:51 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Sperrle", "Fabian", ""], ["El-Assady", "Mennatallah", ""], ["Guo", "Grace", ""], ["Chau", "Duen Horng", ""], ["Endert", "Alex", ""], ["Keim", "Daniel", ""]]}, {"id": "2009.06494", "submitter": "Venkatesh Vijaykumar", "authors": "Venkatesh Vijaykumar", "title": "Play Music An HCI Oriented Evaluation of Googles Default Music Player\n  Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The work embodied in this paper attempts to suggest a few improvements to the\nplaylist creation task interface of the Google Play Music Android application\nbased on recommended practices encountered in the Human-Computer Interaction\ndiscipline. The improvements are largely centered on intuitive navigation and\nselection actions, in order to facilitate a smoother experience in creating,\nordering, and adding to music playlists. The work records the efforts in\nneed-finding, design brainstorming, and prototype design and evaluation. The\nwork was undertaken over a single design life cycle, and is an attempt at\napplying recommended practices in HCI to a widely used real world application.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:54:30 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Vijaykumar", "Venkatesh", ""]]}, {"id": "2009.06522", "submitter": "Damla Cay", "authors": "Damla Cay, Till Nagel, Asim Evren Yantac", "title": "ColVis: Collaborative Visualization Design Workshops for Diverse User\n  Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding different types of users' needs can even be more critical in\ntoday's data visualization field, as exploratory visualizations for novice\nusers are becoming more widespread with an increasing amount of data sources.\nThe complexity of data-driven projects requires input from including\ninterdisciplinary expert and novice users. Our workshop framework helps taking\ndesign decisions collaboratively with experts and novice users, on different\nlevels such as outlining users and goals, identifying tasks, structuring data,\nand creating data visualization ideas. We conducted workshops for two different\ndata visualization projects. For each project, we conducted a workshop with\nproject stakeholders who are domain experts, then a second workshop with novice\nusers. We collected feedback from participants and used critical reflection on\nthe process. Later on, we created recommendations on how this workshop\nstructure can be used by others. Our main contributions are, (1) the workshop\nframework for designing data visualizations, (2) describing the outcomes and\nlessons learned from multiple workshops.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:34:40 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Cay", "Damla", ""], ["Nagel", "Till", ""], ["Yantac", "Asim Evren", ""]]}, {"id": "2009.06526", "submitter": "Pradipta Biswas", "authors": "Shashank Kumar, JeevithaShree DV and Pradipta Biswas", "title": "Accessibility evaluation of websites using WCAG tools and Cambridge\n  Simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is plethora of tools available for automatic evaluation of web\naccessibility with respect to WCAG. This paper compares a set of WCAG tools and\ntheir results in terms of ease of comprehension and implementation by web\ndevelopers. The paper highlights accessibility issues that cannot be captured\nonly through conformance to WCAG tools and propose additional methods to\nevaluate accessibility through an Inclusive User Model. We initially selected\nten WCAG tools from W3 website and used a set of these tools on the landing\npages of BBC and WHO websites. We compared their outcome in terms of\ncommonality, differences, amount of details and usability. Finally, we briefly\nintroduced the Inclusive User Model and demonstrated how simulation of user\ninteraction can capture usability and accessibility issues that are not\ndetected through WCAG analysis. The paper concludes with a proposal on a Common\nUser Profile format that can be used to compare and contrast accessibility\nsystems and services, and to simulate and personalize interaction for users\nwith different range of abilities.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:41:25 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Kumar", "Shashank", ""], ["DV", "JeevithaShree", ""], ["Biswas", "Pradipta", ""]]}, {"id": "2009.06591", "submitter": "Adam Williams", "authors": "Adam S. Williams, Francisco R. Ortega", "title": "Understanding Gesture and Speech Multimodal Interactions for\n  Manipulation Tasks in Augmented Reality Using Unconstrained Elicitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research establishes a better understanding of the syntax choices in\nspeech interactions and of how speech, gesture, and multimodal gesture and\nspeech interactions are produced by users in unconstrained object manipulation\nenvironments using augmented reality. The work presents a multimodal\nelicitation study conducted with 24 participants. The canonical referents for\ntranslation, rotation, and scale were used along with some abstract referents\n(create, destroy, and select). In this study time windows for gesture and\nspeech multimodal interactions are developed using the start and stop times of\ngestures and speech as well as the stoke times for gestures. While gestures\ncommonly precede speech by 81 ms we find that the stroke of the gesture is\ncommonly within 10 ms of the start of speech. Indicating that the information\ncontent of a gesture and its co-occurring speech are well aligned to each\nother. Lastly, the trends across the most common proposals for each modality\nare examined. Showing that the disagreement between proposals is often caused\nby a variation of hand posture or syntax. Allowing us to present aliasing\nrecommendations to increase the percentage of users' natural interactions\ncaptured by future multimodal interactive systems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 17:21:24 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Williams", "Adam S.", ""], ["Ortega", "Francisco R.", ""]]}, {"id": "2009.06773", "submitter": "Cristina Ceja", "authors": "Cristina R. Ceja, Caitlyn M. McColeman, Cindy Xiong, Steven L.\n  Franconeri", "title": "Truth or Square: Aspect Ratio Biases Recall of Position Encodings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bar charts are among the most frequently used visualizations, in part because\ntheir position encoding leads them to convey data values precisely. Yet\nreproductions of single bars or groups of bars within a graph can be biased.\nCuriously, some previous work found that this bias resulted in an\noverestimation of reproduced data values, while other work found an\nunderestimation. Across three empirical studies, we offer an explanation for\nthese conflicting findings: this discrepancy is a consequence of the differing\naspect ratios of the tested bar marks. Viewers are biased to remember a bar\nmark as being more similar to a prototypical square, leading to an\noverestimation of bars with a wide aspect ratio, and an underestimation of bars\nwith a tall aspect ratio. Experiments 1 and 2 showed that the aspect ratio of\nthe bar marks indeed influenced the direction of this bias. Experiment 3\nconfirmed that this pattern of misestimation bias was present for reproductions\nfrom memory, suggesting that this bias may arise when comparing values across\nsequential displays or views. We describe additional visualization designs that\nmight be prone to this bias beyond bar charts (e.g., Mekko charts and\ntreemaps), and speculate that other visual channels might hold similar biases\ntoward prototypical values.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 22:30:46 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Ceja", "Cristina R.", ""], ["McColeman", "Caitlyn M.", ""], ["Xiong", "Cindy", ""], ["Franconeri", "Steven L.", ""]]}, {"id": "2009.06781", "submitter": "Kushal Chawla", "authors": "Kushal Chawla, Gale Lucas", "title": "Pilot: Winner of the Human-Agent Negotiation Challenge at IJCAI 2020", "comments": "Winner at ANAC, IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes our agent Pilot, winner of the Human-Agent\nNegotiation Challenge at ANAC, IJCAI 2020. Pilot is a virtual human that\nparticipates in a sequence of three negotiations with a human partner. Our\nsystem is based on the Interactive Arbitration Guide Online (IAGO) negotiation\nframework. We leverage prior Affective Computing and Psychology research in\nnegotiations to guide various key principles that define the behavior and\npersonality of our agent.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 22:56:47 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 21:42:23 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Chawla", "Kushal", ""], ["Lucas", "Gale", ""]]}, {"id": "2009.06855", "submitter": "Madison Elliott", "authors": "Madison Elliott, Christine Nothelfer, Cindy Xiong and Danielle Szafir", "title": "A Design Space of Vision Science Methods for Visualization Research", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of efforts aim to understand what people see when using a\nvisualization. These efforts provide scientific grounding to complement design\nintuitions, leading to more effective visualization practice. However,\npublished visualization research currently reflects a limited set of available\nmethods for understanding how people process visualized data. Alternative\nmethods from vision science offer a rich suite of tools for understanding\nvisualizations, but no curated collection of these methods exists in either\nperception or visualization research. We introduce a design space of\nexperimental methods for empirically investigating the perceptual processes\ninvolved with viewing data visualizations to ultimately inform visualization\ndesign guidelines. This paper provides a shared lexicon for facilitating\nexperimental visualization research. We discuss popular experimental paradigms,\nadjustment types, response types, and dependent measures used in vision science\nresearch, rooting each in visualization examples. We then discuss the\nadvantages and limitations of each technique. Researchers can use this design\nspace to create innovative studies and progress scientific understanding of\ndesign choices and evaluations in visualization. We highlight a history of\ncollaborative success between visualization and vision science research and\nadvocate for a deeper relationship between the two fields that can elaborate on\nand extend the methodological design space for understanding visualization and\nvision.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 03:51:15 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Elliott", "Madison", ""], ["Nothelfer", "Christine", ""], ["Xiong", "Cindy", ""], ["Szafir", "Danielle", ""]]}, {"id": "2009.06875", "submitter": "Richard Li", "authors": "Richard Li, Eric Whitmire, Michael Stengel, Ben Boudaoud, Jan Kautz,\n  David Luebke, Shwetak Patel, Kaan Ak\\c{s}it", "title": "Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors", "comments": "10 pages, 8 figures, published in IEEE International Symposium on\n  Mixed and Augmented Reality (ISMAR) 2020", "journal-ref": null, "doi": "10.1109/ISMAR50242.2020.00033", "report-no": null, "categories": "eess.SY cs.HC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze tracking is an essential component of next generation displays for\nvirtual reality and augmented reality applications. Traditional camera-based\ngaze trackers used in next generation displays are known to be lacking in one\nor multiple of the following metrics: power consumption, cost, computational\ncomplexity, estimation accuracy, latency, and form-factor. We propose the use\nof discrete photodiodes and light-emitting diodes (LEDs) as an alternative to\ntraditional camera-based gaze tracking approaches while taking all of these\nmetrics into consideration. We begin by developing a rendering-based simulation\nframework for understanding the relationship between light sources and a\nvirtual model eyeball. Findings from this framework are used for the placement\nof LEDs and photodiodes. Our first prototype uses a neural network to obtain an\naverage error rate of 2.67{\\deg} at 400Hz while demanding only 16mW. By\nsimplifying the implementation to using only LEDs, duplexed as light\ntransceivers, and more minimal machine learning model, namely a light-weight\nsupervised Gaussian process regression algorithm, we show that our second\nprototype is capable of an average error rate of 1.57{\\deg} at 250 Hz using 800\nmW.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 05:50:13 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 18:57:46 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Li", "Richard", ""], ["Whitmire", "Eric", ""], ["Stengel", "Michael", ""], ["Boudaoud", "Ben", ""], ["Kautz", "Jan", ""], ["Luebke", "David", ""], ["Patel", "Shwetak", ""], ["Ak\u015fit", "Kaan", ""]]}, {"id": "2009.06876", "submitter": "Ross Maciejewski", "authors": "Yuxin Ma, Arlen Fan, Jingrui He, Arun Reddy Nelakurthi, Ross\n  Maciejewski", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer\n  Learning Processes", "comments": "Accepted to IEEE Transactions on Visualization and Computer Graphics\n  (Proc. IEEE VAST 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical learning models hold an assumption that the training data\nand the future unlabeled data are drawn from the same distribution. However,\nthis assumption is difficult to fulfill in real-world scenarios and creates\nbarriers in reusing existing labels from similar application domains. Transfer\nLearning is intended to relax this assumption by modeling relationships between\ndomains, and is often applied in deep learning applications to reduce the\ndemand for labeled data and training time. Despite recent advances in exploring\ndeep learning models with visual analytics tools, little work has explored the\nissue of explaining and diagnosing the knowledge transfer process between deep\nlearning models. In this paper, we present a visual analytics framework for the\nmulti-level exploration of the transfer learning processes when training deep\nneural networks. Our framework establishes a multi-aspect design to explain how\nthe learned knowledge from the existing model is transferred into the new\nlearning task when training deep neural networks. Based on a comprehensive\nrequirement and task analysis, we employ descriptive visualization with\nperformance measures and detailed inspections of model behaviors from the\nstatistical, instance, feature, and model structure levels. We demonstrate our\nframework through two case studies on image classification by fine-tuning\nAlexNets to illustrate how analysts can utilize our framework.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 05:59:00 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Ma", "Yuxin", ""], ["Fan", "Arlen", ""], ["He", "Jingrui", ""], ["Nelakurthi", "Arun Reddy", ""], ["Maciejewski", "Ross", ""]]}, {"id": "2009.07053", "submitter": "Joseph DeRose", "authors": "Joseph F DeRose, Jiayao Wang, and Matthew Berger", "title": "Attention Flows: Analyzing and Comparing Attention Mechanisms in\n  Language Models", "comments": "11 pages, 12 figures, to be published in IEEE Transactions on\n  Visualization and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in language modeling have led to the development of deep\nattention-based models that are performant across a wide variety of natural\nlanguage processing (NLP) problems. These language models are typified by a\npre-training process on large unlabeled text corpora and subsequently\nfine-tuned for specific tasks. Although considerable work has been devoted to\nunderstanding the attention mechanisms of pre-trained models, it is less\nunderstood how a model's attention mechanisms change when trained for a target\nNLP task. In this paper, we propose a visual analytics approach to\nunderstanding fine-tuning in attention-based language models. Our\nvisualization, Attention Flows, is designed to support users in querying,\ntracing, and comparing attention within layers, across layers, and amongst\nattention heads in Transformer-based language models. To help users gain\ninsight on how a classification decision is made, our design is centered on\ndepicting classification-based attention at the deepest layer and how attention\nfrom prior layers flows throughout words in the input. Attention Flows supports\nthe analysis of a single model, as well as the visual comparison between\npre-trained and fine-tuned models via their similarities and differences. We\nuse Attention Flows to study attention mechanisms in various sentence\nunderstanding tasks and highlight how attention evolves to address the nuances\nof solving these tasks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 19:56:30 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["DeRose", "Joseph F", ""], ["Wang", "Jiayao", ""], ["Berger", "Matthew", ""]]}, {"id": "2009.07095", "submitter": "Eytan Adar", "authors": "Eytan Adar and Elsie Lee", "title": "Communicative Visualizations as a Learning Problem", "comments": "To appear, IEEE Info VIS'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant research has provided robust task and evaluation languages for\nthe analysis of exploratory visualizations. Unfortunately, these taxonomies\nfail when applied to communicative visualizations. Instead, designers often\nresort to evaluating communicative visualizations from the cognitive efficiency\nperspective: \"can the recipient accurately decode my message/insight?\" However,\ndesigners are unlikely to be satisfied if the message went 'in one ear and out\nthe other.' The consequence of this inconsistency is that it is difficult to\ndesign or select between competing options in a principled way. The problem we\naddress is the fundamental mismatch between how designers want to describe\ntheir intent, and the language they have. We argue that visualization designers\ncan address this limitation through a learning lens: that the recipient is a\nstudent and the designer a teacher. By using learning objectives, designers can\nbetter define, assess, and compare communicative visualizations. We illustrate\nhow the learning-based approach provides a framework for understanding a wide\narray of communicative goals. To understand how the framework can be applied\n(and its limitations), we surveyed and interviewed members of the Data\nVisualization Society using their own visualizations as a probe. Through this\nstudy we identified the broad range of objectives in communicative\nvisualizations and the prevalence of certain objective types.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 13:43:35 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Adar", "Eytan", ""], ["Lee", "Elsie", ""]]}, {"id": "2009.07149", "submitter": "Elodie Bouzbib", "authors": "Elodie Bouzbib, Gilles Bailly, Sinan Haliyo, Pascal Frey", "title": "CoVR: A Large-Scale Force-Feedback Robotic Interface for\n  Non-Deterministic Scenarios in VR", "comments": "10 pages (without references), 14 pages total", "journal-ref": null, "doi": "10.1145/3379337.3415891", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CoVR, a novel robotic interface providing strong kinesthetic\nfeedback (100 N) in a room-scale VR arena. It consists of a physical column\nmounted on a 2D Cartesian ceiling robot (XY displacements) with the capacity of\n(1) resisting to body-scaled users' actions such as pushing or leaning; (2)\nacting on the users by pulling or transporting them as well as (3) carrying\nmultiple potentially heavy objects (up to 80kg) that users can freely\nmanipulate or make interact with each other. We describe its implementation and\ndefine a trajectory generation algorithm based on a novel user intention model\nto support non-deterministic scenarios, where the users are free to interact\nwith any virtual object of interest with no regards to the scenarios' progress.\nA technical evaluation and a user study demonstrate the feasibility and\nusability of CoVR, as well as the relevance of whole-body interactions\ninvolving strong forces, such as being pulled through or transported.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 15:03:23 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Bouzbib", "Elodie", ""], ["Bailly", "Gilles", ""], ["Haliyo", "Sinan", ""], ["Frey", "Pascal", ""]]}, {"id": "2009.07227", "submitter": "Tiankai Xie", "authors": "Tiankai Xie, Yuxin Ma, Hanghang Tong, My T. Thai, Ross Maciejewski", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "comments": "11 pages, accepted by IEEE Transactions on Visualization and Computer\n  Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph mining plays a pivotal role across a number of disciplines, and a\nvariety of algorithms have been developed to answer who/what type questions.\nFor example, what items shall we recommend to a given user on an e-commerce\nplatform? The answers to such questions are typically returned in the form of a\nranked list, and graph-based ranking methods are widely used in industrial\ninformation retrieval settings. However, these ranking algorithms have a\nvariety of sensitivities, and even small changes in rank can lead to vast\nreductions in product sales and page hits. As such, there is a need for tools\nand methods that can help model developers and analysts explore the\nsensitivities of graph ranking algorithms with respect to perturbations within\nthe graph structure. In this paper, we present a visual analytics framework for\nexplaining and exploring the sensitivity of any graph-based ranking algorithm\nby performing perturbation-based what-if analysis. We demonstrate our framework\nthrough three case studies inspecting the sensitivity of two classic\ngraph-based ranking algorithms (PageRank and HITS) as applied to rankings in\npolitical news media and social networks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 17:07:20 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Xie", "Tiankai", ""], ["Ma", "Yuxin", ""], ["Tong", "Hanghang", ""], ["Thai", "My T.", ""], ["Maciejewski", "Ross", ""]]}, {"id": "2009.07322", "submitter": "Eren Cakmak", "authors": "Eren Cakmak, Dominik J\\\"ackle, Tobias Schreck, Daniel Keim", "title": "dg2pix: Pixel-Based Visual Analysis of Dynamic Graphs", "comments": "10 pages, 7 figures", "journal-ref": "Symposium on Visualization in Data Science (VDS) at IEEE VIS 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presenting long sequences of dynamic graphs remains challenging due to the\nunderlying large-scale and high-dimensional data. We propose dg2pix, a novel\npixel-based visualization technique, to visually explore temporal and\nstructural properties in long sequences of large-scale graphs. The approach\nconsists of three main steps: (1) the multiscale modeling of the temporal\ndimension; (2) unsupervised graph embeddings to learn low-dimensional\nrepresentations of the dynamic graph data; and (3) an interactive pixel-based\nvisualization to simultaneously explore the evolving data at different temporal\naggregation scales. dg2pix provides a scalable overview of a dynamic graph,\nsupports the exploration of long sequences of high-dimensional graph data, and\nenables the identification and comparison of similar temporal states. We show\nthe applicability of the technique to synthetic and real-world datasets,\ndemonstrating that temporal patterns in dynamic graphs can be identified and\ninterpreted over time. dg2pix contributes a suitable intermediate\nrepresentation between node-link diagrams at the high detail end and matrix\nrepresentations on the low detail end.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 18:55:57 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Cakmak", "Eren", ""], ["J\u00e4ckle", "Dominik", ""], ["Schreck", "Tobias", ""], ["Keim", "Daniel", ""]]}, {"id": "2009.07342", "submitter": "Takayuki Itoh", "authors": "Takayuki Itoh, Asuka Nakabayashi, Mariko Hagita", "title": "Scatterplot Selection Applying a Graph Coloring Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scatterplot selection is an effective approach to represent essential\nportions of multidimensional data in a limited display space. Various metrics\nfor evaluation of scatterplots such as scagnostics have been presented and\napplied to scatterplot selection. This paper presents a new scatterplot\nselection technique that applies multiple metrics. The technique firstly\ncalculates scores of scatterplots with multiple metrics and then constructs a\ngraph by connecting similar scatterplots. The technique applies a graph\ncoloring problem so that different colors are assigned to similar scatterplots.\nWe can extract a set of various scatterplots by selecting them that the\nspecific same color is assigned. This paper introduces visualization examples\nwith a retail dataset containing multidimensional climate and sales values.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 20:22:16 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Itoh", "Takayuki", ""], ["Nakabayashi", "Asuka", ""], ["Hagita", "Mariko", ""]]}, {"id": "2009.07403", "submitter": "Molla Rashied Hussein", "authors": "Molla Rashied Hussein, Md. Ashikur Rahman, Md. Jahidul Hassan\n  Mojumder, Shakib Ahmed, Samia Naz Isha, Shaila Akter, Abdullah Bin Shams,\n  Ehsanul Hoque Apu", "title": "Trust Concerns in Health Apps collecting Personally Identifiable\n  Information during COVID-19-like Zoonosis", "comments": "6 pages, 1 table, submitted to the 23rd International Conference on\n  Computer and Information Technology (ICCIT 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease 2019, or COVID-19 in short, is a zoonosis, i.e., a\ndisease that spreads from animals to humans. Due to its highly epizootic\nnature, it has compelled the public health experts to deploy smartphone\napplications to trace its rapid transmission pattern along with the infected\npersons as well by utilizing the persons' personally identifiable information.\nHowever, these information may summon several undesirable provocations towards\nthe technical experts in terms of privacy and cyber security, particularly the\ntrust concerns. If not resolved by now, the circumstances will affect the mass\nlevel population through inadequate usage of the health applications in the\nsmartphones and thus liberate the forgery of a catastrophe for another\nCOVID-19-like zoonosis to come. Therefore, an extensive study was required to\naddress this severe issue. This paper has fulfilled the study mentioned above\nneeded by not only discussing the recently designed and developed health\napplications all over the regions around the world but also investigating their\nusefulness and limitations. The trust defiance is identified as well as\nscrutinized from the viewpoint of an end-user. Several recommendations are\nsuggested in the later part of this paper to leverage awareness among the\nordinary individuals.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 00:34:05 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Hussein", "Molla Rashied", ""], ["Rahman", "Md. Ashikur", ""], ["Mojumder", "Md. Jahidul Hassan", ""], ["Ahmed", "Shakib", ""], ["Isha", "Samia Naz", ""], ["Akter", "Shaila", ""], ["Shams", "Abdullah Bin", ""], ["Apu", "Ehsanul Hoque", ""]]}, {"id": "2009.07446", "submitter": "Sunny Tian", "authors": "Sunny Tian, Amy X. Zhang, David Karger", "title": "A System for Interleaving Discussion and Summarization in Online\n  Collaboration", "comments": null, "journal-ref": null, "doi": "10.1145/3432940", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many instances of online collaboration, ideation and deliberation about\nwhat to write happen separately from the synthesis of the deliberation into a\ncohesive document. However, this may result in a final document that has little\nconnection to the discussion that came before. In this work, we present\ninterleaved discussion and summarization, a process where discussion and\nsummarization are woven together in a single space, and collaborators can\nswitch back and forth between discussing ideas and summarizing discussion until\nit results in a final document that incorporates and references all discussion\npoints. We implement this process into a tool called Wikum+ that allows groups\nworking together on a project to create living summaries-artifacts that can\ngrow as new collaborators, ideas, and feedback arise and shrink as\ncollaborators come to consensus. We conducted studies where groups of six\npeople each collaboratively wrote a proposal using Wikum+ and a proposal using\na messaging platform along with Google Docs. We found that Wikum+'s integration\nof discussion and summarization helped users be more organized, allowing for\nlight-weight coordination and iterative improvements throughout the\ncollaboration process. A second study demonstrated that in larger groups,\nWikum+ is more inclusive of all participants and more comprehensive in the\nfinal document compared to traditional tools.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 03:29:06 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 21:36:34 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2020 00:39:28 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Tian", "Sunny", ""], ["Zhang", "Amy X.", ""], ["Karger", "David", ""]]}, {"id": "2009.07517", "submitter": "Boris Ivanovic", "authors": "Boris Ivanovic, Amine Elhafsi, Guy Rosman, Adrien Gaidon, Marco Pavone", "title": "MATS: An Interpretable Trajectory Forecasting Representation for\n  Planning and Control", "comments": "14 pages, 6 figures, 1 table. All code, models, and data can be found\n  at https://github.com/StanfordASL/MATS . Conference on Robot Learning (CoRL)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about human motion is a core component of modern human-robot\ninteractive systems. In particular, one of the main uses of behavior prediction\nin autonomous systems is to inform robot motion planning and control. However,\na majority of planning and control algorithms reason about system dynamics\nrather than the predicted agent tracklets (i.e., ordered sets of waypoints)\nthat are commonly output by trajectory forecasting methods, which can hinder\ntheir integration. Towards this end, we propose Mixtures of Affine Time-varying\nSystems (MATS) as an output representation for trajectory forecasting that is\nmore amenable to downstream planning and control use. Our approach leverages\nsuccessful ideas from probabilistic trajectory forecasting works to learn\ndynamical system representations that are well-studied in the planning and\ncontrol literature. We integrate our predictions with a proposed multimodal\nplanning methodology and demonstrate significant computational efficiency\nimprovements on a large-scale autonomous driving dataset.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 07:32:37 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 09:46:46 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Ivanovic", "Boris", ""], ["Elhafsi", "Amine", ""], ["Rosman", "Guy", ""], ["Gaidon", "Adrien", ""], ["Pavone", "Marco", ""]]}, {"id": "2009.07564", "submitter": "Xiaoyi Wang", "authors": "Xiaoyi Wang, Alexander Eiselmayer, Wendy E. Mackay, Kasper\n  Hornb{\\ae}k, Chat Wacharamanotham", "title": "Argus: Interactive a priori Power Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge HCI researchers face when designing a controlled experiment\nis choosing the appropriate number of participants, or sample size. A prior\npower analysis examines the relationships among multiple parameters, including\nthe complexity associated with human participants, e.g., order and fatigue\neffects, to calculate the statistical power of a given experiment design. We\ncreated Argus, a tool that supports interactive exploration of statistical\npower: Researchers specify experiment design scenarios with varying confounds\nand effect sizes. Argus then simulates data and visualizes statistical power\nacross these scenarios, which lets researchers interactively weigh various\ntrade-offs and make informed decisions about sample size. We describe the\ndesign and implementation of Argus, a usage scenario designing a visualization\nexperiment, and a think-aloud study.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 09:17:14 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Wang", "Xiaoyi", ""], ["Eiselmayer", "Alexander", ""], ["Mackay", "Wendy E.", ""], ["Hornb\u00e6k", "Kasper", ""], ["Wacharamanotham", "Chat", ""]]}, {"id": "2009.07828", "submitter": "Kiran Garimella", "authors": "Kirill Martynov, Kiran Garimella, Robert West", "title": "Human biases in body measurement estimation", "comments": "Accepted to the EPJ Data science journal. Please cite the EPJ version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Body measurements, including weight and height, are key indicators of health.\nBeing able to visually assess body measurements reliably is a step towards\nincreased awareness of overweight and obesity and is thus important for public\nhealth. Nevertheless it is currently not well understood how accurately humans\ncan assess weight and height from images, and when and how they fail. To bridge\nthis gap, we start from 1,682 images of persons collected from the Web, each\nannotated with the true weight and height, and ask crowd workers to estimate\nthe weight and height for each image. We conduct a faceted analysis taking into\naccount characteristics of the images as well as the crowd workers assessing\nthe images, revealing several novel findings: (1) Even after aggregation, the\ncrowd's accuracy is overall low. (2) We find strong evidence of contraction\nbias toward a reference value, such that the weight (height) of light (short)\npeople is overestimated, whereas that of heavy (tall) people is underestimated.\n(3) We estimate workers' individual reference values using a Bayesian model,\nfinding that reference values strongly correlate with workers' own height and\nweight, indicating that workers are better at estimating people similar to\nthemselves. (4) The weight of tall people is underestimated more than that of\nshort people; yet, knowing the height decreases the weight error only mildly.\n(5) Accuracy is higher on images of females than of males, but female and male\nworkers are no different in terms of accuracy. (6) Crowd workers improve over\ntime if given feedback on previous guesses. Finally, we explore various bias\ncorrection models for improving the crowd's accuracy, but find that this only\nleads to modest gains. Overall, this work provides important insights on biases\nin body measurement estimation as obesity related conditions are on the rise.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 17:39:44 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 01:07:54 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Martynov", "Kirill", ""], ["Garimella", "Kiran", ""], ["West", "Robert", ""]]}, {"id": "2009.07884", "submitter": "Eleanor Birrell", "authors": "Sean O'Connor, Ryan Nurwono, Aden Siebel, Eleanor Birrell", "title": "(Un)clear and (In)conspicuous: The right to opt-out of sale under CCPA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The California Consumer Privacy Act (CCPA) -- which began enforcement on July\n1, 2020 -- grants California users the affirmative right to opt-out of the sale\nof their personal information. In this work, we perform a series of\nobservational studies to understand how websites implement this right. We\nperform two manual analyses of the top 500 U.S. websites (one conducted in July\n2020 and a second conducted in January 2021) and classify how each site\nimplements this new requirement. We also perform an automated analysis of the\nTop 5000 U.S. websites. We find that the vast majority of sites that implement\nopt-out mechanisms do so with a Do Not Sell link rather than with a privacy\nbanner, and that many of the linked opt-out controls exhibit features such as\nnudging and indirect mechanisms (e.g., fillable forms). We then perform a pair\nof user studies with 4357 unique users (recruited from Google Ads and Amazon\nMechanical Turk) in which we observe how users interact with different opt-out\nmechanisms and evaluate how the implementation choices we observed -- exclusive\nuse of links, prevalent nudging, and indirect mechanisms -- affect the rate at\nwhich users exercise their right to opt-out of sale. We find that these design\nelements significantly deter interactions with opt-out mechanisms -- including\nreducing the opt-out rate for users who are uncomfortable with the sale of\ntheir information -- and that they reduce users' awareness of their ability to\nopt-out. Our results demonstrate the importance of regulations that provide\nclear implementation requirements in order empower users to exercise their\nprivacy rights.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 18:31:23 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 15:59:28 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["O'Connor", "Sean", ""], ["Nurwono", "Ryan", ""], ["Siebel", "Aden", ""], ["Birrell", "Eleanor", ""]]}, {"id": "2009.07951", "submitter": "Xavier-Lewis Palmer", "authors": "Srdjan Lesaja and Xavier-Lewis Palmer", "title": "Brain-Computer Interfaces and the Dangers of Neurocapitalism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review how existing trends are relevant to the discussion of\nbrain-computer interfaces and the data they would generate. Then, we posit how\nthe commerce of neural data, dubbed Neurocapitalism, could be impacted by the\nmaturation of brain-computer interface technology. We explore how this could\npose fundamental changes to our way of interacting, as well as our sense of\nautonomy and identity. Because of the power inherent in the technology, and its\npotentially ruinous abuses, action must be taken before the appearance of the\ntechnology, and not come as a reaction to it. The widespread adoption of\nbrain-computer interface technology will certainly change our way of life.\nWhether it is changed for the better or worse, depends on how well we prepare\nfor its arrival.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 21:51:00 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Lesaja", "Srdjan", ""], ["Palmer", "Xavier-Lewis", ""]]}, {"id": "2009.08127", "submitter": "Thomas Baudel", "authors": "Thomas Baudel, Manon Verbockhaven, Guillaume Roy, Victoire Cousergue\n  and Rida Laarach", "title": "Addressing Cognitive Biases in Augmented Business Decision Systems", "comments": "22 pages, 8 figures, submitted to ACM CHI 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How do algorithmic decision aids introduced in business decision processes\naffect task performance? In a first experiment, we study effective\ncollaboration. Faced with a decision, subjects alone have a success rate of\n72%; Aided by a recommender that has a 75% success rate, their success rate\nreaches 76%. The human-system collaboration had thus a greater success rate\nthan each taken alone. However, we noted a complacency/authority bias that\ndegraded the quality of decisions by 5% when the recommender was wrong. This\nsuggests that any lingering algorithmic bias may be amplified by decision aids.\nIn a second experiment, we evaluated the effectiveness of 5 presentation\nvariants in reducing complacency bias. We found that optional presentation\nincreases subjects' resistance to wrong recommendations. We conclude by arguing\nthat our metrics, in real usage scenarios, where decision aids are embedded as\nsystem-wide features in Business Process Management software, can lead to\nenhanced benefits.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 08:02:25 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Baudel", "Thomas", ""], ["Verbockhaven", "Manon", ""], ["Roy", "Guillaume", ""], ["Cousergue", "Victoire", ""], ["Laarach", "Rida", ""]]}, {"id": "2009.08188", "submitter": "Devis Tuia", "authors": "John E. Vargas-Mu\\~noz, Devis Tuia, Alexandre X. Falc\\~ao", "title": "Deploying machine learning to assist digital humanitarians: making image\n  annotation in OpenStreetMap more efficient", "comments": null, "journal-ref": null, "doi": "10.1080/13658816.2020.1814303", "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Locating populations in rural areas of developing countries has attracted the\nattention of humanitarian mapping projects since it is important to plan\nactions that affect vulnerable areas. Recent efforts have tackled this problem\nas the detection of buildings in aerial images. However, the quality and the\namount of rural building annotated data in open mapping services like\nOpenStreetMap (OSM) is not sufficient for training accurate models for such\ndetection. Although these methods have the potential of aiding in the update of\nrural building information, they are not accurate enough to automatically\nupdate the rural building maps. In this paper, we explore a human-computer\ninteraction approach and propose an interactive method to support and optimize\nthe work of volunteers in OSM. The user is asked to verify/correct the\nannotation of selected tiles during several iterations and therefore improving\nthe model with the new annotated data. The experimental results, with simulated\nand real user annotation corrections, show that the proposed method greatly\nreduces the amount of data that the volunteers of OSM need to verify/correct.\nThe proposed methodology could benefit humanitarian mapping projects, not only\nby making more efficient the process of annotation but also by improving the\nengagement of volunteers.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 10:05:30 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Vargas-Mu\u00f1oz", "John E.", ""], ["Tuia", "Devis", ""], ["Falc\u00e3o", "Alexandre X.", ""]]}, {"id": "2009.08456", "submitter": "Zack Ellerby", "authors": "Zack Ellerby, Christian Wagner and Stephen Broomell", "title": "Capturing Richer Information -- On Establishing the Validity of an\n  Interval-Valued Survey Response Mode", "comments": "59 pages, 12 figures, submitted to Behavior Research Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining quantitative survey responses that are both accurate and\ninformative is crucial to a wide range of fields. Traditional and ubiquitous\nresponse formats such as Likert and Visual Analogue Scales require condensation\nof responses into discrete point values - but sometimes a range of options may\nbetter represent the correct answer. In this paper, we propose an efficient\ninterval-valued response mode, whereby responses are made by marking an ellipse\nalong a continuous scale. We discuss its potential to capture and quantify\nvaluable information that would be lost using conventional approaches, while\npreserving a high degree of response-efficiency. The information captured by\nthe response interval may represent a possible response range - i.e., a\nconjunctive set, such as the real numbers between three and six. Alternatively,\nit may reflect uncertainty in respect to a distinct response - i.e., a\ndisjunctive set, such as a confidence interval. We then report a validation\nstudy, utilizing our recently introduced open-source software (DECSYS) to\nexplore how interval-valued survey responses reflect experimental manipulations\nof several factors hypothesised to influence interval width, across multiple\ncontexts. Results consistently indicate that respondents used interval widths\neffectively, and subjective participant feedback was also positive. We present\nthis as initial empirical evidence for the efficacy and value of\ninterval-valued response capture. Interestingly, our results also provide\ninsight into respondents' reasoning about the different aforementioned types of\nintervals - we replicate a tendency towards overconfidence for those\nrepresenting epistemic uncertainty (i.e., disjunctive sets), but find intervals\nrepresenting inherent range (i.e., conjunctive sets) to be well-calibrated.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 18:25:15 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 11:05:22 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ellerby", "Zack", ""], ["Wagner", "Christian", ""], ["Broomell", "Stephen", ""]]}, {"id": "2009.08761", "submitter": "Dietrich Kammer", "authors": "Elena Stoll and Dietrich Kammer", "title": "Online Knowledge Base for Designing Shape-changing Interfaces using\n  Modular Workshop Elements", "comments": "9 Pages, 2 Figures, Peer-reviewed and accepted to the Workshop on\n  Visual Interface Design Methods (VIDEM 2020), held in conjunction with the\n  International Conference on Advanced Visual Interfaces (AVI 2020). Ischia,\n  Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building and maintaining knowledge about specific interface technologies is a\nchallenge. Current solutions include standard file-based document repositories,\nwikis, and other online tools. However, these solutions are often only\navailable in intranets, become outdated and do not support the acquisition of\nknowledge in an efficient manner. The effort to gain an overview and detailed\nknowledge about novel interface technologies can be overwhelming and requires\nto research and read many technical reports and scientific publications. We\npropose to implement open source online knowledge bases that include building\nblocks for creating custom workshops to understand and apply the contained\nknowledge. We demonstrate this concept with a knowledge base for shape-changing\ninterfaces (SCI-KB). The SCI-KB is hosted online at GitHub and fosters\ncollaborative creation of knowledge elements accompanied by practical exercises\nand workshop elements that can be combined and adapted by individuals or groups\nof people new to the topic of shape-changing interfaces.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 11:56:50 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Stoll", "Elena", ""], ["Kammer", "Dietrich", ""]]}, {"id": "2009.08798", "submitter": "Yu Guan", "authors": "Xi Chen, Yu Guan, Jian-Qing Shi, Xiu-Li Du, Janet Eyre", "title": "Automated Stroke Rehabilitation Assessment using Wearable Accelerometers\n  in Free-Living Environments", "comments": "submitted to ACM IMWUT", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stroke is known as a major global health problem, and for stroke survivors it\nis key to monitor the recovery levels. However, traditional stroke\nrehabilitation assessment methods (such as the popular clinical assessment) can\nbe subjective and expensive, and it is also less convenient for patients to\nvisit clinics in a high frequency. To address this issue, in this work based on\nwearable sensing and machine learning techniques, we developed an automated\nsystem that can predict the assessment score in an objective manner. With\nwrist-worn sensors, accelerometer data was collected from 59 stroke survivors\nin free-living environments for a duration of 8 weeks, and we aim to map the\nweek-wise accelerometer data (3 days per week) to the assessment score by\ndeveloping signal processing and predictive model pipeline. To achieve this, we\nproposed two types of new features, which can encode the rehabilitation\ninformation from both paralysed/non-paralysed sides while suppressing the\nhigh-level noises such as irrelevant daily activities. Based on the proposed\nfeatures, we further developed the longitudinal mixed-effects model with\nGaussian process prior (LMGP), which can model the random effects caused by\ndifferent subjects and time slots (during the 8 weeks). Comprehensive\nexperiments were conducted to evaluate our system on both acute and chronic\npatients, and the results suggested its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:10:48 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 22:47:23 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chen", "Xi", ""], ["Guan", "Yu", ""], ["Shi", "Jian-Qing", ""], ["Du", "Xiu-Li", ""], ["Eyre", "Janet", ""]]}, {"id": "2009.08976", "submitter": "Byungsoo Kim", "authors": "Byungsoo Kim, Hongseok Suh, Jaewe Heo, Youngduck Choi", "title": "AI-Driven Interface Design for Intelligent Tutoring System Improves\n  Student Engagement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Intelligent Tutoring System (ITS) has been shown to improve students'\nlearning outcomes by providing a personalized curriculum that addresses\nindividual needs of every student. However, despite the effectiveness and\nefficiency that ITS brings to students' learning process, most of the studies\nin ITS research have conducted less effort to design the interface of ITS that\npromotes students' interest in learning, motivation and engagement by making\nbetter use of AI features. In this paper, we explore AI-driven design for the\ninterface of ITS describing diagnostic feedback for students' problem-solving\nprocess and investigate its impacts on their engagement. We propose several\ninterface designs powered by different AI components and empirically evaluate\ntheir impacts on student engagement through Santa, an active mobile ITS.\nControlled A/B tests conducted on more than 20K students in the wild show that\nAI-driven interface design improves the factors of engagement by up to 25.13%.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 10:32:01 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kim", "Byungsoo", ""], ["Suh", "Hongseok", ""], ["Heo", "Jaewe", ""], ["Choi", "Youngduck", ""]]}, {"id": "2009.09029", "submitter": "Rahul Arora", "authors": "Rahul Arora and Karan Singh", "title": "Mid-Air Drawing of Curves on 3D Surfaces in Virtual Reality", "comments": "Accepted to ACM Transactions on Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex 3D curves can be created by directly drawing mid-air in immersive\nenvironments (Augmented and Virtual Realities). Drawing mid-air strokes\nprecisely on the surface of a 3D virtual object, however, is difficult;\nnecessitating a projection of the mid-air stroke onto the user \"intended\"\nsurface curve. We present the first detailed investigation of the fundamental\nproblem of 3D stroke projection in VR. An assessment of the design requirements\nof real-time drawing of curves on 3D objects in VR is followed by the\ndefinition and classification of multiple techniques for 3D stroke projection.\nWe analyze the advantages and shortcomings of these approaches both\ntheoretically and via practical pilot testing. We then formally evaluate the\ntwo most promising techniques spraycan and mimicry with 20 users in VR. The\nstudy shows a strong qualitative and quantitative user preference for our novel\nstroke mimicry projection algorithm. We further illustrate the effectiveness\nand utility of stroke mimicry, to draw complex 3D curves on surfaces for\nvarious artistic and functional design applications.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 19:01:08 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 19:42:07 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Arora", "Rahul", ""], ["Singh", "Karan", ""]]}, {"id": "2009.09048", "submitter": "Richard Savery", "authors": "Richard Savery, Lisa Zahray, Gil Weinberg", "title": "Emotional Musical Prosody for the Enhancement of Trust in Robotic Arm\n  Communication", "comments": "SCRITA 2020 Trust, Acceptance and Social Cues in Human-Robot\n  Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robotic arms become prevalent in industry it is crucial to improve levels\nof trust from human collaborators. Low levels of trust in human-robot\ninteraction can reduce overall performance and prevent full robot utilization.\nWe investigated the potential benefits of using emotional musical prosody to\nallow the robot to respond emotionally to the user's actions. We tested\nparticipants' responses to interacting with a virtual robot arm that acted as a\ndecision agent, helping participants select the next number in a sequence. We\ncompared results from three versions of the application in a between-group\nexperiment, where the robot had different emotional reactions to the user's\ninput depending on whether the user agreed with the robot and whether the\nuser's choice was correct. In all versions, the robot reacted with emotional\ngestures. One version used prosody-based emotional audio phrases selected from\nour dataset of singer improvisations, the second version used audio consisting\nof a single pitch randomly assigned to each emotion, and the final version used\nno audio, only gestures. Our results showed no significant difference for the\npercentage of times users from each group agreed with the robot, and no\ndifference between user's agreement with the robot after it made a mistake.\nHowever, participants also took a trust survey following the interaction, and\nwe found that the reported trust ratings of the musical prosody group were\nsignificantly higher than both the single-pitch and no audio groups.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 20:05:13 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Savery", "Richard", ""], ["Zahray", "Lisa", ""], ["Weinberg", "Gil", ""]]}, {"id": "2009.09049", "submitter": "Simon Razniewski", "authors": "Jesse Josua Benjamin, Claudia M\\\"uller-Birn, Simon Razniewski", "title": "Examining the Impact of Algorithm Awareness on Wikidata's Recommender\n  System Recoin", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global infrastructure of the Web, designed as an open and transparent\nsystem, has a significant impact on our society. However, algorithmic systems\nof corporate entities that neglect those principles increasingly populated the\nWeb. Typical representatives of these algorithmic systems are recommender\nsystems that influence our society both on a scale of global politics and\nduring mundane shopping decisions. Recently, such recommender systems have come\nunder critique for how they may strengthen existing or even generate new kinds\nof biases. To this end, designers and engineers are increasingly urged to make\nthe functioning and purpose of recommender systems more transparent. Our\nresearch relates to the discourse of algorithm awareness, that reconsiders the\nrole of algorithm visibility in interface design. We conducted online\nexperiments with 105 participants using MTurk for the recommender system\nRecoin, a gadget for Wikidata. In these experiments, we presented users with\none of a set of three different designs of Recoin's user interface, each of\nthem exhibiting a varying degree of explainability and interactivity. Our\nfindings include a positive correlation between comprehension of and trust in\nan algorithmic system in our interactive redesign. However, our results are not\nconclusive yet, and suggest that the measures of comprehension, fairness,\naccuracy and trust are not yet exhaustive for the empirical study of algorithm\nawareness. Our qualitative insights provide a first indication for further\nmeasures. Our study participants, for example, were less concerned with the\ndetails of understanding an algorithmic calculation than with who or what is\njudging the result of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 20:06:53 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Benjamin", "Jesse Josua", ""], ["M\u00fcller-Birn", "Claudia", ""], ["Razniewski", "Simon", ""]]}, {"id": "2009.09053", "submitter": "Mahmood Jasim", "authors": "Mahmood Jasim, Pooya Khaloo, Somin Wadhwa, Amy X. Zhang, Ali Sarvghad,\n  Narges Mahyar", "title": "CommunityClick: Capturing and Reporting Community Feedback from Town\n  Halls to Improve Inclusivity", "comments": "32 pages, 5 figures, 4 tables, to be published in ACM Conference on\n  Computer-Supported Cooperative Work and Social Computing (CSCW)", "journal-ref": "Proceedings of the ACM on Human-Computer Interaction, January 2021", "doi": "10.1145/3432912", "report-no": "213", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local governments still depend on traditional town halls for community\nconsultation, despite problems such as a lack of inclusive participation for\nattendees and difficulty for civic organizers to capture attendees' feedback in\nreports. Building on a formative study with 66 town hall attendees and 20\norganizers, we designed and developed CommunityClick, a communitysourcing\nsystem that captures attendees' feedback in an inclusive manner and enables\norganizers to author more comprehensive reports. During the meeting, in\naddition to recording meeting audio to capture vocal attendees' feedback, we\nmodify iClickers to give voice to reticent attendees by allowing them to\nprovide real-time feedback beyond a binary signal. This information then\nautomatically feeds into a meeting transcript augmented with attendees'\nfeedback and organizers' tags. The augmented transcript along with a\nfeedback-weighted summary of the transcript generated from text analysis\nmethods is incorporated into an interactive authoring tool for organizers to\nwrite reports. From a field experiment at a town hall meeting, we demonstrate\nhow CommunityClick can improve inclusivity by providing multiple avenues for\nattendees to share opinions. Additionally, interviews with eight expert\norganizers demonstrate CommunityClick's utility in creating more comprehensive\nand accurate reports to inform critical civic decision-making. We discuss the\npossibility of integrating CommunityClick with town hall meetings in the future\nas well as expanding to other domains.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 20:23:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jasim", "Mahmood", ""], ["Khaloo", "Pooya", ""], ["Wadhwa", "Somin", ""], ["Zhang", "Amy X.", ""], ["Sarvghad", "Ali", ""], ["Mahyar", "Narges", ""]]}, {"id": "2009.09266", "submitter": "Johannes Schneider", "authors": "Johannes Schneider", "title": "Humans learn too: Better Human-AI Interaction using Optimized Human\n  Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans rely more and more on systems with AI components. The AI community\ntypically treats human inputs as a given and optimizes AI models only. This\nthinking is one-sided and it neglects the fact that humans can learn, too. In\nthis work, human inputs are optimized for better interaction with an AI model\nwhile keeping the model fixed. The optimized inputs are accompanied by\ninstructions on how to create them. They allow humans to save time and cut on\nerrors, while keeping required changes to original inputs limited. We propose\ncontinuous and discrete optimization methods modifying samples in an iterative\nfashion. Our quantitative and qualitative evaluation including a human study on\ndifferent hand-generated inputs shows that the generated proposals lead to\nlower error rates, require less effort to create and differ only modestly from\nthe original samples.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 16:30:37 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Schneider", "Johannes", ""]]}, {"id": "2009.09354", "submitter": "Ruturaj Raval", "authors": "Ruturaj Raval", "title": "An Improved Approach of Intention Discovery with Machine Learning for\n  POMDP-based Dialogue Management", "comments": "In addition to my thesis: https://scholar.uwindsor.ca/etd/7731/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An Embodied Conversational Agent (ECA) is an intelligent agent that works as\nthe front end of software applications to interact with users through\nverbal/nonverbal expressions and to provide online assistance without the\nlimits of time, location, and language. To help to improve the experience of\nhuman-computer interaction, there is an increasing need to empower ECA with not\nonly the realistic look of its human counterparts but also a higher level of\nintelligence. This thesis first highlights the main topics related to the\nconstruction of ECA, including different approaches of dialogue management, and\nthen discusses existing techniques of trend analysis for its application in\nuser classification. As a further refinement and enhancement to prior work on\nECA, this thesis research proposes a cohesive framework to integrate\nemotion-based facial animation with improved intention discovery. In addition,\na machine learning technique is introduced to support sentiment analysis for\nthe adjustment of policy design in POMDP-based dialogue management. The\nproposed research work is going to improve the accuracy of intention discovery\nwhile reducing the length of dialogues.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 05:28:36 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Raval", "Ruturaj", ""]]}, {"id": "2009.09575", "submitter": "Francisco Cruz", "authors": "Adam Bignold, Francisco Cruz, Richard Dazeley, Peter Vamplew, Cameron\n  Foale", "title": "Human Engagement Providing Evaluative and Informative Advice for\n  Interactive Reinforcement Learning", "comments": "33 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is an approach used by intelligent agents to\nautonomously learn new skills. Although reinforcement learning has been\ndemonstrated to be an effective learning approach in several different\ncontexts, a common drawback exhibited is the time needed in order to\nsatisfactorily learn a task, especially in large state-action spaces. To\naddress this issue, interactive reinforcement learning proposes the use of\nexternally-sourced information in order to speed up the learning process. Up to\nnow, different information sources have been used to give advice to the learner\nagent, among them human-sourced advice. When interacting with a learner agent,\nhumans may provide either evaluative or informative advice. From the agent's\nperspective these styles of interaction are commonly referred to as\nreward-shaping and policy-shaping respectively. Evaluation requires the human\nto provide feedback on the prior action performed, while informative advice\nthey provide advice on the best action to select for a given situation. Prior\nresearch has focused on the effect of human-sourced advice on the interactive\nreinforcement learning process, specifically aiming to improve the learning\nspeed of the agent, while reducing the engagement with the human. This work\npresents an experimental setup for a human-trial designed to compare the\nmethods people use to deliver advice in term of human engagement. Obtained\nresults show that users giving informative advice to the learner agents provide\nmore accurate advice, are willing to assist the learner agent for a longer\ntime, and provide more advice per episode. Additionally, self-evaluation from\nparticipants using the informative approach has indicated that the agent's\nability to follow the advice is higher, and therefore, they feel their own\nadvice to be of higher accuracy when compared to people providing evaluative\nadvice.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 02:14:02 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Bignold", "Adam", ""], ["Cruz", "Francisco", ""], ["Dazeley", "Richard", ""], ["Vamplew", "Peter", ""], ["Foale", "Cameron", ""]]}, {"id": "2009.09585", "submitter": "Yang Li", "authors": "Yang Li, Boxun Fu, Fu Li, Guangming Shi, Wenming Zheng", "title": "A Novel Transferability Attention Neural Network Model for EEG Emotion\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existed methods for electroencephalograph (EEG) emotion recognition\nalways train the models based on all the EEG samples indistinguishably.\nHowever, some of the source (training) samples may lead to a negative influence\nbecause they are significant dissimilar with the target (test) samples. So it\nis necessary to give more attention to the EEG samples with strong\ntransferability rather than forcefully training a classification model by all\nthe samples. Furthermore, for an EEG sample, from the aspect of neuroscience,\nnot all the brain regions of an EEG sample contains emotional information that\ncan transferred to the test data effectively. Even some brain region data will\nmake strong negative effect for learning the emotional classification model.\nConsidering these two issues, in this paper, we propose a transferable\nattention neural network (TANN) for EEG emotion recognition, which learns the\nemotional discriminative information by highlighting the transferable EEG brain\nregions data and samples adaptively through local and global attention\nmechanism. This can be implemented by measuring the outputs of multiple\nbrain-region-level discriminators and one single sample-level discriminator. We\nconduct the extensive experiments on three public EEG emotional datasets. The\nresults validate that the proposed model achieves the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 02:42:30 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Li", "Yang", ""], ["Fu", "Boxun", ""], ["Li", "Fu", ""], ["Shi", "Guangming", ""], ["Zheng", "Wenming", ""]]}, {"id": "2009.09776", "submitter": "Muhammad Umair Khan Kaker", "authors": "Muhammad Umair Khan, Khawar Saeed, Sidra Qadeer", "title": "Weight Training Analysis of Sportsmen with Kinect Bioinformatics for\n  Form Improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports franchises invest a lot in training their athletes. use of latest\ntechnology for this purpose is also very common. We propose a system of\ncapturing motion of athletes during weight training and analyzing that data to\nfind out any shortcomings and imperfections. Our system uses Kinect depth image\nto compute different parameters of athlete's selected joints. These parameters\nare passed through certain algorithms to process them and formulate results on\ntheir basis. Some parameters like range of motion, speed and balance can be\nanalyzed in real time. But for comparison to be performed between motions, data\nis first recorded and stored and then processed for accurate results. Our\nresults depict that this system can be easily deployed and implemented to\nprovide a very valuable insight to dynamics of a work out and help an athlete\nin improving his form.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 04:52:31 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Khan", "Muhammad Umair", ""], ["Saeed", "Khawar", ""], ["Qadeer", "Sidra", ""]]}, {"id": "2009.09859", "submitter": "Karina Roundtree", "authors": "Karina A. Roundtree and Jason R. Cody and Jennifer Leaf and H. Onan\n  Demirel and Julie A. Adams", "title": "Transparency's Influence on Human-Collective Interactions", "comments": "58 pages, 34 figures, 39 tables. arXiv admin note: substantial text\n  overlap with arXiv:2003.10681", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective robotic systems are biologically inspired and advantageous due to\ntheir apparent global intelligence and emergent behaviors. Many applications\ncan benefit from the incorporation of collectives, including environmental\nmonitoring, disaster response missions, and infrastructure support.\nTransparency research has primarily focused on how the design of the models,\nvisualizations, and control mechanisms influence human-collective interactions.\nTraditionally most evaluations have focused only on one particular system\ndesign element, evaluating its respective transparency. This manuscript\nanalyzed two models and visualizations to understand how the system design\nelements impacted human-collective interactions, to quantify which model and\nvisualization combination provided the best transparency, and provide design\nguidance, based on remote supervision of collectives. The consensus\ndecision-making and baseline models, as well as an individual agent and\nabstract visualizations, were analyzed for sequential best-of-n decision-making\ntasks involving four collectives, composed of 200 entities each. Both models\nand visualizations provided transparency and influenced human-collective\ninteractions differently. No single combination provided the best transparency.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 00:49:21 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Roundtree", "Karina A.", ""], ["Cody", "Jason R.", ""], ["Leaf", "Jennifer", ""], ["Demirel", "H. Onan", ""], ["Adams", "Julie A.", ""]]}, {"id": "2009.10158", "submitter": "Lynsay Shepherd", "authors": "Jamie O'Hare and Lynsay A. Shepherd", "title": "Proposal of a Novel Bug Bounty Implementation Using Gamification", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant popularity, the bug bounty process has remained broadly\nunchanged since its inception, with limited implementation of gamification\naspects. Existing literature recognises that current methods generate intensive\nresource demands, and can encounter issues impacting program effectiveness.\nThis paper proposes a novel bug bounty process aiming to alleviate resource\ndemands and mitigate inherent issues. Through the additional crowdsourcing of\nreport verification where fellow hackers perform vulnerability verification and\nreproduction, the client organisation can reduce overheads at the cost of\nrewarding more participants. The incorporation of gamification elements\nprovides a substitute for monetary rewards, as well as presenting possible\nmitigation of bug bounty program effectiveness issues. Collectively, traits of\nthe proposed process appear appropriate for resource and budget-constrained\norganisations - such Higher Education institutions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 20:11:53 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["O'Hare", "Jamie", ""], ["Shepherd", "Lynsay A.", ""]]}, {"id": "2009.10194", "submitter": "Colin M. Gray", "authors": "Colin M. Gray, Cristiana Santos, Nataliia Bielova, Michael Toth,\n  Damian Clifford", "title": "Dark Patterns and the Legal Requirements of Consent Banners: An\n  Interaction Criticism Perspective", "comments": "18 pages", "journal-ref": "Proceedings of the 2021 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/3411764.3445779", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  User engagement with data privacy and security through consent banners has\nbecome a ubiquitous part of interacting with internet services. While previous\nwork has addressed consent banners from either interaction design, legal, and\nethics-focused perspectives, little research addresses the connections among\nmultiple disciplinary approaches, including tensions and opportunities that\ntranscend disciplinary boundaries. In this paper, we draw together perspectives\nand commentary from HCI, design, privacy and data protection, and legal\nresearch communities, using the language and strategies of \"dark patterns\" to\nperform an interaction criticism reading of three different types of consent\nbanners. Our analysis builds upon designer, interface, user, and social context\nlenses to raise tensions and synergies that arise together in complex,\ncontingent, and conflicting ways in the act of designing consent banners. We\nconclude with opportunities for transdisciplinary dialogue across legal,\nethical, computer science, and interactive systems scholarship to translate\nmatters of ethical concern into public policy.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 22:00:51 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 12:06:19 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Gray", "Colin M.", ""], ["Santos", "Cristiana", ""], ["Bielova", "Nataliia", ""], ["Toth", "Michael", ""], ["Clifford", "Damian", ""]]}, {"id": "2009.10259", "submitter": "Weixin Liang", "authors": "Weixin Liang, James Zou, Zhou Yu", "title": "ALICE: Active Learning with Contrastive Natural Language Explanations", "comments": null, "journal-ref": "EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a supervised neural network classifier typically requires many\nannotated training samples. Collecting and annotating a large number of data\npoints are costly and sometimes even infeasible. Traditional annotation process\nuses a low-bandwidth human-machine communication interface: classification\nlabels, each of which only provides several bits of information. We propose\nActive Learning with Contrastive Explanations (ALICE), an expert-in-the-loop\ntraining framework that utilizes contrastive natural language explanations to\nimprove data efficiency in learning. ALICE learns to first use active learning\nto select the most informative pairs of label classes to elicit contrastive\nnatural language explanations from experts. Then it extracts knowledge from\nthese explanations using a semantic parser. Finally, it incorporates the\nextracted knowledge through dynamically changing the learning model's\nstructure. We applied ALICE in two visual recognition tasks, bird species\nclassification and social relationship classification. We found by\nincorporating contrastive explanations, our models outperform baseline models\nthat are trained with 40-100% more training data. We found that adding 1\nexplanation leads to similar performance gain as adding 13-30 labeled training\ndata points.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 01:02:07 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Liang", "Weixin", ""], ["Zou", "James", ""], ["Yu", "Zhou", ""]]}, {"id": "2009.10261", "submitter": "Thao Tran Phuong", "authors": "Tran Phuong Thao", "title": "Influences of Temporal Factors on GPS-based Human Mobility Lifestyle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of human mobility from GPS trajectories becomes crucial in many\naspects such as policy planning for urban citizens, location-based service\nrecommendation/prediction, and especially mitigating the spread of biological\nand mobile viruses. In this paper, we propose a method to find temporal factors\naffecting the human mobility lifestyle. We collected GPS data from 100\nsmartphone users in Japan. We designed a model that consists of 13 temporal\npatterns. We then applied a multiple linear regression and found that people\ntend to keep their mobility habits on Thursday and the days in the second week\nof a month but tend to lose their habits on Friday. We also explained some\nreasons behind these findings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 01:15:51 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 01:48:17 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Thao", "Tran Phuong", ""]]}, {"id": "2009.10267", "submitter": "Ankit Agrawal", "authors": "Ankit Agrawal, Jan-Philipp Steghofer, Jane Cleland-Huang", "title": "Model-Driven Requirements for Humans-on-the-Loop Multi-UAV Missions", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of semi-autonomous Unmanned Aerial Vehicles (UAVs or drones) to\nsupport emergency response scenarios, such as fire surveillance and\nsearch-and-rescue, has the potential for huge societal benefits. Onboard\nsensors and artificial intelligence (AI) allow these UAVs to operate\nautonomously in the environment. However, human intelligence and domain\nexpertise are crucial in planning and guiding UAVs to accomplish the mission.\nTherefore, humans and multiple UAVs need to collaborate as a team to conduct a\ntime-critical mission successfully. We propose a meta-model to describe\ninteractions among the human operators and the autonomous swarm of UAVs. The\nmeta-model also provides a language to describe the roles of UAVs and humans\nand the autonomous decisions. We complement the meta-model with a template of\nrequirements elicitation questions to derive models for specific missions. We\nalso identify common scenarios where humans should collaborate with UAVs to\naugment the autonomy of the UAVs. We introduce the meta-model and the\nrequirements elicitation process with examples drawn from a search-and-rescue\nmission in which multiple UAVs collaborate with humans to respond to the\nemergency. We then apply it to a second scenario in which UAVs support first\nresponders in fighting a structural fire. Our results show that the meta-model\nand the template of questions support the modeling of the human-on-the-loop\nhuman interactions for these complex missions, suggesting that it is a useful\ntool for modeling the human-on-the-loop interactions for multi-UAVs missions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 01:43:05 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Agrawal", "Ankit", ""], ["Steghofer", "Jan-Philipp", ""], ["Cleland-Huang", "Jane", ""]]}, {"id": "2009.10278", "submitter": "Kovila  P.L. Coopamootoo", "authors": "Kovila P.L. Coopamootoo", "title": "Usage Patterns of Privacy-Enhancing Technologies", "comments": "To be published in the Proceedings of the 2020 ACM SIGSAC Conference\n  on Computer and Communications Security (CCS '20)", "journal-ref": null, "doi": "10.1145/3372297.3423347", "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The steady reports of privacy invasions online paints a picture of the\nInternet growing into a more dangerous place. This is supported by reports of\nthe potential scale for online harms facilitated by the mass deployment of\nonline technology and the data-intensive web. While Internet users often\nexpress concern about privacy, some report taking actions to protect their\nprivacy online. We investigate the methods and technologies that individuals\nemploy to protect their privacy online. We conduct two studies, of N=180 and\nN=907, to elicit individuals' use of privacy methods online, within the US, the\nUK and Germany. We find that non-technology methods are among the most used\nmethods in the three countries. We identify distinct groupings of privacy\nmethods usage in a cluster map. The map shows that together with non-technology\nmethods of privacy protection, simple PETs that are integrated in services,\nform the most used cluster, whereas more advanced PETs form a different, least\nused cluster. We further investigate user perception and reasoning for mostly\nusing one set of PETs in a third study with N=183 participants. We do not find\na difference in perceived competency in protecting privacy online between\nadvanced and simpler PETs users. We compare use perceptions between advanced\nand simpler PETs and report on user reasoning for not using advanced PETs, as\nwell as support needed for potential use. This paper contributes to privacy\nresearch by eliciting use and perception of use across $43$ privacy methods,\nincluding $26$ PETs across three countries and provides a map of PETs usage.\nThe cluster map provides a systematic and reliable point of reference for\nfuture user-centric investigations across PETs. Overall, this research provides\na broad understanding of use and perceptions across a collection of PETs, and\ncan lead to future research for scaling use of PETs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 02:17:37 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Coopamootoo", "Kovila P. L.", ""]]}, {"id": "2009.10317", "submitter": "Sirat Samyoun", "authors": "Sirat Samyoun, Sudipta Saha Shubha, Md Abu Sayeed Mondol, John A.\n  Stankovic", "title": "iWash: A Smartwatch Handwashing Quality Assessment and Reminder System\n  with Real-time Feedback in the Context of Infectious Disease", "comments": "19 pages, submitted in the Fifth IEEE/ACM Conference on Connected\n  Health: Applications, Systems and Engineering Technologies (CHASE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Washing hands properly and frequently is the simplest and most cost-effective\ninterventions to prevent the spread of infectious diseases. People are often\nignorant about proper handwashing in different situations and do not know if\nthey wash hands properly. Smartwatches are found to be effective for assessing\nthe quality of handwashing. However, the existing smartwatch based systems are\nnot comprehensive enough in terms of achieving accuracy as well as reminding\npeople to handwash and providing feedback to the user about the quality of\nhandwashing. On-device processing is often required to provide real-time\nfeedback to the user, and so it is important to develop a system that runs\nefficiently on low-resource devices like smartwatches. However, none of the\nexisting systems for handwashing quality assessment are optimized for on-device\nprocessing. We present iWash, a comprehensive system for quality assessment and\ncontext-aware reminder for handwashing with real-time feedback using\nsmartwatches. iWash is a hybrid deep neural network based system that is\noptimized for on-device processing to ensure high accuracy with minimal\nprocessing time and battery usage. Additionally, it is a context-aware system\nthat detects when the user is entering home using a Bluetooth beacon and\nprovides reminders to wash hands. iWash also offers touch-free interaction\nbetween the user and the smartwatch that minimizes the risk of germ\ntransmission. We collected a real-life dataset and conducted extensive\nevaluations to demonstrate the performance of iWash. Compared to the existing\nhandwashing quality assessment systems, we achieve around 12% higher accuracy\nfor quality assessment, as well as we reduce the processing time and battery\nusage by around 37% and 10%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 04:52:35 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Samyoun", "Sirat", ""], ["Shubha", "Sudipta Saha", ""], ["Mondol", "Md Abu Sayeed", ""], ["Stankovic", "John A.", ""]]}, {"id": "2009.10370", "submitter": "Bassem Seddik", "authors": "Bassem Seddik and Najoua Essoukri Ben Amara", "title": "Visual Methods for Sign Language Recognition: A Modality-Based Review", "comments": "This survey paper is accepted as Springer book chapter, currently\n  under edition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language visual recognition from continuous multi-modal streams is still\none of the most challenging fields.\n  Recent advances in human actions recognition are exploiting the ascension of\nGPU-based learning from massive data, and are getting closer to human-like\nperformances.\n  They are then prone to creating interactive services for the deaf and\nhearing-impaired communities.\n  A population that is expected to grow considerably in the years to come.\n  This paper aims at reviewing the human actions recognition literature with\nthe sign-language visual understanding as a scope.\n  The methods analyzed will be mainly organized according to the different\ntypes of unimodal inputs exploited, their relative multi-modal combinations and\npipeline steps.\n  In each section, we will detail and compare the related datasets, approaches\nthen distinguish the still open contribution paths suitable for the creation of\nsign language related services.\n  Special attention will be paid to the approaches and commercial solutions\nhandling facial expressions and continuous signing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 07:56:02 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Seddik", "Bassem", ""], ["Amara", "Najoua Essoukri Ben", ""]]}, {"id": "2009.10444", "submitter": "Manuel Aiple", "authors": "Manuel Aiple, Andre Schiele and Frans C.T. van der Helm", "title": "Self-Adapting Variable Impedance Actuator Control for Precision and\n  Dynamic Tasks", "comments": "12 pages, 13 figures, submitted to IEEE Transactions on Haptics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable impedance actuators (VIAs) as tool devices for teleoperation could\nextend the range of tasks that humans can perform through a teleoperated robot\nby mimicking the change of upper limb stiffness that humans perform for\ndifferent tasks, increasing the dynamic range of the robot. This requires\nappropriate impedance control. Goal of this study is to show the effectiveness\nof a controller that does not require additional sensors, reducing system\ncomplexity and increasing ease of use. The controller should allow to perform\nprecise positioning tasks and dynamic tasks like hammering through\nteleoperation with a VIA tool device automatically adapting the impedance\nsetting of the VIA. This is achieved by a control law according to the\nprinciple \"slow-stiff/fast-soft\". The controller was tested in a human user\nstudy with 24 participants comparing the human-machine performance with the\nself-adapting controller in a bilateral telemanipulation experiment with two\ntasks (precision/dynamic) using three impedance settings (high/low/adaptive\nimpedance). The results indicate that the proposed system performs equally well\nas state of the art stiff teleoperation devices for precision tasks, while\nhaving benefits in terms of increased safety and reduced wear for dynamic\ntasks. This is a step towards teleoperation with a wide dynamic range.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 10:51:59 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Aiple", "Manuel", ""], ["Schiele", "Andre", ""], ["van der Helm", "Frans C. T.", ""]]}, {"id": "2009.10643", "submitter": "Mary Beth Kery", "authors": "Mary Beth Kery, Donghao Ren, Fred Hohman, Dominik Moritz, Kanit\n  Wongsuphasawat, Kayur Patel", "title": "mage: Fluid Moves Between Code and Graphical Work in Computational\n  Notebooks", "comments": null, "journal-ref": null, "doi": "10.1145/3379337.3415842", "report-no": null, "categories": "cs.HC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We aim to increase the flexibility at which a data worker can choose the\nright tool for the job, regardless of whether the tool is a code library or an\ninteractive graphical user interface (GUI). To achieve this flexibility, we\nextend computational notebooks with a new API mage, which supports tools that\ncan represent themselves as both code and GUI as needed. We discuss the design\nof mage as well as design opportunities in the space of flexible code/GUI tools\nfor data work. To understand tooling needs, we conduct a study with nine\nprofessional practitioners and elicit their feedback on mage and potential\nareas for flexible code/GUI tooling. We then implement six client tools for\nmage that illustrate the main themes of our study findings. Finally, we discuss\nopen challenges in providing flexible code/GUI interactions for data workers.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 15:56:28 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Kery", "Mary Beth", ""], ["Ren", "Donghao", ""], ["Hohman", "Fred", ""], ["Moritz", "Dominik", ""], ["Wongsuphasawat", "Kanit", ""], ["Patel", "Kayur", ""]]}, {"id": "2009.10750", "submitter": "Sahisnu Mazumder", "authors": "Bing Liu, Sahisnu Mazumder", "title": "Lifelong Learning Dialogue Systems: Chatbots that Self-Learn On the Job", "comments": "A revised version of this work has been published in AAAI-2021 with\n  title: \"Lifelong and Continual Learning Dialogue Systems: Learning during\n  Conversation\". Please use this revised AAAI-21 version for citation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue systems, also called chatbots, are now used in a wide range of\napplications. However, they still have some major weaknesses. One key weakness\nis that they are typically trained from manually-labeled data and/or written\nwith handcrafted rules, and their knowledge bases (KBs) are also compiled by\nhuman experts. Due to the huge amount of manual effort involved, they are\ndifficult to scale and also tend to produce many errors ought to their limited\nability to understand natural language and the limited knowledge in their KBs.\nThus, the level of user satisfactory is often low. In this paper, we propose to\ndramatically improve this situation by endowing the system the ability to\ncontinually learn (1) new world knowledge, (2) new language expressions to\nground them to actions, and (3) new conversational skills, during conversation\nor \"on the job\" by themselves so that as the systems chat more and more with\nusers, they become more and more knowledgeable and are better and better able\nto understand diverse natural language expressions and improve their\nconversational skills. A key approach to achieving these is to exploit the\nmulti-user environment of such systems to self-learn through interactions with\nusers via verb and non-verb means. The paper discusses not only key challenges\nand promising directions to learn from users during conversation but also how\nto ensure the correctness of the learned knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 18:10:08 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 00:10:21 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Liu", "Bing", ""], ["Mazumder", "Sahisnu", ""]]}, {"id": "2009.10760", "submitter": "Taras Kucherenko", "authors": "Patrik Jonell, Taras Kucherenko, Ilaria Torre, Jonas Beskow", "title": "Can we trust online crowdworkers? Comparing online and offline\n  participants in a preference test of virtual agents", "comments": "Patrik Jonell and Taras Kucherenko contributed equally to this work.\n  Published at the Proceedings of the 20th ACM International Conference on\n  Intelligent Virtual Agent. 8 pages, 7 figures", "journal-ref": null, "doi": "10.1145/3383652.3423860", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conducting user studies is a crucial component in many scientific fields.\nWhile some studies require participants to be physically present, other studies\ncan be conducted both physically (e.g. in-lab) and online (e.g. via\ncrowdsourcing). Inviting participants to the lab can be a time-consuming and\nlogistically difficult endeavor, not to mention that sometimes research groups\nmight not be able to run in-lab experiments, because of, for example, a\npandemic. Crowdsourcing platforms such as Amazon Mechanical Turk (AMT) or\nProlific can therefore be a suitable alternative to run certain experiments,\nsuch as evaluating virtual agents. Although previous studies investigated the\nuse of crowdsourcing platforms for running experiments, there is still\nuncertainty as to whether the results are reliable for perceptual studies. Here\nwe replicate a previous experiment where participants evaluated a gesture\ngeneration model for virtual agents. The experiment is conducted across three\nparticipant pools -- in-lab, Prolific, and AMT -- having similar demographics\nacross the in-lab participants and the Prolific platform. Our results show no\ndifference between the three participant pools in regards to their evaluations\nof the gesture generation models and their reliability scores. The results\nindicate that online platforms can successfully be used for perceptual\nevaluations of this kind.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 18:43:28 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 09:01:24 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Jonell", "Patrik", ""], ["Kucherenko", "Taras", ""], ["Torre", "Ilaria", ""], ["Beskow", "Jonas", ""]]}, {"id": "2009.10947", "submitter": "Glebys Gonzalez", "authors": "Glebys Gonzalez and Juan Wachs", "title": "Pose Imitation Constraints for Collaborative Robots", "comments": "9 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving human-like motion in robots has been a fundamental goal in many\nareas of robotics research. Inverse kinematic (IK) solvers have been explored\nas a solution to provide kinematic structures with anthropomorphic movements.\nIn particular, numeric solvers based on geometry, such as FABRIK, have shown\npotential for producing human-like motion at a low computational cost.\nNevertheless, these methods have shown limitations when solving for robot\nkinematic constraints. This work proposes a framework inspired by FABRIK for\nhuman pose imitation in real-time. The goal is to mitigate the problems of the\noriginal algorithm while retaining the resulting humanlike fluidity and low\ncost. We first propose a human constraint model for pose imitation. Then, we\npresent a pose imitation algorithm (PIC), and it's soft version (PICs) that can\nsuccessfully imitate human poses using the proposed constraint system. PIC was\ntested on two collaborative robots (Baxter and YuMi). Fifty human\ndemonstrations were collected for a bi-manual assembly and an incision task.\nThen, two performance metrics were obtained for both robots: pose accuracy with\nrespect to the human and the percentage of environment occlusion/obstruction.\nThe performance of PIC and PICs was compared against the numerical solver\nbaseline (FABRIK). The proposed algorithms achieve a higher pose accuracy than\nFABRIK for both tasks (25%-FABRIK, 53%-PICs, 58%-PICs). In addition, PIC and\nit's soft version achieve a lower percentage of occlusion during incision\n(10%-FABRIK, 4%-PICs, 9%-PICs). These results indicate that the PIC method can\nreproduce human poses and achieve key desired effects of human imitation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 06:33:48 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 23:48:26 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Gonzalez", "Glebys", ""], ["Wachs", "Juan", ""]]}, {"id": "2009.10991", "submitter": "Darshana Priyasad Madduma Kankanamalage Don Mr", "authors": "Darshana Priyasad, Tharindu Fernando, Simon Denman, Clinton Fookes,\n  Sridha Sridharan", "title": "Attention Driven Fusion for Multi-Modal Emotion Recognition", "comments": "An updated version of the ICASSP 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has emerged as a powerful alternative to hand-crafted methods\nfor emotion recognition on combined acoustic and text modalities. Baseline\nsystems model emotion information in text and acoustic modes independently\nusing Deep Convolutional Neural Networks (DCNN) and Recurrent Neural Networks\n(RNN), followed by applying attention, fusion, and classification. In this\npaper, we present a deep learning-based approach to exploit and fuse text and\nacoustic data for emotion classification. We utilize a SincNet layer, based on\nparameterized sinc functions with band-pass filters, to extract acoustic\nfeatures from raw audio followed by a DCNN. This approach learns filter banks\ntuned for emotion recognition and provides more effective features compared to\ndirectly applying convolutions over the raw speech signal. For text processing,\nwe use two branches (a DCNN and a Bi-direction RNN followed by a DCNN) in\nparallel where cross attention is introduced to infer the N-gram level\ncorrelations on hidden representations received from the Bi-RNN. Following\nexisting state-of-the-art, we evaluate the performance of the proposed system\non the IEMOCAP dataset. Experimental results indicate that the proposed system\noutperforms existing methods, achieving 3.5% improvement in weighted accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 08:07:58 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 22:25:20 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Priyasad", "Darshana", ""], ["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Fookes", "Clinton", ""], ["Sridharan", "Sridha", ""]]}, {"id": "2009.11008", "submitter": "Duy Minh Ho Nguyen", "authors": "Duy M. H. Nguyen, Duy M. Nguyen, Huong Vu, Binh T. Nguyen, Fabrizio\n  Nunnari, Daniel Sonntag", "title": "An Attention Mechanism with Multiple Knowledge Sources for COVID-19\n  Detection from CT Images", "comments": "In AAAI 2021 Workshop: Trustworthy AI for Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until now, Coronavirus SARS-CoV-2 has caused more than 850,000 deaths and\ninfected more than 27 million individuals in over 120 countries. Besides\nprincipal polymerase chain reaction (PCR) tests, automatically identifying\npositive samples based on computed tomography (CT) scans can present a\npromising option in the early diagnosis of COVID-19. Recently, there have been\nincreasing efforts to utilize deep networks for COVID-19 diagnosis based on CT\nscans. While these approaches mostly focus on introducing novel architectures,\ntransfer learning techniques, or construction large scale data, we propose a\nnovel strategy to improve the performance of several baselines by leveraging\nmultiple useful information sources relevant to doctors' judgments.\nSpecifically, infected regions and heat maps extracted from learned networks\nare integrated with the global image via an attention mechanism during the\nlearning process. This procedure not only makes our system more robust to noise\nbut also guides the network focusing on local lesion areas. Extensive\nexperiments illustrate the superior performance of our approach compared to\nrecent baselines. Furthermore, our learned network guidance presents an\nexplainable feature to doctors as we can understand the connection between\ninput and output in a grey-box model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 09:05:24 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 01:31:50 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 16:22:57 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 15:16:06 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Nguyen", "Duy M. H.", ""], ["Nguyen", "Duy M.", ""], ["Vu", "Huong", ""], ["Nguyen", "Binh T.", ""], ["Nunnari", "Fabrizio", ""], ["Sonntag", "Daniel", ""]]}, {"id": "2009.11186", "submitter": "EPTCS", "authors": "Abeer Dyoub (University of L'Aquila, Italy), Stefania Costantini\n  (University of L'Aquila, Italy), Francesca A. Lisi (University of Bari \"A.\n  Moro\", Italy)", "title": "Logic Programming and Machine Ethics", "comments": "In Proceedings ICLP 2020, arXiv:2009.09158. Invited paper for the\n  ICLP2020 Panel on \"Machine Ethics\". arXiv admin note: text overlap with\n  arXiv:1909.08255", "journal-ref": "EPTCS 325, 2020, pp. 6-17", "doi": "10.4204/EPTCS.325.6", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transparency is a key requirement for ethical machines. Verified ethical\nbehavior is not enough to establish justified trust in autonomous intelligent\nagents: it needs to be supported by the ability to explain decisions. Logic\nProgramming (LP) has a great potential for developing such perspective ethical\nsystems, as in fact logic rules are easily comprehensible by humans.\nFurthermore, LP is able to model causality, which is crucial for ethical\ndecision making.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 00:47:18 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Dyoub", "Abeer", "", "University of L'Aquila, Italy"], ["Costantini", "Stefania", "", "University of L'Aquila, Italy"], ["Lisi", "Francesca A.", "", "University of Bari \"A.\n  Moro\", Italy"]]}, {"id": "2009.11195", "submitter": "Amr Gomaa", "authors": "Amr Gomaa, Guillermo Reyes, Alexandra Alles, Lydia Rupp and Michael\n  Feld", "title": "Studying Person-Specific Pointing and Gaze Behavior for Multimodal\n  Referencing of Outside Objects from a Moving Vehicle", "comments": null, "journal-ref": "In Proceedings of the 2020 International Conference on Multimodal\n  Interaction, pp. 501-509. 2020", "doi": "10.1145/3382507.3418817", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand pointing and eye gaze have been extensively investigated in automotive\napplications for object selection and referencing. Despite significant\nadvances, existing outside-the-vehicle referencing methods consider these\nmodalities separately. Moreover, existing multimodal referencing methods focus\non a static situation, whereas the situation in a moving vehicle is highly\ndynamic and subject to safety-critical constraints. In this paper, we\ninvestigate the specific characteristics of each modality and the interaction\nbetween them when used in the task of referencing outside objects (e.g.\nbuildings) from the vehicle. We furthermore explore person-specific differences\nin this interaction by analyzing individuals' performance for pointing and gaze\npatterns, along with their effect on the driving task. Our statistical analysis\nshows significant differences in individual behaviour based on object's\nlocation (i.e. driver's right side vs. left side), object's surroundings,\ndriving mode (i.e. autonomous vs. normal driving) as well as pointing and gaze\nduration, laying the foundation for a user-adaptive approach.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 14:56:19 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Gomaa", "Amr", ""], ["Reyes", "Guillermo", ""], ["Alles", "Alexandra", ""], ["Rupp", "Lydia", ""], ["Feld", "Michael", ""]]}, {"id": "2009.11225", "submitter": "Aditya Jyoti Paul", "authors": "Aditya Jyoti Paul", "title": "Randomized fast no-loss expert system to play tic tac toe like a human", "comments": "Author's version of the paper published in IET Cognitive Computation\n  and Systems. For the journal-typeset version, please see\n  https://doi.org/10.1049/ccs.2020.0018", "journal-ref": "Cognitive Computation and Systems, Volume 2, Issue 4, December\n  2020, pp. 231 - 241", "doi": "10.1049/ccs.2020.0018", "report-no": null, "categories": "cs.AI cs.GT cs.HC cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces a blazingly fast, no-loss expert system for Tic Tac Toe\nusing Decision Trees called T3DT, that tries to emulate human gameplay as\nclosely as possible. It does not make use of any brute force, minimax or\nevolutionary techniques, but is still always unbeatable. In order to make the\ngameplay more human-like, randomization is prioritized and T3DT randomly\nchooses one of the multiple optimal moves at each step. Since it does not need\nto analyse the complete game tree at any point, T3DT is exceptionally faster\nthan any brute force or minimax algorithm, this has been shown theoretically as\nwell as empirically from clock-time analyses in this paper. T3DT also doesn't\nneed the data sets or the time to train an evolutionary model, making it a\npractical no-loss approach to play Tic Tac Toe.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 15:41:10 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 23:37:32 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Paul", "Aditya Jyoti", ""]]}, {"id": "2009.11247", "submitter": "Mohammad Rafayet Ali", "authors": "Mohammad Rafayet Ali, Taylan Sen, Benjamin Kane, Shagun Bose, Thomas M\n  Carroll, Ronald Epstein, Lenhart Schubert, Ehsan Hoque", "title": "Novel Computational Linguistic Measures, Dialogue System and the\n  Development of SOPHIE: Standardized Online Patient for Healthcare Interaction\n  Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the iterative participatory design of SOPHIE, an\nonline virtual patient for feedback-based practice of sensitive\npatient-physician conversations, and discuss an initial qualitative evaluation\nof the system by professional end users. The design of SOPHIE was motivated\nfrom a computational linguistic analysis of the transcripts of 383\npatient-physician conversations from an essential office visit of late stage\ncancer patients with their oncologists. We developed methods for the automatic\ndetection of two behavioral paradigms, lecturing and positive language usage\npatterns (sentiment trajectory of conversation), that are shown to be\nsignificantly associated with patient prognosis understanding. These automated\nmetrics associated with effective communication were incorporated into SOPHIE,\nand a pilot user study identified that SOPHIE was favorably reviewed by a user\ngroup of practicing physicians.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 16:47:51 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ali", "Mohammad Rafayet", ""], ["Sen", "Taylan", ""], ["Kane", "Benjamin", ""], ["Bose", "Shagun", ""], ["Carroll", "Thomas M", ""], ["Epstein", "Ronald", ""], ["Schubert", "Lenhart", ""], ["Hoque", "Ehsan", ""]]}, {"id": "2009.11404", "submitter": "Erin Brintnell", "authors": "Erin Brintnell, Owen Brierley, Neil Christensen, Christian Jacob", "title": "Balancing simulation and gameplay -- applying game user research to\n  LeukemiaSIM", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bioinformatics researcher and a game design researcher walk into a lab...\nThis paper shares two case-studies of a collaboration between a bioinformatics\nresearcher who is developing a set of educational VR simulations for youth and\na consultative game design researcher with a background in games User Research\n(GUR) techniques who assesses and iteratively improves the player experience in\nthe simulations. By introducing games-based player engagement strategies, the\ntwo researchers improve the (re)playability of these VR simulations to\nencourage greater player engagement and retention.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 22:29:04 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Brintnell", "Erin", ""], ["Brierley", "Owen", ""], ["Christensen", "Neil", ""], ["Jacob", "Christian", ""]]}, {"id": "2009.11422", "submitter": "Bruno Augusto Nassif Travencolo", "authors": "Jean R. Ponciano, Claudio D. G. Linhares, Elaine R. Faria, and Bruno\n  A. N. Travencolo", "title": "An Online and Nonuniform Timeslicing Method for Network Visualisation", "comments": null, "journal-ref": null, "doi": "10.1016/j.cag.2021.04.006", "report-no": null, "categories": "cs.SI cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual analysis of temporal networks comprises an effective way to understand\nthe network dynamics, facilitating the identification of patterns, anomalies,\nand other network properties, thus resulting in fast decision making. The\namount of data in real-world networks, however, may result in a layout with\nhigh visual clutter due to edge overlapping. This is particularly relevant in\nthe so-called streaming networks, in which edges are continuously arriving\n(online) and in non-stationary distribution. All three network dimensions,\nnamely node, edge, and time, can be manipulated to reduce such clutter and\nimprove readability. This paper presents an online and nonuniform timeslicing\nmethod, thus considering the underlying network structure and addressing\nstreaming network analyses. We conducted experiments using two real-world\nnetworks to compare our method against uniform and nonuniform timeslicing\nstrategies. The results show that our method automatically selects timeslices\nthat effectively reduce visual clutter in periods with bursts of events. As a\nconsequence, decision making based on the identification of global temporal\npatterns becomes faster and more reliable.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 00:21:56 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Ponciano", "Jean R.", ""], ["Linhares", "Claudio D. G.", ""], ["Faria", "Elaine R.", ""], ["Travencolo", "Bruno A. N.", ""]]}, {"id": "2009.11483", "submitter": "Eshwar Chandrasekharan", "authors": "Eshwar Chandrasekharan, Shagun Jhaver, Amy Bruckman, Eric Gilbert", "title": "Quarantined! Examining the Effects of a Community-Wide Moderation\n  Intervention on Reddit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Should social media platforms intervene when communities repeatedly break\nrules? What actions can they consider? In light of this hotly debated issue,\nplatforms have begun experimenting with softer alternatives to outright bans.\nWe examine one such intervention called quarantining, that impedes direct\naccess to and promotion of controversial communities. Specifically, we present\ntwo case studies of what happened when Reddit quarantined the influential\ncommunities r/TheRedPill (TRP) and r/The_Donald (TD). Working with over 85M\nReddit posts, we apply causal inference methods to examine the quarantine's\neffects on TRP and TD. We find that the quarantine made it more difficult to\nrecruit new members: new user influx to TRP and TD decreased by 79.5% and 58%,\nrespectively. Despite quarantining, existing users' misogyny and racism levels\nremained unaffected. We conclude by reflecting on the effectiveness of this\ndesign friction in limiting the influence of toxic communities and discuss\nbroader implications for content moderation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 04:32:21 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 22:10:06 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Chandrasekharan", "Eshwar", ""], ["Jhaver", "Shagun", ""], ["Bruckman", "Amy", ""], ["Gilbert", "Eric", ""]]}, {"id": "2009.11676", "submitter": "Benedikt Hosp", "authors": "Benedikt Hosp, Florian Schultz, Oliver H\\\"oner, Enkelejda Kasneci", "title": "Eye Movement Feature Classification for Soccer Goalkeeper Expertise\n  Identification in Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest research in expertise assessment of soccer players has affirmed\nthe importance of perceptual skills (especially for decision making) by\nfocusing either on high experimental control or on a realistic presentation. To\nassess the perceptual skills of athletes in an optimized manner, we captured\nomnidirectional in-field scenes and showed these to 12 expert, 10 intermediate\nand 13 novice soccer goalkeepers on virtual reality glasses. All scenes were\nshown from the same natural goalkeeper perspective and ended after the return\npass to the goalkeeper. Based on their gaze behavior we classified their\nexpertise with common machine learning techniques. This pilot study shows\npromising results for objective classification of goalkeepers expertise based\non their gaze behaviour and provided valuable insight to inform the design of\ntraining systems to enhance perceptual skills of athletes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 12:18:41 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 17:22:41 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Hosp", "Benedikt", ""], ["Schultz", "Florian", ""], ["H\u00f6ner", "Oliver", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2009.11890", "submitter": "Kumar Akash", "authors": "Kumar Akash, Neera Jain, Teruhisa Misu", "title": "Toward Adaptive Trust Calibration for Level 2 Driving Automation", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": "10.1145/3382507.3418885", "report-no": null, "categories": "cs.HC cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Properly calibrated human trust is essential for successful interaction\nbetween humans and automation. However, while human trust calibration can be\nimproved by increased automation transparency, too much transparency can\noverwhelm human workload. To address this tradeoff, we present a probabilistic\nframework using a partially observable Markov decision process (POMDP) for\nmodeling the coupled trust-workload dynamics of human behavior in an\naction-automation context. We specifically consider hands-off Level 2 driving\nautomation in a city environment involving multiple intersections where the\nhuman chooses whether or not to rely on the automation. We consider automation\nreliability, automation transparency, and scene complexity, along with human\nreliance and eye-gaze behavior, to model the dynamics of human trust and\nworkload. We demonstrate that our model framework can appropriately vary\nautomation transparency based on real-time human trust and workload belief\nestimates to achieve trust calibration.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 18:23:52 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Akash", "Kumar", ""], ["Jain", "Neera", ""], ["Misu", "Teruhisa", ""]]}, {"id": "2009.12150", "submitter": "Thomas Gross", "authors": "Tom Fordyce, Sam Green, Thomas Gro{\\ss}", "title": "Investigation of the Effect of Fear and Stress on Password Choice\n  (Extended Version)", "comments": "Open Science Framework: https://osf.io/3cd9h. 23 pages. This is the\n  author's technical-report copy. This work was supported by the ERC Starting\n  Grant Confidentiality-Preserving Security Assurance (CASCAde), GA no 716980", "journal-ref": "Proceedings of the 7th Workshop on Socio-Technical Aspects in\n  Security and Trust (STAST'17), ACM Press, December 2018, pp. 3-15", "doi": "10.1145/3167996.3168000", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background. The current cognitive state, such as cognitive effort and\ndepletion, incidental affect or stress may impact the strength of a chosen\npassword unconsciously. Aim. We investigate the effect of incidental fear and\nstress on the measured strength of a chosen password. Method. We conducted two\nexperiments with within-subject designs measuring the Zxcvbn \\textsf{log10}\nnumber of guesses as strength of chosen passwords as dependent variable. In\nboth experiments, participants were signed up to a site holding their personal\ndata and, for the second run a day later, asked under a security incident\npretext to change their password. (a) Fear. $N_\\mathsf{F} = 34$ participants\nwere exposed to standardized fear and happiness stimulus videos in random\norder. (b) \\textbf{Stress.} $N_\\mathsf{S} = 50$ participants were either\nexposed to a battery of standard stress tasks or left in a control condition in\nrandom order. The Zxcvbn password strength was compared across conditions.\nResults. We did not observe a statistically significant difference in mean\nZxcvbn password strengths on fear (Hedges' $g_{\\mathsf{av}} = -0.11$, 95\\% CI\n$[-0.45, 0.23]$) or stress (and control group, Hedges' $g_{\\mathsf{av}} =\n0.01$, 95\\% CI $[-0.31, 0.33]$). However, we found a statistically significant\ncross-over interaction of stress and TLX mental demand. Conclusions. While\nhaving observed negligible main effect size estimates for incidental fear and\nstress, we offer evidence towards the interaction between stress and cognitive\neffort that vouches for further investigation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 12:02:08 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Fordyce", "Tom", ""], ["Green", "Sam", ""], ["Gro\u00df", "Thomas", ""]]}, {"id": "2009.12152", "submitter": "Ian Ruginski", "authors": "Ian T. Ruginski", "title": "Ethical conceptual replication of visualization research considering\n  sources of methodological bias and practical significance", "comments": "Submitted to contribute to the discussion at VisPsych2020, a workshop\n  at the IEEE Visualization Conference 2020 Salt Lake City. For associated\n  code, see https://osf.io/ebwx9/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  General design principles for visualization have been relatively\nwell-established based on a combination of cognitive and perceptual theory and\nempirical evaluations over the past 20 years. To determine how these principles\nhold up across use contexts and end-users, I argue that we should emphasize\nconceptual replication focused on determining practical significance and\nreducing methodological biases. This shift in thinking aims to determine how\ndesign principles interact with methodological approaches, laying the\ngroundwork for visualization meta-science.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 12:02:15 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Ruginski", "Ian T.", ""]]}, {"id": "2009.12316", "submitter": "Xin Qian", "authors": "Xin Qian, Ryan A. Rossi, Fan Du, Sungchul Kim, Eunyee Koh, Sana Malik,\n  Tak Yeon Lee, Joel Chan", "title": "ML-based Visualization Recommendation: Learning to Recommend\n  Visualizations from Data", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization recommendation seeks to generate, score, and recommend to users\nuseful visualizations automatically, and are fundamentally important for\nexploring and gaining insights into a new or existing dataset quickly. In this\nwork, we propose the first end-to-end ML-based visualization recommendation\nsystem that takes as input a large corpus of datasets and visualizations,\nlearns a model based on this data. Then, given a new unseen dataset from an\narbitrary user, the model automatically generates visualizations for that new\ndataset, derive scores for the visualizations, and output a list of recommended\nvisualizations to the user ordered by effectiveness. We also describe an\nevaluation framework to quantitatively evaluate visualization recommendation\nmodels learned from a large corpus of visualizations and datasets. Through\nquantitative experiments, a user study, and qualitative analysis, we show that\nour end-to-end ML-based system recommends more effective and useful\nvisualizations compared to existing state-of-the-art rule-based systems.\nFinally, we observed a strong preference by the human experts in our user study\ntowards the visualizations recommended by our ML-based system as opposed to the\nrule-based system (5.92 from a 7-point Likert scale compared to only 3.45).\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 16:13:29 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Qian", "Xin", ""], ["Rossi", "Ryan A.", ""], ["Du", "Fan", ""], ["Kim", "Sungchul", ""], ["Koh", "Eunyee", ""], ["Malik", "Sana", ""], ["Lee", "Tak Yeon", ""], ["Chan", "Joel", ""]]}, {"id": "2009.12488", "submitter": "Ashad Kabir", "authors": "Abdul Kawsar Tushar, Muhammad Ashad Kabir, Syed Ishtiaque Ahmed", "title": "Mental Health and Sensing", "comments": null, "journal-ref": "Signal processing techniques for computational health informatics,\n  2021", "doi": "10.1007/978-3-030-54932-9_11", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental health is a global epidemic, affecting close to half a billion people\nworldwide. Chronic shortage of resources hamper detection and recovery of\naffected people. Effective sensing technologies can help fight the epidemic\nthrough early detection, prediction, and resulting proper treatment. Existing\nand novel technologies for sensing mental health state could address the\naforementioned concerns by activating granular tracking of physiological,\nbehavioral, and social signals pertaining to problems in mental health. Our\npaper focuses on the available methods of sensing mental health problems\nthrough direct and indirect measures. We see how active and passive sensing by\ntechnologies as well as reporting from relevant sources can contribute toward\nthese detection methods. We also see available methods of therapeutic treatment\navailable through digital means. We highlight a few key intervention\ntechnologies that are being developed by researchers to fight against mental\nillness issues.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 00:43:51 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Tushar", "Abdul Kawsar", ""], ["Kabir", "Muhammad Ashad", ""], ["Ahmed", "Syed Ishtiaque", ""]]}, {"id": "2009.12701", "submitter": "Vidya Setlur", "authors": "Vidya Setlur and Arathi Kumar", "title": "Sentifiers: Interpreting Vague Intent Modifiers in Visual Analysis using\n  Word Co-occurrence and Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language interaction with data visualization tools often involves the\nuse of vague subjective modifiers in utterances such as \"show me the sectors\nthat are performing\" and \"where is a good neighborhood to buy a house?.\"\nInterpreting these modifiers is often difficult for these tools because their\nmeanings lack clear semantics and are in part defined by context and personal\nuser preferences. This paper presents a system called \\system that makes a\nfirst step in better understanding these vague predicates. The algorithm\nemploys word co-occurrence and sentiment analysis to determine which data\nattributes and filters ranges to associate with the vague predicates. The\nprovenance results from the algorithm are exposed to the user as interactive\ntext that can be repaired and refined. We conduct a qualitative evaluation of\nthe Sentifiers system that indicates the usefulness of the interface as well as\nopportunities for better supporting subjective utterances in visual analysis\ntasks through natural language.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 22:37:44 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Setlur", "Vidya", ""], ["Kumar", "Arathi", ""]]}, {"id": "2009.12735", "submitter": "Hainan Zhang", "authors": "Hainan Zhang, Yanyan Lan, Liang Pang, Hongshen Chen, Zhuoye Ding and\n  Dawei Yin", "title": "Modeling Topical Relevance for Multi-Turn Dialogue Generation", "comments": null, "journal-ref": "the 29th International Joint Conference on Artificial\n  Intelligence(IJCAI 2020)", "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic drift is a common phenomenon in multi-turn dialogue. Therefore, an\nideal dialogue generation models should be able to capture the topic\ninformation of each context, detect the relevant context, and produce\nappropriate responses accordingly. However, existing models usually use word or\nsentence level similarities to detect the relevant contexts, which fail to well\ncapture the topical level relevance. In this paper, we propose a new model,\nnamed STAR-BTM, to tackle this problem. Firstly, the Biterm Topic Model is\npre-trained on the whole training dataset. Then, the topic level attention\nweights are computed based on the topic representation of each context.\nFinally, the attention weights and the topic distribution are utilized in the\ndecoding process to generate the corresponding responses. Experimental results\non both Chinese customer services data and English Ubuntu dialogue data show\nthat STAR-BTM significantly outperforms several state-of-the-art methods, in\nterms of both metric-based and human evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 03:33:22 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Zhang", "Hainan", ""], ["Lan", "Yanyan", ""], ["Pang", "Liang", ""], ["Chen", "Hongshen", ""], ["Ding", "Zhuoye", ""], ["Yin", "Dawei", ""]]}, {"id": "2009.12833", "submitter": "Meng Xia", "authors": "Meng Xia, Reshika Palaniyappan Velumani, Yong Wang, Huamin Qu, and\n  Xiaojuan Ma", "title": "QLens: Visual Analytics of Multi-step Problem-solving Behaviors for\n  Improving Question Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of online education in recent years, there has\nbeen an increasing number of learning platforms that provide students with\nmulti-step questions to cultivate their problem-solving skills. To guarantee\nthe high quality of such learning materials, question designers need to inspect\nhow students' problem-solving processes unfold step by step to infer whether\nstudents' problem-solving logic matches their design intent. They also need to\ncompare the behaviors of different groups (e.g., students from different\ngrades) to distribute questions to students with the right level of knowledge.\nThe availability of fine-grained interaction data, such as mouse movement\ntrajectories from the online platforms, provides the opportunity to analyze\nproblem-solving behaviors. However, it is still challenging to interpret,\nsummarize, and compare the high dimensional problem-solving sequence data. In\nthis paper, we present a visual analytics system, QLens, to help question\ndesigners inspect detailed problem-solving trajectories, compare different\nstudent groups, distill insights for design improvements. In particular, QLens\nmodels problem-solving behavior as a hybrid state transition graph and\nvisualizes it through a novel glyph-embedded Sankey diagram, which reflects\nstudents' problem-solving logic, engagement, and encountered difficulties. We\nconduct three case studies and three expert interviews to demonstrate the\nusefulness of QLens on real-world datasets that consist of thousands of\nproblem-solving traces.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 12:39:54 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Xia", "Meng", ""], ["Velumani", "Reshika Palaniyappan", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""], ["Ma", "Xiaojuan", ""]]}, {"id": "2009.12842", "submitter": "Navin Raj Prabhu", "authors": "Navin Raj Prabhu, Chirag Raman, Hayley Hung", "title": "Defining and Quantifying Conversation Quality in Spontaneous\n  Interactions", "comments": "10 pages, 8 figures, Companion Publication of the 2020 International\n  Conference on Multimodal Interaction (ICMI '20 Companion), October 25--29,\n  2020, Virtual event, Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social interactions in general are multifaceted and there exists a wide set\nof factors and events that influence them. In this paper, we quantify social\ninteractions with a holistic viewpoint on individual experiences, particularly\nfocusing on non-task-directed spontaneous interactions. To achieve this, we\ndesign a novel perceived measure, the perceived Conversation Quality, which\nintends to quantify spontaneous interactions by accounting for several\nsocio-dimensional aspects of individual experiences.\n  To further quantitatively study spontaneous interactions, we devise a\nquestionnaire which measures the perceived Conversation Quality, at both the\nindividual- and at the group- level. Using the questionnaire, we collected\nperceived annotations for conversation quality in a publicly available dataset\nusing naive annotators. The results of the analysis performed on the\ndistribution and the inter-annotator agreeability shows that naive annotators\ntend to agree less in cases of low conversation quality samples, especially\nwhile annotating for group-level conversation quality.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 13:41:27 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Prabhu", "Navin Raj", ""], ["Raman", "Chirag", ""], ["Hung", "Hayley", ""]]}, {"id": "2009.12853", "submitter": "Nicolas E. Diaz Ferreyra PhD", "authors": "Nicolas E. D\\'iaz Ferreyra, Esma A\\\"imeur, Hicham Hage, Maritta Heisel\n  and Catherine Garc\\'ia van Hoogstraten", "title": "Persuasion Meets AI: Ethical Considerations for the Design of Social\n  Engineering Countermeasures", "comments": "Accepted for publication at IC3K 2020", "journal-ref": null, "doi": "10.5220/0010142402040211", "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy in Social Network Sites (SNSs) like Facebook or Instagram is closely\nrelated to people's self-disclosure decisions and their ability to foresee the\nconsequences of sharing personal information with large and diverse audiences.\nNonetheless, online privacy decisions are often based on spurious risk\njudgements that make people liable to reveal sensitive data to untrusted\nrecipients and become victims of social engineering attacks. Artificial\nIntelligence (AI) in combination with persuasive mechanisms like nudging is a\npromising approach for promoting preventative privacy behaviour among the users\nof SNSs. Nevertheless, combining behavioural interventions with high levels of\npersonalization can be a potential threat to people's agency and autonomy even\nwhen applied to the design of social engineering countermeasures. This paper\nelaborates on the ethical challenges that nudging mechanisms can introduce to\nthe development of AI-based countermeasures, particularly to those addressing\nunsafe self-disclosure practices in SNSs. Overall, it endorses the elaboration\nof personalized risk awareness solutions as i) an ethical approach to\ncounteract social engineering, and ii) as an effective means for promoting\nreflective privacy decisions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 14:24:29 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Ferreyra", "Nicolas E. D\u00edaz", ""], ["A\u00efmeur", "Esma", ""], ["Hage", "Hicham", ""], ["Heisel", "Maritta", ""], ["van Hoogstraten", "Catherine Garc\u00eda", ""]]}, {"id": "2009.12924", "submitter": "Dustin Arendt", "authors": "Brittany Davis, Maria Glenski, William Sealy, Dustin Arendt", "title": "Measure Utility, Gain Trust: Practical Advice for XAI Researcher", "comments": "To appear in TREX 2020: Workshop on TRust and EXperience in Visual\n  Analytics. https://trexvis.github.io/Workshop2020/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into the explanation of machine learning models, i.e., explainable\nAI (XAI), has seen a commensurate exponential growth alongside deep artificial\nneural networks throughout the past decade. For historical reasons, explanation\nand trust have been intertwined. However, the focus on trust is too narrow, and\nhas led the research community astray from tried and true empirical methods\nthat produced more defensible scientific knowledge about people and\nexplanations. To address this, we contribute a practical path forward for\nresearchers in the XAI field. We recommend researchers focus on the utility of\nmachine learning explanations instead of trust. We outline five broad use cases\nwhere explanations are useful and, for each, we describe pseudo-experiments\nthat rely on objective empirical measurements and falsifiable hypotheses. We\nbelieve that this experimental rigor is necessary to contribute to scientific\nknowledge in the field of XAI.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 18:55:33 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Davis", "Brittany", ""], ["Glenski", "Maria", ""], ["Sealy", "William", ""], ["Arendt", "Dustin", ""]]}, {"id": "2009.12975", "submitter": "Liang Gou", "authors": "Liang Gou, Lincan Zou, Nanxiang Li, Michael Hofmann, Arvind Kumar\n  Shekar, Axel Wendt and Liu Ren", "title": "VATLD: A Visual Analytics System to Assess, Understand and Improve\n  Traffic Light Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic light detection is crucial for environment perception and\ndecision-making in autonomous driving. State-of-the-art detectors are built\nupon deep Convolutional Neural Networks (CNNs) and have exhibited promising\nperformance. However, one looming concern with CNN based detectors is how to\nthoroughly evaluate the performance of accuracy and robustness before they can\nbe deployed to autonomous vehicles. In this work, we propose a visual analytics\nsystem, VATLD, equipped with a disentangled representation learning and\nsemantic adversarial learning, to assess, understand, and improve the accuracy\nand robustness of traffic light detectors in autonomous driving applications.\nThe disentangled representation learning extracts data semantics to augment\nhuman cognition with human-friendly visual summarization, and the semantic\nadversarial learning efficiently exposes interpretable robustness risks and\nenables minimal human interaction for actionable insights. We also demonstrate\nthe effectiveness of various performance improvement strategies derived from\nactionable insights with our visual analytics system, VATLD, and illustrate\nsome practical implications for safety-critical applications in autonomous\ndriving.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 22:39:00 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Gou", "Liang", ""], ["Zou", "Lincan", ""], ["Li", "Nanxiang", ""], ["Hofmann", "Michael", ""], ["Shekar", "Arvind Kumar", ""], ["Wendt", "Axel", ""], ["Ren", "Liu", ""]]}, {"id": "2009.13008", "submitter": "Anjul Tyagi", "authors": "Anjul Tyagi, Cong Xie, Klaus Mueller", "title": "Visual Steering for One-Shot Deep Neural Network Synthesis", "comments": "9 pages, submitted to IEEE Transactions on Visualization and Computer\n  Graphics, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in the area of deep learning have shown the effectiveness\nof very large neural networks in several applications. However, as these deep\nneural networks continue to grow in size, it becomes more and more difficult to\nconfigure their many parameters to obtain good results. Presently, analysts\nmust experiment with many different configurations and parameter settings,\nwhich is labor-intensive and time-consuming. On the other hand, the capacity of\nfully automated techniques for neural network architecture search is limited\nwithout the domain knowledge of human experts. To deal with the problem, we\nformulate the task of neural network architecture optimization as a graph space\nexploration, based on the one-shot architecture search technique. In this\napproach, a super-graph of all candidate architectures is trained in one-shot\nand the optimal neural network is identified as a sub-graph. In this paper, we\npresent a framework that allows analysts to effectively build the solution\nsub-graph space and guide the network search by injecting their domain\nknowledge. Starting with the network architecture space composed of basic\nneural network components, analysts are empowered to effectively select the\nmost promising components via our one-shot search scheme. Applying this\ntechnique in an iterative manner allows analysts to converge to the best\nperforming neural network architecture for a given application. During the\nexploration, analysts can use their domain knowledge aided by cues provided\nfrom a scatterplot visualization of the search space to edit different\ncomponents and guide the search for faster convergence. We designed our\ninterface in collaboration with several deep learning researchers and its final\neffectiveness is evaluated with a user study and two case studies.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 01:48:45 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Tyagi", "Anjul", ""], ["Xie", "Cong", ""], ["Mueller", "Klaus", ""]]}, {"id": "2009.13194", "submitter": "Alfie Abdul-Rahman", "authors": "Min Chen, Alfie Abdul-Rahman, and David H. Laidlaw", "title": "The Huge Variable Space in Empirical Studies for Visualization -- A\n  Challenge as well as an opportunity for Visualization Psychology", "comments": "3 pages, IEEE VIS 2020 Workshop on Visualization Psychology\n  (VisPsych)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In each of the last five years, a few dozen empirical studies appeared in\nvisualization journals and conferences. The existing empirical studies have\nalready featured a large number of variables. There are many more variables yet\nto be studied. While empirical studies enable us to obtain knowledge and\ninsight about visualization processes through observation and analysis of user\nexperience, it seems to be a stupendous challenge for exploring such a huge\nvariable space at the current pace. In this position paper, we discuss the\nimplication of not being able to explore this space effectively and\nefficiently, and propose means for addressing this challenge.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 10:17:45 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Chen", "Min", ""], ["Abdul-Rahman", "Alfie", ""], ["Laidlaw", "David H.", ""]]}, {"id": "2009.13200", "submitter": "Rita Borgo", "authors": "Rita Borgo and Darren J Edwards", "title": "The Development of Visualization Psychology Analysis Tools to Account\n  for Trust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Defining trust is an important endeavor given its applicability to assessing\npublic mood to much of the innovation in the newly formed autonomous industry,\nsuch as artificial intelligence (AI),medical bots, drones, autonomous vehicles,\nand smart factories [19].Through developing a reliable index or means to\nmeasure trust,this may have wide impact from fostering acceptance and adoption\nof smart systems to informing policy makers about the public atmosphere and\nwillingness to adopt innovate change, and has been identified as an important\nindicator in a recent UK policy brief [8].In this paper, we reflect on the\nimportance and potential impact of developing Visualization Psychology in the\ncontext of solving definitions and policy decision making problems for complex\nconstructs such as \"trust\".\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 10:30:09 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Borgo", "Rita", ""], ["Edwards", "Darren J", ""]]}, {"id": "2009.13219", "submitter": "Ahmed Arif", "authors": "Ahmed Sabbir Arif", "title": "Metrics for Multi-Touch Input Technologies", "comments": "2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-touch input technologies are becoming popular with the increased\ninterest in touchscreen- and touchpad-based devices. A great deal of work has\nbeen done on different multi-touch technologies, and researchers and\npractitioners are frequently coming up with new ones. However, it is almost\nimpossible to compare such technologies due to the absence of multi-touch\nperformance metrics. Designers usually use their own methods to report their\ntechniques' performances. Moreover, multi-touch interaction was never modeled.\nThat makes it impossible for designers to predict the performance of a new\ntechnology before developing it, costing them valuable time, effort, and money.\nThis article discusses the necessity of having dedicated performance metrics\nand prediction model for multi-touch technologies, and ways of approaching\nthat.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 11:15:09 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Arif", "Ahmed Sabbir", ""]]}, {"id": "2009.13322", "submitter": "Sanjay Seshan", "authors": "Sanjay Seshan", "title": "Lightweight assistive technology: A wearable, optical-fiber gesture\n  recognition system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this project is to create an inexpensive, lightweight, wearable\nassistive device that can measure hand or finger movements accurately enough to\nidentify a range of hand gestures. One eventual application is to provide\nassistive technology and sign language detection for the hearing impaired. My\nsystem, called LiTe (Light-based Technology), uses optical fibers embedded into\na wristband. The wrist is an optimal place for the band since the light\npropagation in the optical fibers is impacted even by the slight movements of\nthe tendons in the wrist when gestures are performed. The prototype\nincorporates light dependent resistors to measure these light propagation\nchanges. When creating LiTe, I considered a variety of fiber materials, light\nfrequencies, and physical shapes to optimize the tendon movement detection so\nthat it can be accurately correlated with different gestures. I implemented and\nevaluated two approaches for gesture recognition. The first uses an algorithm\nthat combines moving averages of sensor readings with gesture sensor reading\nsignatures to determine the current gesture. The second uses a neural network\ntrained on a labelled set of gesture readings to recognize gestures. Using the\nsignature-based approach, I was able to achieve a 99.8% accuracy at recognizing\ndistinct gestures. Using the neural network the recognition accuracy was 98.8%.\nThis shows that high accuracy is feasible using both approaches. The results\nindicate that this novel method of using fiber optics-based sensors is a\npromising first step to creating a gesture recognition system.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 01:36:44 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Seshan", "Sanjay", ""]]}, {"id": "2009.13368", "submitter": "Ryan Wesslen", "authors": "Ryan Wesslen, Doug Markant, Alireza Karduni, Wenwen Dou", "title": "Using Resource-Rational Analysis to Understand Cognitive Biases in\n  Interactive Data Visualizations", "comments": "IEEE VIS 2020 Workshop on Visualization Psychology (VisPsych)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive biases are systematic errors in judgment. Researchers in data\nvisualizations have explored whether cognitive biases transfer to\ndecision-making tasks with interactive data visualizations. At the same time,\ncognitive scientists have reinterpreted cognitive biases as the product of\nresource-rational strategies under finite time and computational costs. In this\npaper, we argue for the integration of resource-rational analysis through\nconstrained Bayesian cognitive modeling to understand cognitive biases in data\nvisualizations. The benefit would be a more realistic \"bounded rationality\"\nrepresentation of data visualization users and provides a research roadmap for\nstudying cognitive biases in data visualizations through a feedback loop\nbetween future experiments and theory\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:35:58 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 15:22:47 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Wesslen", "Ryan", ""], ["Markant", "Doug", ""], ["Karduni", "Alireza", ""], ["Dou", "Wenwen", ""]]}, {"id": "2009.13646", "submitter": "Amy Fox", "authors": "Amy Rae Fox", "title": "A Psychology of Visualization or (External) Representation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is a visualization? There is limited utility in trifling with\ndefinitions, except insofar as one serves as a tool for communicating and\nconceptualizing our subject matter; a statement of identity for a community. To\nestablish Visualization Psychology as a viable inter-disciplinary research\nprogramme, we must first define the object(s) of our collective inquiry. I\npropose that while we might refer to the study of \"visualization\" for the\nterm's colloquial accessibility and pragmatic alignment with other fields, we\nshould consider for exploration a class of artifacts and corresponding\nprocesses more expansive and profound: external representations. What follows\nis an argument for the study of external representation as the foundation for a\nnew interdisciplinary endeavor, and approach to mapping the corresponding\nproblem space.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 21:45:36 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Fox", "Amy Rae", ""]]}, {"id": "2009.13649", "submitter": "Yuchen Cui", "authors": "Yuchen Cui, Qiping Zhang, Alessandro Allievi, Peter Stone, Scott\n  Niekum and W. Bradley Knox", "title": "The EMPATHIC Framework for Task Learning from Implicit Human Feedback", "comments": "Conference on Robot Learning 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reactions such as gestures, facial expressions, and vocalizations are an\nabundant, naturally occurring channel of information that humans provide during\ninteractions. A robot or other agent could leverage an understanding of such\nimplicit human feedback to improve its task performance at no cost to the\nhuman. This approach contrasts with common agent teaching methods based on\ndemonstrations, critiques, or other guidance that need to be attentively and\nintentionally provided. In this paper, we first define the general problem of\nlearning from implicit human feedback and then propose to address this problem\nthrough a novel data-driven framework, EMPATHIC. This two-stage method consists\nof (1) mapping implicit human feedback to relevant task statistics such as\nreward, optimality, and advantage; and (2) using such a mapping to learn a\ntask. We instantiate the first stage and three second-stage evaluations of the\nlearned mapping. To do so, we collect a dataset of human facial reactions while\nparticipants observe an agent execute a sub-optimal policy for a prescribed\ntraining task. We train a deep neural network on this data and demonstrate its\nability to (1) infer relative reward ranking of events in the training task\nfrom prerecorded human facial reactions; (2) improve the policy of an agent in\nthe training task using live human facial reactions; and (3) transfer to a\nnovel domain in which it evaluates robot manipulation trajectories.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 21:50:38 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 00:37:56 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 18:15:12 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cui", "Yuchen", ""], ["Zhang", "Qiping", ""], ["Allievi", "Alessandro", ""], ["Stone", "Peter", ""], ["Niekum", "Scott", ""], ["Knox", "W. Bradley", ""]]}, {"id": "2009.13758", "submitter": "Paul Parsons", "authors": "Paul Parsons", "title": "How do Visualization Designers Think? Design Cognition as a Core Aspect\n  of Visualization Psychology", "comments": "IEEE VIS 2020 Workshop on Visualization Psychology (VisPsych)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are numerous opportunities for engaging in research at the intersection\nof psychology and visualization. While most opportunities taken up by the VIS\ncommunity will likely focus on the psychology of users, there are also\nopportunities for studying the psychology of designers. In this position paper,\nI argue the importance of studying design cognition as a necessary component of\na holistic program of research on visualization psychology. I provide a brief\noverview of research on design cognition in other disciplines, and discuss\nopportunities for VIS to build an analogous research program. Doing so can lead\nto a stronger integration of research and design practice, can provide a better\nunderstanding of how to educate and train future designers, and will likely\nsurface both challenges and opportunities for future research.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 03:57:57 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Parsons", "Paul", ""]]}, {"id": "2009.13871", "submitter": "Dario Garcia-Gasulla PhD", "authors": "Dario Garcia-Gasulla, Atia Cort\\'es, Sergio Alvarez-Napagao, Ulises\n  Cort\\'es", "title": "Signs for Ethical AI: A Route Towards Transparency", "comments": "27 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) has recently raised to the point where it has a\ndirect impact on the daily life of billions of people. This is the result of\nits application to sectors like finance, health, digital entertainment,\ntransportation, security and advertisement. Today, AI fuels some of the most\nsignificant economic and research institutions in the world, and the impact of\nAI in the near future seems difficult to predict or even bound. In contrast to\nall this power, society remains mostly ignorant of the capabilities,\nrequirements and standard practices of AI today. Society is becoming aware of\nthe dangers that come with that ignorance, and is rightfully asking for\nsolutions. To address this need, improving on current practices of interaction\nbetween people and AI systems, we propose a transparency scheme to be\nimplemented on any AI system open to the public. The scheme is based on two\nmain pillars: Data Privacy and AI Transparency. The first recognizes the\nrelevance of data for AI and is supported by GDPR, the most important\nlegislation on the topic. The second considers aspects of AI transparency yet\nto be regulated: AI capacity, purpose and source. Lacking legislation to build\nupon, we design this pillar based on fundamental ethical principles. For each\nof the two pillars, we define a three-level display. The first level is based\non visual signs, inspired by traffic signs managing the interaction between\npeople and cars, and designed for quick and universal interpretability. The\nsecond level uses a factsheet system, providing further detail while still\nabstracting the subject. The last level provides access to all available\ndetails. After detailing and exemplifying the proposed transparency scheme, we\ndefine a set of principles for creating transparent by design software, to be\nused during the integration of AI components on user-oriented services.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 08:49:44 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Garcia-Gasulla", "Dario", ""], ["Cort\u00e9s", "Atia", ""], ["Alvarez-Napagao", "Sergio", ""], ["Cort\u00e9s", "Ulises", ""]]}, {"id": "2009.13919", "submitter": "Alarith Uhde", "authors": "Holger Klapperich and Alarith Uhde and Marc Hassenzahl", "title": "Designing everyday automation with well-being in mind", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s00779-020-01452-w", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, automation not only permeates industry but also becomes a\nsubstantial part of our private, everyday lives. Driven by the idea of\nincreased convenience and more time for the \"important things in life,\"\nautomation relieves us from many daily chores - robots vacuum floors and\nautomated coffee makers produce supposedly barista-quality coffee on the press\nof a button. In many cases, these offers are embraced by people without further\nquestioning. However, while we save time by delegating more and more everyday\nactivities to automation, we also may lose chances for enjoyable and meaningful\nexperiences. In two field studies, we demonstrate that a manual process has\nexperiential benefits over more automated processes by using the example of\ncoffee-making. We present a way to account for potential experiential costs of\neveryday automation and strategies of how to design interaction with automation\nto reconcile experience with the advantages of a more and more powerful\nautomation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 10:25:37 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Klapperich", "Holger", ""], ["Uhde", "Alarith", ""], ["Hassenzahl", "Marc", ""]]}, {"id": "2009.14074", "submitter": "Rob Procter", "authors": "Jonathan Davies, Rob Procter", "title": "Online platforms of public participation -- a deliberative democracy or\n  a delusion?", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trust and confidence in democratic institutions is at an all-time low. At the\nsame time, many of the complex issues faced by city administrators and\npoliticians remain unresolved. To tackle these concerns, many argue that\ncitizens should, through the use of digital platforms, have greater involvement\nin decision-making processes. This paper describes research into two such\nplatforms, 'Decide Madrid' and 'Better Reykjavik'. Through the use of\ninterviews, questionnaires, ethnographic observation, and analysis of platform\ndata, the study will determine if these platforms provide greater participation\nor simply replicate what is already offered by numerous other digital tools.\nThe findings so far suggest that to be successful platforms must take on a form\nof deliberative democracy, allowing for knowledge co-production and the\nemergence of collective intelligence. Based on this, we aim to identify key\nfeatures of sustainable models of online participation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 15:01:44 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Davies", "Jonathan", ""], ["Procter", "Rob", ""]]}, {"id": "2009.14182", "submitter": "Surabhi Nath", "authors": "Surabhi S Nath", "title": "Hear Her Fear: Data Sonification for Sensitizing Society on Crime\n  Against Women in India", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sonification is a means of representing data through sound and has been\nutilized in a variety of applications. Crime against women has been a rising\nconcern in India. We explore the potential of data sonification to provide an\nimmersive engagement with sensitive data on crime against women in Indian\nstates. The data for nine crime categories covering thirty-five Indian states\nover a period of twelve years is acquired from National records. Sonification\ntechniques of parameter mapping and auditory icons are adopted: sound\nparameters such as frequencies, amplitudes and timbres are incorporated to\nrepresent the crime data, and audio sounds of women screams are employed as\nauditory icons to emphasize the traumatic experience. Higher crime rates are\nassigned higher frequencies, harsher scream textures and larger amplitudes. A\nuser-friendly interface is developed with multiple options for sequential and\ncomparative data sonification. Through the interface, a user can evaluate and\ncompare the extent of crime against women in different states, years or crime\ncategories. Sound spatialization is used to immerse the listener in the sound\nand further intensify the sonification experience. To assess and validate\neffectiveness, a user study on twenty participants is conducted with feedback\nobtained through questionnaires. The responses indicate that the participants\ncould comprehend trends in the data easily and found the data sonification\nexperience impactful. Sonification may therefore prove to be a valuable tool\nfor data representation in fields related to social and human studies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 17:49:31 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 14:25:49 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 14:30:56 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Nath", "Surabhi S", ""]]}, {"id": "2009.14237", "submitter": "Andrew Head", "authors": "Andrew Head (UC Berkeley), Kyle Lo (Allen Institute for AI), Dongyeop\n  Kang (UC Berkeley), Raymond Fok (University of Washington), Sam Skjonsberg\n  (Allen Institute for AI), Daniel S. Weld (Allen Institute for AI, University\n  of Washington), Marti A. Hearst (UC Berkeley)", "title": "Augmenting Scientific Papers with Just-in-Time, Position-Sensitive\n  Definitions of Terms and Symbols", "comments": "18 pages, 17 figures, 2 tables. To appear at the 2021 ACM CHI\n  Conference on Human Factors in Computing Systems. For associated video, see\n  https://youtu.be/yYcQf-Yq8B0. v2 changes: expanded discussion of design\n  process and implementation; improved figure design. v3 changes: fixed typo in\n  cell of Table 2; updated HEDDEx and Schwarz-Hearst accuracy in Section 5.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Despite the central importance of research papers to scientific progress,\nthey can be difficult to read. Comprehension is often stymied when the\ninformation needed to understand a passage resides somewhere else: in another\nsection, or in another paper. In this work, we envision how interfaces can\nbring definitions of technical terms and symbols to readers when and where they\nneed them most. We introduce ScholarPhi, an augmented reading interface with\nfour novel features: (1) tooltips that surface position-sensitive definitions\nfrom elsewhere in a paper, (2) a filter over the paper that \"declutters\" it to\nreveal how the term or symbol is used across the paper, (3) automatic equation\ndiagrams that expose multiple definitions in parallel, and (4) an automatically\ngenerated glossary of important terms and symbols. A usability study showed\nthat the tool helps researchers of all experience levels read papers.\nFurthermore, researchers were eager to have ScholarPhi's definitions available\nto support their everyday reading.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 18:11:19 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 19:05:59 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 18:32:46 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Head", "Andrew", "", "UC Berkeley"], ["Lo", "Kyle", "", "Allen Institute for AI"], ["Kang", "Dongyeop", "", "UC Berkeley"], ["Fok", "Raymond", "", "University of Washington"], ["Skjonsberg", "Sam", "", "Allen Institute for AI"], ["Weld", "Daniel S.", "", "Allen Institute for AI, University\n  of Washington"], ["Hearst", "Marti A.", "", "UC Berkeley"]]}, {"id": "2009.14265", "submitter": "Samreen Anjum", "authors": "Samreen Anjum, Chi Lin, Danna Gurari", "title": "CrowdMOT: Crowdsourcing Strategies for Tracking Multiple Objects in\n  Videos", "comments": "CSCW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is a valuable approach for tracking objects in videos in a more\nscalable manner than possible with domain experts. However, existing frameworks\ndo not produce high quality results with non-expert crowdworkers, especially\nfor scenarios where objects split. To address this shortcoming, we introduce a\ncrowdsourcing platform called CrowdMOT, and investigate two micro-task design\ndecisions: (1) whether to decompose the task so that each worker is in charge\nof annotating all objects in a sub-segment of the video versus annotating a\nsingle object across the entire video, and (2) whether to show annotations from\nprevious workers to the next individuals working on the task. We conduct\nexperiments on a diversity of videos which show both familiar objects (aka -\npeople) and unfamiliar objects (aka - cells). Our results highlight strategies\nfor efficiently collecting higher quality annotations than observed when using\nstrategies employed by today's state-of-art crowdsourcing system.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 19:12:21 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Anjum", "Samreen", ""], ["Lin", "Chi", ""], ["Gurari", "Danna", ""]]}, {"id": "2009.14268", "submitter": "Alex Scarlatos", "authors": "Alexander Scarlatos", "title": "Sonispace: a simulated-space interface for sound design and\n  experimentation", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The world of audio production and design has long been a difficult one to\nbreak into, requiring expertise and a working knowledge of the standard digital\naudio paradigms. This paper describes a novel interface that makes audio\nproduction and design more intuitive for novices, using sound-to-space\nrelations that people have learned throughout daily life, such as the roles of\nbarriers and distance in sound perception. The spatial interface for Sonispace\nallows users to quickly see the relationships between sound-emitting and\nsound-effecting objects, and to receive audio feedback as they make changes to\nthe space. Algorithms were developed to resemble real-world sonic physics while\nbeing efficient enough to provide a user with immediate audio feedback. A\nprototype of the interface was tested by a group of participants, who confirmed\nthat the software is accessible by novices and that the spatial interface is an\nengaging way of mixing audio.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 19:20:12 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Scarlatos", "Alexander", ""]]}, {"id": "2009.14421", "submitter": "Daniel Lee", "authors": "Daniel H. Lee, Tzyy-Ping Jung", "title": "A Virtual Reality Game as a Tool to Assess Physiological Correlations of\n  Stress", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this study is to develop and use a virtual reality game as a\ntool to assess the effects of realistic stress on the behavioral and\nphysiological responses of participants. The game is based on a popular Steam\ngame called Keep Talking Nobody Explodes, where the player collaborates with\nanother person to defuse a bomb. Varying levels of difficulties in solving a\npuzzle and time pressures will result in different stress levels that can be\nmeasured in terms of errors, response time lengths, and other physiological\nmeasurements. The game was developed using 3D programming tools including\nBlender and virtual reality development kit (VRTK). To measure response times\naccurately, we added LSL (Lab Stream Layer) Markers to collect and synchronize\nphysiological signals, behavioral data, and the timing of game events. We\nrecorded Electrocardiogram (ECG) data during gameplay to assess heart rate and\nheart-rate variability (HRV) that have been shown as reliable indicators of\nstress. Our empirical results showed that heart rate increased significantly\nwhile HRV reduced significantly when the participants under high stress, which\nare consistent with the prior mainstream stress research. We further\nexperimented with other tools to enhance communication between two players\nunder adverse conditions and found that an automatic speech recognition\nsoftware effectively enhanced the communication between the players by\ndisplaying keywords into the player's headset that lead to the facilitation of\nfinding the solution of the puzzles or modules. This VR game framework is\npublicly available in Github and allows researchers to measure and synchronize\nother physiological signals such as electroencephalogram, electromyogram, and\npupillometry.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 04:20:07 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Lee", "Daniel H.", ""], ["Jung", "Tzyy-Ping", ""]]}, {"id": "2009.14440", "submitter": "Darshan Gera", "authors": "Darshan Gera and S Balasubramanian", "title": "Affect Expression Behaviour Analysis in the Wild using Spatio-Channel\n  Attention and Complementary Context Information", "comments": "arXiv admin note: text overlap with arXiv:2007.10298 (ABAW2020\n  challenge test set results added)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition(FER) in the wild is crucial for building\nreliable human-computer interactive systems. However, current FER systems fail\nto perform well under various natural and un-controlled conditions. This report\npresents attention based framework used in our submission to expression\nrecognition track of the Affective Behaviour Analysis in-the-wild (ABAW) 2020\ncompetition. Spatial-channel attention net(SCAN) is used to extract local and\nglobal attentive features without seeking any information from landmark\ndetectors. SCAN is complemented by a complementary context information(CCI)\nbranch which uses efficient channel attention(ECA) to enhance the relevance of\nfeatures. The performance of the model is validated on challenging Aff-Wild2\ndataset for categorical expression classification.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 12:26:15 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 06:24:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Gera", "Darshan", ""], ["Balasubramanian", "S", ""]]}, {"id": "2009.14465", "submitter": "Laura Matzen", "authors": "Laura Matzen, Kristin Divis, Deborah Cronin and Michael Haass", "title": "Task Matters When Scanning Data Visualizations", "comments": "IEEE VIS 2020 Workshop on Visualization Psychology (VisPsych)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges for evaluating the effectiveness of data\nvisualizations and visual analytics tools arises from the fact that different\nusers may be using these tools for different tasks. In this paper, we present a\nsimple example of how different tasks lead to different patterns of attention\nto the same underlying data visualizations. We argue that the general approach\nused in this experiment could be applied systematically to task and feature\ntaxonomies that have been developed by visualization researchers. Using eye\ntracking to study the impact of common tasks on how humans attend to common\ntypes of visualizations will support a deeper understanding of visualization\ncognition and the development of more robust methods for evaluating the\neffectiveness of visualizations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 06:19:46 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Matzen", "Laura", ""], ["Divis", "Kristin", ""], ["Cronin", "Deborah", ""], ["Haass", "Michael", ""]]}, {"id": "2009.14515", "submitter": "Kuno Kurzhals", "authors": "Kuno Kurzhals, Michael Burch, Daniel Weiskopf", "title": "What We See and What We Get from Visualization: Eye Tracking Beyond Gaze\n  Distributions and Scanpaths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technical progress in hardware and software enables us to record gaze data in\neveryday situations and over long time spans. Among a multitude of research\nopportunities, this technology enables visualization researchers to catch a\nglimpse behind performance measures and into the perceptual and cognitive\nprocesses of people using visualization techniques. The majority of eye\ntracking studies performed for visualization research is limited to the\nanalysis of gaze distributions and aggregated statistics, thus only covering a\nsmall portion of insights that can be derived from gaze data. We argue that\nincorporating theories and methodology from psychology and cognitive science\nwill benefit the design and evaluation of eye tracking experiments for\nvisualization. This position paper outlines our experiences with eye tracking\nin visualization and states the benefits that an interdisciplinary research\nfield on visualization psychology might bring for better understanding how\npeople interpret visualizations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 09:05:37 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Kurzhals", "Kuno", ""], ["Burch", "Michael", ""], ["Weiskopf", "Daniel", ""]]}]