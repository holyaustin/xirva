[{"id": "1207.0657", "submitter": "Krasimir Yordzhev", "authors": "Krasimir Yordzhev, Ivelina Peneva", "title": "Computer Administering of the Psychological Investigations:\n  Set-relational Representation", "comments": null, "journal-ref": "Open Journal of Applied Sciences, 2012, 2, 110-114", "doi": "10.4236/ojapps.2012.22015", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer administering of a psychological investigation is the computer\nrepresentation of the entire procedure of psychological assessments - test\nconstruction, test implementation, results evaluation, storage and maintenance\nof the developed database, its statistical processing, analysis and\ninterpretation. A mathematical description of psychological assessment with the\naid of personality tests is discussed in this article. The set theory and the\nrelational algebra are used in this description. A relational model of data,\nneeded to design a computer system for automation of certain psychological\nassessments is given. Some finite sets and relation on them, which are\nnecessary for creating a personality psychological test, are described. The\ndescribed model could be used to develop real software for computer\nadministering of any psychological test and there is full automation of the\nwhole process: test construction, test implementation, result evaluation,\nstorage of the developed database, statistical implementation, analysis and\ninterpretation. A software project for computer administering personality\npsychological tests is suggested.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 12:54:41 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Yordzhev", "Krasimir", ""], ["Peneva", "Ivelina", ""]]}, {"id": "1207.1493", "submitter": "Ivan Ruchkin", "authors": "Ivan Ruchkin and Vladimir Prus", "title": "Single-window Integrated Development Environment", "comments": "Proceedings of the 4th Spring/Summer Young Researchers' Colloquium on\n  Software Engineering (SYRCoSE) 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of IDE interface complexity by introducing\nsingle-window graphical user interface. This approach lies in removing\nadditional child windows from IDE, thus allowing a user to keep only text\neditor window open. We describe an abstract model of IDE GUI that is based on\nmost popular modern integrated environments and has generalized user interface\nparts. Then this abstract model is reorganized into single windowed interface\nmodel: access to common IDE functions is provided from the code editing window\nwhile utility windows are removed without loss of IDE functionality. After that\nthe implementation of single-window GUI on KDevelop 4 is described. And finally\ntool views and usability of several well- known IDEs are surveyed.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2012 00:19:58 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Ruchkin", "Ivan", ""], ["Prus", "Vladimir", ""]]}, {"id": "1207.1818", "submitter": "Evangelos Karapanos", "authors": "R\\'uben Gouveia, Evangelos Niforatos, Evangelos Karapanos", "title": "Footprint Tracker: reviewing lifelogs and reconstructing daily\n  experiences", "comments": null, "journal-ref": "Gouveia, R., Niforatos, E., Karapanos, E. (2012) Footprint\n  Tracker: reviewing lifelogs and reconstructing daily experiences, In adjunct\n  proceedings of ACM conference on Designing Interactive Systems, DIS'12", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing emphasis on how mobile technologies are experienced in\neveryday life, researchers are increasingly emphasizing the use of in-situ\nmethods such as Experience Sampling and Day Reconstruction. In our line of\nresearch we explore the concept of Technology-Assisted Reconstruction, in which\npassively logged behavior data assist in the later reconstruction of daily\nexperiences. In this paper we introduce Footprint tracker, a web application\nthat supports participants in reviewing lifelogs and reconstructing their daily\nexperiences. We focus on three kinds of data: visual (as captured through\nMicrosoft's sensecam), location, and context (i.e., SMS and calls received and\nmade). We describe how Footprint Tracker supports the user in reviewing these\nlifelogs and outline a field study that attempts to inquire into whether and\nhow this data support reconstruction from memory.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2012 18:52:06 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Gouveia", "R\u00faben", ""], ["Niforatos", "Evangelos", ""], ["Karapanos", "Evangelos", ""]]}, {"id": "1207.1820", "submitter": "Evangelos Karapanos", "authors": "Jos\\'e Rodrigues, R\\'uben Gouveia, Olga Lyra, Evangelos Karapanos", "title": "Sense me: Supporting awareness in parent-child relationships through\n  mobile sensing", "comments": null, "journal-ref": "Rodrigues, J., Gouveia, R., Lyra, O., Karapanos, E. (2012) Sense\n  me: Supporting awareness in parent-child relationships through mobile\n  sensing, In adjunct proceedings of ACM conference on Designing Interactive\n  Systems, DIS'12", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Sense{\\mu} (pronounced \"sense me\"), a mobile application that\naims at supporting awareness in parent- child relationships through the sensing\ncapabilities of mobile devices. We discuss the relevance of three types of\nawareness information: physical activity inferred from accelerometers, verbal\nactivity during class hours inferred from microphones, and social activity\ninferred from Bluetooth pair-wise proximity sensing. We describe how we attempt\nto contextualize these sensing data with the goal of supporting parents'\nawareness of the educational performance and social wellbeing of their\nchildren, as well as motivating and sustaining a two-way communication between\nparents and teachers over the long term.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2012 18:55:49 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Rodrigues", "Jos\u00e9", ""], ["Gouveia", "R\u00faben", ""], ["Lyra", "Olga", ""], ["Karapanos", "Evangelos", ""]]}, {"id": "1207.1821", "submitter": "Evangelos Karapanos", "authors": "Evangelos Karapanos", "title": "Beyond Experience Sampling: Evaluating Personal Informatics with\n  Technology-Assisted Reconstruction", "comments": null, "journal-ref": "In adjunct proceedings of the conference on Human factors in\n  computing systems (CHI 2012), Workshop on Personal Informatics in Practice:\n  Improving Quality of Life Through Data. Austin, Canada", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experience Sampling has been considered the golden standard of in-situ\nmeasurement, yet, at the expense of high burden to participants. In this paper\nwe propose Technology-Assisted Reconstruction (TAR), a methodological approach\nthat combines passive logging of users' behaviors with use of these data in\nassisting the reconstruction of behaviors and experiences. Through a number of\nrecent and ongoing projects we will discuss how TAR may be employed for the\nevaluation of personal informatics systems, but also, conversely, how ideas\nfrom the field of personal informatics may contribute towards the development\nof new methodologies for in-situ evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2012 18:58:39 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Karapanos", "Evangelos", ""]]}, {"id": "1207.1894", "submitter": "John-John Cabibihan", "authors": "John-John Cabibihan, Wing-Chee So, Sujin Saj, Zhengchen Zhang", "title": "Telerobotic Pointing Gestures Shape Human Spatial Cognition", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": "10.1007/s12369-012-0148-9", "report-no": null, "categories": "cs.HC cs.RO physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper aimed to explore whether human beings can understand gestures\nproduced by telepresence robots. If it were the case, they can derive meaning\nconveyed in telerobotic gestures when processing spatial information. We\nconducted two experiments over Skype in the present study. Participants were\npresented with a robotic interface that had arms, which were teleoperated by an\nexperimenter. The robot could point to virtual locations that represented\ncertain entities. In Experiment 1, the experimenter described spatial locations\nof fictitious objects sequentially in two conditions: speech condition (SO,\nverbal descriptions clearly indicated the spatial layout) and speech and\ngesture condition (SR, verbal descriptions were ambiguous but accompanied by\nrobotic pointing gestures). Participants were then asked to recall the objects'\nspatial locations. We found that the number of spatial locations recalled in\nthe SR condition was on par with that in the SO condition, suggesting that\ntelerobotic pointing gestures compensated ambiguous speech during the process\nof spatial information. In Experiment 2, the experimenter described spatial\nlocations non-sequentially in the SR and SO conditions. Surprisingly, the\nnumber of spatial locations recalled in the SR condition was even higher than\nthat in the SO condition, suggesting that telerobotic pointing gestures were\nmore powerful than speech in conveying spatial information when information was\npresented in an unpredictable order. The findings provide evidence that human\nbeings are able to comprehend telerobotic gestures, and importantly, integrate\nthese gestures with co-occurring speech. This work promotes engaging remote\ncollaboration among humans through a robot intermediary.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2012 17:43:02 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Cabibihan", "John-John", ""], ["So", "Wing-Chee", ""], ["Saj", "Sujin", ""], ["Zhang", "Zhengchen", ""]]}, {"id": "1207.2597", "submitter": "Saket Warade Mr.", "authors": "Saket Warade, Jagannath Aghav, Petitpierre Claude, Sandeep Udayagiri", "title": "Automated Training and Maintenance through Kinect", "comments": "14 pages, 5 figures, 1 Algorithm", "journal-ref": "Warades, S; Aghav, J; Claude, P; Udayagiri, S;,\"Automated Training\n  and Maintenance through Kinect,\"International Journal of Computer Science,\n  Engineering and Applications (IJCSEA) Vol.2, No.3, June 2012", "doi": "10.5121/ijcsea.2012.2315", "report-no": null, "categories": "cs.CV cs.ET cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have worked on reducing burden on mechanic involving\ncomplex automobile maintenance activities that are performed in centralised\nworkshops. We have presented a system prototype that combines Augmented Reality\nwith Kinect. With the use of Kinect, very high quality sensors are available at\nconsiderably low costs, thus reducing overall expenditure for system design.\nThe system can be operated either in Speech mode or in Gesture mode. The system\ncan be controlled by various audio commands if user opts for Speech mode. The\nsame controlling can also be done by using a set of Gestures in Gesture mode.\n  Gesture recognition is the task performed by Kinect system. This system,\nbundled with RGB and Depth camera, processes the skeletal data by keeping track\nof 20 different body joints. Recognizing Gestures is done by verifying user\nmovements and checking them against predefined condition. Augmented Reality\nmodule captures real-time image data streams from high resolution camera. This\nmodule then generates 3D model that is superimposed on real time data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 11:17:28 GMT"}], "update_date": "2012-07-12", "authors_parsed": [["Warade", "Saket", ""], ["Aghav", "Jagannath", ""], ["Claude", "Petitpierre", ""], ["Udayagiri", "Sandeep", ""]]}, {"id": "1207.2922", "submitter": "Surbhi", "authors": "Surbhi, Vishal Arora", "title": "ROI Segmentation for Feature Extraction from Human Facial Images", "comments": "4 pages, 2 figures; International Journal of Research in Computer\n  Science, pp. 61-64 (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Computer Interaction (HCI) is the biggest goal of computer vision\nresearchers. Features form the different facial images are able to provide a\nvery deep knowledge about the activities performed by the different facial\nmovements. In this paper we presented a technique for feature extraction from\nvarious regions of interest with the help of Skin color segmentation technique,\nThresholding, knowledge based technique for face recognition.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 11:11:35 GMT"}], "update_date": "2012-07-13", "authors_parsed": [["Surbhi", "", ""], ["Arora", "Vishal", ""]]}, {"id": "1207.3351", "submitter": "Laurent George", "authors": "Laurent George (INRIA - IRISA), Maud Marchal (INRIA - IRISA), Loe\\\"iz\n  Glondu (INRIA - IRISA), Anatole L\\'ecuyer (INRIA - IRISA)", "title": "Combining Brain-Computer Interfaces and Haptics: Detecting Mental\n  Workload to Adapt Haptic Assistance", "comments": "EuroHaptics (2012)", "journal-ref": null, "doi": "10.1007/978-3-642-31401-8_12", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the combined use of Brain-Computer Interfaces\n(BCI) and Haptic interfaces. We propose to adapt haptic guides based on the\nmental activity measured by a BCI system. This novel approach is illustrated\nwithin a proof-of-concept system: haptic guides are toggled during a\npath-following task thanks to a mental workload index provided by a BCI. The\naim of this system is to provide haptic assistance only when the user's brain\nactivity reflects a high mental workload. A user study conducted with 8\nparticipants shows that our proof-of-concept is operational and exploitable.\nResults show that activation of haptic guides occurs in the most difficult part\nof the path-following task. Moreover it allows to increase task performance by\n53% by activating assistance only 59% of the time. Taken together, these\nresults suggest that BCI could be used to determine when the user needs\nassistance during haptic interaction and to enable haptic guides accordingly.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2012 13:13:27 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["George", "Laurent", "", "INRIA - IRISA"], ["Marchal", "Maud", "", "INRIA - IRISA"], ["Glondu", "Loe\u00efz", "", "INRIA - IRISA"], ["L\u00e9cuyer", "Anatole", "", "INRIA - IRISA"]]}, {"id": "1207.3422", "submitter": "Jonathan Bowen", "authors": "Stefania Boiano and Jonathan P. Bowen and Giuliano Gaia", "title": "Usability, Design and Content Issues of Mobile Apps for Cultural\n  Heritage Promotion: The Malta Culture Guide Experience", "comments": "8 pages, 7 figures, EVA London 2012: Electronic Visualisation and the\n  Arts, London, UK, 10-12 July 2012", "journal-ref": "In Stuart Dunn, Jonathan P. Bowen, and Kia Ng (eds.), EVA London\n  2012 Conference Proceedings, Electronic Workshops in Computing (eWiC),\n  British Computer Society, 2012, pages 66-73", "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper discusses the experience of producing and distributing an iPhone\napp for promotion of the Maltese Cultural Heritage on behalf of the Malta\nTourism Authority. Thanks to its position at the heart of the Mediterranean\nSea, Malta has been a crossroads of civilisations whose traces are still\nvisible today, leaving a particularly rich and varied cultural heritage, from\nmegalithic temples to baroque palaces and Caravaggio masterpieces. Conveying\nall these different aspects within a single application, using textual, visual,\nand audio means, has raised many different issues about the planning and\nproduction of cultural content for mobile usage, together with usability\naspects regarding design and distribution of a mobile app. In this paper, we\noutline all of these aspects, focusing on the design and planning strategies\nfor a long-term user commitment and how to evaluate results for cultural mobile\napplications. We include experience of all the steps of developing a mobile\napp, information that is of possible benefit to other app developers in the\ncultural sector.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2012 13:10:10 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Boiano", "Stefania", ""], ["Bowen", "Jonathan P.", ""], ["Gaia", "Giuliano", ""]]}, {"id": "1207.4776", "submitter": "Anke Brock", "authors": "Anke Brock (IRIT), Philippe Truillet (IRIT), Bernard Oriola (IRIT),\n  Delphine Picard (Octogone), Christophe Jouffrais (IRIT)", "title": "Design and User Satisfaction of Interactive Maps for Visually Impaired\n  People", "comments": null, "journal-ref": "ICCHP 2012 (2012) 544-551", "doi": "10.1007/978-3-642-31534-3_80", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal interactive maps are a solution for presenting spatial information\nto visually impaired people. In this paper, we present an interactive\nmultimodal map prototype that is based on a tactile paper map, a multi-touch\nscreen and audio output. We first describe the different steps for designing an\ninteractive map: drawing and printing the tactile paper map, choice of\nmulti-touch technology, interaction technologies and the software architecture.\nThen we describe the method used to assess user satisfaction. We provide data\nshowing that an interactive map - although based on a unique, elementary,\ndouble tap interaction - has been met with a high level of user satisfaction.\nInterestingly, satisfaction is independent of a user's age, previous visual\nexperience or Braille experience. This prototype will be used as a platform to\ndesign advanced interactions for spatial learning.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 19:29:50 GMT"}], "update_date": "2012-07-20", "authors_parsed": [["Brock", "Anke", "", "IRIT"], ["Truillet", "Philippe", "", "IRIT"], ["Oriola", "Bernard", "", "IRIT"], ["Picard", "Delphine", "", "Octogone"], ["Jouffrais", "Christophe", "", "IRIT"]]}, {"id": "1207.5720", "submitter": "Tomasz Rutkowski", "authors": "Tomasz M. Rutkowski, Hiromu Mori, Yoshihiro Matsumoto, Zhenyu Cai,\n  Moonjeong Chang, Nozomu Nishikawa, Shoji Makino, and Koichi Mori", "title": "Haptic BCI Paradigm based on Somatosensory Evoked Potential", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new concept and an online prototype of haptic BCI paradigm are presented.\nOur main goal is to develop a new, alternative and low cost paradigm, with\nopen-source hardware and software components. We also report results obtained\nwith the novel dry EEG electrodes based signal acquisition system by g.tec,\nwhich further improves experimental comfort. We address the following points: a\nnovel application of the BCI; a new methodological approach used compared to\nearlier projects; a new benefit for potential users of a BCI; the approach\nworking online/in real-time; development of a novel stimuli delivery hardware\nand software. The results with five healthy subjects and discussion of future\ndevelopments conclude this submission.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2012 15:05:50 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2012 08:49:54 GMT"}, {"version": "v3", "created": "Mon, 30 Jul 2012 10:21:31 GMT"}, {"version": "v4", "created": "Tue, 31 Jul 2012 06:10:36 GMT"}, {"version": "v5", "created": "Thu, 23 Aug 2012 13:24:59 GMT"}, {"version": "v6", "created": "Thu, 11 Oct 2012 02:45:58 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Rutkowski", "Tomasz M.", ""], ["Mori", "Hiromu", ""], ["Matsumoto", "Yoshihiro", ""], ["Cai", "Zhenyu", ""], ["Chang", "Moonjeong", ""], ["Nishikawa", "Nozomu", ""], ["Makino", "Shoji", ""], ["Mori", "Koichi", ""]]}, {"id": "1207.6224", "submitter": "J\\'er\\^ome Euzenat", "authors": "J\\'er\\^ome Euzenat", "title": "Evolving knowledge through negotiation", "comments": null, "journal-ref": null, "doi": null, "report-no": "DPA-12221", "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Semantic web information is at the extremities of long pipelines held by\nhuman beings. They are at the origin of information and they will consume it\neither explicitly because the information will be delivered to them in a\nreadable way, or implicitly because the computer processes consuming this\ninformation will affect them. Computers are particularly capable of dealing\nwith information the way it is provided to them. However, people may assign to\nthe information they provide a narrower meaning than semantic technologies may\nconsider. This is typically what happens when people do not think their\nassertions as ambiguous. Model theory, used to provide semantics to the\ninformation on the semantic web, is particularly apt at preserving ambiguity\nand delivering it to the other side of the pipeline. Indeed, it preserves as\nmuch interpretations as possible. This quality for reasoning efficiency,\nbecomes a deficiency for accurate communication and meaning preservation.\nOvercoming it may require either interactive feedback or preservation of the\nsource context. Work from social science and humanities may help solving this\nparticular problem.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2012 10:13:25 GMT"}], "update_date": "2012-07-27", "authors_parsed": [["Euzenat", "J\u00e9r\u00f4me", ""]]}]