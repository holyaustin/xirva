[{"id": "1903.00122", "submitter": "Jesse Thomason", "authors": "Jesse Thomason, Aishwarya Padmakumar, Jivko Sinapov, Nick Walker,\n  Yuqian Jiang, Harel Yedidsion, Justin Hart, Peter Stone and Raymond J. Mooney", "title": "Improving Grounded Natural Language Understanding through Human-Robot\n  Dialog", "comments": null, "journal-ref": null, "doi": "10.1109/ICRA.2019.8794287", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language understanding for robotics can require substantial domain-\nand platform-specific engineering. For example, for mobile robots to\npick-and-place objects in an environment to satisfy human commands, we can\nspecify the language humans use to issue such commands, and connect concept\nwords like red can to physical object properties. One way to alleviate this\nengineering for a new domain is to enable robots in human environments to adapt\ndynamically---continually learning new language constructions and perceptual\nconcepts. In this work, we present an end-to-end pipeline for translating\nnatural language commands to discrete robot actions, and use clarification\ndialogs to jointly improve language parsing and concept grounding. We train and\nevaluate this agent in a virtual setting on Amazon Mechanical Turk, and we\ntransfer the learned agent to a physical robot platform to demonstrate it in\nthe real world.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 01:43:11 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Thomason", "Jesse", ""], ["Padmakumar", "Aishwarya", ""], ["Sinapov", "Jivko", ""], ["Walker", "Nick", ""], ["Jiang", "Yuqian", ""], ["Yedidsion", "Harel", ""], ["Hart", "Justin", ""], ["Stone", "Peter", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1903.00185", "submitter": "Abdelkader Bellarbi", "authors": "Samir Benbelkacem (CDTA), Djamel Aouam, Nadia Zenati-Henda, Abdelkader\n  Bellarbi (CDTA), Ahmed Bouhena, Samir Otmane", "title": "MVC-3D: Adaptive Design Pattern for Virtual and Augmented Reality\n  Systems", "comments": null, "journal-ref": "International Conference on Advanced Aspects of Software\n  Engineering ICAASE'18, Dec 2018, Constantine, Algeria", "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present MVC-3D design pattern to develop virtual and\naugmented (or mixed) reality interfaces that use new types of sensors,\nmodalities and implement specific algorithms and simulation models. The\nproposed pattern represents the extension of classic MVC pattern by enriching\nthe View component (interactive View) and adding a specific component\n(Library). The results obtained on the development of augmented reality\ninterfaces showed that the complexity of M, iV and C components is reduced. The\ncomplexity increases only on the Library component (L). This helps the\nprogrammers to well structure their models even if the interface complexity\nincreases. The proposed design pattern is also used in a design process called\nMVC-3D in the loop that enables a seamless evolution from initial prototype to\nthe final system.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 07:38:23 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Benbelkacem", "Samir", "", "CDTA"], ["Aouam", "Djamel", "", "CDTA"], ["Zenati-Henda", "Nadia", "", "CDTA"], ["Bellarbi", "Abdelkader", "", "CDTA"], ["Bouhena", "Ahmed", ""], ["Otmane", "Samir", ""]]}, {"id": "1903.00213", "submitter": "Ryosuke Homma", "authors": "Ryosuke Homma, Keiichi Soejima, Mitsuo Yoshida and Kyoji Umemura", "title": "Analysis of User Dwell Time on Non-News Pages", "comments": "IEEE BigData 2018 Workshop : The 3rd International Workshop on\n  Application of Big Data for Computational Social Science (ABCSS2018). 2018", "journal-ref": null, "doi": "10.1109/BigData.2018.8621950", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is dwell time as one of the indicators of user's behavior, and this\nindicates how long a user looked at a page. Dwell time is especially useful in\nfields where user ratings are important, such as search engines, recommender\nsystems, and advertisements are important. Despite the importance of this\nindex, however, its characteristics are not well known. In this paper, we\nanalyze the dwell times of various websites by desktop and mobile devices using\ndata of one year. Our aim is to clarify the characteristics of dwell time on\nnon-news websites in order to discover which features are effective for\npredicting the dwell time. In this analysis, we focus on device types, access\ntimes, behavior on the website, and scroll depth. The results indicated that\nthe number of sessions decreased as the dwell time increased, for both desktop\nand mobile devices. We also found that hour and month greatly affected the\ndwell time, but day of the week had little effect. Moreover, we discovered that\ninside and click users tended to have longer dwell times than outside and\nnon-click users. However, we can not find a relationship between dwell time and\nscroll depth. This is because even if a user browsed the bottom of the page,\nthe user might not necessarily have read the entire page.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 09:03:08 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Homma", "Ryosuke", ""], ["Soejima", "Keiichi", ""], ["Yoshida", "Mitsuo", ""], ["Umemura", "Kyoji", ""]]}, {"id": "1903.00283", "submitter": "Stefanie Rinderle-Ma", "authors": "Manuel Gall and Stefanie Rinderle-Ma", "title": "Visualizing Multiple Process Attributes in one 3D Process Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business process models are usually visualized using 2D representations.\nHowever, multiple attributes contained in the models such as time, data, and\nresources can quickly lead to cluttered and complex representations. To address\nthese challenges, this paper proposes techniques utilizing the 3D space (e.g.,\nvisualizing swim lanes as third dimension). All techniques are implemented in a\n3D process viewer. On top of showing the feasibility of the proposed\ntechniques, the 3D process viewer served as live demonstration after which 42\nparticipants completed a survey. The survey results support that 3D\nrepresentations are well-suited to convey information on multiple attributes in\nbusiness process models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 13:18:12 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Gall", "Manuel", ""], ["Rinderle-Ma", "Stefanie", ""]]}, {"id": "1903.00438", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Ivan Sopin", "title": "Web-Based 3D and Haptic Interactive Environments for e-Learning,\n  Simulation, and Training", "comments": "ISBN:978-3-642-01343-0", "journal-ref": null, "doi": "10.1007/978-3-642-01344-7_26", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge creation occurs in the process of social interaction. As our\nservice-based society is evolving into a knowledge-based society there is an\nacute need for more effective collaboration and knowledge-sharing systems to be\nused by geographically scattered people. We present the use of Web3D components\nand standards, such as X3D, in combination with the haptic (tactile) paradigm,\nfor the development of new communication channels for e-Learning and\nsimulation.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 17:58:06 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Sopin", "Ivan", ""]]}, {"id": "1903.00718", "submitter": "Maria Maleshkova", "authors": "Sebastian R. Bader, Maria Maleshkova", "title": "Virtual Representations for Iterative IoT Deployment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central vision of the Internet of Things is the representation of the\nphysical world in a consistent virtual environment. Especially in the context\nof smart factories the connection of the different, heterogeneous production\nmodules through a digital shop floor promises faster conversion rates,\ndata-driven maintenance or automated machine configurations for use cases,\nwhich have not been known at design time. Nevertheless, these scenarios demand\nIoT representations of all participating machines and components, which\nrequires high installation efforts and hardware adjustments.\n  We propose an incremental process for bringing the shop floor closer to the\nIoT vision. Currently the majority of systems, components or parts are not yet\nconnected with the internet and might not even provide the possibility to be\ntechnically equipped with sensors. However, those could be essential parts for\na realistic digital shop floor representation. We, therefore, propose Virtual\nRepresentations, which are capable of independently calculating a physical\nobject's condition by dynamically collecting and interpreting already available\ndata through RESTful Web APIs. The internal logic of such Virtual\nRepresentations are further adjustable at runtime, since changes to its\nrespective physical object, its environment or updates to the resource itself\nshould not cause any downtime.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 15:14:15 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Bader", "Sebastian R.", ""], ["Maleshkova", "Maria", ""]]}, {"id": "1903.01210", "submitter": "James Lewis Mr", "authors": "James Lewis and Benedikte Rorstad", "title": "Immersive VR as a Tool to Enhance Relaxation for Undergraduate Students\n  with the Aim of Reducing Anxiety - A Pilot Study", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite extensive use in related domains, Virtual Reality (VR) for\ngeneralised anxiety disorder (GAD) has received little previous attention. We\nreport upon a VR environment created for the Oculus Rift and Unreal Engine 4\n(UE4) to investigate the potential of a VR simulation to be used as an anxiety\nmanagement tool. We introduce the broad topic of GAD and related publications\non the application of VR to this, and similar, mental health conditions. We\nthen describe the development of a real time simulation tool, based upon the\npassive VR experience of a tranquil, rural alpine scene experienced from a\nseated position with head tracking. Evaluation focused upon qualitative\nfeedback on the application. Testing was carried out over the period of two\nweeks on a sample group of eleven students studying at Nottingham Trent\nUniversity. All participants were asked to complete the Depression, Anxiety and\nStress Scale - 21 Items (DASS21) at the beginning and at the end of the study\norder to assess their profile, and hence suitability to comment upon the\nsoftware. Qualitative feedback was very encouraging, with all participants\nreporting that they believed the experience helped and that they would consider\nutilising it if it was available. Additionally, a psychologist was asked to\ntest the application to provide a specialist opinion on whether it would be\nappropriate for use as an anxiety management tool. The results highlight\nseveral areas for improvement but are positive overall in terms of its\npotential as a therapeutic tool.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 12:44:31 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Lewis", "James", ""], ["Rorstad", "Benedikte", ""]]}, {"id": "1903.01219", "submitter": "Abdelkader Bellarbi", "authors": "Samir Benbelkacem (CDTA), Abdelkader Bellarbi (CDTA), Nadia\n  Zenati-Henda (CDTA), Ahmed Bentaleb, Ahmed Bellabaci, Samir Otmane", "title": "Low-cost VR Collaborative System equipped with Haptic Feedback", "comments": null, "journal-ref": "the 24th ACM Symposium, Nov 2018, Tokyo, Japan. ACM Press, pp.1-2", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a low-cost virtual reality (VR) collaborative\nsystem equipped with a haptic feedback sensation system. This system is\ncomposed of a Kinect sensor for bodies and gestures detection, a\nmicrocontroller and vibrators to simulate outside interactions, and smartphone\npowered cardboard, all of this are put into a network implemented with Unity 3D\ngame engine. CCS CONCEPTS $\\bullet$ Interaction paradigms $\\rightarrow$ Virtual\nreality; Collaborative interaction; $\\bullet$ Hardware $\\rightarrow$ Sensors\nand actuators; Wireless devices; KEYWORDS collaborative virtual reality, haptic\nfeedback system.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 07:36:19 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Benbelkacem", "Samir", "", "CDTA"], ["Bellarbi", "Abdelkader", "", "CDTA"], ["Zenati-Henda", "Nadia", "", "CDTA"], ["Bentaleb", "Ahmed", ""], ["Bellabaci", "Ahmed", ""], ["Otmane", "Samir", ""]]}, {"id": "1903.01249", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Adrian Seitan, Dorin M. Popovici, Crenguta M.\n  Bogdan", "title": "Liver Pathology Simulation: Algorithm for Haptic Rendering and Force\n  Maps for Palpation Assessment", "comments": "arXiv admin note: text overlap with arXiv:1812.03325", "journal-ref": "Medicine Meets Virtual Reality MMVR, 2013, pp. 175-181", "doi": "10.3233/978-1-61499-209-7-175", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preoperative gestures include tactile sampling of the mechanical properties\nof biological tissue for both histological and pathological considerations.\nTactile properties used in conjunction with visual cues can provide useful\nfeedback to the surgeon. Development of novel cost effective haptic-based\nsimulators and their introduction in the minimally invasive surgery learning\ncycle can absorb the learning curve for your residents. Receiving pre-training\nin a core set of surgical skills can reduce skill acquisition time and risks.\nWe present the integration of a real-time surface stiffness adjustment\nalgorithm and a novel paradigm -- force maps -- in a visuo-haptic simulator\nmodule designed to train internal organs disease diagnostics through palpation.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 18:16:20 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Seitan", "Adrian", ""], ["Popovici", "Dorin M.", ""], ["Bogdan", "Crenguta M.", ""]]}, {"id": "1903.01333", "submitter": "Michalis Xenos", "authors": "Michalis Xenos, Vasiliki Velli", "title": "A Serious Game for Introducing Software Engineering Ethics to University\n  Students", "comments": null, "journal-ref": "ICL2018, 21st International Conference on Interactive\n  Collaborative Learning, pp. 263-274, Kos, Greece, 25-28 September 2018", "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a game based on storytelling, in which the players are\nfaced with ethical dilemmas related to software engineering specific issues.\nThe players' choices have consequences on how the story unfolds and could lead\nto various alternative endings. This Ethics Game was used as a tool to mediate\nthe learning activity and it was evaluated by 144 students during a Software\nEngineering Course on the 2017-2018 academic year. This evaluation was based on\na within-subject pre-post design methodology and provided insights on the\nstudents learning gain (academic performance), as well as on the students'\nperceived educational experience. In addition, it provided the results of the\nstudents' usability evaluation of the Ethics Game. The results indicated that\nthe students did improve their knowledge about software engineering ethics by\nplaying this game. Also, they considered this game to be a useful educational\ntool and of high usability. Female students had statistically significant\nhigher knowledge gain and higher evaluation scores than male students, while no\nstatistically significant differences were measured in groups based on the year\nof study.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 16:26:26 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Xenos", "Michalis", ""], ["Velli", "Vasiliki", ""]]}, {"id": "1903.01345", "submitter": "Michalis Xenos", "authors": "Michalis Xenos, Maria Rigou", "title": "Teaching HCI Design in a Flipped Learning M.Sc. Course Using\n  Eye-Tracking Peer Evaluation Data", "comments": null, "journal-ref": "ECEL, 17th European Conference on e-Learning, pp. 611-619, Athens,\n  Greece, 1-2 November 2018", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents experiences from a flipped classroom M.Sc. course on\nHuman-Computer Interaction (HCI). The students that finished successfully this\ncourse participated in twelve short workshops, based on a flipped classroom\nmodel. Each workshop focused on a specific HCI activity, while before the\nworkshops, a two-hour lecture was used to introduce the students in the flipped\nlearning concept. All the rest of the educational material was offered to the\nstudents online before each workshop. Such material was mainly short lectures\nfrom the professor, in the form of videos uploaded in the course's YouTube\nchannel and documents delivered using the university learning management system\n(LMS). For each workshop the students had to be prepared to participate, which\nwas tested using brief quizzes before the start of specific workshops. The\nactivity presented in this paper was the design and evaluation of an\ninteractive system. The students were asked to form six groups comprising of\nthree to four students each. Then a system's description, vague enough to\nstimulate creativity, was randomly assigned to each group. This activity\npresented in this paper was the longest activity of the entire course and it\nwas conducted in four consequent workshops. The paper presents the setting of\nthis experiment, the peer assessment method and the use of eye-tracking data\ncollected and analysed to aid the students towards improving their design. The\nstudents created a working model of the system with limited functionality and\nimproved this model using eye-tracking data from the peer evaluation of this\nmodel. The use of these data offered them the insight to improve their models\nand to undergo design changes. The paper presents samples of the progress made\nbetween various versions of the models and concludes presenting the preliminary\npositive results of the students qualitative evaluation of this experiment.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 16:34:08 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Xenos", "Michalis", ""], ["Rigou", "Maria", ""]]}, {"id": "1903.01601", "submitter": "Behnam Malmir", "authors": "Behnam Malmir, Shing I Chang, Malgorzata Rys and Dylan Darter", "title": "Quantifying Gait Changes Using Microsoft Kinect and Sample Entropy", "comments": "This article is an updated version of a paper entitled 'Quantifying\n  Gait Changes Using Microsoft Kinect and Sample Entropy' presented at the 2018\n  Industrial and Systems Engineering Research Conference (ISERC) in Orlando,\n  Florida (May 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CG cs.HC stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This study describes a method to quantify potential gait changes in human\nsubjects. Microsoft Kinect devices were used to provide and track coordinates\nof fifteen different joints of a subject over time. Three male subjects walk a\n10-foot path multiple times with and without motion-restricting devices. Their\nwalking patterns were recorded via two Kinect devices through frontal and\nsagittal planes. A modified sample entropy (SE) value was computed to quantify\nthe variability of the time series for each joint. The SE values with and\nwithout motion-restricting devices were used to compare the changes in each\njoint. The preliminary results of the experiments show that the proposed\nquantification method can detect differences in walking patterns with and\nwithout motion-restricting devices. The proposed method has the potential to be\napplied to track personal progress in physical therapy sessions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 00:16:23 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Malmir", "Behnam", ""], ["Chang", "Shing I", ""], ["Rys", "Malgorzata", ""], ["Darter", "Dylan", ""]]}, {"id": "1903.01780", "submitter": "Taha Hasan", "authors": "Taha Hassan, D. Scott McCrickard", "title": "Trust and Trustworthiness in Social Recommender Systems", "comments": "WWW '19 FATES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The prevalence of misinformation on online social media has tangible\nempirical connections to increasing political polarization and partisan\nantipathy in the United States. Ranking algorithms for social recommendation\noften encode broad assumptions about network structure (like homophily) and\ngroup cognition (like, social action is largely imitative). Assumptions like\nthese can be na\\\"ive and exclusionary in the era of fake news and ideological\nuniformity towards the political poles. We examine these assumptions with aid\nfrom the user-centric framework of trustworthiness in social recommendation.\nThe constituent dimensions of trustworthiness (diversity, transparency,\nexplainability, disruption) highlight new opportunities for discouraging\ndogmatization and building decision-aware, transparent news recommender\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 12:10:11 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Hassan", "Taha", ""], ["McCrickard", "D. Scott", ""]]}, {"id": "1903.01808", "submitter": "Hugo Lhachemi", "authors": "Hugo Lhachemi, Ammar Malik and Robert Shorten", "title": "Augmented Reality, Cyber-Physical Systems, and Feedback Control for\n  Additive Manufacturing: A Review", "comments": "Preprint", "journal-ref": "IEEE Access, vol. 7, 2019", "doi": "10.1109/ACCESS.2019.2907287", "report-no": null, "categories": "cs.HC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective in this paper is to review the application of feedback ideas in\nthe area of additive manufacturing. Both the application of feedback control to\nthe 3D printing process, and the application of feedback theory to enable users\nto interact better with machines, are reviewed. Where appropriate,\nopportunities for future work are highlighted.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 13:21:44 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Lhachemi", "Hugo", ""], ["Malik", "Ammar", ""], ["Shorten", "Robert", ""]]}, {"id": "1903.01831", "submitter": "Tommy Nilsson", "authors": "Tommy Nilsson, Andy Crabtree, Joel Fischer, Boriana Koleva", "title": "Breaching the Future: Understanding Human Challenges of Autonomous\n  Systems for the Home", "comments": "Personal and Ubiquitous Computing", "journal-ref": null, "doi": "10.1007/s00779-019-01210-7", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The domestic environment is a key area for the design and deployment of\nautonomous systems. Yet research indicates their adoption is already being\nhampered by a variety of critical issues including trust, privacy and security.\nThis paper explores how potential users relate to the concept of autonomous\nsystems in the home and elaborates further points of friction. It makes two\ncontributions. One methodological, focusing on the use of provocative utopian\nand dystopian scenarios of future autonomous systems in the home. These are\nused to drive an innovative workshop-based approach to breaching experiments,\nwhich surfaces the usually tacit and unspoken background expectancies\nimplicated in the organisation of everyday life that have a powerful impact on\nthe acceptability of future and emerging technologies. The other contribution\nis substantive, produced through participants efforts to repair the incongruity\nor \"reality disjuncture\" created by utopian and dystopian visions, and\nhighlights the need to build social as well as computational accountability\ninto autonomous systems, and to enable coordination and control.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 17:46:10 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Nilsson", "Tommy", ""], ["Crabtree", "Andy", ""], ["Fischer", "Joel", ""], ["Koleva", "Boriana", ""]]}, {"id": "1903.01968", "submitter": "Avinash Sharma", "authors": "Avinash Sharma, Wally Niu, Christopher L. Hunt, George Levay, Rahul\n  Kaliki, Nitish V. Thakor", "title": "Augmented Reality Prosthesis Training Setup for Motor Skill Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adjusting to amputation can often time be difficult for the body.\nPost-surgery, amputees have to wait for up to several months before receiving a\nproperly fitted prosthesis. In recent years, there has been a trend toward\nquantitative outcome measures. In this paper, we developed the augmented\nreality (AR) version of one such measure, the Prosthetic Hand Assessment\nMeasure (PHAM). The AR version of the PHAM - HoloPHAM, offers amputees the\nadvantage to train with pattern recognition, at their own time and convenience,\npre- and post-prosthesis fitting. We provide a rigorous analysis of our system,\nfocusing on its ability to simulate reach, grasp, and touch in AR. Similarity\nof motion joint dynamics for reach in physical and AR space were compared, with\nexperiments conducted to illustrate how depth in AR is perceived. To show the\neffectiveness and validity of our system for prosthesis training, we conducted\na 10-day study with able-bodied subjects (N = 3) to see the effect that\ntraining on the HoloPHAM had on other established functional outcome measures.\nA washout phase of 5 days was incorporated to observe the effect without\ntraining. Comparisons were made with standardized outcome metrics, along with\nthe progression of kinematic variability over time. Statistically significant\n(p<0.05) improvements were observed between pre- and post-training stages. Our\nresults show that AR can be an effective tool for prosthesis training with\npattern recognition systems, fostering motor learning for reaching movement\ntasks, and paving the possibility of replacing physical training.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 18:25:39 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Sharma", "Avinash", ""], ["Niu", "Wally", ""], ["Hunt", "Christopher L.", ""], ["Levay", "George", ""], ["Kaliki", "Rahul", ""], ["Thakor", "Nitish V.", ""]]}, {"id": "1903.01977", "submitter": "Emad Aghayi", "authors": "Emad Aghayi, Thomas D. LaToza, Paurav Surendra, Seyedmeysam\n  Abolghasemi", "title": "Crowdsourced Behavior-Driven Development: Implementing Microservices\n  through Microtasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key to the effectiveness of crowdsourcing approaches for software engineering\nis workflow design, describing how complex work is organized into small,\nrelatively independent microtasks. In this paper, we introduce a\nBehavior-Driven Development (BDD) workflow for accomplishing programming work\nthrough self-contained microtasks, implemented as a preconfigured environment\ncalled Crowd Microservices. In our approach, a client, acting on behalf of a\nsoftware team, describes a microservice as a set of endpoints with paths,\nrequests, and responses. A crowd then implements the endpoints, identifying\nindividual endpoint behaviors which they test, implement, and debug, creating\nnew functions and interacting with persistence APIs as needed. To evaluate our\napproach, we conducted a feasibility study in which a small crowd worked to\nimplement a small ToDo microservice. The crowd created an implementation with\nonly four defects, completing 350 microtasks and implementing 13 functions. We\ndiscuss the implications of these findings for incorporating crowdsourced\nprogramming contributions into traditional software projects.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 18:42:57 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 17:19:41 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Aghayi", "Emad", ""], ["LaToza", "Thomas D.", ""], ["Surendra", "Paurav", ""], ["Abolghasemi", "Seyedmeysam", ""]]}, {"id": "1903.02446", "submitter": "Sebastian Stefan Feger", "authors": "Sebastian S. Feger, S\\\"unje Dallmeier-Tiessen, Pawe{\\l} W. Wo\\'zniak,\n  Albrecht Schmidt", "title": "Gamification in Science: A Study of Requirements in the Context of\n  Reproducible Research", "comments": "Accepted for publication at the 2019 Conference on Human Factors in\n  Computing Systems (CHI 2019)", "journal-ref": null, "doi": "10.1145/3290605.3300690", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for data preservation and reproducible research is widely recognized\nin the scientific community. Yet, researchers often struggle to find the\nmotivation to contribute to data repositories and to use tools that foster\nreproducibility. In this paper, we explore possible uses of gamification to\nsupport reproducible practices in High Energy Physics. To understand how\ngamification can be effective in research tools, we participated in a workshop\nand performed interviews with data analysts. We then designed two interactive\nprototypes of a research preservation service that use contrasting gamification\nstrategies. The evaluation of the prototypes showed that gamification needs to\naddress core scientific challenges, in particular the fair reflection of\nquality and individual contribution. Through thematic analysis, we identified\nfour themes which describe perceptions and requirements of gamification in\nresearch: Contribution, Metrics, Applications and Scientific practice. Based on\nthese, we discuss design implications for gamification in science.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 15:20:16 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Feger", "Sebastian S.", ""], ["Dallmeier-Tiessen", "S\u00fcnje", ""], ["Wo\u017aniak", "Pawe\u0142 W.", ""], ["Schmidt", "Albrecht", ""]]}, {"id": "1903.02612", "submitter": "Yousra Javed", "authors": "Yousra Javed, Mohamed Shehab", "title": "Visual Analysis of Photo Policy Misconfigurations Using Treemaps", "comments": "5 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online photo privacy is a major concern for social media users. Numerous\nvisualization tools have been proposed to help the users easily compose and\nunderstand policies on social networks. However, these tools do not incorporate\nthe ability to quickly identify and fix unintended photo sharing. We propose a\ntool that displays the photo albums w.r.t their policy misconfigurations using\na Treemap visualization.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 21:09:31 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Javed", "Yousra", ""], ["Shehab", "Mohamed", ""]]}, {"id": "1903.02618", "submitter": "Yousra Javed", "authors": "Yousra Javed, Boyd Davis, Mohamed Shehab", "title": "Seniors' Media Preference for Receiving Internet Security Information: A\n  Pilot Study", "comments": "5 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing use of Internet by older adults and their low computer\nand Internet security literacy, their susceptibility to online fraud has also\nincreased. This suggests in turn that there are still too few Internet\neducation materials targeting seniors. We take a first step towards developing\ninteractive security information materials for seniors by determining which\nmedia they prefer and can easily comprehend. We studied the reception of two\nmedia, text and audio, as they communicated information about email-based\nphishing attacks. Our preliminary study of 34 seniors shows that the\nparticipants personally preferred the text over the audio. However, the\ncomprehension score was not significantly different for participants who read\nthe phishing training text script as compared to the participants who listened\nto the phishing training audio script.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 21:24:35 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Javed", "Yousra", ""], ["Davis", "Boyd", ""], ["Shehab", "Mohamed", ""]]}, {"id": "1903.02691", "submitter": "Felix Hamza-Lup", "authors": "Dorin M. Popovici, Felix G. Hamza-Lup, Adrian Seitan, Crenguta M.\n  Bogdan", "title": "Comparative Study of APIs and Frameworks for Haptic Application\n  Development", "comments": null, "journal-ref": "Cyber-Worlds, 2012, pp. 37-44", "doi": "10.1109/CW.2012.13", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulation of tactile sensation using haptic devices is increasingly\ninvestigated in conjunction with simulation and training. In this paper we\nexplore the most popular haptic frameworks and APIs. We provide a comprehensive\nreview and comparison of their features and capabilities, from the perspective\nof the need to develop a haptic simulator for medical training purposes. In\norder to compare the studied frameworks and APIs, we identified and applied a\nset of 11 criteria and we obtained a classification of platforms, from the\nperspective of our project. According to this classification, we used the best\nplatform to develop a visual-haptic prototype for liver diagnostics.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 02:11:06 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Popovici", "Dorin M.", ""], ["Hamza-Lup", "Felix G.", ""], ["Seitan", "Adrian", ""], ["Bogdan", "Crenguta M.", ""]]}, {"id": "1903.02723", "submitter": "Zhenliang Zhang", "authors": "Zhenliang Zhang, Cong Wang, Dongdong Weng, Yue Liu, Yongtian Wang", "title": "Symmetrical Reality: Toward a Unified Framework for Physical and Virtual\n  Reality", "comments": "IEEE VR Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we review the background of physical reality, virtual reality,\nand some traditional mixed forms of them. Based on the current knowledge, we\npropose a new unified concept called symmetrical reality to describe the\nphysical and virtual world in a unified perspective. Under the framework of\nsymmetrical reality, the traditional virtual reality, augmented reality,\ninverse virtual reality, and inverse augmented reality can be interpreted using\na unified presentation. We analyze the characteristics of symmetrical reality\nfrom two different observation locations (i.e., from the physical world and\nfrom the virtual world), where all other forms of physical and virtual reality\ncan be treated as special cases of symmetrical reality.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 04:29:50 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Zhang", "Zhenliang", ""], ["Wang", "Cong", ""], ["Weng", "Dongdong", ""], ["Liu", "Yue", ""], ["Wang", "Yongtian", ""]]}, {"id": "1903.02978", "submitter": "Nico Herbig", "authors": "Nico Herbig, Santanu Pal, Josef van Genabith, Antonio Kr\\\"uger", "title": "Integrating Artificial and Human Intelligence for Efficient Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current advances in machine translation increase the need for translators to\nswitch from traditional translation to post-editing of machine-translated text,\na process that saves time and improves quality. Human and artificial\nintelligence need to be integrated in an efficient way to leverage the\nadvantages of both for the translation task. This paper outlines approaches at\nthis boundary of AI and HCI and discusses open research questions to further\nadvance the field.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 15:14:42 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Herbig", "Nico", ""], ["Pal", "Santanu", ""], ["van Genabith", "Josef", ""], ["Kr\u00fcger", "Antonio", ""]]}, {"id": "1903.03104", "submitter": "James Bagrow", "authors": "Abigail Hotaling and James P. Bagrow", "title": "Accurate inference of crowdsourcing properties when using efficient\n  allocation strategies", "comments": "21 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allocation strategies improve the efficiency of crowdsourcing by decreasing\nthe work needed to complete individual tasks accurately. However, these\nalgorithms introduce bias by preferentially allocating workers onto easy tasks,\nleading to sets of completed tasks that are no longer representative of all\ntasks. This bias challenges inference of problem-wide properties such as\ntypical task difficulty or crowd properties such as worker completion times,\nimportant information that goes beyond the crowd responses themselves. Here we\nstudy inference about problem properties when using an allocation algorithm to\nimprove crowd efficiency. We introduce Decision-Explicit Probability Sampling\n(DEPS), a method to perform inference of problem properties while accounting\nfor the potential bias introduced by an allocation strategy. Experiments on\nreal and synthetic crowdsourcing data show that DEPS outperforms baseline\ninference methods while still leveraging the efficiency gains of the allocation\nmethod. The ability to perform accurate inference of general properties when\nusing non-representative data allows crowdsourcers to extract more knowledge\nout of a given crowdsourced dataset.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 18:58:34 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Hotaling", "Abigail", ""], ["Bagrow", "James P.", ""]]}, {"id": "1903.03168", "submitter": "Ganapati Bhat", "authors": "Ganapati Bhat, Ranadeep Deb, Umit Y. Ogras", "title": "OpenHealth: Open Source Platform for Wearable Health Monitoring", "comments": "To appear in a future issue of IEEE Design & Test", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Movement disorders are becoming one of the leading causes of functional\ndisability due to aging populations and extended life expectancy. Wearable\nhealth monitoring is emerging as an effective way to augment clinical care for\nmovement disorders. However, wearable devices face a number of adaptation and\ntechnical challenges that hinder their widespread adoption. To address these\nchallenges, we introduce OpenHealth, an open source platform for wearable\nhealth monitoring. OpenHealth aims to design a standard set of\nhardware/software and wearable devices that can enable autonomous collection of\nclinically relevant data. The OpenHealth platform includes a wearable device,\nstandard software interfaces and reference implementations of human activity\nand gesture recognition applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 01:53:52 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 22:11:27 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Bhat", "Ganapati", ""], ["Deb", "Ranadeep", ""], ["Ogras", "Umit Y.", ""]]}, {"id": "1903.03265", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, William H. Baird", "title": "Feel the Static and Kinetic Friction", "comments": null, "journal-ref": "EuroHaptics, 2012, pp. 181-192", "doi": "10.1007/978--3--642--31401--8_17", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal simulations augment the presentation of abstract concepts\nfacilitating theoretical models understanding and learning. Most simulations\nonly engage two of our five senses: sight and hearing. If we employ additional\nsensory communication channels in simulations, we may gain a deeper\nunderstanding of illustrated concepts by increasing the communication bandwidth\nand providing alternative perspectives. We implemented the sense of touch in 3D\nsimulations to teach important concepts in introductory physics. Specifically,\nwe developed a visual/haptic simulation for friction. We prove that interactive\n3D haptic simulations, if carefully developed and deployed, are useful in\nengaging students and allowing them to understand concepts faster. We\nhypothesize that large scale deployment of such haptic-based simulators in\nscience laboratories is now possible due to the advancements in haptic software\nand hardware technology.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 03:36:48 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Baird", "William H.", ""]]}, {"id": "1903.03266", "submitter": "Huang Yanpei", "authors": "Yanpei Huang, Etienne Burdet, Lin Cao, Phuoc Thien Phan, Anthony Meng\n  Huat Tiong, Pai Zheng and Soo Jay Phee", "title": "Performance evaluation of a foot-controlled human-robot interface", "comments": "7 pages, submit to 2019 IROS RA-Letter", "journal-ref": "IEEE Robotics and Automation Letters, 2019", "doi": "10.1109/LRA.2019.2926215", "report-no": null, "categories": "cs.RO cs.HC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic minimally invasive interventions typically require using more than\ntwo instruments. We thus developed a foot pedal interface which allows the user\nto control a robotic arm (simultaneously to working with the hands) with four\ndegrees of freedom in continuous directions and speeds. This paper evaluates\nand compares the performances of ten naive operators in using this new pedal\ninterface and a traditional button interface in completing tasks. These tasks\nare geometrically complex path-following tasks similar to those in laparoscopic\ntraining, and the traditional button interface allows axis-by-axis control with\nconstant speeds. Precision, time, and smoothness of the subjects' control\nmovements for these tasks are analysed. The results demonstrate that the pedal\ninterface can be used to control a robot for complex motion tasks. The subjects\nkept the average error rate at a low level of around 2.6% with both interfaces,\nbut the pedal interface resulted in about 30% faster operation speed and 60%\nsmoother movement, which indicates improved efficiency and user experience as\ncompared with the button interface. The results of a questionnaire show that\nthe operators found that controlling the robot with the pedal interface was\nmore intuitive, comfortable, and less tiring than using the button interface.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 03:45:45 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Huang", "Yanpei", ""], ["Burdet", "Etienne", ""], ["Cao", "Lin", ""], ["Phan", "Phuoc Thien", ""], ["Tiong", "Anthony Meng Huat", ""], ["Zheng", "Pai", ""], ["Phee", "Soo Jay", ""]]}, {"id": "1903.03268", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Crenguta M. Bogdan, Adrian Seitan", "title": "Haptic Simulator for Liver Diagnostics through Palpation", "comments": null, "journal-ref": "Medicine Meets Virtual Reality 19, 2012, pp.156-160", "doi": "10.3233/978--1--61499--022--2--156", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanical properties of biological tissue for both histological and\npathological considerations are often required in disease diagnostics. Such\nproperties can be simulated and explored with haptic technology. Development of\ncost effective haptic-based simulators and their introduction in the minimally\ninvasive surgery learning cycle is still in its infancy. Receiving pretraining\nin a core set of surgical skills can reduce skill acquisition time and risks.\nWe present the development of a visuo-haptic simulator module designed to train\ninternal organs disease diagnostics through palpation. The module is part of a\nset of tools designed to train and improve basic surgical skills for minimally\ninvasive surgery.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 03:50:21 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Bogdan", "Crenguta M.", ""], ["Seitan", "Adrian", ""]]}, {"id": "1903.03272", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Crenguta M. Bogdan, Dorin M. Popovici, Ovidiu D.\n  Costea", "title": "A Survey of Visuo-Haptic Simulation in Surgical Training", "comments": null, "journal-ref": "International Conference on Mobile, Hybrid, and Online Learning,\n  2011, pp. 57-62", "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgeons must accomplish complex technical and intellectual tasks that can\ngenerate unexpected and serious challenges with little or no room for error. In\nthe last decade, computer simulations have played an increasing role in\nsurgical training, pre-operative planning, and biomedical research.\nSpecifically, visuo-haptic simulations have been the focus of research to\ndevelop advanced e-Learning systems facilitating surgical training. The cost of\nhaptic hardware was reduced through mass scale production and as haptics gained\npopularity in the gaming industry. Visuo-haptic simulations combine the tactile\nsense with visual information and provide training scenarios with a high degree\nof reality. For surgical training, such scenarios can be used as ways to gain,\nimprove, and assess resident and expert surgeons' skills and knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 04:06:21 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Bogdan", "Crenguta M.", ""], ["Popovici", "Dorin M.", ""], ["Costea", "Ovidiu D.", ""]]}, {"id": "1903.03369", "submitter": "Taras Kucherenko", "authors": "Taras Kucherenko, Dai Hasegawa, Gustav Eje Henter, Naoshi Kaneko,\n  Hedvig Kjellstr\\\"om", "title": "Analyzing Input and Output Representations for Speech-Driven Gesture\n  Generation", "comments": "Accepted at IVA '19. Shorter version published at AAMAS '19. The code\n  is available at\n  https://github.com/GestureGeneration/Speech_driven_gesture_generation_with_autoencoder", "journal-ref": null, "doi": "10.1145/3308532.3329472", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for automatic speech-driven gesture\ngeneration, applicable to human-agent interaction including both virtual agents\nand robots. Specifically, we extend recent deep-learning-based, data-driven\nmethods for speech-driven gesture generation by incorporating representation\nlearning. Our model takes speech as input and produces gestures as output, in\nthe form of a sequence of 3D coordinates. Our approach consists of two steps.\nFirst, we learn a lower-dimensional representation of human motion using a\ndenoising autoencoder neural network, consisting of a motion encoder MotionE\nand a motion decoder MotionD. The learned representation preserves the most\nimportant aspects of the human pose variation while removing less relevant\nvariation. Second, we train a novel encoder network SpeechE to map from speech\nto a corresponding motion representation with reduced dimensionality. At test\ntime, the speech encoder and the motion decoder networks are combined: SpeechE\npredicts motion representations based on a given speech signal and MotionD then\ndecodes these representations to produce motion sequences. We evaluate\ndifferent representation sizes in order to find the most effective\ndimensionality for the representation. We also evaluate the effects of using\ndifferent speech features as input to the model. We find that mel-frequency\ncepstral coefficients (MFCCs), alone or combined with prosodic features,\nperform the best. The results of a subsequent user study confirm the benefits\nof the representation learning.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 11:03:43 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 13:57:30 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 12:00:57 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2019 07:13:39 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Kucherenko", "Taras", ""], ["Hasegawa", "Dai", ""], ["Henter", "Gustav Eje", ""], ["Kaneko", "Naoshi", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "1903.03414", "submitter": "Bo Zhang", "authors": "Jinyu Yang, Bo Zhang", "title": "Artificial Intelligence in Intelligent Tutoring Robots: A Systematic\n  Review and Design Guidelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study provides a systematic review of the recent advances in designing\nthe intelligent tutoring robot (ITR), and summarises the status quo of applying\nartificial intelligence (AI) techniques. We first analyse the environment of\nthe ITR and propose a relationship model for describing interactions of ITR\nwith the students, the social milieu and the curriculum. Then, we transform the\nrelationship model into the perception-planning-action model for exploring what\nAI techniques are suitable to be applied in the ITR. This article provides\ninsights on promoting human-robot teaching-learning process and AI-assisted\neducational techniques, illustrating the design guidelines and future research\nperspectives in intelligent tutoring robots.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 07:39:58 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Yang", "Jinyu", ""], ["Zhang", "Bo", ""]]}, {"id": "1903.03446", "submitter": "Stuart Reeves", "authors": "Stuart Reeves, Jordan Beck", "title": "Talking about interaction*", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijhcs.2019.05.010", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has exposed disagreements over the nature and usefulness of\nwhat may (or may not) be Human-Computer Interaction's fundamental phenomenon:\n'interaction'. For some, HCI's theorising about interaction has been deficient,\nimpacting its capacity to inform decisions in design, suggesting the need\neither to perform first-principles definition work or broader administrative\nclarification and formalisation of the multitude of formulations of the\nconcepts of interaction and their particular uses. For others, there remain\nopen questions over the continued relevance of certain 'versions' of\ninteraction as a useful concept in HCI at all. We pursue a different\nperspective in this paper, reviewing how HCI treats interaction through\nexamining its 'conceptual pragmatics' within HCI's discourse. We argue that\narticulations of the concepts of interaction can be a site of productive\nconflict for HCI that for many reasons may resist attempts of formalisation as\nwell as attempts to dispense with them. The main contribution of this paper is\nin specifying how we might go about talking of interaction and the value of\ninteraction language as promiscuous concepts.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 13:53:21 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 13:45:35 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 15:08:43 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Reeves", "Stuart", ""], ["Beck", "Jordan", ""]]}, {"id": "1903.03700", "submitter": "Marco Cavallo", "authors": "Marco Cavallo, Mishal Dholakia, Matous Havlena, Kenneth Ocheltree,\n  Mark Podlaseck", "title": "Dataspace: A Reconfigurable Hybrid Reality Environment for Collaborative\n  Information Analysis", "comments": "IEEE VR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersive environments have gradually become standard for visualizing and\nanalyzing large or complex datasets that would otherwise be cumbersome, if not\nimpossible, to explore through smaller scale computing devices. However, this\ntype of workspace often proves to possess limitations in terms of interaction,\nflexibility, cost and scalability.\n  In this paper we introduce a novel immersive environment called Dataspace,\nwhich features a new combination of heterogeneous technologies and methods of\ninteraction towards creating a better team workspace. Dataspace provides 15\nhigh-resolution displays that can be dynamically reconfigured in space through\nrobotic arms, a central table where information can be projected, and a unique\nintegration with augmented reality (AR) and virtual reality (VR) headsets and\nother mobile devices. In particular, we contribute novel interaction\nmethodologies to couple the physical environment with AR and VR technologies,\nenabling visualization of complex types of data and mitigating the scalability\nissues of existing immersive environments.\n  We demonstrate through four use cases how this environment can be effectively\nused across different domains and reconfigured based on user requirements.\n  Finally, we compare Dataspace with existing technologies, summarizing the\ntrade-offs that should be considered when attempting to build better\ncollaborative workspaces for the future.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 23:53:10 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Cavallo", "Marco", ""], ["Dholakia", "Mishal", ""], ["Havlena", "Matous", ""], ["Ocheltree", "Kenneth", ""], ["Podlaseck", "Mark", ""]]}, {"id": "1903.03980", "submitter": "Jesse Hoey", "authors": "Moojan Ghafurian and Neil Budnarain and Jesse Hoey", "title": "Improving Humanness of Virtual Agents and Users' Cooperation through\n  Emotions", "comments": null, "journal-ref": null, "doi": "10.1109/TAFFC.2021.3096831", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the performance of an agent developed according to\na well-accepted appraisal theory of human emotion with respect to how it\nmodulates play in the context of a social dilemma. We ask if the agent will be\ncapable of generating interactions that are considered to be more human than\nmachine-like. We conduct an experiment with 117 participants and show how\nparticipants rate our agent on dimensions of human-uniqueness (which separates\nhumans from animals) and human-nature (which separates humans from machines).\nWe show that our appraisal theoretic agent is perceived to be more human-like\nthan baseline models, by significantly improving both human-nature and\nhuman-uniqueness aspects of the intelligent agent. We also show that perception\nof humanness positively affects enjoyment and cooperation in the social\ndilemma.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 12:37:15 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Ghafurian", "Moojan", ""], ["Budnarain", "Neil", ""], ["Hoey", "Jesse", ""]]}, {"id": "1903.04047", "submitter": "Michael Xuelin Huang", "authors": "Michael Xuelin Huang, Andreas Bulling", "title": "SacCalib: Reducing Calibration Distortion for Stationary Eye Trackers\n  Using Saccadic Eye Movements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods to automatically calibrate stationary eye trackers were shown\nto effectively reduce inherent calibration distortion. However, these methods\nrequire additional information, such as mouse clicks or on-screen content. We\npropose the first method that only requires users' eye movements to reduce\ncalibration distortion in the background while users naturally look at an\ninterface. Our method exploits that calibration distortion makes straight\nsaccade trajectories appear curved between the saccadic start and end points.\nWe show that this curving effect is systematic and the result of distorted gaze\nprojection plane. To mitigate calibration distortion, our method undistorts\nthis plane by straightening saccade trajectories using image warping. We show\nthat this approach improves over the common six-point calibration and is\npromising for reducing distortion. As such, it provides a non-intrusive\nsolution to alleviating accuracy decrease of eye tracker during long-term use.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 19:34:50 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Huang", "Michael Xuelin", ""], ["Bulling", "Andreas", ""]]}, {"id": "1903.04084", "submitter": "Yang Zheng", "authors": "Yang Zheng, Izzat H. Izzat, John H.L. Hansen", "title": "Exploring OpenStreetMap Availability for Driving Environment\n  Understanding", "comments": "12 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the great achievement of artificial intelligence, vehicle technologies\nhave advanced significantly from human centric driving towards fully automated\ndriving. An intelligent vehicle should be able to understand the driver's\nperception of the environment as well as controlling behavior of the vehicle.\nSince high digital map information has been available to provide rich\nenvironmental context about static roads, buildings and traffic\ninfrastructures, it would be worthwhile to explore map data capability for\ndriving task understanding. Alternative to commercial used maps, the\nOpenStreetMap (OSM) data is a free open dataset, which makes it unique for the\nexploration research. This study is focused on two tasks that leverage OSM for\ndriving environment understanding. First, driving scenario attributes are\nretrieved from OSM elements, which are combined with vehicle dynamic signals\nfor the driving event recognition. Utilizing steering angle changes and based\non a Bi-directional Recurrent Neural Network (Bi-RNN), a driving sequence is\nsegmented and classified as lane-keeping, lane-change-left, lane-change-right,\nturn-left, and turn-right events. Second, for autonomous driving perception,\nOSM data can be used to render virtual street views, represented as prior\nknowledge to fuse with vision/laser systems for road semantic segmentation.\nFive different types of road masks are generated from OSM, images, and Lidar\npoints, and fused to characterize the drivable space at the driver's\nperspective. An alternative data-driven approach is based on a Fully\nConvolutional Network (FCN), OSM availability for deep learning methods are\ndiscussed to reveal potential usage on compensating street view images and\nautomatic road semantic annotation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 00:43:13 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Zheng", "Yang", ""], ["Izzat", "Izzat H.", ""], ["Hansen", "John H. L.", ""]]}, {"id": "1903.04882", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Adrian Seitan, Costin Petre, Mihai Polceanu,\n  Crenguta M. Bogdan, Dorin M. Popovici", "title": "Haptic User Interfaces and Practice-based Learning for Minimally\n  Invasive Surgical Training", "comments": "ISSN: 1844-8933", "journal-ref": "International Conference on Virtual Learning, 2011, pp. 45-54", "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in haptic hardware and software technology have generated\ninterest in novel, multimodal interfaces based on the sense of touch. Such\ninterfaces have the potential to revolutionize the way we think about human\ncomputer interaction and open new possibilities for simulation and training in\na variety of fields. In this paper we review several frameworks, APIs and\ntoolkits for haptic user interface development. We explore these software\ncomponents focusing on minimally invasive surgical simulation systems. In the\narea of medical diagnosis, there is a strong need to determine mechanical\nproperties of biological tissue for both histological and pathological\nconsiderations. Therefore we focus on the development of affordable\nvisuo-haptic simulators to improve practice-based education in this area. We\nenvision such systems, designed for the next generations of learners that\nenhance their knowledge in connection with real-life situations while they\ntrain in mandatory safety conditions.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 03:57:23 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Seitan", "Adrian", ""], ["Petre", "Costin", ""], ["Polceanu", "Mihai", ""], ["Bogdan", "Crenguta M.", ""], ["Popovici", "Dorin M.", ""]]}, {"id": "1903.05238", "submitter": "Sergiu Oprea", "authors": "Sergiu Oprea, Pablo Martinez-Gonzalez, Alberto Garcia-Garcia, John\n  Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez", "title": "A Visually Plausible Grasping System for Object Manipulation and\n  Interaction in Virtual Reality Environments", "comments": null, "journal-ref": null, "doi": "10.1016/j.cag.2019.07.003", "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interaction in virtual reality (VR) environments is essential to achieve a\npleasant and immersive experience. Most of the currently existing VR\napplications, lack of robust object grasping and manipulation, which are the\ncornerstone of interactive systems. Therefore, we propose a realistic, flexible\nand robust grasping system that enables rich and real-time interactions in\nvirtual environments. It is visually realistic because it is completely\nuser-controlled, flexible because it can be used for different hand\nconfigurations, and robust because it allows the manipulation of objects\nregardless their geometry, i.e. hand is automatically fitted to the object\nshape. In order to validate our proposal, an exhaustive qualitative and\nquantitative performance analysis has been carried out. On the one hand,\nqualitative evaluation was used in the assessment of the abstract aspects such\nas: hand movement realism, interaction realism and motor control. On the other\nhand, for the quantitative evaluation a novel error metric has been proposed to\nvisually analyze the performed grips. This metric is based on the computation\nof the distance from the finger phalanges to the nearest contact point on the\nobject surface. These contact points can be used with different application\npurposes, mainly in the field of robotics. As a conclusion, system evaluation\nreports a similar performance between users with previous experience in virtual\nreality applications and inexperienced users, referring to a steep learning\ncurve.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 22:15:51 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Oprea", "Sergiu", ""], ["Martinez-Gonzalez", "Pablo", ""], ["Garcia-Garcia", "Alberto", ""], ["Castro-Vargas", "John Alejandro", ""], ["Orts-Escolano", "Sergio", ""], ["Garcia-Rodriguez", "Jose", ""]]}, {"id": "1903.05251", "submitter": "Lionel Robert", "authors": "Luke Petersen, Lionel Robert, X. Jessie Yang, and Dawn M. Tilbury", "title": "Situational Awareness, Drivers Trust in Automated Driving Systems and\n  Secondary Task Performance", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver assistance systems, also called automated driving systems, allow\ndrivers to immerse themselves in non-driving-related tasks. Unfortunately,\ndrivers may not trust the automated driving system, which prevents either\nhanding over the driving task or fully focusing on the secondary task. We\nassert that enhancing situational awareness can increase trust in automation.\nSituational awareness should increase trust and lead to better secondary task\nperformance. This study manipulated situational awareness by providing them\nwith different types of information: the control condition provided no\ninformation to the driver, the low condition provided a status update, while\nthe high condition provided a status update and a suggested course of action.\nData collected included measures of trust, trusting behavior, and task\nperformance through surveys, eye-tracking, and heart rate data. Results show\nthat situational awareness both promoted and moderated the impact of trust in\nthe automated vehicle, leading to better secondary task performance. This\nresult was evident in measures of self-reported trust and trusting behavior.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 22:57:29 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Petersen", "Luke", ""], ["Robert", "Lionel", ""], ["Yang", "X. Jessie", ""], ["Tilbury", "Dawn M.", ""]]}, {"id": "1903.05277", "submitter": "Jinghui Cheng", "authors": "Jinghui Cheng, Jin L.C. Guo", "title": "Activity-Based Analysis of Open Source Software Contributors: Roles and\n  Dynamics", "comments": "12th International Workshop on Cooperative and Human Aspects of\n  Software Engineering (CHASE 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contributors to open source software (OSS) communities assume diverse roles\nto take different responsibilities. One major limitation of the current OSS\ntools and platforms is that they provide a uniform user interface regardless of\nthe activities performed by the various types of contributors. This paper\nserves as a non-trivial first step towards resolving this challenge by\ndemonstrating a methodology and establishing knowledge to understand how the\ncontributors' roles and their dynamics, reflected in the activities\ncontributors perform, are exhibited in OSS communities. Based on an analysis of\nuser action data from 29 GitHub projects, we extracted six activities that\ndistinguished four Active roles and five Supporting roles of OSS contributors,\nas well as patterns in role changes. Through the lens of the Activity Theory,\nthese findings provided rich design guidelines for OSS tools to support diverse\ncontributor roles.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 01:24:10 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Cheng", "Jinghui", ""], ["Guo", "Jin L. C.", ""]]}, {"id": "1903.05359", "submitter": "Zhan Yang", "authors": "Jun Long, WuQing Sun, Zhan Yang, Osolo Ian Raymond", "title": "Asymmetric Residual Neural Network for Accurate Human Activity\n  Recognition", "comments": "Accepted by Information", "journal-ref": null, "doi": "10.3390/info10060203", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Activity Recognition (HAR) using deep neural network has become a hot\ntopic in human-computer interaction. Machine can effectively identify human\nnaturalistic activities by learning from a large collection of sensor data.\nActivity recognition is not only an interesting research problem, but also has\nmany real-world practical applications. Based on the success of residual\nnetworks in achieving a high level of aesthetic representation of the automatic\nlearning, we propose a novel \\textbf{A}symmetric \\textbf{R}esidual\n\\textbf{N}etwork, named ARN. ARN is implemented using two identical path\nframeworks consisting of (1) a short time window, which is used to capture\nspatial features, and (2) a long time window, which is used to capture fine\ntemporal features. The long time window path can be made very lightweight by\nreducing its channel capacity, yet still being able to learn useful temporal\nrepresentations for activity recognition. In this paper, we mainly focus on\nproposing a new model to improve the accuracy of HAR. In order to demonstrate\nthe effectiveness of ARN model, we carried out extensive experiments on\nbenchmark datasets (i.e., OPPORTUNITY, UniMiB-SHAR) and compared with some\nconventional and state-of-the-art learning-based methods. Then, we discuss the\ninfluence of networks parameters on performance to provide insights about its\noptimization. Results from our experiments show that ARN is effective in\nrecognizing human activities via wearable datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 08:44:01 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 08:41:42 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 13:05:37 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Long", "Jun", ""], ["Sun", "WuQing", ""], ["Yang", "Zhan", ""], ["Raymond", "Osolo Ian", ""]]}, {"id": "1903.05448", "submitter": "Martin Guay", "authors": "Dominik Borer, Dominik Lutz, Martin Guay", "title": "Animating an Autonomous 3D Talking Avatar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges with embodying a conversational agent is\nannotating how and when motions can be played and composed together in\nreal-time, without any visual artifact. The inherent problem is to do so---for\na large amount of motions---without introducing mistakes in the annotation. To\nour knowledge, there is no automatic method that can process animations and\nautomatically label actions and compatibility between them. In practice, a\nstate machine, where clips are the actions, is created manually by setting\nconnections between the states with the timing parameters for these\nconnections. Authoring this state machine for a large amount of motions leads\nto a visual overflow, and increases the amount of possible mistakes. In\nconsequence, conversational agent embodiments are left with little variations\nand quickly become repetitive. In this paper, we address this problem with a\ncompact taxonomy of chit chat behaviors, that we can utilize to simplify and\npartially automate the graph authoring process. We measured the time required\nto label actions of an embodiment using our simple interface, compared to the\nstandard state machine interface in Unreal Engine, and found that our approach\nis 7 times faster. We believe that our labeling approach could be a path to\nautomated labeling: once a sub-set of motions are labeled (using our\ninterface), we could learn a prediction that could attribute a label to new\nclips---allowing to really scale up virtual agent embodiments.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 12:25:34 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Borer", "Dominik", ""], ["Lutz", "Dominik", ""], ["Guay", "Martin", ""]]}, {"id": "1903.05636", "submitter": "Negin Manshouri", "authors": "Negin Manshouri, Temel Kayikcioglu", "title": "A Comprehensive Analysis of 2D&3D Video Watching of EEG Signals by\n  Increasing PLSR and SVM Classification Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the development of two and three dimensional (2D&3D) technology, it\nhas attracted the attention of researchers in recent years. This research is\ndone to reveal the detailed effects of 2D in comparison with 3D technology on\nthe human brain waves. The impact of 2D&3D video watching using\nelectroencephalography (EEG) brain signals is studied. A group of eight healthy\nvolunteers with the average age of 31+-3.06 years old participated in this\nthree-stage test. EEG signal recording consisted of three stages: After a bit\nof relaxation (a), a 2D video was displayed (b), the recording of the signal\ncontinued for a short period of time as rest (c), and finally the trial ended.\nExactly the same steps were repeated for the 3D video. Power spectrum density\n(PSD) based on short time Fourier transform (STFT) was used to analyze the\nbrain signals of 2D&3D video viewers. After testing all the EEG frequency\nbands, delta and theta were extracted as the features. Partial least squares\nregression (PLSR) and Support vector machine (SVM) classification algorithms\nwere considered in order to classify EEG signals obtained as the result of\n2D&3D video watching. Successful classification results were obtained by\nselecting the correct combinations of effective channels representing the brain\nregions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 09:57:47 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Manshouri", "Negin", ""], ["Kayikcioglu", "Temel", ""]]}, {"id": "1903.05757", "submitter": "Xiaofeng Gao", "authors": "Xiaofeng Gao, Ran Gong, Tianmin Shu, Xu Xie, Shu Wang, Song-Chun Zhu", "title": "VRKitchen: an Interactive 3D Virtual Environment for Task-oriented\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges of advancing task-oriented learning such as visual\ntask planning and reinforcement learning is the lack of realistic and\nstandardized environments for training and testing AI agents. Previously,\nresearchers often relied on ad-hoc lab environments. There have been recent\nadvances in virtual systems built with 3D physics engines and photo-realistic\nrendering for indoor and outdoor environments, but the embodied agents in those\nsystems can only conduct simple interactions with the world (e.g., walking\naround, moving objects, etc.). Most of the existing systems also do not allow\nhuman participation in their simulated environments. In this work, we design\nand implement a virtual reality (VR) system, VRKitchen, with integrated\nfunctions which i) enable embodied agents powered by modern AI methods (e.g.,\nplanning, reinforcement learning, etc.) to perform complex tasks involving a\nwide range of fine-grained object manipulations in a realistic environment, and\nii) allow human teachers to perform demonstrations to train agents (i.e.,\nlearning from demonstration). We also provide standardized evaluation\nbenchmarks and data collection tools to facilitate a broad use in research on\ntask-oriented learning and beyond.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 23:31:21 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Gao", "Xiaofeng", ""], ["Gong", "Ran", ""], ["Shu", "Tianmin", ""], ["Xie", "Xu", ""], ["Wang", "Shu", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1903.05875", "submitter": "Sebastian Stefan Feger", "authors": "Sebastian S. Feger, S\\\"unje Dallmeier-Tiessen, Albrecht Schmidt,\n  Pawe{\\l} W. Wo\\'zniak", "title": "Designing for Reproducibility: A Qualitative Study of Challenges and\n  Opportunities in High Energy Physics", "comments": "Accepted for publication at the 2019 Conference on Human Factors in\n  Computing Systems (CHI 2019)", "journal-ref": null, "doi": "10.1145/3290605.3300685", "report-no": null, "categories": "cs.HC hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reproducibility should be a cornerstone of scientific research and is a\ngrowing concern among the scientific community and the public. Understanding\nhow to design services and tools that support documentation, preservation and\nsharing is required to maximize the positive impact of scientific research. We\nconducted a study of user attitudes towards systems that support data\npreservation in High Energy Physics, one of science's most data-intensive\nbranches. We report on our interview study with 12 experimental physicists,\nstudying requirements and opportunities in designing for research preservation\nand reproducibility. Our findings suggest that we need to design for motivation\nand benefits in order to stimulate contributions and to address the observed\nscalability challenge. Therefore, researchers' attitudes towards communication,\nuncertainty, collaboration and automation need to be reflected in design. Based\non our findings, we present a systematic view of user needs and constraints\nthat define the design space of systems supporting reproducible practices.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 09:48:17 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Feger", "Sebastian S.", ""], ["Dallmeier-Tiessen", "S\u00fcnje", ""], ["Schmidt", "Albrecht", ""], ["Wo\u017aniak", "Pawe\u0142 W.", ""]]}, {"id": "1903.05941", "submitter": "Jan Claes", "authors": "Sarah Pissierssens, Jan Claes, Geert Poels", "title": "The \"Physics of Diagrams\": Revealing the scientific basis of graphical\n  representation design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GL cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data is omnipresent in the modern, digital world and a significant number of\npeople need to make sense of data as part of their everyday social and\nprofessional life. Therefore, together with the rise of data, the design of\ngraphical representations has gained importance and attention. Yet, although a\nlarge body of procedural knowledge about effective visualization exists, the\nquality of representations is often reported to be poor, proposedly because\nthese guidelines are scattered, unstructured and sometimes perceived as\ncontradictive. Therefore, this paper describes a literature research addressing\nthese problems. The research resulted in the collection and structuring of 81\nguidelines and 34 underlying propositions, as well as in the derivation of 7\nfoundational principles about graphical representation design, called the\n\"Physics of Diagrams\", which are illustrated with concrete, practical examples\nthroughout the paper.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 12:33:22 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Pissierssens", "Sarah", ""], ["Claes", "Jan", ""], ["Poels", "Geert", ""]]}, {"id": "1903.06047", "submitter": "Matthew Gombolay", "authors": "Rohan Paleja and Matthew Gombolay", "title": "Inferring Personalized Bayesian Embeddings for Learning from\n  Heterogeneous Demonstration", "comments": "8 Pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For assistive robots and virtual agents to achieve ubiquity, machines will\nneed to anticipate the needs of their human counterparts. The field of Learning\nfrom Demonstration (LfD) has sought to enable machines to infer predictive\nmodels of human behavior for autonomous robot control. However, humans exhibit\nheterogeneity in decision-making, which traditional LfD approaches fail to\ncapture. To overcome this challenge, we propose a Bayesian LfD framework to\ninfer an integrated representation of all human task demonstrators by inferring\nhuman-specific embeddings, thereby distilling their unique characteristics. We\nvalidate our approach is able to outperform state-of-the-art techniques on both\nsynthetic and real-world data sets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 14:32:55 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Paleja", "Rohan", ""], ["Gombolay", "Matthew", ""]]}, {"id": "1903.06274", "submitter": "Thomais Asvestopoulou", "authors": "Thomais Asvestopoulou, Victoria Manousaki, Antonis Psistakis, Ioannis\n  Smyrnakis, Vassilios Andreadakis, Ioannis M. Aslanides, Maria Papadopouli", "title": "DysLexML: Screening Tool for Dyslexia Using Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movements during text reading can provide insights about reading\ndisorders. Via eye-trackers, we can measure when, where and how eyes move with\nrelation to the words they read. Machine Learning (ML) algorithms can decode\nthis information and provide differential analysis. This work developed\nDysLexML, a screening tool for developmental dyslexia that applies various ML\nalgorithms to analyze fixation points recorded via eye-tracking during silent\nreading of children. It comparatively evaluated its performance using\nmeasurements collected in a systematic field study with 69 native Greek\nspeakers, children, 32 of which were diagnosed as dyslexic by the official\ngovernmental agency for diagnosing learning and reading difficulties in Greece.\nWe examined a large set of features based on statistical properties of\nfixations and saccadic movements and identified the ones with prominent\npredictive power, performing dimensionality reduction. Specifically, DysLexML\nachieves its best performance using linear SVM, with an a accuracy of 97 %,\nwith a small feature set, namely saccade length, number of short forward\nmovements, and number of multiply fixated words. Furthermore, we analyzed the\nimpact of noise on the fixation positions and showed that DysLexML is accurate\nand robust in the presence of noise. These encouraging results set the basis\nfor developing screening tools in less controlled, larger-scale environments,\nwith inexpensive eye-trackers, potentially reaching a larger population for\nearly intervention.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 21:44:52 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Asvestopoulou", "Thomais", ""], ["Manousaki", "Victoria", ""], ["Psistakis", "Antonis", ""], ["Smyrnakis", "Ioannis", ""], ["Andreadakis", "Vassilios", ""], ["Aslanides", "Ioannis M.", ""], ["Papadopouli", "Maria", ""]]}, {"id": "1903.06319", "submitter": "Yanmei Dong", "authors": "Yanmei Dong, Mingtao Pei, Lijia Zhang, Bin Xu, Yuwei Wu, and Yunde Jia", "title": "Stitching Videos from a Fisheye Lens Camera and a Wide-Angle Lens Camera\n  for Telepresence Robots", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many telepresence robots are equipped with a forward-facing camera for video\ncommunication and a downward-facing camera for navigation. In this paper, we\npropose to stitch videos from the FF-camera with a wide-angle lens and the\nDF-camera with a fisheye lens for telepresence robots. We aim at providing more\ncompact and efficient visual feedback for the user interface of telepresence\nrobots with user-friendly interactive experiences. To this end, we present a\nmulti-homography-based video stitching method which stitches videos from a\nwide-angle camera and a fisheye camera. The method consists of video image\nalignment, seam cutting, and image blending. We directly align the wide-angle\nvideo image and the fisheye video image based on the multi-homography alignment\nwithout calibration, distortion correction, and unwarping procedures. Thus, we\ncan obtain a stitched video with shape preservation in the non-overlapping\nregions and alignment in the overlapping area for telepresence. To alleviate\nghosting effects caused by moving objects and/or moving cameras during\ntelepresence robot driving, an optimal seam is found for aligned video\ncomposition, and the optimal seam will be updated in subsequent frames,\nconsidering spatial and temporal coherence. The final stitched video is created\nby image blending based on the optimal seam. We conducted a user study to\ndemonstrate the effectiveness of our method and the superiority of telepresence\nrobots with a stitched video as visual feedback.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 01:51:05 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Dong", "Yanmei", ""], ["Pei", "Mingtao", ""], ["Zhang", "Lijia", ""], ["Xu", "Bin", ""], ["Wu", "Yuwei", ""], ["Jia", "Yunde", ""]]}, {"id": "1903.06472", "submitter": "Lihi Dery", "authors": "Lihi Dery, Tamir Tassa, and Avishay Yanai", "title": "Fear Not, Vote Truthfully: Secure Multiparty Computation of Score Based\n  Rules", "comments": null, "journal-ref": "Expert Systems with Applications, 168, 114434 (2021)", "doi": "10.1016/j.eswa.2020.114434", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a secure voting protocol for score-based voting rules, where\nindependent talliers perform the tallying procedure. The protocol outputs the\nwinning candidate(s) while preserving the privacy of the voters and the secrecy\nof the ballots. It offers perfect secrecy, in the sense that apart from the\ndesired output, all other information -- the ballots, intermediate values, and\nthe final scores received by each of the candidates -- is not disclosed to any\nparty, including the talliers. Such perfect secrecy may increase the voters'\nconfidence and, consequently, encourage them to vote according to their true\npreferences. The protocol is extremely lightweight, and therefore it can be\neasily deployed in real-life voting scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 11:36:27 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 19:01:36 GMT"}, {"version": "v3", "created": "Sun, 26 Jan 2020 10:11:46 GMT"}, {"version": "v4", "created": "Fri, 11 Dec 2020 16:37:12 GMT"}, {"version": "v5", "created": "Sun, 24 Jan 2021 09:32:12 GMT"}, {"version": "v6", "created": "Sun, 31 Jan 2021 15:13:59 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dery", "Lihi", ""], ["Tassa", "Tamir", ""], ["Yanai", "Avishay", ""]]}, {"id": "1903.06474", "submitter": "Ioannis Agtzidis", "authors": "Ioannis Agtzidis, Mikhail Startsev, Michael Dorr", "title": "A Ground-Truth Data Set and a Classification Algorithm for Eye Movements\n  in 360-degree Videos", "comments": null, "journal-ref": "Proceedings of the 27th ACM International Conference on\n  Multimedia, (2019), p. 1007-1015", "doi": "10.1145/3343031.3350947", "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of a gaze trace into its constituent eye movements has been\nactively researched since the early days of eye tracking. As we move towards\nmore naturalistic viewing conditions, the segmentation becomes even more\nchallenging and convoluted as more complex patterns emerge. The definitions and\nthe well-established methods that were developed for monitor-based eye tracking\nexperiments are often not directly applicable to unrestrained set-ups such as\neye tracking in wearable contexts or with head-mounted displays. The main\ncontributions of this work to the eye movement research for 360-degree content\nare threefold: First, we collect, partially annotate, and make publicly\navailable a new eye tracking data set, which consists of 13 participants\nviewing 15 video clips that are recorded in 360-degree. Second, we propose a\nnew two-stage pipeline for ground truth annotation of the traditional\nfixations, saccades, smooth pursuits, as well as (optokinetic) nystagmus,\nvestibulo-ocular reflex, and pursuit of moving objects performed exclusively\nvia the movement of the head. A flexible user interface for this pipeline is\nimplemented and made freely accessible for use or modification. Lastly, we\ndevelop and test a simple proof-of-concept algorithm for automatic\nclassification of all the eye movement types in our data set based on their\noperational definitions that were used for manual annotation. The data set and\nthe source code for both the annotation tool and the algorithm are publicly\navailable at https://web.gin.g-node.org/ioannis.agtzidis/360_em_dataset.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 11:46:08 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Agtzidis", "Ioannis", ""], ["Startsev", "Mikhail", ""], ["Dorr", "Michael", ""]]}, {"id": "1903.06643", "submitter": "Maria Papadopouli", "authors": "Vasileios Sideridis, Andrew Zacharakis, George Tzagkarakis, Maria\n  Papadopouli", "title": "GestureKeeper: Gesture Recognition for Controlling Devices in IoT\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and evaluates the GestureKeeper, a robust hand-gesture\nrecognition system based on a wearable inertial measurements unit (IMU). The\nidentification of the time windows where the gestures occur, without relying on\nan explicit user action or a special gesture marker, is a very challenging\ntask. To address this problem, GestureKeeper identifies the start of a gesture\nby exploiting the underlying dynamics of the associated time series using a\nrecurrence quantification analysis (RQA). RQA is a powerful method for\nnonlinear time-series analysis, which enables the detection of critical\ntransitions in the system's dynamical behavior. Most importantly, it does not\nmake any assumption about the underlying distribution or model that governs the\ndata. Having estimated the gesture window, a support vector machine is employed\nto recognize the specific gesture. Our proposed method is evaluated by means of\na small-scale pilot study at FORTH and demonstrated that GestureKeeper can\nidentify correctly the start of a gesture with a 87\\% mean balanced accuracy\nand classify correctly the specific hand-gesture with a mean accuracy of over\n96\\%. To the best of our knowledge, GestureKeeper is the first automatic\nhand-gesture identification system based only on accelerometer. The performance\nanalysis reveals the predictive power of the features and the system's\nrobustness in the presence of additive noise. We also performed a sensitivity\nanalysis to examine the impact of various parameters and a comparative analysis\nof different classifiers (SVM, random forests). Most importantly, the system\ncan be extended to incorporate a large dictionary of gestures and operate\nwithout further calibration for a new user.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 16:30:48 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Sideridis", "Vasileios", ""], ["Zacharakis", "Andrew", ""], ["Tzagkarakis", "George", ""], ["Papadopouli", "Maria", ""]]}, {"id": "1903.06656", "submitter": "Felix Hamza-Lup", "authors": "Ivan Sopin, Felix G. Hamza-Lup", "title": "Extending the Web3D: Design of Conventional GUI Libraries in X3D", "comments": null, "journal-ref": "3D Web Technology, 2010, pp. 137-146", "doi": "10.1145/1836049.1836070", "report-no": null, "categories": "cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensible 3D (X3D) modeling language is one of the leading Web3D\ntechnologies. Despite the rich functionality, the language does not currently\nprovide tools for rapid development of conventional graphical user interfaces\n(GUIs). Every X3D author is responsible for building from primitives a purpose\nspecific set of required interface components, often for a single use. We\naddress the challenge of creating consistent, efficient, interactive, and\nvisually appealing GUIs by proposing the X3D User Interface (X3DUI) library.\nThis library includes a wide range of cross-compatible X3D widgets, equipped\nwith configurable appearance and behavior. With this library, we attempt to\nstandardize the GUI construction across various X3D-driven projects, and\nimprove the usability, compatibility, adaptability, readability, and\nflexibility of many existing applications.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 04:19:06 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Sopin", "Ivan", ""], ["Hamza-Lup", "Felix G.", ""]]}, {"id": "1903.06657", "submitter": "Christos Mousas", "authors": "Christos Mousas, Alexandros Koilias, Dimitris Anastasiou, Banafsheh\n  Rekabdar, Christos-Nikolaos Anagnostopoulos", "title": "Effects of Self-Avatar and Gaze on Avoidance Movement Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study investigates users' movement behavior in a virtual\nenvironment when they attempted to avoid a virtual character. At each iteration\nof the experiment, four conditions (Self-Avatar LookAt, No Self-Avatar LookAt,\nSelf-Avatar No LookAt, and No Self-Avatar No LookAt) were applied to examine\nusers' movement behavior based on kinematic measures. During the experiment, 52\nparticipants were asked to walk from a starting position to a target position.\nA virtual character was placed at the midpoint. Participants were asked to wear\na head-mounted display throughout the task, and their locomotion was captured\nusing a motion capture suit. We analyzed the captured trajectories of the\nparticipants' routes on four kinematic measures to explore whether the four\nexperimental conditions influenced the paths they took. The results indicated\nthat the Self-Avatar LookAt condition affected the path the participants chose\nmore significantly than the other three conditions in terms of length,\nduration, and deviation, but not in terms of speed. Overall, the length and\nduration of the task, as well as the deviation of the trajectory from the\nstraight line, were greater when a self-avatar represented participants. An\nadditional effect on kinematic measures was found in the LookAt (Gaze)\nconditions. Implications for future research are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 12:42:47 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Mousas", "Christos", ""], ["Koilias", "Alexandros", ""], ["Anastasiou", "Dimitris", ""], ["Rekabdar", "Banafsheh", ""], ["Anagnostopoulos", "Christos-Nikolaos", ""]]}, {"id": "1903.06847", "submitter": "Matthew Gombolay", "authors": "Esmaeil Seraj and Andrew Silva and Matthew Gombolay", "title": "Safe Coordination of Human-Robot Firefighting Teams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wildfires are destructive and inflict massive, irreversible harm to victims'\nlives and natural resources. Researchers have proposed commissioning unmanned\naerial vehicles (UAVs) to provide firefighters with real-time tracking\ninformation; yet, these UAVs are not able to reason about a fire's track,\nincluding current location, measurement, and uncertainty, as well as\npropagation. We propose a model-predictive, probabilistically safe distributed\ncontrol algorithm for human-robot collaboration in wildfire fighting. The\nproposed algorithm overcomes the limitations of prior work by explicitly\nestimating the latent fire propagation dynamics to enable intelligent,\ntime-extended coordination of the UAVs in support of on-the-ground human\nfirefighters. We derive a novel, analytical bound that enables UAVs to\ndistribute their resources and provides a probabilistic guarantee of the\nhumans' safety while preserving the UAVs' ability to cover an entire fire.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 00:14:34 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Seraj", "Esmaeil", ""], ["Silva", "Andrew", ""], ["Gombolay", "Matthew", ""]]}, {"id": "1903.07032", "submitter": "Susumu Saito", "authors": "Susumu Saito, Chun-Wei Chiang, Saiph Savage, Teppei Nakano, Tetsunori\n  Kobayashi and Jeffrey Bigham", "title": "TurkScanner: Predicting the Hourly Wage of Microtasks", "comments": "Proceedings of the 28th International Conference on World Wide Web\n  (WWW '19), San Francisco, CA, USA, May 13-17, 2019", "journal-ref": null, "doi": "10.1145/3308558.3313716", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workers in crowd markets struggle to earn a living. One reason for this is\nthat it is difficult for workers to accurately gauge the hourly wages of\nmicrotasks, and they consequently end up performing labor with little pay. In\ngeneral, workers are provided with little information about tasks, and are left\nto rely on noisy signals, such as textual description of the task or rating of\nthe requester. This study explores various computational methods for predicting\nthe working times (and thus hourly wages) required for tasks based on data\ncollected from other workers completing crowd work. We provide the following\ncontributions. (i) A data collection method for gathering real-world training\ndata on crowd-work tasks and the times required for workers to complete them;\n(ii) TurkScanner: a machine learning approach that predicts the necessary\nworking time to complete a task (and can thus implicitly provide the expected\nhourly wage). We collected 9,155 data records using a web browser extension\ninstalled by 84 Amazon Mechanical Turk workers, and explored the challenge of\naccurately recording working times both automatically and by asking workers.\nTurkScanner was created using ~150 derived features, and was able to predict\nthe hourly wages of 69.6% of all the tested microtasks within a 75% error.\nDirections for future research include observing the effects of tools on\npeople's working practices, adapting this approach to a requester tool for\nbetter price setting, and predicting other elements of work (e.g., the\nacceptance likelihood and worker task preferences.)\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 06:07:11 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Saito", "Susumu", ""], ["Chiang", "Chun-Wei", ""], ["Savage", "Saiph", ""], ["Nakano", "Teppei", ""], ["Kobayashi", "Tetsunori", ""], ["Bigham", "Jeffrey", ""]]}, {"id": "1903.07033", "submitter": "Rajan Vaish", "authors": "Hana Habib, Neil Shah, Rajan Vaish", "title": "Impact of Contextual Factors on Snapchat Public Sharing", "comments": "In Proceedings of the 37th Annual ACM Conference on Human Factors in\n  Computing Systems (CHI 2019). ACM, New York, NY, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Public sharing is integral to online platforms. This includes the popular\nmultimedia messaging application Snapchat, on which public sharing is\nrelatively new and unexplored in prior research. In mobile-first applications,\nsharing contexts are dynamic. However, it is unclear how context impacts users'\nsharing decisions. As platforms increasingly rely on user-generated content, it\nis important to also broadly understand user motivations and considerations in\npublic sharing. We explored these aspects of content sharing through a survey\nof 1,515 Snapchat users. Our results indicate that users primarily have\nintrinsic motivations for publicly sharing Snaps, such as to share an\nexperience with the world, but also have considerations related to audience and\nsensitivity of content. Additionally, we found that Snaps shared publicly were\ncontextually different from those privately shared. Our findings suggest that\ncontent sharing systems can be designed to support sharing motivations, yet\nalso be sensitive to private contexts.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 06:27:25 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Habib", "Hana", ""], ["Shah", "Neil", ""], ["Vaish", "Rajan", ""]]}, {"id": "1903.07110", "submitter": "Hanif Baharin", "authors": "Hanif Baharin, Norhayati Yusof, Suzilah Ismail", "title": "Inducing Mimicry Through Auditory Icons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to find out if periodic auditory icon loop and non-periodic\nauditory icon loop can induce mimicry in humans. Auditory icons are snippet of\neveryday sounds used to represent information or processes. A within-subject,\nOz-of-Wizard experiment was conducted among forty participants. The\nparticipants were asked to eat an apple while being exposed to different types\nof auditory icon loop. The loops were made using an auditory icon that plays\nthe sound of crunchy apple bite. Both male and female participants were exposed\nto periodic auditory icon loop, with the auditory icon played every 10 second.\nParticipants were also exposed to non-periodic auditory icon loop which uses\nthe same auditory icon but was made to represent the eating behaviour of a real\nperson of the same sex. The results show that only male participants mimicked\nthe male non-periodic auditory icon loop. Although female participants mimicked\nthe female auditory icon loop, the result is not significant. Both male and\nfemale did not mimic the periodic auditory icon loop. Thus, only auditory icons\nthat represent normal biting pace can induce mimicry, significantly in male\nparticipants. The findings from this study has implications on the design of\npersuasive technology that uses auditory icons to encourage behavioural change.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 15:37:20 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Baharin", "Hanif", ""], ["Yusof", "Norhayati", ""], ["Ismail", "Suzilah", ""]]}, {"id": "1903.07136", "submitter": "Alireza Karduni", "authors": "Alireza Karduni", "title": "Human-Misinformation interaction: Understanding the interdisciplinary\n  approach needed to computationally combat false information", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prevalence of new technologies and social media has amplified the effects\nof misinformation on our societies. Thus, it is necessary to create\ncomputational tools to mitigate their effects effectively. This study aims to\nprovide a critical overview of computational approaches concerned with\ncombating misinformation. To this aim, I offer an overview of scholarly\ndefinitions of misinformation. I adopt a framework for studying misinformation\nthat suggests paying attention to the source, content, and consumers as the\nthree main elements involved in the process of misinformation and I provide an\noverview of literature from disciplines of psychology, media studies, and\ncognitive sciences that deal with each of these elements. Using the framework,\nI overview the existing computational methods that deal with 1) misinformation\ndetection and fact-checking using Content 2) Identifying untrustworthy Sources\nand social bots, and 3) Consumer-facing tools and methods aiming to make humans\nresilient to misinformation. I find that the vast majority of works in computer\nscience and information technology is concerned with the crucial tasks of\ndetection and verification of content and sources of misinformation. Moreover,\nI find that computational research focusing on Consumers of Misinformation in\nHuman-Computer Interaction (HCI) and related fields are very sparse and often\ndo not deal with the subtleties of this process. The majority of existing\ninterfaces and systems are less concerned with the usability of the tools\nrather than the robustness and accuracy of the detection methods. Using this\nsurvey, I call for an interdisciplinary approach towards human-misinformation\ninteraction that focuses on building methods and tools that robustly deal with\nsuch complex psychological/social phenomena.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 17:37:39 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Karduni", "Alireza", ""]]}, {"id": "1903.07157", "submitter": "Jiaxiao Zheng", "authors": "Jiaxiao Zheng and Gustavo de Veciana", "title": "Modeling and Optimization of Human-machine Interaction Processes via the\n  Maximum Entropy Principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven framework to enable the modeling and optimization of\nhuman-machine interaction processes, e.g., systems aimed at assisting humans in\ndecision-making or learning, work-load allocation, and interactive advertising.\nThis is a challenging problem for several reasons. First, humans' behavior is\nhard to model or infer, as it may reflect biases, long term memory, and\nsensitivity to sequencing, i.e., transience and exponential complexity in the\nlength of the interaction. Second, due to the interactive nature of such\nprocesses, the machine policy used to engage with a human may bias possible\ndata-driven inferences. Finally, in choosing machine policies that optimize\ninteraction rewards, one must, on the one hand, avoid being overly sensitive to\nerror/variability in the estimated human model, and on the other, being overly\ndeterministic/predictable which may result in poor human 'engagement' in the\ninteraction. To meet these challenges, we propose a robust approach, based on\nthe maximum entropy principle, which iteratively estimates human behavior and\noptimizes the machine policy--Alternating Entropy-Reward Ascent (AREA)\nalgorithm. We characterize AREA, in terms of its space and time complexity and\nconvergence. We also provide an initial validation based on synthetic data\ngenerated by an established noisy nonlinear model for human decision-making.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 20:14:04 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Zheng", "Jiaxiao", ""], ["de Veciana", "Gustavo", ""]]}, {"id": "1903.07195", "submitter": "Wieslaw Kopec", "authors": "Jaros{\\l}aw Kowalski, Anna Jaskulska, Kinga Skorupska, Katarzyna\n  Abramczuk, Cezary Biele, Wies{\\l}aw Kope\\'c, Krzysztof Marasek", "title": "Older Adults and Voice Interaction: A Pilot Study with Google Home", "comments": null, "journal-ref": null, "doi": "10.1145/3290607.3312973", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the results of an exploratory study examining the\npotential of voice assistants (VA) for some groups of older adults in the\ncontext of Smart Home Technology (SHT). To research the aspect of older adults'\ninteraction with voice user interfaces (VUI) we organized two workshops and\ngathered insights concerning possible benefits and barriers to the use of VA\ncombined with SHT by older adults. Apart from evaluating the participants'\ninteraction with the devices during the two workshops we also discuss some\nimprovements to the VA interaction paradigm.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 23:12:45 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Kowalski", "Jaros\u0142aw", ""], ["Jaskulska", "Anna", ""], ["Skorupska", "Kinga", ""], ["Abramczuk", "Katarzyna", ""], ["Biele", "Cezary", ""], ["Kope\u0107", "Wies\u0142aw", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1903.07272", "submitter": "Omid Bazgir", "authors": "Omid Bazgir, Zeynab Mohammadi, Seyed Amir Hassan Habibi", "title": "Emotion Recognition with Machine Learning Using EEG Signals", "comments": null, "journal-ref": "2018 25th National and 3rd International Iranian Conference on\n  Biomedical Engineering (ICBME)(pp. 1-5). IEEE", "doi": "10.1109/ICBME.2018.8703559", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, an emotion recognition system is developed based on\nvalence/arousal model using electroencephalography (EEG) signals. EEG signals\nare decomposed into the gamma, beta, alpha and theta frequency bands using\ndiscrete wavelet transform (DWT), and spectral features are extracted from each\nfrequency band. Principle component analysis (PCA) is applied to the extracted\nfeatures by preserving the same dimensionality, as a transform, to make the\nfeatures mutually uncorrelated. Support vector machine (SVM), K-nearest\nneighbor (KNN) and artificial neural network (ANN) are used to classify\nemotional states. The cross-validated SVM with radial basis function (RBF)\nkernel using extracted features of 10 EEG channels, performs with 91.3%\naccuracy for arousal and 91.1% accuracy for valence, both in the beta frequency\nband. Our approach shows better performance compared to existing algorithms\napplied to the \"DEAP\" dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 06:49:05 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 04:22:53 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Bazgir", "Omid", ""], ["Mohammadi", "Zeynab", ""], ["Habibi", "Seyed Amir Hassan", ""]]}, {"id": "1903.07381", "submitter": "Lucas Gren", "authors": "Lucas Gren, Per Lenberg and Karolina Ljungberg", "title": "What software engineering can learn from research on affect in social\n  psychology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social psychology researchers have, traditionally, focused on the construct\nof thinking rather than on feeling. Since the beginning of the 21st century,\nsocial science researchers have, however, increasingly explored the effects of\naffect. Their work has repeatedly recognized that affects play a crucial role\nin determining people's behavior. In this short paper, we argue that software\nengineering studies on affect would benefit from using more of the knowledge\nthat social science researchers have acquired. Without accounting for their\nfindings, we risk re-inventing the wheel. Also, without a profound\nunderstanding of the complex interplay between social context and affect, we\nrisk creating overly simplistic solutions that might have considerable\nlong-term adverse effects for software engineers.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 12:13:39 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Gren", "Lucas", ""], ["Lenberg", "Per", ""], ["Ljungberg", "Karolina", ""]]}, {"id": "1903.07766", "submitter": "Devi Parikh", "authors": "X. Alice Li and Devi Parikh", "title": "Lemotif: An Affective Visual Journal Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Lemotif, an integrated natural language processing and image\ngeneration system that uses machine learning to (1) parse a text-based input\njournal entry describing the user's day for salient themes and emotions and (2)\nvisualize the detected themes and emotions in creative and appealing image\nmotifs. Synthesizing approaches from artificial intelligence and psychology,\nLemotif acts as an affective visual journal, encouraging users to regularly\nwrite and reflect on their daily experiences through visual reinforcement. By\nmaking patterns in emotions and their sources more apparent, Lemotif aims to\nhelp users better understand their emotional lives, identify opportunities for\naction, and track the effectiveness of behavioral changes over time. We verify\nvia human studies that prospective users prefer motifs generated by Lemotif\nover corresponding baselines, find the motifs representative of their journal\nentries, and think they would be more likely to journal regularly using a\nLemotif-based app.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 23:35:31 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 16:21:41 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 16:48:35 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Li", "X. Alice", ""], ["Parikh", "Devi", ""]]}, {"id": "1903.08454", "submitter": "Lynsay Shepherd", "authors": "Sam Scholefield, Lynsay A. Shepherd", "title": "Gamification Techniques for Raising Cyber Security Awareness", "comments": "14 pages. Human-Computer International 2019, HCII 2019, Orlando,\n  United States (2019), Springer", "journal-ref": null, "doi": "10.1007/978-3-030-22351-9_13", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the prevalence of online services in modern society, such as internet\nbanking and social media, it is important for users to have an understanding of\nbasic security measures in order to keep themselves safe online. However, users\noften do not know how to make their online interactions secure, which\ndemonstrates an educational need in this area. Gamification has grown in\npopularity in recent years and has been used to teach people about a range of\nsubjects. This paper presents an exploratory study investigating the use of\ngamification techniques to educate average users about password security, with\nthe aim of raising overall security awareness. To explore the impact of such\ntechniques, a role-playing quiz application (RPG) was developed for the Android\nplatform to educate users about password security. Results gained from the work\nhighlighted that users enjoyed learning via the use of the password\napplication, and felt they benefitted from the inclusion of gamification\ntechniques. Future work seeks to expand the prototype into a full solution,\ncovering a range of security awareness issues.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 11:45:26 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 00:57:45 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Scholefield", "Sam", ""], ["Shepherd", "Lynsay A.", ""]]}, {"id": "1903.08524", "submitter": "Monica Perusquia-Hernandez", "authors": "Monica Perusqu\\'ia-Hern\\'andez, David Antonio G\\'omez J\\'auregui,\n  Marisabel Cuberos-Balda, Diego Paez-Granados", "title": "Robot mirroring: A framework for self-tracking feedback through empathy\n  with an artificial agent representing the self", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current technologies have enabled us to track and quantify our physical state\nand behavior. Self-tracking aims to achieve increased awareness to decrease\nundesired behaviors and lead to a healthier lifestyle. However, inappropriately\ncommunicated self-tracking results might cause the opposite effect. In this\nwork, we propose a subtle self-tracking feedback by mirroring the self's state\ninto an artificial agent. By eliciting empathy towards the artificial agent and\nfostering helping behaviors, users would help themselves as well. Finally, we\nreflected on the implications of this design framework, and the methodology to\ndesign and implement it. A series of interviews to expert designers pointed out\nto the importance of having multidisciplinary teams working in parallel.\nMoreover, an agile methodology with a sprint zero for the initial design, and\nshifted user research, design, and implementation sprints were proposed.\nSimilar systems with data flow and hardware dependencies would also benefit\nfrom the proposed agile design process.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 14:42:53 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Perusqu\u00eda-Hern\u00e1ndez", "Monica", ""], ["J\u00e1uregui", "David Antonio G\u00f3mez", ""], ["Cuberos-Balda", "Marisabel", ""], ["Paez-Granados", "Diego", ""]]}, {"id": "1903.08592", "submitter": "Yoshinori Umetsu", "authors": "Yoshinori Umetsu, Yugo Nakamura, Yutaka Arakawa, Manato Fujimoto,\n  Hirohiko Suwa", "title": "EHAAS: Energy Harvesters As A Sensor for Place Recognition on Wearables", "comments": "10 pages, 11 figures, accepted for publication in The International\n  Conference on Pervasive Computing and Communications (PerCom) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wearable based long-term lifelogging system is desirable for the purpose of\nreviewing and improving users' lifestyle habits. Energy harvesting (EH) is a\npromising means for realizing sustainable lifelogging. However, present EH\ntechnologies suffer from instability of the generated electricity caused by\nchanges of environment, e.g., the output of a solar cell varies based on its\nmaterial, light intensity, and light wavelength. In this paper, we leverage\nthis instability of EH technologies for other purposes, in addition to its use\nas an energy source. Specifically, we propose to determine the variation of\ngenerated electricity as a sensor for recognizing \"places\" where the user\nvisits, which is important information in the lifelogging system. First, we\ninvestigate the amount of generated electricity of selected energy harvesting\nelements in various environments. Second, we design a system called EHAAS\n(Energy Harvesters As A Sensor) where energy harvesting elements are used as a\nsensor. With EHAAS, we propose a place recognition method based on\nmachine-learning and implement a prototype wearable system. Our prototype\nevaluation confirms that EHAAS achieves a place recognition accuracy of 88.5%\nF-value for nine different indoor and outdoor places. This result is better\nthan the results of existing sensors (3-axis accelerometer and brightness). We\nalso clarify that only two types of solar cells are required for recognizing a\nplace with 86.2% accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:22:33 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Umetsu", "Yoshinori", ""], ["Nakamura", "Yugo", ""], ["Arakawa", "Yutaka", ""], ["Fujimoto", "Manato", ""], ["Suwa", "Hirohiko", ""]]}, {"id": "1903.08644", "submitter": "Christos Katsanos", "authors": "Christos Katsanos, Nikolaos Tselios, Nikolaos Avouris, Stavros\n  Demetriadis, Ioannis Stamelos, Lefteris Angelis", "title": "Cross-study Reliability of the Open Card Sorting Method", "comments": "ACM CHI Conference on Human Factors in Computing Systems (CHI) 2019", "journal-ref": null, "doi": "10.1145/3290607.3312999", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information architecture forms the foundation of users' navigation\nexperience. Open card sorting is a widely-used method to create information\narchitectures based on users' groupings of the content. However, little is\nknown about the method's cross-study reliability: Does it produce consistent\ncontent groupings for similar profile participants involved in different card\nsort studies? This paper presents an empirical evaluation of the method's\ncross-study reliability. Six card sorts involving 140 participants were\nconducted: three open sorts for a travel website, and three for an eshop.\nResults showed that participants provided highly similar card sorting data for\nthe same content. A rather high agreement of the produced navigation schemes\nwas also found. These findings provide support for the cross-study reliability\nof the open card sorting method.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 20:17:49 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Katsanos", "Christos", ""], ["Tselios", "Nikolaos", ""], ["Avouris", "Nikolaos", ""], ["Demetriadis", "Stavros", ""], ["Stamelos", "Ioannis", ""], ["Angelis", "Lefteris", ""]]}, {"id": "1903.08915", "submitter": "Tao Bi", "authors": "Tao Bi, Yiyi Zhang, Chongyang Wang, Amid Ayobi", "title": "Characterizing HCI Research in China: Streams, Methodologies and Future\n  Directions", "comments": null, "journal-ref": "CHI 2019 workshop: HCI in China: Research Agenda, Education\n  Curriculum, Industry Partnership, and Communities Building", "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This position paper takes the first step to attempt to present the initial\ncharacterization of HCI research in China. We discuss the current streams and\nmethodologies of Chinese HCI research based on two well-known HCI theories:\nMicro/Marco-HCI and the Three Paradigms of HCI. We evaluate the discussion with\na survey of Chinese publications at CHI 2019, which shows HCI research in China\nhas less attention to Macro-HCI topics and the third paradigms of HCI\n(Phenomenologically situated Interaction). We then propose future HCI research\ndirections such as paying more attention to Macro-HCI topics and third paradigm\nof HCI, combining research methodologies from multiple HCI paradigms, including\nemergent users who have less access to technology, and addressing the cultural\ndimensions in order to provide better technical solutions and support.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 10:41:43 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 09:50:27 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Bi", "Tao", ""], ["Zhang", "Yiyi", ""], ["Wang", "Chongyang", ""], ["Ayobi", "Amid", ""]]}, {"id": "1903.08918", "submitter": "Xavier Bellekens", "authors": "Xavier Bellekens, Gayan Jayasekara, Hanan Hindy, Miroslav Bures, David\n  Brosset, Christos Tachtatzis, Robert Atkinson", "title": "From Cyber-Security Deception To Manipulation and Gratification Through\n  Gamification", "comments": "17 Pages, Accepted in HCI International 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the ever growing networking capabilities and services offered to users,\nattack surfaces have been increasing exponentially, additionally, the intricacy\nof network architectures has increased the complexity of cyber-defenses, to\nthis end, the use of deception has recently been trending both in academia and\nindustry. Deception enables to create proactive defense systems, luring\nattackers in order to better defend the systems at hand. Current applications\nof deception, only rely on static, or low interactive environments. In this\npaper we present a platform that combines human-computer-interaction,\nanalytics, gamification and deception to lure malicious users into selected\ntraps while piquing their interests. Furthermore we analyse the interactive\ndeceptive aspects of the platform through the addition of a narrative, further\nengaging malicious users into following a predefined path and deflecting\nattacks from key network systems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 10:48:16 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Bellekens", "Xavier", ""], ["Jayasekara", "Gayan", ""], ["Hindy", "Hanan", ""], ["Bures", "Miroslav", ""], ["Brosset", "David", ""], ["Tachtatzis", "Christos", ""], ["Atkinson", "Robert", ""]]}, {"id": "1903.09094", "submitter": "Nimish Awalgaonkar", "authors": "Nimish Awalgaonkar, Ilias Bilionis, Xiaoqi Liu, Panagiota Karava,\n  Athanasios Tzempelikos", "title": "Learning Personalized Thermal Preferences via Bayesian Active Learning\n  with Unimodality Constraints", "comments": "39 pages, 11 figures. References are updated. Typos are corrected.\n  Changed \"room temperatures\" to \"indoor air temperatures\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal preferences vary from person to person and may change over time. The\nmain objective of this paper is to sequentially pose intelligent queries to\noccupants in order to optimally learn the indoor air temperature values which\nmaximize their satisfaction. Our central hypothesis is that an occupant's\npreference relation over indoor air temperature can be described using a scalar\nfunction of these temperatures, which we call the \"occupant's thermal utility\nfunction\". Information about an occupant's preference over these temperatures\nis available to us through their response to thermal preference queries :\n\"prefer warmer,\" \"prefer cooler\" and \"satisfied\" which we interpret as\nstatements about the derivative of their utility function, i.e. the utility\nfunction is \"increasing\", \"decreasing\" and \"constant\" respectively. We model\nthis hidden utility function using a Gaussian process prior with built-in\nunimodality constraint, i.e., the utility function has a unique maximum, and we\ntrain this model using Bayesian inference. This permits an expected improvement\nbased selection of next preference query to pose to the occupant, which takes\ninto account both exploration (sampling from areas of high uncertainty) and\nexploitation (sampling from areas which are likely to offer an improvement over\ncurrent best observation). We use this framework to sequentially design\nexperiments and illustrate its benefits by showing that it requires drastically\nfewer observations to learn the maximally preferred temperature values as\ncompared to other methods. This framework is an important step towards the\ndevelopment of intelligent HVAC systems which would be able to respond to\noccupants' personalized thermal comfort needs. In order to encourage the use of\nour PE framework and ensure reproducibility in results, we publish an\nimplementation of our work named GPPrefElicit as an open-source package in\nPython.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 16:23:35 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 15:13:21 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Awalgaonkar", "Nimish", ""], ["Bilionis", "Ilias", ""], ["Liu", "Xiaoqi", ""], ["Karava", "Panagiota", ""], ["Tzempelikos", "Athanasios", ""]]}, {"id": "1903.09113", "submitter": "Behnam Malmir", "authors": "Behnam Malmir", "title": "Exploratory studies of human gait changes using depth cameras and\n  considering measurement errors", "comments": "73 pages, 26 figures, a thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.AP stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This research aims to quantify human walking patterns through depth cameras\nto (1) detect walking pattern changes of a person with and without a\nmotion-restricting device or a walking aid, and to (2) identify distinct\nwalking patterns from different persons of similar physical attributes.\nMicrosoft Kinect devices, often used for video games, were used to provide and\ntrack coordinates of 25 different joints of people over time to form a human\nskeleton. Then multiple machine learning (ML) models were applied to the SE\ndatasets from ten college-age subjects - five males and five females. In\nparticular, ML models were applied to classify subjects into two categories:\nnormal walking and abnormal walking (i.e. with motion-restricting devices). The\nbest ML model (K-nearest neighborhood) was able to predict 97.3% accuracy using\n10-fold cross-validation. Finally, ML models were applied to classify five gait\nconditions: walking normally, walking while wearing the ankle brace, walking\nwhile wearing the ACL brace, walking while using a cane, and walking while\nusing a walker. The best ML model was again the K-nearest neighborhood\nperforming at 98.7% accuracy rate.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 17:06:40 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Malmir", "Behnam", ""]]}, {"id": "1903.09530", "submitter": "Antonio Camurri", "authors": "Paolo Alborno, Gualtiero Volpe, Maurizio Mancini, Radoslaw\n  Niewiadomski, Stefano Piana, Antonio Camurri", "title": "The Multi-Event-Class Synchronization (MECS) Algorithm", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Synchronization is a fundamental component of computational models of human\nbehavior, at both intra-personal and inter-personal level. Event\nsynchronization analysis was originally conceived with the aim of providing a\nsimple and robust method to measure synchronization between two time series. In\nthis paper we propose a novel method extending the state-of-the-art of the\nevent synchronization techniques: the Multi-Event-Class Synchronization (MECS)\nalgorithm. MECS measures the synchronization between relevant events belonging\nto different event classes that are detected in multiple time series. Its\nmotivation emerged from the need to model non-verbal multimodal signals in\nHuman-Computer Interaction. Using MECS, synchronization can be computed between\nevents belonging to the same class (intra-class synchronization) or between\nevents belonging to different classes (inter-class synchronization). In the\npaper we also show how our technique can deal with macro-events (i.e., sets of\nevents satisfying constraints) and macro-classes (i.e., sets of classes). In\nthe last part of the paper, we apply the proposed method to two types of data\ni) artificial and 2) real-world case study concerning analysis of human\nmultimodal behavior.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 14:39:24 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 14:38:17 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Alborno", "Paolo", ""], ["Volpe", "Gualtiero", ""], ["Mancini", "Maurizio", ""], ["Niewiadomski", "Radoslaw", ""], ["Piana", "Stefano", ""], ["Camurri", "Antonio", ""]]}, {"id": "1903.09623", "submitter": "Jialun Jiang", "authors": "Jialun \"Aaron\" Jiang, Casey Fiesler, Jed R. Brubaker", "title": "\"The Perfect One\": Understanding Communication Practices and Challenges\n  with Animated GIFs", "comments": null, "journal-ref": "Proc. ACM Hum.-Comput. Interact. 2, CSCW: Article 80 (2018)", "doi": "10.1145/3274349", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Animated GIFs are increasingly popular in text-based communication. Finding\nthe perfect GIF can make conversations funny, interesting, and engaging, but\nGIFs also introduce potentials for miscommunication. Through 24 in-depth\nqualitative interviews, this empirical, exploratory study examines the nuances\nof communication practices with animated GIFs to better understand why and how\nGIFs can send unintentional messages. We find participants leverage contexts\nlike source material and interpersonal relationship to find the perfect GIFs\nfor different communication scenarios, while these contexts are also the\nprimary reason for miscommunication and some technical usability issues in\nGIFs. This paper concludes with a discussion of the important role that\ndifferent types of context play in the use and interpretations of GIFs, and\nargues that nonverbal communication tools should account for complex contexts\nand common ground that communication media rely on.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 17:34:21 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Jiang", "Jialun \"Aaron\"", ""], ["Fiesler", "Casey", ""], ["Brubaker", "Jed R.", ""]]}, {"id": "1903.09708", "submitter": "Andrew Anderson", "authors": "Andrew Anderson, Jonathan Dodge, Amrita Sadarangani, Zoe Juozapaitis,\n  Evan Newman, Jed Irvine, Souti Chattopadhyay, Alan Fern, Margaret Burnett", "title": "Explaining Reinforcement Learning to Mere Mortals: An Empirical Study", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a user study to investigate the impact of explanations on\nnon-experts' understanding of reinforcement learning (RL) agents. We\ninvestigate both a common RL visualization, saliency maps (the focus of\nattention), and a more recent explanation type, reward-decomposition bars\n(predictions of future types of rewards). We designed a 124 participant,\nfour-treatment experiment to compare participants' mental models of an RL agent\nin a simple Real-Time Strategy (RTS) game. Our results show that the\ncombination of both saliency and reward bars were needed to achieve a\nstatistically significant improvement in mental model score over the control.\nIn addition, our qualitative analysis of the data reveals a number of effects\nfor further study.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 20:56:55 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 19:47:07 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Anderson", "Andrew", ""], ["Dodge", "Jonathan", ""], ["Sadarangani", "Amrita", ""], ["Juozapaitis", "Zoe", ""], ["Newman", "Evan", ""], ["Irvine", "Jed", ""], ["Chattopadhyay", "Souti", ""], ["Fern", "Alan", ""], ["Burnett", "Margaret", ""]]}, {"id": "1903.09709", "submitter": "Matthew Guzdial", "authors": "Matthew Guzdial and Mark Riedl", "title": "An Interaction Framework for Studying Co-Creative AI", "comments": "6 pages, 2 figures, Human-Centered Machine Learning Perspectives\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has been applied to a number of creative, design-oriented\ntasks. However, it remains unclear how to best empower human users with these\nmachine learning approaches, particularly those users without technical\nexpertise. In this paper we propose a general framework for turn-based\ninteraction between human users and AI agents designed to support human\ncreativity, called {co-creative systems}. The framework can be used to better\nunderstand the space of possible designs of co-creative systems and reveal\nfuture research directions. We demonstrate how to apply this framework in\nconjunction with a pair of recent human subject studies, comparing between the\nfour human-AI systems employed in these studies and generating hypotheses\ntowards future studies.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 20:57:05 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Guzdial", "Matthew", ""], ["Riedl", "Mark", ""]]}, {"id": "1903.09803", "submitter": "Ismail Shahin", "authors": "Ismail Shahin", "title": "Emotion Recognition based on Third-Order Circular Suprasegmental Hidden\n  Markov Model", "comments": "Accepted at The 2019 IEEE Jordan International Joint Conference on\n  Electrical Engineering and Information Technology (JEEIT), Jordan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on recognizing the unknown emotion based on the Third-Order\nCircular Suprasegmental Hidden Markov Model (CSPHMM3) as a classifier. Our work\nhas been tested on Emotional Prosody Speech and Transcripts (EPST) database.\nThe extracted features of EPST database are Mel-Frequency Cepstral Coefficients\n(MFCCs). Our results give average emotion recognition accuracy of 77.8% based\non the CSPHMM3. The results of this work demonstrate that CSPHMM3 is superior\nto the Third-Order Hidden Markov Model (HMM3), Gaussian Mixture Model (GMM),\nSupport Vector Machine (SVM), and Vector Quantization (VQ) by 6.0%, 4.9%, 3.5%,\nand 5.4%, respectively, for emotion recognition. The average emotion\nrecognition accuracy achieved based on the CSPHMM3 is comparable to that found\nusing subjective assessment by human judges.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 11:24:08 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Shahin", "Ismail", ""]]}, {"id": "1903.09866", "submitter": "John Kelleher", "authors": "John D. Kelleher and Simon Dobnik", "title": "Referring to the recently seen: reference and perceptual memory in\n  situated dialog", "comments": "18 Pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From theoretical linguistic and cognitive perspectives, situated dialog\nsystems are interesting as they provide ideal test-beds for investigating the\ninteraction between language and perception. At the same time there are a\ngrowing number of practical applications, for example robotic systems and\ndriver-less cars, where spoken interfaces, capable of situated dialog, promise\nmany advantages. To date, however much of the work on situated dialog has\nfocused resolving anaphoric or exophoric references. This paper, by contrast,\nopens up the question of how perceptual memory and linguistic references\ninteract, and the challenges that this poses to computational models of\nperceptually grounded dialog.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 18:58:38 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Kelleher", "John D.", ""], ["Dobnik", "Simon", ""]]}, {"id": "1903.10512", "submitter": "Shuo Zhang", "authors": "Jason Shuo Zhang, Mike Gartrell, Richard Han, Qin Lv, and Shivakant\n  Mishra", "title": "GEVR: An Event Venue Recommendation System for Groups of Mobile Users", "comments": "in Proceedings of the ACM on Interactive, Mobile, Wearable and\n  Ubiquitous Technologies (IMWUT), Volume 3 Issue 1, March 2019", "journal-ref": null, "doi": "10.1145/3314421", "report-no": null, "categories": "cs.HC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present GEVR, the first Group Event Venue Recommendation\nsystem that incorporates mobility via individual location traces and context\ninformation into a \"social-based\" group decision model to provide venue\nrecommendations for groups of mobile users. Our study leverages a real-world\ndataset collected using the OutWithFriendz mobile app for group event planning,\nwhich contains 625 users and over 500 group events. We first develop a novel\n\"social-based\" group location prediction model, which adaptively applies\ndifferent group decision strategies to groups with different social\nrelationship strength to aggregate each group member's location preference, to\npredict where groups will meet. Evaluation results show that our prediction\nmodel not only outperforms commonly used and state-of-the-art group decision\nstrategies with over 80% accuracy for predicting groups' final meeting location\nclusters, but also provides promising qualities in cold-start scenarios. We\nthen integrate our prediction model with the Foursquare Venue Recommendation\nAPI to construct an event venue recommendation framework for groups of mobile\nusers. Evaluation results show that GEVR outperforms the comparative models by\na significant margin.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 18:00:02 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Zhang", "Jason Shuo", ""], ["Gartrell", "Mike", ""], ["Han", "Richard", ""], ["Lv", "Qin", ""], ["Mishra", "Shivakant", ""]]}, {"id": "1903.11054", "submitter": "Alexandros Liapis Mr", "authors": "Alexandros Liapis, Christos Katsanos, Michalis Xenos and Theofanis\n  Orphanoudakis", "title": "Effect of Personality Traits on UX Evaluation Metrics: A Study on\n  Usability Issues, Valence-Arousal and Skin Conductance", "comments": null, "journal-ref": null, "doi": "10.1145/3290607.3312995", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personality affect the way someone feels or acts. This paper examines the\neffect of personality traits, as operationalized by the Big-five questionnaire,\non the number, type, and severity of the identified usability issues,\nphysiological signals (skin conductance), and subjective emotional ratings\n(valence-arousal).Twenty-four users interacted with a web service and then\nparticipated in a retrospective thinking aloud session. Results revealed that\nthe number of usability issues is significantly affected by the Openness trait.\nEmotional Stability significantly affects the type of reported usability\nissues. Problem severity is not affected by any trait. Valence ratings are\nsignificantly affected by Conscientiousness, whereas Agreeableness, Emotional\nStability and Openness significantly affect arousal ratings. Finally, Openness\nhas a significant effect on the number of detected peaks in user's skin\nconductance.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 07:53:29 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 21:17:30 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Liapis", "Alexandros", ""], ["Katsanos", "Christos", ""], ["Xenos", "Michalis", ""], ["Orphanoudakis", "Theofanis", ""]]}, {"id": "1903.11297", "submitter": "Gregoire Cattan", "authors": "Gr\\'egoire Cattan (GIPSA-Services), A. Andreev, P. Rodrigues, M.\n  Congedo", "title": "Dataset of an EEG-based BCI experiment in Virtual Reality and on a\n  Personal Computer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the experimental procedures for a dataset that we have made\npublicly available at https://doi.org/10.5281/zenodo.2605204 in mat (Mathworks,\nNatick, USA) and csv formats. This dataset contains electroencephalographic\nrecordings on 21 subjects doing a visual P300 experiment on PC (personal\ncomputer) and VR (virtual reality). The visual P300 is an event-related\npotential elicited by a visual stimulation, peaking 240-600 ms after stimulus\nonset. The experiment was designed in order to compare the use of a P300-based\nbrain-computer interface on a PC and with a virtual reality headset, concerning\nthe physiological, subjective and performance aspects. The brain-computer\ninterface is based on electroencephalography (EEG). EEG were recorded thanks to\n16 electrodes. The virtual reality headset consisted of a passive head-mounted\ndisplay, that is, a head-mounted display which does not include any electronics\nat the exception of a smartphone. This experiment was carried out at GIPSA-lab\n(University of Grenoble Alpes, CNRS, Grenoble-INP) in 2018, and promoted by the\nIHMTEK Company (Interaction Homme-Machine Technologie). The study was approved\nby the Ethical Committee of the University of Grenoble Alpes (Comit{\\'e}\nd'Ethique pour la Recherche Non-Interventionnelle). Python code for\nmanipulating the data is available at\nhttps://github.com/plcrodrigues/py.VR.EEG.2018-GIPSA. The ID of this dataset is\nVR.EEG.2018-GIPSA.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 08:58:02 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Cattan", "Gr\u00e9goire", "", "GIPSA-Services"], ["Andreev", "A.", ""], ["Rodrigues", "P.", ""], ["Congedo", "M.", ""]]}, {"id": "1903.11485", "submitter": "Riku Arakawa", "authors": "Riku Arakawa and Hiromu Yakura", "title": "REsCUE: A framework for REal-time feedback on behavioral CUEs using\n  multimodal anomaly detection", "comments": "ACM CHI 2019", "journal-ref": null, "doi": "10.1145/3290605.3300802", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Executive coaching has been drawing more and more attention for developing\ncorporate managers. While conversing with managers, coach practitioners are\nalso required to understand internal states of coachees through objective\nobservations. In this paper, we present REsCUE, an automated system to aid\ncoach practitioners in detecting unconscious behaviors of their clients. Using\nan unsupervised anomaly detection algorithm applied to multimodal behavior data\nsuch as the subject's posture and gaze, REsCUE notifies behavioral cues for\ncoaches via intuitive and interpretive feedback in real-time. Our evaluation\nwith actual coaching scenes confirms that REsCUE provides the informative cues\nto understand internal states of coachees. Since REsCUE is based on the\nunsupervised method and does not assume any prior knowledge, further\napplications beside executive coaching are conceivable using our framework.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 15:24:48 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Arakawa", "Riku", ""], ["Yakura", "Hiromu", ""]]}, {"id": "1903.11567", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Benjamin Page", "title": "Haptics-Augmented Physics Simulation: Coriolis Effect", "comments": "ISSN: 1844-8933", "journal-ref": "ICVL 2012, pp. 34-38", "doi": null, "report-no": null, "categories": "cs.HC physics.ed-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The teaching of abstract physics concepts can be enhanced by incorporating\nvisual and haptic sensory modalities in the classroom, using the correct\nperspectives. We have developed virtual reality simulations to assist students\nin learning the Coriolis effect, an apparent deflection on an object in motion\nwhen observed from within a rotating frame of reference. Twenty four\nundergraduate physics students participated in this study. Students were able\nto feel the forces through feedback on a Novint Falcon device. The assessment\nresults show an improvement in the learning experience and better content\nretention as compared with traditional instruction methods. We prove that large\nscale deployment of visuo-haptic reconfigurable applications is now possible\nand feasible in a science laboratory setup.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 02:04:56 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Page", "Benjamin", ""]]}, {"id": "1903.11579", "submitter": "Yelena Mejova", "authors": "Yelena Mejova and Kyriaki Kalimeri", "title": "Effect of Values and Technology Use on Exercise: Implications for\n  Personalized Behavior Change Interventions", "comments": null, "journal-ref": "27th Conference on User Modeling, Adaptation and Personalization\n  (UMAP) 2019", "doi": "10.1145/3320435.3320451", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology has recently been recruited in the war against the ongoing obesity\ncrisis; however, the adoption of Health & Fitness applications for regular\nexercise is a struggle. In this study, we present a unique demographically\nrepresentative dataset of 15k US residents that combines technology use logs\nwith surveys on moral views, human values, and emotional contagion. Combining\nthese data, we provide a holistic view of individuals to model their physical\nexercise behavior. First, we show which values determine the adoption of Health\n& Fitness mobile applications, finding that users who prioritize the value of\npurity and de-emphasize values of conformity, hedonism, and security are more\nlikely to use such apps. Further, we achieve a weighted AUROC of .673 in\npredicting whether individual exercises, and we also show that the application\nusage data allows for substantially better classification performance (.608)\ncompared to using basic demographics (.513) or internet browsing data (.546).\nWe also find a strong link of exercise to respondent socioeconomic status, as\nwell as the value of happiness. Using these insights, we propose actionable\ndesign guidelines for persuasive technologies targeting health behavior\nmodification.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 17:47:13 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Mejova", "Yelena", ""], ["Kalimeri", "Kyriaki", ""]]}, {"id": "1903.11672", "submitter": "Mimansa Jaiswal", "authors": "Mimansa Jaiswal, Zakaria Aldeneh, Cristian-Paul Bara, Yuanhang Luo,\n  Mihai Burzo, Rada Mihalcea, Emily Mower Provost", "title": "MuSE-ing on the Impact of Utterance Ordering On Crowdsourced Emotion\n  Annotations", "comments": "5 pages, ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition algorithms rely on data annotated with high quality\nlabels. However, emotion expression and perception are inherently subjective.\nThere is generally not a single annotation that can be unambiguously declared\n\"correct\". As a result, annotations are colored by the manner in which they\nwere collected. In this paper, we conduct crowdsourcing experiments to\ninvestigate this impact on both the annotations themselves and on the\nperformance of these algorithms. We focus on one critical question: the effect\nof context. We present a new emotion dataset, Multimodal Stressed Emotion\n(MuSE), and annotate the dataset using two conditions: randomized, in which\nannotators are presented with clips in random order, and contextualized, in\nwhich annotators are presented with clips in order. We find that contextual\nlabeling schemes result in annotations that are more similar to a speaker's own\nself-reported labels and that labels generated from randomized schemes are most\neasily predictable by automated systems.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 19:49:00 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Jaiswal", "Mimansa", ""], ["Aldeneh", "Zakaria", ""], ["Bara", "Cristian-Paul", ""], ["Luo", "Yuanhang", ""], ["Burzo", "Mihai", ""], ["Mihalcea", "Rada", ""], ["Provost", "Emily Mower", ""]]}, {"id": "1903.11812", "submitter": "Arash Kalatian", "authors": "Arash Kalatian, Anae Sobhani, Bilal Farooq", "title": "Analysis of distracted pedestrians' waiting time: Head-Mounted Immersive\n  Virtual Reality application", "comments": "Published in the proceedings of Pedestrian and Evacuation Dynamics\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the distracted pedestrians' waiting time before crossing\nthe road in three conditions: 1) not distracted, 2) distracted with a\nsmartphone and 3) distracted with a smartphone in the presence of virtual\nflashing LED lights on the crosswalk as a safety measure. For the means of data\ncollection, we adapted an in-house developed virtual immersive reality\nenvironment (VIRE). A total of 42 volunteers participated in the experiment.\nParticipants' positions and head movements were recorded and used to calculate\nwalking speeds, acceleration and deceleration rates, surrogate safety measures,\ntime spent playing smartphone game, etc. After a descriptive analysis on the\ndata, the effects of these variables on pedestrians' waiting time are analyzed\nby employing a cox proportional hazard model. Several factors were identified\nas having impact on waiting time. The results show that an increase in initial\nwalk speed, percentage of time the head was oriented toward smartphone during\ncrossing, bigger minimum missed gaps and unsafe crossings resulted in shorter\nwaiting times. On the other hand, an increase in the percentage of time the\nhead was oriented toward smartphone during waiting time, crossing time and maze\nsolving time, means longer waiting times for participants.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 07:23:10 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Kalatian", "Arash", ""], ["Sobhani", "Anae", ""], ["Farooq", "Bilal", ""]]}, {"id": "1903.11944", "submitter": "Mubashar Iqbal", "authors": "Mubashar Iqbal", "title": "Taxonomies in DUI Design Patterns: A Systematic Approach for Removing\n  Overlaps Among Design Patterns and Creating a Clear Hierarchy", "comments": "79 pages, 12 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recently a library of design patterns for designing distributed user\ninterfaces (DUIs) was created to help researchers and designers to create user\ninterfaces and to provide an overview of solutions to common DUIs design\nproblems without requiring a significant amount of time to be spent on reading\ndomain-specific literature and exploring existing DUIs implementations. The\ncurrent version of the DUI design patterns library need to be assessed because\na lot of design patterns are overlapping each other and their relationships are\nnot clear enough to effectively find the most relevant design pattern for\nsolving particular design problem, so the purpose of this thesis is to mature\nthe DUI design patterns knowledge field by removing the duplicate design\npatterns, their description and to create a taxonomy where each design pattern\nshould be organised in a way that will reduce redundancy, possibly leading to\ngrouping or eventually merging similar patterns and allow to navigate to\nrelated patterns. To achieve the defined goals, the first target was to\ninvestigate the possible overlaps among design patterns and their relevancy\nwith each other, in order to get these insights natural language processing\ntool was built for extracting and analysing each design pattern research paper\nto find potential codes. Later in this study thematic analysis was done with\ndomain experts to get themes, their description and higher level categories\nfrom generated codes to organize all related design patterns in a clear\nhierarchy. The outcomes of this thesis included the clarification of the\nrelationships among design patterns by creating a taxonomy, clarified the\ndescription of individual design pattern, overlaps and duplicate design\npatterns were removed and merged similar design patterns.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 10:07:45 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Iqbal", "Mubashar", ""]]}, {"id": "1903.11997", "submitter": "Jaroslaw Jankowski", "authors": "Jaros{\\l}aw Jankowski, Juho Hamari and Jaros{\\l}aw W\\k{a}tr\\'obski", "title": "A gradual approach for maximising user conversion without compromising\n  experience with high visual intensity website elements", "comments": "34 pages", "journal-ref": "Internet Research, 29(1), 194-217 (2019)", "doi": "10.1108/IntR-09-2016-0271", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study develops and tests a method that can gradually find a sweet spot\nbetween user experience and visual intensity of website elements to maximise\nuser conversion with minimal adverse effect. In the first phase of the study,\nwe develop the method. In the second stage, we test and evaluate the method via\nan empirical study; also, an experiment was conducted within web interface with\nthe gradual intensity of visual elements.The findings reveal that negative\nresponse grows faster than conversion when the visual intensity of the web\ninterface is increased. However, a saturation point, where there is coexistence\nbetween maximum conversion and minimum negative response, can be found. The\nfindings imply that efforts to attract user attention should be pursued with\nincreased caution and that a gradual approach presented in this study helps in\nfinding a site-specific sweet-spot for a level of visual intensity by\nincrementally adjusting the elements of the interface and tracking the changes\nin user behaviour. Web marketing and advertising professionals often face the\ndilemma of determining the optimal level of visual intensity of interface\nelement. Excessive use of marketing component and attention-grabbing visual\nelements can lead to an inferior user experience and consequent user churn due\nto growing intrusiveness. At the same time, too little visual intensity can\nfail to steer users. The present study provides a gradual approach which aids\nin finding a balance between user experience and visual intensity, maximising\nuser conversion and thus providing a practical solution for the problem.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 14:17:38 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Jankowski", "Jaros\u0142aw", ""], ["Hamari", "Juho", ""], ["W\u0105tr\u00f3bski", "Jaros\u0142aw", ""]]}, {"id": "1903.12041", "submitter": "Brent Hecht", "authors": "Ashley Colley, Jacob Thebault-Spieker, Allen Yilun Lin, Donald\n  Degraen, Benjamin Fischman, Jonna H\\\"akkil\\\"a, Kate Kuehl, Valentina Nisi,\n  Nuno Jardim Nunes, Nina Wenig, Dirk Wenig, Brent Hecht, Johannes Sch\\\"oning", "title": "The Geography of Pok\\'emon GO: Beneficial and Problematic Effects on\n  Places and Movement", "comments": "This version of the paper contains a fix for a reference issue that\n  appeared in the original version. Proceedings of the 35th Annual ACM\n  Conference on Human Factors in Computing Systems (CHI 2017)", "journal-ref": null, "doi": "10.1145/3025453.3025495", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread popularity of Pok\\'emon GO presents the first opportunity to\nobserve the geographic effects of location-based gaming at scale. This paper\nreports the results of a mixed methods study of the geography of Pok\\'emon GO\nthat includes a five-country field survey of 375 Pok\\'emon GO players and a\nlarge scale geostatistical analysis of game elements. Focusing on the key\ngeographic themes of places and movement, we find that the design of Pok\\'emon\nGO reinforces existing geographically-linked biases (e.g. the game advantages\nurban areas and neighborhoods with smaller minority populations), that\nPok\\'emon GO may have instigated a relatively rare large-scale shift in global\nhuman mobility patterns, and that Pok\\'emon GO has geographically-linked safety\nrisks, but not those typically emphasized by the media. Our results point to\ngeographic design implications for future systems in this space such as a means\nthrough which the geographic biases present in Pok\\'emon GO may be\ncounteracted.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 15:17:57 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Colley", "Ashley", ""], ["Thebault-Spieker", "Jacob", ""], ["Lin", "Allen Yilun", ""], ["Degraen", "Donald", ""], ["Fischman", "Benjamin", ""], ["H\u00e4kkil\u00e4", "Jonna", ""], ["Kuehl", "Kate", ""], ["Nisi", "Valentina", ""], ["Nunes", "Nuno Jardim", ""], ["Wenig", "Nina", ""], ["Wenig", "Dirk", ""], ["Hecht", "Brent", ""], ["Sch\u00f6ning", "Johannes", ""]]}, {"id": "1903.12133", "submitter": "Daniel McDuff", "authors": "Daniel McDuff, Kael Rowan, Piali Choudhury, Jessica Wolk, ThuVan Pham\n  and Mary Czerwinski", "title": "A Multimodal Emotion Sensing Platform for Building Emotion-Aware\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans use a host of signals to infer the emotional state of others. In\ngeneral, computer systems that leverage signals from multiple modalities will\nbe more robust and accurate in the same task. We present a multimodal affect\nand context sensing platform. The system is composed of video, audio and\napplication analysis pipelines that leverage ubiquitous sensors (camera and\nmicrophone) to log and broadcast emotion data in real-time. The platform is\ndesigned to enable easy prototyping of novel computer interfaces that sense,\nrespond and adapt to human emotion. This paper describes the different audio,\nvisual and application processing components and explains how the data is\nstored and/or broadcast for other applications to consume. We hope that this\nplatform helps advance the state-of-the-art in affective computing by enabling\ndevelopment of novel human-computer interfaces.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:19:34 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["McDuff", "Daniel", ""], ["Rowan", "Kael", ""], ["Choudhury", "Piali", ""], ["Wolk", "Jessica", ""], ["Pham", "ThuVan", ""], ["Czerwinski", "Mary", ""]]}, {"id": "1903.12235", "submitter": "Ozan Ozdenizci", "authors": "Ozan Ozdenizci, Deniz Erdogmus", "title": "Information Theoretic Feature Transformation Learning for Brain\n  Interfaces", "comments": "Accepted for publication by IEEE Transactions on Biomedical\n  Engineering", "journal-ref": "IEEE Transactions on Biomedical Engineering, 2019", "doi": "10.1109/TBME.2019.2908099", "report-no": null, "categories": "cs.LG cs.HC cs.IT eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: A variety of pattern analysis techniques for model training in\nbrain interfaces exploit neural feature dimensionality reduction based on\nfeature ranking and selection heuristics. In the light of broad evidence\ndemonstrating the potential sub-optimality of ranking based feature selection\nby any criterion, we propose to extend this focus with an information theoretic\nlearning driven feature transformation concept. Methods: We present a maximum\nmutual information linear transformation (MMI-LinT), and a nonlinear\ntransformation (MMI-NonLinT) framework derived by a general definition of the\nfeature transformation learning problem. Empirical assessments are performed\nbased on electroencephalographic (EEG) data recorded during a four class motor\nimagery brain-computer interface (BCI) task. Exploiting state-of-the-art\nmethods for initial feature vector construction, we compare the proposed\napproaches with conventional feature selection based dimensionality reduction\ntechniques which are widely used in brain interfaces. Furthermore, for the\nmulti-class problem, we present and exploit a hierarchical graphical model\nbased BCI decoding system. Results: Both binary and multi-class decoding\nanalyses demonstrate significantly better performances with the proposed\nmethods. Conclusion: Information theoretic feature transformations are capable\nof tackling potential confounders of conventional approaches in various\nsettings. Significance: We argue that this concept provides significant\ninsights to extend the focus on feature selection heuristics to a broader\ndefinition of feature transformation learning in brain interfaces.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 19:41:05 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 17:10:57 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Ozdenizci", "Ozan", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "1903.12264", "submitter": "Timur Osadchiy", "authors": "Timur Osadchiy, Ivan Poliakov, Patrick Olivier, Maisie Rowland, Emma\n  Foster", "title": "Validation of a recommender system for prompting omitted foods in online\n  dietary assessment surveys", "comments": null, "journal-ref": "PervasiveHealth 2019 Proceedings of the 13th EAI International\n  Conference on Pervasive Computing Technologies for Healthcare", "doi": "10.1145/3329189.3329191", "report-no": "ISBN: 978-1-4503-6126-2", "categories": "cs.CY cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recall assistance methods are among the key aspects that improve the accuracy\nof online dietary assessment surveys. These methods still mainly rely on\nexperience of trained interviewers with nutritional background, but data driven\napproaches could improve cost-efficiency and scalability of automated dietary\nassessment. We evaluated the effectiveness of a recommender algorithm developed\nfor an online dietary assessment system called Intake24, that automates the\nmultiple-pass 24-hour recall method. The recommender builds a model of eating\nbehavior from recalls collected in past surveys. Based on foods they have\nalready selected, the model is used to remind respondents of associated foods\nthat they may have omitted to report. The performance of prompts generated by\nthe model was compared to that of prompts hand-coded by nutritionists in two\ndietary studies. The results of our studies demonstrate that the recommender\nsystem is able to capture a higher number of foods omitted by respondents of\nonline dietary surveys than prompts hand-coded by nutritionists. However, the\nconsiderably lower precision of generated prompts indicates an opportunity for\nfurther improvement of the system.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:42:54 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Osadchiy", "Timur", ""], ["Poliakov", "Ivan", ""], ["Olivier", "Patrick", ""], ["Rowland", "Maisie", ""], ["Foster", "Emma", ""]]}, {"id": "1903.12349", "submitter": "Franz Sauer", "authors": "Yucong (Chris) Ye, Franz Sauer, Kwan-Liu Ma, Konduri Aditya, and\n  Jacqueline Chen", "title": "A User-centered Design Study in Scientific Visualization Targeting\n  Domain Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development and design of visualization solutions that are truly usable\nis essential for ensuring both their adoption and effectiveness. User-centered\ndesign principles, which focus on involving users throughout the entire\ndevelopment process, are well suited for visualization and have been shown to\nbe effective in numerous information visualization endeavors. In this paper, we\nreport a two year long collaboration with combustion scientists that, by\napplying these design principles, generated multiple results including an in\nsitu visualization technique and a post hoc probability distribution function\n(PDF) exploration tool. Furthermore, we examine the importance of user-centered\ndesign principles and describe lessons learned over the design process in an\neffort to aid others who also seek to work with scientists for developing\neffective and usable scientific visualization solutions.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 03:59:33 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Yucong", "", "", "Chris"], ["Ye", "", ""], ["Sauer", "Franz", ""], ["Ma", "Kwan-Liu", ""], ["Aditya", "Konduri", ""], ["Chen", "Jacqueline", ""]]}, {"id": "1903.12616", "submitter": "Emily Huang", "authors": "Emily Huang and Jukka-Pekka Onnela", "title": "Activity Classification Using Smartphone Gyroscope and Accelerometer\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activities, such as walking and sitting, are commonly used in biomedical\nsettings either as an outcome or covariate of interest. Researchers have\ntraditionally relied on surveys to quantify activity levels of subjects in both\nresearch and clinical settings, but surveys are not objective in nature and\nhave many known limitations, such as recall bias. Smartphones provide an\nopportunity for unobtrusive objective measurement of various activities in\nnaturalistic settings, but their data tends to be noisy and needs to be\nanalyzed with care. We explored the potential of smartphone accelerometer and\ngyroscope data to distinguish between five different types of activity:\nwalking, sitting, standing, ascending stairs, and descending stairs. We\nconducted a study in which four participants followed a study protocol and\nperformed a sequence of various activities with one phone in their front pocket\nand another phone in their back pocket. The subjects were filmed throughout,\nand the obtained footage was annotated to establish ground truth activity. We\napplied the so-called movelet method to classify their activity. Our results\ndemonstrate the promise of smartphones for activity detection in naturalistic\nsettings, but they also highlight common challenges in this field of research.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 14:23:33 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Huang", "Emily", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1903.12617", "submitter": "Qiang Wei", "authors": "Wei Qionghua, Wang Hui, Wei Qiang", "title": "Some Experimental Results of Relieving Discomfort in Virtual Reality by\n  Disturbing Feedback Loop in Human Brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, great progress has been made in virtual reality(VR) research and\napplication. However, virtual reality faces a big problem since its appearance,\ni.e. discomfort (nausea, stomach awareness, etc). Discomfort can be relieved by\nincreasing hardware (sensor, cpu and display) speed. But this will increase\ncost. This paper gives another low cost solution. The phenomenon of\ncybersickness is explained with the control theory: discomfort arises if\nfeedback scene differs from expectation, so it can be relieved by disturbing\nfeedback loop in human brain. A hardware platform is build to test this\nexplanation. The VR display on a Samsung S6 is blurred while head movement is\ndetected. The effect is evaluated by comparing responses to the Simulated\nSickness Questionnaire (SSQ) between a control and experimental condition.\nExperimental results show that the new method can ease discomfort remarkably\nwith little extra cost. As a result, VR may be used more widely in teaching\n(like foreign language, medicine). It's also reasonable to expect likewise\nmerits in other VR applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 01:59:29 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Qionghua", "Wei", ""], ["Hui", "Wang", ""], ["Qiang", "Wei", ""]]}]