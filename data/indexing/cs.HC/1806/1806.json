[{"id": "1806.00006", "submitter": "Truong-Huy Nguyen", "authors": "Truong-Huy D. Nguyen, Kasper Grispino, Damian Lyons", "title": "Towards Affective Drone Swarms: A Preliminary Crowd-Sourced Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drone swarms are teams of autonomous un-manned aerial vehicles that act as a\ncollective entity. We are interested in humanizing drone swarms, equipping them\nwith the ability to emotionally affect human users through their non-verbal\nmotions. Inspired by recent findings in how observers are emotionally touched\nby watching dance moves, we investigate the questions of whether and how\ncoordinated drone swarms' motions can achieve emotive impacts on general\naudience. Our preliminary study on Amazon Mechanical Turk led to a number of\ninteresting findings, including both promising results and challenges.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 13:38:08 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Nguyen", "Truong-Huy D.", ""], ["Grispino", "Kasper", ""], ["Lyons", "Damian", ""]]}, {"id": "1806.00154", "submitter": "Najmeh Sadoughi", "authors": "Najmeh Sadoughi, Carlos Busso", "title": "Speech-Driven Expressive Talking Lips with Conditional Sequential\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Articulation, emotion, and personality play strong roles in the orofacial\nmovements. To improve the naturalness and expressiveness of virtual agents\n(VAs), it is important that we carefully model the complex interplay between\nthese factors. This paper proposes a conditional generative adversarial\nnetwork, called conditional sequential GAN (CSG), which learns the relationship\nbetween emotion and lexical content in a principled manner. This model uses a\nset of articulatory and emotional features directly extracted from the speech\nsignal as conditioning inputs, generating realistic movements. A key feature of\nthe approach is that it is a speech-driven framework that does not require\ntranscripts. Our experiments show the superiority of this model over three\nstate-of-the-art baselines in terms of objective and subjective evaluations.\nWhen the target emotion is known, we propose to create emotionally dependent\nmodels by either adapting the base model with the target emotional data\n(CSG-Emo-Adapted), or adding emotional conditions as the input of the model\n(CSG-Emo-Aware). Objective evaluations of these models show improvements for\nthe CSG-Emo-Adapted compared with the CSG model, as the trajectory sequences\nare closer to the original sequences. Subjective evaluations show significantly\nbetter results for this model compared with the CSG model when the target\nemotion is happiness.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 01:09:25 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Sadoughi", "Najmeh", ""], ["Busso", "Carlos", ""]]}, {"id": "1806.00206", "submitter": "Zehong Hu Mr.", "authors": "Zehong Hu, Yitao Liang, Yang Liu, Jie Zhang", "title": "Inference Aided Reinforcement Learning for Incentive Mechanism Design in\n  Crowdsourcing", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incentive mechanisms for crowdsourcing are designed to incentivize\nfinancially self-interested workers to generate and report high-quality labels.\nExisting mechanisms are often developed as one-shot static solutions, assuming\na certain level of knowledge about worker models (expertise levels, costs of\nexerting efforts, etc.). In this paper, we propose a novel inference aided\nreinforcement mechanism that learns to incentivize high-quality data\nsequentially and requires no such prior assumptions. Specifically, we first\ndesign a Gibbs sampling augmented Bayesian inference algorithm to estimate\nworkers' labeling strategies from the collected labels at each step. Then we\npropose a reinforcement incentive learning (RIL) method, building on top of the\nabove estimates, to uncover how workers respond to different payments. RIL\ndynamically determines the payment without accessing any ground-truth labels.\nWe theoretically prove that RIL is able to incentivize rational workers to\nprovide high-quality labels. Empirical results show that our mechanism performs\nconsistently well under both rational and non-fully rational (adaptive\nlearning) worker models. Besides, the payments offered by RIL are more robust\nand have lower variances compared to the existing one-shot mechanisms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 06:00:58 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Hu", "Zehong", ""], ["Liang", "Yitao", ""], ["Liu", "Yang", ""], ["Zhang", "Jie", ""]]}, {"id": "1806.00324", "submitter": "David Puljiz", "authors": "David Puljiz, Gleb Gorbachev, Bj\\\"orn Hein", "title": "Implementation of Augmented Reality in Autonomous Warehouses: Challenges\n  and Opportunities", "comments": "As submitted to the VAM-HRI workshop, HRI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous warehouses with mobile, rack-carrying robots are starting to\nbecome commonplace, with systems such as Amazon's Kiva and Swisslog's CarryPick\nalready implemented in functional warehouses. Such warehouses however still\nrequire human intervention for object picking and maintenance. In the European\nproject SafeLog we are developing a safety-vest, used for safety-critical\nranging and stopping of mobile robots, an improved planner that can handle\nlarge fleets of heterogeneous agents as well as an AR interaction system to\nnavigate and support human workers in such automated environments. Here we\npresent the AR interaction modalities, namely navigation, pick-by-AR and\ngeneral system interactions that were developed at the moment of writing, as\nwell as the overall system concept and planned future work.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 12:58:54 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Puljiz", "David", ""], ["Gorbachev", "Gleb", ""], ["Hein", "Bj\u00f6rn", ""]]}, {"id": "1806.00567", "submitter": "Yongbin Sun", "authors": "Yongbin Sun, Sai Nithin R. Kantareddy, Rahul Bhattacharyya, Sanjay E.\n  Sarma", "title": "X-Vision: An augmented vision tool with real-time sensing ability in\n  tagged environments", "comments": "6 pages", "journal-ref": "2018 IEEE International Conference on RFID Technology &\n  Application (RFID-TA)", "doi": "10.1109/RFID-TA.2018.8552778", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the concept of X-Vision, an enhanced Augmented Reality (AR)-based\nvisualization tool, with the real-time sensing capability in a tagged\nenvironment. We envision that this type of a tool will enhance the\nuser-environment interaction and improve the productivity in factories,\nsmart-spaces, home & office environments, maintenance/facility rooms and\noperation theatres, etc. In this paper, we describe the design of this\nvisualization system built upon combining the object's pose information\nestimated by the depth camera and the object's ID & physical attributes\ncaptured by the RFID tags. We built a physical prototype of the system\ndemonstrating the projection of 3D holograms of the objects encoded with sensed\ninformation like water-level and temperature of common office/household\nobjects. The paper also discusses the quality metrics used to compare the pose\nestimation algorithms for robust reconstruction of the object's 3D data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 01:34:12 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 17:54:33 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Sun", "Yongbin", ""], ["Kantareddy", "Sai Nithin R.", ""], ["Bhattacharyya", "Rahul", ""], ["Sarma", "Sanjay E.", ""]]}, {"id": "1806.00812", "submitter": "Benjamin Gorman PhD", "authors": "Benjamin M. Gorman", "title": "A Framework for Speechreading Acquisition Tools", "comments": "PhD Thesis, supervised by Dr David R. Flatla", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  At least 360 million people worldwide have disabling hearing loss that\nfrequently causes difficulties in day-to-day conversations. Hearing aids often\nfail to offer enough benefits and have low adoption rates. However, people with\nhearing loss find that speechreading can improve their understanding during\nconversation. Speechreading (often called lipreading) refers to using visual\ninformation about the movements of a speaker's lips, teeth, and tongue to help\nunderstand what they are saying. Speechreading is commonly used by people with\nall severities of hearing loss to understand speech, and people with typical\nhearing also speechread (albeit subconsciously) to help them understand others.\nHowever, speechreading is a skill that takes considerable practice to acquire.\nPublicly-funded speechreading classes are sometimes provided, and have been\nshown to improve speechreading acquisition. However, classes are only provided\nin a handful of countries around the world and students can only practice\neffectively when attending class. Existing tools have been designed to help\nimprove speechreading acquisition, but are often not effective because they\nhave not been designed within the context of contemporary speechreading lessons\nor practice. To address this, in this thesis I present a novel speechreading\nacquisition framework that can be used to design Speechreading Acquisition\nTools (SATs) - a new type of technology to improve speechreading acquisition.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 15:23:44 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Gorman", "Benjamin M.", ""]]}, {"id": "1806.00914", "submitter": "Manoj Reddy Dareddy", "authors": "Manoj Reddy Dareddy, Ariyam Das, Junghoo Cho, Carlo Zaniolo", "title": "How Much Are You Willing to Share? A \"Poker-Styled\" Selective Privacy\n  Preserving Framework for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most industrial recommender systems rely on the popular collaborative\nfiltering (CF) technique for providing personalized recommendations to its\nusers. However, the very nature of CF is adversarial to the idea of user\nprivacy, because users need to share their preferences with others in order to\nbe grouped with like-minded people and receive accurate recommendations. While\nprevious privacy preserving approaches have been successful inasmuch as they\nconcealed user preference information to some extent from a centralized\nrecommender system, they have also, nevertheless, incurred significant\ntrade-offs in terms of privacy, scalability, and accuracy. They are also\nvulnerable to privacy breaches by malicious actors. In light of these\nobservations, we propose a novel selective privacy preserving (SP2) paradigm\nthat allows users to custom define the scope and extent of their individual\nprivacies, by marking their personal ratings as either public (which can be\nshared) or private (which are never shared and stored only on the user device).\nOur SP2 framework works in two steps: (i) First, it builds an initial\nrecommendation model based on the sum of all public ratings that have been\nshared by users and (ii) then, this public model is fine-tuned on each user's\ndevice based on the user private ratings, thus eventually learning a more\naccurate model. Furthermore, in this work, we introduce three different\nalgorithms for implementing an end-to-end SP2 framework that can scale\neffectively from thousands to hundreds of millions of items. Our user survey\nshows that an overwhelming fraction of users are likely to rate much more items\nto improve the overall recommendations when they can control what ratings will\nbe publicly shared with others.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 01:25:06 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Dareddy", "Manoj Reddy", ""], ["Das", "Ariyam", ""], ["Cho", "Junghoo", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1806.00923", "submitter": "Nim Dvir Mr.", "authors": "Nim Dvir, Ruti Gafni", "title": "How Content Volume on Landing Pages Influences Consumer Behavior", "comments": "InSITE 2018- Informing Science and Information Technology Education\n  Conference, La Verne, California", "journal-ref": null, "doi": "10.28945/4016", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Does more information elicit users compliance and engagement, or the other\nway around? This paper explores the relationship between content strategy and\nuser experience (UX). Specifically, we examine how the amount of information\nprovided on marketing web pages, often called landing pages,impact users\nwillingness to provide their e-mail address (a behavior called conversion in\nmarketing terms). We describe the results of two large-scale online experiments\n(n= 535 and n= 27,900) conducted in real-world commercial settings. The\nobserved results indicate a negative correlation between the amount of\ninformation on a web page and users decision-making and engagement.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 02:01:33 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Dvir", "Nim", ""], ["Gafni", "Ruti", ""]]}, {"id": "1806.01041", "submitter": "Francisco Crespo Mr", "authors": "Francisco Crespo Estefan\\'ia Mart\\'in", "title": "Applications for mobile devices focused on support for autism spectrum\n  disorder population and / or people in their immediate environment in their\n  daily lives: a systematic and practical review from a Spanish - speaking\n  perspective", "comments": "16 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study has made a review of scientific publications on\napplications focused on autism, most of them developed for communication,\nsocial behavior and learning, which coincides with what is observed in a\ndigital market that practically lacks scientific validation. The study has also\nfound only 135 of these type of applications with a Spanish version available\n(in a practical sense), developed mostly for daily life of an autistic person\nand/or people from their immediate environment. By using these applications,\nthere are positive results in terms of learning and permanent adoption of\nbehaviors and skills, but it is necessary to deepen research and further\ndevelopment of applications focused on leisure, resources for parents and\nprofessionals, and supporting of autistic adult needs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 10:53:57 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Mart\u00edn", "Francisco Crespo Estefan\u00eda", ""]]}, {"id": "1806.01126", "submitter": "Tobias Hossfeld", "authors": "Tobias Hossfeld, Poul E. Heegaard, Martin Varela, Lea Skorin-Kapov", "title": "Confidence Interval Estimators for MOS Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.HC cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the quantification of QoE, subjects often provide individual rating\nscores on certain rating scales which are then aggregated into Mean Opinion\nScores (MOS). From the observed sample data, the expected value is to be\nestimated. While the sample average only provides a point estimator, confidence\nintervals (CI) are an interval estimate which contains the desired expected\nvalue with a given confidence level. In subjective studies, the number of\nsubjects performing the test is typically small, especially in lab\nenvironments. The used rating scales are bounded and often discrete like the\n5-point ACR rating scale. Therefore, we review statistical approaches in the\nliterature for their applicability in the QoE domain for MOS interval\nestimation (instead of having only a point estimator, which is the MOS). We\nprovide a conservative estimator based on the SOS hypothesis and binomial\ndistributions and compare its performance (CI width, outlier ratio of CI\nviolating the rating scale bounds) and coverage probability with well known CI\nestimators. We show that the provided CI estimator works very well in practice\nfor MOS interval estimators, while the commonly used studentized CIs suffer\nfrom a positive outlier ratio, i.e., CIs beyond the bounds of the rating scale.\nAs an alternative, bootstrapping, i.e., random sampling of the subjective\nratings with replacement, is an efficient CI estimator leading to typically\nsmaller CIs, but lower coverage than the proposed estimator.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 13:59:55 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Hossfeld", "Tobias", ""], ["Heegaard", "Poul E.", ""], ["Varela", "Martin", ""], ["Skorin-Kapov", "Lea", ""]]}, {"id": "1806.01499", "submitter": "Yifan Wu", "authors": "Yifan Wu, Remco Chang, Joseph M. Hellerstein, Eugene Wu", "title": "Facilitating Exploration with Interaction Snapshots under High Latency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latency is, unfortunately, a reality when working with large datasets.\nGuaranteeing imperceptible latency for interactivity is often prohibitively\nexpensive: the application developer may be forced to migrate data processing\nengines or deal with complex error bounds on samples, and to limit the\napplication to users with high network bandwidth. Instead of relying on the\nbackend, we propose a simple UX design---interaction snapshots. Responses of\nrequests from the interactions are asynchronously loaded in \"snapshots\". With\ninteraction snapshots, users can interact concurrently while the snapshots\nload. Our user study participants found it useful not to have to wait for each\nresult and easily navigate to prior snapshots. For latency up to 5 seconds,\nparticipants were able to complete extrema, threshold, and trend identification\ntasks with little negative impact.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 05:17:23 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 16:53:43 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Wu", "Yifan", ""], ["Chang", "Remco", ""], ["Hellerstein", "Joseph M.", ""], ["Wu", "Eugene", ""]]}, {"id": "1806.01526", "submitter": "Piek Vossen", "authors": "Piek Vossen, Selene Baez, Lenka Baj\\v{c}eti\\'c, and Bram Kraaijeveld", "title": "Leolani: a reference machine with a theory of mind for social\n  communication", "comments": "Invited keynote at 21st International Conference on Text, Speech and\n  Dialogue, https://www.tsdconference.org/tsd2018/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our state of mind is based on experiences and what other people tell us. This\nmay result in conflicting information, uncertainty, and alternative facts. We\npresent a robot that models relativity of knowledge and perception within\nsocial interaction following principles of the theory of mind. We utilized\nvision and speech capabilities on a Pepper robot to build an interaction model\nthat stores the interpretations of perceptions and conversations in combination\nwith provenance on its sources. The robot learns directly from what people tell\nit, possibly in relation to its perception. We demonstrate how the robot's\ncommunication is driven by hunger to acquire more knowledge from and on people\nand objects, to resolve uncertainties and conflicts, and to share awareness of\nthe per- ceived environment. Likewise, the robot can make reference to the\nworld and its knowledge about the world and the encounters with people that\nyielded this knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 07:36:36 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Vossen", "Piek", ""], ["Baez", "Selene", ""], ["Baj\u010deti\u0107", "Lenka", ""], ["Kraaijeveld", "Bram", ""]]}, {"id": "1806.02291", "submitter": "Marcos Baez", "authors": "Francisco Ibarra, Marcos Baez, Francesca Fiore, Fabio Casati", "title": "Designing for Co-located and Virtual Social Interactions in Residential\n  Care", "comments": null, "journal-ref": null, "doi": "10.1145/3197391.3205424", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the feasibility and design challenges in supporting\nco-located and virtual social interactions in residential care by building on\nthe practice of reminiscence. Motivated by the challenges of social interaction\nin this context, we first explore the feasibility of a reminiscence-based\nsocial interaction tool designed to stimulate conversation in residential care\nwith different stakeholders. Then, we explore the design challenges in\nsupporting an assisting role in co-located reminiscence sessions, by running\npilot studies with a technology probe. Our findings point to the feasibility of\nthe tool and the willingness of stakeholders to contribute in the process,\nalthough with some skepticism about virtual interactions. The reminiscence\nsessions showed that compromises are needed when designing for both story\ncollection and conversation stimulation, evidencing specific design areas where\nfurther exploration is needed.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 09:35:38 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Ibarra", "Francisco", ""], ["Baez", "Marcos", ""], ["Fiore", "Francesca", ""], ["Casati", "Fabio", ""]]}, {"id": "1806.02425", "submitter": "Todd Murphey", "authors": "Aleksandra Kalinowska, Kathleen Fitzsimons, Julius Dewald, and Todd D\n  Murphey", "title": "Online User Assessment for Minimal Intervention During Task-Based\n  Robotic Assistance", "comments": "10 pages", "journal-ref": "Robotics: Science and Systems (RSS), 2018", "doi": "10.15607/RSS.2018.XIV.046", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel criterion for evaluating user input for human-robot\ninterfaces for known tasks. We use the mode insertion gradient (MIG)---a tool\nfrom hybrid control theory---as a filtering criterion that instantaneously\nassesses the impact of user actions on a dynamic system over a time window into\nthe future. As a result, the filter is permissive to many chosen strategies,\nminimally engaging, and skill-sensitive---qualities desired when evaluating\nhuman actions. Through a human study with 28 healthy volunteers, we show that\nthe criterion exhibits a low, but significant, negative correlation between\nskill level, as estimated from task-specific measures in unassisted trials, and\nthe rate of controller intervention during assistance. Moreover, a MIG-based\nfilter can be utilized to create a shared control scheme for training or\nassistance. In the human study, we observe a substantial training effect when\nusing a MIG-based filter to perform cart-pendulum inversion, particularly when\ncomparing improvement via the RMS error measure. Using simulation of a\ncontrolled spring-loaded inverted pendulum (SLIP) as a test case, we observe\nthat the MIG criterion could be used for assistance to guarantee either task\ncompletion or safety of a joint human-robot system, while maintaining the\nsystem's flexibility with respect to user-chosen strategies.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 20:59:48 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Kalinowska", "Aleksandra", ""], ["Fitzsimons", "Kathleen", ""], ["Dewald", "Julius", ""], ["Murphey", "Todd D", ""]]}, {"id": "1806.02689", "submitter": "Lorenzo Sabattini", "authors": "Lorenzo Sabattini, Valeria Villani, Julia N. Czerniak, Frieder Loch,\n  Alexander Mertens, Birgit Vogel-Heuser, Cesare Fantuzzi", "title": "Methodological Approach for the Evaluation of an Adaptive and Assistive\n  Human-Machine System", "comments": null, "journal-ref": "Proceedings of the 14th IEEE International Conference on\n  Automation Science and Engineering (CASE 2018)", "doi": "10.1109/COASE.2018.8560574", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing complexity of modern industrial automatic and robotic\nsystems, an increasing burden is put on the operators, who are requested to\nsupervise and interact with such complex systems, typically under challenging\nand stressful conditions. To overcome this issue, it is necessary to adopt a\nresponsible approach based on the anthropocentric design methodology, such that\nmachines adapt to the humans capabilities. Moving along these lines, a\nmethodological approach called MATE was introduced in [1], which consists in\ndevising complex automatic or robotic solutions that measure current operator's\nstatus, adapting the interaction accordingly, and providing her/him with proper\ntraining to improve the interaction and learn lacking skills and expertise. In\nthis paper we propose an evaluation and validation procedure to guarantee the\nachievement of the requirements of a MATE system.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 14:03:32 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Sabattini", "Lorenzo", ""], ["Villani", "Valeria", ""], ["Czerniak", "Julia N.", ""], ["Loch", "Frieder", ""], ["Mertens", "Alexander", ""], ["Vogel-Heuser", "Birgit", ""], ["Fantuzzi", "Cesare", ""]]}, {"id": "1806.02720", "submitter": "Ryan Wesslen", "authors": "Ryan Wesslen, Sashank Santhanam, Alireza Karduni, Isaac Cho, Samira\n  Shaikh, Wenwen Dou", "title": "Anchored in a Data Storm: How Anchoring Bias Can Affect User Strategy,\n  Confidence, and Decisions in Visual Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive biases have been shown to lead to faulty decision-making. Recent\nresearch has demonstrated that the effect of cognitive biases, anchoring bias\nin particular, transfers to information visualization and visual analytics.\nHowever, it is still unclear how users of visual interfaces can be anchored and\nthe impact of anchoring on user performance and decision-making process. To\ninvestigate, we performed two rounds of between-subjects, in-laboratory\nexperiments with 94 participants to analyze the effect of visual anchors and\nstrategy cues in decision-making with a visual analytic system that employs\ncoordinated multiple view design. The decision-making task is identifying\nmisinformation from Twitter news accounts. Participants were randomly assigned\none of three treatment groups (including control) in which participant training\nprocesses were modified. Our findings reveal that strategy cues and visual\nanchors (scenario videos) can significantly affect user activity, speed,\nconfidence, and, under certain circumstances, accuracy. We discuss the\nimplications of our experiment results on training users how to use a newly\ndeveloped visual interface. We call for more careful consideration into how\nvisualization designers and researchers train users to avoid unintentionally\nanchoring users and thus affecting the end result.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 15:09:57 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Wesslen", "Ryan", ""], ["Santhanam", "Sashank", ""], ["Karduni", "Alireza", ""], ["Cho", "Isaac", ""], ["Shaikh", "Samira", ""], ["Dou", "Wenwen", ""]]}, {"id": "1806.02973", "submitter": "Byungjoo Lee", "authors": "Eunji Park and Byungjoo Lee", "title": "An Intermittent Click Planning Model", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1145/3313831.3376725", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pointing is the task of tracking a target with a pointer and confirming the\ntarget selection through a click action when the pointer is positioned within\nthe target. Little is known about the mechanism by which users plan and execute\nthe click action in the middle of the target tracking process. The Intermittent\nClick Planning model proposed in this study describes the process by which\nusers plan and execute optimal click actions, from which the model predicts the\npointing error rates. In two studies in which users pointed to a stationary\ntarget and a moving target, the model proved to accurately predict the pointing\nerror rates (R2 = 0.992 and 0.985, respectively). The model has also\nsuccessfully identified differences in cognitive characteristics among\nfirst-person shooter game players.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 05:17:18 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 02:37:52 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 03:00:03 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Park", "Eunji", ""], ["Lee", "Byungjoo", ""]]}, {"id": "1806.03023", "submitter": "Lorenzo Sabattini", "authors": "Valeria Villani, Lorenzo Sabattini, Alessio Levratti, Cesare Fantuzzi", "title": "An Industrial Social Network for Sharing Knowledge Among Operators", "comments": null, "journal-ref": "Proceedings of the 16th IFAC Symposium on Information Control\n  Problems in Manufacturing (INCOM 2018)", "doi": "10.1016/j.ifacol.2018.08.233", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing complexity of modern automatic machines typically used\nin several industrial applications, the need for assistive technologies is\nbecoming very relevant. Typical approaches consist in designing advanced and\nadaptive human-machine interfaces (HMIs) that can be effectively used by any\noperator and that provide guided procedures for the most common situations.\nHowever, when dealing with complex systems, infrequent and unforeseen\nsituations may happen, whose solution require the experience owned by a limited\nnumber of skilled operators. To this end, in this paper we propose an\nindustrial social network concept to allow an effective exchange of information\namong the operators and to facilitate the solution of unforeseen events, such\nas unscheduled maintenance activities or troubleshooting.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 08:39:05 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Villani", "Valeria", ""], ["Sabattini", "Lorenzo", ""], ["Levratti", "Alessio", ""], ["Fantuzzi", "Cesare", ""]]}, {"id": "1806.03192", "submitter": "Emilia Gomez", "authors": "Emilia G\\'omez, Carlos Castillo, Vicky Charisi, Ver\\'onica Dahl,\n  Gustavo Deco, Blagoj Delipetrev, Nicole Dewandre, Miguel \\'Angel\n  Gonz\\'alez-Ballester, Fabien Gouyon, Jos\\'e Hern\\'andez-Orallo, Perfecto\n  Herrera, Anders Jonsson, Ansgar Koene, Martha Larson, Ram\\'on L\\'opez de\n  M\\'antaras, Bertin Martens, Marius Miron, Rub\\'en Moreno-Bote, Nuria Oliver,\n  Antonio Puertas Gallardo, Heike Schweitzer, Nuria Sebastian, Xavier Serra,\n  Joan Serr\\`a, Song\\\"ul Tolan, Karina Vold", "title": "Assessing the impact of machine intelligence on human behaviour: an\n  interdisciplinary endeavour", "comments": "Proceedings of 1st HUMAINT (Human Behaviour and Machine Intelligence)\n  workshop, Barcelona, Spain, March 5-6, 2018, edited by European Commission,\n  Seville, 2018, JRC111773\n  https://ec.europa.eu/jrc/communities/community/humaint/document/assessing-impact-machine-intelligence-human-behaviour-interdisciplinary.\n  arXiv admin note: text overlap with arXiv:1409.3097 by other authors", "journal-ref": null, "doi": null, "report-no": "JRC111773", "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This document contains the outcome of the first Human behaviour and machine\nintelligence (HUMAINT) workshop that took place 5-6 March 2018 in Barcelona,\nSpain. The workshop was organized in the context of a new research programme at\nthe Centre for Advanced Studies, Joint Research Centre of the European\nCommission, which focuses on studying the potential impact of artificial\nintelligence on human behaviour. The workshop gathered an interdisciplinary\ngroup of experts to establish the state of the art research in the field and a\nlist of future research challenges to be addressed on the topic of human and\nmachine intelligence, algorithm's potential impact on human cognitive\ncapabilities and decision making, and evaluation and regulation needs. The\ndocument is made of short position statements and identification of challenges\nprovided by each expert, and incorporates the result of the discussions carried\nout during the workshop. In the conclusion section, we provide a list of\nemerging research topics and strategies to be addressed in the near future.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 15:20:09 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["G\u00f3mez", "Emilia", ""], ["Castillo", "Carlos", ""], ["Charisi", "Vicky", ""], ["Dahl", "Ver\u00f3nica", ""], ["Deco", "Gustavo", ""], ["Delipetrev", "Blagoj", ""], ["Dewandre", "Nicole", ""], ["Gonz\u00e1lez-Ballester", "Miguel \u00c1ngel", ""], ["Gouyon", "Fabien", ""], ["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""], ["Herrera", "Perfecto", ""], ["Jonsson", "Anders", ""], ["Koene", "Ansgar", ""], ["Larson", "Martha", ""], ["de M\u00e1ntaras", "Ram\u00f3n L\u00f3pez", ""], ["Martens", "Bertin", ""], ["Miron", "Marius", ""], ["Moreno-Bote", "Rub\u00e9n", ""], ["Oliver", "Nuria", ""], ["Gallardo", "Antonio Puertas", ""], ["Schweitzer", "Heike", ""], ["Sebastian", "Nuria", ""], ["Serra", "Xavier", ""], ["Serr\u00e0", "Joan", ""], ["Tolan", "Song\u00fcl", ""], ["Vold", "Karina", ""]]}, {"id": "1806.03257", "submitter": "Barbara Solenthaler", "authors": "Barbara Solenthaler, Severin Klingler, Tanja K\\\"aser, Markus Gross", "title": "Ten Years of Research on Intelligent Educational Games for Learning\n  Spelling and Mathematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present our findings from ten years of research on\nintelligent educational games. We discuss the architecture of our training\nenvironments for learning spelling and mathematics, and specifically focus on\nthe representation of the content and the controller that enables personalized\ntrainings. We first show the multi-modal representation that reroutes\ninformation through multiple perceptual cues and discuss the game structure. We\nthen present the data-driven student model that is used for a personalized,\nadaptive presentation of the content. We further leverage machine learning for\nanalytics and visualization tools targeted at teachers and experts. A large\ndata set consisting of training sessions of more than 20,000 children allows\nstatistical interpretations and insights into the nature of learning.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 07:55:15 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Solenthaler", "Barbara", ""], ["Klingler", "Severin", ""], ["K\u00e4ser", "Tanja", ""], ["Gross", "Markus", ""]]}, {"id": "1806.03671", "submitter": "Aaron M. Roth", "authors": "Aaron M. Roth, Umang Bhatt, Tamara Amin, Afsaneh Doryab, Fei Fang,\n  Manuela Veloso", "title": "The Impact of Humanoid Affect Expression on Human Behavior in a\n  Game-Theoretic Setting", "comments": "presented at 1st Workshop on Humanizing AI (HAI) at IJCAI'18 in\n  Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of robot and other intelligent and autonomous\nagents, how a human could be influenced by a robot's expressed mood when making\ndecisions becomes a crucial question in human-robot interaction. In this pilot\nstudy, we investigate (1) in what way a robot can express a certain mood to\ninfluence a human's decision making behavioral model; (2) how and to what\nextent the human will be influenced in a game theoretic setting. More\nspecifically, we create an NLP model to generate sentences that adhere to a\nspecific affective expression profile. We use these sentences for a humanoid\nrobot as it plays a Stackelberg security game against a human. We investigate\nthe behavioral model of the human player.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 15:20:20 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Roth", "Aaron M.", ""], ["Bhatt", "Umang", ""], ["Amin", "Tamara", ""], ["Doryab", "Afsaneh", ""], ["Fang", "Fei", ""], ["Veloso", "Manuela", ""]]}, {"id": "1806.03957", "submitter": "Aleksandr Chuklin", "authors": "Aleksandr Chuklin, Aliaksei Severyn, Johanne Trippas, Enrique\n  Alfonseca, Hanna Silen and Damiano Spina", "title": "Prosody Modifications for Question-Answering in Voice-Only Settings", "comments": "Shorter version of this paper was accepted to CLEF'2019, Lugano,\n  Switzerland. The final authenticated version is available online at\n  https://doi.org/10.1007/978-3-030-28577-7_12", "journal-ref": "Lecture Notes in Computer Science, vol 11696 CLEF 2019", "doi": "10.1007/978-3-030-28577-7_12", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many popular form factors of digital assistants---such as Amazon Echo, Apple\nHomepod, or Google Home---enable the user to hold a conversation with these\nsystems based only on the speech modality. The lack of a screen presents unique\nchallenges. To satisfy the information need of a user, the presentation of the\nanswer needs to be optimized for such voice-only interactions. In this paper,\nwe propose a task of evaluating the usefulness of audio transformations (i.e.,\nprosodic modifications) for voice-only question answering. We introduce a\ncrowdsourcing setup where we evaluate the quality of our proposed modifications\nalong multiple dimensions corresponding to the informativeness, naturalness,\nand ability of the user to identify key parts of the answer. We offer a set of\nprosodic modifications that highlight potentially important parts of the answer\nusing various acoustic cues. Our experiments show that some of these prosodic\nmodifications lead to better comprehension at the expense of only slightly\ndegraded naturalness of the audio.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 13:25:23 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 09:41:51 GMT"}, {"version": "v3", "created": "Thu, 4 Jul 2019 10:21:55 GMT"}, {"version": "v4", "created": "Wed, 2 Oct 2019 14:18:34 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Chuklin", "Aleksandr", ""], ["Severyn", "Aliaksei", ""], ["Trippas", "Johanne", ""], ["Alfonseca", "Enrique", ""], ["Silen", "Hanna", ""], ["Spina", "Damiano", ""]]}, {"id": "1806.04212", "submitter": "Lasya Venneti", "authors": "Lasya Venneti and Aniket Alam", "title": "How Curiosity can be modeled for a Clickbait Detector", "comments": "This work was presented at 1st Workshop on Humanizing AI (HAI) at\n  IJCAI'18 in Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of continually evolving digital technologies and the proliferation\nof communications and content has now been widely acknowledged to be central to\nunderstanding our world. What is less acknowledged is that this is based on the\nsuccessful arousing of curiosity both at the collective and individual levels.\nAdvertisers, communication professionals and news editors are in constant\ncompetition to capture attention of the digital population perennially shifty\nand distracted. This paper, tries to understand how curiosity works in the\ndigital world by attempting the first ever work done on quantifying human\ncuriosity, basing itself on various theories drawn from humanities and social\nsciences. Curious communication pushes people to spot, read and click the\nmessage from their social feed or any other form of online presentation. Our\napproach focuses on measuring the strength of the stimulus to generate reader\ncuriosity by using unsupervised and supervised machine learning algorithms, but\nis also informed by philosophical, psychological, neural and cognitive studies\non this topic. Manually annotated news headlines - clickbaits - have been\nselected for the study, which are known to have drawn huge reader response. A\nbinary classifier was developed based on human curiosity (unlike the work done\nso far using words and other linguistic features). Our classifier shows an\naccuracy of 97% . This work is part of the research in computational humanities\non digital politics quantifying the emotions of curiosity and outrage on\ndigital media.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 19:39:42 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Venneti", "Lasya", ""], ["Alam", "Aniket", ""]]}, {"id": "1806.04236", "submitter": "Barara Gi\\.zycka Ms", "authors": "Barbara Gi\\.zycka, Grzegorz J. Nalepa and Pawe{\\l} Jemio{\\l}o", "title": "\"AIded with emotions\" - a new design approach towards affective computer\n  systems", "comments": "Paper presented at Humanizing AI Workshop at IJCAI-ECAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As technologies become more and more pervasive, there is a need for\nconsidering the affective dimension of interaction with computer systems to\nmake them more human-like. Current demands for this matter include accurate\nemotion recognition, reliable emotion modeling, and use of unobtrusive, easily\naccessible and preferably wearable measurement devices. While AI methods\nprovide many possibilities for better affective information processing, it is\nnot a common scenario for both emotion recognition and modeling to be\nintegrated in the design phase. To address this concern, we propose a new\napproach based on affective design patterns in the context of video games,\ntogether with summary of experiments conducted to test the preliminary\nhypotheses.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 20:44:01 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Gi\u017cycka", "Barbara", ""], ["Nalepa", "Grzegorz J.", ""], ["Jemio\u0142o", "Pawe\u0142", ""]]}, {"id": "1806.04298", "submitter": "Malay Bhattacharyya", "authors": "Auroshikha Mandal, Mehul Agarwal and Malay Bhattacharyya", "title": "Collective Story Writing through Linking Images", "comments": "Works-in-Progress, Sixth AAAI Conference on Human Computation and\n  Crowdsourcing (HCOMP 2018), Zurich, Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative creativity is the approach of employing crowd to accomplish\ncreative tasks. In this paper, we present a collaborative crowdsourcing\nplatform for writing stories by means of connecting a series of `images'. These\nconnected images are termed as Image Chains, reflecting successive scenarios.\nUsers can either start or extend an Image Chain by uploading their own image or\nchoosing from the available ones. These users are allowed to pen their stories\nfrom the Image Chains. Finally, stories get published based on the number of\nvotes obtained. This provides an organized framework of story writing unlike\nmost of the state-of-the-art collaborative editing platforms. Our experiments\non 25 contributors highlight their interest in growing shorter Image Chains but\nvoting longer Image Chains.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 01:52:41 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 01:27:32 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Mandal", "Auroshikha", ""], ["Agarwal", "Mehul", ""], ["Bhattacharyya", "Malay", ""]]}, {"id": "1806.04471", "submitter": "Frank Glavin", "authors": "Anthony M. Colwell and Frank G. Glavin", "title": "Colwell's Castle Defence: A Custom Game Using Dynamic Difficulty\n  Adjustment to Increase Player Enjoyment", "comments": "25th Irish Conference on Artificial Intelligence and Cognitive\n  Science At: Dublin Institute of Technology (http://ceur-ws.org/Vol-2086/)", "journal-ref": "In proceedings of AICS, Dublin Institute of Technology, pp.\n  275-282 (2017)", "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Difficulty Adjustment (DDA) is a mechanism used in video games that\nautomatically tailors the individual gaming experience to match an appropriate\ndifficulty setting. This is generally achieved by removing pre-defined\ndifficulty tiers such as Easy, Medium and Hard; and instead concentrates on\nbalancing the gameplay to match the challenge to the individual's abilities.\nThe work presented in this paper examines the implementation of DDA in a custom\nsurvival game developed by the author, namely Colwell's Castle Defence. The\npremise of this arcade-style game is to defend a castle from hordes of oncoming\nenemies. The AI system that we developed adjusts the enemy spawn rate based on\nthe current performance of the player. Specifically, we read the Player Health\nand Gate Health at the end of each level and then assign the player with an\nappropriate difficulty tier for the proceeding level. We tested the impact of\nour technique on thirty human players and concluded, based on questionnaire\nfeedback, that enabling the technique led to more enjoyable gameplay.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 12:46:18 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Colwell", "Anthony M.", ""], ["Glavin", "Frank G.", ""]]}, {"id": "1806.04724", "submitter": "Adnan Mahmood", "authors": "Adnan Mahmood, Bernard Butler and Brendan Jennings", "title": "Potential of Augmented Reality for Intelligent Transportation Systems", "comments": "In: Lee N. (eds) Encyclopedia of Computer Graphics and Games.\n  Springer, Cham, 2018", "journal-ref": null, "doi": "10.1007/978-3-319-08234-9_274-1", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid advances in wireless communication technologies coupled with ongoing\nmassive development in vehicular networking standards and innovations in\ncomputing, sensing, and analytics have paved the way for intelligent\ntransportation systems (ITS) to develop rapidly in the near future. ITS\nprovides a complete solution for the efficient and intelligent management of\nreal-time traffic, wherein sensory data is collected from within the vehicles\n(i.e., via their onboard units) as well as data exchanged between the vehicles,\nbetween the vehicles and their supporting roadside infrastructure/network,\namong the vehicles and vulnerable pedestrians, subsequently paving the way for\nthe realization of the futuristic Internet of Vehicles. The traditional intent\nof an ITS system is to detect, monitor, control, and subsequently reduce\ntraffic congestion based on a real-time analysis of the data pertinent to\ncertain patterns of the road traffic, including traffic density at a\ngeographical area of interest, precise velocity of vehicles, current and\npredicted travelling trajectories and times, etc. However, merely relying on an\nITS framework is not an optimal solution. In case of dense traffic\nenvironments, where communication broadcasts from hundreds of thousands of\nvehicles could potentially choke the entire network (and so could lead to fatal\naccidents in the case of autonomous vehicles that depend on reliable\ncommunications for their operational safety), a fall back to the traditional\ndecentralized vehicular ad hoc network (VANET) approach becomes necessary. It\nis therefore of critical importance to enhance the situational awareness of\nvehicular drivers so as to enable them to make quick but well-founded manual\ndecisions in such safety-critical situations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 03:52:07 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Mahmood", "Adnan", ""], ["Butler", "Bernard", ""], ["Jennings", "Brendan", ""]]}, {"id": "1806.04776", "submitter": "Reuben Brasher", "authors": "Elmar H. Langholz, Reuben Brasher", "title": "Real-time on-device nod and shake recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss methods for teaching systems to identify gestures such as head nod\nand shake. We use iPhone X depth camera to gather data and later use similar\ndata as input for a working app. These methods have proved robust for training\nwith limited datasets and thus we make the argument that similar methods could\nbe adapted to learn other human to human non-verbal gestures. We showcase how\nto augment Euler angle gesture sequences to train models with a relatively\nlarge number of parameters such as LSTM and GRU and gain better performance\nthan reported for smaller models such as HMM. In the examples here, we\ndemonstrate how to train such models with Keras and run the resulting models\nreal time on device with CoreML.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 21:47:13 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Langholz", "Elmar H.", ""], ["Brasher", "Reuben", ""]]}, {"id": "1806.04952", "submitter": "Markus Schr\\\"oder", "authors": "Markus Schr\\\"oder and Christian Jilek and J\\\"orn Hees and Andreas\n  Dengel", "title": "Towards Semantically Enhanced Data Understanding", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of machine learning, data understanding is the practice of\ngetting initial insights in unknown datasets. Such knowledge-intensive tasks\nrequire a lot of documentation, which is necessary for data scientists to grasp\nthe meaning of the data. Usually, documentation is separate from the data in\nvarious external documents, diagrams, spreadsheets and tools which causes\nconsiderable look up overhead. Moreover, other supporting applications are not\nable to consume and utilize such unstructured data. That is why we propose a\nmethodology that uses a single semantic model that interlinks data with its\ndocumentation. Hence, data scientists are able to directly look up the\nconnected information about the data by simply following links. Equally, they\ncan browse the documentation which always refers to the data. Furthermore, the\nmodel can be used by other approaches providing additional support, like\nsearching, comparing, integrating or visualizing data. To showcase our approach\nwe also demonstrate an early prototype.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 11:19:33 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Schr\u00f6der", "Markus", ""], ["Jilek", "Christian", ""], ["Hees", "J\u00f6rn", ""], ["Dengel", "Andreas", ""]]}, {"id": "1806.05426", "submitter": "Lynsay Shepherd", "authors": "Lynsay A. Shepherd, Karen Renaud", "title": "How to design browser security and privacy alerts", "comments": "Symposium on Digital Behaviour Intervention for Cyber Security, AISB\n  2018 Convention", "journal-ref": "AISB 2018: Symposium on Digital Behaviour Intervention for Cyber\n  Security (pp. 21-28). Society for the Study of Artificial Intelligence and\n  Simulation for Behaviour (AISB)", "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to design browser security and privacy alerts so as to\nmaximise their value to the end user, and their efficacy in terms of\ncommunicating risk. We derived a list of design guidelines from the research\nliterature by carrying out a systematic review. We analysed the papers both\nquantitatively and qualitatively to arrive at a comprehensive set of\nguidelines. Our findings aim to to provide designers and developers with\nguidance as to how to construct privacy and security alerts. We conclude by\nproviding an alert template,highlighting its adherence to the derived\nguidelines.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 09:18:42 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Shepherd", "Lynsay A.", ""], ["Renaud", "Karen", ""]]}, {"id": "1806.05660", "submitter": "\\'Angel Alexander Cabrera", "authors": "\\'Angel Alexander Cabrera, Fred Hohman, Jason Lin, Duen Horng Chau", "title": "Interactive Classification for Deep Learning Interpretation", "comments": "Presented as a demo at CVPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interactive system enabling users to manipulate images to\nexplore the robustness and sensitivity of deep learning image classifiers.\nUsing modern web technologies to run in-browser inference, users can remove\nimage features using inpainting algorithms and obtain new classifications in\nreal time, which allows them to ask a variety of \"what if\" questions by\nexperimentally modifying images and seeing how the model reacts. Our system\nallows users to compare and contrast what image regions humans and machine\nlearning models use for classification, revealing a wide range of surprising\nresults ranging from spectacular failures (e.g., a \"water bottle\" image becomes\na \"concert\" when removing a person) to impressive resilience (e.g., a \"baseball\nplayer\" image remains correctly classified even without a glove or base). We\ndemonstrate our system at The 2018 Conference on Computer Vision and Pattern\nRecognition (CVPR) for the audience to try it live. Our system is open-sourced\nat https://github.com/poloclub/interactive-classification. A video demo is\navailable at https://youtu.be/llub5GcOF6w.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 17:36:02 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 20:57:55 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Cabrera", "\u00c1ngel Alexander", ""], ["Hohman", "Fred", ""], ["Lin", "Jason", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1806.05949", "submitter": "Eug\\'enio Rodrigues", "authors": "Marco S. Fernandes, and E. Rodrigues, and Ad\\'elio R. Gaspar, and\n  \\'Alvaro Gomes", "title": "An Aiding Tool for Building Design Generation, Thermal Assessment and\n  Optimization -- EnergyPlus Interaction Overview", "comments": "7 pages, 6 figures, CYTEF 2018 VII Congreso Ib\\'erico, Ciencias y\n  t\\'ecnicas del fr\\'io, 19-21 June 2018, Valencia, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A building design aiding tool for space allocation and thermal performance\noptimization is being developed to help practitioners during the building space\nplanning phase, predicting how it will behave regarding energy consumption and\nthermal comfort. The tool evaluates, ranks, and optimizes generated floor plans\naccording to thermal performance criteria, using the dynamic simulation program\nEnergyPlus. The tool is currently able to use a wide variety of EnergyPlus\nobjects, allowing for various template and detailed HVAC, DHW, and thermal and\nelectrical energy production systems and components, as well as numerous\ninternal gains types, construction elements and energy saving controls, to be\naccounted for and simulated in the generated buildings. This paper presents the\ntool overall concept as well as the main features regarding dynamic simulation.\nSome performance results are presented for distinct systems to illustrate the\nuse and potential of the tool.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 11:45:19 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Fernandes", "Marco S.", ""], ["Rodrigues", "E.", ""], ["Gaspar", "Ad\u00e9lio R.", ""], ["Gomes", "\u00c1lvaro", ""]]}, {"id": "1806.05983", "submitter": "Yuan Liang", "authors": "Yuan Liang", "title": "Online Variant of Parcel Allocation in Last-mile Delivery", "comments": "15 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the problem of last-mile delivery, where a large pool of\ncitizen crowd-workers are hired to perform a variety of location-specific urban\nlogistics parcel delivering tasks. Current approaches focus on offline\nscenarios, where all the spatio temporal information of parcels and workers are\ngiven. However, the offline scenarios can be impractical since parcels and\nworkers appear dynamically in real applications, and their information is\nunknown in advance. In this paper, in order to solve the shortcomings of the\noffline setting, we first formalize the online parcel allocation in last-mile\ndelivery problem, where all parcels were put in pop-stations in advance, while\nworkers arrive dynamically. Then we propose an algorithm which provides\ntheoretical guarantee for the parcel allocation in last-mile delivery. Finally,\nwe verify the effectiveness and efficiency of the proposed method through\nextensive experiments on real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:25:28 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 03:01:15 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 16:03:41 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Liang", "Yuan", ""]]}, {"id": "1806.06084", "submitter": "Ramik Sadana", "authors": "Ramik Sadana, Meeshu Agnihotri, John Stasko", "title": "Touching Data: A Discoverability-based Evaluation of a Visualization\n  Interface for Tablet Computers", "comments": "10 pages, 3 figures, 7 tabels", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a number of touch-based visualization systems have appeared in recent\nyears, relatively little work has been done to evaluate these systems. The\nprevailing methods compare these systems to desktop-class applications or\nutilize traditional training-based usability studies. We argue that existing\nstudies, while useful, fail to address a key aspect of mobile application usage\n- initial impression and discoverability-driven usability. Over the past few\nyears, we have developed a tablet-based visualization system, Tangere, for\nanalyzing tabular data in a multiple coordinated view configuration. This\narticle describes a discoverability-based user study of Tangere in which the\nsystem is compared to a commercially available visualization system for tablets\n- Tableau's Vizable. The study highlights aspects of each system's design that\nresonate with study participants, and we reflect upon those findings to\nidentify design principles for future tablet-based data visualization systems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 18:17:34 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Sadana", "Ramik", ""], ["Agnihotri", "Meeshu", ""], ["Stasko", "John", ""]]}, {"id": "1806.06257", "submitter": "Gal Cohensius", "authors": "Gal Cohensius, Omer Ben Porat, Reshef Meir, Ofra Amir", "title": "Efficient Crowdsourcing via Proxy Voting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing platforms offer a way to label data by aggregating answers of\nmultiple unqualified workers. We introduce a \\textit{simple} and \\textit{budget\nefficient} crowdsourcing method named Proxy Crowdsourcing (PCS). PCS collects\nanswers from two sets of workers: \\textit{leaders} (a.k.a proxies) and\n\\textit{followers}. Each leader completely answers the survey while each\nfollower answers only a small subset of it. We then weigh every leader\naccording to the number of followers to which his answer are closest, and\naggregate the answers of the leaders using any standard aggregation method\n(e.g., Plurality for categorical labels or Mean for continuous labels). We\ncompare empirically the performance of PCS to unweighted aggregation, keeping\nthe total number of questions (the budget) fixed. We show that PCS improves the\naccuracy of aggregated answers across several datasets, both with categorical\nand continuous labels. Overall, our suggested method improves accuracy while\nbeing simple and easy to implement.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 16:01:01 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Cohensius", "Gal", ""], ["Porat", "Omer Ben", ""], ["Meir", "Reshef", ""], ["Amir", "Ofra", ""]]}, {"id": "1806.06454", "submitter": "Bilal Farooq", "authors": "Anae Sobhani and Bilal Farooq", "title": "Impact of Smartphone Distraction on Pedestrians' Crossing Behaviour: An\n  Application of Head-Mounted Immersive Virtual Reality", "comments": "Sobhani, A., Farooq, B. (2018) Impact of Smartphone Distraction on\n  Pedestrians' Crossing Behaviour: An Application of Head-Mounted Immersive\n  Virtual Reality, Accepted in: Transportation Research Part: Traffic\n  Psychology and Behaviour", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel head-mounted virtual immersive/interactive reality environment (VIRE)\nis utilized to evaluate the behaviour of participants in three pedestrian road\ncrossing conditions while 1) not distracted, 2) distracted with a smartphone,\nand 3) distracted with a smartphone with a virtually implemented safety measure\non the road. Forty-two volunteers participated in our research who completed\nthirty successful (complete crossing) trials in blocks of ten trials for each\ncrossing condition. For the two distracted conditions, pedestrians are engaged\nin a maze-solving game on a virtual smartphone, while at the same time checking\nthe traffic for a safe crossing gap. For the proposed safety measure, smart\nflashing and color changing LED lights are simulated on the crosswalk to warn\nthe distracted pedestrian who initiates crossing. Surrogate safety measures as\nwell as speed information and distraction attributes such as direction and\norientation of participant's head were collected and evaluated by employing a\nMultinomial Logit (MNL) model. Results from the model indicate that females\nhave more dangerous crossing behaviour especially in distracted conditions;\nhowever, the smart LED treatment reduces this negative impact. Moreover, the\nnumber of times and the percentage of duration the head was facing the\nsmartphone during a trial and a waiting time respectively increase the\npossibility of unsafe crossings; though, the proposed treatment reduces the\nsafety crossing rate. Hence, our study shows that the smart LED light safety\ntreatment indeed improves the safety of distracted pedestrians and enhances the\nsuccessful crossing rate.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 21:54:11 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Sobhani", "Anae", ""], ["Farooq", "Bilal", ""]]}, {"id": "1806.06670", "submitter": "Lynsay Shepherd", "authors": "Karen Renaud, Lynsay A. Shepherd", "title": "How to Make Privacy Policies both GDPR-Compliant and Usable", "comments": "8 pages, 3 figures. Accepted in IEEE CyberSA 2018 Proceedings", "journal-ref": null, "doi": "10.1109/CyberSA.2018.8551442", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is important for organisations to ensure that their privacy policies are\nGeneral Data Protection Regulation (GDPR) compliant, and this has to be done by\nthe May 2018 deadline. However, it is also important for these policies to be\ndesigned with the needs of the human recipient in mind. We carried out an\ninvestigation to find out how best to achieve this.\n  We commenced by synthesising the GDPR requirements into a checklist-type\nformat. We then derived a list of usability design guidelines for privacy\nnotifications from the research literature. We augmented the recommendations\nwith other findings reported in the research literature, in order to confirm\nthe guidelines. We conclude by providing a usable and GDPR-compliant privacy\npolicy template for the benefit of policy writers.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 13:41:53 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Renaud", "Karen", ""], ["Shepherd", "Lynsay A.", ""]]}, {"id": "1806.06771", "submitter": "Christoph Anderson", "authors": "Christoph Anderson, Isabel H\\\"ubener, Ann-Kathrin Seipp, Sandra Ohly,\n  Klaus David, Veljko Pejovic", "title": "A Survey of Attention Management Systems in Ubiquitous Computing\n  Environments", "comments": "27 pages, 7 figures", "journal-ref": "Proceedings of the ACM on Interactive, Mobile, Wearable and\n  Ubiquitous Technologies, vol. 2, no. 2, pp. 58:1-58:27, June 2018", "doi": "10.1145/3214261", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's information and communication devices provide always-on connectivity,\ninstant access to an endless repository of information, and represent the most\ndirect point of contact to almost any person in the world. Despite these\nadvantages, devices such as smartphones or personal computers lead to the\nphenomenon of attention fragmentation, continuously interrupting individuals'\nactivities and tasks with notifications. Attention management systems aim to\nprovide active support in such scenarios, managing interruptions, for example,\nby postponing notifications to opportune moments for information delivery. In\nthis article, we review attention management system research with a particular\nfocus on ubiquitous computing environments. We first examine cognitive theories\nof attention and extract guidelines for practical attention management systems.\nMathematical models of human attention are at the core of these systems, and in\nthis article, we review sensing and machine learning techniques that make such\nmodels possible. We then discuss design challenges towards the implementation\nof such systems, and finally, we investigate future directions in this area,\npaving the way for new approaches and systems supporting users in their\nattention management.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:27:58 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Anderson", "Christoph", ""], ["H\u00fcbener", "Isabel", ""], ["Seipp", "Ann-Kathrin", ""], ["Ohly", "Sandra", ""], ["David", "Klaus", ""], ["Pejovic", "Veljko", ""]]}, {"id": "1806.06905", "submitter": "Lynsay Shepherd", "authors": "Lynsay A. Shepherd, Jacqueline Archibald", "title": "Security Awareness and Affective Feedback: Categorical Behaviour vs.\n  Reported Behaviour", "comments": "6 pages, 1 figure. Accepted in IEEE CyberSA 2017 Proceedings", "journal-ref": null, "doi": "10.1109/CyberSA.2017.8073387", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A lack of awareness surrounding secure online behaviour can lead to\nend-users, and their personal details becoming vulnerable to compromise. This\npaper describes an ongoing research project in the field of usable security,\nexamining the relationship between end-user-security behaviour, and the use of\naffective feedback to educate end-users. Part of the aforementioned research\nproject considers the link between categorical information users reveal about\nthemselves online, and the information users believe, or report that they have\nrevealed online. The experimental results confirm a disparity between\ninformation revealed, and what users think they have revealed, highlighting a\ndeficit in security awareness. Results gained in relation to the affective\nfeedback delivered are mixed, indicating limited short-term impact. Future work\nseeks to perform a long-term study, with the view that positive behavioural\nchanges may be reflected in the results as end-users become more knowledgeable\nabout security awareness.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 19:57:04 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Shepherd", "Lynsay A.", ""], ["Archibald", "Jacqueline", ""]]}, {"id": "1806.07055", "submitter": "Guohao Lan", "authors": "Guohao Lan, Dong Ma, Weitao Xu, Mahbub Hassan, Wen Hu", "title": "Capacitor Based Activity Sensing for Kinetic Powered Wearable IoTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.ET cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel use of the conventional energy storage component, i.e.,\ncapacitor, in kinetic-powered wearable IoTs as a sensor to detect human\nactivities. Since different activities accumulate energies in the capacitor at\ndifferent rates, these activities can be detected directly by observing the\ncharging rate of the capacitor. The key advantage of the proposed capacitor\nbased activity sensing mechanism, called CapSense, is that it obviates the need\nfor sampling the motion signal during the activity detection period thus\nsignificantly saving power consumption of the wearable device. A challenge we\nface is that capacitors are inherently non-linear energy accumulators, which,\neven for the same activity, leads to significant variations in charging rates\nat different times depending on the current charge level of the capacitor. We\nsolve this problem by jointly configuring the parameters of the capacitor and\nthe associated energy harvesting circuits, which allows us to operate on\ncharging cycles that are approximately linear. We design and implement a\nkinetic-powered shoe sole and conduct experiments with 10 subjects. Our results\nshow that CapSense can classify five different daily activities with 95%\naccuracy while consuming 73% less system power compared to conventional motion\nsignal based activity detection.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 06:00:46 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Lan", "Guohao", ""], ["Ma", "Dong", ""], ["Xu", "Weitao", ""], ["Hassan", "Mahbub", ""], ["Hu", "Wen", ""]]}, {"id": "1806.07108", "submitter": "Qiqi Zhang", "authors": "Qiqi Zhang and Ying Liu", "title": "Improving brain computer interface performance by data augmentation with\n  conditional Deep Convolutional Generative Adversarial Networks", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the big restrictions in brain computer interface field is the very\nlimited training samples, it is difficult to build a reliable and usable system\nwith such limited data. Inspired by generative adversarial networks, we propose\na conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks\nmethod to generate more artificial EEG signal automatically for data\naugmentation to improve the performance of convolutional neural networks in\nbrain computer interface field and overcome the small training dataset\nproblems. We evaluate the proposed cDCGAN method on BCI competition dataset of\nmotor imagery. The results show that the generated artificial EEG data from\nGaussian noise can learn the features from raw EEG data and has no less than\nthe classification accuracy of raw EEG data in the testing dataset. Also by\nusing generated artificial data can effectively improve classification accuracy\nat the same model with limited training data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 08:49:50 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 08:32:32 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Zhang", "Qiqi", ""], ["Liu", "Ying", ""]]}, {"id": "1806.07284", "submitter": "Mejdi Ben Dkhil", "authors": "Mejdi Ben Dkhil, Mohamed Neji, Ali Wali, and Adel M. Alimi", "title": "A new approach for a safe car assistance system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drowsiness, which is the state when drivers do not have scheduled breaks\nwhile traveling long distances, is the main reason behind serious motorway\naccidents. Accordingly, experts claim that drowsy state is hard to be\nrecognized early enough to prevent serious accidents that may lead even to road\ndeaths. In this work, we propose a new drowsiness state detection system based\non physiological signals and eye blinking. An experiment has been directed to\njustify the utility of the proposed approach. This system uses a smart video\ncamera that takes drivers faces images and supervises the eye blink (open and\nclose); also, it uses the Emotiv EPOC headset to acquire the\nelectroencephalogram (EEG) signals. Eye detection is done by Viola and Jones\ntechnique, EEG. Finally, we have chosen the fuzzy logic techniques to classify\nthe EEG signals and eye blinking detection to analyze the results.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 12:43:00 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Dkhil", "Mejdi Ben", ""], ["Neji", "Mohamed", ""], ["Wali", "Ali", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1806.07382", "submitter": "Xinyu Chen", "authors": "Xinyu Chen, Qiang Guan, Li-Ta Lo, Simon Su, James Ahrens and Trilce\n  Estrada", "title": "In situ TensorView: In situ Visualization of Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks(CNNs) are complex systems. They are trained so\nthey can adapt their internal connections to recognize images, texts and more.\nIt is both interesting and helpful to visualize the dynamics within such deep\nartificial neural networks so that people can understand how these artificial\nnetworks are learning and making predictions. In the field of scientific\nsimulations, visualization tools like Paraview have long been utilized to\nprovide insights and understandings. We present in situ TensorView to visualize\nthe training and functioning of CNNs as if they are systems of scientific\nsimulations. In situ TensorView is a loosely coupled in situ visualization open\nframework that provides multiple viewers to help users to visualize and\nunderstand their networks. It leverages the capability of co-processing from\nParaview to provide real-time visualization during training and predicting\nphases. This avoid heavy I/O overhead for visualizing large dynamic systems.\nOnly a small number of lines of codes are injected in TensorFlow framework. The\nvisualization can provide guidance to adjust the architecture of networks, or\ncompress the pre-trained networks. We showcase visualizing the training of\nLeNet-5 and VGG16 using in situ TensorView.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 22:51:12 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Chen", "Xinyu", ""], ["Guan", "Qiang", ""], ["Lo", "Li-Ta", ""], ["Su", "Simon", ""], ["Ahrens", "James", ""], ["Estrada", "Trilce", ""]]}, {"id": "1806.07805", "submitter": "Vicky Charisi", "authors": "Andreia Costa, Tonie Schweich, Louise Charpiot, Georges Steffgen", "title": "Attitudes of Children with Autism towards Robots: An Exploratory Study", "comments": "Presented at Interaction Design and Children (IDC-CRI2018) Workshop\n  (arXiv:submit/2277826)", "journal-ref": null, "doi": null, "report-no": "IDC-CRI/2018/07", "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this exploratory study we assessed how attitudes of children with autism\nspectrum disorder (ASD) towards robots together with children's autism-related\nsocial impairments are linked to indicators of children's preference of an\ninteraction with a robot over an interaction with a person. We found that\nchildren with ASD have overall positive attitudes towards robots and that they\noften prefer interacting with a robot than with a person. Several of children's\nattitudes were linked to children's longer gazes towards a robot compared to a\nperson. Autism-related social impairments were linked to more repetitive and\nstereotyped behaviors and to a shorter gaze duration in the interaction with\nthe robot compared to the person. These preliminary results contribute to\nbetter understand factors that might help determine sub-groups of children with\nASD for whom robots could be particularly useful.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 18:03:22 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Costa", "Andreia", ""], ["Schweich", "Tonie", ""], ["Charpiot", "Louise", ""], ["Steffgen", "Georges", ""]]}, {"id": "1806.07806", "submitter": "Vicky Charisi", "authors": "Maria Blancas-Mu\\~noz, Vasiliki Vouloutsi, Riccardo Zucca, Anna Mura,\n  Paul F.M.J. Verschure", "title": "Hints vs Distractions in Intelligent Tutoring Systems: Looking for the\n  proper type of help", "comments": "Presented at Interaction Design and Children (IDC-CRI2018) Workshop\n  (arXiv:submit/2277826)", "journal-ref": null, "doi": null, "report-no": "IDC-CRI/2018/06", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kind of help a student receives during a task has been shown to play a\nsignificant role in their learning process. We designed an interaction scenario\nwith a robotic tutor, in real-life settings based on an inquiry-based learning\ntask. We aim to explore how learners' performance is affected by the various\nstrategies of a robotic tutor. We explored two kinds of(presumable) help: hints\n(which were specific to the level or general to the task) or distractions\n(information not relevant to the task: either a joke or a curious fact). Our\nresults suggest providing hints to the learner and distracting them with\ncurious facts as more effective than distracting them with humour.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 17:56:47 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Blancas-Mu\u00f1oz", "Maria", ""], ["Vouloutsi", "Vasiliki", ""], ["Zucca", "Riccardo", ""], ["Mura", "Anna", ""], ["Verschure", "Paul F. M. J.", ""]]}, {"id": "1806.07842", "submitter": "Yuri G. Gordienko", "authors": "Olga Barkova, Natalia Pysarevska, Oleg Allenin, Serhii Hamotsky,\n  Nikita Gordienko, Vladyslav Sarnatskyi, Vadym Ovcharenko, Mariia Tkachenko,\n  Yurii Gordienko, Sergei Stirenko", "title": "Gamification for Education of the Digitally Native Generation by Means\n  of Virtual Reality, Augmented Reality, Machine Learning, and Brain-Computing\n  Interfaces in Museums", "comments": "16 pages, 8 figures,\n  http://uncommonculture.org/ojs/index.php/UC/article/view/9238", "journal-ref": "Uncommon Culture, vol. 7, no.1/2(13/14), pp.86-101 (2018)", "doi": null, "report-no": null, "categories": "cs.CY cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particularly close attention is being paid today among researchers in social\nscience disciplines to aspects of learning in the digital age, especially for\nthe Digitally Native Generation. In the context of museums, the question is:\nhow can rich learning experiences be provided for increasingly technologically\nadvanced young visitors in museums? Which high-tech platforms and solutions do\nmuseums need to focus on? At the same time, the software games business is\ngrowing fast and now finding its way into non-entertainment contexts, helping\nto deliver substantial benefits, particularly in education, training, research,\nand health. This article outlines some aspects facing Digitally Native learners\nin museums through an analysis of several radically new key technologies:\nInteractivity, Wearables, Virtual Reality, and Augmented Reality. Special\nattention is paid to use cases for application of games-based scenarios via\nthese technologies in non-leisure contexts and specifically for educational\npurposes in museums.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:03:52 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Barkova", "Olga", ""], ["Pysarevska", "Natalia", ""], ["Allenin", "Oleg", ""], ["Hamotsky", "Serhii", ""], ["Gordienko", "Nikita", ""], ["Sarnatskyi", "Vladyslav", ""], ["Ovcharenko", "Vadym", ""], ["Tkachenko", "Mariia", ""], ["Gordienko", "Yurii", ""], ["Stirenko", "Sergei", ""]]}, {"id": "1806.07974", "submitter": "Josef Mich\\'alek", "authors": "Josef Michalek, Jan Vanek", "title": "A Survey of Recent DNN Architectures on the TIMIT Phone Recognition Task", "comments": "Submitted to TSD 2018, 21st International Conference on Text, Speech\n  and Dialogue. arXiv admin note: substantial text overlap with\n  arXiv:1806.07186", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this survey paper, we have evaluated several recent deep neural network\n(DNN) architectures on a TIMIT phone recognition task. We chose the TIMIT\ncorpus due to its popularity and broad availability in the community. It also\nsimulates a low-resource scenario that is helpful in minor languages. Also, we\nprefer the phone recognition task because it is much more sensitive to an\nacoustic model quality than a large vocabulary continuous speech recognition\n(LVCSR) task. In recent years, many DNN published papers reported results on\nTIMIT. However, the reported phone error rates (PERs) were often much higher\nthan a PER of a simple feed-forward (FF) DNN. That was the main motivation of\nthis paper: To provide a baseline DNNs with open-source scripts to easily\nreplicate the baseline results for future papers with lowest possible PERs.\nAccording to our knowledge, the best-achieved PER of this survey is better than\nthe best-published PER to date.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 12:43:41 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Michalek", "Josef", ""], ["Vanek", "Jan", ""]]}, {"id": "1806.08039", "submitter": "Saif Alabachi", "authors": "Saif Alabachi and Gita Sukthankar", "title": "Intelligently Assisting Human-Guided Quadcopter Photography", "comments": null, "journal-ref": "FLAIRS Conference 2018", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drones are a versatile platform for both amateur and professional\nphotographers, enabling them to capture photos that are impossible to shoot\nwith ground-based cameras. However, when guided by inexperienced pilots, they\nhave a high incidence of collisions, crashes, and poorly framed photographs.\nThis paper presents an intelligent user interface for photographing objects\nthat is robust against navigation errors and reliably collects high quality\nphotographs. By retaining the human in the loop, our system is faster and more\nselective than purely autonomous UAVs that employ simple coverage algorithms.\nThe intelligent user interface operates in multiple modes, allowing the user to\neither directly control the quadcopter or fly in a semi-autonomous mode around\na target object in the environment. To evaluate the interface, users completed\na data set collection task in which they were asked to photograph objects from\nmultiple views. Our sketchbased control paradigm facilitated task completion,\nreduced crashes, and was favorably reviewed by the participants.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 01:32:26 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Alabachi", "Saif", ""], ["Sukthankar", "Gita", ""]]}, {"id": "1806.08332", "submitter": "Luciano Abriata", "authors": "Luciano A. Abriata", "title": "Towards Commodity, Web-Based Augmented Reality Applications for Research\n  and Education in Chemistry and Structural Biology", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.ET cs.MM physics.bio-ph q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reports prototype web apps that use commodity, open-source\ntechnologies for augmented and virtual reality to provide immersive,\ninteractive human-computer interfaces for chemistry, structural biology and\nrelated disciplines. The examples, which run in any standard web browser and\nare accessible at\nhttps://lucianoabriata.altervista.org/jsinscience/arjs/armodeling/ together\nwith demo videos, showcase applications that could go well beyond pedagogy,\ni.e. advancing actual utility in research settings: molecular visualization at\natomistic and coarse-grained levels in interactive immersive 3D, coarse-grained\nmodeling of molecular physics and chemistry, and on-the-fly calculation of\nexperimental observables and overlay onto experimental data. From this\nplayground, I depict perspectives on how these emerging technologies might\ncouple in the future to neural network-based quantum mechanical calculations,\nadvanced forms of human-computer interaction such as speech-based\ncommunication, and sockets for concurrent collaboration through the internet\n-all technologies that are today maturing in web browsers- to deliver the next\ngeneration of tools for truly interactive, immersive molecular modeling that\ncan streamline human thought and intent with the numerical processing power of\ncomputers.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 17:21:17 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 22:12:45 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 21:15:42 GMT"}, {"version": "v4", "created": "Tue, 24 Jul 2018 18:33:06 GMT"}, {"version": "v5", "created": "Fri, 10 Aug 2018 08:17:53 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Abriata", "Luciano A.", ""]]}, {"id": "1806.08479", "submitter": "Xinlei Pan", "authors": "Xinlei Pan, Eshed Ohn-Bar, Nicholas Rhinehart, Yan Xu, Yilin Shen,\n  Kris M. Kitani", "title": "Human-Interactive Subgoal Supervision for Efficient Inverse\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are able to understand and perform complex tasks by strategically\nstructuring the tasks into incremental steps or subgoals. For a robot\nattempting to learn to perform a sequential task with critical subgoal states,\nsuch states can provide a natural opportunity for interaction with a human\nexpert. This paper analyzes the benefit of incorporating a notion of subgoals\ninto Inverse Reinforcement Learning (IRL) with a Human-In-The-Loop (HITL)\nframework. The learning process is interactive, with a human expert first\nproviding input in the form of full demonstrations along with some subgoal\nstates. These subgoal states define a set of subtasks for the learning agent to\ncomplete in order to achieve the final goal. The learning agent queries for\npartial demonstrations corresponding to each subtask as needed when the agent\nstruggles with the subtask. The proposed Human Interactive IRL (HI-IRL)\nframework is evaluated on several discrete path-planning tasks. We demonstrate\nthat subgoal-based interactive structuring of the learning task results in\nsignificantly more efficient learning, requiring only a fraction of the\ndemonstration data needed for learning the underlying reward function with the\nbaseline IRL model.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 03:24:00 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Pan", "Xinlei", ""], ["Ohn-Bar", "Eshed", ""], ["Rhinehart", "Nicholas", ""], ["Xu", "Yan", ""], ["Shen", "Yilin", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1806.08518", "submitter": "Juan Quiroz", "authors": "Juan C. Quiroz, Elena Geangu, Min Hooi Yong", "title": "Emotion-Recognition Using Smart Watch Sensor Data: Mixed-Design Study", "comments": "Published in JMIR Mental Health", "journal-ref": "JMIR Ment Health (2018);5(3):e10153", "doi": "10.2196/10153", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the use of movement sensor data from a smart watch to\ninfer an individual's emotional state. We present our findings on a user study\nwith 50 participants. The experimental design is a mixed-design study;\nwithin-subjects (emotions; happy, sad, neutral) and between-subjects (stimulus\ntype: audio-visual \"movie clips\", audio \"music clips\"). Each participant\nexperienced both emotions in a single stimulus type. All participants walked\n250m while wearing a smart watch on one wrist and a heart rate monitor strap on\ntheir chest. They also had to answer a short questionnaire (20 items; PANAS)\nbefore and after experiencing each emotion. The heart rate monitor served as\nsupplementary information to our data. We performed time-series analysis on the\ndata from the smart watch and a t-test on the questionnaire items to measure\nthe change in emotional state. The heart rate data was analyzed using one-way\nANOVA. We extracted features from the time-series using sliding windows and\nused the features to train and validate classifiers that determined an\nindividual's emotion. Participants reported feeling less negative affect after\nwatching sad videos or after listening to the sad music, P < .006. For the task\nof emotion recognition using classifiers, our results show that the personal\nmodels outperformed personal baselines, and achieved median accuracies higher\nthan 78% for all conditions of the design study for the binary classification\nof happiness vs sadness. Our findings show that we are able to detect the\nchanges in emotional state with data obtained from the smartwatch as well as\nbehavioral responses.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 07:00:14 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 21:23:06 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Quiroz", "Juan C.", ""], ["Geangu", "Elena", ""], ["Yong", "Min Hooi", ""]]}, {"id": "1806.08621", "submitter": "Tanel Alum\\\"ae", "authors": "Martin Karu, Tanel Alum\\\"ae", "title": "Weakly Supervised Training of Speaker Identification Models", "comments": "Odyssey 2018 The Speaker and Language Recognition Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for training speaker identification models in a weakly\nsupervised manner. We concentrate on the setting where the training data\nconsists of a set of audio recordings and the speaker annotation is provided\nonly at the recording level. The method uses speaker diarization to find unique\nspeakers in each recording, and i-vectors to project the speech of each speaker\nto a fixed-dimensional vector. A neural network is then trained to map\ni-vectors to speakers, using a special objective function that allows to\noptimize the model using recording-level speaker labels. We report experiments\non two different real-world datasets. On the VoxCeleb dataset, the method\nprovides 94.6% accuracy on a closed set speaker identification task, surpassing\nthe baseline performance by a large margin. On an Estonian broadcast news\ndataset, the method provides 66% time-weighted speaker identification recall at\n93% precision.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 12:15:35 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Karu", "Martin", ""], ["Alum\u00e4e", "Tanel", ""]]}, {"id": "1806.08648", "submitter": "Markus Pfeiffer", "authors": "Manuel Machado Martins and Markus Pfeiffer", "title": "Francy - An Interactive Discrete Mathematics Framework for GAP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualization and interaction with large data sets is known to be\nessential and critical in many businesses today, and the same applies to\nresearch and teaching, in this case, when exploring large and complex\nmathematical objects. GAP is a computer algebra system for computational\ndiscrete algebra with an emphasis on computational group theory. The existing\nXGAP package for GAP works exclusively on the X Window System. It lacks\nabstraction between its mathematical and graphical cores, making it difficult\nto extend, maintain, or port. In this paper, we present Francy, a graphical\nsemantics package for GAP. Francy is responsible for creating a\nrepresentational structure that can be rendered using many GUI frameworks\nindependent from any particular programming language or operating system.\nBuilding on this, we use state of the art web technologies that take advantage\nof an improved REPL environment, which is currently under development for GAP.\nThe integration of this project with Jupyter provides a rich graphical\nenvironment full of features enhancing the usability and accessibility of GAP.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 11:58:37 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Martins", "Manuel Machado", ""], ["Pfeiffer", "Markus", ""]]}, {"id": "1806.09018", "submitter": "Yao Zhou", "authors": "Yao Zhou, Jingrui He", "title": "Optimizing the Wisdom of the Crowd: Inference, Learning, and Teaching", "comments": "3 pages, SBP-BRiMS 18 Doctoral Consortium", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented demand for large amount of data has catalyzed the trend of\ncombining human insights with machine learning techniques, which facilitate the\nuse of crowdsourcing to enlist label information both effectively and\nefficiently. The classic work on crowdsourcing mainly focuses on the label\ninference problem under the categorization setting. However, inferring the true\nlabel requires sophisticated aggregation models that usually can only perform\nwell under certain assumptions. Meanwhile, no matter how complicated the\naggregation model is, the true model that generated the crowd labels remains\nunknown. Therefore, the label inference problem can never infer the ground\ntruth perfectly. Based on the fact that the crowdsourcing labels are abundant\nand utilizing aggregation will lose such kind of rich annotation information\n(e.g., which worker provided which labels), we believe that it is critical to\ntake the diverse labeling abilities of the crowdsourcing workers as well as\ntheir correlations into consideration. To address the above challenge, we\npropose to tackle three research problems, namely inference, learning, and\nteaching.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 18:29:06 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Zhou", "Yao", ""], ["He", "Jingrui", ""]]}, {"id": "1806.09115", "submitter": "Ahmed Arif", "authors": "Ohoud Alharbi, Ahmed Sabbir Arif", "title": "The Perception of Humanoid Robots for Domestic Use in Saudi Arabia", "comments": "In CHI 2018 Workshop on Exploring Participatory Design Methods to\n  Engage with Arab Communities (April 22, 2018). Montr\\'eal, QC, Canada, 6\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a research to investigate Saudi peoples' perception of humanoid\ndomestic robots and attitude towards the possibility of having one in their\nhouse. Through a series of questionnaires, semi-structured interviews, focus\ngroups, and participatory design sessions, this research will explore Saudi\npeoples' level of acceptance towards domestic robots, the tasks and\nresponsibilities they would feel comfortable assigning to these robots, their\npreferred appearance of domestic robots, and the cultural stereotypes they feel\na domestic robot must mimic.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 09:42:43 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Alharbi", "Ohoud", ""], ["Arif", "Ahmed Sabbir", ""]]}, {"id": "1806.09256", "submitter": "Cagatay Demiralp", "authors": "Marco Cavallo and \\c{C}a\\u{g}atay Demiralp", "title": "Track Xplorer: A System for Visual Analysis of Sensor-based Motor\n  Activity Predictions", "comments": "EuroVis'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid commoditization of wearable sensors, detecting human movements\nfrom sensor datasets has become increasingly common over a wide range of\napplications. To detect activities, data scientists iteratively experiment with\ndifferent classifiers before deciding which model to deploy. Effective\nreasoning about and comparison of alternative classifiers are crucial in\nsuccessful model development. This is, however, inherently difficult in\ndeveloping classifiers for sensor data, where the intricacy of long temporal\nsequences, high prediction frequency, and imprecise labeling make standard\nevaluation methods relatively ineffective and even misleading. We introduce\nTrack Xplorer, an interactive visualization system to query, analyze, and\ncompare the predictions of sensor-data classifiers. Track Xplorer enables users\nto interactively explore and compare the results of different classifiers, and\nassess their accuracy with respect to the ground-truth labels and video.\nThrough integration with a version control system, Track Xplorer supports\ntracking of models and their parameters without additional workload on model\ndevelopers. Track Xplorer also contributes an extensible algebra over track\nrepresentations to filter, compose, and compare classification outputs,\nenabling users to reason effectively about classifier performance. We apply\nTrack Xplorer in a collaborative project to develop classifiers to detect\nmovements from multisensor data gathered from Parkinson's disease patients. We\ndemonstrate how Track Xplorer helps identify early on possible systemic data\nerrors, effectively track and compare the results of different classifiers, and\nreason about and pinpoint the causes of misclassifications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 02:19:24 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Cavallo", "Marco", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1806.09532", "submitter": "Joos Behncke", "authors": "Joos Behncke, Robin Tibor Schirrmeister, Martin V\\\"olker, Ji\\v{r}\\'i\n  Hammer, Petr Marusi\\v{c}, Andreas Schulze-Bonhage, Wolfram Burgard, Tonio\n  Ball", "title": "Cross-paradigm pretraining of convolutional networks improves\n  intracranial EEG decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When it comes to the classification of brain signals in real-life\napplications, the training and the prediction data are often described by\ndifferent distributions. Furthermore, diverse data sets, e.g., recorded from\nvarious subjects or tasks, can even exhibit distinct feature spaces. The fact\nthat data that have to be classified are often only available in small amounts\nreinforces the need for techniques to generalize learned information, as\nperformances of brain-computer interfaces (BCIs) are enhanced by increasing\nquantity of available data. In this paper, we apply transfer learning to a\nframework based on deep convolutional neural networks (deep ConvNets) to prove\nthe transferability of learned patterns in error-related brain signals across\ndifferent tasks. The experiments described in this paper demonstrate the\nusefulness of transfer learning, especially improving performances when only\nlittle data can be used to distinguish between erroneous and correct\nrealization of a task. This effect could be delimited from a transfer of merely\ngeneral brain signal characteristics, underlining the transfer of\nerror-specific information. Furthermore, we could extract similar patterns in\ntime-frequency analyses in identical channels, leading to selective high signal\ncorrelations between the two different paradigms. Classification on the\nintracranial data yields in median accuracies up to $(81.50 \\pm 9.49)\\,\\%$.\nDecoding on only $10\\%$ of the data without pre-training reaches performances\nof $(54.76 \\pm 3.56)\\,\\%$, compared to $(64.95 \\pm 0.79)\\,\\%$ with\npre-training.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 11:34:36 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 10:18:28 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Behncke", "Joos", ""], ["Schirrmeister", "Robin Tibor", ""], ["V\u00f6lker", "Martin", ""], ["Hammer", "Ji\u0159\u00ed", ""], ["Marusi\u010d", "Petr", ""], ["Schulze-Bonhage", "Andreas", ""], ["Burgard", "Wolfram", ""], ["Ball", "Tonio", ""]]}, {"id": "1806.10130", "submitter": "Zhengxing Chen", "authors": "Zhengxing Chen, Truong-Huy D Nguyen, Yuyu Xu, Chris Amato, Seth\n  Cooper, Yizhou Sun, Magy Seif El-Nasr", "title": "The Art of Drafting: A Team-Oriented Hero Recommendation System for\n  Multiplayer Online Battle Arena Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplayer Online Battle Arena (MOBA) games have received increasing\npopularity recently. In a match of such games, players compete in two teams of\nfive, each controlling an in-game avatars, known as heroes, selected from a\nroster of more than 100. The selection of heroes, also known as pick or draft,\ntakes place before the match starts and alternates between the two teams until\neach player has selected one hero. Heroes are designed with different strengths\nand weaknesses to promote team cooperation in a game. Intuitively, heroes in a\nstrong team should complement each other's strengths and suppressing those of\nopponents. Hero drafting is therefore a challenging problem due to the complex\nhero-to-hero relationships to consider. In this paper, we propose a novel hero\nrecommendation system that suggests heroes to add to an existing team while\nmaximizing the team's prospect for victory. To that end, we model the drafting\nbetween two teams as a combinatorial game and use Monte Carlo Tree Search\n(MCTS) for estimating the values of hero combinations. Our empirical evaluation\nshows that hero teams drafted by our recommendation algorithm have\nsignificantly higher win rate against teams constructed by other baseline and\nstate-of-the-art strategies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 17:59:09 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Chen", "Zhengxing", ""], ["Nguyen", "Truong-Huy D", ""], ["Xu", "Yuyu", ""], ["Amato", "Chris", ""], ["Cooper", "Seth", ""], ["Sun", "Yizhou", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "1806.10658", "submitter": "Soheil Khorram", "authors": "Soheil Khorram, Mimansa Jaiswal, John Gideon, Melvin McInnis, Emily\n  Mower Provost", "title": "The PRIORI Emotion Dataset: Linking Mood to Emotion Detected In-the-Wild", "comments": "Interspeech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bipolar Disorder is a chronic psychiatric illness characterized by\npathological mood swings associated with severe disruptions in emotion\nregulation. Clinical monitoring of mood is key to the care of these dynamic and\nincapacitating mood states. Frequent and detailed monitoring improves clinical\nsensitivity to detect mood state changes, but typically requires costly and\nlimited resources. Speech characteristics change during both depressed and\nmanic states, suggesting automatic methods applied to the speech signal can be\neffectively used to monitor mood state changes. However, speech is modulated by\nmany factors, which renders mood state prediction challenging. We hypothesize\nthat emotion can be used as an intermediary step to improve mood state\nprediction. This paper presents critical steps in developing this pipeline,\nincluding (1) a new in the wild emotion dataset, the PRIORI Emotion Dataset,\ncollected from everyday smartphone conversational speech recordings, (2)\nactivation/valence emotion recognition baselines on this dataset (PCC of 0.71\nand 0.41, respectively), and (3) significant correlation between predicted\nemotion and mood state for individuals with bipolar disorder. This provides\nevidence and a working baseline for the use of emotion as a meta-feature for\nmood state monitoring.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 23:28:12 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Khorram", "Soheil", ""], ["Jaiswal", "Mimansa", ""], ["Gideon", "John", ""], ["McInnis", "Melvin", ""], ["Provost", "Emily Mower", ""]]}, {"id": "1806.11046", "submitter": "Ran Yu", "authors": "Ran Yu, Ujwal Gadiraju, Stefan Dietze", "title": "Detecting, Understanding and Supporting Everyday Learning in Web Search", "comments": "6 pages, LILE workshop at ACM WebSci conferentce 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web search is among the most ubiquitous online activities, commonly used to\nacquire new knowledge and to satisfy learning-related objectives through\ninformational search sessions. The importance of learning as an outcome of web\nsearch has been recognized widely, leading to a variety of research at the\nintersection of information retrieval, human computer interaction and\nlearning-oriented sciences. Given the lack of explicit information,\nunderstanding of users and their learning needs has to be derived from their\nsearch behavior and resource interactions. In this paper, we introduce the\ninvolved research challenges and survey related work on the detection of\nlearning needs, understanding of users, e.g. with respect to their knowledge\nstate, learning tasks and learning progress throughout a search session as well\nas the actual consideration of learning needs throughout the retrieval and\nranking process. In addition, we summarise our own research contributing to the\naforementioned tasks and describe our research agenda in this context.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 15:53:26 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Yu", "Ran", ""], ["Gadiraju", "Ujwal", ""], ["Dietze", "Stefan", ""]]}, {"id": "1806.11060", "submitter": "Filipo Sharevski", "authors": "Adam Trowbridge, Filipo Sharevski, Jessica Westbrook", "title": "Malicious User Experience Design Research for Cybersecurity", "comments": null, "journal-ref": "NSPW 2018", "doi": "10.1145/3285002.3285010", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the factors and theory behind the user-centered research\nthat is necessary to create a successful game-like prototype, and user\nexperience, for malicious users in a cybersecurity context. We explore what is\nknown about successful addictive design in the fields of video games and\ngambling to understand the allure of breaking into a system, and the joy of\nthwarting the security to reach a goal or a reward of data. Based on the\nmalicious user research, game user research, and using the GameFlow framework,\nwe propose a novel malicious user experience design approach\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 16:19:37 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Trowbridge", "Adam", ""], ["Sharevski", "Filipo", ""], ["Westbrook", "Jessica", ""]]}, {"id": "1806.11291", "submitter": "Eric Brangier", "authors": "Eric Brangier (PERSEUS), Josefina Gil Urrutia (PERSEUS), V\\'eronique\n  Senderowicz, Laurent Cessat", "title": "Beyond \"Usability and User Experience\" , Towards an Integrative\n  Heuristic Inspection: from Accessibility to Persuasiveness in the UX\n  Evaluation A Case Study on an Insurance Prospecting Tablet Application", "comments": null, "journal-ref": "In T. Ahram and C. Falc?o (eds.), Advances in Usability and User\n  Experience, Advances in Intelligent Systems and Computing, 2017", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heuristic inspections are often carried out in a rather restrictive manner in\nthe sense that they often address one or two of User Experience aspects. These\ntwo generally being: usability and \"user experience\". This fails to consider UX\nas it should be [considered]: through a holistic approach. Thus, we suggest to\ngo beyond that by opting for what we have called an Integrative Heu-ristic\nInspection that takes into account issues of: accessibility, usability,\nemotions \\& motivation and persuasion, and that aims to simplify the overflow\nof recommendations UX professionals are faced with nowadays. We illustrate our\nproposal by a case study carried out on an insurance prospecting tablet\napplication. We analyzed the results of the inspection separately for each\ndimension as well as combined across dimensions. Implications for a reflection\non the struc-turing of the criteria for a general criteria-based approach in UX\nare discussed.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 08:04:22 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Brangier", "Eric", "", "PERSEUS"], ["Urrutia", "Josefina Gil", "", "PERSEUS"], ["Senderowicz", "V\u00e9ronique", ""], ["Cessat", "Laurent", ""]]}, {"id": "1806.11361", "submitter": "Charles Fleming", "authors": "Ilesanmi Olade and Haining Liang and Charles Fleming", "title": "SemanticLock: An authentication method for mobile devices using\n  semantically-linked images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SemanticLock, a single factor graphical authentication solution\nfor mobile devices. SemanticLock uses a set of graphical images as password\ntokens that construct a semantically memorable story representing the user`s\npassword. A familiar and quick action of dragging or dropping the images into\ntheir respective positions either in a \\textit{continous flow} or in\n\\textit{discrete} movements on the the touchscreen is what is required to use\nour solution.\n  The authentication strength of the SemanticLock is based on the large number\nof possible semantic constructs derived from the positioning of the image\ntokens and the type of images selected. Semantic Lock has a high resistance to\nsmudge attacks and it equally exhibits a higher level of memorability due to\nits graphical paradigm.\n  In a three weeks user study with 21 participants comparing SemanticLock\nagainst other authentication systems, we discovered that SemanticLock\noutperformed the PIN and matched the PATTERN both on speed, memorability, user\nacceptance and usability. Furthermore, qualitative test also show that\nSemanticLock was rated more superior in like-ability. SemanticLock was also\nevaluated while participants walked unencumbered and walked encumbered carrying\n\"everyday\" items to analyze the effects of such activities on its usage.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 11:32:04 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 01:39:09 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 18:10:41 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Olade", "Ilesanmi", ""], ["Liang", "Haining", ""], ["Fleming", "Charles", ""]]}, {"id": "1806.11420", "submitter": "Chandrakant Bothe", "authors": "Chandrakant Bothe, Sven Magg, Cornelius Weber, Stefan Wermter", "title": "Discourse-Wizard: Discovering Deep Discourse Structure in your\n  Conversation with RNNs", "comments": "Submitted to EMNLP 2018: System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language understanding is one of the key factors in a dialogue system,\nand a context in a conversation plays an important role to understand the\ncurrent utterance. In this work, we demonstrate the importance of context\nwithin the dialogue for neural network models through an online web interface\nlive demo. We developed two different neural models: a model that does not use\ncontext and a context-based model. The no-context model classifies dialogue\nacts at an utterance-level whereas the context-based model takes some preceding\nutterances into account. We make these trained neural models available as a\nlive demo called Discourse-Wizard using a modular server architecture. The live\ndemo provides an easy to use interface for conversational analysis and for\ndiscovering deep discourse structures in a conversation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 14:02:04 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Bothe", "Chandrakant", ""], ["Magg", "Sven", ""], ["Weber", "Cornelius", ""], ["Wermter", "Stefan", ""]]}, {"id": "1806.11479", "submitter": "Bo Xiao", "authors": "Bo Xiao, Nicholas Monath, Shankar Ananthakrishnan, Abishek Ravi", "title": "Play Duration based User-Entity Affinity Modeling in Spoken Dialog\n  System", "comments": "Interspeech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia streaming services over spoken dialog systems have become\nubiquitous. User-entity affinity modeling is critical for the system to\nunderstand and disambiguate user intents and personalize user experiences.\nHowever, fully voice-based interaction demands quantification of novel\nbehavioral cues to determine user affinities. In this work, we propose using\nplay duration cues to learn a matrix factorization based collaborative\nfiltering model. We first binarize play durations to obtain implicit positive\nand negative affinity labels. The Bayesian Personalized Ranking objective and\nlearning algorithm are employed in our low-rank matrix factorization approach.\nTo cope with uncertainties in the implicit affinity labels, we propose to apply\na weighting function that emphasizes the importance of high confidence samples.\nBased on a large-scale database of Alexa music service records, we evaluate the\naffinity models by computing Spearman correlation between play durations and\npredicted affinities. Comparing different data utilizations and weighting\nfunctions, we find that employing both positive and negative affinity samples\nwith a convex weighting function yields the best performance. Further analysis\ndemonstrates the model's effectiveness on individual entity level and provides\ninsights on the temporal dynamics of observed affinities.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 15:39:43 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Xiao", "Bo", ""], ["Monath", "Nicholas", ""], ["Ananthakrishnan", "Shankar", ""], ["Ravi", "Abishek", ""]]}]