[{"id": "1611.00379", "submitter": "Baptiste Caramiaux", "authors": "Rebecca Fiebrink, Baptiste Caramiaux", "title": "The Machine Learning Algorithm as Creative Musical Tool", "comments": "Pre-print to appear in the Oxford Handbook on Algorithmic Music.\n  Oxford University Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is the capacity of a computational system to learn\nstructures from datasets in order to make predictions on newly seen data. Such\nan approach offers a significant advantage in music scenarios in which\nmusicians can teach the system to learn an idiosyncratic style, or can break\nthe rules to explore the system's capacity in unexpected ways. In this chapter\nwe draw on music, machine learning, and human-computer interaction to elucidate\nan understanding of machine learning algorithms as creative tools for music and\nthe sonic arts. We motivate a new understanding of learning algorithms as\nhuman-computer interfaces. We show that, like other interfaces, learning\nalgorithms can be characterised by the ways their affordances intersect with\ngoals of human users. We also argue that the nature of interaction between\nusers and algorithms impacts the usability and usefulness of those algorithms\nin profound ways. This human-centred view of machine learning motivates our\nconcluding discussion of what it means to employ machine learning as a creative\ntool.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 20:35:46 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Fiebrink", "Rebecca", ""], ["Caramiaux", "Baptiste", ""]]}, {"id": "1611.00447", "submitter": "Peter Krafft", "authors": "Peter M Krafft, Michael Macy, Alex Pentland", "title": "Bots as Virtual Confederates: Design and Ethics", "comments": "Forthcoming in CSCW 2017", "journal-ref": "The 20th ACM Conference on Computer-Supported Cooperative Work and\n  Social Computing (CSCW) (2016)", "doi": "10.1145/2998181.2998354", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of bots as virtual confederates in online field experiments holds\nextreme promise as a new methodological tool in computational social science.\nHowever, this potential tool comes with inherent ethical challenges. Informed\nconsent can be difficult to obtain in many cases, and the use of confederates\nnecessarily implies the use of deception. In this work we outline a design\nspace for bots as virtual confederates, and we propose a set of guidelines for\nmeeting the status quo for ethical experimentation. We draw upon examples from\nprior work in the CSCW community and the broader social science literature for\nillustration. While a handful of prior researchers have used bots in online\nexperimentation, our work is meant to inspire future work in this area and\nraise awareness of the associated ethical issues.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 02:31:18 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Krafft", "Peter M", ""], ["Macy", "Michael", ""], ["Pentland", "Alex", ""]]}, {"id": "1611.00872", "submitter": "Meisam Hejazi Nia", "authors": "Meisam Hejazi Nia", "title": "A Decision Support System for Inbound Marketers: An Empirical Use of\n  Latent Dirichlet Allocation Topic Model to Guide Infographic Designers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infographic is a type of information presentation that inbound marketers use.\nI suggest a method that can allow the infographic designers to benchmark their\ndesign against the previous viral infographics to measure whether a given\ndesign decision can help or hurt the probability of the design becoming viral.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 03:53:25 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Nia", "Meisam Hejazi", ""]]}, {"id": "1611.00954", "submitter": "James Bagrow", "authors": "Thomas C. McAndrew, Elizaveta A. Guseva and James P. Bagrow", "title": "Reply & Supply: Efficient crowdsourcing when workers do more than answer\n  questions", "comments": "20 pages, 6 figures, 1 table", "journal-ref": "PLoS ONE, 12(8): e0182662, 2017", "doi": "10.1371/journal.pone.0182662", "report-no": null, "categories": "cs.SI cond-mat.dis-nn cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing works by distributing many small tasks to large numbers of\nworkers, yet the true potential of crowdsourcing lies in workers doing more\nthan performing simple tasks---they can apply their experience and creativity\nto provide new and unexpected information to the crowdsourcer. One such case is\nwhen workers not only answer a crowdsourcer's questions but also contribute new\nquestions for subsequent crowd analysis, leading to a growing set of questions.\nThis growth creates an inherent bias for early questions since a question\nintroduced earlier by a worker can be answered by more subsequent workers than\na question introduced later. Here we study how to perform efficient\ncrowdsourcing with such growing question sets. By modeling question sets as\nnetworks of interrelated questions, we introduce algorithms to help curtail the\ngrowth bias by efficiently distributing workers between exploring new questions\nand addressing current questions. Experiments and simulations demonstrate that\nthese algorithms can efficiently explore an unbounded set of questions without\nlosing confidence in crowd answers.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 11:03:19 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 22:40:08 GMT"}, {"version": "v3", "created": "Tue, 15 Aug 2017 01:56:11 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["McAndrew", "Thomas C.", ""], ["Guseva", "Elizaveta A.", ""], ["Bagrow", "James P.", ""]]}, {"id": "1611.01056", "submitter": "Maxime Lenormand", "authors": "Fabio Lamanna, Maxime Lenormand, Mar\\'ia Henar Salas-Olmedo, Gustavo\n  Romanillos, Bruno Gon\\c{c}alves and Jos\\'e J. Ramasco", "title": "Immigrant community integration in world cities", "comments": "13 pages, 5 figures + Appendix", "journal-ref": "PLoS ONE 13, e0191612 (2018)", "doi": "10.1371/journal.pone.0191612", "report-no": null, "categories": "physics.soc-ph cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a consequence of the accelerated globalization process, today major cities\nall over the world are characterized by an increasing multiculturalism. The\nintegration of immigrant communities may be affected by social polarization and\nspatial segregation. How are these dynamics evolving over time? To what extent\nthe different policies launched to tackle these problems are working? These are\ncritical questions traditionally addressed by studies based on surveys and\ncensus data. Such sources are safe to avoid spurious biases, but the data\ncollection becomes an intensive and rather expensive work. Here, we conduct a\ncomprehensive study on immigrant integration in 53 world cities by introducing\nan innovative approach: an analysis of the spatio-temporal communication\npatterns of immigrant and local communities based on language detection in\nTwitter and on novel metrics of spatial integration. We quantify the \"Power of\nIntegration\" of cities --their capacity to spatially integrate diverse\ncultures-- and characterize the relations between different cultures when\nacting as hosts or immigrants.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 15:17:07 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 18:13:46 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Lamanna", "Fabio", ""], ["Lenormand", "Maxime", ""], ["Salas-Olmedo", "Mar\u00eda Henar", ""], ["Romanillos", "Gustavo", ""], ["Gon\u00e7alves", "Bruno", ""], ["Ramasco", "Jos\u00e9 J.", ""]]}, {"id": "1611.01152", "submitter": "Gouri Ginde", "authors": "Gouri Ginde", "title": "Visualisation of massive data from scholarly Article and Journal\n  Database A Novel Scheme", "comments": "5 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scholarly articles publishing and getting cited has become a way of life for\nacademicians. These scholarly publications shape up the career growth of not\nonly the authors but also of the country, continent and the technological\ndomains. Author affiliations, country and other information of an author\ncoupled with data analytics can provide useful and insightful results. However,\nmassive and complete data is required to perform this research. Google scholar\nwhich is a comprehensive and free repository of scholarly articles has been\nused as a data source for this purpose. Data scraped from Google scholar when\nstored as a graph and visualized in the form of nodes and relationships, can\noffer discerning and concealed information. Such as, evident domain shift of an\nauthor, various research domains spread for an author, prediction of emerging\ndomain and sub domains, detection of journal and author level citation cartel\nbehaviors etc. The data from graph database is also used in computation of\nscholastic indicators for the journals. Eventually, econometric model, named\nCobb Douglas model is used to compute the journals Modeling \"Internationality\"\nIndex based on these scholastic indicators.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 10:23:59 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Ginde", "Gouri", ""]]}, {"id": "1611.01257", "submitter": "Brian Keegan", "authors": "Marlon Twyman and Brian C. Keegan and Aaron Shaw", "title": "Black Lives Matter in Wikipedia: Collaboration and Collective Memory\n  around Online Social Movements", "comments": "14 pages", "journal-ref": null, "doi": "10.1145/2998181.2998232", "report-no": null, "categories": "cs.SI cs.CY cs.HC physics.soc-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Social movements use social computing systems to complement offline\nmobilizations, but prior literature has focused almost exclusively on movement\nactors' use of social media. In this paper, we analyze participation and\nattention to topics connected with the Black Lives Matter movement in the\nEnglish language version of Wikipedia between 2014 and 2016. Our results point\nto the use of Wikipedia to (1) intensively document and connect historical and\ncontemporary events, (2) collaboratively migrate activity to support coverage\nof new events, and (3) dynamically re-appraise pre-existing knowledge in the\naftermath of new events. These findings reveal patterns of behavior that\ncomplement theories of collective memory and collective action and help explain\nhow social computing systems can encode and retrieve knowledge about social\nmovements as they unfold.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 03:35:58 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Twyman", "Marlon", ""], ["Keegan", "Brian C.", ""], ["Shaw", "Aaron", ""]]}, {"id": "1611.01549", "submitter": "Sarah Evans", "authors": "Sarah Evans, Katie Davis, Abigail Evans, Julie Ann Campbell, David P.\n  Randall, Kodlee Yin, and Cecilia Aragon", "title": "More Than Peer Production: Fanfiction Communities as Sites of\n  Distributed Mentoring", "comments": null, "journal-ref": null, "doi": "10.1145/2998181.2998342", "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From Harry Potter to American Horror Story, fanfiction is extremely popular\namong young people. Sites such as Fanfiction.net host millions of stories, with\nthousands more posted each day. Enthusiasts are sharing their writing and\nreading stories written by others. Exactly how does a generation known more for\nvideogame expertise than long-form writing become so engaged in reading and\nwriting in these communities? Via a nine-month ethnographic investigation of\nfanfiction communities that included participant observation, interviews, a\nthematic analysis of 4,500 reader reviews and an in-depth case study of a\ndiscussion group, we found that members of fanfiction communities spontaneously\nmentor each other in open forums, and that this mentoring builds upon previous\ninteractions in a way that is distinct from traditional forms of mentoring and\nmade possible by the affordances of networked publics. This work extends and\ndevelops the theory of distributed mentoring. Our findings illustrate how\ndistributed mentoring supports fanfiction authors as they work to develop their\nwriting skills. We believe distributed mentoring holds potential for supporting\nlearning in a variety of formal and informal learning environments.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 21:54:11 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Evans", "Sarah", ""], ["Davis", "Katie", ""], ["Evans", "Abigail", ""], ["Campbell", "Julie Ann", ""], ["Randall", "David P.", ""], ["Yin", "Kodlee", ""], ["Aragon", "Cecilia", ""]]}, {"id": "1611.01572", "submitter": "Mark Whiting", "authors": "Mark E. Whiting, Dilrukshi Gamage, Snehalkumar S. Gaikwad, Aaron\n  Gilbee, Shirish Goyal, Alipta Ballav, Dinesh Majeti, Nalin Chhibber, Angela\n  Richmond-Fuller, Freddie Vargus, Tejas Seshadri Sarma, Varshine\n  Chandrakanthan, Teogenes Moura, Mohamed Hashim Salih, Gabriel Bayomi Tinoco\n  Kalejaiye, Adam Ginzberg, Catherine A. Mullings, Yoni Dayan, Kristy Milland,\n  Henrique Orefice, Jeff Regino, Sayna Parsi, Kunz Mainali, Vibhor Sehgal,\n  Sekandar Matin, Akshansh Sinha, Rajan Vaish, Michael S. Bernstein", "title": "Crowd Guilds: Worker-led Reputation and Feedback on Crowdsourcing\n  Platforms", "comments": "12 pages, 6 figures, 1 table. To be presented at CSCW2017", "journal-ref": "ACM Conference on Computer Supported Cooperative Work and Social\n  Computing. ACM, New York, NY, USA, 1902-1913", "doi": "10.1145/2998181.2998234", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd workers are distributed and decentralized. While decentralization is\ndesigned to utilize independent judgment to promote high-quality results, it\nparadoxically undercuts behaviors and institutions that are critical to\nhigh-quality work. Reputation is one central example: crowdsourcing systems\ndepend on reputation scores from decentralized workers and requesters, but\nthese scores are notoriously inflated and uninformative. In this paper, we draw\ninspiration from historical worker guilds (e.g., in the silk trade) to design\nand implement crowd guilds: centralized groups of crowd workers who\ncollectively certify each other's quality through double-blind peer assessment.\nA two-week field experiment compared crowd guilds to a traditional\ndecentralized crowd work model. Crowd guilds produced reputation signals more\nstrongly correlated with ground-truth worker quality than signals available on\ncurrent crowd working platforms, and more accurate than in the traditional\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 23:56:52 GMT"}, {"version": "v2", "created": "Sat, 3 Dec 2016 21:36:58 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 19:05:21 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Whiting", "Mark E.", ""], ["Gamage", "Dilrukshi", ""], ["Gaikwad", "Snehalkumar S.", ""], ["Gilbee", "Aaron", ""], ["Goyal", "Shirish", ""], ["Ballav", "Alipta", ""], ["Majeti", "Dinesh", ""], ["Chhibber", "Nalin", ""], ["Richmond-Fuller", "Angela", ""], ["Vargus", "Freddie", ""], ["Sarma", "Tejas Seshadri", ""], ["Chandrakanthan", "Varshine", ""], ["Moura", "Teogenes", ""], ["Salih", "Mohamed Hashim", ""], ["Kalejaiye", "Gabriel Bayomi Tinoco", ""], ["Ginzberg", "Adam", ""], ["Mullings", "Catherine A.", ""], ["Dayan", "Yoni", ""], ["Milland", "Kristy", ""], ["Orefice", "Henrique", ""], ["Regino", "Jeff", ""], ["Parsi", "Sayna", ""], ["Mainali", "Kunz", ""], ["Sehgal", "Vibhor", ""], ["Matin", "Sekandar", ""], ["Sinha", "Akshansh", ""], ["Vaish", "Rajan", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "1611.01817", "submitter": "Tommy Nilsson", "authors": "Jakub Binter, Kate\\v{r}ina Klapilov\\'a, Tereza Zik\\'anov\\'a, Tommy\n  Nilsson, Kl\\'ara B\\'artov\\'a, Lucie Krejcov\\'a, Renata Androvicov\\'a, Jitka\n  Lindov\\'a, Denisa Pru\\v{s}ov\\'a, Timothy Wells, Daniel Riha", "title": "Exploring the Pathways of Adaptation an Avatar 3D Animation Procedures\n  and Virtual Reality Arenas in Research of Human Courtship Behaviour and\n  Sexual Reactivity in Psychological Research", "comments": "10 pages, Virtual, Augmented and Mixed Reality: Changing the Face of\n  Learning, Special Conference Stream within The Experiential Learning in\n  Virtual Worlds Project: 5th Global Meeting, Dubrovnik, Croatia, 10-12 May\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many reasons for utilising 3D animation and virtual reality in\nsexuality research. Apart from providing a mean with which to (re)experience\ncertain situations there are four main advantages: a) bespoke animated stimuli\ncan be created and customized, which is especially important when researching\nparaphilia and sexual preferences, b) stimulus production is less expensive and\neasier to produce compared to real world stimuli, c) virtual reality allows us\nto capture data such as physiological reasons to stimuli, that we would not be\nable to otherwise (without resorting to self-report measures which are\nespecially problematic in this research domain), d) ethical, legal, and health\nand safety issues are less complex since neither physical nor psychological\nharm is caused to animated characters allowing for the safe presentation of\nstimuli involving vulnerable targets. The animation sub-group has been\nexploring so far several production quality levels and various animation\nprocedures in a number of available software. The aim is to develop static as\nwell as dynamic, interactive sexual stimuli for sexual diagnostic and\ntherapeutic purposes. We are aware of number of ethical issues related to the\nuse of virtual reality in proposed research are analysed in this chapter.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 18:27:09 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Binter", "Jakub", ""], ["Klapilov\u00e1", "Kate\u0159ina", ""], ["Zik\u00e1nov\u00e1", "Tereza", ""], ["Nilsson", "Tommy", ""], ["B\u00e1rtov\u00e1", "Kl\u00e1ra", ""], ["Krejcov\u00e1", "Lucie", ""], ["Androvicov\u00e1", "Renata", ""], ["Lindov\u00e1", "Jitka", ""], ["Pru\u0161ov\u00e1", "Denisa", ""], ["Wells", "Timothy", ""], ["Riha", "Daniel", ""]]}, {"id": "1611.02119", "submitter": "Denis Parra", "authors": "Ivania Donoso and Denis Parra", "title": "EpistAid: Interactive Interface for Document Filtering in Evidence-based\n  Health Care", "comments": "5 pages, 4 figures, pre-print submitted to ACM IUI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence-based health care (EBHC) is an important practice of medicine which\nattempts to provide systematic scientific evidence to answer clinical\nquestions. In this context, Epistemonikos (www.epistemonikos.org) is one of the\nfirst and most important online systems in the field, providing an interface\nthat supports users on searching and filtering scientific articles for\npracticing EBHC. The system nowadays requires a large amount of expert human\neffort, where close to 500 physicians manually curate articles to be utilized\nin the platform. In order to scale up the large and continuous amount of data\nto keep the system updated, we introduce EpistAid, an interactive intelligent\ninterface which supports clinicians in the process of curating documents for\nEpistemonikos within lists of papers called evidence matrices. We introduce the\ncharacteristics, design and algorithms of our solution, as well as a prototype\nimplementation and a case study to show how our solution addresses the\ninformation overload problem in this area.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 15:38:07 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Donoso", "Ivania", ""], ["Parra", "Denis", ""]]}, {"id": "1611.02145", "submitter": "Olga Russakovsky", "authors": "Adriana Kovashka, Olga Russakovsky, Li Fei-Fei, Kristen Grauman", "title": "Crowdsourcing in Computer Vision", "comments": "A 69-page meta review of the field, Foundations and Trends in\n  Computer Graphics and Vision, 2016", "journal-ref": null, "doi": "10.1561/0600000073", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision systems require large amounts of manually annotated data to\nproperly learn challenging visual concepts. Crowdsourcing platforms offer an\ninexpensive method to capture human knowledge and understanding, for a vast\nnumber of visual perception tasks. In this survey, we describe the types of\nannotations computer vision researchers have collected using crowdsourcing, and\nhow they have ensured that this data is of high quality while annotation effort\nis minimized. We begin by discussing data collection on both classic (e.g.,\nobject recognition) and recent (e.g., visual story-telling) vision tasks. We\nthen summarize key design decisions for creating effective data collection\ninterfaces and workflows, and present strategies for intelligently selecting\nthe most important data instances to annotate. Finally, we conclude with some\nthoughts on the future of crowdsourcing in computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 16:11:19 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Kovashka", "Adriana", ""], ["Russakovsky", "Olga", ""], ["Fei-Fei", "Li", ""], ["Grauman", "Kristen", ""]]}, {"id": "1611.02459", "submitter": "Johann Schrammel", "authors": "Helmut Schrom-Feiertag and Martin Stubenschrott and Georg Regal and\n  Johann Schrammel and Volker Settgast", "title": "Using cognitive agent-based simulation for the evaluation of indoor\n  wayfinding systems", "comments": "13th International Conference on Design & Decision Support Systems in\n  Architecture and Urban Planning. June 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to simulate human wayfinding behaviour\nincorporating visual cognition into a software agent for a computer aided\nevaluation of wayfinding systems in large infrastructures. The proposed\napproach follows the Sense-Plan-Act paradigm comprised of a model for visual\nattention, navigation behaviour and pedestrian movement. Stochastic features of\nperception are incorporated to enhance generality and diversity of the\ndeveloped wayfinding simulation to reflect a variety of behaviours. The\nvalidity of the proposed approach was evaluated based on empirical data\ncollected through wayfinding experiments with 20 participants in an immersive\nvirtual reality environment using a life-sized 3D replica of Vienna's new\ncentral railway station. The results show that the developed cognitive\nagent-based simulation provides a further contribution to the simulation of\nhuman wayfinding and subsequently a further step to an effective evaluation\ntool for the planning of wayfinding and signage.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 10:04:44 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Schrom-Feiertag", "Helmut", ""], ["Stubenschrott", "Martin", ""], ["Regal", "Georg", ""], ["Schrammel", "Johann", ""], ["Settgast", "Volker", ""]]}, {"id": "1611.02666", "submitter": "Joy Kim", "authors": "Joy Kim, Maneesh Agrawala, Michael S. Bernstein", "title": "Mosaic: Designing Online Creative Communities for Sharing\n  Works-in-Progress", "comments": null, "journal-ref": null, "doi": "10.1145/2998181.2998195", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online creative communities allow creators to share their work with a large\naudience, maximizing opportunities to showcase their work and connect with fans\nand peers. However, sharing in-progress work can be technically and socially\nchallenging in environments designed for sharing completed pieces. We propose\nan online creative community where sharing process, rather than showcasing\noutcomes, is the main method of sharing creative work. Based on this, we\npresent Mosaic---an online community where illustrators share work-in-progress\nsnapshots showing how an artwork was completed from start to finish. In an\nonline deployment and observational study, artists used Mosaic as a vehicle for\nreflecting on how they can improve their own creative process, developed a\nsocial norm of detailed feedback, and became less apprehensive of sharing early\nversions of artwork. Through Mosaic, we argue that communities oriented around\nsharing creative process can create a collaborative environment that is\nbeneficial for creative growth.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 19:34:51 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Kim", "Joy", ""], ["Agrawala", "Maneesh", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "1611.02682", "submitter": "Joy Kim", "authors": "Joy Kim, Sarah Sterman, Allegra Argent Beal Cohen, Michael S.\n  Bernstein", "title": "Mechanical Novel: Crowdsourcing Complex Work through Reflection and\n  Revision", "comments": null, "journal-ref": null, "doi": "10.1145/2998181.2998196", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing systems accomplish large tasks with scale and speed by breaking\nwork down into independent parts. However, many types of complex creative work,\nsuch as fiction writing, have remained out of reach for crowds because work is\ntightly interdependent: changing one part of a story may trigger changes to the\noverall plot and vice versa. Taking inspiration from how expert authors write,\nwe propose a technique for achieving interdependent complex goals with crowds.\nWith this technique, the crowd loops between reflection, to select a high-level\ngoal, and revision, to decompose that goal into low-level, actionable tasks. We\nembody this approach in Mechanical Novel, a system that crowdsources short\nfiction stories on Amazon Mechanical Turk. In a field experiment, Mechanical\nNovel resulted in higher-quality stories than an iterative crowdsourcing\nworkflow. Our findings suggest that orienting crowd work around high-level\ngoals may enable workers to coordinate their effort to accomplish complex work.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 20:42:03 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Kim", "Joy", ""], ["Sterman", "Sarah", ""], ["Cohen", "Allegra Argent Beal", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "1611.03202", "submitter": "Mohammad Abu Alsheikh", "authors": "Mohammad Abu Alsheikh, Dusit Niyato, Shaowei Lin, Hwee-Pink Tan, and\n  Dong In Kim", "title": "Fast Adaptation of Activity Sensing Policies in Mobile Devices", "comments": "14 pages, 10 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of sensors, such as accelerometers, in mobile devices,\nactivity and motion tracking has become a viable technology to understand and\ncreate an engaging user experience. This paper proposes a fast adaptation and\nlearning scheme of activity tracking policies when user statistics are unknown\na priori, varying with time, and inconsistent for different users. In our\nstochastic optimization, user activities are required to be synchronized with a\nbackend under a cellular data limit to avoid overcharges from cellular\noperators. The mobile device is charged intermittently using wireless or wired\ncharging for receiving the required energy for transmission and sensing\noperations. Firstly, we propose an activity tracking policy by formulating a\nstochastic optimization as a constrained Markov decision process (CMDP).\nSecondly, we prove that the optimal policy of the CMDP has a threshold\nstructure using a Lagrangian relaxation approach and the submodularity concept.\nWe accordingly present a fast Q-learning algorithm by considering the policy\nstructure to improve the convergence speed over that of conventional\nQ-learning. Finally, simulation examples are presented to support the\ntheoretical findings of this paper.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 07:19:12 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Alsheikh", "Mohammad Abu", ""], ["Niyato", "Dusit", ""], ["Lin", "Shaowei", ""], ["Tan", "Hwee-Pink", ""], ["Kim", "Dong In", ""]]}, {"id": "1611.03340", "submitter": "Karen Renaud", "authors": "Noura Aleisa and Karen Renaud", "title": "Privacy of the Internet of Things: A Systematic Literature Review\n  (Extended Discussion)", "comments": "Extended version of a paper to appear in HICSS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things' potential for major privacy invasion is a concern.\nThis paper reports on a systematic literature review of privacy-preserving\nsolutions appearing in the research literature and in the media. We analysed\nproposed solutions in terms of the techniques they deployed and the extent to\nwhich they satisfied core privacy principles. We found that very few solutions\nsatisfied all core privacy principles. We also identified a number of key\nknowledge gaps in the course of the analysis. In particular, we found that most\nsolution providers assumed that end users would be willing to expend effort to\npreserve their privacy; that they would be motivated to act to preserve their\nprivacy. The validity of this assumption needs to be proved, since it cannot\nsimply be assumed that people would necessarily be willing to engage with these\nsolutions. We suggest this as a topic for future research.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 06:16:57 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Aleisa", "Noura", ""], ["Renaud", "Karen", ""]]}, {"id": "1611.03354", "submitter": "Felix Bork", "authors": "Felix Bork, Roghayeh Barmaki, Ulrich Eck, Pascal Fallavollita,\n  Bernhard Fuerst, Nassir Navab", "title": "Exploring Non-Reversing Magic Mirrors for Screen-Based Augmented Reality\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screen-based Augmented Reality (AR) systems can be built as a window into the\nreal world as often done in mobile AR applications or using the Magic Mirror\nmetaphor, where users can see themselves with augmented graphics on a large\ndisplay. Such Magic Mirror systems have been used in digital clothing\nenvironments to create virtual dressing rooms, to teach human anatomy, and for\ncollaborative design tasks. The term Magic Mirror implies that the display\nshows the users enantiomorph, i.e. the mirror image, such that the system\nmimics a real-world physical mirror. However, the question arises whether one\nshould design a traditional mirror, or instead display the true mirror image by\nmeans of a non-reversing mirror? This is an intriguing perceptual question, as\nthe image one observes in a mirror is not a real view, as it would be seen by\nan external observer, but a reflection, i.e. a front-to-back reversed image. In\nthis paper, we discuss the perceptual differences between these two mirror\nvisualization concepts and present a first comparative study in the context of\nMagic Mirror anatomy teaching. We investigate the ability of users to identify\nthe correct placement of virtual anatomical structures in our screen-based AR\nsystem for two conditions: a regular mirror and a non-reversing mirror setup.\nThe results of our study indicate that the latter is more suitable for\napplications where previously acquired domain-specific knowledge plays an\nimportant role. The lessons learned open up new research directions in the\nfields of user interfaces and interaction in non-reversing mirror environments\nand could impact the implementation of general screen-based AR systems in other\ndomains.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 15:45:35 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 15:02:12 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Bork", "Felix", ""], ["Barmaki", "Roghayeh", ""], ["Eck", "Ulrich", ""], ["Fallavollita", "Pascal", ""], ["Fuerst", "Bernhard", ""], ["Navab", "Nassir", ""]]}, {"id": "1611.03618", "submitter": "Isaac Skog", "authors": "Johan Wahlstr\\\"om, Isaac Skog, Peter H\\\"andel", "title": "Smartphone-based Vehicle Telematics - A Ten-Year Anniversary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just like it has irrevocably reshaped social life, the fast growth of\nsmartphone ownership is now beginning to revolutionize the driving experience\nand change how we think about automotive insurance, vehicle safety systems, and\ntraffic research. This paper summarizes the first ten years of research in\nsmartphone-based vehicle telematics, with a focus on user-friendly\nimplementations and the challenges that arise due to the mobility of the\nsmartphone. Notable academic and industrial projects are reviewed, and system\naspects related to sensors, energy consumption, cloud computing, vehicular ad\nhoc networks, and human-machine interfaces are examined. Moreover, we highlight\nthe differences between traditional and smartphonebased automotive navigation,\nand survey the state-of-the-art in smartphone-based transportation mode\nclassification, driver classification, and road condition monitoring. Future\nadvances are expected to be driven by improvements in sensor technology,\nevidence of the societal benefits of current implementations, and the\nestablishment of industry standards for sensor fusion and driver assessment\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 08:55:27 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Wahlstr\u00f6m", "Johan", ""], ["Skog", "Isaac", ""], ["H\u00e4ndel", "Peter", ""]]}, {"id": "1611.03799", "submitter": "Rishin Haldar", "authors": "Rohan Kar, Rishin Haldar", "title": "Applying Chatbots to the Internet of Things: Opportunities and\n  Architectural Elements", "comments": "9 pages, 3 figures, 5 Use Cases", "journal-ref": null, "doi": "10.14569/IJACSA.2016.071119", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) is emerging as a significant technology in shaping\nthe future by connecting physical devices or things with internet. It also\npresents various opportunities for intersection of other technological trends\nwhich can allow it to become even more intelligent and efficient. In this paper\nwe focus our attention on the integration of Intelligent Conversational\nSoftware Agents or Chatbots with IoT. Literature surveys have looked into\nvarious applications, features, underlying technologies and known challenges of\nIoT. On the other hand, Chatbots are being adopted in greater numbers due to\nmajor strides in development of platforms and frameworks. The novelty of this\npaper lies in the specific integration of Chatbots in the IoT scenario. We\nanalyzed the shortcomings of existing IoT systems and put forward ways to\ntackle them by incorporating chatbots. A general architecture is proposed for\nimplementing such a system, as well as platforms and frameworks, both\ncommercial and open source, which allow for implementation of such systems.\nIdentification of the newer challenges and possible future directions with this\nnew integration, have also been addressed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 17:56:04 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Kar", "Rohan", ""], ["Haldar", "Rishin", ""]]}, {"id": "1611.03906", "submitter": "Thanapong Intharah", "authors": "Thanapong Intharah, Daniyar Turmukhambetov, Gabriel J. Brostow", "title": "Help, It Looks Confusing: GUI Task Automation Through Demonstration and\n  Follow-up Questions", "comments": "Camera Ready version. Accepted to be presented at the ACM IUI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-programming users should be able to create their own customized scripts\nto perform computer-based tasks for them, just by demonstrating to the machine\nhow it's done. To that end, we develop a system prototype which\nlearns-by-demonstration called HILC (Help, It Looks Confusing). Users train\nHILC to synthesize a task script by demonstrating the task, which produces the\nneeded screenshots and their corresponding mouse-keyboard signals. After the\ndemonstration, the user answers follow-up questions.\n  We propose a user-in-the-loop framework that learns to generate scripts of\nactions performed on visible elements of graphical applications. While pure\nprogramming-by-demonstration is still unrealistic, we use quantitative and\nqualitative experiments to show that non-programming users are willing and\neffective at answering follow-up queries posed by our system. Our models of\nevents and appearance are surprisingly simple, but are combined effectively to\ncope with varying amounts of supervision.\n  The best available baseline, Sikuli Slides, struggled with the majority of\nthe tests in our user study experiments. The prototype with our proposed\napproach successfully helped users accomplish simple linear tasks, complicated\ntasks (monitoring, looping, and mixed), and tasks that span across multiple\nexecutables. Even when both systems could ultimately perform a task, ours was\ntrained and refined by the user in less time.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 22:38:59 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 16:03:27 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Intharah", "Thanapong", ""], ["Turmukhambetov", "Daniyar", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1611.04833", "submitter": "Enrico Calore", "authors": "Enrico Calore", "title": "Steady State Visually Evoked Potentials detection using a single\n  electrode consumer-grade EEG device for BCI applications", "comments": "Work conducted between 2013 and 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Brain-Computer Interfaces (BCIs) implement a direct communication pathway\nbetween the brain of an user and an external device, as a computer or a machine\nin general. One of the most used brain responses to implement non-invasive BCIs\nis the so called steady-state visually evoked potential (SSVEP). This periodic\nresponse is generated when an user gazes to a light flickering at a constant\nfrequency. The SSVEP response can be detected in the user's\nelectroencephalogram (EEG) at the corresponding frequency of the attended\nflickering stimulus. In SSVEP based BCIs, multiple stimuli, flickering at\ndifferent frequencies, are commonly presented to the user, where to each\nstimulus is associated a command for an actuator. One of the limitations to a\nwider adoption of BCIs is given by the need of EEG acquisition devices and\nsoftware tools which are commonly not meant for end-user usage. In this work,\nexploiting state-of-the-art software tools, the use of a low cost easy to wear\nsingle electrode EEG device is demonstrated to be exploitable to implement\nsimple SSVEP based BCIs. The obtained results, although less impressive than\nthe ones obtainable with professional EEG equipment, are interesting in view of\npractical low cost BCI applications meant for end-users.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 14:02:47 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Calore", "Enrico", ""]]}, {"id": "1611.05379", "submitter": "Roger Moore", "authors": "Prof. Roger K. Moore", "title": "PCT and Beyond: Towards a Computational Framework for `Intelligent'\n  Communicative Systems", "comments": "To appear in A. McElhone & W. Mansell (Eds.), Living Control Systems\n  IV: Perceptual Control Theory and the Future of the Life and Social Sciences,\n  Benchmark Publications Inc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed increasing interest in the potential benefits of\n`intelligent' autonomous machines such as robots. Honda's Asimo humanoid robot,\niRobot's Roomba robot vacuum cleaner and Google's driverless cars have fired\nthe imagination of the general public, and social media buzz with speculation\nabout a utopian world of helpful robot assistants or the coming robot\napocalypse! However, there is a long way to go before autonomous systems reach\nthe level of capabilities required for even the simplest of tasks involving\nhuman-robot interaction - especially if it involves communicative behaviour\nsuch as speech and language. Of course the field of Artificial Intelligence\n(AI) has made great strides in these areas, and has moved on from abstract\nhigh-level rule-based paradigms to embodied architectures whose operations are\ngrounded in real physical environments. What is still missing, however, is an\noverarching theory of intelligent communicative behaviour that informs\nsystem-level design decisions in order to provide a more coherent approach to\nsystem integration. This chapter introduces the beginnings of such a framework\ninspired by the principles of Perceptual Control Theory (PCT). In particular,\nit is observed that PCT has hitherto tended to view perceptual processes as a\nrelatively straightforward series of transformations from sensation to\nperception, and has overlooked the potential of powerful generative model-based\nsolutions that have emerged in practical fields such as visual or auditory\nscene analysis. Starting from first principles, a sequence of arguments is\npresented which not only shows how these ideas might be integrated into PCT,\nbut which also extend PCT towards a remarkably symmetric architecture for a\nneeds-driven communicative agent. It is concluded that, if behaviour is the\ncontrol of perception, then perception is the simulation of behaviour.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 17:32:10 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Moore", "Prof. Roger K.", ""]]}, {"id": "1611.05469", "submitter": "Daniel Smilkov", "authors": "Daniel Smilkov, Nikhil Thorat, Charles Nicholson, Emily Reif, Fernanda\n  B. Vi\\'egas, Martin Wattenberg", "title": "Embedding Projector: Interactive Visualization and Interpretation of\n  Embeddings", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings are ubiquitous in machine learning, appearing in recommender\nsystems, NLP, and many other applications. Researchers and developers often\nneed to explore the properties of a specific embedding, and one way to analyze\nembeddings is to visualize them. We present the Embedding Projector, a tool for\ninteractive visualization and interpretation of embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 21:21:11 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Smilkov", "Daniel", ""], ["Thorat", "Nikhil", ""], ["Nicholson", "Charles", ""], ["Reif", "Emily", ""], ["Vi\u00e9gas", "Fernanda B.", ""], ["Wattenberg", "Martin", ""]]}, {"id": "1611.06175", "submitter": "Adrien Bibal", "authors": "Adrien Bibal and Benoit Fr\\'enay", "title": "Learning Interpretability for Visualizations using Adapted Cox Models\n  through a User Experiment", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to be useful, visualizations need to be interpretable. This paper\nuses a user-based approach to combine and assess quality measures in order to\nbetter model user preferences. Results show that cluster separability measures\nare outperformed by a neighborhood conservation measure, even though the former\nare usually considered as intuitively representative of user motives. Moreover,\ncombining measures, as opposed to using a single measure, further improves\nprediction performances.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 17:52:23 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Bibal", "Adrien", ""], ["Fr\u00e9nay", "Benoit", ""]]}, {"id": "1611.06292", "submitter": "Thiago Porcino", "authors": "Thiago M. Porcino, Esteban W. Clua, Cristina N. Vasconcelos, Daniela\n  Trevisan, Luis Valente", "title": "Minimizing cyber sickness in head mounted display systems: design\n  guidelines and applications", "comments": "11 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We are experiencing an upcoming trend of using head mounted display systems\nin games and serious games, which is likely to become an established practice\nin the near future. While these systems provide highly immersive experiences,\nmany users have been reporting discomfort symptoms, such as nausea, sickness,\nand headaches, among others. When using VR for health applications, this is\nmore critical, since the discomfort may interfere a lot in treatments. In this\nwork we discuss possible causes of these issues, and present possible solutions\nas design guidelines that may mitigate them. In this context, we go deeper\nwithin a dynamic focus solution to reduce discomfort in immersive virtual\nenvironments, when using first-person navigation. This solution applies an\nheuristic model of visual attention that works in real time. This work also\ndiscusses a case study (as a first-person spatial shooter demo) that applies\nthis solution and the proposed design guidelines.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 02:01:44 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Porcino", "Thiago M.", ""], ["Clua", "Esteban W.", ""], ["Vasconcelos", "Cristina N.", ""], ["Trevisan", "Daniela", ""], ["Valente", "Luis", ""]]}, {"id": "1611.06478", "submitter": "Salman Mahmood", "authors": "Salman Mahmood, Rami Al-Rfou, Klaus Mueller", "title": "Visualizing Linguistic Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based models are a very powerful tool for creating word\nembeddings, the objective of these models is to group similar words together.\nThese embeddings have been used as features to improve results in various\napplications such as document classification, named entity recognition, etc.\nNeural language models are able to learn word representations which have been\nused to capture semantic shifts across time and geography. The objective of\nthis paper is to first identify and then visualize how words change meaning in\ndifferent text corpus. We will train a neural language model on texts from a\ndiverse set of disciplines philosophy, religion, fiction etc. Each text will\nalter the embeddings of the words to represent the meaning of the word inside\nthat text. We will present a computational technique to detect words that\nexhibit significant linguistic shift in meaning and usage. We then use enhanced\nscatterplots and storyline visualization to visualize the linguistic shift.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 06:50:16 GMT"}], "update_date": "2016-11-27", "authors_parsed": [["Mahmood", "Salman", ""], ["Al-Rfou", "Rami", ""], ["Mueller", "Klaus", ""]]}, {"id": "1611.06587", "submitter": "Mithileysh Sathiyanarayanan Mr", "authors": "Mithileysh Sathiyanarayanan and Tobias Mulling", "title": "Wellformedness Properties in Euler Diagrams: An Eye Tracking Study for\n  Visualisation Evaluation", "comments": "4 pages, 2 figures, the Brazilian Computing Society, the XIV\n  Brazilian Symposium on Human Factors in Computer Systems (IHC 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the field of information visualisation, Euler diagrams are an important\ntool used in various application areas such as engineering, medicine and social\nanalysis. To effectively use Euler diagrams, some of the wellformedness\nproperties needs to be avoided, as they are considered to reduce user\ncomprehension. From the previous empirical studies, we know some properties are\nswappable but there is no clear justification which property would be the best\nto use. In this paper, we considered two main wellformedness properties\n(duplicated curve labels and disconnected zones) to test which among the two\naffect user comprehension the most, based on the task performance (accuracy and\nresponse time), preference and eye movements of the users. Twelve participants\nperformed three different types of tasks with nine diagrams of each property\n(so, in total eighteen diagrams) and the results showed that duplicated curve\nlabels property slows down and trigger extra eye movements, causing delays for\nthe tasks. Though there is no significant difference in the accuracy but the\ninsights obtained from the response time, preference and eye movements will be\nuseful for software developers on the optimal way to visualise Euler diagrams\nin real world application areas.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 20:33:11 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Sathiyanarayanan", "Mithileysh", ""], ["Mulling", "Tobias", ""]]}, {"id": "1611.07135", "submitter": "Jason Portenoy", "authors": "Jason Portenoy, Jessica Hullman, Jevin D. West", "title": "Leveraging Citation Networks to Visualize Scholarly Influence Over Time", "comments": null, "journal-ref": null, "doi": "10.3389/frma.2017.00008", "report-no": null, "categories": "cs.HC cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the influence of a scholar's work is an important task for funding\norganizations, academic departments, and researchers. Common methods, such as\nmeasures of citation counts, can ignore much of the nuance and\nmultidimensionality of scholarly influence. We present an approach for\ngenerating dynamic visualizations of scholars' careers. This approach uses an\nanimated node-link diagram showing the citation network accumulated around the\nresearcher over the course of the career in concert with key indicators,\nhighlighting influence both within and across fields. We developed our design\nin collaboration with one funding organization---the Pew Biomedical Scholars\nprogram---but the methods are generalizable to visualizations of scholarly\ninfluence. We applied the design method to the Microsoft Academic Graph, which\nincludes more than 120 million publications. We validate our abstractions\nthroughout the process through collaboration with the Pew Biomedical Scholars\nprogram officers and summative evaluations with their scholars.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 03:17:23 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 23:25:00 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Portenoy", "Jason", ""], ["Hullman", "Jessica", ""], ["West", "Jevin D.", ""]]}, {"id": "1611.07139", "submitter": "Reza Rawassizadeh", "authors": "Reza Rawassizadeh, Chelsea Dobbins, Manouchehr Nourizadeh, Zahra\n  Ghamchili, Michael Pazzani", "title": "A Natural Language Query Interface for Searching Personal Information on\n  Smartwatches", "comments": "6 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, personal assistant systems, run on smartphones and use natural\nlanguage interfaces. However, these systems rely mostly on the web for finding\ninformation. Mobile and wearable devices can collect an enormous amount of\ncontextual personal data such as sleep and physical activities. These\ninformation objects and their applications are known as quantified-self, mobile\nhealth or personal informatics, and they can be used to provide a deeper\ninsight into our behavior. To our knowledge, existing personal assistant\nsystems do not support all types of quantified-self queries. In response to\nthis, we have undertaken a user study to analyze a set of \"textual\nquestions/queries\" that users have used to search their quantified-self or\nmobile health data. Through analyzing these questions, we have constructed a\nlight-weight natural language based query interface, including a text parser\nalgorithm and a user interface, to process the users' queries that have been\nused for searching quantified-self information. This query interface has been\ndesigned to operate on small devices, i.e. smartwatches, as well as augmenting\nthe personal assistant systems by allowing them to process end users' natural\nlanguage queries about their quantified-self data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 03:42:44 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Rawassizadeh", "Reza", ""], ["Dobbins", "Chelsea", ""], ["Nourizadeh", "Manouchehr", ""], ["Ghamchili", "Zahra", ""], ["Pazzani", "Michael", ""]]}, {"id": "1611.08154", "submitter": "Byungjoo Lee", "authors": "Byungjoo Lee, Mathieu Nancel, Sunjun Kim, Antti Oulasvirta", "title": "AutoGain: Gain Function Adaptation with Submovement Efficiency\n  Optimization", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-designed control-to-display gain function can improve pointing\nperformance with indirect pointing devices like trackpads. However, the design\nof gain functions is challenging and mostly based on trial and error. AutoGain\nis a novel method to individualize a gain function for indirect pointing\ndevices in contexts where cursor trajectories can be tracked. It gradually\nimproves pointing efficiency by using a novel submovement-level\ntracking+optimization technique that minimizes aiming error\n(undershooting/overshooting) for each submovement. We first show that AutoGain\ncan produce, from scratch, gain functions with performance comparable to\ncommercial designs, in less than a half-hour of active use. Second, we\ndemonstrate AutoGain's applicability to emerging input devices (here, a Leap\nMotion controller) with no reference gain functions. Third, a one-month\nlongitudinal study of normal computer use with AutoGain showed performance\nimprovements from participants' default functions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 11:36:32 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 14:29:47 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Lee", "Byungjoo", ""], ["Nancel", "Mathieu", ""], ["Kim", "Sunjun", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "1611.08215", "submitter": "Andrea Palazzi", "authors": "Andrea Palazzi, Francesco Solera, Simone Calderara, Stefano Alletto,\n  Rita Cucchiara", "title": "Learning Where to Attend Like a Human Driver", "comments": "To appear in IEEE Intelligent Vehicles Symposium 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the advent of autonomous cars, it's likely - at least in the near\nfuture - that human attention will still maintain a central role as a guarantee\nin terms of legal responsibility during the driving task. In this paper we\nstudy the dynamics of the driver's gaze and use it as a proxy to understand\nrelated attentional mechanisms. First, we build our analysis upon two\nquestions: where and what the driver is looking at? Second, we model the\ndriver's gaze by training a coarse-to-fine convolutional network on short\nsequences extracted from the DR(eye)VE dataset. Experimental comparison against\ndifferent baselines reveal that the driver's gaze can indeed be learnt to some\nextent, despite i) being highly subjective and ii) having only one driver's\ngaze available for each sequence due to the irreproducibility of the scene.\nEventually, we advocate for a new assisted driving paradigm which suggests to\nthe driver, with no intervention, where she should focus her attention.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 15:14:23 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 16:24:16 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Palazzi", "Andrea", ""], ["Solera", "Francesco", ""], ["Calderara", "Simone", ""], ["Alletto", "Stefano", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1611.08492", "submitter": "Wei-Long Zheng", "authors": "Wei-Long Zheng and Bao-Liang Lu", "title": "A Multimodal Approach to Estimating Vigilance Using EEG and Forehead EOG", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective. Covert aspects of ongoing user mental states provide key context\ninformation for user-aware human computer interactions. In this paper, we focus\non the problem of estimating the vigilance of users using EEG and EOG signals.\nApproach. To improve the feasibility and wearability of vigilance estimation\ndevices for real-world applications, we adopt a novel electrode placement for\nforehead EOG and extract various eye movement features, which contain the\nprincipal information of traditional EOG. We explore the effects of EEG from\ndifferent brain areas and combine EEG and forehead EOG to leverage their\ncomplementary characteristics for vigilance estimation. Considering that the\nvigilance of users is a dynamic changing process because the intrinsic mental\nstates of users involve temporal evolution, we introduce continuous conditional\nneural field and continuous conditional random field models to capture dynamic\ntemporal dependency. Main results. We propose a multimodal approach to\nestimating vigilance by combining EEG and forehead EOG and incorporating the\ntemporal dependency of vigilance into model training. The experimental results\ndemonstrate that modality fusion can improve the performance compared with a\nsingle modality, EOG and EEG contain complementary information for vigilance\nestimation, and the temporal dependency-based models can enhance the\nperformance of vigilance estimation. From the experimental results, we observe\nthat theta and alpha frequency activities are increased, while gamma frequency\nactivities are decreased in drowsy states in contrast to awake states.\nSignificance. The forehead setup allows for the simultaneous collection of EEG\nand EOG and achieves comparative performance using only four shared electrodes\nin comparison with the temporal and posterior sites.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 15:29:46 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Zheng", "Wei-Long", ""], ["Lu", "Bao-Liang", ""]]}, {"id": "1611.08520", "submitter": "Teemu K\\\"am\\\"ar\\\"ainen", "authors": "Teemu K\\\"am\\\"ar\\\"ainen, Matti Siekkinen, Antti Yl\\\"a-J\\\"a\\\"aski,\n  Wenxiao Zhang, Pan Hui", "title": "Dissecting the End-to-end Latency of Interactive Mobile Video\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we measure the step-wise latency in the pipeline of three kinds\nof interactive mobile video applications that are rapidly gaining popularity,\nnamely Remote Graphics Rendering (RGR) of which we focus on mobile cloud\ngaming, Mobile Augmented Reality (MAR), and Mobile Virtual Reality (MVR). The\napplications differ from each other by the way in which the user interacts with\nthe application, i.e., video I/O and user controls, but they all share in\ncommon the fact that their user experience is highly sensitive to end-to-end\nlatency. Long latency between a user control event and display update renders\nthe application unusable. Hence, understanding the nature and origins of\nlatency of these applications is of paramount importance. We show through\nextensive measurements that control input and display buffering have a\nsubstantial effect on the overall delay. Our results shed light on the latency\nbottlenecks and the maturity of technology for seamless user experience with\nthese applications.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 17:11:57 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["K\u00e4m\u00e4r\u00e4inen", "Teemu", ""], ["Siekkinen", "Matti", ""], ["Yl\u00e4-J\u00e4\u00e4ski", "Antti", ""], ["Zhang", "Wenxiao", ""], ["Hui", "Pan", ""]]}, {"id": "1611.08754", "submitter": "Lex Fridman", "authors": "Lex Fridman, Heishiro Toyoda, Sean Seaman, Bobbie Seppelt, Linda\n  Angell, Joonbum Lee, Bruce Mehler, Bryan Reimer", "title": "What Can Be Predicted from Six Seconds of Driver Glances?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a large dataset of real-world, on-road driving from a 100-car\nnaturalistic study to explore the predictive power of driver glances and,\nspecifically, to answer the following question: what can be predicted about the\nstate of the driver and the state of the driving environment from a 6-second\nsequence of macro-glances? The context-based nature of such glances allows for\napplication of supervised learning to the problem of vision-based gaze\nestimation, making it robust, accurate, and reliable in messy, real-world\nconditions. So, it's valuable to ask whether such macro-glances can be used to\ninfer behavioral, environmental, and demographic variables? We analyze 27\nbinary classification problems based on these variables. The takeaway is that\nglance can be used as part of a multi-sensor real-time system to predict\nradio-tuning, fatigue state, failure to signal, talking, and several\nenvironment variables.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 22:41:51 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Fridman", "Lex", ""], ["Toyoda", "Heishiro", ""], ["Seaman", "Sean", ""], ["Seppelt", "Bobbie", ""], ["Angell", "Linda", ""], ["Lee", "Joonbum", ""], ["Mehler", "Bruce", ""], ["Reimer", "Bryan", ""]]}, {"id": "1611.08860", "submitter": "Xucong Zhang", "authors": "Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling", "title": "It's Written All Over Your Face: Full-Face Appearance-Based Gaze\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye gaze is an important non-verbal cue for human affect analysis. Recent\ngaze estimation work indicated that information from the full face region can\nbenefit performance. Pushing this idea further, we propose an appearance-based\nmethod that, in contrast to a long-standing line of work in computer vision,\nonly takes the full face image as input. Our method encodes the face image\nusing a convolutional neural network with spatial weights applied on the\nfeature maps to flexibly suppress or enhance information in different facial\nregions. Through extensive evaluation, we show that our full-face method\nsignificantly outperforms the state of the art for both 2D and 3D gaze\nestimation, achieving improvements of up to 14.3% on MPIIGaze and 27.7% on\nEYEDIAP for person-independent 3D gaze estimation. We further show that this\nimprovement is consistent across different illumination conditions and gaze\ndirections and particularly pronounced for the most challenging extreme head\nposes.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 15:00:10 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 13:54:13 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Zhang", "Xucong", ""], ["Sugano", "Yusuke", ""], ["Fritz", "Mario", ""], ["Bulling", "Andreas", ""]]}, {"id": "1611.08981", "submitter": "Iva Bojic", "authors": "Iva Bojic, Giulia Marra and Vera Naydenova", "title": "Online tools for public engagement: case studies from Reykjavik", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ubiquity of Internet technologies and growing demands for\ntransparency and open data policies, the role of social networking and online\ndeliberation tools for public engagement in decision-making has increased\nsubstantially in the last decades. In this paper, we present the analysis of\nhow social media are used by different public bodies to enhance public\nparticipation in deliberative democracy. We collected and reviewed published\ninformation on the subject and carried out a field base assessment, involving\nstructured interviews with different government representatives and urban\npolicymakers. In order to compare collected data, we used a framework for\nsystematic analysis and comparison of e-participation platforms called the\nparticipatory cube. The results we got were the following. Participatory\ndecision-making on matters of public concern justly consumes time and\nresources, therefore online tools should be applied with consideration of scale\nand efficiency, i.e. on burning issues for a majority of citizens or\nsmall-scale local platforms, and in combination with meetings in real time and\nspace. The budget and workforce allocated to managing online engagement tools\nshould be proportionate to other political and administrative efforts to bring\nto execution proposed ideas and act on collected feedback in order to satisfy\nthe needs expressed by the communities and not undermine their beliefs about\ntheir power to influence decisions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 04:35:24 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Bojic", "Iva", ""], ["Marra", "Giulia", ""], ["Naydenova", "Vera", ""]]}, {"id": "1611.09083", "submitter": "Alexey Drutsa", "authors": "Alexey Drutsa (Yandex, Moscow, Russia), Gleb Gusev (Yandex, Moscow,\n  Russia), Pavel Serdyukov (Yandex, Moscow, Russia)", "title": "Prediction of Video Popularity in the Absence of Reliable Data from\n  Video Hosting Services: Utility of Traces Left by Users on the Web", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of user-generated content, we observe the constant rise of\nthe number of companies, such as search engines, content aggregators, etc.,\nthat operate with tremendous amounts of web content not being the services\nhosting it. Thus, aiming to locate the most important content and promote it to\nthe users, they face the need of estimating the current and predicting the\nfuture content popularity.\n  In this paper, we approach the problem of video popularity prediction not\nfrom the side of a video hosting service, as done in all previous studies, but\nfrom the side of an operating company, which provides a popular video search\nservice that aggregates content from different video hosting websites. We\ninvestigate video popularity prediction based on features from three primary\nsources available for a typical operating company: first, the content hosting\nprovider may deliver its data via its API, second, the operating company makes\nuse of its own search and browsing logs, third, the company crawls information\nabout embeds of a video and links to a video page from publicly available\nresources on the Web. We show that video popularity prediction based on the\nembed and link data coupled with the internal search and browsing data\nsignificantly improves video popularity prediction based only on the data\nprovided by the video hosting and can even adequately replace the API data in\nthe cases when it is partly or completely unavailable.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 11:43:31 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Drutsa", "Alexey", "", "Yandex, Moscow, Russia"], ["Gusev", "Gleb", "", "Yandex, Moscow,\n  Russia"], ["Serdyukov", "Pavel", "", "Yandex, Moscow, Russia"]]}, {"id": "1611.09427", "submitter": "Phung Manh Duong", "authors": "Manh Duong Phung, Quang Vinh Tran, Kenji Hara, Hirohito Inagaki,\n  Masanobu Abe", "title": "Easy-setup eye movement recording system for human-computer interaction", "comments": "In IEEE International Conference on Research, Innovation and Vision\n  for the Future (RIVF), 2008", "journal-ref": null, "doi": "10.1109/RIVF.2008.4586369", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking the movement of human eyes is expected to yield natural and\nconvenient applications based on human-computer interaction (HCI). To implement\nan effective eye-tracking system, eye movements must be recorded without\nplacing any restriction on the user's behavior or user discomfort. This paper\ndescribes an eye movement recording system that offers free-head, simple\nconfiguration. It does not require the user to wear anything on her head, and\nshe can move her head freely. Instead of using a computer, the system uses a\nvisual digital signal processor (DSP) camera to detect the position of eye\ncorner, the center of pupil and then calculate the eye movement. Evaluation\ntests show that the sampling rate of the system can be 300 Hz and the accuracy\nis about 1.8 degree/s.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 23:35:01 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Phung", "Manh Duong", ""], ["Tran", "Quang Vinh", ""], ["Hara", "Kenji", ""], ["Inagaki", "Hirohito", ""], ["Abe", "Masanobu", ""]]}, {"id": "1611.09433", "submitter": "Phung Manh Duong", "authors": "P. M. Duong, T. T. Hoang, N. T. T. Van, D. A. Viet, T. Q. Vinh", "title": "A novel platform for internet-based mobile robot systems", "comments": "In 2012 7th IEEE Conference on Industrial Electronics and\n  Applications (ICIEA)", "journal-ref": null, "doi": "10.1109/ICIEA.2012.6361052", "report-no": null, "categories": "cs.RO cs.HC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a software and hardware structure for on-line\nmobile robotic systems. The hardware mainly consists of a Multi-Sensor Smart\nRobot connected to the Internet through 3G mobile network. The system employs a\nclient-server software architecture in which the exchanged data between the\nclient and the server is transmitted through different transport protocols.\nAutonomous mechanisms such as obstacle avoidance and safe-point achievement are\nimplemented to ensure the robot safety. This architecture is put into operation\non the real Internet and the preliminary result is promising. By adopting this\nstructure, it will be very easy to construct an experimental platform for the\nresearch on diverse tele-operation topics such as remote control algorithms,\ninterface designs, network protocols and applications etc.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 23:47:43 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Duong", "P. M.", ""], ["Hoang", "T. T.", ""], ["Van", "N. T. T.", ""], ["Viet", "D. A.", ""], ["Vinh", "T. Q.", ""]]}, {"id": "1611.10095", "submitter": "Pietro Speroni Di Fenizio Ph.D.", "authors": "Pietro Speroni di Fenizio, Cyril Velikanov", "title": "System-Generated Requests for Rewriting Proposals", "comments": "9 pages, 1 figure, presented at e-Part 2011 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online deliberation system using mutual evaluation in order to\ncollaboratively develop solutions. Participants submit their proposals and\nevaluate each other's proposals; some of them may then be invited by the system\nto rewrite 'problematic' proposals. Two cases are discussed: a proposal\nsupported by many, but not by a given person, who is then invited to rewrite it\nfor making yet more acceptable; and a poorly presented but presumably\ninteresting proposal. The first of these cases has been successfully\nimplemented. Proposals are evaluated along two axes-understandability (or\nclarity, or, more generally, quality), and agreement. The latter is used by the\nsystem to cluster proposals according to their ideas, while the former is used\nboth to present the best proposals on top of their clusters, and to find poorly\nwritten proposals candidates for rewriting. These functionalities may be\nconsidered as important components of a large scale online deliberation system.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 11:29:25 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["di Fenizio", "Pietro Speroni", ""], ["Velikanov", "Cyril", ""]]}, {"id": "1611.10120", "submitter": "Nattapong Thammasan", "authors": "Nattapong Thammasan, Ken-ichi Fukui, Masayuki Numao", "title": "Fusion of EEG and Musical Features in Continuous Music-emotion\n  Recognition", "comments": "The short version of this paper is accepted to appear as an abstract\n  in the proceedings of AAAI-17 (student abstract and poster program)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion estimation in music listening is confronting challenges to capture\nthe emotion variation of listeners. Recent years have witnessed attempts to\nexploit multimodality fusing information from musical contents and\nphysiological signals captured from listeners to improve the performance of\nemotion recognition. In this paper, we present a study of fusion of signals of\nelectroencephalogram (EEG), a tool to capture brainwaves at a high-temporal\nresolution, and musical features at decision level in recognizing the\ntime-varying binary classes of arousal and valence. Our empirical results\nshowed that the fusion could outperform the performance of emotion recognition\nusing only EEG modality that was suffered from inter-subject variability, and\nthis suggested the promise of multimodal fusion in improving the accuracy of\nmusic-emotion recognition.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 12:24:57 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Thammasan", "Nattapong", ""], ["Fukui", "Ken-ichi", ""], ["Numao", "Masayuki", ""]]}, {"id": "1611.10162", "submitter": "Hosnieh Sattar", "authors": "Hosnieh Sattar and Andreas Bulling and Mario Fritz", "title": "Predicting the Category and Attributes of Visual Search Targets Using\n  Deep Gaze Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the target of visual search from eye fixation (gaze) data is a\nchallenging problem with many applications in human-computer interaction. In\ncontrast to previous work that has focused on individual instances as a search\ntarget, we propose the first approach to predict categories and attributes of\nsearch targets based on gaze data. However, state of the art models for\ncategorical recognition, in general, require large amounts of training data,\nwhich is prohibitive for gaze data. To address this challenge, we propose a\nnovel Gaze Pooling Layer that integrates gaze information into CNN-based\narchitectures as an attention mechanism - incorporating both spatial and\ntemporal aspects of human gaze behavior. We show that our approach is effective\neven when the gaze pooling layer is added to an already trained CNN, thus\neliminating the need for expensive joint data collection of visual and gaze\ndata. We propose an experimental setup and data set and demonstrate the\neffectiveness of our method for search target prediction based on gaze\nbehavior. We further study how to integrate temporal and spatial gaze\ninformation most effectively, and indicate directions for future research in\nthe gaze-based prediction of mental states.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 07:44:49 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 11:52:29 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 11:05:07 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Sattar", "Hosnieh", ""], ["Bulling", "Andreas", ""], ["Fritz", "Mario", ""]]}]