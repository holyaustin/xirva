[{"id": "2104.00038", "submitter": "Jason Hoffman", "authors": "Jason S. Hoffman, Varun Viswanath, Xinyi Ding, Matthew J. Thompson,\n  Eric C. Larson, Shwetak N. Patel and Edward Wang", "title": "Smartphone Camera Oximetry in an Induced Hypoxemia Study", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Hypoxemia, a medical condition that occurs when the blood is not carrying\nenough oxygen to adequately supply the tissues, is a leading indicator for\ndangerous complications of respiratory diseases like asthma, COPD, and\nCOVID-19. While purpose-built pulse oximeters can provide accurate blood-oxygen\nsaturation (SpO$_2$) readings that allow for diagnosis of hypoxemia, enabling\nthis capability in unmodified smartphone cameras via a software update could\ngive more people access to important information about their health, as well as\nimprove physicians' ability to remotely diagnose and treat respiratory\nconditions. In this work, we take a step towards this goal by performing the\nfirst clinical development validation on a smartphone-based SpO$_2$ sensing\nsystem using a varied fraction of inspired oxygen (FiO$_2$) protocol, creating\na clinically relevant validation dataset for solely smartphone-based methods on\na wide range of SpO$_2$ values (70%-100%) for the first time. This contrasts\nwith previous studies, which evaluated performance on a far smaller range\n(85%-100%). We build a deep learning model using this data to demonstrate\naccurate reporting of SpO$_2$ level with an overall MAE=5.00% SpO$_2$ and\nidentifying positive cases of low SpO$_2$<90% with 81% sensitivity and 79%\nspecificity. We ground our analysis with a summary of recent literature in\nsmartphone-based SpO2 monitoring, and we provide the data from the FiO$_2$\nstudy in open-source format, so that others may build on this work.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 18:10:10 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Hoffman", "Jason S.", ""], ["Viswanath", "Varun", ""], ["Ding", "Xinyi", ""], ["Thompson", "Matthew J.", ""], ["Larson", "Eric C.", ""], ["Patel", "Shwetak N.", ""], ["Wang", "Edward", ""]]}, {"id": "2104.00086", "submitter": "Carine Rognon", "authors": "Carine Rognon, Taylor Bunge, Meiyuzi Gao, Chip Connor, Benjamin\n  Stephens-Fripp, Casey Brown, Ali Israr", "title": "An Online Survey on the Perception of Mediated Social Touch Interaction\n  and Device Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social touch is essential for our social interactions, communication, and\nwell-being. It has been shown to reduce anxiety and loneliness; and is a key\nchannel to transmit emotions for which words are not sufficient, such as love,\nsympathy, reassurance, etc. However, direct physical contact is not always\npossible due to being remotely located, interacting in a virtual environment,\nor as a result of a health issue. Mediated social touch enables physical\ninteractions, despite the distance, by transmitting the haptic cues that\nconstitute social touch through devices. As this technology is fairly new, the\nusers' needs and their expectations on a device design and its features are\nunclear, as well as who would use this technology, and in which conditions. To\nbetter understand these aspects of the mediated interaction, we conducted an\nonline survey on 258 respondents located in the USA. Results give insights on\nthe type of interactions and device features that the US population would like\nto use.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:01:32 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Rognon", "Carine", ""], ["Bunge", "Taylor", ""], ["Gao", "Meiyuzi", ""], ["Connor", "Chip", ""], ["Stephens-Fripp", "Benjamin", ""], ["Brown", "Casey", ""], ["Israr", "Ali", ""]]}, {"id": "2104.00148", "submitter": "Gonzalo M\\'endez Dr", "authors": "Gonzalo Gabriel M\\'endez, Luis Gal\\'arraga and Katherine Chiluiza", "title": "Showing Academic Performance Predictions during Term Planning: Effects\n  on Students' Decisions, Behaviors, and Preferences", "comments": "17 pages", "journal-ref": null, "doi": "10.1145/3411764", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Course selection is a crucial activity for students as it directly impacts\ntheir workload and performance. It is also time-consuming, prone to\nsubjectivity, and often carried out based on incomplete information. This task\ncan, nevertheless, be assisted with computational tools, for instance, by\npredicting performance based on historical data. We investigate the effects of\nshowing grade predictions to students through an interactive visualization\ntool. A qualitative study suggests that in the presence of predictions,\nstudents may focus too much on maximizing their performance, to the detriment\nof other factors such as the workload. A follow-up quantitative study explored\nwhether these effects are mitigated by changing how predictions are conveyed.\nOur observations suggest the presence of a framing effect that induces students\nto put more effort into course selection when faced with more specific\npredictions. We discuss these and other findings and outline considerations for\ndesigning better data-driven course selection tools.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 22:32:21 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["M\u00e9ndez", "Gonzalo Gabriel", ""], ["Gal\u00e1rraga", "Luis", ""], ["Chiluiza", "Katherine", ""]]}, {"id": "2104.00316", "submitter": "Andrea Bottino", "authors": "Edoardo Battegazzorre, Andrea Bottino, Fabrizio Lamberti", "title": "Training Medical Communication Skills with Virtual Patients: Literature\n  Review and Directions for Future Research", "comments": null, "journal-ref": null, "doi": null, "report-no": "Accepted to INTETAIN 2020", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effective communication is a crucial skill for healthcare providers since it\nleads to better patient health, satisfaction and avoids malpractice claims. In\nstandard medical education, students' communication skills are trained with\nrole-playing and Standardized Patients (SPs), i.e., actors. However, SPs are\ndifficult to standardize, and are very resource consuming. Virtual Patients\n(VPs) are interactive computer-based systems that represent a valuable\nalternative to SPs. VPs are capable of portraying patients in realistic\nclinical scenarios and engage learners in realistic conversations. Approaching\nmedical communication skill training with VPs has been an active research area\nin the last ten years. As a result, the number of works in this field has grown\nsignificantly. The objective of this work is to survey the recent literature,\nassessing the state of the art of this technology with a specific focus on the\ninstructional and technical design of VP simulations. After having classified\nand analysed the VPs selected for our research, we identified several areas\nthat require further investigation, and we drafted practical recommendations\nfor VP developers on design aspects that, based on our findings, are pivotal to\ncreate novel and effective VP simulations or improve existing ones.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:49:24 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Battegazzorre", "Edoardo", ""], ["Bottino", "Andrea", ""], ["Lamberti", "Fabrizio", ""]]}, {"id": "2104.00358", "submitter": "Daniel Buschek", "authors": "Daniel Buschek, Lukas Mecke, Florian Lehmann, Hai Dang", "title": "Nine Potential Pitfalls when Designing Human-AI Co-Creative Systems", "comments": "8 pages, 2 figures, 1 table, HAI-GEN Workshop at IUI'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This position paper examines potential pitfalls on the way towards achieving\nhuman-AI co-creation with generative models in a way that is beneficial to the\nusers' interests. In particular, we collected a set of nine potential pitfalls,\nbased on the literature and our own experiences as researchers working at the\nintersection of HCI and AI. We illustrate each pitfall with examples and\nsuggest ideas for addressing it. Reflecting on all pitfalls, we discuss and\nconclude with implications for future research directions. With this\ncollection, we hope to contribute to a critical and constructive discussion on\nthe roles of humans and AI in co-creative interactions, with an eye on related\nassumptions and potential side-effects for creative practices and beyond.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 09:27:30 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Buschek", "Daniel", ""], ["Mecke", "Lukas", ""], ["Lehmann", "Florian", ""], ["Dang", "Hai", ""]]}, {"id": "2104.00541", "submitter": "Kamil \\.Zbikowski", "authors": "Kamil \\.Zbikowski, Micha{\\l} Ostapowicz, Piotr Gawrysiak", "title": "Deep Reinforcement Learning for Resource Allocation in Business\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assigning resources in business processes execution is a repetitive task that\ncan be effectively automated. However, different automation methods may give\nvarying results that may not be optimal. Proper resource allocation is crucial\nas it may lead to significant cost reductions or increased effectiveness that\nresults in increased revenues.\n  In this work, we first propose a novel representation that allows modeling of\na multi-process environment with different process-based rewards. These\nprocesses can share resources that differ in their eligibility. Then, we use\ndouble deep reinforcement learning to look for optimal resource allocation\npolicy. We compare those results with two popular strategies that are widely\nused in the industry. Learning optimal policy through reinforcement learning\nrequires frequent interactions with the environment, so we also designed and\ndeveloped a simulation engine that can mimic real-world processes.\n  The results obtained are promising. Deep reinforcement learning based\nresource allocation achieved significantly better results compared to two\ncommonly used techniques.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:20:25 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["\u017bbikowski", "Kamil", ""], ["Ostapowicz", "Micha\u0142", ""], ["Gawrysiak", "Piotr", ""]]}, {"id": "2104.00805", "submitter": "Zoya Bylinskii", "authors": "Zoya Bylinskii, Lore Goetschalckx, Anelise Newman, Aude Oliva", "title": "Memorability: An image-computable measure of information utility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pixels in an image, and the objects, scenes, and actions that they\ncompose, determine whether an image will be memorable or forgettable. While\nmemorability varies by image, it is largely independent of an individual\nobserver. Observer independence is what makes memorability an image-computable\nmeasure of information, and eligible for automatic prediction. In this chapter,\nwe zoom into memorability with a computational lens, detailing the\nstate-of-the-art algorithms that accurately predict image memorability relative\nto human behavioral data, using image features at different scales from raw\npixels to semantic labels. We discuss the design of algorithms and\nvisualizations for face, object, and scene memorability, as well as algorithms\nthat generalize beyond static scenes to actions and videos. We cover the\nstate-of-the-art deep learning approaches that are the current front runners in\nthe memorability prediction space. Beyond prediction, we show how recent A.I.\napproaches can be used to create and modify visual memorability. Finally, we\npreview the computational applications that memorability can power, from\nfiltering visual streams to enhancing augmented reality interfaces.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 23:38:30 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Bylinskii", "Zoya", ""], ["Goetschalckx", "Lore", ""], ["Newman", "Anelise", ""], ["Oliva", "Aude", ""]]}, {"id": "2104.00828", "submitter": "Yifan Sun", "authors": "Yifan Sun, Yixuan Zhang, Ali Mosallaei, Michael D. Shah, Cody Dunne,\n  David Kaeli", "title": "Daisen: A Framework for Visualizing Detailed GPU Execution", "comments": "EuroVis Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graphics Processing Units (GPUs) have been widely used to accelerate\nartificial intelligence, physics simulation, medical imaging, and information\nvisualization applications. To improve GPU performance, GPU hardware designers\nneed to identify performance issues by inspecting a huge amount of\nsimulator-generated traces. Visualizing the execution traces can reduce the\ncognitive burden of users and facilitate making sense of behaviors of GPU\nhardware components. In this paper, we first formalize the process of GPU\nperformance analysis and characterize the design requirements of visualizing\nexecution traces based on a survey study and interviews with GPU hardware\ndesigners. We contribute data and task abstraction for GPU performance\nanalysis. Based on our task analysis, we propose Daisen, a framework that\nsupports data collection from GPU simulators and provides visualization of the\nsimulator-generated GPU execution traces. Daisen features a data abstraction\nand trace format that can record simulator-generated GPU execution traces.\nDaisen also includes a web-based visualization tool that helps GPU hardware\ndesigners examine GPU execution traces, identify performance bottlenecks, and\nverify performance improvement. Our qualitative evaluation with GPU hardware\ndesigners demonstrates that the design of Daisen reflects the typical workflow\nof GPU hardware designers. Using Daisen, participants were able to effectively\nidentify potential performance bottlenecks and opportunities for performance\nimprovement. The open-sourced implementation of Daisen can be found at\ngitlab.com/akita/vis. Supplemental materials including a demo video, survey\nquestions, evaluation study guide, and post-study evaluation survey are\navailable at osf.io/j5ghq.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 00:52:37 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sun", "Yifan", ""], ["Zhang", "Yixuan", ""], ["Mosallaei", "Ali", ""], ["Shah", "Michael D.", ""], ["Dunne", "Cody", ""], ["Kaeli", "David", ""]]}, {"id": "2104.00870", "submitter": "Anam Ahmad Khan", "authors": "Anam Ahmad Khan and Joshua Newn and Ryan Kelly and Namrata Srivastava\n  and James Bailey and Eduardo Velloso", "title": "GAVIN: Gaze-Assisted Voice-Based Implicit Note-taking", "comments": "In press, ACM Transactions on Computer-Human Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotation is an effective reading strategy people often undertake while\ninteracting with digital text. It involves highlighting pieces of text and\nmaking notes about them. Annotating while reading in a desktop environment is\nconsidered trivial but, in a mobile setting where people read while\nhand-holding devices, the task of highlighting and typing notes on a mobile\ndisplay is challenging. In this paper, we introduce GAVIN, a gaze-assisted\nvoice note-taking application, which enables readers to seamlessly take voice\nnotes on digital documents by implicitly anchoring them to text passages. We\nfirst conducted a contextual enquiry focusing on participants' note-taking\npractices on digital documents. Using these findings, we propose a method which\nleverages eye-tracking and machine learning techniques to annotate voice notes\nwith reference text passages. To evaluate our approach, we recruited 32\nparticipants performing voice note-taking. Following, we trained a classifier\non the data collected to predict text passage where participants made voice\nnotes. Lastly, we employed the classifier to built GAVIN and conducted a user\nstudy to demonstrate the feasibility of the system. This research demonstrates\nthe feasibility of using gaze as a resource for implicit anchoring of voice\nnotes, enabling the design of systems that allow users to record voice notes\nwith minimal effort and high accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 03:25:45 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Khan", "Anam Ahmad", ""], ["Newn", "Joshua", ""], ["Kelly", "Ryan", ""], ["Srivastava", "Namrata", ""], ["Bailey", "James", ""], ["Velloso", "Eduardo", ""]]}, {"id": "2104.00926", "submitter": "Theo Jaunet", "authors": "Theo Jaunet, Corentin Kervadec, Romain Vuillemot, Grigory Antipov,\n  Moez Baccouche and Christian Wolf", "title": "VisQA: X-raying Vision and Language Reasoning in Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering systems target answering open-ended textual\nquestions given input images. They are a testbed for learning high-level\nreasoning with a primary use in HCI, for instance assistance for the visually\nimpaired. Recent research has shown that state-of-the-art models tend to\nproduce answers exploiting biases and shortcuts in the training data, and\nsometimes do not even look at the input image, instead of performing the\nrequired reasoning steps. We present VisQA, a visual analytics tool that\nexplores this question of reasoning vs. bias exploitation. It exposes the key\nelement of state-of-the-art neural models -- attention maps in transformers.\nOur working hypothesis is that reasoning steps leading to model predictions are\nobservable from attention distributions, which are particularly useful for\nvisualization. The design process of VisQA was motivated by well-known bias\nexamples from the fields of deep learning and vision-language reasoning and\nevaluated in two ways. First, as a result of a collaboration of three fields,\nmachine learning, vision and language reasoning, and data analytics, the work\nlead to a better understanding of bias exploitation of neural models for VQA,\nwhich eventually resulted in an impact on its design and training through the\nproposition of a method for the transfer of reasoning patterns from an oracle\nmodel. Second, we also report on the design of VisQA, and a goal-oriented\nevaluation of VisQA targeting the analysis of a model decision process from\nmultiple experts, providing evidence that it makes the inner workings of models\naccessible to users.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:08:25 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 09:57:29 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Jaunet", "Theo", ""], ["Kervadec", "Corentin", ""], ["Vuillemot", "Romain", ""], ["Antipov", "Grigory", ""], ["Baccouche", "Moez", ""], ["Wolf", "Christian", ""]]}, {"id": "2104.01022", "submitter": "Benjamin Kahl", "authors": "Benjamin Kahl", "title": "Human Biases Preventing The Widespread Adoption Of Self-Driving Cars", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-driving cars offer a plethora of safety advantages over our accustomed\nhuman-driven ones, yet many individuals feel uneasy sharing the road with these\nmachines and entrusting their lives to their driving capabilities. Thus,\nbringing about a widespread adoption of autonomous cars requires overcoming\nthese compulsions through careful planning and forethought. Here we break down\nthe three primary psychological barriers that may hamstring or even wholly\nprevent their widespread adoption as well as how to tackle them.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 12:45:03 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Kahl", "Benjamin", ""]]}, {"id": "2104.01088", "submitter": "Cagatay Basdogan", "authors": "Atakan Arasan, Cagatay Basdogan, T. Metin Sezgin", "title": "HaptiStylus: A Novel Stylus Capable of Displaying Movement and\n  Rotational Torque Effects", "comments": null, "journal-ref": "IEEE Computer Graphics and Applications, 2016, Vol. 36, No. 1, pp.\n  30-41", "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of pen-enabled tablets and mobile devices, stylus-based\ninteraction has been receiving increasing attention. Unfortunately, styluses\navailable in the market today are all passive instruments that are primarily\nused for writing and pointing. In this paper, we describe a novel stylus\ncapable of displaying certain vibro-tactile and inertial haptic effects to the\nuser. Our stylus is equipped with two vibration actuators at the ends, which\nare used to create a tactile sensation of up and down movement along the\nstylus. The stylus is also embedded with a DC motor, which is used to create a\nsense of bidirectional rotational torque about the long axis of the pen.\nThrough two psychophysical experiments, we show that, when driven with\ncarefully selected timing and actuation patterns, our haptic stylus can convey\nmovement and rotational torque information to the user. Results from a further\npsychophysical experiment provide insight on how the shape of the actuation\npatterns effects the perception of rotational torque effect. Finally,\nexperimental results from our interactive pen-based game show that our haptic\nstylus is effective in practical settings\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:17:18 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Arasan", "Atakan", ""], ["Basdogan", "Cagatay", ""], ["Sezgin", "T. Metin", ""]]}, {"id": "2104.01090", "submitter": "Cagatay Basdogan", "authors": "Omer Sirin, Allan Barrea, Philippe Lef\\`evre, Jean-Louis Thonnard,\n  Cagatay Basdogan", "title": "Fingerpad Contact Evolution Under Electrovibration", "comments": null, "journal-ref": "J.R. Soc. Interface, 2019, Vol.16, No. 156, 20190166", "doi": "10.1098/rsif.2019.0166", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Displaying tactile feedback through a touchscreen via electrovibration has\nmany potential applications in mobile devices, consumer electronics, home\nappliances, and automotive industry though our knowledge and understanding on\nthe underlying contact mechanics is very limited. An experimental study was\nconducted to investigate the contact evolution between the human finger and a\ntouchscreen under electrovibration using a robotic set-up and an imaging\nsystem. The results show that the effect of electrovibration is only present\nduring full slip but not before slip. Hence, coefficient of friction increases\nunder electrovibration as expected during full slip, but the apparent contact\narea is significantly smaller during full slip when compared to that of no\nelectrovibration condition. It is suggested that the main cause of the increase\nin friction during full slip is due to an increase in real contact area and the\nreduction in apparent area is due to stiffening of the finger skin in\ntangential direction.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:25:52 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sirin", "Omer", ""], ["Barrea", "Allan", ""], ["Lef\u00e8vre", "Philippe", ""], ["Thonnard", "Jean-Louis", ""], ["Basdogan", "Cagatay", ""]]}, {"id": "2104.01106", "submitter": "Vlad Atanasiu", "authors": "Vlad Atanasiu, Isabelle Marthot-Santaniello", "title": "Legibility Enhancement of Papyri Using Color Processing and Visual\n  Illusions: A Case Study in Critical Vision", "comments": "Article accepted with minor revisions by the International Journal on\n  Document Analysis and Recognition (IJDAR) on 2021.03.11. Open Source software\n  accessible at https://hierax.ch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Purpose: This article develops theoretical, algorithmic, perceptual, and\ninteraction aspects of script legibility enhancement in the visible light\nspectrum for the purpose of scholarly editing of papyri texts. - Methods: Novel\nlegibility enhancement algorithms based on color processing and visual\nillusions are proposed and compared to classic methods. A user experience\nexperiment was carried out to evaluate the solutions and better understand the\nproblem on an empirical basis. - Results: (1) The proposed methods outperformed\nthe comparison methods. (2) The methods that most successfully enhanced script\nlegibility were those that leverage human perception. (3) Users exhibited a\nbroad behavioral spectrum of text-deciphering strategies, under the influence\nof factors such as personality and social conditioning, tasks and application\ndomains, expertise level and image quality, and affordances of software,\nhardware, and interfaces. No single method satisfied all factor configurations.\nTherefore, using synergetically a range of enhancement methods and interaction\nmodalities is suggested for optimal results and user satisfaction. (4) A\nparadigm of legibility enhancement for critical applications is outlined,\ncomprising the following criteria: interpreting images skeptically; approaching\nenhancement as a system problem; considering all image structures as potential\ninformation; deriving interpretations from connections across distinct spatial\nlocations; and making uncertainty and alternative interpretations explicit,\nboth visually and numerically.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 23:48:17 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Atanasiu", "Vlad", ""], ["Marthot-Santaniello", "Isabelle", ""]]}, {"id": "2104.01129", "submitter": "Yu Zhang", "authors": "Yu Zhang and Martijn Tennekes and Tim de Jong and Lyana Curier and Bob\n  Coecke and Min Chen", "title": "Using Simulation to Aid the Design and Optimization of Intelligent User\n  Interfaces for Quality Assurance Processes in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many mission-critical applications of machine learning (ML) in the real-world\nrequire a quality assurance (QA) process before the decisions or predictions of\nan ML model can be deployed. Because QA4ML users have to view a non-trivial\namount of data and perform many input actions to correct errors made by the ML\nmodel, an optimally-designed user interface (UI) can reduce the cost of\ninteractions significantly. A UI's effectiveness can be affected by many\nfactors, such as the number of data objects processed concurrently, the types\nof commands for correcting errors, and the availability of algorithms for\nassisting users. We propose using simulation to aid the design and optimization\nof intelligent user interfaces for QA4ML processes. In particular, we focus on\nsimulating the combined effects of human intelligence in selecting appropriate\ncommands and algorithms, and machine intelligence in providing a collection of\ngeneral-purpose algorithms for reordering data objects to be quality-assured.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:10:24 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zhang", "Yu", ""], ["Tennekes", "Martijn", ""], ["de Jong", "Tim", ""], ["Curier", "Lyana", ""], ["Coecke", "Bob", ""], ["Chen", "Min", ""]]}, {"id": "2104.01160", "submitter": "Wenjie Luo Mr", "authors": "Wenjie Luo, Zhenyu Yan, Qun Song, Rui Tan", "title": "PhyAug: Physics-Directed Data Augmentation for Deep Sensing Model\n  Transfer in Cyber-Physical Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3412382.3458255", "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Run-time domain shifts from training-phase domains are common in sensing\nsystems designed with deep learning. The shifts can be caused by sensor\ncharacteristic variations and/or discrepancies between the design-phase model\nand the actual model of the sensed physical process. To address these issues,\nexisting transfer learning techniques require substantial target-domain data\nand thus incur high post-deployment overhead. This paper proposes to exploit\nthe first principle governing the domain shift to reduce the demand on\ntarget-domain data. Specifically, our proposed approach called PhyAug uses the\nfirst principle fitted with few labeled or unlabeled source/target-domain data\npairs to transform the existing source-domain training data into augmented data\nfor updating the deep neural networks. In two case studies of keyword spotting\nand DeepSpeech2-based automatic speech recognition, with 5-second unlabeled\ndata collected from the target microphones, PhyAug recovers the recognition\naccuracy losses due to microphone characteristic variations by 37% to 72%. In a\ncase study of seismic source localization with TDoA fngerprints, by exploiting\nthe frst principle of signal propagation in uneven media, PhyAug only requires\n3% to 8% of labeled TDoA measurements required by the vanilla fingerprinting\napproach in achieving the same localization accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 13:49:45 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 12:42:02 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Luo", "Wenjie", ""], ["Yan", "Zhenyu", ""], ["Song", "Qun", ""], ["Tan", "Rui", ""]]}, {"id": "2104.01266", "submitter": "Kenneth Holstein", "authors": "Kenneth Holstein and Vincent Aleven", "title": "Designing for human-AI complementarity in K-12 education", "comments": "To appear in AI Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has explored how complementary strengths of humans and artificial\nintelligence (AI) systems might be productively combined. However, successful\nforms of human-AI partnership have rarely been demonstrated in real-world\nsettings. We present the iterative design and evaluation of Lumilo, smart\nglasses that help teachers help their students in AI-supported classrooms by\npresenting real-time analytics about students' learning, metacognition, and\nbehavior. Results from a field study conducted in K-12 classrooms indicate that\nstudents learn more when teachers and AI tutors work together during class. We\ndiscuss implications of this research for the design of human-AI partnerships.\nWe argue for more participatory approaches to research and design in this area,\nin which practitioners and other stakeholders are deeply, meaningfully involved\nthroughout the process. Furthermore, we advocate for theory-building and for\nprincipled approaches to the study of human-AI decision-making in real-world\ncontexts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 22:38:50 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 22:54:50 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Holstein", "Kenneth", ""], ["Aleven", "Vincent", ""]]}, {"id": "2104.01420", "submitter": "Lei Shi", "authors": "Lei Shi", "title": "A Design Process of Visual Analytics Applications using Conceptual Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art visual analytics techniques in application domains are often\ndesigned by VA professionals over qualitative requirement collected from end\nusers. These VA techniques may not leverage users' domain knowledge about how\nto achieve their analytical goals. In this position paper, we propose a\nuser-driven design process of VA applications centered around a new concept\ncalled analytical representation (AR). AR features a formal abstraction of user\nrequirement and their desired analytical trails for certain VA application, and\nis independent of the actual visualization design. A conceptual graph schema is\nintroduced to define the AR abstraction, which can be created manually or\nconstructed by semi-automated tools. Designing VA applications with AR provides\na shared opportunity for both optimal analysis blueprint from the perspective\nof end users and optimal visualization/algorithm from the perspective of VA\ndesigners. We demonstrate the usage of the design process in two case studies.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 14:39:20 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shi", "Lei", ""]]}, {"id": "2104.01543", "submitter": "Esha Singh", "authors": "Esha Singh, Anu Bompelli, Ruyuan Wan, Jiang Bian, Serguei Pakhomov,\n  and Rui Zhang", "title": "A Conversational Agent System for Dietary Supplements Use", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Dietary supplements (DS) have been widely used by consumers, but the\ninformation around the efficacy and safety of DS is disparate or incomplete,\nthus creating barriers for consumers to find information effectively.\nConversational agent (CA) systems have been applied to the healthcare domain,\nbut there is no such a system to answer consumers regarding DS use, although\nwidespread use of DS. In this study, we develop the first CA system for DS use\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 05:47:04 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 22:16:49 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Singh", "Esha", ""], ["Bompelli", "Anu", ""], ["Wan", "Ruyuan", ""], ["Bian", "Jiang", ""], ["Pakhomov", "Serguei", ""], ["Zhang", "Rui", ""]]}, {"id": "2104.01558", "submitter": "Mingjiang Liu", "authors": "Mingjiang Liu, Chengli Xiao, Chunlin Chen", "title": "Perspective-corrected Spatial Referring Expression Generation for\n  Human-Robot Interaction", "comments": "Under review, 20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent robots designed to interact with humans in real scenarios need to\nbe able to refer to entities actively by natural language. In spatial referring\nexpression generation, the ambiguity is unavoidable due to the diversity of\nreference frames, which will lead to an understanding gap between humans and\nrobots. To narrow this gap, in this paper, we propose a novel\nperspective-corrected spatial referring expression generation (PcSREG) approach\nfor human-robot interaction by considering the selection of reference frames.\nThe task of referring expression generation is simplified into the process of\ngenerating diverse spatial relation units. First, we pick out all landmarks in\nthese spatial relation units according to the entropy of preference and allow\nits updating through a stack model. Then all possible referring expressions are\ngenerated according to different reference frame strategies. Finally, we\nevaluate every expression using a probabilistic referring expression resolution\nmodel and find the best expression that satisfies both of the appropriateness\nand effectiveness. We implement the proposed approach on a robot system and\nempirical experiments show that our approach can generate more effective\nspatial referring expressions for practical applications.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 08:00:02 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 09:02:27 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Liu", "Mingjiang", ""], ["Xiao", "Chengli", ""], ["Chen", "Chunlin", ""]]}, {"id": "2104.01976", "submitter": "Orhan Can G\\\"or\\\"ur", "authors": "O. Can G\\\"or\\\"ur, Benjamin Rosman, Fikret Sivrikaya, Sahin Albayrak", "title": "FABRIC: A Framework for the Design and Evaluation of Collaborative\n  Robots with Extended Human Adaptation", "comments": "The article is in review for publication in International Journal of\n  Robotics Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A limitation for collaborative robots (cobots) is their lack of ability to\nadapt to human partners, who typically exhibit an immense diversity of\nbehaviors. We present an autonomous framework as a cobot's real-time\ndecision-making mechanism to anticipate a variety of human characteristics and\nbehaviors, including human errors, toward a personalized collaboration. Our\nframework handles such behaviors in two levels: 1) short-term human behaviors\nare adapted through our novel Anticipatory Partially Observable Markov Decision\nProcess (A-POMDP) models, covering a human's changing intent (motivation),\navailability, and capability; 2) long-term changing human characteristics are\nadapted by our novel Adaptive Bayesian Policy Selection (ABPS) mechanism that\nselects a short-term decision model, e.g., an A-POMDP, according to an estimate\nof a human's workplace characteristics, such as her expertise and collaboration\npreferences. To design and evaluate our framework over a diversity of human\nbehaviors, we propose a pipeline where we first train and rigorously test the\nframework in simulation over novel human models. Then, we deploy and evaluate\nit on our novel physical experiment setup that induces cognitive load on humans\nto observe their dynamic behaviors, including their mistakes, and their\nchanging characteristics such as their expertise. We conduct user studies and\nshow that our framework effectively collaborates non-stop for hours and adapts\nto various changing human behaviors and characteristics in real-time. That\nincreases the efficiency and naturalness of the collaboration with a higher\nperceived collaboration, positive teammate traits, and human trust. We believe\nthat such an extended human adaptation is key to the long-term use of cobots.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:52:38 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["G\u00f6r\u00fcr", "O. Can", ""], ["Rosman", "Benjamin", ""], ["Sivrikaya", "Fikret", ""], ["Albayrak", "Sahin", ""]]}, {"id": "2104.01991", "submitter": "Antonio Bucchiarone Dr.", "authors": "Dongliang Chen, Antonio Bucchiarone, Zhihan Lv", "title": "MeetDurian: A Gameful Mobile App to Prevent COVID-19 Infection", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 problem has not gone away with the passing of the seasons. Even\nthough most countries have achieved remarkable results in fighting against\nepidemic diseases and preventing and controlling viruses, the general public is\nstill far from understanding the new crown virus and lacks imagination on its\ntransmission law. In this paper, we propose MeetDurian: a cross-platform mobile\napplication that exploits a location-based game to improve users' hygiene\nhabits and reduce virus dispersal. We present its main features, its\narchitecture, and its core technologies. Finally, we report a set of\nexperiments that prove the acceptability and usability of MeetDurian. An\nillustrative demo of the mobile app features is shown in the following video:\nhttps://youtu.be/Vqg7nFDQuOU.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 10:34:07 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chen", "Dongliang", ""], ["Bucchiarone", "Antonio", ""], ["Lv", "Zhihan", ""]]}, {"id": "2104.02015", "submitter": "Jessica Hullman", "authors": "Jessica Hullman and Andrew Gelman", "title": "To design interfaces for exploratory data analysis, we need theories of\n  graphical inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research and development in computer science and statistics have produced\nincreasingly sophisticated software interfaces for interactive and exploratory\nanalysis, optimized for easy pattern finding and data exposure. But design\nphilosophies that emphasize exploration over other phases of analysis risk\nconfusing a need for flexibility with a conclusion that exploratory visual\nanalysis is inherently model-free and cannot be formalized. We describe how\nwithout a grounding in theories of human statistical inference, research in\nexploratory visual analysis can lead to contradictory interface objectives and\nrepresentations of uncertainty that can discourage users from drawing valid\ninferences. We discuss how the concept of a model check in a Bayesian\nstatistical framework unites exploratory and confirmatory analysis, and how\nthis understanding relates to other proposed theories of graphical inference.\nViewing interactive analysis as driven by model checks suggests new directions\nfor software and empirical research around exploratory and visual analysis. For\nexample, systems should enable specifying and explicitly comparing data to null\nand other reference distributions and better representations of uncertainty.\nImplications of Bayesian and other theories of graphical inference should be\ntested against outcomes of interactive analysis by people to drive theory\ndevelopment.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:08:43 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 23:38:43 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Hullman", "Jessica", ""], ["Gelman", "Andrew", ""]]}, {"id": "2104.02046", "submitter": "Wieslaw Kopec", "authors": "Wies{\\l}aw Kope\\'c, Jaros{\\l}aw Kowalski, Julia Paluch, Anna\n  Jaskulska, Kinga Skorupska, Marcin Niewi\\'nski, Maciej Krzywicki, Cezary\n  Biele", "title": "Older Adults and Brain-Computer Interface: An Exploratory Study", "comments": null, "journal-ref": null, "doi": "10.1145/3411763.3451663", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this exploratory study, we examine the possibilities of non-invasive\nBrain-Computer Interface (BCI) in the context of Smart Home Technology (SHT)\ntargeted at older adults. During two workshops, one stationary, and one online\nvia Zoom, we researched the insights of the end users concerning the potential\nof the BCI in the SHT setting. We explored its advantages and drawbacks, and\nthe features older adults see as vital as well as the ones that they would\nbenefit from. Apart from evaluating the participants' perception of such\ndevices during the two workshops we also analyzed some key considerations\nresulting from the insights gathered during the workshops, such as potential\nbarriers, ways to mitigate them, strengths and opportunities connected to BCI.\nThese may be useful for designing BCI interaction paradigms and pinpointing\nareas of interest to pursue in further studies.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:49:46 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kope\u0107", "Wies\u0142aw", ""], ["Kowalski", "Jaros\u0142aw", ""], ["Paluch", "Julia", ""], ["Jaskulska", "Anna", ""], ["Skorupska", "Kinga", ""], ["Niewi\u0144ski", "Marcin", ""], ["Krzywicki", "Maciej", ""], ["Biele", "Cezary", ""]]}, {"id": "2104.02100", "submitter": "Wieslaw Kopec", "authors": "Wies{\\l}aw Kope\\'c, Krzysztof Kalinowski, Monika Kornacka, Kinga\n  Skorupska, Julia Paluch, Anna Jaskulska, Grzegorz Pochwatko, Jakub Mo\\.zaryn,\n  Pawe{\\l} Kobyli\\'nski, Piotr Gago", "title": "VR Hackathon with Goethe Institute: Lessons Learned from Organizing a\n  Transdisciplinary VR Hackathon", "comments": null, "journal-ref": null, "doi": "10.1145/3411763.3443432", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we report a case study of a Language Learning Bauhaus VR\nhackathon with Goethe Institute. It was organized as an educational and\nresearch project to tap into the dynamics of transdisciplinary teams challenged\nwith a specific requirement. In our case, it was to build a Bauhaus-themed\nGerman Language Learning VR App. We constructed this experiment to simulate how\nrepresentatives of different disciplines may work together towards a very\nspecific purpose under time pressure. So, each participating team consisted of\nmembers of various expert-fields: software development (Unity or Unreal),\ndesign, psychology and linguistics. The results of this study cast light on the\nrecommended cycle of design thinking and customer-centered design in VR.\nEspecially in interdisciplinary rapid prototyping conditions, where\nstakeholders initially do not share competences. They also showcase educational\nbenefits of working in transdisciplinary environments. This study, combined\nwith our previous work on human factors in rapid software development and\nco-design, including hackathon dynamics, allowed us to formulate\nrecommendations for organizing content creation VR hackathons for specific\npurposes. We also provide guidelines on how to prepare the participants to work\nin rapid prototyping VR environments and benefit from such experiences in the\nlong term.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:09:24 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kope\u0107", "Wies\u0142aw", ""], ["Kalinowski", "Krzysztof", ""], ["Kornacka", "Monika", ""], ["Skorupska", "Kinga", ""], ["Paluch", "Julia", ""], ["Jaskulska", "Anna", ""], ["Pochwatko", "Grzegorz", ""], ["Mo\u017caryn", "Jakub", ""], ["Kobyli\u0144ski", "Pawe\u0142", ""], ["Gago", "Piotr", ""]]}, {"id": "2104.02330", "submitter": "Yuecong Min", "authors": "Yuecong Min, Aiming Hao, Xiujuan Chai, Xilin Chen", "title": "Visual Alignment Constraint for Continuous Sign Language Recognition", "comments": "The code will be released: https://github.com/Blueprintf/VAC_CSLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based Continuous Sign Language Recognition (CSLR) aims to recognize\nunsegmented gestures from image sequences. To better train CSLR models, the\niterative training scheme is widely adopted to alleviate the overfitting of the\nalignment model. Although the iterative training scheme can improve\nperformance, it will also increase the training time. In this work, we revisit\nthe overfitting problem in recent CTC-based CSLR works and attribute it to the\ninsufficient training of the feature extractor. To solve this problem, we\npropose a Visual Alignment Constraint (VAC) to enhance the feature extractor\nwith more alignment supervision. Specifically, the proposed VAC is composed of\ntwo auxiliary losses: one makes predictions based on visual features only, and\nthe other aligns short-term visual and long-term contextual features. Moreover,\nwe further propose two metrics to evaluate the contributions of the feature\nextractor and the alignment model, which provide evidence for the overfitting\nproblem. The proposed VAC achieves competitive performance on two challenging\nCSLR datasets and experimental results show its effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 07:24:58 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Min", "Yuecong", ""], ["Hao", "Aiming", ""], ["Chai", "Xiujuan", ""], ["Chen", "Xilin", ""]]}, {"id": "2104.02362", "submitter": "Eleonora Mencarini", "authors": "Eleonora Mencarini, Amon Rapp, Massimo Zancanaro", "title": "Underground Astronauts: Understanding the Sporting Science of Speleology\n  and its Implications for HCI", "comments": "27 pages, 3 images, 3 tables", "journal-ref": "International Journal of Human-Computer Studies 151 (2021) 1-12", "doi": "10.1016/j.ijhcs.2021.102621", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we present a qualitative study on speleology that aims to\nwiden the current understanding of people's practices in Nature and identify a\ndesign space for technology that supports such practices. Speleology is a\npractice based on the discovery, study, and dissemination of natural cavities.\nSpeleologists are amateur experts who often collaborate with scientists and\nlocal institutions to understand the geology, hydrology, and biology of a\nterritory. Their skills are at the same time physical, technical, and\ntheoretical; this is why speleology is defined as a 'sporting science'. Being\nat the boundary between outdoor adventure sports and citizen science,\nspeleology is an interesting case study for investigating the variety and\ncomplexity of activities carried out in the natural context. We interviewed 15\nexperienced speleologists to explore their goals, routines, vision of the\noutdoors, and attitude towards technology. From our study, it emerged that i)\nthe excitement of discovery and the unpredictability of an explorative trip are\nthe strongest motivations for people to engage in speleology; ii) physical\nskilfulness is a means for knowledge generation; iii) the practice is\nnecessarily collective and requires group coordination. From these findings, an\nambivalent attitude towards technology emerged: on the one hand, the scientific\nvocation of speleology welcomes technology supporting the development of\nknowledge; on the other hand, aspects typical of adventure sports lead to\nresistance to technology facilitating the physical performance. We conclude the\narticle by presenting design considerations for devices supporting speleology,\nas well as a few reflections on how communities of speleologists can inspire\ncitizen science projects.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 08:39:42 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 09:17:26 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Mencarini", "Eleonora", ""], ["Rapp", "Amon", ""], ["Zancanaro", "Massimo", ""]]}, {"id": "2104.02392", "submitter": "Thomas Steiner", "authors": "Thomas Steiner, Fran\\c{c}ois Beaufort", "title": "Accessing HID Devices on the Web With the WebHID API: How to play the\n  Chrome Dino Game by Jumping With a Nintendo Joy-Con Controller in One's\n  Pocket", "comments": "2 pages, accepted at the Developers Track of The Web Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this demonstration, we show how special hardware like Nintendo Joy-Con\ncontrollers can be made accessible from the Web through the new WebHID API.\nThis novel technology proposal allows developers to write Web drivers in pure\nJavaScript that talk to Human Interface Device (HID) devices via the HID\nprotocol. One such example of a driver has been realized in the project\nJoy-Con-WebHID, which allows for fun pastimes like playing the Google Chrome\nbrowser's offline dinosaur game by jumping. This works thanks to the\naccelerometers built into Joy-Con controllers whose signals are read out by the\ndriver and used to control the game character in the browser. A video of the\nexperience is available.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 09:49:53 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Steiner", "Thomas", ""], ["Beaufort", "Fran\u00e7ois", ""]]}, {"id": "2104.02580", "submitter": "David Pastor-Escuredo", "authors": "David Pastor-Escuredo", "title": "Future of work: ethics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Work must be reshaped in the upcoming new era characterized by new challenges\nand the presence of new technologies and computational tools. Over-automation\nseems to be the driver of the digitalization process. Substitution is the\nparadigm leading Artificial Intelligence and robotics development against human\ncognition. Digital technology should be designed to enhance human skills and\nmake more productive use of human cognition and capacities. Digital technology\nis characterized also by scalability because of its easy and inexpensive\ndeployment. Thus, automation can lead to the absence of jobs and scalable\nnegative impact in human development and the performance of business. A look at\ndigitalization from the lens of Sustainable Development Goals can tell us how\ndigitalization impact in different sectors and areas considering society as a\ncomplex interconnected system. Here, reflections on how AI and Data impact\nfuture of work and sustainable development are provided grounded on an ethical\ncore that comprises human-level principles and also systemic principles.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 15:20:30 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Pastor-Escuredo", "David", ""]]}, {"id": "2104.02643", "submitter": "David Melhart", "authors": "David Melhart, Antonios Liapis, Georgios N. Yannakakis", "title": "The Affect Game AnnotatIoN (AGAIN) Dataset", "comments": "Under review in IEEE Transactions on Affective Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we model affect in a general fashion, across dissimilar tasks, and to\nwhich degree are such general representations of affect even possible? To\naddress such questions and enable research towards general affective computing\nthis paper introduces the Affect Game AnnotatIoN (AGAIN) dataset. AGAIN is a\nlarge-scale affective corpus that features over 1,100 in-game videos (with\ncorresponding gameplay data) from nine different games, which are annotated for\narousal from 124 participants in a first-person continuous fashion. Even though\nAGAIN is created for the purpose of investigating the generality of affective\ncomputing across dissimilar tasks, affect modelling can be studied within each\nof its 9 specific interactive games. To the best of our knowledge AGAIN is the\nlargest - over 37 hours of annotated video and game logs - and the most diverse\npublicly available affective dataset based on games as interactive affect\nelicitors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:27:21 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Melhart", "David", ""], ["Liapis", "Antonios", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "2104.02712", "submitter": "Eunice Jun", "authors": "Eunice Jun, Melissa Birchfield, Nicole de Moura, Jeffrey Heer and Rene\n  Just", "title": "Hypothesis Formalization: Empirical Findings, Software Limitations, and\n  Design Implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.HC cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Data analysis requires translating higher level questions and hypotheses into\ncomputable statistical models. We present a mixed-methods study aimed at\nidentifying the steps, considerations, and challenges involved in\noperationalizing hypotheses into statistical models, a process we refer to as\nhypothesis formalization. In a formative content analysis of research papers,\nwe find that researchers highlight decomposing a hypothesis into\nsub-hypotheses, selecting proxy variables, and formulating statistical models\nbased on data collection design as key steps. In a lab study, we find that\nanalysts fixated on implementation and shaped their analysis to fit familiar\napproaches, even if sub-optimal. In an analysis of software tools, we find that\ntools provide inconsistent, low-level abstractions that may limit the\nstatistical models analysts use to formalize hypotheses. Based on these\nobservations, we characterize hypothesis formalization as a dual-search process\nbalancing conceptual and statistical considerations constrained by data and\ncomputation, and discuss implications for future tools.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:11:28 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Jun", "Eunice", ""], ["Birchfield", "Melissa", ""], ["de Moura", "Nicole", ""], ["Heer", "Jeffrey", ""], ["Just", "Rene", ""]]}, {"id": "2104.02797", "submitter": "Archit Rathore", "authors": "Archit Rathore, Sunipa Dev, Jeff M. Phillips, Vivek Srikumar, Yan\n  Zheng, Chin-Chia Michael Yeh, Junpeng Wang, Wei Zhang, Bei Wang", "title": "VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word\n  Representations", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word vector embeddings have been shown to contain and amplify biases in data\nthey are extracted from. Consequently, many techniques have been proposed to\nidentify, mitigate, and attenuate these biases in word representations. In this\npaper, we utilize interactive visualization to increase the interpretability\nand accessibility of a collection of state-of-the-art debiasing techniques. To\naid this, we present Visualization of Embedding Representations for deBiasing\nsystem (\"VERB\"), an open-source web-based visualization tool that helps the\nusers gain a technical understanding and visual intuition of the inner workings\nof debiasing techniques, with a focus on their geometric properties. In\nparticular, VERB offers easy-to-follow use cases in exploring the effects of\nthese debiasing techniques on the geometry of high-dimensional word vectors. To\nhelp understand how various debiasing techniques change the underlying\ngeometry, VERB decomposes each technique into interpretable sequences of\nprimitive transformations and highlights their effect on the word vectors using\ndimensionality reduction and interactive visual exploration. VERB is designed\nto target natural language processing (NLP) practitioners who are designing\ndecision-making systems on top of word embeddings, and also researchers working\nwith fairness and ethics of machine learning systems in NLP. It can also serve\nas a visual medium for education, which helps an NLP novice to understand and\nmitigate biases in word embeddings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:29:16 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Rathore", "Archit", ""], ["Dev", "Sunipa", ""], ["Phillips", "Jeff M.", ""], ["Srikumar", "Vivek", ""], ["Zheng", "Yan", ""], ["Yeh", "Chin-Chia Michael", ""], ["Wang", "Junpeng", ""], ["Zhang", "Wei", ""], ["Wang", "Bei", ""]]}, {"id": "2104.02818", "submitter": "Aditi Mishra", "authors": "Aditi Mishra, Utkarsh Soni, Jinbin Huang, Chris Bryan", "title": "Why? Why not? When? Visual Explanations of Agent Behavior in\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) is a widely-used technique in many domains,\nincluding autonomous driving, robotics, stock trading, and video games.\nUnfortunately, the black box nature of RL agents, combined with increasing\nlegal and ethical considerations, makes it increasingly important that humans\nunderstand the reasoning behind the actions taken by an RL agent, particularly\nin safety-critical domains. To help address this challenge, we introduce\nPolicyExplainer, a visual analytics interface which lets the user directly\nquery an RL agent. PolicyExplainer visualizes the states, policy, and expected\nfuture rewards for an agent, and supports asking and answering questions such\nas: \"Why take this action? Why not this other action? When is this action\ntaken?\". PolicyExplainer is designed based upon a domain analysis with RL\nexperts, and is evaluated via empirical assessments on a trio of domains: taxi\nnavigation, an inventory application, and the safety-critical domain of drug\nrecommendation for HIV patients.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 22:36:29 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Mishra", "Aditi", ""], ["Soni", "Utkarsh", ""], ["Huang", "Jinbin", ""], ["Bryan", "Chris", ""]]}, {"id": "2104.03010", "submitter": "Thomas Mildner", "authors": "Thomas Mildner and Gian-Luca Savino", "title": "Ethical User Interfaces: Exploring the Effects of Dark Patterns on\n  Facebook", "comments": null, "journal-ref": null, "doi": "10.1145/3411763.3451659", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Many researchers have been concerned with whether social media has a negative\nimpact on the well-being of their audience. With the popularity of social\nnetworking sites (SNS) steadily increasing, psychological and social sciences\nhave shown great interest in their effects and consequences on humans. In this\nwork, we investigate Facebook using the tools of HCI to find connections\nbetween interface features and the concerns raised by these domains. Using an\nempirical design analysis, we identify interface interferences impacting users'\nonline privacy. Through a subsequent survey (n=116), we find usage behaviour\nchanges due to increased privacy concerns and report individual cases of\naddiction and mental health issues. These observations are the results of a\nrapidly changing SNS creating a gap of understanding between users'\ninteractions with the platform and future consequences. We explore how HCI can\nhelp close this gap and work towards more ethical user interfaces in the\nfuture.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:31:24 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Mildner", "Thomas", ""], ["Savino", "Gian-Luca", ""]]}, {"id": "2104.03019", "submitter": "Chao Wang", "authors": "Chao Wang, Thomas H. Weisswange, Matti Krueger, Christiane B.\n  Wiebel-Herboth", "title": "Human-Vehicle Cooperation on Prediction-Level: Enhancing Automated\n  Driving with Human Foresight", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To maximize safety and driving comfort, autonomous driving systems can\nbenefit from implementing foresighted action choices that take different\npotential scenario developments into account. While artificial scene prediction\nmethods are making fast progress, an attentive human driver may still be able\nto identify relevant contextual features which are not adequately considered by\nthe system or for which the human driver may have a lack of trust into the\nsystem's capabilities to treat them appropriately. We implement an approach\nthat lets a human driver quickly and intuitively supplement scene predictions\nto an autonomous driving system by gaze. We illustrate the feasibility of this\napproach in an existing autonomous driving system running a variety of\nscenarios in a simulator. Furthermore, a Graphical User Interface (GUI) was\ndesigned and integrated to enhance the trust and explainability of the system.\nThe utilization of such cooperatively augmented scenario predictions has the\npotential to improve a system's foresighted driving abilities and make\nautonomous driving more trustable, comfortable and personalized.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:49:34 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:33:29 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 08:12:36 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wang", "Chao", ""], ["Weisswange", "Thomas H.", ""], ["Krueger", "Matti", ""], ["Wiebel-Herboth", "Christiane B.", ""]]}, {"id": "2104.03483", "submitter": "Q.Vera Liao", "authors": "Q. Vera Liao, Milena Pribi\\'c, Jaesik Han, Sarah Miller, Daby Sow", "title": "Question-Driven Design Process for Explainable AI User Experiences", "comments": "working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A pervasive design issue of AI systems is their explainability--how to\nprovide appropriate information to help users understand the AI. The technical\nfield of explainable AI (XAI) has produced a rich toolbox of techniques.\nDesigners are now tasked with the challenges of how to select the most suitable\nXAI techniques and translate them into UX solutions. Informed by our previous\nwork studying design challenges around XAI UX, this work proposes a design\nprocess to tackle these challenges. We review our and related prior work to\nidentify requirements that the process should fulfill, and accordingly, propose\na Question-Driven Design Process that grounds the user needs, choices of XAI\ntechniques, design, and evaluation of XAI UX all in the user questions. We\nprovide a mapping guide between prototypical user questions and exemplars of\nXAI techniques to reframe the technical space of XAI, also serving as boundary\nobjects to support collaboration between designers and AI engineers. We\ndemonstrate it with a use case of designing XAI for healthcare adverse events\nprediction, and discuss lessons learned for tackling design challenges of AI\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 02:51:36 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 01:17:55 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liao", "Q. Vera", ""], ["Pribi\u0107", "Milena", ""], ["Han", "Jaesik", ""], ["Miller", "Sarah", ""], ["Sow", "Daby", ""]]}, {"id": "2104.03795", "submitter": "Marc Satkowski", "authors": "Marc Satkowski, Wolfgang B\\\"uschel, Raimund Dachselt", "title": "Experiences with User Studies in Augmented Reality", "comments": "This work has been accepted for the ACM CHI 2021 Workshop \"Evaluating\n  User Experiences in Mixed Reality\" (https://sputze.github.io/evaluating-mr/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research field of augmented reality (AR) is of increasing popularity, as\nseen, among others, in several recently published surveys. To produce further\nadvancements in AR, it is not only necessary to create new systems or\napplications, but also to evaluate them. One important aspect in regards to the\nevaluation is the general understanding of how users experience a given AR\napplication, which can also be seen by the increased number of papers focusing\non this topic that were published in the last years. With the steadily growing\nunderstanding and development of AR in general, it is only a matter of time\nuntil AR devices make the leap into the consumer market where such an in-depth\nuser understanding is even more essential. Thus, a better understanding of\nfactors that could influence the design and results of user experience studies\ncan help us to make them more robust and dependable in the future.\n  In this position paper, we describe three challenges which researchers face\nwhile designing and conducting AR users studies. We encountered these\nchallenges in our past and current research, including papers that focus on\nperceptual studies of visualizations, interaction studies, and studies\nexploring the use of AR applications and their design spaces.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:18:51 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Satkowski", "Marc", ""], ["B\u00fcschel", "Wolfgang", ""], ["Dachselt", "Raimund", ""]]}, {"id": "2104.03800", "submitter": "Yuta Itoh", "authors": "Yuta Itoh, Takumi Kaminokado, Kaan Aksit", "title": "Beaming Displays", "comments": "10 pages. This is a preprint of a publication at IEEE Transactions on\n  Visualization and Computer Graphics (TVCG), 2021. Presented at and nominated\n  for best journal papers at IEEE Virtual Reality (VR) 2021\n  (https://ieeevr.org/2021/awards/conference-awards/)", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2021", "doi": "10.1109/TVCG.2021.3067764", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing near-eye display designs struggle to balance between multiple\ntrade-offs such as form factor, weight, computational requirements, and battery\nlife. These design trade-offs are major obstacles on the path towards an\nall-day usable near-eye display. In this work, we address these trade-offs by,\nparadoxically, \\textit{removing the display} from near-eye displays. We present\nthe beaming displays, a new type of near-eye display system that uses a\nprojector and an all passive wearable headset. We modify an off-the-shelf\nprojector with additional lenses. We install such a projector to the\nenvironment to beam images from a distance to a passive wearable headset. The\nbeaming projection system tracks the current position of a wearable headset to\nproject distortion-free images with correct perspectives. In our system, a\nwearable headset guides the beamed images to a user's retina, which are then\nperceived as an augmented scene within a user's field of view. In addition to\nproviding the system design of the beaming display, we provide a physical\nprototype and show that the beaming display can provide resolutions as high as\nconsumer-level near-eye displays. We also discuss the different aspects of the\ndesign space for our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:24:39 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Itoh", "Yuta", ""], ["Kaminokado", "Takumi", ""], ["Aksit", "Kaan", ""]]}, {"id": "2104.03809", "submitter": "Vikram Shree", "authors": "Vikram Shree, Beatriz Asfora, Rachel Zheng, Samantha Hong, Jacopo\n  Banfi, and Mark Campbell", "title": "Exploiting Natural Language for Efficient Risk-Aware Multi-robot SaR\n  Planning", "comments": "8 pages, 5 figures. To be presented at the IEEE International\n  Conference on Robotics and Automation, 2021. Dataset available at:\n  https://github.com/vikshree/DISC-L.git", "journal-ref": "IEEE Robotics and Automation Letters, vol. 6, no. 2, pp.\n  3152-3159, April 2021", "doi": "10.1109/LRA.2021.3062798", "report-no": null, "categories": "cs.RO cs.HC cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The ability to develop a high-level understanding of a scene, such as\nperceiving danger levels, can prove valuable in planning multi-robot search and\nrescue (SaR) missions. In this work, we propose to uniquely leverage natural\nlanguage descriptions from the mission commander in chief and image data\ncaptured by robots to estimate scene danger. Given a description and an image,\na state-of-the-art deep neural network is used to assess a corresponding\nsimilarity score, which is then converted into a probabilistic distribution of\ndanger levels. Because commonly used visio-linguistic datasets do not represent\nSaR missions well, we collect a large-scale image-description dataset from\nsynthetic images taken from realistic disaster scenes and use it to train our\nmachine learning model. A risk-aware variant of the Multi-robot Efficient\nSearch Path Planning (MESPP) problem is then formulated to use the danger\nestimates in order to account for high-risk locations in the environment when\nplanning the searchers' paths. The problem is solved via a distributed approach\nbased on Mixed-Integer Linear Programming. Our experiments demonstrate that our\nframework allows to plan safer yet highly successful search missions, abiding\nto the two most important aspects of SaR missions: to ensure both searchers'\nand victim safety.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:41:51 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Shree", "Vikram", ""], ["Asfora", "Beatriz", ""], ["Zheng", "Rachel", ""], ["Hong", "Samantha", ""], ["Banfi", "Jacopo", ""], ["Campbell", "Mark", ""]]}, {"id": "2104.03820", "submitter": "Justin Weisz", "authors": "Justin D. Weisz, Michael Muller, Stephanie Houde, John Richards,\n  Steven I. Ross, Fernando Martinez, Mayank Agarwal, Kartik Talamadupula", "title": "Perfection Not Required? Human-AI Partnerships in Code Translation", "comments": "18 pages, 1 figure. To be published in IUI 2021", "journal-ref": null, "doi": "10.1145/3397481.3450656", "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models have become adept at producing artifacts such as images,\nvideos, and prose at human-like levels of proficiency. New generative\ntechniques, such as unsupervised neural machine translation (NMT), have\nrecently been applied to the task of generating source code, translating it\nfrom one programming language to another. The artifacts produced in this way\nmay contain imperfections, such as compilation or logical errors. We examine\nthe extent to which software engineers would tolerate such imperfections and\nexplore ways to aid the detection and correction of those errors. Using a\ndesign scenario approach, we interviewed 11 software engineers to understand\ntheir reactions to the use of an NMT model in the context of application\nmodernization, focusing on the task of translating source code from one\nlanguage to another. Our three-stage scenario sparked discussions about the\nutility and desirability of working with an imperfect AI system, how acceptance\nof that system's outputs would be established, and future opportunities for\ngenerative AI in application modernization. Our study highlights how UI\nfeatures such as confidence highlighting and alternate translations help\nsoftware engineers work with and better understand generative NMT models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:57:49 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Weisz", "Justin D.", ""], ["Muller", "Michael", ""], ["Houde", "Stephanie", ""], ["Richards", "John", ""], ["Ross", "Steven I.", ""], ["Martinez", "Fernando", ""], ["Agarwal", "Mayank", ""], ["Talamadupula", "Kartik", ""]]}, {"id": "2104.03892", "submitter": "Laura Petrich", "authors": "Laura Petrich, Jun Jin, Masood Dehghan and Martin Jagersand", "title": "A Quantitative Analysis of Activities of Daily Living: Insights into\n  Improving Functional Independence with Assistive Robotics", "comments": "Submitted to IROS 2021. arXiv admin note: substantial text overlap\n  with arXiv:2101.02750", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human assistive robotics have the potential to help the elderly and\nindividuals living with disabilities with their Activities of Daily Living\n(ADL). Robotics researchers focus on assistive tasks from the perspective of\nvarious control schemes and motion types. Health research on the other hand\nfocuses on clinical assessment and rehabilitation, arguably leaving important\ndifferences between the two domains. In particular, little is known\nquantitatively on which ADLs are typically carried out in a persons everyday\nenvironment - at home, work, etc. Understanding what activities are frequently\ncarried out during the day can help guide the development and prioritization of\nrobotic technology for in-home assistive robotic deployment. This study targets\nseveral lifelogging databases, where we compute (i) ADL task frequency from\nlong-term low sampling frequency video and Internet of Things (IoT) sensor\ndata, and (ii) short term arm and hand movement data from 30 fps video data of\ndomestic tasks. Robotics and health care communities have differing terms and\ntaxonomies for representing tasks and motions. In this work, we derive and\ndiscuss a robotics-relevant taxonomy from quantitative ADL task and motion data\nin attempt to ameliorate taxonomic differences between the two communities. Our\nquantitative results provide direction for the development of better assistive\nrobots to support the true demands of the healthcare community.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:00:52 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Petrich", "Laura", ""], ["Jin", "Jun", ""], ["Dehghan", "Masood", ""], ["Jagersand", "Martin", ""]]}, {"id": "2104.03893", "submitter": "Mehrshad Zandigohar", "authors": "Mehrshad Zandigohar, Mo Han, Mohammadreza Sharif, Sezen Yagmur Gunay,\n  Mariusz P. Furmanek, Mathew Yarossi, Paolo Bonato, Cagdas Onal, Taskin Padir,\n  Deniz Erdogmus, Gunar Schirner", "title": "Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in\n  Prosthetic Hand Control", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For lower arm amputees, robotic prosthetic hands offer the promise to regain\nthe capability to perform fine object manipulation in activities of daily\nliving. Current control methods based on physiological signals such as EEG and\nEMG are prone to poor inference outcomes due to motion artifacts, variability\nof skin electrode junction impedance over time, muscle fatigue, and other\nfactors. Visual evidence is also susceptible to its own artifacts, most often\ndue to object occlusion, lighting changes, variable shapes of objects depending\non view-angle, among other factors. Multimodal evidence fusion using\nphysiological and vision sensor measurements is a natural approach due to the\ncomplementary strengths of these modalities.\n  In this paper, we present a Bayesian evidence fusion framework for grasp\nintent inference using eye-view video, gaze, and EMG from the forearm processed\nby neural network models. We analyze individual and fused performance as a\nfunction of time as the hand approaches the object to grasp it. For this\npurpose, we have also developed novel data processing and augmentation\ntechniques to train neural network components. Our experimental data analyses\ndemonstrate that EMG and visual evidence show complementary strengths, and as a\nconsequence, fusion of multimodal evidence can outperform each individual\nevidence modality at any given time. Specifically, results indicate that, on\naverage, fusion improves the instantaneous upcoming grasp type classification\naccuracy while in the reaching phase by 13.66% and 14.8%, relative to EMG and\nvisual evidence individually. An overall fusion accuracy of 95.3% among 13\nlabels (compared to a chance level of 7.7%) is achieved, and more detailed\nanalysis indicate that the correct grasp is inferred sufficiently early and\nwith high confidence compared to the top contender, in order to allow\nsuccessful robot actuation to close the loop.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:01:19 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zandigohar", "Mehrshad", ""], ["Han", "Mo", ""], ["Sharif", "Mohammadreza", ""], ["Gunay", "Sezen Yagmur", ""], ["Furmanek", "Mariusz P.", ""], ["Yarossi", "Mathew", ""], ["Bonato", "Paolo", ""], ["Onal", "Cagdas", ""], ["Padir", "Taskin", ""], ["Erdogmus", "Deniz", ""], ["Schirner", "Gunar", ""]]}, {"id": "2104.03940", "submitter": "Abhishek Kaushik Mr.", "authors": "Abhishek Kaushik and Gareth J. F. Jones", "title": "A Conceptual Framework for Implicit Evaluation of Conversational Search\n  Interfaces", "comments": "Accepted in MICROS (Mixed-Initiative ConveRsatiOnal Systems) Workshop\n  at 43rd European Conference on Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search (CS) has recently become a significant focus of the\ninformation retrieval (IR) research community. Multiple studies have been\nconducted which explore the concept of conversational search. Understanding and\nadvancing research in CS requires careful and detailed evaluation. Existing CS\nstudies have been limited to evaluation based on simple user feedback on task\ncompletion. We propose a CS evaluation framework which includes multiple\ndimensions: search experience, knowledge gain, software usability, cognitive\nload and user experience, based on studies of conversational systems and IR. We\nintroduce these evaluation criteria and propose their use in a framework for\nthe evaluation of CS systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:33:18 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kaushik", "Abhishek", ""], ["Jones", "Gareth J. F.", ""]]}, {"id": "2104.04034", "submitter": "Zichao Wang", "authors": "Zichao Wang, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan\n  Zaykov, Jose Miguel Hernandez-Lobato, Richard E. Turner, Richard G. Baraniuk,\n  Craig Barton, Simon Peyton Jones, Simon Woodhead, Cheng Zhang", "title": "Results and Insights from Diagnostic Questions: The NeurIPS 2020\n  Education Challenge", "comments": "arXiv admin note: text overlap with arXiv:2007.12061", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This competition concerns educational diagnostic questions, which are\npedagogically effective, multiple-choice questions (MCQs) whose distractors\nembody misconceptions. With a large and ever-increasing number of such\nquestions, it becomes overwhelming for teachers to know which questions are the\nbest ones to use for their students. We thus seek to answer the following\nquestion: how can we use data on hundreds of millions of answers to MCQs to\ndrive automatic personalized learning in large-scale learning scenarios where\nmanual personalization is infeasible? Success in using MCQ data at scale helps\nbuild more intelligent, personalized learning platforms that ultimately improve\nthe quality of education en masse. To this end, we introduce a new,\nlarge-scale, real-world dataset and formulate 4 data mining tasks on MCQs that\nmimic real learning scenarios and target various aspects of the above question\nin a competition setting at NeurIPS 2020. We report on our NeurIPS competition\nin which nearly 400 teams submitted approximately 4000 submissions, with\nencouragingly diverse and effective approaches to each of our tasks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:09:58 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Wang", "Zichao", ""], ["Lamb", "Angus", ""], ["Saveliev", "Evgeny", ""], ["Cameron", "Pashmina", ""], ["Zaykov", "Yordan", ""], ["Hernandez-Lobato", "Jose Miguel", ""], ["Turner", "Richard E.", ""], ["Baraniuk", "Richard G.", ""], ["Barton", "Craig", ""], ["Jones", "Simon Peyton", ""], ["Woodhead", "Simon", ""], ["Zhang", "Cheng", ""]]}, {"id": "2104.04039", "submitter": "Zhiyu Lin", "authors": "Zhiyu Lin, Mark Riedl", "title": "Plug-and-Blend: A Framework for Controllable Story Generation with\n  Blended Control Codes", "comments": "8 pages, To appear at AIIDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained neural language models (LM) have very powerful text\ngeneration capabilities. However, in practice, they are hard to control for\ncreative purposes. We describe a Plug-and-Play controllable language generation\nframework, Plug-and-Blend, that allows a human user to input multiple control\ncodes (topics). In the context of automated story generation, this allows a\nhuman user loose or fine-grained control of the topics and transitions between\nthem that will appear in the generated story, and can even allow for\noverlapping, blended topics. Automated evaluations show our framework, working\nwith different generative LMs, controls the generation towards given\ncontinuous-weighted control codes while keeping the generated sentences fluent,\ndemonstrating strong blending capability. A human participant evaluation shows\nthat the generated stories are observably transitioning between two topics.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 03:15:14 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 14:33:47 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lin", "Zhiyu", ""], ["Riedl", "Mark", ""]]}, {"id": "2104.04042", "submitter": "Andrew McNutt", "authors": "Andrew McNutt", "title": "What are Table Cartograms Good for Anyway? An Algebraic Analysis", "comments": "EuroVis Camera Ready. 16 Pages, 19 Figures, including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unfamiliar or esoteric visual forms arise in many areas of visualization.\nWhile such forms can be intriguing, it can be unclear how to make effective use\nof them without long periods of practice or costly user studies. In this work\nwe analyze the table cartogram-a graphic which visualizes tabular data by\nbringing the areas of a grid of quadrilaterals into correspondence with the\ninput data, like a heat map that has been \"area-ed\" rather than colored.\nDespite having existed for several years, little is known about its appropriate\nusage. We mend this gap by using Algebraic Visualization Design to show that\nthey are best suited to relatively small tables with ordinal axes for some\ncomparison and outlier identification tasks. In doing so we demonstrate a\ndiscount theory-based analysis that can be used to cheaply determine best\npractices for unknown visualizations.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:36:32 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["McNutt", "Andrew", ""]]}, {"id": "2104.04072", "submitter": "Soaad Hossain Mr", "authors": "Soaad Hossain, Syed Ishtiaque Ahmed", "title": "Towards a New Participatory Approach for Designing Artificial\n  Intelligence and Data-Driven Technologies", "comments": "5 pages, 2 figures, accepted to Artificially Intelligent Technology\n  for the Margins workshop at Conference on Human Factors in Computing Systems\n  (CHI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With there being many technical and ethical issues with artificial\nintelligence (AI) that involve marginalized communities, there is a growing\ninterest for design methods used with marginalized people that may be\ntransferable to the design of AI technologies. Participatory design (PD) is a\ndesign method that is often used with marginalized communities for the design\nof social development, policy, IT and other matters and solutions. However,\nthere are issues with the current PD, raising concerns when it is applied to\nthe design of technologies, including AI technologies. This paper argues for\nthe use of PD for the design of AI technologies, and introduces and proposes a\nnew PD, which we call agile participatory design, that not only can could be\nused for the design of AI and data-driven technologies, but also overcomes\nissues surrounding current PD and its use in the design of such technologies.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 23:36:25 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Hossain", "Soaad", ""], ["Ahmed", "Syed Ishtiaque", ""]]}, {"id": "2104.04077", "submitter": "Filipo Sharevski", "authors": "Donald Gover and Filipo Sharevski", "title": "Two Truths and a Lie: Exploring Soft Moderation of COVID-19\n  Misinformation with Amazon Alexa", "comments": "arXiv admin note: text overlap with arXiv:2104.00779", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we analyzed the perceived accuracy of COVID-19 vaccine Tweets\nwhen they were spoken back by a third-party Amazon Alexa skill. We mimicked the\nsoft moderation that Twitter applies to COVID-19 misinformation content in both\nforms of warning covers and warning tags to investigate whether the third-party\nskill could affect how and when users heed these warnings. The results from a\n304-participant study suggest that the spoken back warning covers may not work\nas intended, even when converted from text to speech. We controlled for\nCOVID-19 vaccination hesitancy and political leanings and found that the\nvaccination hesitant Alexa users ignored any type of warning as long as the\nTweets align with their personal beliefs. The politically independent users\ntrusted Alexa less then their politically-laden counterparts and that helped\nthem accurately perceiving truthful COVID-19 information. We discuss soft\nmoderation adaptations for voice assistants to achieve the intended effect of\ncurbing COVID-19 misinformation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 22:37:34 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Gover", "Donald", ""], ["Sharevski", "Filipo", ""]]}, {"id": "2104.04118", "submitter": "Tica Lin", "authors": "Tica Lin, Rishi Singh, Yalong Yang, Carolina Nobre, Johanna Beyer,\n  Maurice A. Smith and Hanspeter Pfister", "title": "Towards an Understanding of Situated AR Visualization for Basketball\n  Free-Throw Training", "comments": "To appear in the 2021 ACM Conference on Human Factors in Computing\n  Systems (CHI 2021)", "journal-ref": null, "doi": "10.1145/3411764.3445649", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an observational study to compare co-located and situated\nreal-time visualizations in basketball free-throw training. Our goal is to\nunderstand the advantages and concerns of applying immersive visualization to\nreal-world skill-based sports training and to provide insights for designing AR\nsports training systems. We design both a situated 3D visualization on a\nhead-mounted display and a 2D visualization on a co-located display to provide\nimmediate visual feedback on a player's shot performance. Using a\nwithin-subject study design with experienced basketball shooters, we\ncharacterize user goals, report on qualitative training experiences, and\ncompare the quantitative training results. Our results show that real-time\nvisual feedback helps athletes refine subsequent shots. Shooters in our study\nachieve greater angle consistency with our visual feedback. Furthermore, AR\nvisualization promotes an increased focus on body form in athletes. Finally, we\npresent suggestions for the design of future sports AR studies.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 23:54:31 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 16:24:04 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lin", "Tica", ""], ["Singh", "Rishi", ""], ["Yang", "Yalong", ""], ["Nobre", "Carolina", ""], ["Beyer", "Johanna", ""], ["Smith", "Maurice A.", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "2104.04122", "submitter": "Michael Desmond", "authors": "Michael Desmond, Zahra Ashktorab, Michelle Brachman, Kristina\n  Brimijoin, Evelyn Duesterwald, Casey Dugan, Catherine Finegan-Dollak, Michael\n  Muller, Narendra Nath Joshi, Qian Pan, and Aabhas Sharma", "title": "Increasing the Speed and Accuracy of Data LabelingThrough an AI Assisted\n  Interface", "comments": null, "journal-ref": null, "doi": "10.1145/3397481.3450698", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Labeling data is an important step in the supervised machine learning\nlifecycle. It is a laborious human activity comprised of repeated decision\nmaking: the human labeler decides which of several potential labels to apply to\neach example. Prior work has shown that providing AI assistance can improve the\naccuracy of binary decision tasks. However, the role of AI assistance in more\ncomplex data-labeling scenarios with a larger set of labels has not yet been\nexplored. We designed an AI labeling assistant that uses a semi-supervised\nlearning algorithm to predict the most probable labels for each example. We\nleverage these predictions to provide assistance in two ways: (i) providing a\nlabel recommendation and (ii) reducing the labeler's decision space by focusing\ntheir attention on only the most probable labels. We conducted a user study\n(n=54) to evaluate an AI-assisted interface for data labeling in this context.\nOur results highlight that the AI assistance improves both labeler accuracy and\nspeed, especially when the labeler finds the correct label in the reduced label\nspace. We discuss findings related to the presentation of AI assistance and\ndesign implications for intelligent labeling interfaces.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 00:41:11 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Desmond", "Michael", ""], ["Ashktorab", "Zahra", ""], ["Brachman", "Michelle", ""], ["Brimijoin", "Kristina", ""], ["Duesterwald", "Evelyn", ""], ["Dugan", "Casey", ""], ["Finegan-Dollak", "Catherine", ""], ["Muller", "Michael", ""], ["Joshi", "Narendra Nath", ""], ["Pan", "Qian", ""], ["Sharma", "Aabhas", ""]]}, {"id": "2104.04147", "submitter": "David Leslie", "authors": "David Leslie, Christopher Burr, Mhairi Aitken, Josh Cowls, Michael\n  Katell and Morgan Briggs", "title": "Artificial intelligence, human rights, democracy, and the rule of law: a\n  primer", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.4639743", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In September 2019, the Council of Europe's Committee of Ministers adopted the\nterms of reference for the Ad Hoc Committee on Artificial Intelligence (CAHAI).\nThe CAHAI is charged with examining the feasibility and potential elements of a\nlegal framework for the design, development, and deployment of AI systems that\naccord with Council of Europe standards across the interrelated areas of human\nrights, democracy, and the rule of law. As a first and necessary step in\ncarrying out this responsibility, the CAHAI's Feasibility Study, adopted by its\nplenary in December 2020, has explored options for an international legal\nresponse that fills existing gaps in legislation and tailors the use of binding\nand non-binding legal instruments to the specific risks and opportunities\npresented by AI systems. The Study examines how the fundamental rights and\nfreedoms that are already codified in international human rights law can be\nused as the basis for such a legal framework. The purpose of this primer is to\nintroduce the main concepts and principles presented in the CAHAI's Feasibility\nStudy for a general, non-technical audience. It also aims to provide some\nbackground information on the areas of AI innovation, human rights law,\ntechnology policy, and compliance mechanisms covered therein. In keeping with\nthe Council of Europe's commitment to broad multi-stakeholder consultations,\noutreach, and engagement, this primer has been designed to help facilitate the\nmeaningful and informed participation of an inclusive group of stakeholders as\nthe CAHAI seeks feedback and guidance regarding the essential issues raised by\nthe Feasibility Study.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 05:58:42 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Leslie", "David", ""], ["Burr", "Christopher", ""], ["Aitken", "Mhairi", ""], ["Cowls", "Josh", ""], ["Katell", "Michael", ""], ["Briggs", "Morgan", ""]]}, {"id": "2104.04311", "submitter": "Hendrik Heuer", "authors": "Hendrik Heuer", "title": "Helping People Deal With Disinformation -- A Socio-Technical Perspective", "comments": "This paper will be presented at the Workshop on Human Aspects of\n  Misinformation at CHI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  At the latest since the advent of the Internet, disinformation and conspiracy\ntheories have become ubiquitous. Recent examples like QAnon and Pizzagate prove\nthat false information can lead to real violence. In this motivation statement\nfor the Workshop on Human Aspects of Misinformation at CHI 2021, I explain my\nresearch agenda focused on 1. why people believe in disinformation, 2. how\npeople can be best supported in recognizing disinformation, and 3. what the\npotentials and risks of different tools designed to fight disinformation are.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 11:33:02 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Heuer", "Hendrik", ""]]}, {"id": "2104.04375", "submitter": "Yunfeng Zhang", "authors": "Shweta Narkar, Yunfeng Zhang, Q. Vera Liao, Dakuo Wang, Justin D Weisz", "title": "Model LineUpper: Supporting Interactive Model Comparison at Multiple\n  Levels for AutoML", "comments": null, "journal-ref": null, "doi": "10.1145/3397481.3450658", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Machine Learning (AutoML) is a rapidly growing set of technologies\nthat automate the model development pipeline by searching model space and\ngenerating candidate models. A critical, final step of AutoML is human\nselection of a final model from dozens of candidates. In current AutoML\nsystems, selection is supported only by performance metrics. Prior work has\nshown that in practice, people evaluate ML models based on additional criteria,\nsuch as the way a model makes predictions. Comparison may happen at multiple\nlevels, from types of errors, to feature importance, to how the model makes\npredictions of specific instances. We developed \\tool{} to support interactive\nmodel comparison for AutoML by integrating multiple Explainable AI (XAI) and\nvisualization techniques. We conducted a user study in which we both evaluated\nthe system and used it as a technology probe to understand how users perform\nmodel comparison in an AutoML system. We discuss design implications for\nutilizing XAI techniques for model comparison and supporting the unique needs\nof data scientists in comparing AutoML models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 14:06:13 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Narkar", "Shweta", ""], ["Zhang", "Yunfeng", ""], ["Liao", "Q. Vera", ""], ["Wang", "Dakuo", ""], ["Weisz", "Justin D", ""]]}, {"id": "2104.04415", "submitter": "Steve Schmidt", "authors": "Steve Schmidt, Denley Lam, Patrick Hayden", "title": "Automatic Knowledge Extraction with Human Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  OrbWeaver, an automatic knowledge extraction system paired with a human\ninterface, streamlines the use of unintuitive natural language processing\nsoftware for modeling systems from their documentation. OrbWeaver enables the\nindirect transfer of knowledge about legacy systems by leveraging open source\ntools in document understanding and processing as well as using web based user\ninterface constructs. By design, OrbWeaver is scalable, extensible, and usable;\nwe demonstrate its utility by evaluating its performance in processing a corpus\nof documents related to advanced persistent threats in the cyber domain. The\nresults indicate better knowledge extraction by revealing hidden relationships,\nlinking co-related entities, and gathering evidence.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:07:11 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Schmidt", "Steve", ""], ["Lam", "Denley", ""], ["Hayden", "Patrick", ""]]}, {"id": "2104.04429", "submitter": "Tanvi Dinkar", "authors": "Utku Norman, Tanvi Dinkar, Barbara Bruno, Chlo\\'e Clavel", "title": "Studying Alignment in Spontaneous Speech via Automatic Methods: How Do\n  Children Use Task-specific Referents to Succeed in a Collaborative Learning\n  Activity?", "comments": "* The authors contributed equally to this work. This article a\n  preprint under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dialogue is successful when there is alignment between the speakers, at\ndifferent linguistic levels. In this work, we consider the dialogue occurring\nbetween interlocutors engaged in a collaborative learning task, and explore how\nperformance and learning (i.e. task success) relate to dialogue alignment\nprocesses. The main contribution of this work is to propose new measures to\nautomatically study alignment, to consider completely spontaneous spoken\ndialogues among children in the context of a collaborative learning activity.\nOur measures of alignment consider the children's use of expressions that are\nrelated to the task at hand, their follow-up actions of these expressions, and\nhow it links to task success. Focusing on expressions related to the task gives\nus insight into the way children use (potentially unfamiliar) terminology\nrelated to the task. A first finding of this work is the discovery that the\nmeasures we propose can capture elements of lexical alignment in such a\ncontext. Through these measures, we find that teams with bad performance often\naligned too late in the dialogue to achieve task success, and that they were\nlate to follow up each other's instructions with actions. We also found that\nwhile interlocutors do not exhibit hesitation phenomena (which we measure by\nlooking at fillers) in introducing expressions pertaining to the task, they do\nexhibit hesitation before accepting the expression, in the role of\nclarification. Lastly, we show that information management markers (measured by\nthe discourse marker 'oh') occur in the general vicinity of the follow up\nactions from (automatically) inferred instructions. However, good performers\ntend to have this marker closer to these actions. Our measures still reflect\nsome fine-grained aspects of learning in the dialogue, even if we cannot\nconclude that overall they are linked to the final measure of learning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:26:12 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Norman", "Utku", ""], ["Dinkar", "Tanvi", ""], ["Bruno", "Barbara", ""], ["Clavel", "Chlo\u00e9", ""]]}, {"id": "2104.04478", "submitter": "Alexandria LeClerc", "authors": "Glencora Borradaile, Kelsy Kretschmer, Michele Gretes and Alexandria\n  LeClerc", "title": "The Motivated Can Encrypt (Even with PGP)", "comments": "To be published in: Proceedings of the 21st Privacy Enhancing\n  Technology Symposium (PoPETS), Issue 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing end-to-end-encrypted (E2EE) email systems, mainly PGP, have long\nbeen evaluated in controlled lab settings. While these studies have exposed\nusability obstacles for the average user and offer design improvements, there\nexist users with an immediate need for private communication, who must cope\nwith existing software and its limitations. We seek to understand whether\nindividuals motivated by concrete privacy threats, such as those vulnerable to\nstate surveillance, can overcome usability issues to adopt complex E2EE tools\nfor long-term use. We surveyed regional activists, as surveillance of social\nmovements is well-documented. Our study group includes individuals from 9\nsocial movement groups in the US who had elected to participate in a workshop\non using Thunderbird+Enigmail for email encryption. These workshops tool place\nprior to mid-2017, via a partnership with a non-profit which supports social\nmovement groups. Six to 40 months after their PGP email encryption training,\nmore than half of the study participants were continuing to use PGP email\nencryption despite intervening widespread deployment of simple E2EE messaging\napps such as Signal. We study the interplay of usability with social factors\nsuch as motivation and the risks that individuals undertake through their\nactivism. We find that while usability is an important factor, it is not enough\nto explain long term use. For example, we find that riskiness of one's activism\nis negatively correlated with long-term PGP use. This study represents the\nfirst long-term study, and the first in-the-wild study, of PGP email encryption\nadoption.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:53:02 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Borradaile", "Glencora", ""], ["Kretschmer", "Kelsy", ""], ["Gretes", "Michele", ""], ["LeClerc", "Alexandria", ""]]}, {"id": "2104.04501", "submitter": "Abhishek Kaushik Mr.", "authors": "Abhishek Kaushik and Gareth J. F. Jones", "title": "Exploring Current User Web Search Behaviours in Analysis Tasks to be\n  Supported in Conversational Search", "comments": "Accepted in SIGIR 2018 Second International Workshop on\n  Conversational Approaches to Information Retrieval (CAIR 18), July 12, 2018,\n  Ann Arbor Michigan, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search presents opportunities to support users in their search\nactivities to improve the effectiveness and efficiency of search while reducing\ntheir cognitive load. Limitations of the potential competency of conversational\nagents restrict the situations for which conversational search agents can\nreplace human intermediaries. It is thus more interesting, initially at least,\nto investigate opportunities for conversational interaction to support less\ncomplex information retrieval tasks, such as typical web search, which do not\nrequire human-level intelligence in the conversational agent. In order to move\ntowards the development of a system to enable conversational search of this\ntype, we need to understand their required capabilities. To progress our\nunderstanding of these, we report a study examining the behaviour of users when\nusing a standard web search engine, designed to enable us to identify\nopportunities to support their search activities using a conversational agent.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:38:03 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Kaushik", "Abhishek", ""], ["Jones", "Gareth J. F.", ""]]}, {"id": "2104.04584", "submitter": "Swakkhar Shatabda", "authors": "Md. Mahinur Rashid, Hasin Kawsar Jahan, Annysha Huzzat, Riyasaat Ahmed\n  Rahul, Tamim Bin Zakir, Farhana Meem, Md. Saddam Hossain Mukta and Swakkhar\n  Shatabda", "title": "Text2Chart: A Multi-Staged Chart Generator from Natural Language Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generation of scientific visualization from analytical natural language text\nis a challenging task. In this paper, we propose Text2Chart, a multi-staged\nchart generator method. Text2Chart takes natural language text as input and\nproduce visualization as two-dimensional charts. Text2Chart approaches the\nproblem in three stages. Firstly, it identifies the axis elements of a chart\nfrom the given text known as x and y entities. Then it finds a mapping of\nx-entities with its corresponding y-entities. Next, it generates a chart type\nsuitable for the given text: bar, line or pie. Combination of these three\nstages is capable of generating visualization from the given analytical text.\nWe have also constructed a dataset for this problem. Experiments show that\nText2Chart achieves best performances with BERT based encodings with LSTM\nmodels in the first stage to label x and y entities, Random Forest classifier\nfor the mapping stage and fastText embedding with LSTM for the chart type\nprediction. In our experiments, all the stages show satisfactory results and\neffectiveness considering formation of charts from analytical text, achieving a\ncommendable overall performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 19:42:24 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Rashid", "Md. Mahinur", ""], ["Jahan", "Hasin Kawsar", ""], ["Huzzat", "Annysha", ""], ["Rahul", "Riyasaat Ahmed", ""], ["Zakir", "Tamim Bin", ""], ["Meem", "Farhana", ""], ["Mukta", "Md. Saddam Hossain", ""], ["Shatabda", "Swakkhar", ""]]}, {"id": "2104.04685", "submitter": "Andreea Danielescu", "authors": "Andreea Danielescu, David Piorkowski", "title": "Iterative Design of Gestures during Elicitation: A Gateway into User's\n  Mental Models", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The design of gestural interfaces through gesture elicitation studies is\ninfluenced by legacy bias, wherein users will draw from touch and desktop\ninteractions when specifying gestures for novel interactions, such as full-body\nand free-space applications. This may inhibit users from developing gestures\nthat are more appropriate and less fatiguing for the interaction. Increasing\nproduction during elicitation studies has shown promising results in moving\nusers beyond legacy biased gestures. However, when increasing production, users\nbegin to iterate on their gestures. In this paper, we present a gesture\nelicitation methodology aimed at understanding design iteration in studies that\nuse increased production. We show that gestural refinements provide insight\ninto the gestural features that matter for users to assign semantic meaning and\ndiscuss implications for training gesture classifiers. We also discuss a\npotential relationship between refined features and fatigue, showing that\nrefinements may map to less fatiguing gestures.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 05:26:43 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Danielescu", "Andreea", ""], ["Piorkowski", "David", ""]]}, {"id": "2104.04689", "submitter": "Zhi Chen", "authors": "Zhi Chen, Lu Chen, Yanbin Zhao, Ruisheng Cao, Zihan Xu, Su Zhu and Kai\n  Yu", "title": "ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser", "comments": "Accepted at NAACL2021, 11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a database schema, Text-to-SQL aims to translate a natural language\nquestion into the corresponding SQL query. Under the setup of cross-domain,\ntraditional semantic parsing models struggle to adapt to unseen database\nschemas. To improve the model generalization capability for rare and unseen\nschemas, we propose a new architecture, ShadowGNN, which processes schemas at\nabstract and semantic levels. By ignoring names of semantic items in databases,\nabstract schemas are exploited in a well-designed graph projection neural\nnetwork to obtain delexicalized representation of question and schema. Based on\nthe domain-independent representations, a relation-aware transformer is\nutilized to further extract logical linking between question and schema.\nFinally, a SQL decoder with context-free grammar is applied. On the challenging\nText-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms\nstate-of-the-art models. When the annotated data is extremely limited (only\n10\\% training set), ShadowGNN gets over absolute 5\\% performance gain, which\nshows its powerful generalization ability. Our implementation will be\nopen-sourced at \\url{https://github.com/WowCZ/shadowgnn}.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 05:48:28 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 07:06:55 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Chen", "Zhi", ""], ["Chen", "Lu", ""], ["Zhao", "Yanbin", ""], ["Cao", "Ruisheng", ""], ["Xu", "Zihan", ""], ["Zhu", "Su", ""], ["Yu", "Kai", ""]]}, {"id": "2104.04842", "submitter": "Xu Han", "authors": "Xu Han, Michelle Zhou, Matthew Turner, Tom Yeh", "title": "Designing Effective Interview Chatbots: Automatic Chatbot Profiling and\n  Design Suggestion Generation for Chatbot Debugging", "comments": "16 pages, 5 figures, accepted paper to CHI 2021 conference", "journal-ref": null, "doi": "10.1145/3411764.3445569", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies show the effectiveness of interview chatbots for information\nelicitation. However, designing an effective interview chatbot is non-trivial.\nFew tools exist to help designers design, evaluate, and improve an interview\nchatbot iteratively. Based on a formative study and literature reviews, we\npropose a computational framework for quantifying the performance of interview\nchatbots. Incorporating the framework, we have developed iChatProfile, an\nassistive chatbot design tool that can automatically generate a profile of an\ninterview chatbot with quantified performance metrics and offer design\nsuggestions for improving the chatbot based on such metrics. To validate the\neffectiveness of iChatProfile, we designed and conducted a between-subject\nstudy that compared the performance of 10 interview chatbots designed with or\nwithout using iChatProfile. Based on the live chats between the 10 chatbots and\n1349 users, our results show that iChatProfile helped the designers build\nsignificantly more effective interview chatbots, improving both interview\nquality and user experience.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 19:13:00 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Han", "Xu", ""], ["Zhou", "Michelle", ""], ["Turner", "Matthew", ""], ["Yeh", "Tom", ""]]}, {"id": "2104.04846", "submitter": "Marc Erich Latoschik", "authors": "Marc Erich Latoschik and Carolin Wienrich", "title": "Coherence and Plausibility, not Presence?! Pivotal Conditions for XR\n  Experiences and Effects, a Novel Model", "comments": "10 pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Presence often is considered the most important quale describing the\nsubjective feeling of being in a computer-generated (virtual) or\ncomputer-mediated environment. The identification and separation of two\northogonal presence components, i.e., the place illusion and the plausibility\nillusion, has been an accepted theoretical model describing Virtual Reality\n(VR) experiences for some time. In this model, immersion is a proposed\ncontributing factor to the place illusion. Lately, copresence and social\npresence illusions have extended this model, and coherence was proposed as a\ncontributing factor to the plausibility illusion. Such factors strive to\nidentify (objectively) measurable characteristics of an experience, e.g.,\nsystems properties that allow controlled manipulations of VR experiences. This\nperspective article challenges this presence-oriented VR theory. First, we\nargue that a place illusion cannot be the major construct to describe the much\nwider scope of Virtual, Augmented, and Mixed Reality (VR, AR, MR: or XR for\nshort). Second, we argue that there is no plausibility illusion but merely\nplausibility, and we derive the place illusion as a consequence of a plausible\ngeneration of spatial cues, and similarly for all of the current model's\nso-defined illusions. Finally, we propose coherence and plausibility to become\nthe central essential conditions in a novel theoretical model describing XR\nexperiences and effects.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 19:25:17 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 06:51:11 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 14:59:38 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Latoschik", "Marc Erich", ""], ["Wienrich", "Carolin", ""]]}, {"id": "2104.04862", "submitter": "Reza Hadi Mogavi", "authors": "Reza Hadi Mogavi, Yankun Zhao, Ehsan Ul Haq, Pan Hui, Xiaojuan Ma", "title": "Student Barriers to Active Learning in Synchronous Online Classes:\n  Characterization, Reflections, and Suggestions", "comments": "Accepted in the Proceedings of the Eighth ACM Conference on Learning\n  @ Scale (L@S '21)", "journal-ref": null, "doi": "10.1145/3430895.3460126", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more and more face-to-face classes move to online environments, it becomes\nincreasingly important to explore any emerging barriers to students' learning.\nThis work focuses on characterizing student barriers to active learning in\nsynchronous online environments. The aim is to help novice educators develop a\nbetter understanding of those barriers and prepare more student-centered course\nplans for their active online classes. Towards this end, we adopt a qualitative\nresearch approach and study information from different sources: social media\ncontent, interviews, and surveys from students and expert educators. Through a\nthematic analysis, we craft a nuanced list of students' online active learning\nbarriers within the themes of human-side, technological, and environmental\nbarriers. Each barrier is explored from the three aspects of frequency,\nimportance, and exclusiveness to active online classes. Finally, we conduct a\nsummative study with 12 novice educators and explain the benefits of using our\nbarrier list for course planning in active online classes.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 21:03:15 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mogavi", "Reza Hadi", ""], ["Zhao", "Yankun", ""], ["Haq", "Ehsan Ul", ""], ["Hui", "Pan", ""], ["Ma", "Xiaojuan", ""]]}, {"id": "2104.04912", "submitter": "Swaroop Panda", "authors": "Swaroop Panda, Shatarupa Thakurta Roy", "title": "Visualization Improvisation", "comments": "This paper has been peer-reviewed and accepted to VisActivities: IEEE\n  VIS Workshop on Data Vis Activities held in conjunction with IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Teaching visualization design involve making students familiar and make them\nwork with visualization models, framework and perspectives. Visualization\nresearch accommodates a plethora of perspectives emerging from researchers of\nvaried backgrounds. These diverse range of perspectives give rise to multiples\nmodels, frameworks and perspectives to teach visualization design. In this\npaper, we look at an approach to visualization teaching by using\nimprovisational techniques. The basic idea is to design a visualization without\nusing an existing predefined model. Since improvisation, by definition, is not\na model or a framework, this work presents a reflection on how improvisation\ncan be a way of teaching visualization design.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 04:21:54 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Panda", "Swaroop", ""], ["Roy", "Shatarupa Thakurta", ""]]}, {"id": "2104.04922", "submitter": "Swaroop Panda", "authors": "Swaroop Panda, Shatarupa Thakurta Roy", "title": "A Preliminary Model for the Design of Music Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music Visualization is basically the transformation of data from the aural to\nthe visual space. There are a variety of music visualizations, across\napplications, present on the web. Models of Visualization include conceptual\nframeworks helpful for designing, understanding and making sense of\nvisualizations. In this paper, we propose a preliminary model for Music\nVisualization. We build the model by using two conceptual pivots, Visualization\nStimulus and Data Property. To demonstrate the utility of the model we\ndeconstruct and design visualizations with toy examples using the model and\nfinally conclude by proposing further applications of and future work on our\nproposed model.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 05:41:48 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Panda", "Swaroop", ""], ["Roy", "Shatarupa Thakurta", ""]]}, {"id": "2104.05030", "submitter": "Divine Maloney", "authors": "Divine Maloney, Guo Freeman, Andrew Robb", "title": "Social Virtual Reality: Ethical Considerations and Future Directions for\n  An Emerging Research Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The boom of commercial social virtual reality (VR) platforms in recent years\nhas signaled the growth and wide-spread adoption of consumer VR. Social VR\nplatforms draw aspects from traditional 2D virtual worlds where users engage in\nvarious immersive experiences, interactive activities, and choices in\navatar-based representation. However, social VR also demonstrates specific\nnuances that extend traditional 2D virtual worlds and other online social\nspaces, such as full/partial body tracked avatars, experiencing mundane\neveryday activities in a new way (e.g., sleeping), and an immersive means to\nexplore new and complex identities. The growing popularity has signaled\ninterest and investment from top technology companies who each have their own\nsocial VR platforms. Thus far, social VR has become an emerging research space,\nmainly focusing on design strategies, communication and interaction modalities,\nnuanced activities, self-presentation, harassment, privacy, and\nself-disclosure. These recent works suggest that many questions still remain in\nsocial VR scholarship regarding how to ethically conduct research on these\nsites and which research areas require additional attention. Therefore, in this\npaper, we provide an overview of modern Social VR, critically review current\nscholarship in the area, raise ethical considerations for conducting research\non these sites, and highlight unexplored areas.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 15:34:59 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Maloney", "Divine", ""], ["Freeman", "Guo", ""], ["Robb", "Andrew", ""]]}, {"id": "2104.05340", "submitter": "Ella Velner", "authors": "Ella Velner, Khiet P. Truong, Vanessa Evers", "title": "Speaking of Trust -- Speech as a Measure of Trust", "comments": "in TRAITS Workshop Proceedings (arXiv:2103.12679) held in conjunction\n  with Companion of the 2021 ACM/IEEE International Conference on Human-Robot\n  Interaction, March 2021, Pages 709-711", "journal-ref": null, "doi": null, "report-no": "TRAITS/2021/13", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Since trust measures in human-robot interaction are often subjective or not\npossible to implement real-time, we propose to use speech cues (on what, when\nand how the user talks) as an objective real-time measure of trust. This could\nbe implemented in the robot to calibrate towards appropriate trust. However, we\nwould like to open the discussion on how to deal with the ethical implications\nsurrounding this trust measure.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 10:39:25 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Velner", "Ella", ""], ["Truong", "Khiet P.", ""], ["Evers", "Vanessa", ""]]}, {"id": "2104.05356", "submitter": "Anthony Steed", "authors": "Anthony Steed", "title": "What We Measure in Mixed Reality Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many potential measures that one might use when evaluating\nmixed-reality experiences. In this position paper I will argue that there are\nvarious stances to take for evaluation, depending on the framing of the\nexperience within a larger body of work. I will draw upon various types of work\nthat my team has been involved with in order to illustrate these different\nstances. I will then sketch out some directions for developing more robust\nmeasures that can help the field move forward.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:18:35 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Steed", "Anthony", ""]]}, {"id": "2104.05359", "submitter": "Anthony Steed", "authors": "Anthony Steed, Daniel Archer, Ben Congdon, Sebastian Friston, David\n  Swapp, Felix J. Thiel", "title": "Some Lessons Learned Running Virtual Reality Experiments Out of the\n  Laboratory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past twelve months, our team has had to move rapidly from conducting\nmost of our user experiments in a laboratory setting, to running experiments in\nthe wild away from the laboratory and without direct synchronous oversight from\nan experimenter. This has challenged us to think about what types of experiment\nwe can run, and to improve our tools and methods to allow us to reliably\ncapture the necessary data. It has also offered us an opportunity to engage\nwith a more diverse population than we would normally engage with in the\nlaboratory. In this position paper we elaborate on the challenges and\nopportunities, and give some lessons learned from our own experience.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:23:38 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Steed", "Anthony", ""], ["Archer", "Daniel", ""], ["Congdon", "Ben", ""], ["Friston", "Sebastian", ""], ["Swapp", "David", ""], ["Thiel", "Felix J.", ""]]}, {"id": "2104.05366", "submitter": "Anthony Steed", "authors": "Lisa Izzouzi, Anthony Steed", "title": "Effectiveness of Social Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of work in social virtual reality, including our own group's, has\nfocused on effectiveness of specific social behaviours such as eye-gaze, turn\ntaking, gestures and other verbal and non-verbal cues. We have built upon these\nto look at emergent phenomena such as co-presence, leadership and trust. These\ngive us good information about the usability issues of specific social VR\nsystems, but they don't give us much information about the requirements for\nsuch systems going forward. In this short paper we discuss how we are\nbroadening the scope of our work on social systems, to move out of the\nlaboratory to more ecologically valid situations and to study groups using\nsocial VR for longer periods of time.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:34:14 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Izzouzi", "Lisa", ""], ["Steed", "Anthony", ""]]}, {"id": "2104.05432", "submitter": "Nils Gumpfer", "authors": "Michael Guckert, Nils Gumpfer, Jennifer Hannig, Till Keller and Neil\n  Urquhart", "title": "A Conceptual Framework for Establishing Trust in Real World Intelligent\n  Systems", "comments": null, "journal-ref": null, "doi": "10.1016/j.cogsys.2021.04.001", "report-no": null, "categories": "cs.CY cs.AI cs.GT cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent information systems that contain emergent elements often\nencounter trust problems because results do not get sufficiently explained and\nthe procedure itself can not be fully retraced. This is caused by a control\nflow depending either on stochastic elements or on the structure and relevance\nof the input data. Trust in such algorithms can be established by letting users\ninteract with the system so that they can explore results and find patterns\nthat can be compared with their expected solution. Reflecting features and\npatterns of human understanding of a domain against algorithmic results can\ncreate awareness of such patterns and may increase the trust that a user has in\nthe solution. If expectations are not met, close inspection can be used to\ndecide whether a solution conforms to the expectations or whether it goes\nbeyond the expected. By either accepting or rejecting a solution, the user's\nset of expectations evolves and a learning process for the users is\nestablished. In this paper we present a conceptual framework that reflects and\nsupports this process. The framework is the result of an analysis of two\nexemplary case studies from two different disciplines with information systems\nthat assist experts in their complex tasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 12:58:47 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Guckert", "Michael", ""], ["Gumpfer", "Nils", ""], ["Hannig", "Jennifer", ""], ["Keller", "Till", ""], ["Urquhart", "Neil", ""]]}, {"id": "2104.05456", "submitter": "Marek \\v{S}uppa", "authors": "Marek \\v{S}uppa, Ondrej Jariabka, Adri\\'an Matejov, and Marek Nagy", "title": "TermAdventure: Interactively Teaching UNIX Command Line, Text Adventure\n  Style", "comments": "Accepted at ITiCSE 2021", "journal-ref": null, "doi": "10.1145/3430665.3456387", "report-no": null, "categories": "cs.CY cs.HC cs.OS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Introductory UNIX courses are typically organized as lectures, accompanied by\na set of exercises, whose solutions are submitted to and reviewed by the\nlecturers. While this arrangement has become standard practice, it often\nrequires the use of an external tool or interface for submission and does not\nautomatically check its correctness. That in turn leads to increased workload\nand makes it difficult to deal with potential plagiarism.\n  In this work we present TermAdventure (TA), a suite of tools for creating\ninteractive UNIX exercises. These resemble text adventure games, which immerse\nthe user in a text environment and let them interact with it using textual\ncommands. In our case the ''adventure'' takes place inside a UNIX system and\nthe user interaction happens via the standard UNIX command line. The adventure\nis a set of exercises, which are presented and automatically evaluated by the\nsystem, all from within the command line environment. The suite is released\nunder an open source license, has minimal dependencies and can be used either\non a UNIX-style server or a desktop computer running any major OS platform\nthrough Docker.\n  We also reflect on our experience of using the presented suite as the primary\nteaching tool for an introductory UNIX course for Data Scientists and discuss\nthe implications of its deployment in similar courses. The suite is released\nunder the terms of an open-source license at\n\\url{https://github.com/NaiveNeuron/TermAdventure}.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 13:23:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["\u0160uppa", "Marek", ""], ["Jariabka", "Ondrej", ""], ["Matejov", "Adri\u00e1n", ""], ["Nagy", "Marek", ""]]}, {"id": "2104.05470", "submitter": "Yuan Shen", "authors": "Yuan Shen and Niviru Wijayaratne and Katherine Driggs-Campbell", "title": "Building Mental Models through Preview of Autopilot Behaviors", "comments": "in TRAITS Workshop Proceedings (arXiv:2103.12679) held in conjunction\n  with Companion of the 2021 ACM/IEEE International Conference on Human-Robot\n  Interaction, March 2021, Pages 709-711", "journal-ref": null, "doi": null, "report-no": "TRAITS/2021/03", "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effective human-vehicle collaboration requires an appropriate un-derstanding\nof vehicle behavior for safety and trust. Improvingon our prior work by adding\na future prediction module, we in-troduce our framework, calledAutoPreview, to\nenable humans topreview autopilot behaviors prior to direct interaction with\nthevehicle. Previewing autopilot behavior can help to ensure\nsmoothhuman-vehicle collaboration during the initial exploration stagewith the\nvehicle. To demonstrate its practicality, we conducted acase study on\nhuman-vehicle collaboration and built a prototypeof our framework with the\nCARLA simulator. Additionally, weconducted a between-subject control experiment\n(n=10) to studywhether ourAutoPreviewframework can provide a deeper\nunder-standing of autopilot behavior compared to direct interaction. Ourresults\nsuggest that theAutoPreviewframework does, in fact, helpusers understand\nautopilot behavior and develop appropriate men-tal models\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 13:46:55 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Shen", "Yuan", ""], ["Wijayaratne", "Niviru", ""], ["Driggs-Campbell", "Katherine", ""]]}, {"id": "2104.05688", "submitter": "Vil\\'em Zouhar", "authors": "Vil\\'em Zouhar, Michal Nov\\'ak, Mat\\'u\\v{s} \\v{Z}ilinec, Ond\\v{r}ej\n  Bojar, Mateo Obreg\\'on, Robin L. Hill, Fr\\'ed\\'eric Blain, Marina Fomicheva,\n  Lucia Specia, Lisa Yankovskaya", "title": "Backtranslation Feedback Improves User Confidence in MT, Not Quality", "comments": "9 pages (excluding references); to appear at NAACL-HWT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Translating text into a language unknown to the text's author, dubbed\noutbound translation, is a modern need for which the user experience has\nsignificant room for improvement, beyond the basic machine translation\nfacility. We demonstrate this by showing three ways in which user confidence in\nthe outbound translation, as well as its overall final quality, can be\naffected: backward translation, quality estimation (with alignment) and source\nparaphrasing. In this paper, we describe an experiment on outbound translation\nfrom English to Czech and Estonian. We examine the effects of each proposed\nfeedback module and further focus on how the quality of machine translation\nsystems influence these findings and the user perception of success. We show\nthat backward translation feedback has a mixed effect on the whole process: it\nincreases user confidence in the produced translation, but not the objective\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:50:24 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zouhar", "Vil\u00e9m", ""], ["Nov\u00e1k", "Michal", ""], ["\u017dilinec", "Mat\u00fa\u0161", ""], ["Bojar", "Ond\u0159ej", ""], ["Obreg\u00f3n", "Mateo", ""], ["Hill", "Robin L.", ""], ["Blain", "Fr\u00e9d\u00e9ric", ""], ["Fomicheva", "Marina", ""], ["Specia", "Lucia", ""], ["Yankovskaya", "Lisa", ""]]}, {"id": "2104.05710", "submitter": "Bereket Abera Yilma Mr.", "authors": "Bereket Abera Yilma, Herv\\'e Panetto, Yannick Naudet", "title": "Systemic formalisation of Cyber-Physical-Social System (CPSS): A\n  systematic literature review", "comments": null, "journal-ref": "Computers in Industry, Volume 129, 2021, 103458, ISSN 0166-3615", "doi": "10.1016/j.compind.2021.103458", "report-no": "Volume 129", "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The notion of Cyber-Physical-Social System (CPSS) is an emerging concept\ndeveloped as a result of the need to understand the impact of Cyber-Physical\nSystems (CPS) on humans and vice versa. This paradigm shift from CPS to CPSS\nwas mainly attributed to the increasing use of sensor-enabled smart devices and\nthe tight link with the users. The concept of CPSS has been around for over a\ndecade and it has gained increasing attention over the past few years. The\nevolution to incorporate human aspects in the CPS research has unlocked a\nnumber of research challenges. Particularly human dynamics brings additional\ncomplexity that is yet to be explored. The exploration to conceptualise the\nnotion of CPSS has been partially addressed in few scientific literatures.\nAlthough its conceptualisation has always been use-case dependent. Thus, there\nis a lack of generic view as most works focus on specific domains. Furthermore,\nthe systemic core and design principles linking it with the theory of systems\nare loose. This work aims at addressing these issues by first exploring and\nanalysing scientific literature to understand the complete spectrum of CPSS\nthrough a Systematic Literature Review (SLR). Thereby identifying the\nstate-of-the-art perspectives on CPSS regarding definitions, underlining\nprinciples and application areas. Subsequently, based on the findings of the\nSLR, we propose a domain-independent definition and a meta-model for CPSS,\ngrounded in the Theory of Systems. Finally, a discussion on feasible future\nresearch directions is presented based on the systemic notion and the proposed\nmeta-models.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 22:31:57 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Yilma", "Bereket Abera", ""], ["Panetto", "Herv\u00e9", ""], ["Naudet", "Yannick", ""]]}, {"id": "2104.05742", "submitter": "Tarik A. Rashid", "authors": "Nitish Maharjan, Abeer Alsadoon, P.W.C. Prasad, Salma Abdullah, Tarik\n  A. Rashid", "title": "A Novel Visualization System of Using Augmented Reality in Knee\n  Replacement Surgery: Enhanced Bidirectional Maximum Correntropy Algorithm", "comments": "27 pages", "journal-ref": "The International Journal of Medical Robotics and Computer\n  Assisted Surgery, 2020", "doi": "10.1002/rcs.2154", "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background and aim: Image registration and alignment are the main limitations\nof augmented reality-based knee replacement surgery. This research aims to\ndecrease the registration error, eliminate outcomes that are trapped in local\nminima to improve the alignment problems, handle the occlusion, and maximize\nthe overlapping parts. Methodology: markerless image registration method was\nused for Augmented reality-based knee replacement surgery to guide and\nvisualize the surgical operation. While weight least square algorithm was used\nto enhance stereo camera-based tracking by filling border occlusion in right to\nleft direction and non-border occlusion from left to right direction. Results:\nThis study has improved video precision to 0.57 mm~0.61 mm alignment error.\nFurthermore, with the use of bidirectional points, for example, forwards and\nbackwards directional cloud point, the iteration on image registration was\ndecreased. This has led to improve the processing time as well. The processing\ntime of video frames was improved to 7.4~11.74 fps. Conclusions: It seems clear\nthat this proposed system has focused on overcoming the misalignment difficulty\ncaused by movement of patient and enhancing the AR visualization during knee\nreplacement surgery. The proposed system was reliable and favorable which helps\nin eliminating alignment error by ascertaining the optimal rigid transformation\nbetween two cloud points and removing the outliers and non-Gaussian noise. The\nproposed augmented reality system helps in accurate visualization and\nnavigation of anatomy of knee such as femur, tibia, cartilage, blood vessels,\netc.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 19:18:16 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Maharjan", "Nitish", ""], ["Alsadoon", "Abeer", ""], ["Prasad", "P. W. C.", ""], ["Abdullah", "Salma", ""], ["Rashid", "Tarik A.", ""]]}, {"id": "2104.05809", "submitter": "Cristian Consonni", "authors": "Cristian Consonni, Silvia Basile, Matteo Manca, Ludovico Boratto,\n  Andr\\'e Freitas, Tatiana Kovacikova, Ghadir Pourhashem, Yannick Cornet", "title": "What's Your Value of Travel Time? Collecting Traveler-Centered Mobility\n  Data via Crowdsourcing", "comments": "10 pages, 1 figure, LaTeX. Final camera-ready version accepted at the\n  15th International AAAI Conference on Web and Social Media (ICWSM 2021) -\n  7-11 June 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Mobility and transport, by their nature, involve crowds and require the\ncoordination of multiple stakeholders - such as policy-makers, planners,\ntransport operators, and the travelers themselves. However, traditional\napproaches have been focused on time savings, proposing to users solutions that\ninclude the shortest or fastest paths. We argue that this approach towards\ntravel time value is not centered on a traveler's perspective. To date, very\nfew works have mined data from crowds of travelers to test the efficacy and\nefficiency of novel mobility paradigms. In this paper, we build upon a\ndifferent paradigm of worthwhile time in which travelers can use their travel\ntime for other activities; we present a new dataset, which contains data about\ntravelers and their journeys, collected from a dedicated mobile application.\nEach trip contains multi-faceted information: from the transport mode, through\nits evaluation, to the positive and negative experience factors. To showcase\nthis new dataset's potential, we also present a use case, which compares\ncorresponding trip legs with different transport modes, studying experience\nfactors that negatively impact users using cycling and public transport as\nalternatives to cars. We conclude by discussing other application domains and\nresearch opportunities enabled by the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 20:48:28 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Consonni", "Cristian", ""], ["Basile", "Silvia", ""], ["Manca", "Matteo", ""], ["Boratto", "Ludovico", ""], ["Freitas", "Andr\u00e9", ""], ["Kovacikova", "Tatiana", ""], ["Pourhashem", "Ghadir", ""], ["Cornet", "Yannick", ""]]}, {"id": "2104.05843", "submitter": "Rohit Khurana", "authors": "Rohit Khurana, Aurel Coza", "title": "Remote Recording of Emotional and Activity Data: A Methodological Study", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The impact of physical exercise on emotional well-being is one of the most\nimportant factors that drive sustained physical activity engagement. It is also\none of the least studied topics on account of the rather elaborated setups\nrequired to quantify emotional expression during exercise. The wide adoption of\nat-home, physical exercise solutions has compounded this problem due to the\nsecluded nature of these activities. We propose here a new methodology that\nwould allow for mass emotional expression and physical activity data gathering\nusing publicly available sources such as at-home exercise videos. We have shown\nthat the methodology is robust enough to extract high resolution, reliable data\nfrom home videos posted on popular video share sites. The source-code and\ninstructions for practical use are published online such that researchers can\naccess data pertinent to emotional response during exertion in real world\nsituations with applicability for prospective and retrospective studies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 22:08:48 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Khurana", "Rohit", ""], ["Coza", "Aurel", ""]]}, {"id": "2104.05979", "submitter": "Rachael Zehrung", "authors": "Rachael Zehrung, Lily Huang, Bongshin Lee, Eun Kyoung Choe", "title": "Investigating Opportunities to Support Kids' Agency and Well-being: A\n  Review of Kids' Wearables", "comments": "20 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wearable devices hold great potential for promoting children's health and\nwell-being. However, research on kids' wearables is sparse and often focuses on\ntheir use in the context of parental surveillance. To gain insight into the\ncurrent landscape of kids' wearables, we surveyed 47 wearable devices marketed\nfor children. We collected rich data on the functionality of these devices and\nassessed how different features satisfy parents' information needs, and\nidentified opportunities for wearables to support children's needs and\ninterests. We found that many kids' wearables are technologically sophisticated\ndevices that focus on parents' ability to communicate with their children and\nkeep them safe, as well as encourage physical activity and nurture good habits.\nWe discuss how our findings could inform the design of wearables that serve as\nmore than monitoring devices, and instead support children and parents as equal\nstakeholders, providing implications for kids' agency, long-term development,\nand overall well-being. Finally, we identify future research efforts related to\ndesigning for kids' self-tracking and collaborative tracking with parents.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 07:21:12 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zehrung", "Rachael", ""], ["Huang", "Lily", ""], ["Lee", "Bongshin", ""], ["Choe", "Eun Kyoung", ""]]}, {"id": "2104.06059", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee and Peter G\\\"ardenfors and Magnus Johnsson", "title": "First and Second Order Dynamics in a Hierarchical SOM system for Action\n  Recognition", "comments": null, "journal-ref": null, "doi": "10.1016/j.asoc.2017.06.007", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human recognition of the actions of other humans is very efficient and is\nbased on patterns of movements. Our theoretical starting point is that the\ndynamics of the joint movements is important to action categorization. On the\nbasis of this theory, we present a novel action recognition system that employs\na hierarchy of Self-Organizing Maps together with a custom supervised neural\nnetwork that learns to categorize actions. The system preprocesses the input\nfrom a Kinect like 3D camera to exploit the information not only about joint\npositions, but also their first and second order dynamics. We evaluate our\nsystem in two experiments with publicly available data sets, and compare its\nperformance to the performance with less sophisticated preprocessing of the\ninput. The results show that including the dynamics of the actions improves the\nperformance. We also apply an attention mechanism that focuses on the parts of\nthe body that are the most involved in performing the actions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:46:40 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gharaee", "Zahra", ""], ["G\u00e4rdenfors", "Peter", ""], ["Johnsson", "Magnus", ""]]}, {"id": "2104.06070", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee and Peter G\\\"ardenfors and Magnus Johnsson", "title": "Online Recognition of Actions Involving Objects", "comments": null, "journal-ref": null, "doi": "10.1016/j.bica.2017.09.007", "report-no": null, "categories": "cs.RO cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an online system for real time recognition of actions involving\nobjects working in online mode. The system merges two streams of information\nprocessing running in parallel. One is carried out by a hierarchical\nself-organizing map (SOM) system that recognizes the performed actions by\nanalysing the spatial trajectories of the agent's movements. It consists of two\nlayers of SOMs and a custom made supervised neural network. The activation\nsequences in the first layer SOM represent the sequences of significant\npostures of the agent during the performance of actions. These activation\nsequences are subsequently recoded and clustered in the second layer SOM, and\nthen labeled by the activity in the third layer custom made supervised neural\nnetwork. The second information processing stream is carried out by a second\nsystem that determines which object among several in the agent's vicinity the\naction is applied to. This is achieved by applying a proximity measure. The\npresented method combines the two information processing streams to determine\nwhat action the agent performed and on what object. The action recognition\nsystem has been tested with excellent performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 10:08:20 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gharaee", "Zahra", ""], ["G\u00e4rdenfors", "Peter", ""], ["Johnsson", "Magnus", ""]]}, {"id": "2104.06080", "submitter": "Huang Yanpei", "authors": "Yanpei Huang, Jonathan Eden, Ekaterina Ivanova, Soo Jay Phee and\n  Etienne Burdet", "title": "Trimanipulation: Evaluation of human performance in a 3-handed\n  coordination task", "comments": "5 figures, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many teleoperation tasks require three or more tools working together, which\nneed the cooperation of multiple operators. The effectiveness of such schemes\nmay be limited by communication. Trimanipulation by a single operator using an\nartificial third arm controlled together with their natural arms is a promising\nsolution to this issue. Foot-controlled interfaces have previously shown the\ncapability to be used for the continuous control of robot arms. However, the\nuse of such interfaces for controlling a supernumerary robotic limb (SRLs) in\ncoordination with the natural limbs, is not well understood. In this paper, a\nteleoperation task imitating physically coupled hands in a virtual reality\nscene was conducted with 14 subjects to evaluate human performance during\ntri-manipulation. The participants were required to move three limbs together\nin a coordinated way mimicking three arms holding a shared physical object. It\nwas found that after a short practice session, the three-hand tri-manipulation\nusing a single subject's hands and foot was still slower than dyad operation,\nhowever, they displayed similar performance in success rate and higher motion\nefficiency than two person's cooperation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 10:25:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Huang", "Yanpei", ""], ["Eden", "Jonathan", ""], ["Ivanova", "Ekaterina", ""], ["Phee", "Soo Jay", ""], ["Burdet", "Etienne", ""]]}, {"id": "2104.06127", "submitter": "Maartje De Graaf", "authors": "Sven Y. Neuteboom and Maartje M.A. de Graaf", "title": "Cobbler Stick With Your Reads: People's Perceptions of Gendered Robots\n  Performing Gender Stereotypical Tasks", "comments": "in TRAITS Workshop Proceedings (arXiv:2103.12679) held in conjunction\n  with Companion of the 2021 ACM/IEEE International Conference on Human-Robot\n  Interaction, March 2021, Pages 709-711", "journal-ref": null, "doi": null, "report-no": "TRAITS/2021/01", "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Previous research found that robots should best be designed to fit their\ngiven task, whilst others identified gender effects in people's evaluations of\nrobots. This study combines this knowledge to investigate stereotyping effects\nof robot genderedness and assigned tasks in an online experiment (n = 89)\nmanipulating robot gender (male vs. female) and task type (analytical vs.\nsocial) in a between subject's design in terms of trust, social perception, and\nhumanness. People deem robots more competent and have higher trust in their\ncapacity when they perform analytical tasks compared to social tasks,\nindependent of the robot's gender. Furthermore, we observed a trend in the data\nindicating that people seem to dehumanize female robots (regardless of task\nperformed) to animals lacking higher-level mental processes, and additionally\nthat people seem to dehumanize robots to emotionless objects only when gendered\nrobots perform tasks contradicting the stereotypes of their gender.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 12:00:17 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Neuteboom", "Sven Y.", ""], ["de Graaf", "Maartje M. A.", ""]]}, {"id": "2104.06221", "submitter": "Tobias Drey", "authors": "Tobias Drey, Michael Rietzler, Enrico Rukzio", "title": "Questionnaires and Qualitative Feedback Methods to Measure User\n  Experience in Mixed Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the user experience of a software system is an essential final\nstep of every research. Several concepts such as flow, affective state,\npresences, or immersion exist to measure user experience. Typical measurement\ntechniques analyze physiological data, gameplay data, and questionnaires.\nQualitative feedback methods are another approach to collect detailed user\ninsights. In this position paper, we will discuss how we used questionnaires\nand qualitative feedback methods in previous mixed reality work to measure user\nexperience. We will present several measurement examples, discuss their current\nlimitations, and provide guideline propositions to support comparable mixed\nreality user experience research in the future.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:15:16 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Drey", "Tobias", ""], ["Rietzler", "Michael", ""], ["Rukzio", "Enrico", ""]]}, {"id": "2104.06512", "submitter": "Susannah Kate Devitt", "authors": "S.K. Devitt, R. Horne, Z. Assaad, E. Broad, H. Kurniawati, B. Cardier,\n  A. Scott, S. Lazar, M. Gould, C. Adamson, C. Karl, F. Schrever, S. Keay, K.\n  Tranter, E. Shellshear, D. Hunter, M. Brady, T. Putland", "title": "Trust and Safety", "comments": "Approved for publication as a chapter in 'Robotics Roadmap for\n  Australia V2' by Robotics Australia Network [forthcoming]. 31 pages, 10\n  Figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Robotics in Australia have a long history of conforming with safety standards\nand risk managed practices. This chapter articulates the current state of trust\nand safety in robotics including society's expectations, safety management\nsystems and system safety as well as emerging issues and methods for ensuring\nsafety in increasingly autonomous robotics. The future of trust and safety will\ncombine standards with iterative, adaptive and responsive regulatory and\nassurance methods for diverse applications of robotics, autonomous systems and\nartificial intelligence (RAS-AI). Robotics will need novel technical and social\napproaches to achieve assurance, particularly for game-changing innovations.\nThe ability for users to easily update algorithms and software, which alters\nthe performance of a system, implies that traditional machine assurance\nperformed prior to deployment or sale, will no longer be viable. Moreover, the\nhigh frequency of updates implies that traditional certification that requires\nsubstantial time will no longer be practical. To alleviate these difficulties,\nautomation of assurance will likely be needed; something like\n'ASsurance-as-a-Service' (ASaaS), where APIs constantly ping RAS-AI to ensure\nabidance with various rules, frameworks and behavioural expectations. There are\nexceptions to this, such as in contested or communications denied environments,\nor in underground or undersea mining; and these systems need their own risk\nassessments and limitations imposed. Indeed, self-monitors are already\noperating within some systems. To ensure safe operations of future robotics\nsystems, Australia needs to invest in RAS-AI assurance research, stakeholder\nengagement and continued development and refinement of robust frameworks,\nmethods, guidelines and policy in order to educate and prepare its technology\ndevelopers, certifiers, and general population.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 20:53:30 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Devitt", "S. K.", ""], ["Horne", "R.", ""], ["Assaad", "Z.", ""], ["Broad", "E.", ""], ["Kurniawati", "H.", ""], ["Cardier", "B.", ""], ["Scott", "A.", ""], ["Lazar", "S.", ""], ["Gould", "M.", ""], ["Adamson", "C.", ""], ["Karl", "C.", ""], ["Schrever", "F.", ""], ["Keay", "S.", ""], ["Tranter", "K.", ""], ["Shellshear", "E.", ""], ["Hunter", "D.", ""], ["Brady", "M.", ""], ["Putland", "T.", ""]]}, {"id": "2104.06536", "submitter": "Jennifer Healey", "authors": "Duotun Wang (1), Jennifer Healey (2), Jing Qian (3), Curtis Wigington\n  (2), Tong Sun (2) and Huaishu Peng (1) ((1) University of Maryland, (2)\n  Adobe, (3) Brown University)", "title": "Lets Make A Story Measuring MR Child Engagement", "comments": "3 pages, 3 figures, to be presented in the CHI Workshop on Evaluating\n  User Experiences in Mixed Reality, see\n  \"https://sputze.github.io/evaluating-mr/\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the result of a pilot study measuring child engagement with the\nLets Make A Story system, a novel mixed reality, MR, collaborative storytelling\nsystem designed for grandparents and grandchildren. We compare our MR\nexperience against an equivalent paper story experience. The goal of our pilot\nwas to test the system with actual child users and assess the goodness of using\nmetrics of time, user generated story content and facial expression analysis as\nmetrics of child engagement. We find that multiple confounding variables make\nthese metrics problematic including attribution of engagement time, spontaneous\nnon-story related conversation and having the childs full forward face\ncontinuously in view during the story. We present our platform and experiences\nand our finding that the strongest metric was user comments in the\npost-experiential interview.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 22:36:50 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Wang", "Duotun", ""], ["Healey", "Jennifer", ""], ["Qian", "Jing", ""], ["Wigington", "Curtis", ""], ["Sun", "Tong", ""], ["Peng", "Huaishu", ""]]}, {"id": "2104.06552", "submitter": "Jennifer Healey", "authors": "Victor S. Bursztyn (1), Jennifer Healey (2), Eunyee Koh (2), Nedim\n  Lipka (2), Larry Birnbaum (1) ((1) Northwestern University, (2) Adobe)", "title": "Developing a Conversational Recommendation System for Navigating Limited\n  Options", "comments": "7 pages, 4 figures, to appear in CHI 2021 as a Late Breaking Work,\n  see \"https://chi2021.acm.org/\"", "journal-ref": null, "doi": "10.1145/3411763.3451596", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a conversational recommendation system designed to help\nusers navigate through a set of limited options to find the best choice. Unlike\nmany internet scale systems that use a singular set of search terms and return\na ranked list of options from amongst thousands, our system uses multi-turn\nuser dialog to deeply understand the users preferences. The system responds in\ncontext to the users specific and immediate feedback to make sequential\nrecommendations. We envision our system would be highly useful in situations\nwith intrinsic constraints, such as finding the right restaurant within walking\ndistance or the right retail item within a limited inventory. Our research\nprototype instantiates the former use case, leveraging real data from Google\nPlaces, Yelp, and Zomato. We evaluated our system against a similar system that\ndid not incorporate user feedback in a 16 person remote study, generating 64\nscenario-based search journeys. When our recommendation system was successfully\ntriggered, we saw both an increase in efficiency and a higher confidence rating\nwith respect to final user choice. We also found that users preferred our\nsystem (75%) compared with the baseline.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 23:46:10 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bursztyn", "Victor S.", "", "Northwestern University"], ["Healey", "Jennifer", "", "Adobe"], ["Koh", "Eunyee", "", "Adobe"], ["Lipka", "Nedim", "", "Adobe"], ["Birnbaum", "Larry", "", "Northwestern University"]]}, {"id": "2104.06556", "submitter": "Matthew Zurek", "authors": "Matthew Zurek, Andreea Bobu, Daniel S. Brown, Anca D. Dragan", "title": "Situational Confidence Assistance for Lifelong Shared Autonomy", "comments": "In proceedings ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared autonomy enables robots to infer user intent and assist in\naccomplishing it. But when the user wants to do a new task that the robot does\nnot know about, shared autonomy will hinder their performance by attempting to\nassist them with something that is not their intent. Our key idea is that the\nrobot can detect when its repertoire of intents is insufficient to explain the\nuser's input, and give them back control. This then enables the robot to\nobserve unhindered task execution, learn the new intent behind it, and add it\nto this repertoire. We demonstrate with both a case study and a user study that\nour proposed method maintains good performance when the human's intent is in\nthe robot's repertoire, outperforms prior shared autonomy approaches when it\nisn't, and successfully learns new skills, enabling efficient lifelong learning\nfor confidence-based shared autonomy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 00:00:07 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zurek", "Matthew", ""], ["Bobu", "Andreea", ""], ["Brown", "Daniel S.", ""], ["Dragan", "Anca D.", ""]]}, {"id": "2104.06603", "submitter": "Jennifer Healey", "authors": "Namrata Srivastava (1,2,3), Rajiv Jain (2), Jennifer Healey (2), Zoya\n  Bylinskii (2) and Tilman Dingler (1) ((1) University of Melbourne (2) Adobe\n  (3) Monash University)", "title": "Mitigating the Effects of Reading Interruptions by Providing Reviews and\n  Previews", "comments": "6 pages, 4 figures, to appear as a Late Breaking Work in CHI 2021,\n  see \"https://chi2021.acm.org/\"", "journal-ref": null, "doi": "10.1145/3411763.3451610", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As reading on mobile devices is becoming more ubiquitous, content is consumed\nin shorter intervals and is punctuated by frequent interruptions. In this work,\nwe explore the best way to mitigate the effects of reading interruptions on\nlonger text passages. Our hypothesis is that short summaries of either\npreviously read content (reviews) or upcoming content (previews) will help the\nreader re-engage with the reading task. Our target use case is for students who\nstudy using electronic textbooks and who are frequently mobile. We present a\nseries of pilot studies that examine the benefits of different types of\nsummaries and their locations, with respect to variations in text content and\nparticipant cohorts. We find that users prefer reviews after an interruption,\nbut that previews shown after interruptions have a larger positive influence on\ncomprehension. Our work is a first step towards smart reading applications that\nproactively provide text summaries to mitigate interruptions on the go.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 03:04:39 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Srivastava", "Namrata", ""], ["Jain", "Rajiv", ""], ["Healey", "Jennifer", ""], ["Bylinskii", "Zoya", ""], ["Dingler", "Tilman", ""]]}, {"id": "2104.06861", "submitter": "Michael Toth", "authors": "Cristiana Santos, Midas Nouwens, Michael Toth, Nataliia Bielova,\n  Vincent Roca", "title": "Consent Management Platforms under the GDPR: processors and/or\n  controllers?", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Consent Management Providers (CMPs) provide consent pop-ups that are embedded\nin ever more websites over time to enable streamlined compliance with the legal\nrequirements for consent mandated by the ePrivacy Directive and the General\nData Protection Regulation (GDPR). They implement the standard for consent\ncollection from the Transparency and Consent Framework (TCF) (current version\nv2.0) proposed by the European branch of the Interactive Advertising Bureau\n(IAB Europe). Although the IAB's TCF specifications characterize CMPs as data\nprocessors, CMPs factual activities often qualifies them as data controllers\ninstead. Discerning their clear role is crucial since compliance obligations\nand CMPs liability depend on their accurate characterization. We perform\nempirical experiments with two major CMP providers in the EU: Quantcast and\nOneTrust and paired with a legal analysis. We conclude that CMPs process\npersonal data, and we identify multiple scenarios wherein CMPs are controllers.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 13:54:02 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Santos", "Cristiana", ""], ["Nouwens", "Midas", ""], ["Toth", "Michael", ""], ["Bielova", "Nataliia", ""], ["Roca", "Vincent", ""]]}, {"id": "2104.06916", "submitter": "Hao Cheng", "authors": "Yang Li, Hao Cheng, Zhe Zeng, Hailong Liu and Monika Sester", "title": "Autonomous Vehicles Drive into Shared Spaces: eHMI Design Concept\n  Focusing on Vulnerable Road Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In comparison to conventional traffic designs, shared spaces promote a more\npleasant urban environment with slower motorized movement, smoother traffic,\nand less congestion. In the foreseeable future, shared spaces will be populated\nwith a mixture of autonomous vehicles (AVs) and vulnerable road users (VRUs)\nlike pedestrians and cyclists. However, a driver-less AV lacks a way to\ncommunicate with the VRUs when they have to reach an agreement of a\nnegotiation, which brings new challenges to the safety and smoothness of the\ntraffic. To find a feasible solution to integrating AVs seamlessly into\nshared-space traffic, we first identified the possible issues that the\nshared-space designs have not considered for the role of AVs. Then an online\nquestionnaire was used to ask participants about how they would like a driver\nof the manually driving vehicle to communicate with VRUs in a shared space. We\nfound that when the driver wanted to give some suggestions to the VRUs in a\nnegotiation, participants thought that the communications via the driver's body\nbehaviors were necessary. Besides, when the driver conveyed information about\nher/his intentions and cautions to the VRUs, participants selected different\ncommunication methods with respect to their transport modes (as a driver,\npedestrian, or cyclist). These results suggest that novel eHMIs might be useful\nfor AV-VRU communication when the original drivers are not present. Hence, a\npotential eHMI design concept was proposed for different VRUs to meet their\nvarious expectations. In the end, we further discussed the effects of the eHMIs\non improving the sociality in shared spaces and the autonomous driving systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 15:06:03 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 11:18:19 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 19:25:04 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Yang", ""], ["Cheng", "Hao", ""], ["Zeng", "Zhe", ""], ["Liu", "Hailong", ""], ["Sester", "Monika", ""]]}, {"id": "2104.06963", "submitter": "Tom Williams", "authors": "Nichole D. Starr and Bertram Malle and Tom Williams", "title": "I Need Your Advice... Human Perceptions of Robot Moral Advising\n  Behaviors", "comments": "in TRAITS Workshop Proceedings (arXiv:2103.12679) held in conjunction\n  with Companion of the 2021 ACM/IEEE International Conference on Human-Robot\n  Interaction, March 2021, Pages 709-711", "journal-ref": null, "doi": null, "report-no": "TRAITS/2021/09", "categories": "cs.RO cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their unique persuasive power, language-capable robots must be able to\nboth act in line with human moral norms and clearly and appropriately\ncommunicate those norms. These requirements are complicated by the possibility\nthat humans may ascribe blame differently to humans and robots. In this work,\nwe explore how robots should communicate in moral advising scenarios, in which\nthe norms they are expected to follow (in a moral dilemma scenario) may be\ndifferent from those their advisees are expected to follow. Our results suggest\nthat, in fact, both humans and robots are judged more positively when they\nprovide the advice that favors the common good over an individual's life. These\nresults raise critical new questions regarding people's moral responses to\nrobots and the design of autonomous moral agents.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:45:02 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Starr", "Nichole D.", ""], ["Malle", "Bertram", ""], ["Williams", "Tom", ""]]}, {"id": "2104.07086", "submitter": "Diogo Cortiz", "authors": "Diogo Cortiz, Newton Calegari, Fabiana Oliveira, Daniel Couto Gatti", "title": "Game Design for Blockchain Learning", "comments": "Paper published in the II International Conference on Game, Game Art\n  and Gamification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain is a new technological approach that has gained popularity on the\nmarket due to its application in several areas such as education, health,\nsecurity, and smart cities, among others. However, understanding how blockchain\nworks is not easy at first, especially for non-technical people, because it\nrelies on a non-trivial computational process. We have developed a game board -\ncalled Blocktrain - whose game mechanics are based on the blockchain processing\nmodel. This game gives people the opportunity to learn key blockchain concepts\nwhile playing. In this paper, we describe the game design process and\nassessment of the game as pedagogical instrument.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:24:10 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Cortiz", "Diogo", ""], ["Calegari", "Newton", ""], ["Oliveira", "Fabiana", ""], ["Gatti", "Daniel Couto", ""]]}, {"id": "2104.07153", "submitter": "Wendy Ju", "authors": "Andrea Cuadra, Hansol Lee, Jason Cho, Wendy Ju", "title": "Look at Me When I Talk to You: A Video Dataset to Enable Voice\n  Assistants to Recognize Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People interacting with voice assistants are often frustrated by voice\nassistants' frequent errors and inability to respond to backchannel cues. We\nintroduce an open-source video dataset of 21 participants' interactions with a\nvoice assistant, and explore the possibility of using this dataset to enable\nautomatic error recognition to inform self-repair. The dataset includes clipped\nand labeled videos of participants' faces during free-form interactions with\nthe voice assistant from the smart speaker's perspective. To validate our\ndataset, we emulated a machine learning classifier by asking crowdsourced\nworkers to recognize voice assistant errors from watching soundless video clips\nof participants' reactions. We found trends suggesting it is possible to\ndetermine the voice assistant's performance from a participant's facial\nreaction alone. This work posits elicited datasets of interactive responses as\na key step towards improving error recognition for repair for voice assistants\nin a wide variety of applications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 22:39:44 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Cuadra", "Andrea", ""], ["Lee", "Hansol", ""], ["Cho", "Jason", ""], ["Ju", "Wendy", ""]]}, {"id": "2104.07377", "submitter": "Jianlong Zhou", "authors": "Jianlong Zhou, Weidong Huang, and Fang Chen", "title": "Facilitating Machine Learning Model Comparison and Explanation Through A\n  Radial Visualisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building an effective Machine Learning (ML) model for a data set is a\ndifficult task involving various steps. One of the most important steps is to\ncompare generated substantial amounts of ML models to find the optimal one for\nthe deployment. It is challenging to compare such models with dynamic number of\nfeatures. Comparison is more than just finding differences of ML model\nperformance, users are also interested in the relations between features and\nmodel performance such as feature importance for ML explanations. This paper\nproposes RadialNet Chart, a novel visualisation approach to compare ML models\ntrained with a different number of features of a given data set while revealing\nimplicit dependent relations. In RadialNet Chart, ML models and features are\nrepresented by lines and arcs respectively. These lines are generated\neffectively using a recursive function. The dependence of ML models with\ndynamic number of features is encoded into the structure of visualisation,\nwhere ML models and their dependent features are directly revealed from related\nline connections. ML model performance information is encoded with colour and\nline width in RadialNet Chart. Together with the structure of visualisation,\nfeature importance can be directly discerned in RadialNet Chart for ML\nexplanations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:13:48 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Zhou", "Jianlong", ""], ["Huang", "Weidong", ""], ["Chen", "Fang", ""]]}, {"id": "2104.07519", "submitter": "Th\\'eis Bazin", "authors": "Th\\'eis Bazin and Ga\\\"etan Hadjeres and Philippe Esling and Mikhail\n  Malt", "title": "Spectrogram Inpainting for Interactive Generation of Instrument Sounds", "comments": "8 pages + references + appendices. 4 figures. Published as a\n  conference paper at the The 2020 Joint Conference on AI Music Creativity,\n  October 19-23, 2020, organized and hosted virtually by the Royal Institute of\n  Technology (KTH), Stockholm, Sweden", "journal-ref": "Proceedings of the 1st Joint Conference on AI Music Creativity,\n  2020 (p. 10). Stockholm, Sweden: AIMC", "doi": "10.30746/978-91-519-5560-5", "report-no": null, "categories": "cs.SD cs.AI cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern approaches to sound synthesis using deep neural networks are hard to\ncontrol, especially when fine-grained conditioning information is not\navailable, hindering their adoption by musicians.\n  In this paper, we cast the generation of individual instrumental notes as an\ninpainting-based task, introducing novel and unique ways to iteratively shape\nsounds. To this end, we propose a two-step approach: first, we adapt the\nVQ-VAE-2 image generation architecture to spectrograms in order to convert\nreal-valued spectrograms into compact discrete codemaps, we then implement\ntoken-masked Transformers for the inpainting-based generation of these\ncodemaps.\n  We apply the proposed architecture on the NSynth dataset on masked resampling\ntasks. Most crucially, we open-source an interactive web interface to transform\nsounds by inpainting, for artists and practitioners alike, opening up to new,\ncreative uses.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:17:31 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Bazin", "Th\u00e9is", ""], ["Hadjeres", "Ga\u00ebtan", ""], ["Esling", "Philippe", ""], ["Malt", "Mikhail", ""]]}, {"id": "2104.07553", "submitter": "Joonyoung Yi", "authors": "Joonyoung Yi, Buru Chang", "title": "Efficient Click-Through Rate Prediction for Developing Countries via\n  Tabular Learning", "comments": "ICLR 2021 Workshop (PML4DC), 8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the rapid growth of online advertisement in developing countries,\nexisting highly over-parameterized Click-Through Rate (CTR) prediction models\nare difficult to be deployed due to the limited computing resources. In this\npaper, by bridging the relationship between CTR prediction task and tabular\nlearning, we present that tabular learning models are more efficient and\neffective in CTR prediction than over-parameterized CTR prediction models.\nExtensive experiments on eight public CTR prediction datasets show that tabular\nlearning models outperform twelve state-of-the-art CTR prediction models.\nFurthermore, compared to over-parameterized CTR prediction models, tabular\nlearning models can be fast trained without expensive computing resources\nincluding high-performance GPUs. Finally, through an A/B test on an actual\nonline application, we show that tabular learning models improve not only\noffline performance but also the CTR of real users.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:07:25 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yi", "Joonyoung", ""], ["Chang", "Buru", ""]]}, {"id": "2104.07595", "submitter": "Hariharan Subramonyam", "authors": "Hariharan Subramonyam, Colleen Seifert, Eytan Adar", "title": "Towards A Process Model for Co-Creating AI Experiences", "comments": "ACM DIS'21 pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thinking of technology as a design material is appealing. It encourages\ndesigners to explore the material's properties to understand its capabilities\nand limitations, a prerequisite to generative design thinking. However, as a\nmaterial, AI resists this approach because its properties emerge as part of the\ndesign process itself. Therefore, designers and AI engineers must collaborate\nin new ways to create both the material and its application experience. We\ninvestigate the co-creation process through a design study with 10 pairs of\ndesigners and engineers. We find that design 'probes' with user data are a\nuseful tool in defining AI materials. Through data probes, designers construct\ndesignerly representations of the envisioned AI experience (AIX) to identify\ndesirable AI characteristics. Data probes facilitate divergent thinking,\nmaterial testing, and design validation. Based on our findings, we propose a\nprocess model for co-creating AIX and offer design considerations for\nincorporating data probes in design tools.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:53:34 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 14:27:03 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Subramonyam", "Hariharan", ""], ["Seifert", "Colleen", ""], ["Adar", "Eytan", ""]]}, {"id": "2104.07598", "submitter": "Elz\\.e Sigut\\.e Mikalonyt\\.e", "authors": "Elz\\.e Sigut\\.e Mikalonyt\\.e and Markus Kneer", "title": "Can Artificial Intelligence Make Art?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In two experiments (total N=693) we explored whether people are willing to\nconsider paintings made by AI-driven robots as art, and robots as artists.\nAcross the two experiments, we manipulated three factors: (i) agent type\n(AI-driven robot v. human agent), (ii) behavior type (intentional creation of a\npainting v. accidental creation), and (iii) object type (abstract v.\nrepresentational painting). We found that people judge robot paintings and\nhuman painting as art to roughly the same extent. However, people are much less\nwilling to consider robots as artists than humans, which is partially explained\nby the fact that they are less disposed to attribute artistic intentions to\nrobots.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:57:47 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Mikalonyt\u0117", "Elz\u0117 Sigut\u0117", ""], ["Kneer", "Markus", ""]]}, {"id": "2104.07624", "submitter": "Diego Monteiro", "authors": "Diego Monteiro, Xian Wang, Hai-Ning Liang and Yiyu Cai", "title": "Spatial Knowledge Acquisition in Virtual and Physical Reality: A\n  Comparative Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Virtual Reality (VR) head-mounted displays (HMDs) have been studied widely as\ntools for the most diverse kinds of training activities. One special kind that\nis the basis for many real-world applications is spatial knowledge acquisition\nand navigation. For example, knowing well by heart escape routes can be an\nimportant factor for firefighters and soldiers. Prior research on how well\nknowledge acquired in virtual worlds translates to real, physical one has had\nmixed results, with some suggesting spatial learning in VR is akin to using a\nregular 2D display. However, VR HMDs have evolved drastically in the last\ndecade, and little is known about how spatial training skills in a simulated\nenvironment using up-to-date VR HMDs compares to training in the real world. In\nthis paper, we aim to investigate how people trained in a VR maze compare\nagainst those trained in a physical maze in terms of recall of the position of\nitems inside the environment. While our results did not find significant\ndifferences in time performance for people who experienced the physical and\nthose who trained in VR, other behavioural factors were different.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:40:52 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Monteiro", "Diego", ""], ["Wang", "Xian", ""], ["Liang", "Hai-Ning", ""], ["Cai", "Yiyu", ""]]}, {"id": "2104.07662", "submitter": "Deepak Pathak", "authors": "Yuqing Du, Olivia Watkins, Trevor Darrell, Pieter Abbeel, Deepak\n  Pathak", "title": "Auto-Tuned Sim-to-Real Transfer", "comments": "ICRA 2021. First two authors contributed equally. Website at\n  https://yuqingd.github.io/autotuned-sim2real/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policies trained in simulation often fail when transferred to the real world\ndue to the `reality gap' where the simulator is unable to accurately capture\nthe dynamics and visual properties of the real world. Current approaches to\ntackle this problem, such as domain randomization, require prior knowledge and\nengineering to determine how much to randomize system parameters in order to\nlearn a policy that is robust to sim-to-real transfer while also not being too\nconservative. We propose a method for automatically tuning simulator system\nparameters to match the real world using only raw RGB images of the real world\nwithout the need to define rewards or estimate state. Our key insight is to\nreframe the auto-tuning of parameters as a search problem where we iteratively\nshift the simulation system parameters to approach the real-world system\nparameters. We propose a Search Param Model (SPM) that, given a sequence of\nobservations and actions and a set of system parameters, predicts whether the\ngiven parameters are higher or lower than the true parameters used to generate\nthe observations. We evaluate our method on multiple robotic control tasks in\nboth sim-to-sim and sim-to-real transfer, demonstrating significant improvement\nover naive domain randomization. Project videos and code at\nhttps://yuqingd.github.io/autotuned-sim2real/\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:59:55 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 17:58:26 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Du", "Yuqing", ""], ["Watkins", "Olivia", ""], ["Darrell", "Trevor", ""], ["Abbeel", "Pieter", ""], ["Pathak", "Deepak", ""]]}, {"id": "2104.07724", "submitter": "Hyeok Kim", "authors": "Hyeok Kim, Dominik Moritz, Jessica Hullman", "title": "Design Patterns and Trade-Offs in Responsive Visualization for\n  Communication", "comments": "10 pages, 9 figures, accepted to Computer Graphics Forum (EuroVis\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Increased access to mobile devices motivates the need to design communicative\nvisualizations that are responsive to varying screen sizes. However, relatively\nlittle design guidance or tooling is currently available to authors. We\ncontribute a detailed characterization of responsive visualization strategies\nin communication-oriented visualizations, identifying 76 total strategies by\nanalyzing 378 pairs of large screen (LS) and small screen (SS) visualizations\nfrom online articles and reports. Our analysis distinguishes between the\nTargets of responsive visualization, referring to what elements of a design are\nchanged and Actions representing how targets are changed. We identify key\ntrade-offs related to authors' need to maintain graphical density, referring to\nthe amount of information per pixel, while also maintaining the \"message\" or\nintended takeaways for users of a visualization. We discuss implications of our\nfindings for future visualization tool design to support responsive\ntransformation of visualization designs, including requirements for automated\nrecommenders for communication-oriented responsive visualizations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 19:13:26 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 17:34:10 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kim", "Hyeok", ""], ["Moritz", "Dominik", ""], ["Hullman", "Jessica", ""]]}, {"id": "2104.07763", "submitter": "Iulian Vlad Serban", "authors": "Francois St-Hilaire, Nathan Burns, Robert Belfer, Muhammad Shayan,\n  Ariella Smofsky, Dung Do Vu, Antoine Frau, Joseph Potochny, Farid Faraji,\n  Vincent Pavero, Neroli Ko, Ansona Onyi Ching, Sabina Elkins, Anush Stepanyan,\n  Adela Matajova, Laurent Charlin, Yoshua Bengio, Iulian Vlad Serban and\n  Ekaterina Kochmar", "title": "Comparative Study of Learning Outcomes for Online Learning Platforms", "comments": "14 pages, 3 figures, 2 tables, accepted at AIED 2021 (2021 Conference\n  on Artificial Intelligence in Education)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization and active learning are key aspects to successful learning.\nThese aspects are important to address in intelligent educational applications,\nas they help systems to adapt and close the gap between students with varying\nabilities, which becomes increasingly important in the context of online and\ndistance learning. We run a comparative head-to-head study of learning outcomes\nfor two popular online learning platforms: Platform A, which follows a\ntraditional model delivering content over a series of lecture videos and\nmultiple-choice quizzes, and Platform B, which creates a personalized learning\nenvironment and provides problem-solving exercises and personalized feedback.\nWe report on the results of our study using pre- and post-assessment quizzes\nwith participants taking courses on an introductory data science topic on two\nplatforms. We observe a statistically significant increase in the learning\noutcomes on Platform B, highlighting the impact of well-designed and\nwell-engineered technology supporting active learning and problem-based\nlearning in online education. Moreover, the results of the self-assessment\nquestionnaire, where participants reported on perceived learning gains, suggest\nthat participants using Platform B improve their metacognition.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:40:24 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["St-Hilaire", "Francois", ""], ["Burns", "Nathan", ""], ["Belfer", "Robert", ""], ["Shayan", "Muhammad", ""], ["Smofsky", "Ariella", ""], ["Vu", "Dung Do", ""], ["Frau", "Antoine", ""], ["Potochny", "Joseph", ""], ["Faraji", "Farid", ""], ["Pavero", "Vincent", ""], ["Ko", "Neroli", ""], ["Ching", "Ansona Onyi", ""], ["Elkins", "Sabina", ""], ["Stepanyan", "Anush", ""], ["Matajova", "Adela", ""], ["Charlin", "Laurent", ""], ["Bengio", "Yoshua", ""], ["Serban", "Iulian Vlad", ""], ["Kochmar", "Ekaterina", ""]]}, {"id": "2104.07807", "submitter": "Jan Smeddinck", "authors": "Jack Holt, James Nicholson, Jan David Smeddinck", "title": "From Personal Data to Digital Legacy: Exploring Conflicts in the\n  Sharing, Security and Privacy of Post-mortem Data", "comments": "WWW '21", "journal-ref": null, "doi": "10.1145/3442381.3450030", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As digital technologies become more prevalent there is a growing awareness of\nthe importance of good security and privacy practices. The tools and techniques\nused to achieve this are typically designed with the living user in mind, with\nlittle consideration of how they should or will perform after the user has\ndied. We report on two workshops carried out with users of password managers to\nexplore their views on the post-mortem sharing, security and privacy of a range\nof common digital assets. We discuss a post-mortem privacy paradox where users\nrecognise value in planning for their digital legacy, yet avoid actively doing\nso. Importantly, our findings highlight a tension between the use of\nrecommended security tools during life and facilitating appropriate post-mortem\naccess to chosen assets. We offer design recommendations to facilitate and\nencourage digital legacy planning while promoting good security habits during\nlife.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 22:32:39 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Holt", "Jack", ""], ["Nicholson", "James", ""], ["Smeddinck", "Jan David", ""]]}, {"id": "2104.07901", "submitter": "Aaditeshwar Seth", "authors": "Aparna Moitra, Archna Kumar, Aaditeshwar Seth", "title": "An Analysis of Impact Pathways arising from a Mobile-based Community\n  Media Platform in Rural India", "comments": "Manuscript, Gram Vaani Community Media", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our research presents the case-study of a mobile phone based, voice-driven\nplatform - Mobile Vaani, established with a goal to empower poor and\nmarginalized communities to create their own local media. In this paper, we\nderive a comprehensive theory of change for Mobile Vaani from data gathered\nusing the Most Significant Change technique. This paper contributes towards\nformulating a theory of change for technology-driven community media platforms\nwhich can be adapted to other ICTD interventions too.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 05:55:50 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Moitra", "Aparna", ""], ["Kumar", "Archna", ""], ["Seth", "Aaditeshwar", ""]]}, {"id": "2104.07941", "submitter": "Robert West", "authors": "Roland Aydin, Lars Klein, Arnaud Miribel, Robert West", "title": "Broccoli: Sprinkling Lightweight Vocabulary Learning into Everyday\n  Information Diets", "comments": "Proceedings of The Web Conference 2020", "journal-ref": null, "doi": "10.1145/3366423.3380209", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning of a new language remains to this date a cognitive task that\nrequires considerable diligence and willpower, recent advances and tools\nnotwithstanding. In this paper, we propose Broccoli, a new paradigm aimed at\nreducing the required effort by seamlessly embedding vocabulary learning into\nusers' everyday information diets. This is achieved by inconspicuously\nswitching chosen words encountered by the user for their translation in the\ntarget language. Thus, by seeing words in context, the user can assimilate new\nvocabulary without much conscious effort. We validate our approach in a careful\nuser study, finding that the efficacy of the lightweight Broccoli approach is\ncompetitive with traditional, memorization-based vocabulary learning. The low\ncognitive overhead is manifested in a pronounced decrease in learners' usage of\nmnemonic learning strategies, as compared to traditional learning. Finally, we\nestablish that language patterns in typical information diets are compatible\nwith spaced-repetition strategies, thus enabling an efficient use of the\nBroccoli paradigm. Overall, our work establishes the feasibility of a novel and\npowerful \"install-and-forget\" approach for embedded language acquisition.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 07:38:05 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Aydin", "Roland", ""], ["Klein", "Lars", ""], ["Miribel", "Arnaud", ""], ["West", "Robert", ""]]}, {"id": "2104.08123", "submitter": "Arash Kalatian", "authors": "Arash Kalatian and Bilal Farooq", "title": "A context-aware pedestrian trajectory prediction framework for automated\n  vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the unprecedented shift towards automated urban environments in recent\nyears, a new paradigm is required to study pedestrian behaviour. Studying\npedestrian behaviour in futuristic scenarios requires modern data sources that\nconsider both the Automated Vehicle (AV) and pedestrian perspectives. Current\nopen datasets on AVs predominantly fail to account for the latter, as they do\nnot include an adequate number of events and associated details that involve\npedestrian and vehicle interactions. To address this issue, we propose using\nVirtual Reality (VR) data as a complementary resource to current datasets,\nwhich can be designed to measure pedestrian behaviour under specific\nconditions. In this research, we focus on the context-aware pedestrian\ntrajectory prediction framework for automated vehicles at mid-block\nunsignalized crossings. For this purpose, we develop a novel multi-input\nnetwork of Long Short-Term Memory (LSTM) and fully connected dense layers. In\naddition to past trajectories, the proposed framework incorporates pedestrian\nhead orientations and distance to the upcoming vehicles as sequential input\ndata. By merging the sequential data with contextual information of the\nenvironment, we train a model to predict the future pedestrian trajectory. Our\nresults show that the prediction error and overfitting to the training data are\nreduced by considering contextual information in the model. To analyze the\napplication of the methods to real AV data, the proposed framework is trained\nand applied to pedestrian trajectory extracted from an open-access video\ndataset. Finally, by implementing a game theory-based model interpretability\nmethod, we provide detailed insights and propose recommendations to improve the\ncurrent automated vehicle sensing systems from a pedestrian-oriented point of\nview.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:04:17 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 10:41:37 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kalatian", "Arash", ""], ["Farooq", "Bilal", ""]]}, {"id": "2104.08463", "submitter": "Akhila Sri Manasa Venigalla", "authors": "Akhila Sri Manasa Venigalla, Dheeraj Vagavolu, Sridhar Chimalakonda", "title": "SurviveCovid-19++ : A collaborative healthcare game towards educating\n  people about safety measures and vaccination for Covid-19", "comments": "17 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Covid-19 has been affecting population across the world for more than an\nyear, with diverse strains of this virus being identified in many countries.\nVaccines to help in curbing the virus are being developed and administered.\nPreventing the spread of the disease requires collaborative efforts from\neveryone. People with varied professional backgrounds have varied\nresponsibilities in controlling the pandemic. It is important that everyone is\naware of their respective responsibilities and also empathize with efforts and\nduties of other individuals. It is here, we wish to leverage the potential of\ngames in healthcare domain, towards educating about Covid-19. With an aim to\neducate the population about vaccination against Covid-19, responsibilities of\ncitizens with varied professional backgrounds, and emphasize on the need for\ncollaboration to fight against the pandemic, by following safety measures, we\npresent SurviveCovid-19++, a collaborative multiplayer desktop based game. The\ngame essentially revolves around four roles - doctor, sanitation worker,\ncitizen and law enforcer, delivering their duties, following safety measures\nand collaboratively clearing multiple stages in the game. We have performed a\npreliminary evaluation of the game through a qualitative and quantitative user\nsurvey. The results of the user survey were encouraging, with volunteers\nexpressing their increased empathy towards efforts of individuals with varied\nprofessional backgrounds, and better understanding of the importance of safety\nmeasures against Covid-19.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 06:12:57 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 10:40:52 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Venigalla", "Akhila Sri Manasa", ""], ["Vagavolu", "Dheeraj", ""], ["Chimalakonda", "Sridhar", ""]]}, {"id": "2104.08579", "submitter": "Robbe Cools", "authors": "Robbe Cools, Jihae Han, Adalberto L. Simeone", "title": "SelectVisAR: Selective Visualisation of Virtual Environments in\n  Augmented Reality", "comments": null, "journal-ref": null, "doi": "10.1145/3461778.3462096", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When establishing a visual connection between a virtual reality user and an\naugmented reality user, it is important to consider whether the augmented\nreality user faces a surplus of information. Augmented reality, compared to\nvirtual reality, involves two, not one, planes of information: the physical and\nthe virtual. We propose SelectVisAR, a selective visualisation system of\nvirtual environments in augmented reality. Our system enables an augmented\nreality spectator to perceive a co-located virtual reality user in the context\nof four distinct visualisation conditions: Interactive, Proximity, Everything,\nand Dollhouse. We explore an additional two conditions, Context and Spotlight,\nin a follow-up study. Our design uses a human-centric approach to information\nfiltering, selectively visualising only parts of the virtual environment\nrelated to the interactive possibilities of a virtual reality user. The\nresearch investigates how selective visualisations can be helpful or trivial\nfor the augmented reality user when observing a virtual reality user.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:47:48 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 17:29:28 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Cools", "Robbe", ""], ["Han", "Jihae", ""], ["Simeone", "Adalberto L.", ""]]}, {"id": "2104.08600", "submitter": "Judith Dineley Dr", "authors": "Judith Dineley, Grace Lavelle, Daniel Leightley, Faith Matcham, Sara\n  Siddi, Maria Teresa Pe\\~narrubia-Mar\\'ia, Katie M. White, Alina Ivan, Carolin\n  Oetzmann, Sara Simblett, Erin Dawe-Lane, Stuart Bruce, Daniel Stahl, Josep\n  Maria Haro, Til Wykes, Vaibhav A. Narayan, Matthew Hotopf, Bj\\\"orn W.\n  Schuller, Nicholas Cummins", "title": "Remote smartphone-based speech collection: acceptance and barriers in\n  individuals with major depressive disorder", "comments": "5 pages, 4 figures, 2 tables, submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The ease of in-the-wild speech recording using smartphones has sparked\nconsiderable interest in the combined application of speech, remote measurement\ntechnology (RMT) and advanced analytics as a research and healthcare tool. For\nthis to be realised, feasibility must be established not only from an\nanalytical perspective, but also the acceptability of the approach to the user.\nTo understand the acceptance, facilitators and barriers of smartphone-based\nspeech recording, we invited 384 individuals with major depressive disorder\n(MDD) from the Remote Assessment of Disease and Relapse - Central Nervous\nSystem (RADAR-CNS) research programme in Spain and the UK to complete a survey\non their experiences recording their speech. In this analysis, we demonstrate\nthat study participants were more comfortable completing a scripted speech task\nthan a free speech task. For both speech tasks, we found depression severity\nand country to be significant predictors of comfort. Not seeing smartphone\nnotifications of the scheduled speech tasks, low mood and forgetfulness were\nthe most commonly reported obstacles to providing speech recordings.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 17:41:19 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dineley", "Judith", ""], ["Lavelle", "Grace", ""], ["Leightley", "Daniel", ""], ["Matcham", "Faith", ""], ["Siddi", "Sara", ""], ["Pe\u00f1arrubia-Mar\u00eda", "Maria Teresa", ""], ["White", "Katie M.", ""], ["Ivan", "Alina", ""], ["Oetzmann", "Carolin", ""], ["Simblett", "Sara", ""], ["Dawe-Lane", "Erin", ""], ["Bruce", "Stuart", ""], ["Stahl", "Daniel", ""], ["Haro", "Josep Maria", ""], ["Wykes", "Til", ""], ["Narayan", "Vaibhav A.", ""], ["Hotopf", "Matthew", ""], ["Schuller", "Bj\u00f6rn W.", ""], ["Cummins", "Nicholas", ""]]}, {"id": "2104.08631", "submitter": "Marina Y Aoyama", "authors": "Marina Y. Aoyama, Matthew Howard", "title": "Training Humans to Train Robots Dynamic Motor Skills", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning from demonstration (LfD) is commonly considered to be a natural and\nintuitive way to allow novice users to teach motor skills to robots. However,\nit is important to acknowledge that the effectiveness of LfD is heavily\ndependent on the quality of teaching, something that may not be assured with\nnovices. It remains an open question as to the most effective way of guiding\ndemonstrators to produce informative demonstrations beyond ad hoc advice for\nspecific teaching tasks. To this end, this paper investigates the use of\nmachine teaching to derive an index for determining the quality of\ndemonstrations and evaluates its use in guiding and training novices to become\nbetter teachers. Experiments with a simple learner robot suggest that guidance\nand training of teachers through the proposed approach can lead to up to 66.5%\ndecrease in error in the learnt skill.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 19:39:07 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 06:01:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Aoyama", "Marina Y.", ""], ["Howard", "Matthew", ""]]}, {"id": "2104.08774", "submitter": "Antonios Liapis", "authors": "Theodoros Galanos and Antonios Liapis and Georgios N. Yannakakis and\n  Reinhard Koenig", "title": "ARCH-Elites: Quality-Diversity for Urban Design", "comments": "Published at Genetic and Evolutionary Computation Conference\n  Companion, 2021, 2 pages, 1 figure", "journal-ref": null, "doi": "10.1145/3449726.3459490", "report-no": null, "categories": "cs.NE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces ARCH-Elites, a MAP-Elites implementation that can\nreconfigure large-scale urban layouts at real-world locations via a pre-trained\nsurrogate model instead of costly simulations. In a series of experiments, we\ngenerate novel urban designs for two real-world locations in Boston,\nMassachusetts. Combining the exploration of a possibility space with real-time\nperformance evaluation creates a powerful new paradigm for architectural\ngenerative design that can extract and articulate design intelligence.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 08:46:38 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Galanos", "Theodoros", ""], ["Liapis", "Antonios", ""], ["Yannakakis", "Georgios N.", ""], ["Koenig", "Reinhard", ""]]}, {"id": "2104.08792", "submitter": "Mimansa Jaiswal", "authors": "Mimansa Jaiswal, Emily Mower Provost", "title": "Why Should I Trust a Model is Private? Using Shifts in Model Explanation\n  for Evaluating Privacy-Preserving Emotion Recognition Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Privacy preservation is a crucial component of any real-world application.\nYet, in applications relying on machine learning backends, this is challenging\nbecause models often capture more than a designer may have envisioned,\nresulting in the potential leakage of sensitive information. For example,\nemotion recognition models are susceptible to learning patterns between the\ntarget variable and other sensitive variables, patterns that can be maliciously\nre-purposed to obtain protected information. In this paper, we concentrate on\nusing interpretable methods to evaluate a model's efficacy to preserve privacy\nwith respect to sensitive variables. We focus on saliency-based explanations,\nexplanations that highlight regions of the input text, which allows us to\nunderstand how model explanations shift when models are trained to preserve\nprivacy. We show how certain commonly-used methods that seek to preserve\nprivacy might not align with human perception of privacy preservation. We also\nshow how some of these induce spurious correlations in the model between the\ninput and the primary as well as secondary task, even if the improvement in\nevaluation metric is significant. Such correlations can hence lead to false\nassurances about the perceived privacy of the model because especially when\nused in cross corpus conditions. We conduct crowdsourcing experiments to\nevaluate the inclination of the evaluators to choose a particular model for a\ngiven task when model explanations are provided, and find that correlation of\ninterpretation differences with sociolinguistic biases can be used as a proxy\nfor user trust.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:56:41 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jaiswal", "Mimansa", ""], ["Provost", "Emily Mower", ""]]}, {"id": "2104.08885", "submitter": "Patrick Zschech", "authors": "Christoph Sager, Christian Janiesch, Patrick Zschech", "title": "A survey of image labelling for computer vision applications", "comments": "Published online first in Journal of Business Analytics", "journal-ref": null, "doi": "10.1080/2573234X.2021.1908861", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised machine learning methods for image analysis require large amounts\nof labelled training data to solve computer vision problems. The recent rise of\ndeep learning algorithms for recognising image content has led to the emergence\nof many ad-hoc labelling tools. With this survey, we capture and systematise\nthe commonalities as well as the distinctions between existing image labelling\nsoftware. We perform a structured literature review to compile the underlying\nconcepts and features of image labelling software such as annotation\nexpressiveness and degree of automation. We structure the manual labelling task\nby its organisation of work, user interface design options, and user support\ntechniques to derive a systematisation schema for this survey. Applying it to\navailable software and the body of literature, enabled us to uncover several\napplication archetypes and key domains such as image retrieval or instance\nidentification in healthcare or television.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:01:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sager", "Christoph", ""], ["Janiesch", "Christian", ""], ["Zschech", "Patrick", ""]]}, {"id": "2104.09091", "submitter": "Jim Samuel", "authors": "Jim Samuel, Margaret Brennan-Tonetta, Yana Samuel, Pradeep Subedi and\n  Jack Smith", "title": "Strategies for Democratization of Supercomputing: Availability,\n  Accessibility and Usability of High Performance Computing for Education and\n  Practice of Big Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been an increasing interest in and growing need for high\nperformance computing (HPC), popularly known as supercomputing, in domains such\nas textual analytics, business domains analytics, forecasting and natural\nlanguage processing (NLP), in addition to the relatively mature supercomputing\ndomains of quantum physics and biology. HPC has been widely used in computer\nscience (CS) and other traditionally computation intensive disciplines, but has\nremained largely siloed away from the vast array of social, behavioral,\nbusiness and economics disciplines. However, with ubiquitous big data, there is\na compelling need to make HPC technologically and economically accessible, easy\nto use, and operationally democratized. Therefore, this research focuses on\nmaking two key contributions, the first is the articulation of strategies based\non availability, accessibility and usability for the demystification and\ndemocratization of HPC, based on an analytical review of Caliburn, a notable\nsupercomputer at its inception. The second contribution is a set of principles\nfor HPC adoption based on an experiential narrative of HPC usage for textual\nanalytics and NLP of social media data from a first time user perspective.\nBoth, the HPC usage process and the output of the early stage analytics are\nsummarized. This research study synthesizes expert input on HPC democratization\nstrategies, and chronicles the challenges and opportunities from a\nmultidisciplinary perspective, of a case of rapid adoption of supercomputing\nfor textual analytics and NLP. Deductive logic is used to identify strategies\nwhich can lead to efficacious engagement, adoption, production and sustained\nusage for research, teaching, application and innovation by researchers,\nfaculty, professionals and students across a broad range of disciplines.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 07:32:18 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Samuel", "Jim", ""], ["Brennan-Tonetta", "Margaret", ""], ["Samuel", "Yana", ""], ["Subedi", "Pradeep", ""], ["Smith", "Jack", ""]]}, {"id": "2104.09231", "submitter": "Ryuji Watanabe", "authors": "Ryuji Watanabe, Hideaki Ishibashi, Tetsuo Furukawa", "title": "Visual analytics of set data for knowledge discovery and member\n  selection support", "comments": "This is accepted manuscript in Decision Support Systems. The codes of\n  prototype system are available on\n  https://github.com/furukawa-laboratory/demo-visual-analytics-set-data", "journal-ref": null, "doi": "10.1016/j.dss.2021.113635", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual analytics (VA) is a visually assisted exploratory analysis approach in\nwhich knowledge discovery is executed interactively between the user and system\nin a human-centered manner. The purpose of this study is to develop a method\nfor the VA of set data aimed at supporting knowledge discovery and member\nselection. A typical target application is a visual support system for team\nanalysis and member selection, by which users can analyze past teams and\nexamine candidate lineups for new teams. Because there are several\ndifficulties, such as the combinatorial explosion problem, developing a VA\nsystem of set data is challenging. In this study, we first define the\nrequirements that the target system should satisfy and clarify the accompanying\nchallenges. Then we propose a method for the VA of set data, which satisfies\nthe requirements. The key idea is to model the generation process of sets and\ntheir outputs using a manifold network model. The proposed method visualizes\nthe relevant factors as a set of topographic maps on which various information\nis visualized. Furthermore, using the topographic maps as a bidirectional\ninterface, users can indicate their targets of interest in the system on these\nmaps. We demonstrate the proposed method by applying it to basketball teams,\nand compare with a benchmark system for outcome prediction and lineup\nreconstruction tasks. Because the method can be adapted to individual\napplication cases by extending the network structure, it can be a general\nmethod by which practical systems can be built.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 08:22:01 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 06:44:39 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Watanabe", "Ryuji", ""], ["Ishibashi", "Hideaki", ""], ["Furukawa", "Tetsuo", ""]]}, {"id": "2104.09233", "submitter": "Reza Khani Shekarab", "authors": "Reza Khani-Shekarab, Alireza khani-shekarab", "title": "Comprehensive systematic review into combinations of artificial\n  intelligence, human factors, and automation", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial intelligence (AI)-based models used to improve different fields\nincluding healthcare, and finance. One of the field that receive advantages of\nAI is automation. However, it is important to consider human factors in\napplication of AI in automation. This paper reports on a systematic review of\nthe published studies used to investigate the application of AI in PM. This\ncomprehensive systematic review used ScienceDirect to identify relevant\narticles. Of the 422 articles found, 40 met the inclusion and exclusion\ncriteria and were used in the review. Selected articles were classified based\non categories of human factors and areas of application. The results indicated\nthat application of AI in automation with respect to human factors could be\ndivided into three areas of physical ergonomics, cognitive ergonomic and\norganizational ergonomics. The main areas of application in physical and\ncognitive ergonomics are including transportation, User experience, and\nhuman-machine interactions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 19:01:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Khani-Shekarab", "Reza", ""], ["khani-shekarab", "Alireza", ""]]}, {"id": "2104.09237", "submitter": "Nathan Sandholtz", "authors": "Nathan Sandholtz, Yohsuke Miyamoto, Luke Bornn, Maurice Smith", "title": "Inverse Bayesian Optimization: Learning Human Search Strategies in a\n  Sequential Optimization Task", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG math.OC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a popular algorithm for sequential optimization of a\nlatent objective function when sampling from the objective is costly. The\nsearch path of the algorithm is governed by the acquisition function, which\ndefines the agent's search strategy. Conceptually, the acquisition function\ncharacterizes how the optimizer balances exploration and exploitation when\nsearching for the optimum of the latent objective. In this paper, we explore\nthe inverse problem of Bayesian optimization; we seek to estimate the agent's\nlatent acquisition function based on observed search paths. We introduce a\nprobabilistic solution framework for the inverse problem which provides a\nprincipled framework to quantify both the variability with which the agent\nperforms the optimization task as well as the uncertainty around their\nestimated acquisition function.\n  We illustrate our methods by analyzing human behavior from an experiment\nwhich was designed to force subjects to balance exploration and exploitation in\nsearch of an invisible target location. We find that while most subjects\ndemonstrate clear trends in their search behavior, there is significant\nvariation around these trends from round to round. A wide range of search\nstrategies are exhibited across the subjects in our study, but upper confidence\nbound acquisition functions offer the best fit for the majority of subjects.\nFinally, some subjects do not map well to any of the acquisition functions we\ninitially consider; these subjects tend to exhibit exploration preferences\nbeyond that of standard acquisition functions to capture. Guided by the model\ndiscrepancies, we augment the candidate acquisition functions to yield a\nsuperior fit to the human behavior in this task.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:40:34 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sandholtz", "Nathan", ""], ["Miyamoto", "Yohsuke", ""], ["Bornn", "Luke", ""], ["Smith", "Maurice", ""]]}, {"id": "2104.09263", "submitter": "Shuo Liu", "authors": "Shuo Liu, Jing Han, Estela Laporta Puyal, Spyridon Kontaxis, Shaoxiong\n  Sun, Patrick Locatelli, Judith Dineley, Florian B. Pokorny, Gloria Dalla\n  Costa, Letizia Leocan, Ana Isabel Guerrero, Carlos Nos, Ana Zabalza, Per\n  Soelberg S{\\o}rensen, Mathias Buron, Melinda Magyari, Yatharth Ranjan,\n  Zulqarnain Rashid, Pauline Conde, Callum Stewart, Amos A Folarin, Richard JB\n  Dobson, Raquel Bail\\'on, Srinivasan Vairavan, Nicholas Cummins, Vaibhav A\n  Narayan, Matthew Hotopf, Giancarlo Comi, Bj\\\"orn Schuller", "title": "Fitbeat: COVID-19 Estimation based on Wristband Heart Rate", "comments": "34pages, 4figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the potential of deep learning methods to identify\nindividuals with suspected COVID-19 infection using remotely collected\nheart-rate data. The study utilises data from the ongoing EU IMI RADAR-CNS\nresearch project that is investigating the feasibility of wearable devices and\nsmart phones to monitor individuals with multiple sclerosis (MS), depression or\nepilepsy. Aspart of the project protocol, heart-rate data was collected from\nparticipants using a Fitbit wristband. The presence of COVID-19 in the cohort\nin this work was either confirmed through a positive swab test, or inferred\nthrough the self-reporting of a combination of symptoms including fever,\nrespiratory symptoms, loss of smell or taste, tiredness and gastrointestinal\nsymptoms. Experimental results indicate that our proposed contrastive\nconvolutional auto-encoder (contrastive CAE), i. e., a combined architecture of\nan auto-encoder and contrastive loss, outperforms a conventional convolutional\nneural network (CNN), as well as a convolutional auto-encoder (CAE) without\nusing contrastive loss. Our final contrastive CAE achieves 95.3% unweighted\naverage recall, 86.4% precision, anF1 measure of 88.2%, a sensitivity of 100%\nand a specificity of 90.6% on a testset of 19 participants with MS who reported\nsymptoms of COVID-19. Each of these participants was paired with a participant\nwith MS with no COVID-19 symptoms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 13:08:53 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Liu", "Shuo", ""], ["Han", "Jing", ""], ["Puyal", "Estela Laporta", ""], ["Kontaxis", "Spyridon", ""], ["Sun", "Shaoxiong", ""], ["Locatelli", "Patrick", ""], ["Dineley", "Judith", ""], ["Pokorny", "Florian B.", ""], ["Costa", "Gloria Dalla", ""], ["Leocan", "Letizia", ""], ["Guerrero", "Ana Isabel", ""], ["Nos", "Carlos", ""], ["Zabalza", "Ana", ""], ["S\u00f8rensen", "Per Soelberg", ""], ["Buron", "Mathias", ""], ["Magyari", "Melinda", ""], ["Ranjan", "Yatharth", ""], ["Rashid", "Zulqarnain", ""], ["Conde", "Pauline", ""], ["Stewart", "Callum", ""], ["Folarin", "Amos A", ""], ["Dobson", "Richard JB", ""], ["Bail\u00f3n", "Raquel", ""], ["Vairavan", "Srinivasan", ""], ["Cummins", "Nicholas", ""], ["Narayan", "Vaibhav A", ""], ["Hotopf", "Matthew", ""], ["Comi", "Giancarlo", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "2104.09396", "submitter": "Saurav Jha", "authors": "Saurav Jha, Martin Schiemer, Franco Zambonelli and Juan Ye", "title": "Continual Learning in Sensor-based Human Activity Recognition: an\n  Empirical Benchmark Analysis", "comments": "Accepted in the Information Sciences journal", "journal-ref": null, "doi": "10.1016/j.ins.2021.04.062", "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sensor-based human activity recognition (HAR), i.e., the ability to discover\nhuman daily activity patterns from wearable or embedded sensors, is a key\nenabler for many real-world applications in smart homes, personal healthcare,\nand urban planning. However, with an increasing number of applications being\ndeployed, an important question arises: how can a HAR system autonomously learn\nnew activities over a long period of time without being re-engineered from\nscratch? This problem is known as continual learning and has been particularly\npopular in the domain of computer vision, where several techniques to attack it\nhave been developed. This paper aims to assess to what extent such continual\nlearning techniques can be applied to the HAR domain. To this end, we propose a\ngeneral framework to evaluate the performance of such techniques on various\ntypes of commonly used HAR datasets. We then present a comprehensive empirical\nanalysis of their computational cost and effectiveness of tackling HAR-specific\nchallenges (i.e., sensor noise and labels' scarcity). The presented results\nuncover useful insights on their applicability and suggest future research\ndirections for HAR systems. Our code, models and data are available at\nhttps://github.com/srvCodes/continual-learning-benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:38:22 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jha", "Saurav", ""], ["Schiemer", "Martin", ""], ["Zambonelli", "Franco", ""], ["Ye", "Juan", ""]]}, {"id": "2104.09469", "submitter": "Spencer Frazier", "authors": "Md Sultan Al Nahian, Spencer Frazier, Brent Harrison, Mark Riedl", "title": "Training Value-Aligned Reinforcement Learning Agents Using a Normative\n  Prior", "comments": "(Nahian and Frazier contributed equally to this work)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As more machine learning agents interact with humans, it is increasingly a\nprospect that an agent trained to perform a task optimally, using only a\nmeasure of task performance as feedback, can violate societal norms for\nacceptable behavior or cause harm. Value alignment is a property of intelligent\nagents wherein they solely pursue non-harmful behaviors or human-beneficial\ngoals. We introduce an approach to value-aligned reinforcement learning, in\nwhich we train an agent with two reward signals: a standard task performance\nreward, plus a normative behavior reward. The normative behavior reward is\nderived from a value-aligned prior model previously shown to classify text as\nnormative or non-normative. We show how variations on a policy shaping\ntechnique can balance these two sources of reward and produce policies that are\nboth effective and perceived as being more normative. We test our\nvalue-alignment technique on three interactive text-based worlds; each world is\ndesigned specifically to challenge agents with a task as well as provide\nopportunities to deviate from the task to engage in normative and/or altruistic\nbehavior.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:33:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Nahian", "Md Sultan Al", ""], ["Frazier", "Spencer", ""], ["Harrison", "Brent", ""], ["Riedl", "Mark", ""]]}, {"id": "2104.09578", "submitter": "Hunter Priniski", "authors": "J. Hunter Priniski, Negar Mokhberian, Bahareh Harandizadeh, Fred\n  Morstatter, Kristina Lerman, Hongjing Lu, P. Jeffrey Brantingham", "title": "Mapping Moral Valence of Tweets Following the Killing of George Floyd", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The viral video documenting the killing of George Floyd by Minneapolis police\nofficer Derek Chauvin inspired nation-wide protests that brought national\nattention to widespread racial injustice and biased policing practices towards\nblack communities in the United States. The use of social media by the Black\nLives Matter movement was a primary route for activists to promote the cause\nand organize over 1,400 protests across the country. Recent research argues\nthat moral discussions on social media are a catalyst for social change. This\nstudy sought to shed light on the moral dynamics shaping Black Lives Matter\nTwitter discussions by analyzing over 40,000 Tweets geo-located to Los Angeles.\nThe goal of this study is to (1) develop computational techniques for mapping\nthe structure of moral discourse on Twitter and (2) understand the connections\nbetween social media activism and protest.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:16:55 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Priniski", "J. Hunter", ""], ["Mokhberian", "Negar", ""], ["Harandizadeh", "Bahareh", ""], ["Morstatter", "Fred", ""], ["Lerman", "Kristina", ""], ["Lu", "Hongjing", ""], ["Brantingham", "P. Jeffrey", ""]]}, {"id": "2104.09612", "submitter": "Ronal Singh", "authors": "Ronal Singh, Upol Ehsan, Marc Cheong, Mark O. Riedl, Tim Miller", "title": "LEx: A Framework for Operationalising Layers of Machine Learning\n  Explanations", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several social factors impact how people respond to AI explanations used to\njustify AI decisions affecting them personally. In this position paper, we\ndefine a framework called the \\textit{layers of explanation} (LEx), a lens\nthrough which we can assess the appropriateness of different types of\nexplanations. The framework uses the notions of \\textit{sensitivity} (emotional\nresponsiveness) of features and the level of \\textit{stakes} (decision's\nconsequence) in a domain to determine whether different types of explanations\nare \\textit{appropriate} in a given context. We demonstrate how to use the\nframework to assess the appropriateness of different types of explanations in\ndifferent domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 23:31:04 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Singh", "Ronal", ""], ["Ehsan", "Upol", ""], ["Cheong", "Marc", ""], ["Riedl", "Mark O.", ""], ["Miller", "Tim", ""]]}, {"id": "2104.09777", "submitter": "Inkyu Sa", "authors": "JongYoon Lim, Inkyu Sa, Ho Seok Ahn, Norina Gasteiger, Sanghyub John\n  Lee, Bruce MacDonald", "title": "Subsentence Extraction from Text Using Coverage-Based Deep Learning\n  Language Models", "comments": "27 pages, 16 figures", "journal-ref": "MDPI Sensors 2021, 21(8), 2712", "doi": "10.3390/s21082712", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment prediction remains a challenging and unresolved task in various\nresearch fields, including psychology, neuroscience, and computer science. This\nstems from its high degree of subjectivity and limited input sources that can\neffectively capture the actual sentiment. This can be even more challenging\nwith only text-based input. Meanwhile, the rise of deep learning and an\nunprecedented large volume of data have paved the way for artificial\nintelligence to perform impressively accurate predictions or even human-level\nreasoning. Drawing inspiration from this, we propose a coverage-based sentiment\nand subsentence extraction system that estimates a span of input text and\nrecursively feeds this information back to the networks. The predicted\nsubsentence consists of auxiliary information expressing a sentiment. This is\nan important building block for enabling vivid and epic sentiment delivery\n(within the scope of this paper) and for other natural language processing\ntasks such as text summarisation and Q&A. Our approach outperforms the\nstate-of-the-art approaches by a large margin in subsentence prediction (i.e.,\nAverage Jaccard scores from 0.72 to 0.89). For the evaluation, we designed\nrigorous experiments consisting of 24 ablation studies. Finally, our learned\nlessons are returned to the community by sharing software packages and a public\ndataset that can reproduce the results presented in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 06:24:49 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 03:49:23 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Lim", "JongYoon", ""], ["Sa", "Inkyu", ""], ["Ahn", "Ho Seok", ""], ["Gasteiger", "Norina", ""], ["Lee", "Sanghyub John", ""], ["MacDonald", "Bruce", ""]]}, {"id": "2104.10122", "submitter": "Ali Abedi", "authors": "Ali Abedi and Shehroz S. Khan", "title": "Improving state-of-the-art in Detecting Student Engagement with Resnet\n  and TCN Hybrid Network", "comments": "7 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic detection of students' engagement in online learning settings is a\nkey element to improve the quality of learning and to deliver personalized\nlearning materials to them. Varying levels of engagement exhibited by students\nin an online classroom is an affective behavior that takes place over space and\ntime. Therefore, we formulate detecting levels of students' engagement from\nvideos as a spatio-temporal classification problem. In this paper, we present a\nnovel end-to-end Residual Network (ResNet) and Temporal Convolutional Network\n(TCN) hybrid neural network architecture for students' engagement level\ndetection in videos. The 2D ResNet extracts spatial features from consecutive\nvideo frames, and the TCN analyzes the temporal changes in video frames to\ndetect the level of engagement. The spatial and temporal arms of the hybrid\nnetwork are jointly trained on raw video frames of a large publicly available\nstudents' engagement detection dataset, DAiSEE. We compared our method with\nseveral competing students' engagement detection methods on this dataset. The\nResNet+TCN architecture outperforms all other studied methods, improves the\nstate-of-the-art engagement level detection accuracy, and sets a new baseline\nfor future research.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:10:13 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Abedi", "Ali", ""], ["Khan", "Shehroz S.", ""]]}, {"id": "2104.10263", "submitter": "Alexander Spangher", "authors": "Alexander Spangher and Jonathan May", "title": "\\textit{StateCensusLaws.org}: A Web Application for Consuming and\n  Annotating Legal Discourse Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we create a web application to highlight the output of NLP\nmodels trained to parse and label discourse segments in law text. Our system is\nbuilt primarily with journalists and legal interpreters in mind, and we focus\non state-level law that uses U.S. Census population numbers to allocate\nresources and organize government.\n  Our system exposes a corpus we collect of 6,000 state-level laws that pertain\nto the U.S. census, using 25 scrapers we built to crawl state law websites,\nwhich we release. We also build a novel, flexible annotation framework that can\nhandle span-tagging and relation tagging on an arbitrary input text document\nand be embedded simply into any webpage. This framework allows journalists and\nresearchers to add to our annotation database by correcting and tagging new\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 22:00:54 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Spangher", "Alexander", ""], ["May", "Jonathan", ""]]}, {"id": "2104.10438", "submitter": "Sergey Kruzhilov", "authors": "Sergey I. Kruzhilov", "title": "Unification of computer reality", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The work attempts to unify the conceptual model of the user's virtual\ncomputer environment, with the aim of combining the local environments of\noperating systems and the global Internet environment into a single virtual\nenvironment built on general principles. To solve this problem, it is proposed\nto unify the conceptual basis of these environments. The existing conceptual\nbasis of operating systems, built on the \"desktop\" metaphor, contains redundant\nconcepts associated with computer architecture. The use of the spatial\nconceptual basis \"object - place\" with the concepts of \"domain\", \"site\", and\n\"data object\" allows to completely virtualize the user environment, separating\nit from the hardware concepts. The virtual concept \"domain\" is becoming a\nuniversal way of structuring the user's space. The introduction of this concept\nto describe the environments of operating systems provides at the mental level\nthe integration of the structures of the local and global space. The use of the\nconcept of \"personal domain\" will allow replacing the concept of \"personal\ncomputer\" in the mind of the user. The virtual concept of \"site\" as an\nenvironment for activities and data storage will allow abandoning such concepts\nas \"application\" (program), or \"memory device\". The site in the mind of the\nuser is a virtual environment that includes both places for storing data\nobjects and places for working with them. The introduction of the concept\n\"site\" into the structure of operating systems environments and the concept of\n\"data site\" into the structure of the global network integrates the structure\nof the global and local space in the user's mind. The introduction of the\nconcept of \"portal\" as a means of integrating information necessary for\ninteraction, allows ensuring the methodological homogeneity of the user's work\nin a single virtual environment.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 10:12:41 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Kruzhilov", "Sergey I.", ""]]}, {"id": "2104.10480", "submitter": "Ye Wang", "authors": "Ye Wang and Yu Chen and Shenyi Wang and Chenhang Zhou and Xiaochen\n  Zheng and Roger Wattenhofer", "title": "Print Your Own Money: A Cash-Like Experience for Digital Payment Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital money is getting a lot of traction recently, a process which may\naccelerate even more with the advent of Central Bank Digital Currency (CBDC).\nDigital money has several disadvantages: Payments are difficult or outright\nimpossible in emergency situations such as the failure of the electricity grid\nor Internet. CBDC may also be difficult to handle for children, the elderly, or\nnon-resident travelers.\n  To overcome these problems, we design a cash-like CBDC experience. Our design\nprocess has three stages: interviews, system design, and system tests. Based on\nthe interviews, we design and implement a CBDC component which is physical and\noffline. We then test our system for usability.\n  We discuss a spectrum of trust levels to allow for offline payments and argue\nthat our system can handle emergencies and exceptional situations for digital\npayments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 11:59:05 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Wang", "Ye", ""], ["Chen", "Yu", ""], ["Wang", "Shenyi", ""], ["Zhou", "Chenhang", ""], ["Zheng", "Xiaochen", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2104.10489", "submitter": "Dillon Lohr", "authors": "Dillon Lohr, Henry Griffith, and Oleg V Komogortsev", "title": "Eye Know You: Metric Learning for End-to-end Biometric Authentication\n  Using Eye Movements from a Longitudinal Dataset", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While numerous studies have explored eye movement biometrics since the\nmodality's inception in 2004, the permanence of eye movements remains largely\nunexplored as most studies utilize datasets collected within a short time\nframe. This paper presents a convolutional neural network for authenticating\nusers using their eye movements. The network is trained with an established\nmetric learning loss function, multi-similarity loss, which seeks to form a\nwell-clustered embedding space and directly enables the enrollment and\nauthentication of out-of-sample users. Performance measures are computed on\nGazeBase, a task-diverse and publicly-available dataset collected over a\n37-month period. This study includes an exhaustive analysis of the effects of\ntraining on various tasks and downsampling from 1000 Hz to several lower\nsampling rates. Our results reveal that reasonable authentication accuracy may\nbe achieved even during a low-cognitive-load task or at low sampling rates.\nMoreover, we find that eye movements are quite resilient against template aging\nafter 3 years.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 12:21:28 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Lohr", "Dillon", ""], ["Griffith", "Henry", ""], ["Komogortsev", "Oleg V", ""]]}, {"id": "2104.10550", "submitter": "Felix Beierle", "authors": "Felix Beierle, Uttam Dhakal, Caroline Cohrdes, Sophie Eicher,\n  R\\\"udiger Pryss", "title": "Public Perception of the German COVID-19 Contact-Tracing App\n  Corona-Warn-App", "comments": "Accepted for publication at the 34th IEEE International Symposium on\n  Computer-Based Medical Systems (IEEE CBMS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several governments introduced or promoted the use of contact-tracing apps\nduring the ongoing COVID-19 pandemic. In Germany, the related app is called\nCorona-Warn-App, and by end of 2020, it had 22.8 million downloads. Contact\ntracing is a promising approach for containing the spread of the novel\ncoronavirus. It is only effective if there is a large user base, which brings\nnew challenges like app users unfamiliar with using smartphones or apps. As\nCorona-Warn-App is voluntary to use, reaching many users and gaining a positive\npublic perception is crucial for its effectiveness. Based on app reviews and\ntweets, we are analyzing the public perception of Corona-Warn-App. We collected\nand analyzed all 78,963 app reviews for the Android and iOS versions from\nrelease (June 2020) to beginning of February 2021, as well as all original\ntweets until February 2021 containing #CoronaWarnApp (43,082). For the reviews,\nthe most common words and n-grams point towards technical issues, but it\nremains unclear, to what extent this is due to the app itself, the used\nExposure Notification Framework, system settings on the user's phone, or the\nuser's misinterpretations of app content. For Twitter data, overall, based on\ntweet content, frequent hashtags, and interactions with tweets, we conclude\nthat the German Twitter-sphere widely reports adopting the app and promotes its\nuse.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:17:38 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Beierle", "Felix", ""], ["Dhakal", "Uttam", ""], ["Cohrdes", "Caroline", ""], ["Eicher", "Sophie", ""], ["Pryss", "R\u00fcdiger", ""]]}, {"id": "2104.10671", "submitter": "Yongfeng Zhang", "authors": "Yunqi Li, Hanxiong Chen, Zuohui Fu, Yingqiang Ge, Yongfeng Zhang", "title": "User-oriented Fairness in Recommendation", "comments": "Accepted to the 30th Web Conference (WWW 2021)", "journal-ref": null, "doi": "10.1145/3442381.3449866", "report-no": null, "categories": "cs.IR cs.AI cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a highly data-driven application, recommender systems could be affected by\ndata bias, resulting in unfair results for different data groups, which could\nbe a reason that affects the system performance. Therefore, it is important to\nidentify and solve the unfairness issues in recommendation scenarios. In this\npaper, we address the unfairness problem in recommender systems from the user\nperspective. We group users into advantaged and disadvantaged groups according\nto their level of activity, and conduct experiments to show that current\nrecommender systems will behave unfairly between two groups of users.\nSpecifically, the advantaged users (active) who only account for a small\nproportion in data enjoy much higher recommendation quality than those\ndisadvantaged users (inactive). Such bias can also affect the overall\nperformance since the disadvantaged users are the majority. To solve this\nproblem, we provide a re-ranking approach to mitigate this unfairness problem\nby adding constraints over evaluation metrics. The experiments we conducted on\nseveral real-world datasets with various recommendation algorithms show that\nour approach can not only improve group fairness of users in recommender\nsystems, but also achieve better overall recommendation performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 17:50:31 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Li", "Yunqi", ""], ["Chen", "Hanxiong", ""], ["Fu", "Zuohui", ""], ["Ge", "Yingqiang", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2104.10741", "submitter": "Florian Kadner", "authors": "Florian Kadner, Yannik Keller and Constantin A. Rothkopf", "title": "AdaptiFont: Increasing Individuals' Reading Speed with a Generative Font\n  Model and Bayesian Optimization", "comments": "18 pages, 11 figures", "journal-ref": "In Proceedings of the 2021 CHI Conference on Human Factors in\n  Computing Systems (CHI '21). Association for Computing Machinery, New York,\n  NY, USA, Article 585, 1-11", "doi": "10.1145/3411764.3445140", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital text has become one of the primary ways of exchanging knowledge, but\ntext needs to be rendered to a screen to be read. We present AdaptiFont, a\nhuman-in-the-loop system that is aimed at interactively increasing readability\nof text displayed on a monitor. To this end, we first learn a generative font\nspace with non-negative matrix factorization from a set of classic fonts. In\nthis space we generate new true-type-fonts through active learning, render\ntexts with the new font, and measure individual users' reading speed. Bayesian\noptimization sequentially generates new fonts on the fly to progressively\nincrease individuals' reading speed. The results of a user study show that this\nadaptive font generation system finds regions in the font space corresponding\nto high reading speeds, that these fonts significantly increase participants'\nreading speed, and that the found fonts are significantly different across\nindividual readers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 19:56:28 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Kadner", "Florian", ""], ["Keller", "Yannik", ""], ["Rothkopf", "Constantin A.", ""]]}, {"id": "2104.10968", "submitter": "Let\\'icia Seixas Pereira", "authors": "Let\\'icia Seixas Pereira, Jos\\'e Coelho, Andr\\'e Rodrigues, Jo\\~ao\n  Guerreiro, Tiago Guerreiro, Carlos Duarte", "title": "Barriers and Opportunities to Accessible Social Media Content Authoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  User-generated content plays a key role in social networking, allowing a more\nactive participation, socialisation, and collaboration among users. In\nparticular, media content has been gaining a lot of ground, allowing users to\nexpress themselves through different types of formats such as images, GIFs and\nvideos. The majority of this growing type of online content remains\ninaccessible to a part of the population, despite available tools to mitigate\nthis source of exclusion. We sought to understand how people are perceiving\nthese online contents in their networks and how to support tools are being\nused. To do so, we performed an online survey of 258 social network users and a\nfollow-up interview conducted with 20 of them - 7 of them self-reporting blind\nand 13 sighted users without a disability. Results show how the different\napproaches being employed by major platforms are still not sufficient to\nproperly address this issue. Our findings reveal that mainstream users are not\naware of the possibility and the benefits of adopting accessible practices.\nFrom the general perspectives of end-users experiencing accessible practices,\nconcerning barriers encountered, and motivational factors, we also discuss\nfurther approaches to create more user engagement and awareness.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 10:06:00 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Pereira", "Let\u00edcia Seixas", ""], ["Coelho", "Jos\u00e9", ""], ["Rodrigues", "Andr\u00e9", ""], ["Guerreiro", "Jo\u00e3o", ""], ["Guerreiro", "Tiago", ""], ["Duarte", "Carlos", ""]]}, {"id": "2104.11032", "submitter": "Moeen Mostafavi", "authors": "Moeen Mostafavi and Michael D. Porter", "title": "How emoji and word embedding helps to unveil emotional transitions\n  during online messaging", "comments": null, "journal-ref": null, "doi": "10.1109/SysCon48628.2021.9447137", "report-no": null, "categories": "cs.HC cs.CL cs.SI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During online chats, body-language and vocal characteristics are not part of\nthe communication mechanism making it challenging to facilitate an accurate\ninterpretation of feelings, emotions, and attitudes. The use of emojis to\nexpress emotional feeling is an alternative approach in these types of\ncommunication. In this project, we focus on modeling a customer's emotion in an\nonline messaging session with a chatbot. We use Affect Control Theory (ACT) to\npredict emotional change during the interaction. To let the customer use\nemojis, we also extend the affective dictionaries used by ACT. For this\npurpose, we mapped Emoji2vec embedding to the affective space. Our framework\ncan find emotional change during messaging and how a customer's reaction is\nchanged accordingly.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 12:45:17 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mostafavi", "Moeen", ""], ["Porter", "Michael D.", ""]]}, {"id": "2104.11043", "submitter": "Stephan Schl\\\"ogl PhD", "authors": "Markus Thaler, Stephan Schl\\\"ogl and Aleksander Groth", "title": "Agent vs. Avatar: Comparing Embodied Conversational Agents Concerning\n  Characteristics of the Uncanny Valley", "comments": "6 pages", "journal-ref": null, "doi": "10.1109/ICHMS49158.2020.9209539", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual appearance is an important aspect influencing the perception and\nconsequent acceptance of Embodied Conversational Agents (ECA). To this end, the\nUncanny Valley theory contradicts the common assumption that increased\nhumanization of characters leads to better acceptance. Rather, it shows that\nanthropomorphic behavior may trigger feelings of eeriness and rejection in\npeople. The work presented in this paper explores whether four different\nautonomous ECAs, specifically build for a European research project, are\naffected by this effect, and how they compare to two slightly more\nrealistically looking human-controlled, i.e. face-tracked, ECAs with respect to\nperceived humanness, eeriness, and attractiveness. Short videos of the ECAs in\ncombination with a validated questionnaire were used to investigate potential\ndifferences. Results support existing theories highlighting that increased\nperceived humanness correlates with increased perceived eeriness. Furthermore,\nit was found, that neither the gender of survey participants, their age, nor\nthe sex of the ECA influences this effect, and that female ECAs are perceived\nto be significantly more attractive than their male counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 13:19:43 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Thaler", "Markus", ""], ["Schl\u00f6gl", "Stephan", ""], ["Groth", "Aleksander", ""]]}, {"id": "2104.11153", "submitter": "Felix Schoeller", "authors": "Felix Schoeller, Mark Miller, Roy Salomon, Karl J. Friston", "title": "Trust as Extended Control: Active Inference and User Feedback During\n  Human-Robot Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To interact seamlessly with robots, users must infer the causes of a robot's\nbehavior and be confident about that inference. Hence, trust is a necessary\ncondition for human-robot collaboration (HRC). Despite its crucial role, it is\nlargely unknown how trust emerges, develops, and supports human interactions\nwith nonhuman artefacts. Here, we review the literature on trust, human-robot\ninteraction, human-robot collaboration, and human interaction at large. Early\nmodels of trust suggest that trust entails a trade-off between benevolence and\ncompetence, while studies of human-to-human interaction emphasize the role of\nshared behavior and mutual knowledge in the gradual building of trust. We then\nintroduce a model of trust as an agent's best explanation for reliable sensory\nexchange with an extended motor plant or partner. This model is based on the\ncognitive neuroscience of active inference and suggests that, in the context of\nHRC, trust can be cast in terms of virtual control over an artificial agent. In\nthis setting, interactive feedback becomes a necessary component of the\ntrustor's perception-action cycle. The resulting model has important\nimplications for understanding human-robot interaction and collaboration, as it\nallows the traditional determinants of human trust to be defined in terms of\nactive inference, information exchange and empowerment. Furthermore, this model\nsuggests that boredom and surprise may be used as markers for under and\nover-reliance on the system. Finally, we examine the role of shared behavior in\nthe genesis of trust, especially in the context of dyadic collaboration,\nsuggesting important consequences for the acceptability and design of\nhuman-robot collaborative systems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 16:11:22 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Schoeller", "Felix", ""], ["Miller", "Mark", ""], ["Salomon", "Roy", ""], ["Friston", "Karl J.", ""]]}, {"id": "2104.11214", "submitter": "Bei Wang", "authors": "Youjia Zhou, Archit Rathore, Emilie Purvine, Bei Wang", "title": "Topological Simplifications of Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study hypergraph visualization via its topological simplification. We\nexplore both vertex simplification and hyperedge simplification of hypergraphs\nusing tools from topological data analysis. In particular, we transform a\nhypergraph to its graph representations known as the line graph and clique\nexpansion. A topological simplification of such a graph representation induces\na simplification of the hypergraph. In simplifying a hypergraph, we allow\nvertices to be combined if they belong to almost the same set of hyperedges,\nand hyperedges to be merged if they share almost the same set of vertices. Our\nproposed approaches are general, mathematically justifiable, and they put\nvertex simplification and hyperedge simplification in a unifying framework.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:49:36 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zhou", "Youjia", ""], ["Rathore", "Archit", ""], ["Purvine", "Emilie", ""], ["Wang", "Bei", ""]]}, {"id": "2104.11271", "submitter": "Hansoo Lee Lee", "authors": "Hansoo Lee, Joonyoung Park, Uichin Lee", "title": "A Systematic Survey on Android API Usage for Data-Driven Analytics with\n  Smartphones", "comments": "39 pages, 4 figures, 15 tables, This work has been submitted to the\n  ACM for possible publication. Copyright may be transferred without notice,\n  after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been an increase in industrial and academic research on\ndata-driven analytics with smartphones based on the collection of app usage\npatterns and surrounding context data. The Android mobile operating system\nutilizes Usage Statistics API (US API) and Accessibility Service API (AS API)\nas representative APIs to passively collect app usage data. These APIs are used\nfor various research purposes as they can collect app usage patterns (e.g., app\nstatus, usage time, app name, user interaction state, and smartphone use state)\nand fine-grained data (e.g., user interface elements \\& hierarchy and user\ninteraction type \\& target \\& time) of each application. In addition, other\nsensing APIs help to collect the user's surroundings context (location,\nnetwork, ambient environment) and device state data, along with AS/US API. In\nthis review, we provide insights on the types of mobile usage and sensor data\nthat can be collected for each research purpose by considering Android built-in\nAPIs and sensors (AS/US API, and other sensing APIs). Moreover, we classify the\nresearch purposes of the surveyed papers into four categories and 17\nsub-categories, and create a hierarchical structure for data classification,\ncomprising three layers. We present the important trends in the usage of\nAndroid's built-in APIs and sensors, including AS/US API, the types of data\ncollected using the presented APIs, and discuss the utilization of mobile usage\nand sensor data in future research.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 18:27:09 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lee", "Hansoo", ""], ["Park", "Joonyoung", ""], ["Lee", "Uichin", ""]]}, {"id": "2104.11276", "submitter": "Krenare Pireva Nuci", "authors": "Lumbardh Elshani, Krenare Pireva Nu\\c{c}i", "title": "Constructing a personalized learning path using genetic algorithms\n  approach", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A substantial disadvantage of traditional learning is that all students\nfollow the same learning sequence, but not all of them have the same background\nof knowledge, the same preferences, the same learning goals, and the same\nneeds. Traditional teaching resources, such as textbooks, in most cases pursue\nstudents to follow fixed sequences during the learning process, thus impairing\ntheir performance. Learning sequencing is an important research issue as part\nof the learning process because no fixed learning paths will be appropriate for\nall learners. For this reason, many research papers are focused on the\ndevelopment of mechanisms to offer personalization on learning paths,\nconsidering the learner needs, interests, behaviors, and abilities. In most\ncases, these researchers are totally focused on the student's preferences,\nignoring the level of difficulty and the relation degree that exists between\nvarious concepts in a course. This research paper presents the possibility of\nconstructing personalized learning paths using genetic algorithm-based model,\nencountering the level of difficulty and relation degree of the constituent\nconcepts of a course. The experimental results shows that the genetic algorithm\nis suitable to generate optimal learning paths based on learning object\ndifficulty level, duration, rating, and relation degree between each learning\nobject as elementary parts of the sequence of the learning path. From these\nresults compared to the quality of the traditional learning path, we observed\nthat even the quality of the weakest learning path generated by our GA approach\nis in a favor compared to quality of the traditional learning path, with a\ndifference of 3.59\\%, while the highest solution generated in the end resulted\n8.34\\% in favor of our proposal compared to the traditional learning paths.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 18:43:47 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Elshani", "Lumbardh", ""], ["Nu\u00e7i", "Krenare Pireva", ""]]}, {"id": "2104.11340", "submitter": "Katherine Isbister", "authors": "Katherine Isbister, Peter Cottrell, Alessia Cecchet, Ella Dagan, Nikki\n  Theofanopoulou, Ferran Altarriba Bertran, Aaron J. Horowitz, Nick Mead, Joel\n  B. Schwarz, Petr Slovak", "title": "Design not Lost in Translation: A Case Study of an Intimate-Space\n  Socially Assistive Robot for Emotion Regulation", "comments": "currently in review (as of April 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Research-through-Design case study of the design and development\nof an intimate-space tangible device perhaps best understood as a socially\nassistive robot, aimed at scaffolding children's efforts at emotional\nregulation. This case study covers the initial research device development, as\nwell as knowledge transfer to a product development company towards translating\nthe research into a workable commercial product that could also serve as a\nrobust research product for field trials. Key contributions to the literature\ninclude: 1. sharing of lessons learned from the knowledge transfer process that\ncan be useful to others interested in developing robust products, whether\ncommercial or research, that preserve design values, while allowing for large\nscale deployment and research; 2. articulation of a design space in\nHCI/HRI--Human Robot Interaction--of intimate space socially assistive robots,\nwith the current artifact as a central exemplar, contextualized alongside other\nrelated HRI artifacts.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 22:46:16 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Isbister", "Katherine", ""], ["Cottrell", "Peter", ""], ["Cecchet", "Alessia", ""], ["Dagan", "Ella", ""], ["Theofanopoulou", "Nikki", ""], ["Bertran", "Ferran Altarriba", ""], ["Horowitz", "Aaron J.", ""], ["Mead", "Nick", ""], ["Schwarz", "Joel B.", ""], ["Slovak", "Petr", ""]]}, {"id": "2104.11365", "submitter": "Meia Chita-Tegmark", "authors": "Meia Chita-Tegmark, Theresa Law, Nicholas Rabb and Matthias Scheutz", "title": "Can You Trust Your Trust Measure?", "comments": "9 pages", "journal-ref": "In Proceedings of the 2021 ACM/IEEE International Conference on\n  Human-Robot Interaction (HRI '21), March 8-11, 2021, Boulder, CO, USA ACM,\n  New York, NY, USA", "doi": "10.1145/3434073.3444677", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Trust in human-robot interactions (HRI) is measured in two main ways: through\nsubjective questionnaires and through behavioral tasks. To optimize\nmeasurements of trust through questionnaires, the field of HRI faces two\nchallenges: the development of standardized measures that apply to a variety of\nrobots with different capabilities, and the exploration of social and\nrelational dimensions of trust in robots (e.g., benevolence). In this paper we\nlook at how different trust questionnaires fare given these challenges that\npull in different directions (being general vs. being exploratory) by studying\nwhether people think the items in these questionnaires are applicable to\ndifferent kinds of robots and interactions. In Study 1 we show that after being\npresented with a robot (non-humanoid) and an interaction scenario (fire\nevacuation), participants rated multiple questionnaire items such as \"This\nrobot is principled\" as \"Non-applicable to robots in general\" or\n\"Non-applicable to this robot\". In Study 2 we show that the frequency of these\nratings change (indeed, even for items rated as N/A to robots in general) when\na new scenario is presented (game playing with a humanoid robot). Finally,\nwhile overall trust scores remained robust to N/A ratings, our results revealed\npotential fallacies in the way these scores are commonly interpreted. We\nconclude with recommendations for the development, use and results-reporting of\ntrust questionnaires for future studies, as well as theoretical implications\nfor the field of HRI.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 01:26:48 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Chita-Tegmark", "Meia", ""], ["Law", "Theresa", ""], ["Rabb", "Nicholas", ""], ["Scheutz", "Matthias", ""]]}, {"id": "2104.11370", "submitter": "Zheng Wang", "authors": "Zheng Wang", "title": "Analysis and Modeling of Driver Behavior with Integrated Feedback of\n  Visual and Haptic Information Under Shared Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The thesis presents contributions made to the evaluation and design of a\nhaptic guidance system on improving driving performance in cases of normal and\ndegraded visual information, which are based on behavior experiments, modeling\nand numerical simulations. The effect of shared control on driver behavior in\ncases of normal and degraded visual information has been successfully evaluated\nexperimentally and numerically. The evaluation results indicate that the\nproposed haptic guidance system is capable of providing reliable haptic\ninformation, and is effective on improving lane following performance in the\nconditions of visual occlusion from road ahead and declined visual attention\nunder fatigue driving. Moreover, the appropriate degree of haptic guidance is\nhighly related to the reliability of visual information perceived by the\ndriver, which suggests that designing the haptic guidance system based on the\nreliability of visual information would allow for greater driver acceptance.\nFurthermore, the parameterized driver model, which considers the integrated\nfeedback of visual and haptic information, is capable of predicting driver\nbehavior under shared control, and has the potential of being used for\ndesigning and evaluating the haptic guidance system.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 01:41:37 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wang", "Zheng", ""]]}, {"id": "2104.11386", "submitter": "Nam Wook Kim", "authors": "Nam Wook Kim", "title": "Recording Reusable and Guided Analytics From Interaction Histories", "comments": "2 pages, 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of visual analytics tools has gained popularity in various domains,\nhelping users discover meaningful information from complex and large data sets.\nUsers often face difficulty in disseminating the knowledge discovered without\nclear recall of their exploration paths and analysis processes. We introduce a\nvisual analysis tool that allows analysts to record reusable and guided\nanalytics from their interaction logs. To capture the analysis process, we use\na decision tree whose node embeds visualizations and guide to define a visual\nanalysis task. The tool enables analysts to formalize analysis strategies,\nbuild best practices, and guide novices through systematic workflows.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 02:46:00 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Kim", "Nam Wook", ""]]}, {"id": "2104.11483", "submitter": "Sofia Yfantidou", "authors": "Sofia Yfantidou, Pavlos Sermpezis, Athena Vakali", "title": "Self-Tracking Technology for mHealth: A Systematic Review and the PAST\n  SELF Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In today's connected society, many people rely on mHealth and self-tracking\n(ST) technology to help them break their sedentary lifestyle and stay fit.\nHowever, there is scarce evidence of such technological interventions'\neffectiveness, and there are no standardized methods to evaluate the short- and\nlong-term impact of such technologies on people's physical activity and health.\nThis work aims to help ST and HCI practitioners and researchers by empowering\nthem with systematic guidelines and an extensible framework for constructing\nsuch technological interventions. This survey and the proposed design and\nevaluation framework aim to contribute to health behavior change and user\nengagement sustainability. To this end, we conduct a literature review of 117\npapers between 2008 and 2020, which identifies the core ST HCI design methods\nand their efficacy, as well as and the most comprehensive list to date of user\nengagement evaluation metrics for ST. Based on the review's findings, we\npropose the PAST SELF end-to-end framework to facilitate the classification,\ndesign, and evaluation of ST technology. PAST SELF systematically organizes\ncommon methods and guidelines from existing works in ubiquitous ST research.\nHence, it has potential applications in industrial and scientific settings and\ncan be utilized by practitioners and researchers alike.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 08:57:42 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Yfantidou", "Sofia", ""], ["Sermpezis", "Pavlos", ""], ["Vakali", "Athena", ""]]}, {"id": "2104.11485", "submitter": "Xuanwu Yue", "authors": "Xuanwu Yue, Qiao Gu, Deyun Wang, Huamin Qu and Yong Wang", "title": "iQUANT: Interactive Quantitative Investment Using Sparse Regression\n  Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The model-based investing using financial factors is evolving as a principal\nmethod for quantitative investment. The main challenge lies in the selection of\neffective factors towards excess market returns. Existing approaches, either\nhand-picking factors or applying feature selection algorithms, do not\norchestrate both human knowledge and computational power. This paper presents\niQUANT, an interactive quantitative investment system that assists equity\ntraders to quickly spot promising financial factors from initial\nrecommendations suggested by algorithmic models, and conduct a joint refinement\nof factors and stocks for investment portfolio composition. We work closely\nwith professional traders to assemble empirical characteristics of \"good\"\nfactors and propose effective visualization designs to illustrate the\ncollective performance of financial factors, stock portfolios, and their\ninteractions. We evaluate iQUANT through a formal user study, two case studies,\nand expert interviews, using a real stock market dataset consisting of 3000\nstocks times 6000 days times 56 factors.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 09:02:30 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Yue", "Xuanwu", ""], ["Gu", "Qiao", ""], ["Wang", "Deyun", ""], ["Qu", "Huamin", ""], ["Wang", "Yong", ""]]}, {"id": "2104.11637", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee", "title": "Online recognition of unsegmented actions with hierarchical SOM\n  architecture", "comments": null, "journal-ref": "Cogn Process 22, 77-91 (2021)", "doi": "10.1007/s10339-020-00986-4", "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic recognition of an online series of unsegmented actions requires a\nmethod for segmentation that determines when an action starts and when it ends.\nIn this paper, a novel approach for recognizing unsegmented actions in online\ntest experiments is proposed. The method uses self-organizing neural networks\nto build a three-layer cognitive architecture. The unique features of an action\nsequence are represented as a series of elicited key activations by the\nfirst-layer self-organizing map. An average length of a key activation vector\nis calculated for all action sequences in a training set and adjusted in\nlearning trials to generate input patterns to the second-layer self-organizing\nmap. The pattern vectors are clustered in the second layer, and the clusters\nare then labeled by an action identity in the third layer neural network. The\nexperiment results show that although the performance drops slightly in online\nexperiments compared to the offline tests, the ability of the proposed\narchitecture to deal with the unsegmented action sequences as well as the\nonline performance makes the system more plausible and practical in real-case\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 14:41:46 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Gharaee", "Zahra", ""]]}, {"id": "2104.11806", "submitter": "Wengran Wang", "authors": "Wengran Wang, Archit Kwatra, James Skripchuk, Neeloy Gomes, Alexandra\n  Milliken, Chris Martens, Tiffany Barnes, Thomas Price", "title": "Novices' Learning Barriers When Using Code Examples in Open-Ended\n  Programming", "comments": null, "journal-ref": null, "doi": "10.1145/3430665.3456370", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-ended programming increases students' motivation by allowing them to\nsolve authentic problems and connect programming to their own interests.\nHowever, such open-ended projects are also challenging, as they often encourage\nstudents to explore new programming features and attempt tasks that they have\nnot learned before. Code examples are effective learning materials for students\nand are well-suited to supporting open-ended programming. However, there is\nlittle work to understand how novices learn with examples during open-ended\nprogramming, and few real-world deployments of such tools. In this paper, we\nexplore novices' learning barriers when interacting with code examples during\nopen-ended programming. We deployed Example Helper, a tool that offers\ngalleries of code examples to search and use, with 44 novice students in an\nintroductory programming classroom, working on an open-ended project in Snap.\nWe found three high-level barriers that novices encountered when using\nexamples: decision, search and integration barriers. We discuss how these\nbarriers arise and design opportunities to address them.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 20:08:56 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Wengran", ""], ["Kwatra", "Archit", ""], ["Skripchuk", "James", ""], ["Gomes", "Neeloy", ""], ["Milliken", "Alexandra", ""], ["Martens", "Chris", ""], ["Barnes", "Tiffany", ""], ["Price", "Thomas", ""]]}, {"id": "2104.11812", "submitter": "Wengran Wang", "authors": "Wengran Wang, Chenhao Zhang, Andreas Stahlbauer, Gordon Fraser, Thomas\n  Price", "title": "SnapCheck: Automated Testing for Snap Programs", "comments": null, "journal-ref": null, "doi": "10.1145/3430665.3456367", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming environments such as Snap, Scratch, and Processing engage\nlearners by allowing them to create programming artifacts such as apps and\ngames, with visual and interactive output. Learning programming with such a\nmedia-focused context has been shown to increase retention and success rate.\nHowever, assessing these visual, interactive projects requires time and\nlaborious manual effort, and it is therefore difficult to offer automated or\nreal-time feedback to students as they work. In this paper, we introduce\nSnapCheck, a dynamic testing framework for Snap that enables instructors to\nauthor test cases with Condition-Action templates. The goal of SnapCheck is to\nallow instructors or researchers to author property-based test cases that can\nautomatically assess students' interactive programs with high accuracy. Our\nevaluation of SnapCheck on 162 code snapshots from a Pong game assignment in an\nintroductory programming course shows that our automated testing framework\nachieves at least 98% accuracy over all rubric items, showing potentials to use\nSnapCheck for auto-grading and providing formative feedback to students.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 20:18:06 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Wengran", ""], ["Zhang", "Chenhao", ""], ["Stahlbauer", "Andreas", ""], ["Fraser", "Gordon", ""], ["Price", "Thomas", ""]]}, {"id": "2104.11863", "submitter": "Zhibin Niu", "authors": "Zhibin Niu, Junqi Wu, Dawei Cheng, and Jiawan Zhang", "title": "Regshock: Interactive Visual Analytics of Systemic Risk in Financial\n  Networks", "comments": "9 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial regulatory agencies are struggling to manage the systemic risks\nattributed to negative economic shocks. Preventive interventions are prominent\nto eliminate the risks and help to build a more resilient financial system.\nAlthough tremendous efforts have been made to measure multi-risk severity\nlevels, understand the contagion behaviors and other risk management problems,\nthere still lacks a theoretical framework revealing what and how regulatory\nintervention measurements can mitigate systemic risk. Here we demonstrate\nregshock, a practical visual analytical approach to support the exploration and\nevaluation of financial regulation measurements. We propose risk-island, an\nunprecedented risk-centered visualization algorithm to help uncover the risk\npatterns while preserving the topology of financial networks. We further\npropose regshock, a novel visual exploration and assessment approach based on\nthe simulation-intervention-evaluation analysis loop, to provide a heuristic\nsurgical intervention capability for systemic risk mitigation. We evaluate our\napproach through extensive case studies and expert reviews. To our knowledge,\nthis is the first practical systemic method for the financial network\nintervention and risk mitigation problem; our validated approach potentially\nimproves the risk management and control capabilities of financial experts.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 02:53:02 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Niu", "Zhibin", ""], ["Wu", "Junqi", ""], ["Cheng", "Dawei", ""], ["Zhang", "Jiawan", ""]]}, {"id": "2104.12020", "submitter": "Dmitry Alexandrovsky", "authors": "Dmitry Alexandrovsky, Susanne Putze, Alexander Sch\\\"ulke, Rainer\n  Malaka", "title": "Towards Low-burden Responses to Open Questions in VR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective self-reports in VR user studies is a burdening and often tedious\ntask for the participants. To minimize the disruption with the ongoing\nexperience VR research has started to administer the surveying directly inside\nthe virtual environments. However, due to the tedious nature of text-entry in\nVR, most VR surveying tools focus on closed questions with predetermined\nresponses, while open questions with free-text responses remain unexplored.\nThis neglects a crucial part of UX research. To provide guidance on suitable\nself-reporting methods for open questions in VR user studies, this position\npaper presents a comparative study with three text-entry methods in VR and\noutlines future directions towards low-burden qualitative responding.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 20:47:06 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Alexandrovsky", "Dmitry", ""], ["Putze", "Susanne", ""], ["Sch\u00fclke", "Alexander", ""], ["Malaka", "Rainer", ""]]}, {"id": "2104.12032", "submitter": "Jason Hong", "authors": "Jason I. Hong, Yuvraj Agarwal, Matt Fredrikson, Mike Czapik, Shawn\n  Hanna, Swarup Sahoo, Judy Chun, Won-Woo Chung, Aniruddh Iyer, Ally Liu, Shen\n  Lu, Rituparna Roychoudhury, Qian Wang, Shan Wang, Siqi Wang, Vida Zhang,\n  Jessica Zhao, Yuan Jiang, Haojian Jin, Sam Kim, Evelyn Kuo, Tianshi Li,\n  Jinping Liu, Yile Liu, Robert Zhang", "title": "The Design of the User Interfaces for Privacy Enhancements for Android", "comments": "58 pages, 21 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the design and design rationale for the user interfaces for\nPrivacy Enhancements for Android (PE for Android). These UIs are built around\ntwo core ideas, namely that developers should explicitly declare the purpose of\nwhy sensitive data is being used, and these permission-purpose pairs should be\nsplit by first party and third party uses. We also present a taxonomy of\npurposes and ways of how these ideas can be deployed in the existing Android\necosystem.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 22:24:18 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Hong", "Jason I.", ""], ["Agarwal", "Yuvraj", ""], ["Fredrikson", "Matt", ""], ["Czapik", "Mike", ""], ["Hanna", "Shawn", ""], ["Sahoo", "Swarup", ""], ["Chun", "Judy", ""], ["Chung", "Won-Woo", ""], ["Iyer", "Aniruddh", ""], ["Liu", "Ally", ""], ["Lu", "Shen", ""], ["Roychoudhury", "Rituparna", ""], ["Wang", "Qian", ""], ["Wang", "Shan", ""], ["Wang", "Siqi", ""], ["Zhang", "Vida", ""], ["Zhao", "Jessica", ""], ["Jiang", "Yuan", ""], ["Jin", "Haojian", ""], ["Kim", "Sam", ""], ["Kuo", "Evelyn", ""], ["Li", "Tianshi", ""], ["Liu", "Jinping", ""], ["Liu", "Yile", ""], ["Zhang", "Robert", ""]]}, {"id": "2104.12154", "submitter": "Kuldar Taveter", "authors": "Kuldar Taveter and Eliise Marie Taveter", "title": "Case Study on Using Colours in Constructing Emotions by Interactive\n  Digital Narratives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the possibility of supporting the construction of\nemotions in the participants of Interactive Digital Narratives (IDN) by means\nof colours. The article uses goal models for expressing protostories. The core\nof the article consists of the case study where two colour synes-thetes were\nasked to choose colours for eight emotions. Thereafter the same synesthetes\nwere asked to choose colours for the emotions that support the attainment of\ngoals in the Cinderella narrative, which serves as an example protostory. The\narticle also discusses the perspectives for applying the method proposed by us\nto using colours in constructing emotions by IDNs.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 13:22:32 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Taveter", "Kuldar", ""], ["Taveter", "Eliise Marie", ""]]}, {"id": "2104.12182", "submitter": "Jingbo Zhao", "authors": "Jingbo Zhao, Ruize An, Ruolin Xu, Banghao Lin", "title": "Comparing Hand Gestures and the Gamepad Interfaces for Locomotion in\n  Virtual Environments", "comments": "This manuscript has been submitted to a journal for publication\n  consideration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand gesture is a new and promising interface for locomotion in virtual\nenvironments. While several previous studies have proposed different hand\ngestures for virtual locomotion, little is known about their differences in\nterms of performance and user preference in virtual locomotion tasks. In the\npresent paper, we presented three different hand gesture interfaces and their\nalgorithms for locomotion, which are called the Finger Distance gesture, the\nFinger Number gesture and the Finger Tapping gesture. These gestures were\ninspired by previous studies of gesture-based locomotion interfaces and are\ntypical gestures that people are familiar with in their daily lives.\nImplementing these hand gesture interfaces in the present study enabled us to\nsystematically compare the differences between these gestures. In addition, to\ncompare the usability of these gestures to locomotion interfaces using\ngamepads, we also designed and implemented a gamepad interface based on the\nXbox One controller. We compared these four interfaces through two virtual\nlocomotion tasks. These tasks assessed their performance and user preference on\nspeed control and waypoints navigation. Results showed that user preference and\nperformance of the Finger Distance gesture were comparable to that of the\ngamepad interface. The Finger Number gesture also had close performance and\nuser preference to that of the Finger Distance gesture. Our study demonstrates\nthat the Finger Distance gesture and the Finger Number gesture are very\npromising interfaces for virtual locomotion. We also discuss that the Finger\nTapping gesture needs further improvements before it can be used for virtual\nwalking.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 15:25:24 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhao", "Jingbo", ""], ["An", "Ruize", ""], ["Xu", "Ruolin", ""], ["Lin", "Banghao", ""]]}, {"id": "2104.12187", "submitter": "Jing Mu", "authors": "Jing Mu, David B. Grayden, Ying Tan, Denny Oetomo", "title": "Frequency Superposition -- A Multi-Frequency Stimulation Method in\n  SSVEP-based BCIs", "comments": "4 pages, 5 figures. This work has been submitted to the IEEE EMBC for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible. arXiv admin note: text\n  overlap with arXiv:2011.05861", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The steady-state visual evoked potential (SSVEP) is one of the most widely\nused modalities in brain-computer interfaces (BCIs) due to its many advantages.\nHowever, the existence of harmonics and the limited range of responsive\nfrequencies in SSVEP make it challenging to further expand the number of\ntargets without sacrificing other aspects of the interface or putting\nadditional constraints on the system. This paper introduces a novel\nmulti-frequency stimulation method for SSVEP and investigates its potential to\neffectively and efficiently increase the number of targets presented. The\nproposed stimulation method, obtained by the superposition of the stimulation\nsignals at different frequencies, is size-efficient, allows single-step target\nidentification, puts no strict constraints on the usable frequency range, can\nbe suited to self-paced BCIs, and does not require specific light sources. In\naddition to the stimulus frequencies and their harmonics, the evoked SSVEP\nwaveforms include frequencies that are integer linear combinations of the\nstimulus frequencies. Results of decoding SSVEPs collected from nine subjects\nusing canonical correlation analysis (CCA) with only the frequencies and\nharmonics as reference, also demonstrate the potential of using such a\nstimulation paradigm in SSVEP-based BCIs.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 15:56:34 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mu", "Jing", ""], ["Grayden", "David B.", ""], ["Tan", "Ying", ""], ["Oetomo", "Denny", ""]]}, {"id": "2104.12345", "submitter": "Decky Aspandi", "authors": "Nuria Rodriguez-Diaz, Decky Aspandi, Federico Sukno, Xavier Binefa", "title": "Machine Learning-based Lie Detector applied to a Novel Annotated Game\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lie detection is considered a concern for everyone in their day to day life\ngiven its impact on human interactions. Thus, people normally pay attention to\nboth what their interlocutors are saying and also to their visual appearances,\nincluding faces, to try to find any signs that indicate whether the person is\ntelling the truth or not. While automatic lie detection may help us to\nunderstand this lying characteristics, current systems are still fairly\nlimited, partly due to lack of adequate datasets to evaluate their performance\nin realistic scenarios. In this work, we have collected an annotated dataset of\nfacial images, comprising both 2D and 3D information of several participants\nduring a card game that encourages players to lie. Using our collected dataset,\nWe evaluated several types of machine learning-based lie detectors in terms of\ntheir generalization, person-specific and cross-domain experiments. Our results\nshow that models based on deep learning achieve the best accuracy, reaching up\nto 57\\% for the generalization task and 63\\% when dealing with a single\nparticipant. Finally, we also highlight the limitation of the deep learning\nbased lie detector when dealing with cross-domain lie detection tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 04:48:42 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 04:00:37 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Rodriguez-Diaz", "Nuria", ""], ["Aspandi", "Decky", ""], ["Sukno", "Federico", ""], ["Binefa", "Xavier", ""]]}, {"id": "2104.12601", "submitter": "Freddie Hong", "authors": "Freddie Hong, Luca Tendera, Connor Myant, David Boyle", "title": "Vacuum-formed 3D printed electronics: fabrication of thin, rigid and\n  free-form interactive surfaces", "comments": "9 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vacuum-forming is a common manufacturing technique for constructing thin\nplastic shell products by pressing heated plastic sheets onto a mold using\natmospheric pressure. Vacuum-forming is ubiquitous in packaging and casing\nproducts in industry spanning fast moving consumer goods to connected devices.\nIntegrating advanced functionality, which may include sensing, computation and\ncommunication, within thin structures is desirable for various next-generation\ninteractive devices. Hybrid additive manufacturing techniques like\nthermoforming are becoming popular for prototyping freeform electronics given\nits design flexibility, speed and cost-effectiveness. In this paper, we present\na new hybrid method for constructing thin, rigid and free-form interconnected\nsurfaces via fused deposition modelling (FDM) 3D printing and vacuum-forming.\nWhile 3D printing a mold for vacuum-forming has been explored by many,\nutilising 3D printing to construct sheet materials has remains unexplored. 3D\nprinting the sheet material allows embedding conductive traces within thin\nlayers of the substrate, which can be vacuum-formed but remain conductive and\ninsulated. We characterise the behaviour of the vacuum-formed 3D printed sheet,\nanalyse the electrical performance of 3D printed traces after vacuum-forming,\nand showcase a range of examples constructed using the technique. We\ndemonstrate a new design interface specifically for designing conformal\ninterconnects, which allows designers to draw conductive patterns in 3D and\nexport pre-distorted sheet models ready to be 3D printed.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:03:33 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 10:25:35 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Hong", "Freddie", ""], ["Tendera", "Luca", ""], ["Myant", "Connor", ""], ["Boyle", "David", ""]]}, {"id": "2104.12653", "submitter": "Kerstin Bongard-Blanchy Dr", "authors": "Kerstin Bongard-Blanchy, Arianna Rossi, Salvador Rivas, Sophie\n  Doublet, Vincent Koenig, Gabriele Lenzini", "title": "I am Definitely Manipulated, Even When I am Aware of it. It s\n  Ridiculous! -- Dark Patterns from the End-User Perspective", "comments": "ACM DIS Conference on Designing interactive systems, June 28 - July\n  2, 2021, online. ACM, New York, NY, USA", "journal-ref": null, "doi": "10.1145/3461778.3462086", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online services pervasively employ manipulative designs (i.e., dark patterns)\nto influence users to purchase goods and subscriptions, spend more time\non-site, or mindlessly accept the harvesting of their personal data. To protect\nusers from the lure of such designs, we asked: are users aware of the presence\nof dark patterns? If so, are they able to resist them? By surveying 406\nindividuals, we found that they are generally aware of the influence that\nmanipulative designs can exert on their online behaviour. However, being aware\ndoes not equip users with the ability to oppose such influence. We further find\nthat respondents, especially younger ones, often recognise the \"darkness\" of\ncertain designs, but remain unsure of the actual harm they may suffer. Finally,\nwe discuss a set of interventions (e.g., bright patterns, design frictions,\ntraining games, applications to expedite legal enforcement) in the light of our\nfindings.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:30:22 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 06:31:05 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Bongard-Blanchy", "Kerstin", ""], ["Rossi", "Arianna", ""], ["Rivas", "Salvador", ""], ["Doublet", "Sophie", ""], ["Koenig", "Vincent", ""], ["Lenzini", "Gabriele", ""]]}, {"id": "2104.12675", "submitter": "Henry Turner", "authors": "Henry Turner and Simon Eberz and Ivan Martinovic", "title": "Recurring Turking: Conducting Daily Task Studies on Mechanical Turk", "comments": "15 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present our system design for conducting recurring daily\nstudies on Amazon Mechanical Turk. We implement this system to conduct a study\ninto touch dynamics, and present our experiences, challenges and lessons\nlearned from doing so. Study participants installed our application on their\nApple iOS phones and completed two tasks daily for 31 days. Each task involves\nperforming a series of scrolling or swiping gestures, from which behavioral\ninformation such as movement speed or pressure is extracted.\n  Taking place over a time period of 31 days, our study utilized a\nself-contained app which workers used to complete daily tasks without requiring\nextra HITs. This differs somewhat from the typical rapid completion of one-off\ntasks on Amazon Mechanical Turk. This atypical use of the platform prompted us\nto study aspects related to long-term user retention over the study period:\npayment schedule (amount and structure over time), regular notifications,\npayment satisfaction and overall satisfaction. We also investigate the specific\nconcern of reconciling informed consent with workers' desire to complete tasks\nquickly.\n  We find that using the Mechanical Turk platform in this way leads to data of\ncomparable quality to that of lab based studies, and that our study design\nchoices show a statistically significant effect in keeping workers engaged.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 16:03:16 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Turner", "Henry", ""], ["Eberz", "Simon", ""], ["Martinovic", "Ivan", ""]]}, {"id": "2104.12816", "submitter": "Mohammed Elbadry", "authors": "Sarah Baig and Mohammed Elbadry", "title": "Survey: Vitals Screening Techniques for a Safer Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With COVID-19 disrupting operations across various sectors of the workforce\n(e.g., offices, airports, libraries, schools), preventative measures enabling\nresumption of work are quickly becoming a necessity. In this paper, we present\nthe need for Vitals Screening Techniques (VIST) where more than one vital is\nscreened to ensure the safety of the population (e.g. temperature, heart rate,\nand blood oxygen levels). VIST can be deployed in crowded environments to\nprovide the new necessary layer of safety. We provide extensive coverage of\nstate-of-art technology that can assist in tackling this emerging problem, and\nevaluate one of the existing products on the market that employ VIST.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:16:28 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Baig", "Sarah", ""], ["Elbadry", "Mohammed", ""]]}, {"id": "2104.12891", "submitter": "Denae Ford", "authors": "Yu Huang, Denae Ford, Thomas Zimmermann", "title": "Leaving My Fingerprints: Motivations and Challenges of Contributing to\n  OSS for Social Good", "comments": "13 pages, 6 tables", "journal-ref": "International Conference on Software Engineering (ICSE) 2021", "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When inspiring software developers to contribute to open source software, the\nact is often referenced as an opportunity to build tools to support the\ndeveloper community. However, that is not the only charge that propels\ncontributions -- growing interest in open source has also been attributed to\nsoftware developers deciding to use their technical skills to benefit a common\nsocietal good. To understand how developers identify these projects, their\nmotivations for contributing, and challenges they face, we conducted 21\nsemi-structured interviews with OSS for Social Good (OSS4SG) contributors. From\nour interview analysis, we identified themes of contribution styles that we\nwanted to understand at scale by deploying a survey to over 5765 OSS and Open\nSource Software for Social Good contributors. From our quantitative analysis of\n517 responses, we find that the majority of contributors demonstrate a\ndistinction between OSS4SG and OSS. Likewise, contributors described\ndefinitions based on what societal issue the project was to mitigate and who\nthe outcomes of the project were going to benefit. In addition, we find that\nOSS4SG contributors focus less on benefiting themselves by padding their resume\nwith new technology skills and are more interested in leaving their mark on\nsociety at statistically significant levels. We also find that OSS4SG\ncontributors evaluate the owners of the project significantly more than OSS\ncontributors. These findings inform implications to help contributors identify\nhigh societal impact projects, help project maintainers reduce barriers to\nentry, and help organizations understand why contributors are drawn to these\nprojects to sustain active participation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 21:50:11 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Huang", "Yu", ""], ["Ford", "Denae", ""], ["Zimmermann", "Thomas", ""]]}, {"id": "2104.12920", "submitter": "Kenneth Holstein", "authors": "Kenneth Holstein and Shayan Doroudi", "title": "Equity and Artificial Intelligence in Education: Will \"AIEd\" Amplify or\n  Alleviate Inequities in Education?", "comments": "The co-first authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of educational AI (AIEd) systems has often been motivated by\ntheir potential to promote educational equity and reduce achievement gaps\nacross different groups of learners -- for example, by scaling up the benefits\nof one-on-one human tutoring to a broader audience, or by filling gaps in\nexisting educational services. Given these noble intentions, why might AIEd\nsystems have inequitable impacts in practice? In this chapter, we discuss four\nlenses that can be used to examine how and why AIEd systems risk amplifying\nexisting inequities. Building from these lenses, we then outline possible paths\ntowards more equitable futures for AIEd, while highlighting debates surrounding\neach proposal. In doing so, we hope to provoke new conversations around the\ndesign of equitable AIEd, and to push ongoing conversations in the field\nforward.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 00:28:38 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Holstein", "Kenneth", ""], ["Doroudi", "Shayan", ""]]}, {"id": "2104.12964", "submitter": "Zawar Hussain Mr", "authors": "Zawar Hussain, Quan Z. Sheng, Wei Emma Zhang, Jorge Ortiz, Seyedamin\n  Pouriyeh", "title": "A Review of the Non-Invasive Techniques for Monitoring Different Aspects\n  of Sleep", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quality sleep is very important for a healthy life. Nowadays, many people\naround the world are not getting enough sleep which is having negative impacts\non their lifestyles. Studies are being conducted for sleep monitoring and have\nnow become an important tool for understanding sleep behavior. The gold\nstandard method for sleep analysis is polysomnography (PSG) conducted in a\nclinical environment but this method is both expensive and complex for\nlong-term use. With the advancements in the field of sensors and the\nintroduction of off-the-shelf technologies, unobtrusive solutions are becoming\ncommon as alternatives for in-home sleep monitoring. Various solutions have\nbeen proposed using both wearable and non-wearable methods which are cheap and\neasy to use for in-home sleep monitoring. In this paper, we present a\ncomprehensive survey of the latest research works (2015 and after) conducted in\nvarious categories of sleep monitoring including sleep stage classification,\nsleep posture recognition, sleep disorders detection, and vital signs\nmonitoring. We review the latest works done using the non-invasive approach and\ncover both wearable and non-wearable methods. We discuss the design approaches\nand key attributes of the work presented and provide an extensive analysis\nbased on 10 key factors, to give a comprehensive overview of the recent\ndevelopments and trends in all four categories of sleep monitoring. We also\npresent some publicly available datasets for different categories of sleep\nmonitoring. In the end, we discuss several open issues and provide future\nresearch directions in the area of sleep monitoring.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 04:12:43 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Hussain", "Zawar", ""], ["Sheng", "Quan Z.", ""], ["Zhang", "Wei Emma", ""], ["Ortiz", "Jorge", ""], ["Pouriyeh", "Seyedamin", ""]]}, {"id": "2104.13259", "submitter": "Maurice Jakesch", "authors": "Maurice Jakesch, Kiran Garimella, Dean Eckles, Mor Naaman", "title": "#Trend Alert: How a Cross-Platform Organization Manipulated Twitter\n  Trends in the Indian General Election", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Political organizations worldwide keep innovating their use of social media\ntechnologies. Here, we document a novel configuration of technologies and\norganizational forms used to manipulate Twitter trends in the 2019 Indian\ngeneral election. The organizers rely on an extensive network of WhatsApp\ngroups to coordinate mass-postings by loosely affiliated political supporters.\nTo investigate the campaigns, we joined more than 600 political WhatsApp groups\nthat support the Bharatiya Janata Party, the right-wing party that won the\ngeneral election. We found direct evidence of 75 hashtag manipulation\ncampaigns, including mobilization messages and lists of pre-written tweets. We\nestimate the campaigns' size and whether they succeeded in creating controlled\nsocial media narratives. We show that the campaigns are smaller than what media\nreports suggest; still, they reliably produce Twitter trends drawing on the\nvoices of loosely affiliated supporters. Centrally controlled but voluntary in\nparticipation, this novel configuration of a campaign complicates the debates\nover the legitimate use of digital tools for political participation. It may\nhave provided a blueprint for participatory media manipulation by a party with\npopular support.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 15:25:32 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Jakesch", "Maurice", ""], ["Garimella", "Kiran", ""], ["Eckles", "Dean", ""], ["Naaman", "Mor", ""]]}, {"id": "2104.13266", "submitter": "Filipe Calegario", "authors": "Filipe Calegario and Jo\\~ao Tragtenberg and Giordano Cabral and Geber\n  Ramalho", "title": "Batebit Controller: Popularizing Digital Musical Instruments Development\n  Process", "comments": "2 pages, 2 figures, 17th Brazilian Symposium on Computer Music", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present an ongoing research project related to popularizing\nthe mindset of building new digital musical instruments. We developed a\nphysical kit and software intended to provide beginner users with the first\ngrasp on the development process of a digital musical instrument. We expect\nthat, by using the kit and the software, the users could experiment in a short\nperiod the various steps in developing a DMI such as physical structure,\nelectronics, programming, mapping, and sound design. Our approach to\npopularizing the DMI development process is twofold: reducing the cognitive\nload for beginners by encapsulating technical details and lowering the costs of\nthe kit by using simple components and open-source software. In the end, we\nexpect that by increasing the interest of beginners in the building process of\ndigital musical instruments, we could make the community of new interfaces for\nmusical expression stronger.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 15:34:53 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Calegario", "Filipe", ""], ["Tragtenberg", "Jo\u00e3o", ""], ["Cabral", "Giordano", ""], ["Ramalho", "Geber", ""]]}, {"id": "2104.13327", "submitter": "Paulo Knob", "authors": "Paulo Knob, Willian S. Dias, Natanael Kuniechick, Joao Moraes, Soraia\n  Raupp Musse", "title": "Arthur: a new ECA that uses Memory to improve Communication", "comments": "8 pages, 4 figures, ICSC 2021", "journal-ref": "2021 IEEE 15th International Conference on Semantic Computing\n  (ICSC)", "doi": "10.1109/ICSC50631.2021.00036", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article proposes an embodied conversational agent named Arthur. In\naddition to being able to talk to a person (using text and voice), he is also\nable to recognize the person he is talking to and detect his/her expressed\nemotion through facial expressions. Arthur uses these skills to improve\ncommunication with the user, also using his artificial memory, which stores and\nretrieves data about events and facts, based on a human memory model. We\nconducted some experiments to collect quantitative and qualitative information,\nwhich show that our model provides a consistent impact on users.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 16:56:15 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Knob", "Paulo", ""], ["Dias", "Willian S.", ""], ["Kuniechick", "Natanael", ""], ["Moraes", "Joao", ""], ["Musse", "Soraia Raupp", ""]]}, {"id": "2104.13406", "submitter": "Eda Okur", "authors": "Saurav Sahay, Eda Okur, Nagib Hakim, Lama Nachman", "title": "Semi-supervised Interactive Intent Labeling", "comments": "NAACL 2021 - Workshop on Data Science with Human-in-the-loop:\n  Language Advances (DaSH-LA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building the Natural Language Understanding (NLU) modules of task-oriented\nSpoken Dialogue Systems (SDS) involves a definition of intents and entities,\ncollection of task-relevant data, annotating the data with intents and\nentities, and then repeating the same process over and over again for adding\nany functionality/enhancement to the SDS. In this work, we showcase an Intent\nBulk Labeling system where SDS developers can interactively label and augment\ntraining data from unlabeled utterance corpora using advanced clustering and\nvisual labeling methods. We extend the Deep Aligned Clustering work with a\nbetter backbone BERT model, explore techniques to select the seed data for\nlabeling, and develop a data balancing method using an oversampling technique\nthat utilizes paraphrasing models. We also look at the effect of data\naugmentation on the clustering process. Our results show that we can achieve\nover 10% gain in clustering accuracy on some datasets using the combination of\nthe above techniques. Finally, we extract utterance embeddings from the\nclustering model and plot the data to interactively bulk label the samples,\nreducing the time and effort for data labeling of the whole dataset\nsignificantly.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 18:06:55 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 02:01:51 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Sahay", "Saurav", ""], ["Okur", "Eda", ""], ["Hakim", "Nagib", ""], ["Nachman", "Lama", ""]]}, {"id": "2104.13514", "submitter": "Brooke Krajancich", "authors": "Brooke Krajancich, Petr Kellnhofer, Gordon Wetzstein", "title": "A Perceptual Model for Eccentricity-dependent Spatio-temporal Flicker\n  Fusion and its Applications to Foveated Graphics", "comments": null, "journal-ref": "ACM Trans. Graph. 40, 4, Article 47 (August 2021), 11 pages", "doi": "10.1145/3450626.3459784", "report-no": null, "categories": "cs.HC cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual and augmented reality (VR/AR) displays strive to provide a\nresolution, framerate and field of view that matches the perceptual\ncapabilities of the human visual system, all while constrained by limited\ncompute budgets and transmission bandwidths of wearable computing systems.\nFoveated graphics techniques have emerged that could achieve these goals by\nexploiting the falloff of spatial acuity in the periphery of the visual field.\nHowever, considerably less attention has been given to temporal aspects of\nhuman vision, which also vary across the retina. This is in part due to\nlimitations of current eccentricity-dependent models of the visual system. We\nintroduce a new model, experimentally measuring and computationally fitting\neccentricity-dependent critical flicker fusion thresholds jointly for both\nspace and time. In this way, our model is unique in enabling the prediction of\ntemporal information that is imperceptible for a certain spatial frequency,\neccentricity, and range of luminance levels. We validate our model with an\nimage quality user study, and use it to predict potential bandwidth savings 7x\nhigher than those afforded by current spatial-only foveated models. As such,\nthis work forms the enabling foundation for new temporally foveated graphics\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 00:51:14 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 08:12:08 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 14:31:16 GMT"}, {"version": "v4", "created": "Wed, 26 May 2021 07:15:21 GMT"}, {"version": "v5", "created": "Sun, 20 Jun 2021 03:53:06 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Krajancich", "Brooke", ""], ["Kellnhofer", "Petr", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2104.13695", "submitter": "Ga\\\"el Poux-M\\'edard", "authors": "Ga\\\"el Poux-M\\'edard and Julien Velcin and Sabine Loudcher", "title": "Information Interaction Profile of Choice Adoption", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interactions between pieces of information (entities) play a substantial role\nin the way an individual acts on them: adoption of a product, the spread of\nnews, strategy choice, etc. However, the underlying interaction mechanisms are\noften unknown and have been little explored in the literature. We introduce an\nefficient method to infer both the entities interaction network and its\nevolution according to the temporal distance separating interacting entities;\ntogether, they form the interaction profile. The interaction profile allows\ncharacterizing the mechanisms of the interaction processes. We approach this\nproblem via a convex model based on recent advances in multi-kernel inference.\nWe consider an ordered sequence of exposures to entities (URL, ads, situations)\nand the actions the user exerts on them (share, click, decision). We study how\nusers exhibit different behaviors according to combinations of exposures they\nhave been exposed to. We show that the effect of a combination of exposures on\na user is more than the sum of each exposure's independent effect--there is an\ninteraction. We reduce this modeling to a non-parametric convex optimization\nproblem that can be solved in parallel. Our method recovers state-of-the-art\nresults on interaction processes on three real-world datasets and outperforms\nbaselines in the inference of the underlying data generation mechanisms.\nFinally, we show that interaction profiles can be visualized intuitively,\neasing the interpretation of the model.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 10:42:25 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Poux-M\u00e9dard", "Ga\u00ebl", ""], ["Velcin", "Julien", ""], ["Loudcher", "Sabine", ""]]}, {"id": "2104.13823", "submitter": "Stephan Schl\\\"ogl PhD", "authors": "Natascha Mariacher, Stephan Schl\\\"ogl, Alexander Monz", "title": "Investigating Perceptions of Social Intelligence in Simulated\n  Human-Chatbot Interactions", "comments": "18 pages", "journal-ref": null, "doi": "10.1007/978-981-15-5093-5_44", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ongoing penetration of conversational user interfaces, a better\nunderstanding of social and emotional characteristic inherent to dialogue is\nrequired. Chatbots in particular face the challenge of conveying human-like\nbehaviour while being restricted to one channel of interaction, i.e., text. The\ngoal of the presented work is thus to investigate whether characteristics of\nsocial intelligence embedded in human-chatbot interactions are perceivable by\nhuman interlocutors and if yes, whether such influences the experienced\ninteraction quality. Focusing on the social intelligence dimensions\nAuthenticity, Clarity and Empathy, we first used a questionnaire survey\nevaluating the level of perception in text utterances, and then conducted a\nWizard of Oz study to investigate the effects of these utterances in a more\ninteractive setting. Results show that people have great difficulties\nperceiving elements of social intelligence in text. While on the one hand they\nfind anthropomorphic behaviour pleasant and positive for the naturalness of a\ndialogue, they may also perceive it as frightening and unsuitable when\nexpressed by an artificial agent in the wrong way or at the wrong time.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:13:03 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Mariacher", "Natascha", ""], ["Schl\u00f6gl", "Stephan", ""], ["Monz", "Alexander", ""]]}, {"id": "2104.13836", "submitter": "Stephan Schl\\\"ogl PhD", "authors": "Luisa Brinkschulte, Natascha Mariacher, Stephan Schl\\\"ogl, Mar\\'ia\n  In\\'es Torres, Raquel Justo, Javier Mikel Olaso, Anna Esposito, Gennaro\n  Cordasco, G\\'erard Chollet, Cornelius Glackin, Colin Pickard, Dijana\n  Petrovska-Delacretaz, Mohamed Amine Hmani, Ayment Mtibaa, Ana\\\"is Fernandez,\n  Daria Kyslitska, Bego\\~na Fernandez-Ruanova, Jofre Tenorio-Laranga, Mari\n  Aksnes, Maria Stylianou Korsnes, Miriam Reiner, Fredrik Lindner, Olivier\n  Deroo, Olga Gordeeva", "title": "The EMPATHIC Project: Building an Expressive, Advanced Virtual Coach to\n  Improve Independent Healthy-Life-Years of the Elderly", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines the EMPATHIC Research & Innovation project, which aims to\nresearch, innovate, explore and validate new interaction paradigms and\nplat-forms for future generations of Personalized Virtual Coaches to assist\nelderly people living independently at and around their home. Innovative\nmultimodal face analytics, adaptive spoken dialogue systems, and natural\nlanguage inter-faces are part of what the project investigates and innovates,\naiming to help dependent aging persons and their carers. It will uses remote,\nnon-intrusive technologies to extract physiological markers of emotional states\nand adapt respective coach responses. In doing so, it aims to develop causal\nmodels for emotionally believable coach-user interactions, which shall engage\nelders and thus keep off loneliness, sustain health, enhance quality of life,\nand simplify access to future telecare services. Through measurable end-user\nvalidations performed in Spain, Norway and France (and complementary user\nevaluations in Italy), the proposed methods and solutions will have to\ndemonstrate useful-ness, reliability, flexibility and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:39:58 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Brinkschulte", "Luisa", ""], ["Mariacher", "Natascha", ""], ["Schl\u00f6gl", "Stephan", ""], ["Torres", "Mar\u00eda In\u00e9s", ""], ["Justo", "Raquel", ""], ["Olaso", "Javier Mikel", ""], ["Esposito", "Anna", ""], ["Cordasco", "Gennaro", ""], ["Chollet", "G\u00e9rard", ""], ["Glackin", "Cornelius", ""], ["Pickard", "Colin", ""], ["Petrovska-Delacretaz", "Dijana", ""], ["Hmani", "Mohamed Amine", ""], ["Mtibaa", "Ayment", ""], ["Fernandez", "Ana\u00efs", ""], ["Kyslitska", "Daria", ""], ["Fernandez-Ruanova", "Bego\u00f1a", ""], ["Tenorio-Laranga", "Jofre", ""], ["Aksnes", "Mari", ""], ["Korsnes", "Maria Stylianou", ""], ["Reiner", "Miriam", ""], ["Lindner", "Fredrik", ""], ["Deroo", "Olivier", ""], ["Gordeeva", "Olga", ""]]}, {"id": "2104.13889", "submitter": "Arash Tavakoli", "authors": "Arash Tavakoli, Shashwat Kumar, Mehdi Boukhechba, and Arsalan\n  Heydarian", "title": "Driver State and Behavior Detection Through Smart Wearables", "comments": "Accepted in IEEE Intelligent Vehicles Symposium 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating driver, in-cabin, and outside environment's contextual cues into\nthe vehicle's decision making is the centerpiece of semi-automated vehicle\nsafety. Multiple systems have been developed for providing context to the\nvehicle, which often rely on video streams capturing drivers' physical and\nenvironmental states. While video streams are a rich source of information,\ntheir ability in providing context can be challenging in certain situations,\nsuch as low illuminance environments (e.g., night driving), and they are highly\nprivacy-intrusive. In this study, we leverage passive sensing through\nsmartwatches for classifying elements of driving context. Specifically, through\nusing the data collected from 15 participants in a naturalistic driving study,\nand by using multiple machine learning algorithms such as random forest, we\nclassify driver's activities (e.g., using phone and eating), outside events\n(e.g., passing intersection and changing lane), and outside road attributes\n(e.g., driving in a city versus a highway) with an average F1 score of 94.55,\n98.27, and 97.86 % respectively, through 10-fold cross-validation. Our results\nshow the applicability of multimodal data retrieved through smart wearable\ndevices in providing context in real-world driving scenarios and pave the way\nfor a better shared autonomy and privacy-aware driving data-collection,\nanalysis, and feedback for future autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:21:30 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Tavakoli", "Arash", ""], ["Kumar", "Shashwat", ""], ["Boukhechba", "Mehdi", ""], ["Heydarian", "Arsalan", ""]]}, {"id": "2104.14089", "submitter": "Ronal Singh", "authors": "Ronal Singh, Tim Miller, Darryn Reid", "title": "Collaborative Human-Agent Planning for Resilience", "comments": "International Workshop on Coordination, Organizations, Institutions,\n  Norms and Ethics for Governance of Multi-Agent Systems (COINE), co-located\n  with AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent agents powered by AI planning assist people in complex scenarios,\nsuch as managing teams of semi-autonomous vehicles. However, AI planning models\nmay be incomplete, leading to plans that do not adequately meet the stated\nobjectives, especially in unpredicted situations. Humans, who are apt at\nidentifying and adapting to unusual situations, may be able to assist planning\nagents in these situations by encoding their knowledge into a planner at\nrun-time. We investigate whether people can collaborate with agents by\nproviding their knowledge to an agent using linear temporal logic (LTL) at\nrun-time without changing the agent's domain model. We presented 24\nparticipants with baseline plans for situations in which a planner had\nlimitations, and asked the participants for workarounds for these limitations.\nWe encoded these workarounds as LTL constraints. Results show that\nparticipants' constraints improved the expected return of the plans by 10% ($p\n< 0.05$) relative to baseline plans, demonstrating that human insight can be\nused in collaborative planning for resilience. However, participants used more\ndeclarative than control constraints over time, but declarative constraints\nproduced plans less similar to the expectation of the participants, which could\nlead to potential trust issues.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 03:21:31 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Singh", "Ronal", ""], ["Miller", "Tim", ""], ["Reid", "Darryn", ""]]}, {"id": "2104.14315", "submitter": "Zhenlv Lv", "authors": "Zhenlv Lv, Juan Liu, Liangfa Xu", "title": "A multi-plane augmented reality head-up display system based on volume\n  holographic optical elements with large area", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional head-up display (HUD) system has the disadvantages of a small\narea and a single display plane, here we propose and design an augmented\nreality (AR) HUD system with multi-plane, large area, high diffraction\nefficiency and a single picture generation unit (PGU) based on holographic\noptical elements (HOEs). Since volume HOEs have excellent angle selectivity and\nwavelength selectivity, HOEs of different wavelengths can be designed to\ndisplay images in different planes. Experimental and simulated results verify\nthe feasibility of this method. Experimental results show that the diffraction\nefficiencies of the red, green and blue HOEs are 75.2%, 73.1% and 67.5%. And\nthe size of HOEs is 20cm*15cm. Moreover, the three HOEs of red, green and blue\ndisplay images at different depths of 150cm, 500cm and 1000cm, respectively. In\naddition, the field of view (FOV) and eye-box (EB) of the system are\n12{\\deg}*10{\\deg} and 9.5cm*11.2cm. Furthermore, the light transmittance of the\nsystem has reached 60%. It is believed that this technique can be applied to\nthe augmented reality navigation display of vehicles and aviation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 12:24:25 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Lv", "Zhenlv", ""], ["Liu", "Juan", ""], ["Xu", "Liangfa", ""]]}, {"id": "2104.14595", "submitter": "Hannah Bast", "authors": "Hannah Bast, Johannes Kalmbach, Theresa Klumpp, Florian Kramer, Niklas\n  Schnelle", "title": "Efficient SPARQL Autocompletion via SPARQL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to achieve fast autocompletion for SPARQL queries on very large\nknowledge bases. At any position in the body of a SPARQL query, the\nautocompletion suggests matching subjects, predicates, or objects. The\nsuggestions are context-sensitive in the sense that they lead to a non-empty\nresult and are ranked by their relevance to the part of the query already\ntyped. The suggestions can be narrowed down by prefix search on the names and\naliases of the desired subject, predicate, or object. All suggestions are\nthemselves obtained via SPARQL queries, which we call autocompletion queries.\nFor existing SPARQL engines, these queries are impractically slow on large\nknowledge bases. We present various algorithmic and engineering improvements of\nan existing SPARQL engine such that these autocompletion queries are executed\nefficiently. We provide an extensive evaluation of a variety of suggestion\nmethods on three large knowledge bases, including Wikidata (6.9B triples). We\nexplore the trade-off between the relevance of the suggestions and the\nprocessing time of the autocompletion queries. We compare our results with two\nwidely used SPARQL engines, Virtuoso and Blazegraph. On Wikidata, we achieve\nfully sensitive suggestions with sub-second response times for over 90% of a\nlarge and diverse set of thousands of autocompletion queries. Materials for\nfull reproducibility, an interactive evaluation web app, and a demo are\navailable on: https://ad.informatik.uni-freiburg.de/publications .\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:29:39 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Bast", "Hannah", ""], ["Kalmbach", "Johannes", ""], ["Klumpp", "Theresa", ""], ["Kramer", "Florian", ""], ["Schnelle", "Niklas", ""]]}, {"id": "2104.14699", "submitter": "Ana Paula Chaves", "authors": "Ana Paula Chaves and Marco Aurelio Gerosa", "title": "Why should we care about register? Reflections on chatbot language\n  design", "comments": "arXiv admin note: text overlap with arXiv:2101.11089", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This position paper discusses the relevance of register as a theoretical\nframework for chatbot language design. We present the concept of register and\ndiscuss how using register-specific language influence the user's perceptions\nof the interaction with chatbots. Additionally, we point several research\nopportunities that are important to pursue to establish register as a\nfoundation for advancing chatbot's communication skills.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 23:47:13 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Chaves", "Ana Paula", ""], ["Gerosa", "Marco Aurelio", ""]]}, {"id": "2104.14810", "submitter": "Guy Marshall", "authors": "Guy Clarke Marshall and Caroline Jay and Andre Freitas", "title": "Structuralist analysis for neural network system diagrams", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This short paper examines diagrams describing neural network systems in\nacademic conference proceedings. Many aspects of scholarly communication are\ncontrolled, particularly with relation to text and formatting, but often\ndiagrams are not centrally curated beyond a peer review. Using a corpus-based\napproach, we argue that the heterogeneous diagrammatic notations used for\nneural network systems has implications for signification in this domain. We\ndivide this into (i) what content is being represented and (ii) how relations\nare encoded. Using a novel structuralist framework, we use a corpus analysis to\nquantitatively cluster diagrams according to the author's representational\nchoices. This quantitative diagram classification in a heterogeneous domain may\nprovide a foundation for further analysis.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:50:19 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Marshall", "Guy Clarke", ""], ["Jay", "Caroline", ""], ["Freitas", "Andre", ""]]}, {"id": "2104.14811", "submitter": "Guy Marshall", "authors": "Guy Clarke Marshall and Caroline Jay and Andre Freitas", "title": "Scholarly AI system diagrams as an access point to mental models", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Complex systems, such as Artificial Intelligence (AI) systems, are comprised\nof many interrelated components. In order to represent these systems,\ndemonstrating the relations between components is essential. Perhaps because of\nthis, diagrams, as \"icons of relation\", are a prevalent medium for signifying\ncomplex systems. Diagrams used to communicate AI system architectures are\ncurrently extremely varied. The diversity in diagrammatic conceptual modelling\nchoices provides an opportunity to gain insight into the aspects which are\nbeing prioritised for communication. In this philosophical exploration of AI\nsystems diagrams, we integrate theories of conceptual models, communication\ntheory, and semiotics. We discuss consequences of standardised diagrammatic\nlanguages for AI systems, concluding that while we expect engineers\nimplementing systems to benefit from standards, researchers would have a larger\nbenefit from guidelines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:55:18 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Marshall", "Guy Clarke", ""], ["Jay", "Caroline", ""], ["Freitas", "Andre", ""]]}, {"id": "2104.14870", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee", "title": "Action in Mind: A Neural Network Approach to Action Recognition and\n  Segmentation", "comments": "Lund University Cognitive Science 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recognizing and categorizing human actions is an important task with\napplications in various fields such as human-robot interaction, video analysis,\nsurveillance, video retrieval, health care system and entertainment industry.\nThis thesis presents a novel computational approach for human action\nrecognition through different implementations of multi-layer architectures\nbased on artificial neural networks. Each system level development is designed\nto solve different aspects of the action recognition problem including online\nreal-time processing, action segmentation and the involvement of objects. The\nanalysis of the experimental results are illustrated and described in six\narticles. The proposed action recognition architecture of this thesis is\ncomposed of several processing layers including a preprocessing layer, an\nordered vector representation layer and three layers of neural networks. It\nutilizes self-organizing neural networks such as Kohonen feature maps and\ngrowing grids as the main neural network layers. Thus the architecture presents\na biological plausible approach with certain features such as topographic\norganization of the neurons, lateral interactions, semi-supervised learning and\nthe ability to represent high dimensional input space in lower dimensional\nmaps. For each level of development the system is trained with the input data\nconsisting of consecutive 3D body postures and tested with generalized input\ndata that the system has never met before. The experimental results of\ndifferent system level developments show that the system performs well with\nquite high accuracy for recognizing human actions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 09:53:28 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Gharaee", "Zahra", ""]]}, {"id": "2104.14961", "submitter": "Janet Rafner", "authors": "Janet Rafner, Miroslav Gajdacz, Gitte Kragh, Arthur Hjorth, Anna\n  Gander, Blanka Palfi, Aleks Berditchevskaia, Fran\\c{c}ois Grey, Kobi Gal, Avi\n  Segal, Mike Walmsley, Josh Aaron Miller, Dominik Dellerman, Muki Haklay,\n  Pietro Michelucci, Jacob Sherson", "title": "Revisiting Citizen Science Through the Lens of Hybrid Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) can augment and sometimes even replace human\ncognition. Inspired by efforts to value human agency alongside productivity, we\ndiscuss the benefits of solving Citizen Science (CS) tasks with Hybrid\nIntelligence (HI), a synergetic mixture of human and artificial intelligence.\nCurrently there is no clear framework or methodology on how to create such an\neffective mixture. Due to the unique participant-centered set of values and the\nabundance of tasks drawing upon both human common sense and complex 21st\ncentury skills, we believe that the field of CS offers an invaluable testbed\nfor the development of HI and human-centered AI of the 21st century, while\nbenefiting CS as well. In order to investigate this potential, we first relate\nCS to adjacent computational disciplines. Then, we demonstrate that CS projects\ncan be grouped according to their potential for HI-enhancement by examining two\nkey dimensions: the level of digitization and the amount of knowledge or\nexperience required for participation. Finally, we propose a framework for\ntypes of human-AI interaction in CS based on established criteria of HI. This\n\"HI lens\" provides the CS community with an overview of several ways to utilize\nthe combination of AI and human intelligence in their projects. It also allows\nthe AI community to gain ideas on how developing AI in CS projects can further\ntheir own field.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 12:55:44 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Rafner", "Janet", ""], ["Gajdacz", "Miroslav", ""], ["Kragh", "Gitte", ""], ["Hjorth", "Arthur", ""], ["Gander", "Anna", ""], ["Palfi", "Blanka", ""], ["Berditchevskaia", "Aleks", ""], ["Grey", "Fran\u00e7ois", ""], ["Gal", "Kobi", ""], ["Segal", "Avi", ""], ["Walmsley", "Mike", ""], ["Miller", "Josh Aaron", ""], ["Dellerman", "Dominik", ""], ["Haklay", "Muki", ""], ["Michelucci", "Pietro", ""], ["Sherson", "Jacob", ""]]}, {"id": "2104.15040", "submitter": "Ruth Hoffmann", "authors": "Joan Espasa, Ian P. Gent, Ruth Hoffmann, Christopher Jefferson, Alice\n  M. Lynch", "title": "Using Small MUSes to Explain How to Solve Pen and Paper Puzzles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pen and paper puzzles like Sudoku, Futoshiki and Skyscrapers are hugely\npopular. Solving such puzzles can be a trivial task for modern AI systems.\nHowever, most AI systems solve problems using a form of backtracking, while\npeople try to avoid backtracking as much as possible. This means that existing\nAI systems do not output explanations about their reasoning that are meaningful\nto people. We present Demystify, a tool which allows puzzles to be expressed in\na high-level constraint programming language and uses MUSes to allow us to\nproduce descriptions of steps in the puzzle solving. We give several\nimprovements to the existing techniques for solving puzzles with MUSes, which\nallow us to solve a range of significantly more complex puzzles and give higher\nquality explanations. We demonstrate the effectiveness and generality of\nDemystify by comparing its results to documented strategies for solving a range\nof pen and paper puzzles by hand, showing that our technique can find many of\nthe same explanations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:07:51 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Espasa", "Joan", ""], ["Gent", "Ian P.", ""], ["Hoffmann", "Ruth", ""], ["Jefferson", "Christopher", ""], ["Lynch", "Alice M.", ""]]}, {"id": "2104.15135", "submitter": "Piyawat Lertvittayakumjorn", "authors": "Piyawat Lertvittayakumjorn, Francesca Toni", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To fix a bug in a program, we need to locate where the bug is, understand why\nit causes the problem, and patch the code accordingly. This process becomes\nharder when the program is a trained machine learning model and even harder for\nopaque deep learning models. In this survey, we review papers that exploit\nexplanations to enable humans to debug NLP models. We call this problem\nexplanation-based human debugging (EBHD). In particular, we categorize and\ndiscuss existing works along three main dimensions of EBHD (the bug context,\nthe workflow, and the experimental setting), compile findings on how EBHD\ncomponents affect human debuggers, and highlight open problems that could be\nfuture research directions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 17:53:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Lertvittayakumjorn", "Piyawat", ""], ["Toni", "Francesca", ""]]}]