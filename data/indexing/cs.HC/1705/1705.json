[{"id": "1705.00204", "submitter": "Tanmay Sinha", "authors": "Tanmay Sinha, Zhen Bai, Justine Cassell", "title": "Curious Minds Wonder Alike: Studying Multimodal Behavioral Dynamics to\n  Design Social Scaffolding of Curiosity", "comments": "14 pages, 12th European Conference on Technology Enhanced Learning\n  (ECTEL 2017)", "journal-ref": null, "doi": "10.1007/978-3-319-66610-5_20", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curiosity is the strong desire to learn or know more about something or\nsomeone. Since learning is often a social endeavor, social dynamics in\ncollaborative learning may inevitably influence curiosity. There is a scarcity\nof research, however, focusing on how curiosity can be evoked in group learning\ncontexts. Inspired by a recently proposed theoretical framework that\narticulates an integrated socio-cognitive infrastructure of curiosity, in this\nwork, we use data-driven approaches to identify fine-grained social scaffolding\nof curiosity in child-child interaction, and propose how they can be used to\nelicit and maintain curiosity in technology-enhanced learning environments. For\nexample, we discovered sequential patterns of multimodal behaviors across group\nmembers and we describe those that maximize an individual's utility, or\nlikelihood, of demonstrating curiosity during open-ended problem-solving in\ngroup work. We also discovered, and describe here, behaviors that directly or\nin a mediated manner cause curiosity related conversational behaviors in the\ninteraction, with twice as many interpersonal causal influences compared to\nintrapersonal ones. We explain how these findings form a solid foundation for\ndeveloping curiosity-increasing learning technologies or even assisting a human\ncoach to induce curiosity among learners.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 15:18:49 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 21:30:21 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Sinha", "Tanmay", ""], ["Bai", "Zhen", ""], ["Cassell", "Justine", ""]]}, {"id": "1705.00242", "submitter": "Huy Kang Kim", "authors": "Hana Kim, Seongil Yang, Huy Kang Kim", "title": "Crime Scene Re-investigation: A Postmortem Analysis of Game Account\n  Stealers' Behaviors", "comments": "7 pages, 8 figures, In Proceedings of the 15th Annual Workshop on\n  Network and Systems Support for Games (NetGames 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As item trading becomes more popular, users can change their game items or\nmoney into real money more easily. At the same time, hackers turn their eyes on\nstealing other users game items or money because it is much easier to earn\nmoney than traditional gold-farming by running game bots. Game companies\nprovide various security measures to block account- theft attempts, but many\nsecurity measures on the user-side are disregarded by users because of lack of\nusability. In this study, we propose a server-side account theft detection\nsystem base on action sequence analysis to protect game users from malicious\nhackers. We tested this system in the real Massively Multiplayer Online Role\nPlaying Game (MMORPG). By analyzing users full game play log, our system can\nfind the particular action sequences of hackers with high accuracy. Also, we\ncan trace where the victim accounts stolen money goes.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 21:54:50 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Kim", "Hana", ""], ["Yang", "Seongil", ""], ["Kim", "Huy Kang", ""]]}, {"id": "1705.00594", "submitter": "Randal Olson", "authors": "Randal S. Olson, Moshe Sipper, William La Cava, Sharon Tartarone,\n  Steven Vitale, Weixuan Fu, Patryk Orzechowski, Ryan J. Urbanowicz, John H.\n  Holmes, Jason H. Moore", "title": "A System for Accessible Artificial Intelligence", "comments": "14 pages, 5 figures, submitted to Genetic Programming Theory and\n  Practice 2017 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While artificial intelligence (AI) has become widespread, many commercial AI\nsystems are not yet accessible to individual researchers nor the general public\ndue to the deep knowledge of the systems required to use them. We believe that\nAI has matured to the point where it should be an accessible technology for\neveryone. We present an ongoing project whose ultimate goal is to deliver an\nopen source, user-friendly AI system that is specialized for machine learning\nanalysis of complex data in the biomedical and health care domains. We discuss\nhow genetic programming can aid in this endeavor, and highlight specific\nexamples where genetic programming has automated machine learning analyses in\nprevious projects.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:11:48 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 17:14:14 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Olson", "Randal S.", ""], ["Sipper", "Moshe", ""], ["La Cava", "William", ""], ["Tartarone", "Sharon", ""], ["Vitale", "Steven", ""], ["Fu", "Weixuan", ""], ["Orzechowski", "Patryk", ""], ["Urbanowicz", "Ryan J.", ""], ["Holmes", "John H.", ""], ["Moore", "Jason H.", ""]]}, {"id": "1705.00947", "submitter": "Guilherme Ramos", "authors": "Jo\\~ao Sa\\'ude, Guilherme Ramos, Carlos Caleiro, Soummya Kar", "title": "Robust reputation-based ranking on multipartite rating networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of online reviews, ratings and opinions and its growing influence\non people's behavior and decisions boosted the interest to extract meaningful\ninformation from this data deluge. Hence, crowdsourced ratings of products and\nservices gained a critical role in business, governments, and others. We\npropose a new reputation-based ranking system utilizing multipartite rating\nsubnetworks, that clusters users by their similarities, using Kolmogorov\ncomplexity. Our system is novel in that it reflects a diversity of\nopinions/preferences by assigning possibly distinct rankings, for the same\nitem, for different groups of users. We prove the convergence and efficiency of\nthe system and show that it copes better with spamming/spurious users, and it\nis more robust to attacks than state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 13:05:45 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Sa\u00fade", "Jo\u00e3o", ""], ["Ramos", "Guilherme", ""], ["Caleiro", "Carlos", ""], ["Kar", "Soummya", ""]]}, {"id": "1705.01398", "submitter": "Benjamin Finley", "authors": "Benjamin Finley, Arturo Basaure", "title": "Benefits of Mobile End User Network Switching and Multihoming", "comments": "Accepted Manuscript", "journal-ref": "B. Finley, A. Basaure, Benefits of mobile end user network\n  switching and multihoming, Computer Communications, Volume 117, 2018, Pages\n  24-35", "doi": "10.1016/j.comcom.2017.12.013", "report-no": null, "categories": "cs.NI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile users have not been able to exploit spatio-temporal differences\nbetween individual mobile networks operators for a variety of reasons. End user\nnetwork switching and multihoming are two promising mechanisms that could allow\nsuch exploitation. However these mechanisms have not been thoroughly explored\nat a general system level with QoE metrics. Therefore, in this work we analyze\nthese mechanisms in a variety of diverse scenarios through a system level model\nbased on an agent based modeling framework. In terms of results, we find that\nin all scenarios end user network switching provides significant benefits in\nterms of both throughput and mean opinion score as the number of available\nnetworks increases. However, contrastingly, end user multihoming in most\nscenarios does not provide significant benefits over network switching given\nthe same number of available networks. The major reason is inefficient radio\nresource allocation resulting from individual networks not taking the\nmultihoming nature of end users into account. Though, in low user density\nsituations this inefficiency is not a problem and multihoming does provide\nincreased throughput though not increased mean opinion scores. Finally,\nscenarios that vary the fraction of users adopting multihoming suggests that\nboth early and late adopters will have similar gains over users not adopting\nmultihoming. Thus the adoption dynamics of multihoming appear favorable.\nOverall, the results support the applicability of end user network switching\nfor improving mobile user experience and the applicability of end user\nmultihoming in more limited situations.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 13:13:05 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 12:28:31 GMT"}, {"version": "v3", "created": "Thu, 21 Dec 2017 11:17:40 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Finley", "Benjamin", ""], ["Basaure", "Arturo", ""]]}, {"id": "1705.02395", "submitter": "Markus Borg", "authors": "Markus Borg, Iben Lennerstad, Rasmus Ros, Elizabeth Bjarnason", "title": "On Using Active Learning and Self-Training when Mining Performance\n  Discussions on Stack Overflow", "comments": "Preprint of paper accepted for the Proc. of the 21st International\n  Conference on Evaluation and Assessment in Software Engineering, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abundant data is the key to successful machine learning. However, supervised\nlearning requires annotated data that are often hard to obtain. In a\nclassification task with limited resources, Active Learning (AL) promises to\nguide annotators to examples that bring the most value for a classifier. AL can\nbe successfully combined with self-training, i.e., extending a training set\nwith the unlabelled examples for which a classifier is the most certain. We\nreport our experiences on using AL in a systematic manner to train an SVM\nclassifier for Stack Overflow posts discussing performance of software\ncomponents. We show that the training examples deemed as the most valuable to\nthe classifier are also the most difficult for humans to annotate. Despite\ncarefully evolved annotation criteria, we report low inter-rater agreement, but\nwe also propose mitigation strategies. Finally, based on one annotator's work,\nwe show that self-training can improve the classification accuracy. We conclude\nthe paper by discussing implication for future text miners aspiring to use AL\nand self-training.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 20:47:36 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Borg", "Markus", ""], ["Lennerstad", "Iben", ""], ["Ros", "Rasmus", ""], ["Bjarnason", "Elizabeth", ""]]}, {"id": "1705.02689", "submitter": "Seyed Sajjadi", "authors": "Seyed A Sajjadi, Danial Moazen, Ani Nahapetian", "title": "AirDraw: Leveraging Smart Watch Motion Sensors for Mobile Human Computer\n  Interactions", "comments": "6 pages, AirDraw, Leveraging Smart Watch Motion Sensors for Mobile\n  Human Computer Interactions : IEEE, CCNC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable computing is one of the fastest growing technologies today. Smart\nwatches are poised to take over at least of half the wearable devices market in\nthe near future. Smart watch screen size, however, is a limiting factor for\ngrowth, as it restricts practical text input. On the other hand, wearable\ndevices have some features, such as consistent user interaction and hands-free,\nheads-up operations, which pave the way for gesture recognition methods of text\nentry. This paper proposes a new text input method for smart watches, which\nutilizes motion sensor data and machine learning approaches to detect letters\nwritten in the air by a user. This method is less computationally intensive and\nless expensive when compared to computer vision approaches. It is also not\naffected by lighting factors, which limit computer vision solutions. The\nAirDraw system prototype developed to test this approach is presented.\nAdditionally, experimental results close to 71% accuracy are presented.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 19:58:54 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Sajjadi", "Seyed A", ""], ["Moazen", "Danial", ""], ["Nahapetian", "Ani", ""]]}, {"id": "1705.02694", "submitter": "Amol Patwardhan", "authors": "Amol S Patwardhan and Gerald M Knapp", "title": "Multimodal Affect Analysis for Product Feedback Assessment", "comments": "10 pages, ISERC 2013, IIE Annual Conference. Proceedings. Institute\n  of Industrial Engineers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumers often react expressively to products such as food samples, perfume,\njewelry, sunglasses, and clothing accessories. This research discusses a\nmultimodal affect recognition system developed to classify whether a consumer\nlikes or dislikes a product tested at a counter or kiosk, by analyzing the\nconsumer's facial expression, body posture, hand gestures, and voice after\ntesting the product. A depth-capable camera and microphone system - Kinect for\nWindows - is utilized. An emotion identification engine has been developed to\nanalyze the images and voice to determine affective state of the customer. The\nimage is segmented using skin color and adaptive threshold. Face, body and\nhands are detected using the Haar cascade classifier. Canny edges are\nidentified and the lip, body and hand contours are extracted using spatial\nfiltering. Edge count and orientation around the mouth, cheeks, eyes,\nshoulders, fingers and the location of the edges are used as features.\nClassification is done by an emotion template mapping algorithm and training a\nclassifier using support vector machines. The real-time performance, accuracy\nand feasibility for multimodal affect recognition in feedback assessment are\nevaluated.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 20:39:35 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Patwardhan", "Amol S", ""], ["Knapp", "Gerald M", ""]]}, {"id": "1705.02823", "submitter": "Per B{\\ae}kgaard", "authors": "Per B{\\ae}kgaard, Michael Kai Petersen and Jakob Eg Larsen", "title": "The Blank Stare: Retrieving Unique Eye Tracking Signatures Independent\n  of Visual Stimuli", "comments": "12 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using Low Cost Portable Eye Tracking for Biometric Identification Or\nVerification: Eye tracking technologies have in recent years become available\noutside of specialised labs, and are starting to become integrated in tablets\nand virtual reality headsets. This offers new opportunities for use in common\noffice- and home environments, such as for biometric recognition\n(identification or verification), alone or in combination with other\ntechnologies. This paper exposes two fundamentally different approaches that\nhave been suggested, based on spatial and temporal signatures respectively.\nWhile deploying different stimulation paradigms for recording, it also proposes\nan alternative way to analyze spatial domain signatures using Fourier\ntransformation. Empirical data recorded from two subjects over two weeks, three\nmonths apart, are found to support previous results. Further, variations and\nstability of some of the proposed signatures are analyzed over the extended\ntimeframe and under slightly varying conditions.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 11:01:51 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["B\u00e6kgaard", "Per", ""], ["Petersen", "Michael Kai", ""], ["Larsen", "Jakob Eg", ""]]}, {"id": "1705.02937", "submitter": "Zhibin Niu", "authors": "Zhibin Niu, Dawei Cheng, Liqing Zhang, Jiawan Zhang", "title": "Visual analytics for networked-guarantee loans risk management", "comments": "Please cite our published version", "journal-ref": "2018 IEEE Pacific Visualization Symposium (PacificVis), Kobe,\n  2018, pp. 160-169", "doi": "10.1109/PacificVis.2018.00028", "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Groups of enterprises guarantee each other and form complex guarantee\nnetworks when they try to obtain loans from banks. Such secured loan can\nenhance the solvency and promote the rapid growth in the economic upturn\nperiod. However, potential systemic risk may happen within the risk binding\ncommunity. Especially, during the economic down period, the crisis may spread\nin the guarantee network like a domino. Monitoring the financial status,\npreventing or reducing systematic risk when crisis happens is highly concerned\nby the regulatory commission and banks. We propose visual analytics approach\nfor loan guarantee network risk management, and consolidate the five analysis\ntasks with financial experts: i) visual analytics for enterprises default risk,\nwhereby a hybrid representation is devised to predict the default risk and\ndeveloped an interface to visualize key indicators; ii) visual analytics for\nhigh default groups, whereby a community detection based interactive approach\nis presented; iii) visual analytics for high defaults pattern, whereby a motif\ndetection based interactive approach is described, and we adopt a Shneiderman\nMantra strategy to reduce the computation complexity. iv) visual analytics for\nevolving guarantee network, whereby animation is used to help understanding the\nguarantee dynamic; v) visual analytics approach and interface for default\ndiffusion path. The temporal diffusion path analysis can be useful for the\ngovernment and bank to monitor the default spread status. It also provides\ninsight for taking precautionary measures to prevent and dissolve systemic\nfinancial risk. We implement the system with case studies on a real-world\nguarantee network. Two financial experts are consulted with endorsement on the\ndeveloped tool. To the best of our knowledge, this is the first visual\nanalytics tool to explore the guarantee network risks in a systematic manner.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 04:14:21 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 10:12:10 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Niu", "Zhibin", ""], ["Cheng", "Dawei", ""], ["Zhang", "Liqing", ""], ["Zhang", "Jiawan", ""]]}, {"id": "1705.03227", "submitter": "Guido Giunti", "authors": "Guido Giunti, Estefania Guisado-Fernandez, Brian Caulfield", "title": "Connected Health in Multiple Sclerosis: a mobile applications review", "comments": "Article submitted to the 30th IEEE International Symposium on\n  Computer-Based Medical Systems - IEEE CBMS 2017, Thessaloniki, Greece. 6\n  pages, 2 figures, 5 tables", "journal-ref": null, "doi": "10.1109/CBMS.2017.27", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Sclerosis (MS) is an unpredictable, often disabling disease that can\nadversely affect any body function; this often requires persons with MS to be\nactive patients who are able to self-manage. There are currently thousands of\nhealth applications available but it is unknown how many concern MS. We\nconducted a systematic review of all MS apps present in the most popular app\nstores (iTunes and Google Play store) on June 2016 to identify all relevant MS\napps. After discarding non-MS related apps and duplicates, only a total of 25\nMS apps were identified. App description contents and features were explored to\nassess target audience, functionalities, and developing entities. The vast\nmajority of apps were focused on disease and treatment information with disease\nmanagement being a close second. This is the first study that reviews MS apps\nand it highlights an interesting gap in the current repertoire of MS mHealth\nresources.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 08:35:42 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Giunti", "Guido", ""], ["Guisado-Fernandez", "Estefania", ""], ["Caulfield", "Brian", ""]]}, {"id": "1705.03228", "submitter": "Guido Giunti", "authors": "Guido Giunti, Diego H Giunta, Santiago Hors-Fraile, Minna Isomursu,\n  Diana Karoseviciute", "title": "Detecting Gamification in Breast Cancer Apps: an automatic methodology\n  for screening purposes", "comments": "Article submitted to the 30th IEEE International Symposium on\n  Computer-Based Medical Systems - IEEE CBMS 2017, Thessaloniki, Greece. 6\n  pages, 2 figures, 5 tables", "journal-ref": null, "doi": "10.1109/CBMS.2017.21", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the most common cancer in women both in developed and\ndeveloping countries. More than half of all cancer mobile application concern\nbreast cancer. Gamification is widely used in mobile software applications\ncreated for health-related services. Current prevalence of gamification in\nbreast cancer apps is unknown and detection must be manually performed. The\npurpose of this study is to describe and produce a tool allowing automatic\ndetection of apps which contain gamification elements and thus empowering\nresearchers to study gamification using large data samples. Predictive logistic\nregression model was designed on data extracted from breast cancer apps' title\nand description text available in app stores. Model was validated comparing\nestimated and benchmark values, observed by gamification specialists. Study's\noutcome can be applied as a screening tool to efficiently identify gamification\npresence in breast cancer apps for further research.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 08:41:27 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Giunti", "Guido", ""], ["Giunta", "Diego H", ""], ["Hors-Fraile", "Santiago", ""], ["Isomursu", "Minna", ""], ["Karoseviciute", "Diana", ""]]}, {"id": "1705.03259", "submitter": "Atalanti Mastakouri", "authors": "Anastasia-Atalanti Mastakouri, Sebastian Weichwald, Ozan \\\"Ozdenizci,\n  Timm Meyer, Bernhard Sch\\\"olkopf and Moritz Grosse-Wentrup", "title": "Personalized Brain-Computer Interface Models for Motor Rehabilitation", "comments": "6 pages, 6 figures, conference submission", "journal-ref": "IEEE International Conference on Systems, Man , and Cybernetics\n  (SCM 2017), 2017", "doi": "10.1109/SMC.2017.8123089", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to fuse two currently separate research lines on novel therapies\nfor stroke rehabilitation: brain-computer interface (BCI) training and\ntranscranial electrical stimulation (TES). Specifically, we show that BCI\ntechnology can be used to learn personalized decoding models that relate the\nglobal configuration of brain rhythms in individual subjects (as measured by\nEEG) to their motor performance during 3D reaching movements. We demonstrate\nthat our models capture substantial across-subject heterogeneity, and argue\nthat this heterogeneity is a likely cause of limited effect sizes observed in\nTES for enhancing motor performance. We conclude by discussing how our\npersonalized models can be used to derive optimal TES parameters, e.g.,\nstimulation site and frequency, for individual patients.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 10:18:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mastakouri", "Anastasia-Atalanti", ""], ["Weichwald", "Sebastian", ""], ["\u00d6zdenizci", "Ozan", ""], ["Meyer", "Timm", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1705.03290", "submitter": "Iiris Sundin", "authors": "Iiris Sundin, Tomi Peltola, Muntasir Mamun Majumder, Pedram Daee,\n  Marta Soare, Homayun Afrabandpey, Caroline Heckman, Samuel Kaski and Pekka\n  Marttinen", "title": "Improving drug sensitivity predictions in precision medicine through\n  active expert knowledge elicitation", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": "10.1093/bioinformatics/bty257", "report-no": null, "categories": "cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the efficacy of a drug for a given individual, using\nhigh-dimensional genomic measurements, is at the core of precision medicine.\nHowever, identifying features on which to base the predictions remains a\nchallenge, especially when the sample size is small. Incorporating expert\nknowledge offers a promising alternative to improve a prediction model, but\ncollecting such knowledge is laborious to the expert if the number of candidate\nfeatures is very large. We introduce a probabilistic model that can incorporate\nexpert feedback about the impact of genomic measurements on the sensitivity of\na cancer cell for a given drug. We also present two methods to intelligently\ncollect this feedback from the expert, using experimental design and\nmulti-armed bandit models. In a multiple myeloma blood cancer data set (n=51),\nexpert knowledge decreased the prediction error by 8%. Furthermore, the\nintelligent approaches can be used to reduce the workload of feedback\ncollection to less than 30% on average compared to a naive approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 12:04:33 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Sundin", "Iiris", ""], ["Peltola", "Tomi", ""], ["Majumder", "Muntasir Mamun", ""], ["Daee", "Pedram", ""], ["Soare", "Marta", ""], ["Afrabandpey", "Homayun", ""], ["Heckman", "Caroline", ""], ["Kaski", "Samuel", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1705.03507", "submitter": "Valery Vilisov", "authors": "V.Ya. Vilisov, D.A. Dyatlova", "title": "Analysis of Information Technologies Used to Insure Working Efficiency\n  of Personnel", "comments": "7 pages, 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work is devoted to a modern state, methods and tools of monitoring,\nassessment and prediction of the indicators showing physical condition of a\nperson and his/her capabilities to perform work duties. The work contains an\nanalysis of existing gadgets and software that allow tracking physical\ncondition of personnel at the working place. The analysis showing significant\ninterconnections and factors that determine a necessary level of working\ncapacity and productivity of personnel allows organizing Work & Rest Schedule\nof employees in an effective manner.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 19:34:17 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Vilisov", "V. Ya.", ""], ["Dyatlova", "D. A.", ""]]}, {"id": "1705.03691", "submitter": "Luis Fernandez-Luque", "authors": "Michael Aupetit, Luis Fernandez-Luque, Meghna Singh and Jaideep\n  Srivastava", "title": "Visualization of Wearable Data and Biometrics for Analysis and\n  Recommendations in Childhood Obesity", "comments": "2 pages short paper IEEE CBMS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obesity is one of the major health risk factors be- hind the rise of\nnon-communicable conditions. Understanding the factors influencing obesity is\nvery complex since there are many variables that can affect the health\nbehaviors leading to it. Nowadays, multiple data sources can be used to study\nhealth behaviors, such as wearable sensors for physical activity and sleep,\nsocial media, mobile and health data. In this paper we describe the design of a\ndashboard for the visualization of actigraphy and biometric data from a\nchildhood obesity camp in Qatar. This dashboard allows quantitative discoveries\nthat can be used to guide patient behavior and orient qualitative research.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 10:44:45 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Aupetit", "Michael", ""], ["Fernandez-Luque", "Luis", ""], ["Singh", "Meghna", ""], ["Srivastava", "Jaideep", ""]]}, {"id": "1705.03822", "submitter": "Sabrina Klos (N\\'ee M\\\"uller)", "authors": "Sabrina Klos (n\\'ee M\\\"uller), Cem Tekin, Mihaela van der Schaar, Anja\n  Klein", "title": "Context-Aware Hierarchical Online Learning for Performance Maximization\n  in Mobile Crowdsourcing", "comments": "18 pages, 10 figures", "journal-ref": "IEEE/ACM Transactions on Networking, vol. 26, no. 3, pp.\n  1334-1347, June 2018", "doi": "10.1109/TNET.2018.2828415", "report-no": null, "categories": "cs.LG cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mobile crowdsourcing (MCS), mobile users accomplish outsourced human\nintelligence tasks. MCS requires an appropriate task assignment strategy, since\ndifferent workers may have different performance in terms of acceptance rate\nand quality. Task assignment is challenging, since a worker's performance (i)\nmay fluctuate, depending on both the worker's current personal context and the\ntask context, (ii) is not known a priori, but has to be learned over time.\nMoreover, learning context-specific worker performance requires access to\ncontext information, which may not be available at a central entity due to\ncommunication overhead or privacy concerns. Additionally, evaluating worker\nperformance might require costly quality assessments. In this paper, we propose\na context-aware hierarchical online learning algorithm addressing the problem\nof performance maximization in MCS. In our algorithm, a local controller (LC)\nin the mobile device of a worker regularly observes the worker's context,\nher/his decisions to accept or decline tasks and the quality in completing\ntasks. Based on these observations, the LC regularly estimates the worker's\ncontext-specific performance. The mobile crowdsourcing platform (MCSP) then\nselects workers based on performance estimates received from the LCs. This\nhierarchical approach enables the LCs to learn context-specific worker\nperformance and it enables the MCSP to select suitable workers. In addition,\nour algorithm preserves worker context locally, and it keeps the number of\nrequired quality assessments low. We prove that our algorithm converges to the\noptimal task assignment strategy. Moreover, the algorithm outperforms simpler\ntask assignment strategies in experiments based on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 15:34:54 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 09:33:39 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Klos", "Sabrina", "", "n\u00e9e M\u00fcller"], ["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""], ["Klein", "Anja", ""]]}, {"id": "1705.03973", "submitter": "Ilya V. Osipov", "authors": "Ilya V Osipov", "title": "Transreality puzzle as new genres of entertainment technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The author considers a class of mechatronic puzzles falling in the\nmixed-reality category, present examples of such devices, and propose a way to\ncategorize them. Close relationships of such devices with the Tangible User\nInterface are described. The device designed by the author as an illustration\nof a mixed reality puzzle is presented.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 23:22:43 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Osipov", "Ilya V", ""]]}, {"id": "1705.04317", "submitter": "Konstantin Pugachev", "authors": "A. Korol, K. Pugachev", "title": "Management system for the SND experiments", "comments": "6 pages, 3 figures; INSTR17 conference", "journal-ref": null, "doi": "10.1088/1748-0221/12/09/C09006", "report-no": null, "categories": "physics.ins-det cs.HC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new management system for the SND detector experiments (at VEPP-2000\ncollider in Novosibirsk) is developed. We describe here the interaction between\na user and the SND databases. These databases contain experiment configuration,\nconditions and metadata. The new system is designed in client-server\narchitecture. It has several logical layers corresponding to the users roles. A\nnew template engine is created. A web application is implemented using Node.js\nframework. At the time the application provides: showing and editing\nconfiguration; showing experiment metadata and experiment conditions data\nindex; showing SND log (prototype).\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 11:12:43 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 20:45:26 GMT"}, {"version": "v3", "created": "Mon, 26 Jun 2017 09:14:03 GMT"}, {"version": "v4", "created": "Mon, 24 Jul 2017 19:20:35 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Korol", "A.", ""], ["Pugachev", "K.", ""]]}, {"id": "1705.04683", "submitter": "Xingjun Ma", "authors": "Sudanthi Wijewickrema, Xingjun Ma, James Bailey, Gregor Kennedy and\n  Stephen O'Leary", "title": "Feedback Techniques in Computer-Based Simulation Training: A Survey", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-based simulation training (CBST) is gaining popularity in a vast\nrange of applications such as surgery, rehabilitation therapy, military\napplications, and driver/pilot training, as it offers a low-cost,\neasily-accessible and effective training environment. Typically, CBST systems\ncomprise of two essential components: 1) a simulation environment that provides\nan immersive and interactive learning experience, and 2) a feedback\nintervention system that supports knowledge/skill acquisition and decision\nmaking. The simulation environment is created using technologies such as\nvirtual or augmented reality, and this is an area which has gained much\ninterest in recent years. The provision of automated feedback in CBST however,\nhas not been investigated as much, and thus, is the focus of this paper.\nFeedback is an essential component in learning, and should be provided to the\ntrainee during the training process in order to improve skills, to correct\nmistakes, and most importantly, to inspire reasoning and critical thinking. In\nCBST, feedback should be provided in a useful and timely manner, ideally in a\nway that mimics the advice of an experienced tutor. Here, we explore the\nprovision of feedback in CBST from three perspectives: 1) types of feedback to\nbe provided, 2) presentation modalities of feedback, and 3) methods for\nfeedback extraction/learning. This review is aimed at providing insight into\nhow feedback is extracted, organized, and delivered in current applications, to\nbe used as a guide to the development of future feedback intervention systems\nin CBST applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 07:18:58 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Wijewickrema", "Sudanthi", ""], ["Ma", "Xingjun", ""], ["Bailey", "James", ""], ["Kennedy", "Gregor", ""], ["O'Leary", "Stephen", ""]]}, {"id": "1705.05142", "submitter": "Felip Mart\\'i Carrillo", "authors": "Felip Mart\\'i (1 and 2), Jo Butchart (3 and 4), Sarah Knight (4 and\n  3), Adam Scheinberg (3 and 4), Lisa Wise (1), Leon Sterling (1), Chris\n  McCarthy (1) ((1) Swinburne University of Technology, (2) Data61 CSIRO, (3)\n  Royal Children's Hospital, (4) Murdoch Childrens Research Institute)", "title": "Adapting a General Purpose Social Robot for Paediatric Rehabilitation\n  through In-situ Design", "comments": "Submitted to the Journal of Human-Robot Interaction (JHRI). Journal\n  rebranded to Transactions of Human-Robot Interaction (THRI). Paper presented\n  in the 13th Annual ACM/IEEE International Conference on Human Robot\n  Interaction, Chicago, 8 March 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Socially Assistive Robots (SARs) offer great promise for improving outcomes\nin paediatric rehabilitation. However, the design of software and interactive\ncapabilities for SARs must be carefully considered in the context of their\nintended clinical use. While previous work has explored specific roles and\nfunctionalities to support paediatric rehabilitation, few have considered the\ndesign of such capabilities in the context of ongoing clinical deployment. In\nthis paper we present a two-phase In-situ design process for SARs in health\ncare, emphasising stakeholder engagement and on-site development. We explore\nthis in the context of developing the humanoid social robot NAO as a socially\nassistive rehabilitation aid for children with cerebral palsy. We present and\nevaluate our design process, outcomes achieved, and preliminary results from\nongoing clinical testing with 9 patients and 5 therapists over 14 sessions. We\nargue that our in-situ Design methodology has been central to the rapid and\nsuccessful deployment of our system.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 10:03:59 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 02:37:47 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Mart\u00ed", "Felip", "", "1 and 2"], ["Butchart", "Jo", "", "3 and 4"], ["Knight", "Sarah", "", "4 and\n  3"], ["Scheinberg", "Adam", "", "3 and 4"], ["Wise", "Lisa", ""], ["Sterling", "Leon", ""], ["McCarthy", "Chris", ""]]}, {"id": "1705.05172", "submitter": "Thomas Moerland", "authors": "Thomas M. Moerland, Joost Broekens, Catholijn M. Jonker", "title": "Emotion in Reinforcement Learning Agents and Robots: A Survey", "comments": "To be published in Machine Learning Journal", "journal-ref": "Machine Learning 2017", "doi": "10.1007/s10994-017-5666-0", "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides the first survey of computational models of emotion in\nreinforcement learning (RL) agents. The survey focuses on agent/robot emotions,\nand mostly ignores human user emotions. Emotions are recognized as functional\nin decision-making by influencing motivation and action selection. Therefore,\ncomputational emotion models are usually grounded in the agent's decision\nmaking architecture, of which RL is an important subclass. Studying emotions in\nRL-based agents is useful for three research fields. For machine learning (ML)\nresearchers, emotion models may improve learning efficiency. For the\ninteractive ML and human-robot interaction (HRI) community, emotions can\ncommunicate state and enhance user investment. Lastly, it allows affective\nmodelling (AM) researchers to investigate their emotion theories in a\nsuccessful AI agent class. This survey provides background on emotion theory\nand RL. It systematically addresses 1) from what underlying dimensions (e.g.,\nhomeostasis, appraisal) emotions can be derived and how these can be modelled\nin RL-agents, 2) what types of emotions have been derived from these\ndimensions, and 3) how these emotions may either influence the learning\nefficiency of the agent or be useful as social signals. We also systematically\ncompare evaluation criteria, and draw connections to important RL sub-domains\nlike (intrinsic) motivation and model-based RL. In short, this survey provides\nboth a practical overview for engineers wanting to implement emotions in their\nRL agents, and identifies challenges and directions for future emotion-RL\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 11:49:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Moerland", "Thomas M.", ""], ["Broekens", "Joost", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "1705.05206", "submitter": "Chen Zhang", "authors": "Chen Zhang, Hao Wang, Yingcai Wu", "title": "ResumeVis: A Visual Analytics System to Discover Semantic Information in\n  Semi-structured Resume Data", "comments": null, "journal-ref": null, "doi": "10.1145/3230707", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive public resume data emerging on the WWW indicates individual-related\ncharacteristics in terms of profile and career experiences. Resume Analysis\n(RA) provides opportunities for many applications, such as talent seeking and\nevaluation. Existing RA studies based on statistical analyzing have primarily\nfocused on talent recruitment by identifying explicit attributes. However, they\nfailed to discover the implicit semantic information, i.e., individual career\nprogress patterns and social-relations, which are vital to comprehensive\nunderstanding of career development. Besides, how to visualize them for better\nhuman cognition is also challenging. To tackle these issues, we propose a\nvisual analytics system ResumeVis to mine and visualize resume data. Firstly, a\ntext-mining based approach is presented to extract semantic information. Then,\na set of visualizations are devised to represent the semantic information in\nmultiple perspectives. By interactive exploration on ResumeVis performed by\ndomain experts, the following tasks can be accomplished: to trace individual\ncareer evolving trajectory; to mine latent social-relations among individuals;\nand to hold the full picture of massive resumes' collective mobility. Case\nstudies with over 2500 online officer resumes demonstrate the effectiveness of\nour system. We provide a demonstration video.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 13:13:47 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhang", "Chen", ""], ["Wang", "Hao", ""], ["Wu", "Yingcai", ""]]}, {"id": "1705.05283", "submitter": "Jean-Daniel Fekete", "authors": "Nicolas Heulot, Jean-Daniel Fekete, Michael Aupetit", "title": "Visualizing Dimensionality Reduction Artifacts: An Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multidimensional scaling allows visualizing high-dimensional data as 2D maps\nwith the premise that insights in 2D reveal valid information in\nhigh-dimensions. However, the resulting projections suffer from artifacts such\nas bad local neighborhood preservation and clusters tearing. Interactively\ncoloring the projection according to the discrepancy between original\nproximities relative to a reference item reveals these artifacts, but it is not\nclear if conveying these proximities using color and displaying only local\ninformation really helps the visual analysis of projections. We conducted a\ncontrolled experiment to investigate the relevance of this interactive\ntechnique to help the visual analysis of any projection regardless its quality.\nWe compared the bare projection to the interactive coloring of the original\nproximities on different visual analysis tasks involving outliers and clusters.\nResults indicate that the interactive coloring is worthwhile for local tasks as\nit is significantly robust to projection artifacts whereas the projection is\nnot. However this interactive technique does not help significantly for visual\nclustering tasks for that projections already give a suitable overview.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 15:08:53 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Heulot", "Nicolas", ""], ["Fekete", "Jean-Daniel", ""], ["Aupetit", "Michael", ""]]}, {"id": "1705.05546", "submitter": "Zhenpeng Chen", "authors": "Zhenpeng Chen and Xuan Lu and Wei Ai and Huoran Li and Qiaozhu Mei and\n  Xuanzhe Liu", "title": "Through a Gender Lens: Learning Usage Patterns of Emojis from\n  Large-Scale Android Users", "comments": "The Web Conference 2018 (WWW 2018)", "journal-ref": null, "doi": "10.1145/3178876.3186157", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a large data set of emoji using behavior collected from smartphone\nusers over the world, this paper investigates gender-specific usage of emojis.\nWe present various interesting findings that evidence a considerable difference\nin emoji usage by female and male users. Such a difference is significant not\njust in a statistical sense; it is sufficient for a machine learning algorithm\nto accurately infer the gender of a user purely based on the emojis used in\ntheir messages. In real world scenarios where gender inference is a necessity,\nmodels based on emojis have unique advantages over existing models that are\nbased on textual or contextual information. Emojis not only provide\nlanguage-independent indicators, but also alleviate the risk of leaking private\nuser information through the analysis of text and metadata.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 06:19:18 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 23:05:53 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Chen", "Zhenpeng", ""], ["Lu", "Xuan", ""], ["Ai", "Wei", ""], ["Li", "Huoran", ""], ["Mei", "Qiaozhu", ""], ["Liu", "Xuanzhe", ""]]}, {"id": "1705.05613", "submitter": "Qian Liu", "authors": "Xiao Xu, Qian Liu and Eckehard Steinbach", "title": "Toward QoE-Driven Dynamic Control Scheme Switching for Time-Delayed\n  Teleoperation Systems: A Dedicated Case Study", "comments": "6 pages, 9 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networked teleoperation with haptic feedback is a prime example for the\nemerging Tactile Internet, which requires a careful orchestration of haptic\ncommunication and control. One major challenge in this context is how to\nmaximize the user's quality-of-experience (QoE) while ensuring at the same time\nthe stability of the global control loop in the presence of communication\ndelay. In this paper, we propose a dynamic control scheme switching strategy\nfor teleoperation systems, which maximizes the QoE for time-varying\ncommunication delay. In order to validate the feasibility of the proposed\napproach, we perform a dedicated case study for a virtual teleoperation\nenvironment consisting of a one-dimensional spring-damper system, and conduct\nextensive subjective tests under various delay conditions for two control\nschemes: (1) teleoperation with the time-domain passivity approach (TDPA),\nwhich is highly delay-sensitive but supports highly dynamic interaction between\nthe operator and a potentially quickly changing remote environment; (2)\nmodel-mediated teleoperation (MMT), which is tolerable to relatively larger\ncommunication delays, but unsuitable for quickly changing, highly dynamic\nremote environments. For both schemes, we use recently proposed extensions,\nwhich incorporate perceptual data reduction to reduce the required packet rate\nbetween the operator and the teleoperator. One key contribution of this paper\nlies in the exploration of the intrinsic relationship among QoE, communication\ndelay and the control schemes which provides a fundamental guidance, not only\nto this research, but also to the future joint optimization of communication\nand control for time-delayed teleoperation systems.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 09:41:38 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Xu", "Xiao", ""], ["Liu", "Qian", ""], ["Steinbach", "Eckehard", ""]]}, {"id": "1705.05720", "submitter": "Rui Meng", "authors": "Rui Meng, Hao Xin, Lei Chen, Yangqiu Song", "title": "Subjective Knowledge Acquisition and Enrichment Powered By Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases (KBs) have attracted increasing attention due to its great\nsuccess in various areas, such as Web and mobile search.Existing KBs are\nrestricted to objective factual knowledge, such as city population or fruit\nshape, whereas,subjective knowledge, such as big city, which is commonly\nmentioned in Web and mobile queries, has been neglected. Subjective knowledge\ndiffers from objective knowledge in that it has no documented or observed\nground truth. Instead, the truth relies on people's dominant opinion. Thus, we\ncan use the crowdsourcing technique to get opinion from the crowd. In our work,\nwe propose a system, called crowdsourced subjective knowledge acquisition\n(CoSKA),for subjective knowledge acquisition powered by crowdsourcing and\nexisting KBs. The acquired knowledge can be used to enrich existing KBs in the\nsubjective dimension which bridges the gap between existing objective knowledge\nand subjective queries.The main challenge of CoSKA is the conflict between\nlarge scale knowledge facts and limited crowdsourcing resource. To address this\nchallenge, in this work, we define knowledge inference rules and then select\nthe seed knowledge judiciously for crowdsourcing to maximize the inference\npower under the resource constraint. Our experimental results on real knowledge\nbase and crowdsourcing platform verify the effectiveness of CoSKA system.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 14:25:02 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Meng", "Rui", ""], ["Xin", "Hao", ""], ["Chen", "Lei", ""], ["Song", "Yangqiu", ""]]}, {"id": "1705.05884", "submitter": "Babak Toghiani-Rizi", "authors": "Babak Toghiani-Rizi, Christofer Lind, Maria Svensson, Marcus Windmark", "title": "Static Gesture Recognition using Leap Motion", "comments": "Results based on a study conducted during the course Intelligent\n  Interactive Systems at Uppsala University, spring 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, an automated bartender system was developed for making orders\nin a bar using hand gestures. The gesture recognition of the system was\ndeveloped using Machine Learning techniques, where the model was trained to\nclassify gestures using collected data. The final model used in the system\nreached an average accuracy of 95%. The system raised ethical concerns both in\nterms of user interaction and having such a system in a real world scenario,\nbut it could initially work as a complement to a real bartender.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 19:38:20 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Toghiani-Rizi", "Babak", ""], ["Lind", "Christofer", ""], ["Svensson", "Maria", ""], ["Windmark", "Marcus", ""]]}, {"id": "1705.06224", "submitter": "Kleomenis Katevas", "authors": "Kleomenis Katevas, Ilias Leontiadis, Martin Pielot, Joan Serr\\`a", "title": "Practical Processing of Mobile Sensor Data for Continual Deep Learning\n  Predictions", "comments": "6 pages, 3 figures, 3 tables", "journal-ref": "DeepMobile Workshop, MobileHCI 2017", "doi": "10.1145/3089801.3089802", "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical approach for processing mobile sensor time series data\nfor continual deep learning predictions. The approach comprises data cleaning,\nnormalization, capping, time-based compression, and finally classification with\na recurrent neural network. We demonstrate the effectiveness of the approach in\na case study with 279 participants. On the basis of sparse sensor events, the\nnetwork continually predicts whether the participants would attend to a\nnotification within 10 minutes. Compared to a random baseline, the classifier\nachieves a 40% performance increase (AUC of 0.702) on a withheld test set. This\napproach allows to forgo resource-intensive, domain-specific, error-prone\nfeature engineering, which may drastically increase the applicability of\nmachine learning to mobile phone sensor data.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 15:55:53 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Katevas", "Kleomenis", ""], ["Leontiadis", "Ilias", ""], ["Pielot", "Martin", ""], ["Serr\u00e0", "Joan", ""]]}, {"id": "1705.06694", "submitter": "Tommy Nilsson", "authors": "Kevin K. Bowden, Tommy Nilsson, Christine P. Spencer, Kubra Cengiz,\n  Alexandru Ghitulescu, Jelte B. van Waterschoot", "title": "I Probe, Therefore I Am: Designing a Virtual Journalist with Human\n  Emotions", "comments": "eNTERFACE16 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By utilizing different communication channels, such as verbal language,\ngestures or facial expressions, virtually embodied interactive humans hold a\nunique potential to bridge the gap between human-computer interaction and\nactual interhuman communication. The use of virtual humans is consequently\nbecoming increasingly popular in a wide range of areas where such a natural\ncommunication might be beneficial, including entertainment, education, mental\nhealth research and beyond. Behind this development lies a series of\ntechnological advances in a multitude of disciplines, most notably natural\nlanguage processing, computer vision, and speech synthesis. In this paper we\ndiscuss a Virtual Human Journalist, a project employing a number of novel\nsolutions from these disciplines with the goal to demonstrate their viability\nby producing a humanoid conversational agent capable of naturally eliciting and\nreacting to information from a human user. A set of qualitative and\nquantitative evaluation sessions demonstrated the technical feasibility of the\nsystem whilst uncovering a number of deficits in its capacity to engage users\nin a way that would be perceived as natural and emotionally engaging. We argue\nthat naturalness should not always be seen as a desirable goal and suggest that\ndeliberately suppressing the naturalness of virtual human interactions, such as\nby altering its personality cues, might in some cases yield more desirable\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 17:01:10 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Bowden", "Kevin K.", ""], ["Nilsson", "Tommy", ""], ["Spencer", "Christine P.", ""], ["Cengiz", "Kubra", ""], ["Ghitulescu", "Alexandru", ""], ["van Waterschoot", "Jelte B.", ""]]}, {"id": "1705.07273", "submitter": "Corneliu Ilisescu", "authors": "Corneliu Ilisescu and Halil Aytac Kanaci and Matteo Romagnoli and\n  Neill D. F. Campbell and Gabriel J. Brostow", "title": "Responsive Action-based Video Synthesis", "comments": "10 pages, 12 figures, 1 table, accepted and published in Proceedings\n  of the 2017 CHI Conference on Human Factors in Computing Systems", "journal-ref": null, "doi": "10.1145/3025453.3025880", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose technology to enable a new medium of expression, where video\nelements can be looped, merged, and triggered, interactively. Like audio, video\nis easy to sample from the real world but hard to segment into clean reusable\nelements. Reusing a video clip means non-linear editing and compositing with\nnovel footage. The new context dictates how carefully a clip must be prepared,\nso our end-to-end approach enables previewing and easy iteration.\n  We convert static-camera videos into loopable sequences, synthesizing them in\nresponse to simple end-user requests. This is hard because a) users want\nessentially semantic-level control over the synthesized video content, and b)\nautomatic loop-finding is brittle and leaves users limited opportunity to work\nthrough problems. We propose a human-in-the-loop system where adding effort\ngives the user progressively more creative control. Artists help us evaluate\nhow our trigger interfaces can be used for authoring of videos and\nvideo-performances.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 07:46:31 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Ilisescu", "Corneliu", ""], ["Kanaci", "Halil Aytac", ""], ["Romagnoli", "Matteo", ""], ["Campbell", "Neill D. F.", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1705.07490", "submitter": "Rami Puzis", "authors": "Ori Ossmy, Ofir Tam, Rami Puzis, Lior Rokach, Ohad Inbar, Yuval\n  Elovici", "title": "MindDesktop: a general purpose brain computer interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in electroencephalography (EEG) and electromyography (EMG)\nenable communication for people with severe disabilities. In this paper we\npresent a system that enables the use of regular computers using an\noff-the-shelf EEG/EMG headset, providing a pointing device and virtual keyboard\nthat can be used to operate any Windows based system, minimizing the user\neffort required for interacting with a personal computer. Effectiveness of the\nproposed system is evaluated by a usability study, indicating decreasing\nlearning curve for completing various tasks. The proposed system is available\nin the link provided.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 19:08:09 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Ossmy", "Ori", ""], ["Tam", "Ofir", ""], ["Puzis", "Rami", ""], ["Rokach", "Lior", ""], ["Inbar", "Ohad", ""], ["Elovici", "Yuval", ""]]}, {"id": "1705.07589", "submitter": "Stephan Wyder", "authors": "Stephan Wyder and Philippe C. Cattin", "title": "Eye Tracker Accuracy: Quantitative Evaluation of the Invisible Eye\n  Center Location", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose. We present a new method to evaluate the accuracy of an eye tracker\nbased eye localization system. Measuring the accuracy of an eye tracker's\nprimary intention, the estimated point of gaze, is usually done with volunteers\nand a set of fixation points used as ground truth. However, verifying the\naccuracy of the location estimate of a volunteer's eye center in 3D space is\nnot easily possible. This is because the eye center is an intangible point\nhidden by the iris. Methods. We evaluate the eye location accuracy by using an\neye phantom instead of eyes of volunteers. For this, we developed a testing\nstage with a realistic artificial eye and a corresponding kinematic model,\nwhich we trained with {\\mu}CT data. This enables us to precisely evaluate the\neye location estimate of an eye tracker. Results. We show that the proposed\ntesting stage with the corresponding kinematic model is suitable for such a\nvalidation. Further, we evaluate a particular eye tracker based navigation\nsystem and show that this system is able to successfully determine the eye\ncenter with sub-millimeter accuracy. Conclusions. We show the suitability of\nthe evaluated eye tracker for eye interventions, using the proposed testing\nstage and the corresponding kinematic model. The results further enable\nspecific enhancement of the navigation system to potentially get even better\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 07:35:34 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Wyder", "Stephan", ""], ["Cattin", "Philippe C.", ""]]}, {"id": "1705.07771", "submitter": "Kang Wang", "authors": "Kang Wang, Xueqian Wang, Gang Li", "title": "Simulation Experiment of BCI Based on Imagined Speech EEG Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain Computer Interface (BCI) can help patients of neuromuscular diseases\nrestore parts of the movement and communication abilities that they have lost.\nMost of BCIs rely on mapping brain activities to device instructions, but\nlimited number of brain activities decides the limited abilities of BCIs. To\ndeal with the problem of limited ablility of BCI, this paper verified the\nfeasibility of constructing BCI based on decoding imagined speech\nelectroencephalography (EEG). As sentences decoded from EEG can have rich\nmeanings, BCIs based on EEG decoding can achieve numerous control instructions.\nBy combining a modified EEG feature extraction mehtod with connectionist\ntemporal classification (CTC), this paper simulated decoding imagined speech\nEEG using synthetic EEG data without help of speech signal. The performance of\ndecoding model over synthetic data to a certain extent demonstrated the\nfeasibility of constructing BCI based on imagined speech brain signal.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 14:34:20 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Wang", "Kang", ""], ["Wang", "Xueqian", ""], ["Li", "Gang", ""]]}, {"id": "1705.08012", "submitter": "Thommen George Karimpanal", "authors": "Thommen Karimpanal George, Harit Maganlal Gadhia, Ruben S/O Sukumar,\n  John-John Cabibihan", "title": "Sensing discomfort of standing passengers in public rail transportation\n  systems using a smart phone", "comments": "Document prepared for IEEE International Conference on Control and\n  Automation (ICCA), 2013, 5 pages, 8 figures", "journal-ref": "10th IEEE International Conference on Control & Automation (IEEE\n  ICCA 2013), HangZhou China, June 12-14, 2013, pp. 1509-1513", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to investigate the effect of acceleration on the discomfort\nof standing passengers. The acceleration levels from different public rail\ntransport lines such as the mass rapid transits (MRTs) and light rail transits\n(LRTs) of Singapore, as well as the associated qualitative data indicating the\ndiscomfort of standing passengers were collected and analyzed. Based on a\nlogistic regression model to analyze the data, a discomfort index was\nintroduced, which can be used to compare various rail lines based on ride\ncomfort. A method for predicting the discomfort of passengers based on the\nacceleration values was proposed for any given train line.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 21:31:50 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["George", "Thommen Karimpanal", ""], ["Gadhia", "Harit Maganlal", ""], ["Sukumar", "Ruben S/O", ""], ["Cabibihan", "John-John", ""]]}, {"id": "1705.09222", "submitter": "Ashwini Jaya Kumar", "authors": "Ashwini Jaya Kumar, S\\\"oren Auer, Christoph Schmidt, Joachim k\\\"ohler", "title": "Towards a Knowledge Graph based Speech Interface", "comments": "Under Review in International Workshop on Grounding Language\n  Understanding, Satellite of Interspeech 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications which use human speech as an input require a speech interface\nwith high recognition accuracy. The words or phrases in the recognised text are\nannotated with a machine-understandable meaning and linked to knowledge graphs\nfor further processing by the target application. These semantic annotations of\nrecognised words can be represented as a subject-predicate-object triples which\ncollectively form a graph often referred to as a knowledge graph. This type of\nknowledge representation facilitates to use speech interfaces with any spoken\ninput application, since the information is represented in logical, semantic\nform, retrieving and storing can be followed using any web standard query\nlanguages. In this work, we develop a methodology for linking speech input to\nknowledge graphs and study the impact of recognition errors in the overall\nprocess. We show that for a corpus with lower WER, the annotation and linking\nof entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight,\na tool to interlink text documents with the linked open data is used to link\nthe speech recognition output to the DBpedia knowledge graph. Such a\nknowledge-based speech recognition interface is useful for applications such as\nquestion answering or spoken dialog systems.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:54:32 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Kumar", "Ashwini Jaya", ""], ["Auer", "S\u00f6ren", ""], ["Schmidt", "Christoph", ""], ["k\u00f6hler", "Joachim", ""]]}, {"id": "1705.09413", "submitter": "David Bau iii", "authors": "David Bau, Jeff Gray, Caitlin Kelleher, Josh Sheldon, Franklyn Turbak", "title": "Learnable Programming: Blocks and Beyond", "comments": null, "journal-ref": "Communications of the ACM, June 2017, pp. 72-80", "doi": "10.1145/3015455", "report-no": null, "categories": "cs.PL cs.CY cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blocks-based programming has become the lingua franca for introductory\ncoding. Studies have found that experience with blocks-based programming can\nhelp beginners learn more traditional text-based languages. We explore how\nblocks environments improve learnability for novices by 1) favoring recognition\nover recall, 2) reducing cognitive load, and 3) preventing errors. Increased\nusability of blocks programming has led to widespread adoption within\nintroductory programming contexts across a range of ages. Ongoing work explores\nfurther reducing barriers to programming, supporting novice programmers in\nexpanding their programming skills, and transitioning to textual programming.\nNew blocks frameworks are making it easier to access a variety of APIs through\nblocks environments, opening the doors to a greater diversity of programming\ndomains and supporting greater experimentation for novices and professionals\nalike.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 02:25:19 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Bau", "David", ""], ["Gray", "Jeff", ""], ["Kelleher", "Caitlin", ""], ["Sheldon", "Josh", ""], ["Turbak", "Franklyn", ""]]}, {"id": "1705.09656", "submitter": "Claudia Orellana-Rodriguez", "authors": "Terrence Szymanski, Claudia Orellana-Rodriguez and Mark T. Keane", "title": "Helping News Editors Write Better Headlines: A Recommender to Improve\n  the Keyword Contents & Shareability of News Headlines", "comments": null, "journal-ref": "Natural Language Processing meets Journalism. IJCAI-16 workshop.\n  Pages 30-34. 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a software tool that employs state-of-the-art natural language\nprocessing (NLP) and machine learning techniques to help newspaper editors\ncompose effective headlines for online publication. The system identifies the\nmost salient keywords in a news article and ranks them based on both their\noverall popularity and their direct relevance to the article. The system also\nuses a supervised regression model to identify headlines that are likely to be\nwidely shared on social media. The user interface is designed to simplify and\nspeed the editor's decision process on the composition of the headline. As\nsuch, the tool provides an efficient way to combine the benefits of automated\npredictors of engagement and search-engine optimization (SEO) with human\njudgments of overall headline quality.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 17:40:58 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Szymanski", "Terrence", ""], ["Orellana-Rodriguez", "Claudia", ""], ["Keane", "Mark T.", ""]]}, {"id": "1705.10294", "submitter": "Tariq Abbasi", "authors": "Tariq Abbasi and Hans Weigand", "title": "The Impact of Digital Financial Services on Firm's Performance: a\n  Literature Review", "comments": "15 pages, 7 tables and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Financial Services continue to expand and replace the delivery of\ntraditional banking services to the customers through innovative technologies\nto meet the growing complex needs and globalization challenges. These\ndiversified digital products help the organizations (service providers) to\nimprove their firm performance and to remain competitive in the market. It also\nassists in increasing market share to grow their profitability and improve\nfinancial position. There is a growing literature on Digital Financial Services\nand firm performance. At this point of the development, this paper systemically\nreviews the existing (within last one decade) amount of literature\ninvestigating the impact of DFS on firm performance, analyzes and identifies\nthe research gaps. We identify 39 works that have appeared in a wide range of\npeer-reviewed scientific journals. We classify the methodologies and approaches\nthat researchers have used to predict the effect of such services on the\nfinancial growth and profitability. We observe that despite rapid technological\nadvancement in DFS during the last ten years, Digital Financial Services being\nthe factor affecting firm performance did not get the reasonable attention in\nacademic literature. One of the reason is that almost all the authors limit\ntheir research to banking sector while ignoring others particularly mobile\nnetwork operators (providing branchless banking) and new non-banking entrants.\nWe also notice that newer researchers often ignore past research and\ninvestigate the same issues. This study also makes several recommendations and\nsuggest directions for future research in this still emerging field.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 20:22:08 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Abbasi", "Tariq", ""], ["Weigand", "Hans", ""]]}, {"id": "1705.11181", "submitter": "Ayushman Dash", "authors": "Ayushman Dash, Amit Sahu, Rajveer Shringi, John Cristian Borges\n  Gamboa, Muhammad Zeshan Afzal, Muhammad Imran Malik, Sheraz Ahmed, Andreas\n  Dengel", "title": "AirScript - Creating Documents in Air", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach, called AirScript, for creating,\nrecognizing and visualizing documents in air. We present a novel algorithm,\ncalled 2-DifViz, that converts the hand movements in air (captured by a\nMyo-armband worn by a user) into a sequence of x, y coordinates on a 2D\nCartesian plane, and visualizes them on a canvas. Existing sensor-based\napproaches either do not provide visual feedback or represent the recognized\ncharacters using prefixed templates. In contrast, AirScript stands out by\ngiving freedom of movement to the user, as well as by providing a real-time\nvisual feedback of the written characters, making the interaction natural.\nAirScript provides a recognition module to predict the content of the document\ncreated in air. To do so, we present a novel approach based on deep learning,\nwhich uses the sensor data and the visualizations created by 2-DifViz. The\nrecognition module consists of a Convolutional Neural Network (CNN) and two\nGated Recurrent Unit (GRU) Networks. The output from these three networks is\nfused to get the final prediction about the characters written in air.\nAirScript can be used in highly sophisticated environments like a smart\nclassroom, a smart factory or a smart laboratory, where it would enable people\nto annotate pieces of texts wherever they want without any reference surface.\nWe have evaluated AirScript against various well-known learning models (HMM,\nKNN, SVM, etc.) on the data of 12 participants. Evaluation results show that\nthe recognition module of AirScript largely outperforms all of these models by\nachieving an accuracy of 91.7% in a person independent evaluation and a 96.7%\naccuracy in a person dependent evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 12:43:31 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Dash", "Ayushman", ""], ["Sahu", "Amit", ""], ["Shringi", "Rajveer", ""], ["Gamboa", "John Cristian Borges", ""], ["Afzal", "Muhammad Zeshan", ""], ["Malik", "Muhammad Imran", ""], ["Ahmed", "Sheraz", ""], ["Dengel", "Andreas", ""]]}]