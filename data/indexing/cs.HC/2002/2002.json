[{"id": "2002.00044", "submitter": "Timothy Herr PhD", "authors": "Timothy M. Herr, Therese A. Nelson, and Justin B. Starren", "title": "Design Principles and Clinician Preferences for Pharmacogenomic Clinical\n  Decision Support Alerts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OBJECTIVE: To better understand clinician needs and preferences for the\ndisplay of pharmacogenomic (PGx) information in clinical decision support (CDS)\ntools.\n  MATERIALS AND METHODS: We developed a semi-structured interview to collect\nfeedback and preferences in six key areas of PGx CDS design, from clinicians\nwho had prior experience with live PGx CDS tools. Eight clinicians from\nNorthwestern Medicine's (NM) General Internal Medicine clinic participated in\nthe study.\n  RESULTS: Clinicians expressed preference for interruptive pop-up alerts\nduring order entry, brief descriptions of relevant drug-gene interactions, and\na clear and specific recommended alternative course of action when a medication\nis contraindicated. They did not wish to see detailed genetic data, preferring\nphenotypic information predicted from the genotype. Nor did they wish to be\ninterrupted when genetic test results do not indicate a change in treatment\nplan. Clinicians reported little familiarity with Clinical Pharmacogenetic\nImplementation Consortium prescribing recommendations but reported trusting\nrecommendations of their professional societies and resources like UpToDate.\nAnalysis of unstructured comments concurred with structured results, indicating\na general uncertainty among participants around how to interpret and apply PGx\ninformation in practice.\n  DISCUSSION: Results point to several underlying principles that can inform\nfuture PGx CDS alert designs: Be Specific and Actionable; Be Brief; Display\nPhenotypes not Genotypes; Rely on Sources Clinicians Already Trust; and, Be\nAdaptable to Learning Effects.\n  CONCLUSION: This study is part of a broader socio-technical design approach\nto PGx CDS design underway at NM and provides a baseline for future PGx CDS\ndevelopment. Designs based on these results have the potential to improve\nclinician education and adherence levels, and to improve patient outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 20:18:13 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Herr", "Timothy M.", ""], ["Nelson", "Therese A.", ""], ["Starren", "Justin B.", ""]]}, {"id": "2002.00047", "submitter": "Timothy Herr PhD", "authors": "Timothy M. Herr, Therese A. Nelson, and Luke V. Rasmussen, and Yinan\n  Zheng, Nicola Lancki, MPH and Justin B. Starren", "title": "Design Principles Developed through User-Centered and Socio-Technical\n  Methods Improve Clinician Satisfaction, Speed, and Confidence in\n  Pharmacogenomic Clinical Decision Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OBJECTIVE: To design and evaluate new pharmacogenomic (PGx) clinical decision\nsupport (CDS) alerts, built to adhere to PGx CDS design principles developed\nthrough socio-technical approaches.\n  MATERIALS AND METHODS: Based on previously identified design principles, we\ncreated 11 new PGx CDS alert designs and developed an interactive web\napplication containing realistic clinical scenarios and user workflows that\nmimicked a real-world EHR system. We recruited General Internal Medicine and\nCardiology clinicians from Northwestern Medicine and recorded their\ninteractions with the original and new designs. We measured clinician response,\nsatisfaction, speed, and confidence through questionnaires and analysis of the\nrecordings.\n  RESULTS: The study included 12 clinicians. Participants were significantly\nmore satisfied (p=0.0000001), faster (p=0.009), and more confident (p<.05) with\nthe new designs than the original ones. The study lacked statistical power to\ndetermine whether prescribing accuracy was improved, but participants were no\nless accurate, and clinical actions were more concordant with alert\ninteractions (p=0.004) with the new designs. We found a significant learning\ncurve associated with the original designs, which was eliminated with the new\ndesigns.\n  DISCUSSION: This study successfully demonstrates that socio-technical and\nuser-centered design techniques can improve PGx CDS alert designs. Best\npractices for PGx CDS design are limited in the literature, with few\neffectiveness studies available. These results can help guide future PGx CDS\nimplementations to be more clinician friendly and less time-consuming.\n  CONCLUSION: The results of this study support the PGx CDS design principles\nwe proposed in previous work. As a next step, the new designs should be\nimplemented in a live setting for further validation.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 20:24:26 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Herr", "Timothy M.", ""], ["Nelson", "Therese A.", ""], ["Rasmussen", "Luke V.", ""], ["Zheng", "Yinan", ""], ["Lancki", "Nicola", ""], ["MPH", "", ""], ["Starren", "Justin B.", ""]]}, {"id": "2002.00091", "submitter": "Scott Carter", "authors": "Scott A. Carter, Daniel Avrahami, Nami Tokunaga", "title": "Using Inaudible Audio to Improve Indoor-Localization- and\n  Proximity-Aware Intelligent Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it is often critical for indoor-location- and proximity-aware\napplications to know whether a user is in a space or not (e.g., a specific room\nor office), a key challenge is that the difference between standing on one side\nor another of a doorway or wall is well within the error range of most RF-based\napproaches. In this work, we address this challenge by augmenting RF-based\nlocalization and proximity detection with active ultrasonic sensing, taking\nadvantage of the limited propagation of sound waves. This simple and\ncost-effective approach can allow, for example, a Bluetooth smart-lock to\ndiscern whether a user is inside or outside their home in order to lock or\nunlock doors automatically. We describe a configurable architecture for our\nsolution and present experiments that validate this approach but also\ndemonstrate that different user behavior and application needs can impact\nsystem configuration decisions. Finally, we describe applications that could\nbenefit from our solution and address privacy concerns.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 23:04:16 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Carter", "Scott A.", ""], ["Avrahami", "Daniel", ""], ["Tokunaga", "Nami", ""]]}, {"id": "2002.00152", "submitter": "Waqas Ahmed", "authors": "Waqas Ahmed, Sheikh Muhamad Hizam, Ilham Sentosa, Habiba Akter, Eiad\n  Yafi, Jawad Ali", "title": "Predicting IoT Service Adoption towards Smart Mobility in Malaysia:\n  SEM-Neural Hybrid Pilot Study", "comments": "12 pages, 08 figures, 05 tables", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications (IJACSA), Vol. 11, No. 1, 2020", "doi": "10.14569/IJACSA.2020.0110165", "report-no": null, "categories": "cs.HC stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smart city is synchronized with digital environment and its transportation\nsystem is vitalized with RFID sensors, Internet of Things (IoT) and Artificial\nIntelligence. However, without user's behavioral assessment of technology, the\nultimate usefulness of smart mobility cannot be achieved. This paper aims to\nformulate the research framework for prediction of antecedents of smart\nmobility by using SEM-Neural hybrid approach towards preliminary data analysis.\nThis research undertook smart mobility services adoption in Malaysia as study\nperspective and applied the Technology Acceptance Model (TAM) as theoretical\nbasis. An extended TAM model was hypothesized with five external factors\n(digital dexterity, IoT service quality, intrusiveness concerns, social\nelectronic word of mouth and subjective norm). The data was collected through a\npilot survey in Klang Valley, Malaysia. Then responses were analyzed for\nreliability, validity and accuracy of model. Finally, the causal relationship\nwas explained by Structural Equation Modeling (SEM) and Artificial Neural\nNetworking (ANN). The paper will share better understanding of road technology\nacceptance to all stakeholders to refine, revise and update their policies. The\nproposed framework will suggest a broader approach to individual level\ntechnology acceptance.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 06:34:58 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 06:11:27 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ahmed", "Waqas", ""], ["Hizam", "Sheikh Muhamad", ""], ["Sentosa", "Ilham", ""], ["Akter", "Habiba", ""], ["Yafi", "Eiad", ""], ["Ali", "Jawad", ""]]}, {"id": "2002.00511", "submitter": "Sara Mohammad Taheri Mrs", "authors": "Sara Mohammad Taheri, Omkar Terse, Eralp Dogu, Magy Seif El-Nasr, Olga\n  Vitek", "title": "Investigating usability of MSstatsQC software", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MSstatsQC [3] is an open-source software that provides longitudinal system\nsuitability monitoring tools in the form of control charts for proteomic\nexperiments. It includes simultaneous tools for the mean and dispersion of\nsuitability metrics and presents alternative methods of monitoring through\ndifferent tabs that are designed in the interface. This research focuses on\ninvestigating the usability of MSstatsQC software and the interpretability of\nthe designed plots. In this study, we ask 4 test users, from the proteomics\nfield, to complete a series of tasks and questionnaires. The tasks are designed\nto test the usability of the software in terms of importing data files,\nselecting appropriate metrics, guide set, and peptides, and finally creating\ndecision rules (tasks 1 and 3 in appendix). The questionnaires ask about\ninterpretability of the plots including control charts, box plots, heat maps,\nriver plots, and radar plots (tasks 1 and 4 in appendix). The goal of the\nquestions is to determine if the test users understand the plots and can\ninterpret them. Results show limitations in usability and plot\ninterpretability, especially in the data import section. We suggest the\nfollowing modifications. I) providing conspicuous guides close to the window\nrelated to up-loading a datafile as well as providing error messages that\npop-up when the data set has a wrong format II) providing plot descriptions,\nhints to interpret plots, plot titles and appropriate axis labels, and, III)\nNumbering tabs to show the flow of procedures in the software.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 23:35:35 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Taheri", "Sara Mohammad", ""], ["Terse", "Omkar", ""], ["Dogu", "Eralp", ""], ["El-Nasr", "Magy Seif", ""], ["Vitek", "Olga", ""]]}, {"id": "2002.00556", "submitter": "Jeong-Hyun Cho", "authors": "Jeong-Hyun Cho, Ji-Hoon Jeong, Dong-Joo Kim, and Seong-Whan Lee", "title": "A novel approach to classify natural grasp actions by estimating muscle\n  activity patterns from EEG signals", "comments": "4 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing electroencephalogram (EEG) based brain-computer interface (BCI)\nsystems is challenging. In this study, we analyzed natural grasp actions from\nEEG. Ten healthy subjects participated in this experiment. They executed and\nimagined three sustained grasp actions. We proposed a novel approach which\nestimates muscle activity patterns from EEG signals to improve the overall\nclassification accuracy. For implementation, we have recorded EEG and\nelectromyogram (EMG) simultaneously. Using the similarity of the estimated\npattern from EEG signals compare to the activity pattern from EMG signals\nshowed higher classification accuracy than competitive methods. As a result, we\nobtained the average classification accuracy of 63.89($\\pm$7.54)% for actual\nmovement and 46.96($\\pm$15.30)% for motor imagery. These are 21.59% and 5.66%\nhigher than the result of the competitive model, respectively. This result is\nencouraging, and the proposed method could potentially be used in future\napplications, such as a BCI-driven robot control for handling various daily use\nobjects.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 04:40:17 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Cho", "Jeong-Hyun", ""], ["Jeong", "Ji-Hoon", ""], ["Kim", "Dong-Joo", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2002.00732", "submitter": "Xiang Liu", "authors": "Junhan Zhao, Xiang Liu, Chen Guo, Zhenyu Cheryl Qian, Yingjie Victor\n  Chen", "title": "Phoenixmap: An Abstract Approach to Visualize 2D Spatial Distributions", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2945960", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multidimensional nature of spatial data poses a challenge for\nvisualization. In this paper, we introduce Phoenixmap, a simple abstract\nvisualization method to address the issue of visualizing multiple spatial\ndistributions at once. The Phoenixmap approach starts by identifying the\nenclosed outline of the point collection, then assigns different widths to\noutline segments according to the segments' corresponding inside regions. Thus,\none 2D distribution is represented as an outline with varied thicknesses.\nPhoenixmap is capable of overlaying multiple outlines and comparing them across\ncategories of objects in a 2D space. We chose heatmap as a benchmark spatial\nvisualization method and conducted user studies to compare performances among\nPhoenixmap, heatmap, and dot distribution map. Based on the analysis and\nparticipant feedback, we demonstrate that Phoenixmap 1) allows users to\nperceive and compare spatial distribution data efficiently; 2) frees up\ngraphics space with a concise form that can provide visualization design\npossibilities like overlapping; and 3) provides a good quantitative perceptual\nestimating capability given the proper legends. Finally, we discuss several\npossible applications of Phoenixmap and present one visualization of multiple\nspecies of birds' active regions in a nature preserve.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 20:13:45 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Zhao", "Junhan", ""], ["Liu", "Xiang", ""], ["Guo", "Chen", ""], ["Qian", "Zhenyu Cheryl", ""], ["Chen", "Yingjie Victor", ""]]}, {"id": "2002.00747", "submitter": "Maartje ter Hoeve", "authors": "Maartje ter Hoeve, Robert Sim, Elnaz Nouri, Adam Fourney, Maarten de\n  Rijke, Ryen W. White", "title": "Conversations with Documents. An Exploration of Document-Centered\n  Assistance", "comments": "Accepted as full paper at CHIIR 2020; 9 pages + Appendix", "journal-ref": null, "doi": "10.1145/3343413.3377971", "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of conversational assistants has become more prevalent in helping\npeople increase their productivity. Document-centered assistance, for example\nto help an individual quickly review a document, has seen less significant\nprogress, even though it has the potential to tremendously increase a user's\nproductivity. This type of document-centered assistance is the focus of this\npaper. Our contributions are three-fold: (1) We first present a survey to\nunderstand the space of document-centered assistance and the capabilities\npeople expect in this scenario. (2) We investigate the types of queries that\nusers will pose while seeking assistance with documents, and show that\ndocument-centered questions form the majority of these queries. (3) We present\na set of initial machine learned models that show that (a) we can accurately\ndetect document-centered questions, and (b) we can build reasonably accurate\nmodels for answering such questions. These positive results are encouraging,\nand suggest that even greater results may be attained with continued study of\nthis interesting and novel problem space. Our findings have implications for\nthe design of intelligent systems to support task completion via natural\ninteractions with documents.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 17:10:11 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["ter Hoeve", "Maartje", ""], ["Sim", "Robert", ""], ["Nouri", "Elnaz", ""], ["Fourney", "Adam", ""], ["de Rijke", "Maarten", ""], ["White", "Ryen W.", ""]]}, {"id": "2002.00762", "submitter": "Kartik Talamadupula", "authors": "Mayank Agarwal, Jorge J. Barroso, Tathagata Chakraborti, Eli M. Dow,\n  Kshitij Fadnis, Borja Godoy, Madhavan Pallan, Kartik Talamadupula", "title": "Project CLAI: Instrumenting the Command Line as a New Environment for AI\n  Agents", "comments": "http://ibm.biz/clai-home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This whitepaper reports on Project CLAI (Command Line AI), which aims to\nbring the power of AI to the command line interface (CLI). The CLAI platform\nsets up the CLI as a new environment for AI researchers to conquer by surfacing\nthe command line as a generic environment that researchers can interface to\nusing a simple sense-act API, much like the traditional AI agent architecture.\nIn this paper, we discuss the design and implementation of the platform in\ndetail, through illustrative use cases of new end user interaction patterns\nenabled by this design, and through quantitative evaluation of the system\nfootprint of a CLAI-enabled terminal. We also report on some early user\nfeedback on CLAI's features from an internal survey.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 05:01:05 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 23:11:15 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Agarwal", "Mayank", ""], ["Barroso", "Jorge J.", ""], ["Chakraborti", "Tathagata", ""], ["Dow", "Eli M.", ""], ["Fadnis", "Kshitij", ""], ["Godoy", "Borja", ""], ["Pallan", "Madhavan", ""], ["Talamadupula", "Kartik", ""]]}, {"id": "2002.00764", "submitter": "Mehdi Ghatee Dr.", "authors": "Ruhallah Ahmadian, Mehdi Ghatee", "title": "Driver Identification by Neural Network on Extracted Statistical\n  Features from Smartphone Data", "comments": "13 pages, 2 Figures, 5 Tables, The 18th International Conference on\n  Traffic and Transportation Engineering, 2020, Tehran, Iran", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The future of transportation is driven by the use of artificial intelligence\nto improve living and transportation. This paper presents a neural\nnetwork-based system for driver identification using data collected by a\nsmartphone. This system identifies the driver automatically, reliably and in\nreal-time without the need for facial recognition and also does not violate\nprivacy. The system architecture consists of three modules data collection,\npreprocessing and identification. In the data collection module, the data of\nthe accelerometer and gyroscope sensors are collected using a smartphone. The\npreprocessing module includes noise removal, data cleaning, and segmentation.\nIn this module, lost values will be retrieved and data of stopped vehicle will\nbe deleted. Finally, effective statistical properties are extracted from\ndata-windows. In the identification module, machine learning algorithms are\nused to identify drivers' patterns. According to experiments, the best\nalgorithm for driver identification is MLP with a maximum accuracy of 96%. This\nsolution can be used in future transportation to develop driver-based insurance\nsystems as well as the development of systems used to apply penalties and\nincentives.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 23:49:52 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 22:05:59 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Ahmadian", "Ruhallah", ""], ["Ghatee", "Mehdi", ""]]}, {"id": "2002.00772", "submitter": "Philipp Wei{\\ss}", "authors": "Ahmed Alqaraawi, Martin Schuessler, Philipp Wei{\\ss}, Enrico Costanza\n  and Nadia Berthouze", "title": "Evaluating Saliency Map Explanations for Convolutional Neural Networks:\n  A User Study", "comments": "10 pages, 6 figures, accepted long paper for ACM IUI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) offer great machine learning performance\nover a range of applications, but their operation is hard to interpret, even\nfor experts. Various explanation algorithms have been proposed to address this\nissue, yet limited research effort has been reported concerning their user\nevaluation. In this paper, we report on an online between-group user study\ndesigned to evaluate the performance of \"saliency maps\" - a popular explanation\nalgorithm for image classification applications of CNNs. Our results indicate\nthat saliency maps produced by the LRP algorithm helped participants to learn\nabout some specific image features the system is sensitive to. However, the\nmaps seem to provide very limited help for participants to anticipate the\nnetwork's output for new images. Drawing on our findings, we highlight\nimplications for design and further research on explainable AI. In particular,\nwe argue the HCI and AI communities should look beyond instance-level\nexplanations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 14:17:11 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Alqaraawi", "Ahmed", ""], ["Schuessler", "Martin", ""], ["Wei\u00df", "Philipp", ""], ["Costanza", "Enrico", ""], ["Berthouze", "Nadia", ""]]}, {"id": "2002.00918", "submitter": "Aythami Morales", "authors": "Alejandro Acien and Aythami Morales and Julian Fierrez and Ruben\n  Vera-Rodriguez and Ivan Bartolome", "title": "BeCAPTCHA: Detecting Human Behavior in Smartphone Interaction using\n  Multiple Inbuilt Sensors", "comments": "AAAI-20 Workshop on Artificial Intelligence for Ciber Security\n  (AICS), New York, NY, USA, February 2020. AICS-2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel multimodal mobile database called HuMIdb (Human Mobile\nInteraction database) that comprises 14 mobile sensors acquired from 600 users.\nThe heterogeneous flow of data generated during the interaction with the\nsmartphones can be used to model human behavior when interacting with the\ntechnology. Based on this new dataset, we explore the capacity of smartphone\nsensors to improve bot detection. We propose a CAPTCHA method based on the\nanalysis of the information obtained during a single drag and drop task. We\nevaluate the method generating fake samples synthesized with Generative\nAdversarial Neural Networks and handcrafted methods. Our results suggest the\npotential of mobile sensors to characterize the human behavior and develop a\nnew generation of CAPTCHAs.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 17:56:56 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 17:12:20 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 09:22:10 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Acien", "Alejandro", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Vera-Rodriguez", "Ruben", ""], ["Bartolome", "Ivan", ""]]}, {"id": "2002.00940", "submitter": "David Glowacki", "authors": "David R. Glowacki, Mark D. Wonnacott, Rachel Freire, Becca R.\n  Glowacki, Ella M. Gale, James E. Pike, Tiu de Haan, Mike Chatziapostolou,\n  Oussama Metatla", "title": "Isness: Using Multi-Person VR to Design Peak Mystical-Type Experiences\n  Comparable to Psychedelics", "comments": null, "journal-ref": "CHI 2020: Proceedings of the 2020 CHI Conference on Human Factors\n  in Computing Systems", "doi": "10.1145/3313831.3376649", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Studies combining psychotherapy with psychedelic drugs (PsiDs) have\ndemonstrated positive outcomes that are often associated with PsiDs' ability to\ninduce 'mystical-type' experiences (MTEs) - i.e., subjective experiences whose\ncharacteristics include a sense of connectedness, transcendence, and\nineffability. We suggest that both PsiDs and virtual reality can be situated on\na broader spectrum of psychedelic technologies. To test this hypothesis, we\nused concepts, methods, and analysis strategies from PsiD research to design\nand evaluate 'Isness', a multi-person VR journey where participants experience\nthe collective emergence, fluctuation, and dissipation of their bodies as\nenergetic essences. A study (N=57) analyzing participant responses to a\ncommonly used PsiD experience questionnaire (MEQ30) indicates that Isness\nparticipants had MTEs comparable to those reported in double-blind clinical\nstudies after high doses of psilocybin & LSD. Within a supportive setting and\nconceptual framework, VR phenomenology can create the conditions for MTEs from\nwhich participants derive insight and meaning.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 18:58:09 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 16:47:53 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Glowacki", "David R.", ""], ["Wonnacott", "Mark D.", ""], ["Freire", "Rachel", ""], ["Glowacki", "Becca R.", ""], ["Gale", "Ella M.", ""], ["Pike", "James E.", ""], ["de Haan", "Tiu", ""], ["Chatziapostolou", "Mike", ""], ["Metatla", "Oussama", ""]]}, {"id": "2002.00941", "submitter": "Andreea Bobu", "authors": "Andreea Bobu, Andrea Bajcsy, Jaime F. Fisac, Sampada Deglurkar, Anca\n  D. Dragan", "title": "Quantifying Hypothesis Space Misspecification in Learning from\n  Human-Robot Demonstrations and Physical Corrections", "comments": "20 pages. 12 figures, 1 table. IEEE Transactions on Robotics, 2020", "journal-ref": null, "doi": "10.1109/TRO.2020.2971415", "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human input has enabled autonomous systems to improve their capabilities and\nachieve complex behaviors that are otherwise challenging to generate\nautomatically. Recent work focuses on how robots can use such input - like\ndemonstrations or corrections - to learn intended objectives. These techniques\nassume that the human's desired objective already exists within the robot's\nhypothesis space. In reality, this assumption is often inaccurate: there will\nalways be situations where the person might care about aspects of the task that\nthe robot does not know about. Without this knowledge, the robot cannot infer\nthe correct objective. Hence, when the robot's hypothesis space is\nmisspecified, even methods that keep track of uncertainty over the objective\nfail because they reason about which hypothesis might be correct, and not\nwhether any of the hypotheses are correct. In this paper, we posit that the\nrobot should reason explicitly about how well it can explain human inputs given\nits hypothesis space and use that situational confidence to inform how it\nshould incorporate human input. We demonstrate our method on a 7\ndegree-of-freedom robot manipulator in learning from two important types of\nhuman input: demonstrations of manipulation tasks, and physical corrections\nduring the robot's task execution.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 18:59:23 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 23:59:41 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Bobu", "Andreea", ""], ["Bajcsy", "Andrea", ""], ["Fisac", "Jaime F.", ""], ["Deglurkar", "Sampada", ""], ["Dragan", "Anca D.", ""]]}, {"id": "2002.01025", "submitter": "Christian Krupitzer", "authors": "Christian Krupitzer (1), Sebastian M\\\"uller (2), Veronika Lesch (1),\n  Marwin Z\\\"ufle (1), Janick Edinger (2), Alexander Lemken (3), Dominik\n  Sch\\\"afer (4), Samuel Kounev (1), Christian Becker (2) ((1) University of\n  W\\\"urzburg, W\\\"urzburg, Germany, (2) University of Mannheim, Mannheim,\n  Germany, (3) ioxp GmbH, Mannheim, Germany, (4) Syntax Systems GmbH, Weinheim,\n  Germany)", "title": "A Survey on Human Machine Interaction in Industry 4.0", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 or Industrial IoT both describe new paradigms for seamless\ninteraction between humans and machines. Both concepts rely on intelligent,\ninter-connected cyber-physical production systems that are able to control the\nprocess flow of industrial production. As those machines take many decisions\nautonomously and further interact with production and manufacturing planning\nsystems, the integration of human users requires new paradigms. In this paper,\nwe provide an analysis of the current state-of-the-art in human-machine\ninteraction in the Industry 4.0 domain.We focus on new paradigms that integrate\nthe application of augmented and virtual reality technology. Based on our\nanalysis, we further provide a discussion of research challenges.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 21:53:48 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Krupitzer", "Christian", ""], ["M\u00fcller", "Sebastian", ""], ["Lesch", "Veronika", ""], ["Z\u00fcfle", "Marwin", ""], ["Edinger", "Janick", ""], ["Lemken", "Alexander", ""], ["Sch\u00e4fer", "Dominik", ""], ["Kounev", "Samuel", ""], ["Becker", "Christian", ""]]}, {"id": "2002.01077", "submitter": "Sunshine Chong", "authors": "Sunshine Chong, Andr\\'es Abeliuk", "title": "Quantifying the Effects of Recommendation Systems", "comments": "8 pages, 6 figures, accepted into the National Symposium of IEEE Big\n  Data 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems today exert a strong influence on consumer behavior\nand individual perceptions of the world. By using collaborative filtering (CF)\nmethods to create recommendations, it generates a continuous feedback loop in\nwhich user behavior becomes magnified in the algorithmic system. Popular items\nget recommended more frequently, creating the bias that affects and alters user\npreferences. In order to visualize and compare the different biases, we will\nanalyze the effects of recommendation systems and quantify the inequalities\nresulting from them.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 01:21:46 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Chong", "Sunshine", ""], ["Abeliuk", "Andr\u00e9s", ""]]}, {"id": "2002.01085", "submitter": "Young-Eun Lee", "authors": "Young-Eun Lee, Minji Lee", "title": "Decoding Visual Responses based on Deep Neural Networks with Ear-EEG\n  Signals", "comments": "6 pages, 3 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, practical brain-computer interface is actively carried out,\nespecially, in an ambulatory environment. However, the electroencephalography\nsignals are distorted by movement artifacts and electromyography signals in\nambulatory condition, which make hard to recognize human intention. In\naddition, as hardware issues are also challenging, ear-EEG has been developed\nfor practical brain-computer interface and is widely used. However, ear-EEG\nstill contains contaminated signals. In this paper, we proposed robust\ntwo-stream deep neural networks in walking conditions and analyzed the visual\nresponse EEG signals in the scalp and ear in terms of statistical analysis and\nbrain-computer interface performance. We validated the signals with the visual\nresponse paradigm, steady-state visual evoked potential. The brain-computer\ninterface performance deteriorated as 3~14% when walking fast at 1.6 m/s. When\napplying the proposed method, the accuracies increase 15% in cap-EEG and 7% in\near-EEG. The proposed method shows robust to the ambulatory condition in\nsession dependent and session-to-session experiments.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 02:07:04 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Lee", "Young-Eun", ""], ["Lee", "Minji", ""]]}, {"id": "2002.01092", "submitter": "Upol Ehsan", "authors": "Upol Ehsan and Mark O. Riedl", "title": "Human-centered Explainable AI: Towards a Reflective Sociotechnical\n  Approach", "comments": "In Proceedings of HCI International 2020: 22nd International\n  Conference On Human-Computer Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanations--a form of post-hoc interpretability--play an instrumental role\nin making systems accessible as AI continues to proliferate complex and\nsensitive sociotechnical systems. In this paper, we introduce Human-centered\nExplainable AI (HCXAI) as an approach that puts the human at the center of\ntechnology design. It develops a holistic understanding of \"who\" the human is\nby considering the interplay of values, interpersonal dynamics, and the\nsocially situated nature of AI systems. In particular, we advocate for a\nreflective sociotechnical approach. We illustrate HCXAI through a case study of\nan explanation system for non-technical end-users that shows how technical\nadvancements and the understanding of human factors co-evolve. Building on the\ncase study, we lay out open research questions pertaining to further refining\nour understanding of \"who\" the human is and extending beyond 1-to-1\nhuman-computer interactions. Finally, we propose that a reflective HCXAI\nparadigm-mediated through the perspective of Critical Technical Practice and\nsupplemented with strategies from HCI, such as value-sensitive design and\nparticipatory design--not only helps us understand our intellectual blind\nspots, but it can also open up new design and research spaces.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 02:30:33 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 05:33:14 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Ehsan", "Upol", ""], ["Riedl", "Mark O.", ""]]}, {"id": "2002.01111", "submitter": "Keri Mallari", "authors": "Keri Mallari, Kori Inkpen, Paul Johns, Sarah Tan, Divya Ramesh, Ece\n  Kamar", "title": "Do I Look Like a Criminal? Examining how Race Presentation Impacts Human\n  Judgement of Recidivism", "comments": "This paper has been accepted for publication at CHI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how racial information impacts human decision making in online\nsystems is critical in today's world. Prior work revealed that race information\nof criminal defendants, when presented as a text field, had no significant\nimpact on users' judgements of recidivism. We replicated and extended this work\nto explore how and when race information influences users' judgements, with\nrespect to the saliency of presentation. Our results showed that adding photos\nto the race labels had a significant impact on recidivism predictions for users\nwho identified as female, but not for those who identified as male. The race of\nthe defendant also impacted these results, with black defendants being less\nlikely to be predicted to recidivate compared to white defendants. These\nresults have strong implications for how system-designers choose to display\nrace information, and cautions researchers to be aware of gender and race\neffects when using Amazon Mechanical Turk workers.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 03:51:01 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Mallari", "Keri", ""], ["Inkpen", "Kori", ""], ["Johns", "Paul", ""], ["Tan", "Sarah", ""], ["Ramesh", "Divya", ""], ["Kamar", "Ece", ""]]}, {"id": "2002.01116", "submitter": "Hyeong-Jin Kim", "authors": "Hyeong-Jin Kim, Min-Ho Lee, Minji Lee", "title": "A BCI based Smart Home System Combined with Event-related Potentials and\n  Speech Imagery Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, smart home systems based on brain-computer interface (BCI) has\nattracted a wide range of interests in both industry and academia. However, the\ncurrent BCI system has several shortcomings as it produces a comparatively\nlower accuracy for real-time implementations as well as the intuitive paradigm\nfor the users cannot be well established here. Therefore, in this study, we\nproposed a highly intuitive BCI paradigm that combines event-related potential\n(ERP) with the speech-imagery task for the individual target objects. The\ndecoding accuracy of the proposed paradigm was 88.1% (plus or minus 5.90) which\nis a much significant higher performance than a conventional ERP system.\nFurthermore, the amplitude of N700 components was significantly enhanced over\nfrontal regions which are priory evoked by the speech-imagery task. Our results\ncould be utilized to develop a smart home system so that it could be more\nuser-friendly and convenient by means of delivering user's intentions both,\nintuitively and accurately.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 04:18:16 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Kim", "Hyeong-Jin", ""], ["Lee", "Min-Ho", ""], ["Lee", "Minji", ""]]}, {"id": "2002.01117", "submitter": "Seo-Hyun Lee", "authors": "Seo-Hyun Lee, Minji Lee, Seong-Whan Lee", "title": "Spatio-Temporal Dynamics of Visual Imagery for Intuitive Brain-Computer\n  Interface", "comments": "5 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual imagery is an intuitive brain-computer interface paradigm, referring\nto the emergence of the visual scene. Despite its convenience, analysis of its\nintrinsic characteristics is limited. In this study, we demonstrate the effect\nof time interval and channel selection that affects the decoding performance of\nthe multi-class visual imagery. We divided the epoch into time intervals of 0-1\ns and 1-2 s and performed six-class classification in three different brain\nregions: whole brain, visual cortex, and prefrontal cortex. In the time\ninterval, 0-1 s group showed 24.2 % of average classification accuracy, which\nwas significantly higher than the 1-2 s group in the prefrontal cortex. In the\nthree different regions, the classification accuracy of the prefrontal cortex\nshowed significantly higher performance than the visual cortex in 0-1 s\ninterval group, implying the cognitive arousal during the visual imagery. This\nfinding would provide crucial information in improving the decoding\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 04:25:53 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 05:05:48 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Lee", "Seo-Hyun", ""], ["Lee", "Minji", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2002.01120", "submitter": "Byoung-Hee Kwon", "authors": "Byoung-Hee Kwon, Ji-Hoon Jeong, Dong-Joo Kim", "title": "A Novel Framework for Visual Motion Imagery Classification Using 3D\n  Virtual BCI Platform", "comments": "5 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, 3D brain-computer interface (BCI) training platforms were used\nto stimulate the subjects for visual motion imagery and visual perception. We\nmeasured the activation brain region and alpha-band power activity when the\nsubjects perceived and imagined the stimuli. Based on this, 4-class were\nclassified in visual stimuli session and visual motion imagery session\nrespectively. The results showed that the occipital region is involved in\nvisual perception and visual motion imagery, and alpha-band power is increased\nin visual motion imagery session and decreased in visual motion stimuli\nsession. Compared with the performance of visual motion imagery and motor\nimagery, visual motion imagery has higher performance than motor imagery. The\nbinary class was classified using one versus rest approach as well as analysis\nof brain activation to prove that visual-related brain wave signals are\nmeaningful, and the results were significant.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 04:30:38 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Kwon", "Byoung-Hee", ""], ["Jeong", "Ji-Hoon", ""], ["Kim", "Dong-Joo", ""]]}, {"id": "2002.01121", "submitter": "Do-Yeun Lee", "authors": "D.-Y. Lee, J.-H. Jeong, K.-H. Shim, D.-J. Kim", "title": "Classification of Upper Limb Movements \\newline Using Convolutional\n  Neural Network \\newline with 3D Inception Block", "comments": "5 pages, accepted by BCI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain-machine interface (BMI) based on electroencephalography (EEG) can\novercome the movement deficits for patients and real-world applications for\nhealthy people. Ideally, the BMI system detects user movement intentions\ntransforms them into a control signal for a robotic arm movement. In this\nstudy, we made progress toward user intention decoding and successfully\nclassified six different reaching movements of the right arm in the movement\nexecution (ME). Notably, we designed an experimental environment using robotic\narm movement and proposed a convolutional neural network architecture (CNN)\nwith inception block for robust classify executed movements of the same limb.\nAs a result, we confirmed the classification accuracies of six different\ndirections show 0.45 for the executed session. The results proved that the\nproposed architecture has approximately 6~13% performance increase compared to\nits conventional classification models. Hence, we demonstrate the 3D inception\nCNN architecture to contribute to the continuous decoding of ME.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 04:33:49 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Lee", "D. -Y.", ""], ["Jeong", "J. -H.", ""], ["Shim", "K. -H.", ""], ["Kim", "D. -J.", ""]]}, {"id": "2002.01122", "submitter": "Byeong-Hoo Lee", "authors": "Byeong-Hoo Lee, Ji-Hoon Jeong, Kyung-Hwan Shim, Dong-Joo Kim", "title": "Motor Imagery Classification of Single-Arm Tasks Using Convolutional\n  Neural Network based on Feature Refining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interface (BCI) decodes brain signals to understand user\nintention and status. Because of its simple and safe data acquisition process,\nelectroencephalogram (EEG) is commonly used in non-invasive BCI. One of EEG\nparadigms, motor imagery (MI) is commonly used for recovery or rehabilitation\nof motor functions due to its signal origin. However, the EEG signals are an\noscillatory and non-stationary signal that makes it difficult to collect and\nclassify MI accurately. In this study, we proposed a band-power feature\nrefining convolutional neural network (BFR-CNN) which is composed of two\nconvolution blocks to achieve high classification accuracy. We collected EEG\nsignals to create MI dataset contained the movement imagination of a\nsingle-arm. The proposed model outperforms conventional approaches in 4-class\nMI tasks classification. Hence, we demonstrate that the decoding of user\nintention is possible by using only EEG signals with robust performance using\nBFR-CNN.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 04:36:09 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Lee", "Byeong-Hoo", ""], ["Jeong", "Ji-Hoon", ""], ["Shim", "Kyung-Hwan", ""], ["Kim", "Dong-Joo", ""]]}, {"id": "2002.01171", "submitter": "Aung Aung Phyo Wai", "authors": "Aung Aung Phyo Wai, Yangsong Zhang, Heng Guo, Ying Chi, Lei Zhang,\n  Xian-Sheng Hua, Seong Whan Lee and Cuntai Guan", "title": "Towards a Fast Steady-State Visual Evoked Potentials (SSVEP)\n  Brain-Computer Interface (BCI)", "comments": "Further improvements or modifications required to algorithm design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steady-state visual evoked potentials (SSVEP) brain-computer interface (BCI)\nprovides reliable responses leading to high accuracy and information\nthroughput. But achieving high accuracy typically requires a relatively long\ntime window of one second or more. Various methods were proposed to improve\nsub-second response accuracy through subject-specific training and calibration.\nSubstantial performance improvements were achieved with tedious calibration and\nsubject-specific training; resulting in the user's discomfort. So, we propose a\ntraining-free method by combining spatial-filtering and temporal alignment\n(CSTA) to recognize SSVEP responses in sub-second response time. CSTA exploits\nlinear correlation and non-linear similarity between steady-state responses and\nstimulus templates with complementary fusion to achieve desirable performance\nimprovements. We evaluated the performance of CSTA in terms of accuracy and\nInformation Transfer Rate (ITR) in comparison with both training-based and\ntraining-free methods using two SSVEP data-sets. We observed that CSTA achieves\nthe maximum mean accuracy of 97.43$\\pm$2.26 % and 85.71$\\pm$13.41 % with\nfour-class and forty-class SSVEP data-sets respectively in sub-second response\ntime in offline analysis. CSTA yields significantly higher mean performance\n(p<0.001) than the training-free method on both data-sets. Compared with\ntraining-based methods, CSTA shows 29.33$\\pm$19.65 % higher mean accuracy with\nstatistically significant differences in time window less than 0.5 s. In longer\ntime windows, CSTA exhibits either better or comparable performance though not\nstatistically significantly better than training-based methods. We show that\nthe proposed method brings advantages of subject-independent SSVEP\nclassification without requiring training while enabling high target\nrecognition performance in sub-second response time.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 08:48:36 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 05:40:04 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Wai", "Aung Aung Phyo", ""], ["Zhang", "Yangsong", ""], ["Guo", "Heng", ""], ["Chi", "Ying", ""], ["Zhang", "Lei", ""], ["Hua", "Xian-Sheng", ""], ["Lee", "Seong Whan", ""], ["Guan", "Cuntai", ""]]}, {"id": "2002.01288", "submitter": "Malin Eiband", "authors": "Malin Eiband, Sarah Theres V\\\"olkel, Daniel Buschek, Sophia Cook,\n  Heinrich Hussmann", "title": "A Method and Analysis to Elicit User-reported Problems in Intelligent\n  Everyday Applications", "comments": "28 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complex nature of intelligent systems motivates work on supporting users\nduring interaction, for example through explanations. However, as of yet, there\nis little empirical evidence in regard to specific problems users face when\napplying such systems in everyday situations. This paper contributes a novel\nmethod and analysis to investigate such problems as reported by users: We\nanalysed 45,448 reviews of four apps on the Google Play Store (Facebook,\nNetflix, Google Maps and Google Assistant) with sentiment analysis and topic\nmodelling to reveal problems during interaction that can be attributed to the\napps' algorithmic decision-making. We enriched this data with users' coping and\nsupport strategies through a follow-up online survey (N=286). In particular, we\nfound problems and strategies related to content, algorithm, user choice, and\nfeedback. We discuss corresponding implications for designing user support,\nhighlighting the importance of user control and explanations of output, rather\nthan processes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 14:05:43 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Eiband", "Malin", ""], ["V\u00f6lkel", "Sarah Theres", ""], ["Buschek", "Daniel", ""], ["Cook", "Sophia", ""], ["Hussmann", "Heinrich", ""]]}, {"id": "2002.01537", "submitter": "Laura Sanely Gayt\\'an-Lugo", "authors": "Francisco J. Gutierrez, Yazmin Magallanes, Laura S. Gayt\\'an-Lugo,\n  Claudia L\\'opez, Cleidson R. B. de Souza", "title": "Academic viewpoints and concerns on CSCW education and training in Latin\n  America", "comments": "https://dl.acm.org/doi/abs/10.1145/3358961.3358971", "journal-ref": null, "doi": "10.1145/3358961.3358971", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-Supported Cooperative Work, or simply CSCW, is the research area\nthat studies the design and use of socio-technical technology for supporting\ngroup work. CSCW has a long tradition in interdisciplinary work exploring\ntechnical, social, and theoretical challenges for the design of technologies to\nsupport cooperative and collaborative work and life activities. However, most\nof the research tradition, methods, and theories in the field follow a strong\ntrend grounded in social and cultural aspects from North America and Western\nEurope. Therefore, it is inevitable that some of the underlying, and\nestablished, knowledge in the field will not be directly transferrable or\napplicable to other populations. This paper presents the results of an\ninterview study conducted with Latin American faculty on the feasability,\nviability, and prospect of a curriculum proposal for CSCW Education in Latin\nAmerica: To this end, we conducted nine interviews with faculty currently based\nin six countries of the region, aiming to understand how a CSCW course targeted\nto undergraduate and/or graduate students in Latin America might be deployed.\nOur findings suggest that there are specific traits that need to be addressed\nin such a course, such as: tailoring foundational CSCW concepts to the\ndiversity of local cultures, motivating the involvement of students by tackling\nrelevant problems to their local communities, and revitalizing CSCW research\nand practice in the continent.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 21:08:10 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Gutierrez", "Francisco J.", ""], ["Magallanes", "Yazmin", ""], ["Gayt\u00e1n-Lugo", "Laura S.", ""], ["L\u00f3pez", "Claudia", ""], ["de Souza", "Cleidson R. B.", ""]]}, {"id": "2002.01543", "submitter": "Christian Meske", "authors": "Christian Meske, Enrico Bunde", "title": "Transparency and Trust in Human-AI-Interaction: The Role of\n  Model-Agnostic Explanations in Computer Vision-Based Decision Support", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-50334-5_4", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer Vision, and hence Artificial Intelligence-based extraction of\ninformation from images, has increasingly received attention over the last\nyears, for instance in medical diagnostics. While the algorithms' complexity is\na reason for their increased performance, it also leads to the \"black box\"\nproblem, consequently decreasing trust towards AI. In this regard, \"Explainable\nArtificial Intelligence\" (XAI) allows to open that black box and to improve the\ndegree of AI transparency. In this paper, we first discuss the theoretical\nimpact of explainability on trust towards AI, followed by showcasing how the\nusage of XAI in a health-related setting can look like. More specifically, we\nshow how XAI can be applied to understand why Computer Vision, based on deep\nlearning, did or did not detect a disease (malaria) on image data (thin blood\nsmear slide images). Furthermore, we investigate, how XAI can be used to\ncompare the detection strategy of two different deep learning models often used\nfor Computer Vision: Convolutional Neural Network and Multi-Layer Perceptron.\nOur empirical results show that i) the AI sometimes used questionable or\nirrelevant data features of an image to detect malaria (even if correctly\npredicted), and ii) that there may be significant discrepancies in how\ndifferent deep learning models explain the same prediction. Our theoretical\ndiscussion highlights that XAI can support trust in Computer Vision systems,\nand AI systems in general, especially through an increased understandability\nand predictability.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 21:19:56 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 20:17:07 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Meske", "Christian", ""], ["Bunde", "Enrico", ""]]}, {"id": "2002.01618", "submitter": "Jonggi Hong", "authors": "Jonggi Hong, Kyungjun Lee, June Xu, Hernisa Kacorri", "title": "Crowdsourcing the Perception of Machine Teaching", "comments": "10 pages, 8 figures, 5 tables, CHI2020 conference", "journal-ref": "Proceedings of the 2020 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/3313831.3376428", "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teachable interfaces can empower end-users to attune machine learning systems\nto their idiosyncratic characteristics and environment by explicitly providing\npertinent training examples. While facilitating control, their effectiveness\ncan be hindered by the lack of expertise or misconceptions. We investigate how\nusers may conceptualize, experience, and reflect on their engagement in machine\nteaching by deploying a mobile teachable testbed in Amazon Mechanical Turk.\nUsing a performance-based payment scheme, Mechanical Turkers (N = 100) are\ncalled to train, test, and re-train a robust recognition model in real-time\nwith a few snapshots taken in their environment. We find that participants\nincorporate diversity in their examples drawing from parallels to how humans\nrecognize objects independent of size, viewpoint, location, and illumination.\nMany of their misconceptions relate to consistency and model capabilities for\nreasoning. With limited variation and edge cases in testing, the majority of\nthem do not change strategies on a second training attempt.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 03:20:25 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Hong", "Jonggi", ""], ["Lee", "Kyungjun", ""], ["Xu", "June", ""], ["Kacorri", "Hernisa", ""]]}, {"id": "2002.01621", "submitter": "Yunfeng Zhang", "authors": "Yunfeng Zhang, Rachel K. E. Bellamy, Kush R. Varshney", "title": "Joint Optimization of AI Fairness and Utility: A Human-Centered Approach", "comments": "To appear in AIES 2020 proceedings", "journal-ref": null, "doi": "10.1145/3375627.3375862", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, AI is increasingly being used in many high-stakes decision-making\napplications in which fairness is an important concern. Already, there are many\nexamples of AI being biased and making questionable and unfair decisions. The\nAI research community has proposed many methods to measure and mitigate\nunwanted biases, but few of them involve inputs from human policy makers. We\nargue that because different fairness criteria sometimes cannot be\nsimultaneously satisfied, and because achieving fairness often requires\nsacrificing other objectives such as model accuracy, it is key to acquire and\nadhere to human policy makers' preferences on how to make the tradeoff among\nthese objectives. In this paper, we propose a framework and some exemplar\nmethods for eliciting such preferences and for optimizing an AI model according\nto these preferences.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 03:31:48 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Zhang", "Yunfeng", ""], ["Bellamy", "Rachel K. E.", ""], ["Varshney", "Kush R.", ""]]}, {"id": "2002.01862", "submitter": "Ziang Xiao", "authors": "Ziang Xiao, Michelle X. Zhou, Wenxi Chen, Huahai Yang, Changyan Chi", "title": "If I Hear You Correctly: Building and Evaluating Interview Chatbots with\n  Active Listening Skills", "comments": "Working draft. To appear in the ACM CHI Conference on Human Factors\n  in Computing Systems (CHI 2020)", "journal-ref": null, "doi": "10.1145/3313831.3376131", "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interview chatbots engage users in a text-based conversation to draw out\ntheir views and opinions. It is, however, challenging to build effective\ninterview chatbots that can handle user free-text responses to open-ended\nquestions and deliver engaging user experience. As the first step, we are\ninvestigating the feasibility and effectiveness of using publicly available,\npractical AI technologies to build effective interview chatbots. To demonstrate\nfeasibility, we built a prototype scoped to enable interview chatbots with a\nsubset of active listening skills - the abilities to comprehend a user's input\nand respond properly. To evaluate the effectiveness of our prototype, we\ncompared the performance of interview chatbots with or without active listening\nskills on four common interview topics in a live evaluation with 206 users. Our\nwork presents practical design implications for building effective interview\nchatbots, hybrid chatbot platforms, and empathetic chatbots beyond interview\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 16:52:52 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Xiao", "Ziang", ""], ["Zhou", "Michelle X.", ""], ["Chen", "Wenxi", ""], ["Yang", "Huahai", ""], ["Chi", "Changyan", ""]]}, {"id": "2002.02086", "submitter": "Siping Liu", "authors": "Di Wu and Huayan Wan and Siping Liu and Weiren Yu and Zhanpeng Jin and\n  Dakuo Wang", "title": "DeepBrain: Towards Personalized EEG Interaction through Attentional and\n  Embedded LSTM Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"mind-controlling\" capability has always been in mankind's fantasy. With\nthe recent advancements of electroencephalograph (EEG) techniques,\nbrain-computer interface (BCI) researchers have explored various solutions to\nallow individuals to perform various tasks using their minds. However, the\ncommercial off-the-shelf devices to run accurate EGG signal collection are\nusually expensive and the comparably cheaper devices can only present coarse\nresults, which prevents the practical application of these devices in domestic\nservices. To tackle this challenge, we propose and develop an end-to-end\nsolution that enables fine brain-robot interaction (BRI) through embedded\nlearning of coarse EEG signals from the low-cost devices, namely DeepBrain, so\nthat people having difficulty to move, such as the elderly, can mildly command\nand control a robot to perform some basic household tasks. Our contributions\nare two folds: 1) We present a stacked long short term memory (Stacked LSTM)\nstructure with specific pre-processing techniques to handle the time-dependency\nof EEG signals and their classification. 2) We propose personalized design to\ncapture multiple features and achieve accurate recognition of individual EEG\nsignals by enhancing the signal interpretation of Stacked LSTM with attention\nmechanism. Our real-world experiments demonstrate that the proposed end-to-end\nsolution with low cost can achieve satisfactory run-time speed, accuracy and\nenergy-efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 03:34:08 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 04:37:12 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Wu", "Di", ""], ["Wan", "Huayan", ""], ["Liu", "Siping", ""], ["Yu", "Weiren", ""], ["Jin", "Zhanpeng", ""], ["Wang", "Dakuo", ""]]}, {"id": "2002.02159", "submitter": "Daisuke Iwai", "authors": "Daiki Tone, Daisuke Iwai, Shinsaku Hiura, Kosuke Sato", "title": "FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers\n  in Dynamic Projection Mapping", "comments": "11 pages, 14 figures", "journal-ref": null, "doi": "10.1109/TVCG.2020.2973444", "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel active marker for dynamic projection mapping (PM)\nthat emits a temporal blinking pattern of infrared (IR) light representing its\nID. We used a multi-material three dimensional (3D) printer to fabricate a\nprojection object with optical fibers that can guide IR light from LEDs\nattached on the bottom of the object. The aperture of an optical fiber is\ntypically very small; thus, it is unnoticeable to human observers under\nprojection and can be placed on a strongly curved part of a projection surface.\nIn addition, the working range of our system can be larger than previous\nmarker-based methods as the blinking patterns can theoretically be recognized\nby a camera placed at a wide range of distances from markers. We propose an\nautomatic marker placement algorithm to spread multiple active markers over the\nsurface of a projection object such that its pose can be robustly estimated\nusing captured images from arbitrary directions. We also propose an\noptimization framework for determining the routes of the optical fibers in such\na way that collisions of the fibers can be avoided while minimizing the loss of\nlight intensity in the fibers. Through experiments conducted using three\nfabricated objects containing strongly curved surfaces, we confirmed that the\nproposed method can achieve accurate dynamic PMs in a significantly wide\nworking range.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 08:56:46 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Tone", "Daiki", ""], ["Iwai", "Daisuke", ""], ["Hiura", "Shinsaku", ""], ["Sato", "Kosuke", ""]]}, {"id": "2002.02167", "submitter": "Daisuke Iwai", "authors": "Tatsuyuki Ueda, Daisuke Iwai, Takefumi Hiraki, Kosuke Sato", "title": "IlluminatedFocus: Vision Augmentation using Spatial Defocusing via Focal\n  Sweep Eyeglasses and High-Speed Projector", "comments": "11 pages, 21 figures", "journal-ref": null, "doi": "10.1109/TVCG.2020.2973496", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at realizing novel vision augmentation experiences, this paper\nproposes the IlluminatedFocus technique, which spatially defocuses real-world\nappearances regardless of the distance from the user's eyes to observed real\nobjects. With the proposed technique, a part of a real object in an image\nappears blurred, while the fine details of the other part at the same distance\nremain visible. We apply Electrically Focus-Tunable Lenses (ETL) as eyeglasses\nand a synchronized high-speed projector as illumination for a real scene. We\nperiodically modulate the focal lengths of the glasses (focal sweep) at more\nthan 60 Hz so that a wearer cannot perceive the modulation. A part of the scene\nto appear focused is illuminated by the projector when it is in focus of the\nuser's eyes, while another part to appear blurred is illuminated when it is out\nof the focus. As the basis of our spatial focus control, we build mathematical\nmodels to predict the range of distance from the ETL within which real objects\nbecome blurred on the retina of a user. Based on the blur range, we discuss a\ndesign guideline for effective illumination timing and focal sweep range. We\nalso model the apparent size of a real scene altered by the focal length\nmodulation. This leads to an undesirable visible seam between focused and\nblurred areas. We solve this unique problem by gradually blending the two\nareas. Finally, we demonstrate the feasibility of our proposal by implementing\nvarious vision augmentation applications.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 09:16:11 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Ueda", "Tatsuyuki", ""], ["Iwai", "Daisuke", ""], ["Hiraki", "Takefumi", ""], ["Sato", "Kosuke", ""]]}, {"id": "2002.02358", "submitter": "Gregoire Cattan", "authors": "Gr\\'egoire Cattan (GIPSA-VIBS, IHMTEK), Anton Andreev\n  (GIPSA-Services), Cesar Mendoza (IHMTEK), Marco Congedo (GIPSA-VIBS)", "title": "A comparison of mobile VR display running on an ordinary smartphone with\n  standard PC display for P300-BCI stimulus presentation", "comments": "IEEE Transactions on Games, Institute of Electrical and Electronics\n  Engineers, In press", "journal-ref": null, "doi": "10.1109/TG.2019.2957963", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain-computer interface (BCI) based on electroencephalography (EEG) is a\npromising technology for enhancing virtual reality (VR) applications-in\nparticular, for gaming. We focus on the so-called P300-BCI, a stable and\naccurate BCI paradigm relying on the recognition of a positive event-related\npotential (ERP) occurring in the EEG about 300 ms post-stimulation. We\nimplemented a basic version of such a BCI displayed on an ordinary and\naffordable smartphone-based head-mounted VR device: that is, a mobile and\npassive VR system (with no electronic components beyond the smartphone). The\nmobile phone performed the stimuli presentation, EEG synchronization (tagging)\nand feedback display. We compared the ERPs and the accuracy of the BCI on the\nVR device with a traditional BCI running on a personal computer (PC). We also\nevaluated the impact of subjective factors on the accuracy. The study was\nwithin-subjects, with 21 participants and one session in each modality. No\nsignificant difference in BCI accuracy was found between the PC and VR systems,\nalthough the P200 ERP was significantly wider and larger in the VR system as\ncompared to the PC system.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 17:04:17 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Cattan", "Gr\u00e9goire", "", "GIPSA-VIBS, IHMTEK"], ["Andreev", "Anton", "", "GIPSA-Services"], ["Mendoza", "Cesar", "", "IHMTEK"], ["Congedo", "Marco", "", "GIPSA-VIBS"]]}, {"id": "2002.02453", "submitter": "Shomik Jain", "authors": "Shomik Jain, Balasubramanian Thiagarajan, Zhonghao Shi, Caitlyn\n  Clabaugh, Maja J. Matari\\'c", "title": "Modeling Engagement in Long-Term, In-Home Socially Assistive Robot\n  Interventions for Children with Autism Spectrum Disorders", "comments": "This manuscript was published in Science Robotics on February 26,\n  2020", "journal-ref": "Sci. Robot. 5, eaaz3791 (2020)", "doi": "10.1126/scirobotics.aaz3791", "report-no": null, "categories": "cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Socially assistive robotics (SAR) has great potential to provide accessible,\naffordable, and personalized therapeutic interventions for children with autism\nspectrum disorders (ASD). However, human-robot interaction (HRI) methods are\nstill limited in their ability to autonomously recognize and respond to\nbehavioral cues, especially in atypical users and everyday settings. This work\napplies supervised machine learning algorithms to model user engagement in the\ncontext of long-term, in-home SAR interventions for children with ASD.\nSpecifically, we present two types of engagement models for each user: (i)\ngeneralized models trained on data from different users; and (ii)\nindividualized models trained on an early subset of the user's data. The models\nachieved approximately 90% accuracy (AUROC) for post hoc binary classification\nof engagement, despite the high variance in data observed across users,\nsessions, and engagement states. Moreover, temporal patterns in model\npredictions could be used to reliably initiate re-engagement actions at\nappropriate times. These results validate the feasibility and challenges of\nrecognition and response to user disengagement in long-term, real-world HRI\nsettings. The contributions of this work also inform the design of engaging and\npersonalized HRI, especially for the ASD community.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 18:26:11 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 03:26:36 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Jain", "Shomik", ""], ["Thiagarajan", "Balasubramanian", ""], ["Shi", "Zhonghao", ""], ["Clabaugh", "Caitlyn", ""], ["Matari\u0107", "Maja J.", ""]]}, {"id": "2002.02526", "submitter": "Tim Schrills", "authors": "Tim Schrills, Thomas Franke", "title": "How to Answer Why -- Evaluating the Explanations of AI Through Mental\n  Model Analysis", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve optimal human-system integration in the context of user-AI\ninteraction it is important that users develop a valid representation of how AI\nworks. In most of the everyday interaction with technical systems users\nconstruct mental models (i.e., an abstraction of the anticipated mechanisms a\nsystem uses to perform a given task). If no explicit explanations are provided\nby a system (e.g. by a self-explaining AI) or other sources (e.g. an\ninstructor), the mental model is typically formed based on experiences, i.e.\nthe observations of the user during the interaction. The congruence of this\nmental model and the actual systems functioning is vital, as it is used for\nassumptions, predictions and consequently for decisions regarding system use. A\nkey question for human-centered AI research is therefore how to validly survey\nusers' mental models. The objective of the present research is to identify\nsuitable elicitation methods for mental model analysis. We evaluated whether\nmental models are suitable as an empirical research method. Additionally,\nmethods of cognitive tutoring are integrated. We propose an exemplary method to\nevaluate explainable AI approaches in a human-centered way.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 17:15:58 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Schrills", "Tim", ""], ["Franke", "Thomas", ""]]}, {"id": "2002.02591", "submitter": "Yu Liu", "authors": "Yu Liu, Yuheng Wang, Haipeng Liu, Anfu Zhou, Jianhua Liu, and Ning\n  Yang", "title": "Long-Range Gesture Recognition Using Millimeter Wave Radar", "comments": "15pages,16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millimeter wave (mmWave) based gesture recognition technology provides a good\nhuman computer interaction (HCI) experience. Prior works focus on the\nclose-range gesture recognition, but fall short in range extension, i.e., they\nare unable to recognize gestures more than one meter away from considerable\nnoise motions. In this paper, we design a long-range gesture recognition model\nwhich utilizes a novel data processing method and a customized artificial\nConvolutional Neural Network (CNN). Firstly, we break down gestures into\nmultiple reflection points and extract their spatial-temporal features which\ndepict gesture details. Secondly, we design a CNN to learn changing patterns of\nextracted features respectively and output the recognition result. We\nthoroughly evaluate our proposed system by implementing on a commodity mmWave\nradar. Besides, we also provide more extensive assessments to demonstrate that\nthe proposed system is practical in several real-world scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 02:29:38 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Liu", "Yu", ""], ["Wang", "Yuheng", ""], ["Liu", "Haipeng", ""], ["Zhou", "Anfu", ""], ["Liu", "Jianhua", ""], ["Yang", "Ning", ""]]}, {"id": "2002.02635", "submitter": "Takaaki Kamigaki Dr", "authors": "Takaaki Kamigaki, Shun Suzuki, and Hiroyuki Shinoda", "title": "Noncontact Thermal and Vibrotactile Display Using Focused Airborne\n  Ultrasound", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a typical mid-air haptics system, focused airborne ultrasound provides\nvibrotactile sensations to localized areas on a bare skin. Herein, a method for\ndisplaying thermal sensations to hands where mesh fabric gloves are worn is\nproposed. The gloves employed in this study are commercially available mesh\nfabric gloves with sound absorption characteristics, such as cotton work gloves\nwithout any additional devices such as Peltier elements. The method proposed in\nthis study can also provide vibrotactile sensations by changing the ultrasonic\nirradiation pattern. In this paper, we report basic experimental investigations\non the proposed method. By performing thermal measurements, we evaluate the\nlocal heat generation on the surfaces of both the glove and the skin by focused\nairborne ultrasound irradiation. In addition, we performed perceptual\nexperiments, thereby confirming that the proposed method produced both thermal\nand vibrotactile sensations. Furthermore, these sensations were selectively\nprovided to a certain extent by changing the ultrasonic irradiation pattern.\nThese results validate the effectiveness of our method and its feasibility in\nmid-air haptics applications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 06:32:05 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Kamigaki", "Takaaki", ""], ["Suzuki", "Shun", ""], ["Shinoda", "Hiroyuki", ""]]}, {"id": "2002.02671", "submitter": "Ali Asadipour", "authors": "Efstratios Doukakis, Kurt Debattista, Thomas Bashford-Rogers, Amar\n  Dhokia, Ali Asadipour, Alan Chalmers and Carlo Harvey", "title": "Audio-Visual-Olfactory Resource Allocation for Tri-modal Virtual\n  Environments", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2898823", "report-no": null, "categories": "cs.GR cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual Environments (VEs) provide the opportunity to simulate a wide range\nof applications, from training to entertainment, in a safe and controlled\nmanner. For applications which require realistic representations of real world\nenvironments, the VEs need to provide multiple, physically accurate sensory\nstimuli. However, simulating all the senses that comprise the human sensory\nsystem (HSS) is a task that requires significant computational resources. Since\nit is intractable to deliver all senses at the highest quality, we propose a\nresource distribution scheme in order to achieve an optimal perceptual\nexperience within the given computational budgets. This paper investigates\nresource balancing for multi-modal scenarios composed of aural, visual and\nolfactory stimuli. Three experimental studies were conducted. The first\nexperiment identified perceptual boundaries for olfactory computation. In the\nsecond experiment, participants (N=25) were asked, across a fixed number of\nbudgets (M=5), to identify what they perceived to be the best visual, acoustic\nand olfactory stimulus quality for a given computational budget. Results\ndemonstrate that participants tend to prioritise visual quality compared to\nother sensory stimuli. However, as the budget size is increased, users prefer a\nbalanced distribution of resources with an increased preference for having\nsmell impulses in the VE. Based on the collected data, a quality prediction\nmodel is proposed and its accuracy is validated against previously unused\nbudgets and an untested scenario in a third and final experiment.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 08:59:41 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Doukakis", "Efstratios", ""], ["Debattista", "Kurt", ""], ["Bashford-Rogers", "Thomas", ""], ["Dhokia", "Amar", ""], ["Asadipour", "Ali", ""], ["Chalmers", "Alan", ""], ["Harvey", "Carlo", ""]]}, {"id": "2002.03037", "submitter": "Jens Grubert", "authors": "Tim Menzner and Travis Gesslein and Alexander Otte and Jens Grubert", "title": "Above Surface Interaction for Multiscale Navigation in Mobile Virtual\n  Reality", "comments": "Accepted for IEEE VR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality enables the exploration of large information spaces. In\nphysically constrained spaces such as airplanes or buses, controller-based or\nmid-air interaction in mobile Virtual Reality can be challenging. Instead, the\ninput space on and above touch-screen enabled devices such as smartphones or\ntablets could be employed for Virtual Reality interaction in those spaces.\n  In this context, we compared an above surface interaction technique with\ntraditional 2D on-surface input for navigating large planar information spaces\nsuch as maps in a controlled user study (n = 20). We find that our proposed\nabove surface interaction technique results in significantly better performance\nand user preference compared to pinch-to-zoom and drag-to-pan when navigating\nplanar information spaces.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 22:38:47 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Menzner", "Tim", ""], ["Gesslein", "Travis", ""], ["Otte", "Alexander", ""], ["Grubert", "Jens", ""]]}, {"id": "2002.03058", "submitter": "Ronak Tanna", "authors": "Ronak Tanna, Shivam Dhar, Ashwin Sudhir, Shreyash Devan, Shubham Verma", "title": "Lessons Learned Developing and Extending a Visual Analytics Solution for\n  Investigative Analysis of Scamming Activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybersecurity analysts work on large communication data sets to perform\ninvestigative analysis by painstakingly going over thousands of email\nconversations to find potential scamming activities and the network of cyber\nscammers. Traditionally,experts used email clients, database systems and text\neditors to perform this investigation. With the advent of technology,elaborate\ntools that summarize data more efficiently by using cutting edge data\nvisualization techniques have come out. Beagle[1] is one such tool which\nvisualizes the large communication data using different panels such that the\ninspector has better chances of finding the scam network. This paper is a\nreport on our work to implement and improve the work done by Jay Koven et al.\n[1]. We have proposed and demonstrated via implementation, a few more\nvisualizations that we feel would help in grouping and analyzing the e-mail\ndata more efficiently. Lastly, we have also presented a case study that shows\nthe potential use of our tool in a real-world scenario.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 00:59:41 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Tanna", "Ronak", ""], ["Dhar", "Shivam", ""], ["Sudhir", "Ashwin", ""], ["Devan", "Shreyash", ""], ["Verma", "Shubham", ""]]}, {"id": "2002.03062", "submitter": "Meia Chita-Tegmark", "authors": "Meia Chita-Tegmark and Matthias Scheutz (Tufts)", "title": "Assistive robots for the social management of health: a framework for\n  robot design and human-robot interaction research", "comments": "21 pages, 2 figs", "journal-ref": "International Journal of Social Robotics, 1-21 (March 2020)", "doi": "10.1007/s12369-020-00634-z", "report-no": null, "categories": "cs.HC cs.CY cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a close connection between health and the quality of one's social\nlife. Strong social bonds are essential for health and wellbeing, but often\nhealth conditions can detrimentally affect a person's ability to interact with\nothers. This can become a vicious cycle resulting in further decline in health.\nFor this reason, the social management of health is an important aspect of\nhealthcare. We propose that socially assistive robots (SARs) could help people\nwith health conditions maintain positive social lives by supporting them in\nsocial interactions. This paper makes three contributions, as detailed below.\nWe develop a framework of social mediation functions that robots could perform,\nmotivated by the special social needs that people with health conditions have.\nIn this framework we identify five types of functions that SARs could perform:\na) changing how the person is perceived, b) enhancing the social behavior of\nthe person, c) modifying the social behavior of others, d) providing structure\nfor interactions, and e) changing how the person feels. We thematically\norganize and review the existing literature on robots supporting human-human\ninteractions, in both clinical and non-clinical settings, and explain how the\nfindings and design ideas from these studies can be applied to the functions\nidentified in the framework. Finally, we point out and discuss challenges in\ndesigning SARs for supporting social interactions, and highlight opportunities\nfor future robot design and HRI research on the mediator role of robots.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 01:32:21 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 03:06:00 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Chita-Tegmark", "Meia", "", "Tufts"], ["Scheutz", "Matthias", "", "Tufts"]]}, {"id": "2002.03082", "submitter": "Nan Jiang", "authors": "Nan Jiang, Sheng Jin, Zhiyao Duan, Changshui Zhang", "title": "RL-Duet: Online Music Accompaniment Generation Using Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep reinforcement learning algorithm for online\naccompaniment generation, with potential for real-time interactive\nhuman-machine duet improvisation. Different from offline music generation and\nharmonization, online music accompaniment requires the algorithm to respond to\nhuman input and generate the machine counterpart in a sequential order. We cast\nthis as a reinforcement learning problem, where the generation agent learns a\npolicy to generate a musical note (action) based on previously generated\ncontext (state). The key of this algorithm is the well-functioning reward\nmodel. Instead of defining it using music composition rules, we learn this\nmodel from monophonic and polyphonic training data. This model considers the\ncompatibility of the machine-generated note with both the machine-generated\ncontext and the human-generated context. Experiments show that this algorithm\nis able to respond to the human part and generate a melodic, harmonic and\ndiverse machine part. Subjective evaluations on preferences show that the\nproposed algorithm generates music pieces of higher quality than the baseline\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 03:53:52 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Jiang", "Nan", ""], ["Jin", "Sheng", ""], ["Duan", "Zhiyao", ""], ["Zhang", "Changshui", ""]]}, {"id": "2002.03103", "submitter": "Changjian Chen", "authors": "Changjian Chen, Jun Yuan, Yafeng Lu, Yang Liu, Hang Su, Songtao Yuan,\n  Shixia Liu", "title": "OoDAnalyzer: Interactive Analysis of Out-of-Distribution Samples", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major cause of performance degradation in predictive models is that the\ntest samples are not well covered by the training data. Such not\nwell-represented samples are called OoD samples. In this paper, we propose\nOoDAnalyzer, a visual analysis approach for interactively identifying OoD\nsamples and explaining them in context. Our approach integrates an ensemble OoD\ndetection method and a grid-based visualization. The detection method is\nimproved from deep ensembles by combining more features with algorithms in the\nsame family. To better analyze and understand the OoD samples in context, we\nhave developed a novel kNN-based grid layout algorithm motivated by Hall's\ntheorem. The algorithm approximates the optimal layout and has $O(kN^2)$ time\ncomplexity, faster than the grid layout algorithm with overall best performance\nbut $O(N^3)$ time complexity. Quantitative evaluation and case studies were\nperformed on several datasets to demonstrate the effectiveness and usefulness\nof OoDAnalyzer.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 06:58:33 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Chen", "Changjian", ""], ["Yuan", "Jun", ""], ["Lu", "Yafeng", ""], ["Liu", "Yang", ""], ["Su", "Hang", ""], ["Yuan", "Songtao", ""], ["Liu", "Shixia", ""]]}, {"id": "2002.03144", "submitter": "Andrei Bytes", "authors": "Andrei Bytes and Jay Prakash and Jianying Zhou and Tony Q.S. Quek", "title": "Why is My Secret Leaked? Discovering Vulnerabilities in Device-to-Device\n  File Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of active users of Wi-Fi Direct Device-to-Device file sharing\napplications on Android has exceeded 1.8 billion. Wi-Fi Direct, also known as\nWi-Fi P2P, is commonly used for peer-to-peer, high-speed file transfer between\nmobile devices, as well as a close proximity connection mode for wireless\ncameras, network printers, TVs and other IoT and mobile devices. For its end\nusers, such type of direct file transfer does not incur cellular data charges.\nHowever, despite the popularity of such applications, we observe that the\nsoftware vendors tend to prioritize the ease of user flow over the security in\ntheir implementations, which leads to serious security flaws. We perform a\ncomprehensive security analysis in the context of security and usability and\nreport our findings in the form of 17 Common Vulnerabilities and Exposures\n(CVE) which have been disclosed to the corresponding vendors. To address the\nsimilar flaws at the early stage of the application design, we propose a joint\nconsideration of security and usability for such applications and their\nprotocols that can be visualized in form of a customised User Journey Map\n(UJM).\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 11:21:21 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 15:23:23 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bytes", "Andrei", ""], ["Prakash", "Jay", ""], ["Zhou", "Jianying", ""], ["Quek", "Tony Q. S.", ""]]}, {"id": "2002.03387", "submitter": "Samir Passi", "authors": "Samir Passi, Steven J. Jackson", "title": "Data Vision: Learning to See Through Algorithmic Abstraction", "comments": null, "journal-ref": "In Proceedings of the 2017 ACM Conference on Computer Supported\n  Cooperative Work and Social Computing. ACM, New York, NY, USA, 2436-2447", "doi": "10.1145/2998181.2998331", "report-no": null, "categories": "cs.HC cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to see through data is central to contemporary forms of algorithmic\nknowledge production. While often represented as a mechanical application of\nrules, making algorithms work with data requires a great deal of situated work.\nThis paper examines how the often-divergent demands of mechanization and\ndiscretion manifest in data analytic learning environments. Drawing on research\nin CSCW and the social sciences, and ethnographic fieldwork in two data\nlearning environments, we show how an algorithm's application is seen sometimes\nas a mechanical sequence of rules and at other times as an array of situated\ndecisions. Casting data analytics as a rule-based (rather than rule-bound)\npractice, we show that effective data vision requires would-be analysts to\nstraddle the competing demands of formal abstraction and empirical contingency.\nWe conclude by discussing how the notion of data vision can help better\nleverage the role of human work in data analytic learning, research, and\npractice.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 15:46:18 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Passi", "Samir", ""], ["Jackson", "Steven J.", ""]]}, {"id": "2002.03466", "submitter": "Filipo Sharevski", "authors": "Filipo Sharevski, Paige Treebridge, Peter Jachim, Audrey Li, Adam\n  Babin, Jessica Westbrook", "title": "Meet Malexa, Alexa's Malicious Twin: Malware-Induced Misperception\n  Through Intelligent Voice Assistants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports the findings of a study where users (N=220) interacted\nwith Malexa, Alexa's malicious twin. Malexa is an intelligent voice assistant\nwith a simple and seemingly harmless third-party skill that delivers news\nbriefings to users. The twist, however, is that Malexa covertly rewords these\nbriefings to intentionally introduce misperception about the reported events.\nThis covert rewording is referred to as a Malware-Induced Misperception (MIM)\nattack. It differs from squatting or invocation hijacking attacks in that it is\nfocused on manipulating the \"content\" delivered through a third-party skill\ninstead of the skill's \"invocation logic.\" Malexa, in the study, reworded\nregulatory briefings to make a government response sound more accidental or\nlenient than the original news delivered by Alexa. The results show that users\nwho interacted with Malexa perceived that the government was less friendly to\nworking people and more in favor of big businesses. The results also show that\nMalexa is capable of inducing misperceptions regardless of the user's gender,\npolitical ideology or frequency of interaction with intelligent voice\nassistants. We discuss the implications in the context of using Malexa as a\ncovert \"influencer\" in people's living or working environments.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 22:44:06 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Sharevski", "Filipo", ""], ["Treebridge", "Paige", ""], ["Jachim", "Peter", ""], ["Li", "Audrey", ""], ["Babin", "Adam", ""], ["Westbrook", "Jessica", ""]]}, {"id": "2002.03582", "submitter": "Shiyoh Goetsu", "authors": "Shiyoh Goetsu and Tetsuya Sakai", "title": "Different Types of Voice User Interface Failures May Cause Different\n  Degrees of Frustration", "comments": "5 pages;1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on an investigation into how different types of failures in a voice\nuser interface (VUI) affects user frustration. To this end, we conducted a\npilot user study ($n=10$) and a main user study ($n=30$), both with a simple\nvoice-operated calendar application that we built using the Alexa Skills Kit.\nIn our pilot study, we identified three major failure types as perceived by the\nusers, namely, Reason Unknown, Speech Misrecognition, and Utterance Pattern\nMatch Failure, along with more fine-grained failure types from the developer's\nviewpoint such as Intent Pattern Match Failure and Intent Misclassification.\nThen, in our main study, we set up three user tasks that were designed to each\ninduce a specific failure type, and collected user frustration ratings for each\ntask. Our main findings are:\n  (a)Users may be relatively tolerant to user-perceived Speech Misrecognition,\nand not so to user-perceived Reason Unknown and Utterance Mattern Match\nFailures;\n  (b)Regarding the relationship between developer-perceived and user-perceived\nfailure types, 68.8\\% of developer-perceived Intent Misclassification instances\ncaused user-perceived Reason Unkown failures.\n  From (a) and (b), a practical design implication would be to try to prevent\nIntent Misclassification from happening by carefully crafting the utterance\npatterns for each intent.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 07:38:28 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Goetsu", "Shiyoh", ""], ["Sakai", "Tetsuya", ""]]}, {"id": "2002.03681", "submitter": "Athanasios Mazarakis", "authors": "Athanasios Mazarakis and Paula Br\\\"auer", "title": "First Directions for Using Gamification to Motivate for Open Access", "comments": "10 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most scientists are aware that, in addition to the traditional and\nsubscription-based publication model, there is also the possibility of\npublishing their research in open access. Various surveys show that scientists\nare in favour of this new model. Nevertheless, the transition to open access\nhas been very slow so far. In order to accelerate this process, we are looking\nfor new opportunities to create incentives for researchers to deal with the\ntopic of open access. In a field study with 28 participants the effects of the\ngame design elements badge and progress bar on the motivation when working on\nan online quiz on the topic of open access are examined. In our study both game\ndesign elements provide a statistically significant increase in the number of\nquestions answered compared to a control group. This suggests that gamification\nis useful to motivate for open access.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 12:29:12 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Mazarakis", "Athanasios", ""], ["Br\u00e4uer", "Paula", ""]]}, {"id": "2002.03749", "submitter": "Hartmut Feld", "authors": "Hartmut Feld, Bruno Mirbach, Jigyasa Katrolia, Mohamed Selim, Oliver\n  Wasenm\\\"uller, Didier Stricker", "title": "DFKI Cabin Simulator: A Test Platform for Visual In-Cabin Monitoring\n  Functions", "comments": "corrected typos and bad reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a test platform for visual in-cabin scene analysis and occupant\nmonitoring functions. The test platform is based on a driving simulator\ndeveloped at the DFKI, consisting of a realistic in-cabin mock-up and a\nwide-angle projection system for a realistic driving experience. The platform\nhas been equipped with a wide-angle 2D/3D camera system monitoring the entire\ninterior of the vehicle mock-up of the simulator. It is also supplemented with\na ground truth reference sensor system that allows to track and record the\noccupant's body movements synchronously with the 2D and 3D video streams of the\ncamera. Thus, the resulting test platform will serve as a basis to validate\nnumerous in-cabin monitoring functions, which are important for the realization\nof novel human-vehicle interfaces, advanced driver assistant systems, and\nautomated driving. Among the considered functions are occupant presence\ndetection, size and 3D-pose estimation and driver intention recognition. In\naddition, our platform will be the basis for the creation of large-scale\nin-cabin benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 07:15:50 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 12:27:42 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Feld", "Hartmut", ""], ["Mirbach", "Bruno", ""], ["Katrolia", "Jigyasa", ""], ["Selim", "Mohamed", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "2002.03885", "submitter": "Filipo Sharevski", "authors": "Filipo Sharevski, Paige Treebridge, Peter Jachim, Audrey Li, Adam\n  Babin, Jessica Westbrook", "title": "Beyond Trolling: Malware-Induced Misperception Attacks on Polarized\n  Facebook Discourse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media trolling is a powerful tactic to manipulate public opinion on\nissues with a high moral component. Troll farms, as evidenced in the past,\ncreated fabricated content to provoke or silence people to share their opinion\non social media during the US presidential election in 2016. In this paper, we\nintroduce an alternate way of provoking or silencing social media discourse by\nmanipulating how users perceive authentic content. This manipulation is\nperformed by man-in-the-middle malware that covertly rearranges the linguistic\ncontent of an authentic social media post and comments. We call this attack\nMalware-Induced Misperception (MIM) because the goal is to socially engineer\nspiral-of-silence conditions on social media by inducing perception. We\nconducted experimental tests in controlled settings (N = 311) where a malware\ncovertly altered selected words in a Facebook post about the freedom of\npolitical expression on college campuses. The empirical results (1) confirm the\nprevious findings about the presence of the spiral-of-silence effect on social\nmedia; and (2) demonstrate that inducing misperception is an effective tactic\nto silence or provoke targeted users on Facebook to express their opinion on a\npolarizing political issue.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 15:55:23 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Sharevski", "Filipo", ""], ["Treebridge", "Paige", ""], ["Jachim", "Peter", ""], ["Li", "Audrey", ""], ["Babin", "Adam", ""], ["Westbrook", "Jessica", ""]]}, {"id": "2002.04087", "submitter": "Ben Shneiderman", "authors": "Ben Shneiderman", "title": "Human-Centered Artificial Intelligence: Reliable, Safe & Trustworthy", "comments": "15 pages, followed by 4 pages of references, 6 figures", "journal-ref": null, "doi": null, "report-no": "UMD HCIL-2020-01", "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well-designed technologies that offer high levels of human control and high\nlevels of computer automation can increase human performance, leading to wider\nadoption. The Human-Centered Artificial Intelligence (HCAI) framework clarifies\nhow to (1) design for high levels of human control and high levels of computer\nautomation so as to increase human performance, (2) understand the situations\nin which full human control or full computer control are necessary, and (3)\navoid the dangers of excessive human control or excessive computer control. The\nmethods of HCAI are more likely to produce designs that are Reliable, Safe &\nTrustworthy (RST). Achieving these goals will dramatically increase human\nperformance, while supporting human self-efficacy, mastery, creativity, and\nresponsibility.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 21:02:48 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 19:03:46 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Shneiderman", "Ben", ""]]}, {"id": "2002.04202", "submitter": "Devleena Das", "authors": "Devleena Das, Sonia Chernova", "title": "Leveraging Rationales to Improve Human Task Performance", "comments": "ACM IUI 2020", "journal-ref": null, "doi": "10.1145/3377325.3377512", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) systems across many application areas are increasingly\ndemonstrating performance that is beyond that of humans. In response to the\nproliferation of such models, the field of Explainable AI (XAI) has sought to\ndevelop techniques that enhance the transparency and interpretability of\nmachine learning methods. In this work, we consider a question not previously\nexplored within the XAI and ML communities: Given a computational system whose\nperformance exceeds that of its human user, can explainable AI capabilities be\nleveraged to improve the performance of the human? We study this question in\nthe context of the game of Chess, for which computational game engines that\nsurpass the performance of the average player are widely available. We\nintroduce the Rationale-Generating Algorithm, an automated technique for\ngenerating rationales for utility-based computational methods, which we\nevaluate with a multi-day user study against two baselines. The results show\nthat our approach produces rationales that lead to statistically significant\nimprovement in human task performance, demonstrating that rationales\nautomatically generated from an AI's internal task model can be used not only\nto explain what the system is doing, but also to instruct the user and\nultimately improve their task performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 04:51:35 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Das", "Devleena", ""], ["Chernova", "Sonia", ""]]}, {"id": "2002.04242", "submitter": "Rui Liu", "authors": "Boyi Song, Yuntao Peng, Ruijiao Luo, Rui Liu", "title": "An Attention Transfer Model for Human-Assisted Failure Avoidance in\n  Robot Manipulations", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to real-world dynamics and hardware uncertainty, robots inevitably fail\nin task executions, resulting in undesired or even dangerous executions. In\norder to avoid failures and improve robot performance, it is critical to\nidentify and correct abnormal robot executions at an early stage. However, due\nto limited reasoning capability and knowledge storage, it is challenging for\nrobots to self-diagnose and -correct their own abnormality in both planning and\nexecuting. To improve robot self diagnosis capability, in this research a novel\nhuman-to-robot attention transfer (\\textit{\\textbf{H2R-AT}}) method was\ndeveloped to identify robot manipulation errors by leveraging human\ninstructions. \\textit{\\textbf{H2R-AT}} was developed by fusing attention\nmapping mechanism into a novel stacked neural networks model, transferring\nhuman verbal attention into robot visual attention. With the attention\ntransfer, a robot understands \\textit{what} and \\textit{where} human concerns\nare to identify and correct abnormal manipulations. Two representative task\nscenarios: ``serve water for a human in a kitchen\" and ``pick up a defective\ngear in a factory\" were designed in a simulation framework CRAIhri with\nabnormal robot manipulations; and $252$ volunteers were recruited to provide\nabout 12000 verbal reminders to learn and test \\textit{\\textbf{H2R-AT}}. The\nmethod effectiveness was validated by the high accuracy of $73.68\\%$ in\ntransferring attention, and the high accuracy of $66.86\\%$ in avoiding grasping\nfailures.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 07:58:48 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 14:56:52 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 14:52:43 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Song", "Boyi", ""], ["Peng", "Yuntao", ""], ["Luo", "Ruijiao", ""], ["Liu", "Rui", ""]]}, {"id": "2002.04258", "submitter": "Manuel Gomez Rodriguez", "authors": "Vahid Balazadeh Meresht and Abir De and Adish Singla and Manuel\n  Gomez-Rodriguez", "title": "Learning to Switch Between Machines and Humans", "comments": "Added support for unknown transition probabilities and multiple teams", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.HC cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning agents have been mostly developed and evaluated under\nthe assumption that they will operate in a fully autonomous manner -- they will\ntake all actions. In this work, our goal is to develop algorithms that, by\nlearning to switch control between machine and human agents, allow existing\nreinforcement learning agents to operate under different automation levels. To\nthis end, we first formally define the problem of learning to switch control\namong agents in a team via a 2-layer Markov decision process. Then, we develop\nan online learning algorithm that uses upper confidence bounds on the agents'\npolicies and the environment's transition probabilities to find a sequence of\nswitching policies. We prove that the total regret of our algorithm with\nrespect to the optimal switching policy is sublinear in the number of learning\nsteps. Moreover, we also show that our algorithm can be used to find multiple\nsequences of switching policies across several independent teams of agents\noperating in similar environments, where it greatly benefits from maintaining\nshared confidence bounds for the environments' transition probabilities.\nSimulation experiments in obstacle avoidance in a semi-autonomous driving\nscenario illustrate our theoretical findings and demonstrate that, by\nexploiting the specific structure of the problem, our proposed algorithm is\nsuperior to problem-agnostic algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 08:50:52 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 08:43:23 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Meresht", "Vahid Balazadeh", ""], ["De", "Abir", ""], ["Singla", "Adish", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "2002.04263", "submitter": "Titas De", "authors": "Catalin Voss, Nick Haber, Peter Washington, Aaron Kline, Beth\n  McCarthy, Jena Daniels, Azar Fazel, Titas De, Carl Feinstein, Terry Winograd,\n  Dennis Wall", "title": "Designing a Holistic At-Home Learning Aid for Autism", "comments": "Conference Workshop", "journal-ref": "CHI 2016 - Autism Technology Workshop", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, much focus has been put on employing technology to make\nnovel behavioural aids for those with autism. Most of these are digital\nadaptations of tools used in standard behavioural therapy to enforce normative\nskills. These digital counterparts are often used outside of both the larger\ntherapeutic context and the real world, in which the learned skills might\napply. To address this, we are designing a system of automatic expression\nrecognition on wearable devices that integrates directly into the families\ndaily social interactions, to give children and their caregivers the tools and\ninformation they need to design their own holistic therapy. In order to develop\na tool that will be truly useful to families, we proactively include children\nwith autism and their families as co-designers in the development process. By\nproviding an app and interface with interchangeable social feedback options, we\naim to produce a framework for therapy that folds into their daily lives,\ntailored to their specific needs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 09:11:37 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Voss", "Catalin", ""], ["Haber", "Nick", ""], ["Washington", "Peter", ""], ["Kline", "Aaron", ""], ["McCarthy", "Beth", ""], ["Daniels", "Jena", ""], ["Fazel", "Azar", ""], ["De", "Titas", ""], ["Feinstein", "Carl", ""], ["Winograd", "Terry", ""], ["Wall", "Dennis", ""]]}, {"id": "2002.04317", "submitter": "Baptiste Caramiaux", "authors": "Baptiste Caramiaux, Jules Fran\\c{c}oise, Wanyu Liu, T\\'eo Sanchez and\n  Fr\\'ed\\'eric Bevilacqua", "title": "Machine Learning Approaches For Motor Learning: A Short Review", "comments": "Mini Review, Frontiers Comput. Sci. - Human-Media Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning approaches have seen considerable applications in human\nmovement modeling, but remain limited for motor learning. Motor learning\nrequires accounting for motor variability, and poses new challenges as the\nalgorithms need to be able to differentiate between new movements and variation\nof known ones. In this short review, we outline existing machine learning\nmodels for motor learning and their adaptation capabilities. We identify and\ndescribe three types of adaptation: Parameter adaptation in probabilistic\nmodels, Transfer and meta-learning in deep neural networks, and Planning\nadaptation by reinforcement learning. To conclude, we discuss challenges for\napplying these models in the domain of motor learning support systems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 11:11:26 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 07:40:27 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 13:07:49 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2020 15:00:42 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Caramiaux", "Baptiste", ""], ["Fran\u00e7oise", "Jules", ""], ["Liu", "Wanyu", ""], ["Sanchez", "T\u00e9o", ""], ["Bevilacqua", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2002.04333", "submitter": "Stratis Tsirtsis", "authors": "Stratis Tsirtsis and Manuel Gomez-Rodriguez", "title": "Decisions, Counterfactual Explanations and Strategic Behavior", "comments": "Transportation of mass experiment in main. Clarification of model\n  assumptions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.GT cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data-driven predictive models are increasingly used to inform decisions,\nit has been argued that decision makers should provide explanations that help\nindividuals understand what would have to change for these decisions to be\nbeneficial ones. However, there has been little discussion on the possibility\nthat individuals may use the above counterfactual explanations to invest effort\nstrategically and maximize their chances of receiving a beneficial decision. In\nthis paper, our goal is to find policies and counterfactual explanations that\nare optimal in terms of utility in such a strategic setting. We first show\nthat, given a pre-defined policy, the problem of finding the optimal set of\ncounterfactual explanations is NP-hard. Then, we show that the corresponding\nobjective is nondecreasing and satisfies submodularity and this allows a\nstandard greedy algorithm to enjoy approximation guarantees. In addition, we\nfurther show that the problem of jointly finding both the optimal policy and\nset of counterfactual explanations reduces to maximizing a non-monotone\nsubmodular function. As a result, we can use a recent randomized algorithm to\nsolve the problem, which also offers approximation guarantees. Finally, we\ndemonstrate that, by incorporating a matroid constraint into the problem\nformulation, we can increase the diversity of the optimal set of counterfactual\nexplanations and incentivize individuals across the whole spectrum of the\npopulation to self improve. Experiments on synthetic and real lending and\ncredit card data illustrate our theoretical findings and show that the\ncounterfactual explanations and decision policies found by our algorithms\nachieve higher utility than several competitive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 12:04:41 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 11:28:00 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 16:55:44 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Tsirtsis", "Stratis", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "2002.04494", "submitter": "Leon Derczynski", "authors": "Nanna Inie, Jeanette Falk Olesen, Leon Derczynski", "title": "The Rumour Mill: Making the Spread of Misinformation Explicit and\n  Tangible", "comments": "Accepted to CHI 2020 Interactivity", "journal-ref": null, "doi": "10.1145/3334480.3383159", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Misinformation spread presents a technological and social threat to society.\nWith the advance of AI-based language models, automatically generated texts\nhave become difficult to identify and easy to create at scale. We present \"The\nRumour Mill\", a playful art piece, designed as a commentary on the spread of\nrumours and automatically-generated misinformation. The mill is a tabletop\ninteractive machine, which invites a user to experience the process of creating\nbelievable text by interacting with different tangible controls on the mill.\nThe user manipulates visible parameters to adjust the genre and type of an\nautomatically generated text rumour. The Rumour Mill is a physical\ndemonstration of the state of current technology and its ability to generate\nand manipulate natural language text, and of the act of starting and spreading\nrumours.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 15:49:32 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 14:23:28 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Inie", "Nanna", ""], ["Olesen", "Jeanette Falk", ""], ["Derczynski", "Leon", ""]]}, {"id": "2002.04587", "submitter": "Jinghui Cheng", "authors": "Nasim Sharbatdar, Yassine Lamine, Brigitte Milord, Catherine Morency,\n  Jinghui Cheng", "title": "Capturing the Practices, Challenges, and Needs of Transportation\n  Decision-Makers", "comments": "7 pages, 0 figures, ACM CHI LBW Paper (2020). For personas created in\n  the project, see https://github.com/HCDLab/TDMPersonas", "journal-ref": null, "doi": "10.1145/3334480.3382864", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation decision-makers from government agencies play an important\nrole in addressing the traffic network conditions, which in turn, have a major\nimpact on the well-being of citizens. The practices, challenges, and needs of\nthis group of practitioners are less represented in the HCI literature. We\naddress this gap through an interview study with 19 practitioners from\nTransports Qu\\'ebec, a government agency responsible for transportation\ninfrastructures in Qu\\'ebec, Canada. We found that this group of\ndecision-makers can most benefit from research about data analysis tools and\nplatforms that (1) provide information to support data quality awareness, (2)\nare interoperable with other tools in the complex workflow of the\npractitioners, and (3) support intuitive and customizable visual analytics.\nThese implications can also be informative to the design of tools supporting\nother decision-making tasks and domains.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 18:30:01 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Sharbatdar", "Nasim", ""], ["Lamine", "Yassine", ""], ["Milord", "Brigitte", ""], ["Morency", "Catherine", ""], ["Cheng", "Jinghui", ""]]}, {"id": "2002.04631", "submitter": "Pardis Emami-Naeini", "authors": "Pardis Emami-Naeini, Yuvraj Agarwal, Lorrie Faith Cranor, Hanan Hibshi", "title": "Ask the Experts: What Should Be on an IoT Privacy and Security Label?", "comments": "To appear at the 41st IEEE Symposium on Security and Privacy (S&P'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information about the privacy and security of Internet of Things (IoT)\ndevices is not readily available to consumers who want to consider it before\nmaking purchase decisions. While legislators have proposed adding succinct,\nconsumer accessible, labels, they do not provide guidance on the content of\nthese labels. In this paper, we report on the results of a series of interviews\nand surveys with privacy and security experts, as well as consumers, where we\nexplore and test the design space of the content to include on an IoT privacy\nand security label. We conduct an expert elicitation study by following a\nthree-round Delphi process with 22 privacy and security experts to identify the\nfactors that experts believed are important for consumers when comparing the\nprivacy and security of IoT devices to inform their purchase decisions. Based\non how critical experts believed each factor is in conveying risk to consumers,\nwe distributed these factors across two layers---a primary layer to display on\nthe product package itself or prominently on a website, and a secondary layer\navailable online through a web link or a QR code. We report on the experts'\nrationale and arguments used to support their choice of factors. Moreover, to\nstudy how consumers would perceive the privacy and security information\nspecified by experts, we conducted a series of semi-structured interviews with\n15 participants, who had purchased at least one IoT device (smart home device\nor wearable). Based on the results of our expert elicitation and consumer\nstudies, we propose a prototype privacy and security label to help consumers\nmake more informed IoT-related purchase decisions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 19:01:14 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Emami-Naeini", "Pardis", ""], ["Agarwal", "Yuvraj", ""], ["Cranor", "Lorrie Faith", ""], ["Hibshi", "Hanan", ""]]}, {"id": "2002.04671", "submitter": "Christopher Birmingham", "authors": "Chris Birmingham, Zijian Hu, Kartik Mahajan, Eli Reber, and Maja J\n  Mataric", "title": "Can I Trust You? A User Study of Robot Mediation of a Support Group", "comments": "6 pages, 4 figures, accepted for publication in ICRA 2020", "journal-ref": "IEEE International Conference on Robotics and Automation (ICRA\n  2020)", "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Socially assistive robots have the potential to improve group dynamics when\ninteracting with groups of people in social settings. This work contributes to\nthe understanding of those dynamics through a user study of trust dynamics in\nthe novel context of a robot mediated support group. For this study, a novel\nframework for robot mediation of a support group was developed and validated.\nTo evaluate interpersonal trust in the multi-party setting, a dyadic trust\nscale was implemented and found to be uni-factorial, validating it as an\nappropriate measure of general trust. The results of this study demonstrate a\nsignificant increase in average interpersonal trust after the group interaction\nsession, and qualitative post-session interview data report that participants\nfound the interaction helpful and successfully supported and learned from one\nother. The results of the study validate that a robot-mediated support group\ncan improve trust among strangers and allow them to share and receive support\nfor their academic stress.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 20:43:12 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Birmingham", "Chris", ""], ["Hu", "Zijian", ""], ["Mahajan", "Kartik", ""], ["Reber", "Eli", ""], ["Mataric", "Maja J", ""]]}, {"id": "2002.04700", "submitter": "Ziyang Wang", "authors": "Ziyang Wang", "title": "A Single RGB Camera Based Gait Analysis with a Mobile Tele-Robot for\n  Healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing awareness of high-quality life, there is a growing need\nfor health monitoring devices running robust algorithms in home environment.\nHealth monitoring technologies enable real-time analysis of users' health\nstatus, offering long-term healthcare support and reducing hospitalization\ntime. The purpose of this work is twofold, the software focuses on the analysis\nof gait, which is widely adopted for joint correction and assessing any lower\nlimb or spinal problem. On the hardware side, we design a novel marker-less\ngait analysis device using a low-cost RGB camera mounted on a mobile\ntele-robot. As gait analysis with a single camera is much more challenging\ncompared to previous works utilizing multi-cameras, a RGB-D camera or wearable\nsensors, we propose using vision-based human pose estimation approaches. More\nspecifically, based on the output of two state-of-the-art human pose estimation\nmodels (Openpose and VNect), we devise measurements for four bespoke gait\nparameters: inversion/eversion, dorsiflexion/plantarflexion, ankle and foot\nprogression angles. We thereby classify walking patterns into normal,\nsupination, pronation and limp. We also illustrate how to run the purposed\nmachine learning models in low-resource environments such as a single\nentry-level CPU. Experiments show that our single RGB camera method achieves\ncompetitive performance compared to state-of-the-art methods based on depth\ncameras or multi-camera motion capture system, at smaller hardware costs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:42:22 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 16:24:28 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 14:53:55 GMT"}, {"version": "v4", "created": "Sun, 15 Mar 2020 03:27:52 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wang", "Ziyang", ""]]}, {"id": "2002.04833", "submitter": "Smitha Milli", "authors": "Hong Jun Jeon, Smitha Milli, Anca D. Dragan", "title": "Reward-rational (implicit) choice: A unifying formalism for reward\n  learning", "comments": "Published at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often difficult to hand-specify what the correct reward function is for\na task, so researchers have instead aimed to learn reward functions from human\nbehavior or feedback. The types of behavior interpreted as evidence of the\nreward function have expanded greatly in recent years. We've gone from\ndemonstrations, to comparisons, to reading into the information leaked when the\nhuman is pushing the robot away or turning it off. And surely, there is more to\ncome. How will a robot make sense of all these diverse types of behavior? Our\nkey insight is that different types of behavior can be interpreted in a single\nunifying formalism - as a reward-rational choice that the human is making,\noften implicitly. The formalism offers both a unifying lens with which to view\npast work, as well as a recipe for interpreting new sources of information that\nare yet to be uncovered. We provide two examples to showcase this: interpreting\na new feedback type, and reading into how the choice of feedback itself leaks\ninformation about the reward.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 08:07:49 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 18:21:03 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 05:17:25 GMT"}, {"version": "v4", "created": "Fri, 11 Dec 2020 17:56:03 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Jeon", "Hong Jun", ""], ["Milli", "Smitha", ""], ["Dragan", "Anca D.", ""]]}, {"id": "2002.04980", "submitter": "Travis Gesslein", "authors": "Travis Gesslein, Jens Grubert", "title": "C-D Ratio in multi-display environments", "comments": "10 pages, master thesis paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Research in user interaction with mixed reality environments using multiple\ndisplays has become increasingly relevant with the prevalence of mobile devices\nin everyday life and increased commoditization of large display area\ntechnologies using projectors or large displays. Previous work often combines\ntouch-based input with other approaches, such as gesture-based input, to expand\nthe possible interaction space or deal with limitations of other\ntwo-dimensional input methods. In contrast to previous methods, we examine the\npossibilities when the control-display (C-D) ratio is significantly smaller\nthan one and small input movements result in large output movements. To this\nend one specific multi-display configuration is implemented in the form of a\nspatial-augmented reality sandbox environment, and used to explore various\ninteraction techniques based on a variety of mobile device touch-based input\nand optical marker tracking-based finger input. A small pilot study determines\nthe most promising input candidate, which is compared to traditional\ntouch-input based techniques in a user study that tests it for practical\nrelevance. Results and conclusions of the study are presented.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 13:37:24 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Gesslein", "Travis", ""], ["Grubert", "Jens", ""]]}, {"id": "2002.05036", "submitter": "Pengcheng An", "authors": "Pengcheng An, Saskia Bakker, Sara Ordanovski, Chris L.E. Paffen, Ruurd\n  Taconis, Berry Eggen", "title": "Dandelion Diagram: Aggregating Positioning and Orientation Data in the\n  Visualization of Classroom Proxemics", "comments": "To be published in CHI'20 Extended Abstracts (April 25-30, 2020), 8\n  pages, 4 figures", "journal-ref": null, "doi": "10.1145/3334480.3382795", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past two years, an emerging body of HCI work has been focused on\nclassroom proxemics - how teachers divide time and attention over students in\nthe different regions of the classroom. Tracking and visualizing this implicit\nyet relevant dimension of teaching can benefit both research and teacher\nprofessionalization. Prior work has proved the value of depicting teachers'\nwhereabouts. Yet a major opportunity remains in the design of new, synthesized\nvisualizations that help researchers and practitioners to gain more insights in\nthe vast tracking data. We present Dandelion Diagram, a synthesized heatmap\ntechnique that combines both teachers' positioning and orientation (heading)\ndata, and affords richer representations in addition to whereabouts - For\nexample, teachers' attention pattern (which directions they were attending to),\nand their mobility pattern (i.e., trajectories in the classroom). Utilizing\nvarious classroom data from a field study, this paper illustrates the design\nand utility of Dandelion Diagram.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 14:56:18 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["An", "Pengcheng", ""], ["Bakker", "Saskia", ""], ["Ordanovski", "Sara", ""], ["Paffen", "Chris L. E.", ""], ["Taconis", "Ruurd", ""], ["Eggen", "Berry", ""]]}, {"id": "2002.05242", "submitter": "Nataniel Ruiz", "authors": "Nataniel Ruiz, Mona Jalal, Vitaly Ablavsky, Danielle Allessio, John\n  Magee, Jacob Whitehill, Ivon Arroyo, Beverly Woolf, Stan Sclaroff, Margrit\n  Betke", "title": "Leveraging Affect Transfer Learning for Behavior Prediction in an\n  Intelligent Tutoring System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of building an intelligent tutoring system (ITS), which\nimproves student learning outcomes by intervention, we set out to improve\nprediction of student problem outcome. In essence, we want to predict the\noutcome of a student answering a problem in an ITS from a video feed by\nanalyzing their face and gestures. For this, we present a novel transfer\nlearning facial affect representation and a user-personalized training scheme\nthat unlocks the potential of this representation. We model the temporal\nstructure of video sequences of students solving math problems using a\nrecurrent neural network architecture. Additionally, we extend the largest\ndataset of student interactions with an intelligent online math tutor by a\nfactor of two. Our final model, coined ATL-BP (Affect Transfer Learning for\nBehavior Prediction) achieves an increase in mean F-score over state-of-the-art\nof 45% on this new dataset in the general case and 50% in a more challenging\nleave-users-out experimental setting when we use a user-personalized training\nscheme.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 21:30:34 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Ruiz", "Nataniel", ""], ["Jalal", "Mona", ""], ["Ablavsky", "Vitaly", ""], ["Allessio", "Danielle", ""], ["Magee", "John", ""], ["Whitehill", "Jacob", ""], ["Arroyo", "Ivon", ""], ["Woolf", "Beverly", ""], ["Sclaroff", "Stan", ""], ["Betke", "Margrit", ""]]}, {"id": "2002.05271", "submitter": "Min Chen", "authors": "Qianwen Wang, William Alexander, Jack Pegg, Huamin Qu, and Min Chen", "title": "HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine\n  Learning Models", "comments": "This article was submitted to EuroVis 2020 on 5 December 2020. It was\n  not accepted. Because the reviews have not identified any technical problems\n  that would undermine the novelty and validity of this work, we think that the\n  article is ready to be released as an arXiv report. The EuroVis 2020 reviews\n  and authors' short feedback can be found in the anc folder", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, Feb.\n  2021", "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a visual analytics tool for enabling\nhypothesis-based evaluation of machine learning (ML) models. We describe a\nnovel ML-testing framework that combines the traditional statistical hypothesis\ntesting (commonly used in empirical research) with logical reasoning about the\nconclusions of multiple hypotheses. The framework defines a controlled\nconfiguration for testing a number of hypotheses as to whether and how some\nextra information about a \"concept\" or \"feature\" may benefit or hinder a ML\nmodel. Because reasoning multiple hypotheses is not always straightforward, we\nprovide HypoML as a visual analysis tool, with which, the multi-thread testing\ndata is transformed to a visual representation for rapid observation of the\nconclusions and the logical flow between the testing data and hypotheses.We\nhave applied HypoML to a number of hypothesized concepts, demonstrating the\nintuitive and explainable nature of the visual analysis.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 23:03:44 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Wang", "Qianwen", ""], ["Alexander", "William", ""], ["Pegg", "Jack", ""], ["Qu", "Huamin", ""], ["Chen", "Min", ""]]}, {"id": "2002.05282", "submitter": "Min Chen", "authors": "Min Chen, Mateu Sbert, Alfie Abdul-Rahman, and Deborah Silver", "title": "A Bounded Measure for Estimating the Benefit of Visualization", "comments": "Comment on version 2: This revised version, which includes a new\n  formal proof, many additions, and a detailed revision report, was submitted\n  to SciVis 2020. Unexpectedly, our revision effort did not have much influence\n  on the SciVis 2020 reviewers who gave an outright rejection with lower scores\n  than EuroVis reviews. We will share these reviews after we have completed our\n  feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GR cs.HC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theory can be used to analyze the cost-benefit of visualization\nprocesses. However, the current measure of benefit contains an unbounded term\nthat is neither easy to estimate nor intuitive to interpret. In this work, we\npropose to revise the existing cost-benefit measure by replacing the unbounded\nterm with a bounded one. We examine a number of bounded measures that include\nthe Jenson-Shannon divergence and a new divergence measure formulated as part\nof this work. We use visual analysis to support the multi-criteria comparison,\nnarrowing the search down to those options with better mathematical properties.\nWe apply those remaining options to two visualization case studies to\ninstantiate their uses in practical scenarios, while the collected real world\ndata further informs the selection of a bounded measure, which can be used to\nestimate the benefit of visualization.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 23:39:07 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 20:33:33 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chen", "Min", ""], ["Sbert", "Mateu", ""], ["Abdul-Rahman", "Alfie", ""], ["Silver", "Deborah", ""]]}, {"id": "2002.05305", "submitter": "Wanze Xie", "authors": "Wanze Xie, Yining Liang, Janet Johnson, Andrea Mower, Samuel Burns,\n  Colleen Chelini, Paul D Alessandro, Nadir Weibel, J\\\"urgen P. Schulze", "title": "Interactive Multi-User 3D Visual Analytics in Augmented Reality", "comments": "In Proceedings of IS&T The Engineering Reality of Virtual Reality\n  2020", "journal-ref": "Electronic Imaging, The Engineering Reality of Virtual Reality\n  2020, pp. 363-1-363-6(6)", "doi": "10.2352/ISSN.2470-1173.2020.13.ERVR-363", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This publication reports on a research project in which we set out to explore\nthe advantages and disadvantages augmented reality (AR) technology has for\nvisual data analytics. We developed a prototype of an AR data analytics\napplication, which provides users with an interactive 3D interface, hand\ngesture-based controls and multi-user support for a shared experience, enabling\nmultiple people to collaboratively visualize, analyze and manipulate data with\nhigh dimensional features in 3D space. Our software prototype, called DataCube,\nruns on the Microsoft HoloLens - one of the first true stand-alone AR headsets,\nthrough which users can see computer-generated images overlaid onto real-world\nobjects in the user's physical environment. Using hand gestures, the users can\nselect menu options, control the 3D data visualization with various filtering\nand visualization functions, and freely arrange the various menus and virtual\ndisplays in their environment. The shared multi-user experience allows all\nparticipating users to see and interact with the virtual environment, changes\none user makes will become visible to the other users instantly. As users\nengage together they are not restricted from observing the physical world\nsimultaneously and therefore they can also see non-verbal cues such as\ngesturing or facial reactions of other users in the physical environment. The\nmain objective of this research project was to find out if AR interfaces and\ncollaborative analysis can provide an effective solution for data analysis\ntasks, and our experience with our prototype system confirms this.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 01:35:56 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Xie", "Wanze", ""], ["Liang", "Yining", ""], ["Johnson", "Janet", ""], ["Mower", "Andrea", ""], ["Burns", "Samuel", ""], ["Chelini", "Colleen", ""], ["Alessandro", "Paul D", ""], ["Weibel", "Nadir", ""], ["Schulze", "J\u00fcrgen P.", ""]]}, {"id": "2002.05409", "submitter": "David Baum", "authors": "David Baum, Pascal Kovacs, Ulrich Eisenecker, Richard M\\\"uller", "title": "A User-centered Approach for Optimizing Information Visualizations", "comments": null, "journal-ref": "WSCG2016", "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of information visualizations is time consuming and\nexpensive. To reduce this we propose an improvement of existing optimization\napproaches based on user-centered design, focusing on readability,\ncomprehensibility, and user satisfaction as optimization goals. The changes\ncomprise (1) a separate optimization of user interface and representation, (2)\na fully automated evaluation of the representation, and (3) qualitative user\nstudies for simultaneously creating and evaluating interface variants. On the\nbasis of these results we are able to find a local optimum of an information\nvisualization in an efficient way.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 09:50:34 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Baum", "David", ""], ["Kovacs", "Pascal", ""], ["Eisenecker", "Ulrich", ""], ["M\u00fcller", "Richard", ""]]}, {"id": "2002.05505", "submitter": "Byungsoo Kim", "authors": "Youngduck Choi, Youngnam Lee, Junghyun Cho, Jineon Baek, Dongmin Shin,\n  Hangyeol Yu, Yugeun Shim, Seewoo Lee, Jonghun Shin, Chan Bae, Byungsoo Kim,\n  Jaewe Heo", "title": "Assessment Modeling: Fundamental Pre-training Tasks for Interactive\n  Educational Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like many other domains in Artificial Intelligence (AI), there are specific\ntasks in the field of AI in Education (AIEd) for which labels are scarce and\nexpensive, such as predicting exam score or review correctness. A common way of\ncircumventing label-scarce problems is pre-training a model to learn\nrepresentations of the contents of learning items. However, such methods fail\nto utilize the full range of student interaction data available and do not\nmodel student learning behavior. To this end, we propose Assessment Modeling, a\nclass of fundamental pre-training tasks for general interactive educational\nsystems. An assessment is a feature of student-system interactions which can\nserve as a pedagogical evaluation. Examples include the correctness and\ntimeliness of a student's answer. Assessment Modeling is the prediction of\nassessments conditioned on the surrounding context of interactions. Although it\nis natural to pre-train on interactive features available in large amounts,\nlimiting the prediction targets to assessments focuses the tasks' relevance to\nthe label-scarce educational problems and reduces less-relevant noise. While\nthe effectiveness of different combinations of assessments is open for\nexploration, we suggest Assessment Modeling as a first-order guiding principle\nfor selecting proper pre-training tasks for label-scarce educational problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 02:00:07 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 04:57:54 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 06:45:40 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2020 07:00:48 GMT"}, {"version": "v5", "created": "Fri, 14 Aug 2020 01:31:13 GMT"}, {"version": "v6", "created": "Mon, 28 Jun 2021 05:00:25 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Choi", "Youngduck", ""], ["Lee", "Youngnam", ""], ["Cho", "Junghyun", ""], ["Baek", "Jineon", ""], ["Shin", "Dongmin", ""], ["Yu", "Hangyeol", ""], ["Shim", "Yugeun", ""], ["Lee", "Seewoo", ""], ["Shin", "Jonghun", ""], ["Bae", "Chan", ""], ["Kim", "Byungsoo", ""], ["Heo", "Jaewe", ""]]}, {"id": "2002.05593", "submitter": "Christoph Klemenjak", "authors": "Hafsa Bousbiat, Christoph Klemenjak, Gerhard Leitner and Wilfried\n  Elmenreich", "title": "Augmenting an Assisted Living Lab with Non-Intrusive Load Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for reducing our energy consumption footprint and the increasing\nnumber of electric devices in today's homes is calling for new solutions that\nallow users to efficiently manage their energy consumption. Real-time feedback\nat device level would be of a significant benefit for this application. In\naddition, the aging population and their wish to be more autonomous have\nmotivated the use of this same real-time data to indirectly monitor the\nhousehold's occupants for their safety. By breaking down aggregate power\nconsumption into its components, Non-Intrusive Load Monitoring provides\ninformation on individual appliances and their current state of operation.\nSince no additional metering equipment is required, residents are not\nconfronted with intrusion into their familiar environment. Our work aims to\ndepict an architecture supporting non-intrusive measurement with a smart\nelectricity meter and the handling of these data using an open-source platform\nthat allows to visualize and process real-time data about the total energy\nconsumed. As a case study, we describe a series of measurements from common\nhousehold devices and show how abnormal behavior can be detected.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 16:17:02 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Bousbiat", "Hafsa", ""], ["Klemenjak", "Christoph", ""], ["Leitner", "Gerhard", ""], ["Elmenreich", "Wilfried", ""]]}, {"id": "2002.05674", "submitter": "Micha{\\l} Ku\\'zba", "authors": "Micha{\\l} Ku\\'zba, Przemys{\\l}aw Biecek", "title": "What Would You Ask the Machine Learning Model? Identification of User\n  Needs for Model Explanations Based on Human-Model Conversations", "comments": "13 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently we see a rising number of methods in the field of eXplainable\nArtificial Intelligence. To our surprise, their development is driven by model\ndevelopers rather than a study of needs for human end users. The analysis of\nneeds, if done, takes the form of an A/B test rather than a study of open\nquestions. To answer the question \"What would a human operator like to ask the\nML model?\" we propose a conversational system explaining decisions of the\npredictive model. In this experiment, we developed a chatbot called dr_ant to\ntalk about machine learning model trained to predict survival odds on Titanic.\nPeople can talk with dr_ant about different aspects of the model to understand\nthe rationale behind its predictions. Having collected a corpus of 1000+\ndialogues, we analyse the most common types of questions that users would like\nto ask. To our knowledge, it is the first study which uses a conversational\nsystem to collect the needs of human operators from the interactive and\niterative dialogue explorations of a predictive model.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 15:59:49 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 17:14:08 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 13:49:43 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Ku\u017aba", "Micha\u0142", ""], ["Biecek", "Przemys\u0142aw", ""]]}, {"id": "2002.05721", "submitter": "Baptiste Wojtkowski", "authors": "Baptiste Wojtkowski (Heudiasyc), Pedro Castillo (Heudiasyc), Indira\n  Thouvenin (Heudiasyc)", "title": "A New Exocentric Metaphor for Complex Path Following to Control a UAV\n  Using Mixed Reality", "comments": null, "journal-ref": "Journ\\'ees Fran\\c{c}aise de l'Informatique graphique et de\n  R\\'ealit\\'e Virtuelle, Nov 2019, Marseille, France", "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teleoperation of Unmanned Aerial Vehicles (UAVs) has recently become an\nnoteworthly research topic in the field of human robot interaction. Each year,\na variety of devices is being studied to design adapted interface for diverse\npurpose such as view taking, search and rescue operation or suveillance. New\ninterfaces have to be precise, simple and intuitive even for complex path\nplanning. Moreover, when teleoperation involves long distance control, user\nneeds to get proper feedbacks and avoid motion sickness. In order to overcome\nall these challenges, a new interaction metaphor named DrEAM (Drone Exocentric\nAdvanced Metaphor) was designed. User can see the UAV he is controlling in a\nvirtual environment mapped to the real world. He can interact with it as a\nsimple object in a classical virtual world. An experiment was lead in order to\nevaluate the perfomances of this metaphor, comparing performance of novice user\nusing either a direct-view joystick control or using DrEAM.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 11:02:33 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Wojtkowski", "Baptiste", "", "Heudiasyc"], ["Castillo", "Pedro", "", "Heudiasyc"], ["Thouvenin", "Indira", "", "Heudiasyc"]]}, {"id": "2002.05790", "submitter": "Chang Xu", "authors": "Ningbo Yu, Chang Xu", "title": "An Improved Wrist Kinematic Model for Human-Robot Interaction", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human kinematics is of fundamental importance for rehabilitation and\nassistive robotic systems that physically interact with human. The wrist plays\nan essential role for dexterous human-robot interaction, but its conventional\nkinematic model is oversimplified with intrinsic inaccuracies and its\nbiomechanical model is too complicated for robotic applications. In this work,\nwe establish an improved kinematic model of the wrist. In vivo kinematic\nbehavior of the wrist was investigated through noninvasive marker-less optical\ntracking. Data analysis demonstrated the existence of measurable dynamic axes\nin carpal rotation, justifying inevitable misalignment between the wrist and\nrobotic representation if using the conventional wrist model. A novel wrist\nkinematic model was then proposed with rigid body transformation in fusion with\na varying prismatic term indicating the dynamic axes location. Accurate and\nreal-time estimation of this term has been achieved through coupled wrist\nangles with nonlinear regression. The proposed model is not only accurate but\nalso conveniently applicable for translating anatomical behaviors into robotic\nimplementation and functional assessment for precise and dexterous human-robot\ninteraction.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 21:48:00 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Yu", "Ningbo", ""], ["Xu", "Chang", ""]]}, {"id": "2002.05886", "submitter": "Jadab Kumar Pal Dr", "authors": "Jimut Bahan Pal", "title": "How to cluster nearest unique nodes from different classes using\n  JJCluster in Wisp application?", "comments": "A new type of clustering algorithm is built which helps to find the\n  best place for any location by giving a set of preferences to the\n  application. Source code can be found here: https://github.com/Jimut123/wisp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work of finding the best place according to user preference is a tedious\ntask. It needs manual research and lot of intuitive process to find the best\nlocation according to some earlier knowledge about the place. It is mainly\nabout accessing publicly available spatial data, applying a simple algorithm to\nsummarize the data according to given preferences, and visualizing the result\non a map. We introduced JJCluster to eliminate the rigorous way of researching\nabout a place and visualizing the location in real time. This algorithm\nsuccessfully finds the heart of a city when used in Wisp application. The main\npurpose of designing Wisp application is used for finding the perfect location\nfor a trip to unknown place which is nearest to a set of preferences. We also\ndiscussed the various optimization algorithms that are pioneer of today's\ndynamic programming and the need for visualization to find patterns when the\ndata is cluttered. Yet, this general clustering algorithm can be used in other\nareas where we can explore every possible preference to maximize its utility.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 06:38:01 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 08:42:56 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Pal", "Jimut Bahan", ""]]}, {"id": "2002.05936", "submitter": "Marcus Scheunemann", "authors": "Marcus M. Scheunemann, Christoph Salge, Daniel Polani, Kerstin\n  Dautenhahn", "title": "Human Perception of Intrinsically Motivated Autonomy in Human-Robot\n  Interaction", "comments": "32 pages, 3 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenge in using robots in human-inhabited environments is to design\nbehavior that is engaging, yet robust to the perturbations induced by human\ninteraction. Our idea is to imbue the robot with intrinsic motivation (IM) so\nthat it can handle new situations and appears as a genuine social other to\nhumans and thus be of more interest to a human interaction partner. Human-robot\ninteraction (HRI) experiments mainly focus on scripted or teleoperated robots,\nthat mimic characteristics such as IM to control isolated behavior factors.\nThis article presents a \"robotologist\" study design that allows comparing\nautonomously generated behaviors with each other, and, for the first time,\nevaluates the human perception of IM-based generated behavior in robots. We\nconducted a within-subjects user study (N=24) where participants interacted\nwith a fully autonomous Sphero BB8 robot with different behavioral regimes: one\nrealizing an adaptive, intrinsically motivated behavior and the other being\nreactive, but not adaptive. A quantitative analysis of post-interaction\nquestionnaires showed a significantly higher perception of the dimension\n\"Warmth\" compared to the reactive baseline behavior. Warmth is considered a\nprimary dimension for social attitude formation in human social cognition. A\nhuman perceived as warm (friendly, trustworthy) experiences more positive\nsocial interactions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 09:49:36 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 20:40:53 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 17:54:22 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Scheunemann", "Marcus M.", ""], ["Salge", "Christoph", ""], ["Polani", "Daniel", ""], ["Dautenhahn", "Kerstin", ""]]}, {"id": "2002.05963", "submitter": "Leonel Merino", "authors": "Leonel Merino, Boris Sotomayor-G\\'omez, Xingyao Yu, Ronie Salgado,\n  Alexandre Bergel, Michael Sedlmair, Daniel Weiskopf", "title": "Toward Agile Situated Visualization: An Exploratory User Study", "comments": "CHI '20 Extended Abstracts", "journal-ref": null, "doi": "10.1145/3334480.3383017", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce AVAR, a prototypical implementation of an agile situated\nvisualization (SV) toolkit targeting liveness, integration, and expressiveness.\nWe report on results of an exploratory study with AVAR and seven expert users.\nIn it, participants wore a Microsoft HoloLens device and used a Bluetooth\nkeyboard to program a visualization script for a given dataset. To support our\nanalysis, we (i) video recorded sessions, (ii) tracked users' interactions, and\n(iii) collected data of participants' impressions. Our prototype confirms that\nagile SV is feasible. That is, liveness boosted participants' engagement when\nprogramming an SV, and so, the sessions were highly interactive and\nparticipants were willing to spend much time using our toolkit (i.e., median >=\n1.5 hours). Participants used our integrated toolkit to deal with data\ntransformations, visual mappings, and view transformations without leaving the\nimmersive environment. Finally, participants benefited from our expressive\ntoolkit and employed multiple of the available features when programming an SV.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 10:53:10 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Merino", "Leonel", ""], ["Sotomayor-G\u00f3mez", "Boris", ""], ["Yu", "Xingyao", ""], ["Salgado", "Ronie", ""], ["Bergel", "Alexandre", ""], ["Sedlmair", "Michael", ""], ["Weiskopf", "Daniel", ""]]}, {"id": "2002.06069", "submitter": "Yixuan Zhang", "authors": "Yixuan Zhang and Andrea G. Parker", "title": "Eat4Thought: A Design of Food Journaling", "comments": "8 pages", "journal-ref": null, "doi": "10.1145/3334480.3383044", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food journaling is an effective method to help people identify their eating\npatterns and encourage healthy eating habits as it requires self-reflection on\neating behaviors. Current tools have predominately focused on tracking food\nintake, such as carbohydrates, proteins, fats, and calories. Other factors,\nsuch as contextual information and momentary thoughts and feelings that are\ninternal to an individual, are also essential to help people reflect upon and\nchange attitudes about eating behaviors. However, current dietary tracking\ntools rarely support capturing these elements as a way to foster deep\nreflection. In this work, we present Eat4Thought -- a food journaling\napplication that allows users to track their emotional, sensory, and\nspatio-temporal elements of meals as a means of supporting self-reflection. The\napplication enables vivid documentation of experiences and self-reflection on\nthe past through video recording. We describe our design process and an initial\nevaluation of the application. We also provide design recommendations for\nfuture work on food journaling.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 15:19:40 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Zhang", "Yixuan", ""], ["Parker", "Andrea G.", ""]]}, {"id": "2002.06093", "submitter": "Anthony Steed", "authors": "Anthony Steed, Sebastian Friston, Vijay Pawar, David Swapp", "title": "Docking Haptics: Extending the Reach of Haptics by Dynamic Combinations\n  of Grounded and Worn Devices", "comments": null, "journal-ref": null, "doi": "10.1145/3385956.3418943", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounded haptic devices can provide a variety of forces but have limited\nworking volumes. Wearable haptic devices operate over a large volume but are\nrelatively restricted in the types of stimuli they can generate. We propose the\nconcept of docking haptics, in which different types of haptic devices are\ndynamically docked at run time. This creates a hybrid system, where the\npotential feedback depends on the user's location. We show a prototype docking\nhaptic workspace, combining a grounded six degree-of-freedom force feedback arm\nwith a hand exoskeleton. We are able to create the sensation of weight on the\nhand when it is within reach of the grounded device, but away from the grounded\ndevice, hand-referenced force feedback is still available. A user study\ndemonstrates that users can successfully discriminate weight when using docking\nhaptics, but not with the exoskeleton alone. Such hybrid systems would be able\nto change configuration further, for example docking two grounded devices to a\nhand in order to deliver twice the force, or extend the working volume. We\nsuggest that the docking haptics concept can thus extend the practical utility\nof haptics in user interfaces.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 15:55:13 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 16:50:49 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Steed", "Anthony", ""], ["Friston", "Sebastian", ""], ["Pawar", "Vijay", ""], ["Swapp", "David", ""]]}, {"id": "2002.06125", "submitter": "Raul Lima", "authors": "Raul de Ara\\'ujo Lima and Simone Diniz Junqueira Barbosa", "title": "VisMaker: a Question-Oriented Visualization Recommender System for Data\n  Exploration", "comments": "14 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasingly rapid growth of data production and the consequent need to\nexplore data to obtain answers to the most varied questions have promoted the\ndevelopment of tools to facilitate the manipulation and construction of data\nvisualizations. However, building useful data visualizations is not a trivial\ntask: it may involve a large number of subtle decisions that require experience\nfrom their designer. In this paper, we present VisMaker, a visualization\nrecommender tool that uses a set of rules to present visualization\nrecommendations organized and described through questions, in order to\nfacilitate the understanding of the recommendations and assisting the visual\nexploration process. We carried out two studies comparing our tool with Voyager\n2 and analyzed some aspects of the use of tools. We collected feedback from\nparticipants to identify the advantages and disadvantages of our recommendation\napproach. As a result, we gathered comments to help improve the development of\ntools in this domain.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 16:56:59 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Lima", "Raul de Ara\u00fajo", ""], ["Barbosa", "Simone Diniz Junqueira", ""]]}, {"id": "2002.06280", "submitter": "Ammar Malik", "authors": "Ammar Malik, Hugo Lhachemi, Robert Shorten", "title": "I-nteract: A cyber-physical system for real-time interaction with\n  physical and virtual objects using mixed reality technologies for additive\n  manufacturing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents I-nteract, a cyber-physical system that enables real-time\ninteraction with real and virtual objects in a mixed augmented reality\nenvironment to design 3D models for additive manufacturing. The system has been\ndeveloped using mixed reality technologies such as HoloLens, for augmenting\nvisual feedback, and haptic gloves, for augmenting haptic force feedback. The\nefficacy of the system has been demonstrated by generating 3D model using a\nnovel scanning method to 3D print a customized orthopedic cast for human arm,\nby estimating spring rates of compression springs, and by simulating\ninteraction with a virtual spring using hand.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 22:57:35 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Malik", "Ammar", ""], ["Lhachemi", "Hugo", ""], ["Shorten", "Robert", ""]]}, {"id": "2002.06288", "submitter": "Sriram Gopalakrishnan", "authors": "Sriram Gopalakrishnan, Utkarsh Soni", "title": "Let Me At Least Learn What You Really Like: Dealing With Noisy Humans\n  When Learning Preferences", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the preferences of a human improves the quality of the interaction\nwith the human. The number of queries available to learn preferences maybe\nlimited especially when interacting with a human, and so active learning is a\nmust. One approach to active learning is to use uncertainty sampling to decide\nthe informativeness of a query. In this paper, we propose a modification to\nuncertainty sampling which uses the expected output value to help speed up\nlearning of preferences. We compare our approach with the uncertainty sampling\nbaseline, as well as conduct an ablation study to test the validity of each\ncomponent of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 00:36:23 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Gopalakrishnan", "Sriram", ""], ["Soni", "Utkarsh", ""]]}, {"id": "2002.06308", "submitter": "Matteo Zallio Dr.", "authors": "Matteo Zallio, John McGrory, and Damon Berry", "title": "How to democratize Internet of Things devices. A participatory design\n  research", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": "10.1007/978-3-030-51194-4_19", "report-no": null, "categories": "cs.HC cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global introduction of affordable Internet of Things (IoT) devices offers\nan opportunity to empower a large variety of users with different needs.\nHowever, many off-the-shelf digital products are still not widely adopted by\npeople who are hesitant technology users or by older adults, notwithstanding\nthat the design and user-interaction of these devices is recognized to be\nuser-friendly. In view of the potential of IoT-based devices, how can we reduce\nthe obstacles of a cohort with low digital literacy and technology anxiety and\nenable them to be equal participants in the digitalized world? This article\nshows the method and results achieved in a community-stakeholder workshop,\ndeveloped through the participatory design methodology, aiming at brainstorming\nproblems and scenarios through a focus group and a structured survey. The\nresearch activity focused on understanding factors to increase the usability of\noff-the-shelf IoT devices for hesitant users and identify strategies for\nimproving digital literacy and reducing technology anxiety. A notable result\nwas a series of feedback items pointing to the importance of creating learning\nresources to support individuals with different abilities, age, gender\nexpression, to better adopt off-the-shelf IoT-based solutions.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 03:06:46 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 00:01:57 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 14:36:35 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 23:13:19 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Zallio", "Matteo", ""], ["McGrory", "John", ""], ["Berry", "Damon", ""]]}, {"id": "2002.06417", "submitter": "Chao Wang", "authors": "Chao Wang, Stephan Hasler, Manuel Muehlig, Frank Joublin, Antonello\n  Ceravola, Joerg Deigmoeller, Lydia Fischer", "title": "Designing Interaction for Multi-agent Cooperative System in an Office\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future intelligent system will involve very various types of artificial\nagents, such as mobile robots, smart home infrastructure or personal devices,\nwhich share data and collaborate with each other to execute certain\ntasks.Designing an efficient human-machine interface, which can support users\nto express needs to the system, supervise the collaboration progress of\ndifferent entities and evaluate the result, will be challengeable. This paper\npresents the design and implementation of the human-machine interface of\nIntelligent Cyber-Physical system (ICPS),which is a multi-entity coordination\nsystem of robots and other smart devices in a working environment. ICPS gathers\nsensory data from entities and then receives users' command, then optimizes\nplans to utilize the capability of different entities to serve people. Using\nmulti-model interaction methods, e.g. graphical interfaces, speech interaction,\ngestures and facial expressions, ICPS is able to receive inputs from users\nthrough different entities, keep users aware of the progress and accomplish the\ntask efficiently\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 17:36:00 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Wang", "Chao", ""], ["Hasler", "Stephan", ""], ["Muehlig", "Manuel", ""], ["Joublin", "Frank", ""], ["Ceravola", "Antonello", ""], ["Deigmoeller", "Joerg", ""], ["Fischer", "Lydia", ""]]}, {"id": "2002.06445", "submitter": "Daniel Krutz", "authors": "Weishi Shi, Samuel Malachowsky, Yasmine El-Glaly, Qi Yu, Daniel E.\n  Krutz", "title": "Presenting and Evaluating the Impact of Experiential Learning in\n  Computing Accessibility Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies indicate that much of the software created today is not accessible to\nall users, indicating that developers don't see the need to devote sufficient\nresources to creating accessible software. Compounding this problem, there is a\nlack of robust, easily adoptable educational accessibility material available\nto instructors for inclusion in their curricula. To address these issues, we\nhave created five Accessibility Learning Labs (ALL) using an experiential\nlearning structure. The labs are designed to educate and create awareness of\naccessibility needs in computing. The labs enable easy classroom integration by\nproviding instructors with complete educational materials including lecture\nslides, activities, and quizzes. The labs are hosted on our servers and require\nonly a browser to be utilized.\n  To demonstrate the benefit of our material and the potential benefits of our\nexperiential lab format with empathy-creating material, we conducted a study\ninvolving 276 students in ten sections of an introductory computing course. Our\nfindings include: (I) The demonstrated potential of the proposed experiential\nlearning format and labs are effective in motivating and educating students\nabout the importance of accessibility (II) The labs are effective in informing\nstudents about foundational accessibility topics (III) Empathy-creating\nmaterial is demonstrated to be a beneficial component in computing\naccessibility education, supporting students in placing a higher value on the\nimportance of creating accessible software. Created labs and project materials\nare publicly available on the project website: http://all.rit.edu\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 20:32:12 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 17:43:40 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 11:19:25 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Shi", "Weishi", ""], ["Malachowsky", "Samuel", ""], ["El-Glaly", "Yasmine", ""], ["Yu", "Qi", ""], ["Krutz", "Daniel E.", ""]]}, {"id": "2002.06581", "submitter": "Titas De", "authors": "Catalin Voss, Peter Washington, Nick Haber, Aaron Kline, Jena Daniels,\n  Azar Fazel, Titas De, Beth McCarthy, Carl Feinstein, Terry Winograd, Dennis\n  Wall", "title": "Superpower Glass: Delivering Unobtrusive Real-time Social Cues in\n  Wearable Systems", "comments": "UbiComp ISWC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a system for automatic facial expression recognition, which\nruns on Google Glass and delivers real-time social cues to the wearer. We\nevaluate the system as a behavioral aid for children with Autism Spectrum\nDisorder (ASD), who can greatly benefit from real-time non-invasive emotional\ncues and are more sensitive to sensory input than neurotypically developing\nchildren. In addition, we present a mobile application that enables users of\nthe wearable aid to review their videos along with auto-curated emotional\ninformation on the video playback bar. This integrates our learning aid into\nthe context of behavioral therapy. Expanding on our previous work describing\nin-lab trials, this paper presents our system and application-level design\ndecisions in depth as well as the interface learnings gathered during the use\nof the system by multiple children with ASD in an at-home iterative trial.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 13:49:58 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Voss", "Catalin", ""], ["Washington", "Peter", ""], ["Haber", "Nick", ""], ["Kline", "Aaron", ""], ["Daniels", "Jena", ""], ["Fazel", "Azar", ""], ["De", "Titas", ""], ["McCarthy", "Beth", ""], ["Feinstein", "Carl", ""], ["Winograd", "Terry", ""], ["Wall", "Dennis", ""]]}, {"id": "2002.06620", "submitter": "Shuai Zhao", "authors": "Shuai Zhao, Achir Kalra, Cristian Borcea and Yi Chen", "title": "To be Tough or Soft: Measuring the Impact of Counter-Ad-blocking\n  Strategies on User Engagement", "comments": "In Proceedings of The Web Conference 2020 (WWW 20)", "journal-ref": null, "doi": "10.1145/3366423.3380025", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The fast growing ad-blocker usage results in large revenue decrease for\nad-supported online websites. Facing this problem, many online publishers\nchoose either to cooperate with ad-blocker software companies to show\nacceptable ads or to build a wall that requires users to whitelist the site for\ncontent access. However, there is lack of studies on the impact of these two\ncounter-ad-blocking strategies on user behaviors. To address this issue, we\nconduct a randomized field experiment on the website of Forbes Media, a major\nUS media publisher. The ad-blocker users are divided into a treatment group,\nwhich receives the wall strategy, and a control group, which receives the\nacceptable ads strategy. We utilize the difference-in-differences method to\nestimate the causal effects. Our study shows that the wall strategy has an\noverall negative impact on user engagements. However, it has no statistically\nsignificant effect on high-engaged users as they would view the pages no matter\nwhat strategy is used. It has a big impact on low-engaged users, who have no\nloyalty to the site. Our study also shows that revisiting behavior decreases\nover time, but the ratio of session whitelisting increases over time as the\nremaining users have relatively high loyalty and high engagement. The paper\nconcludes with discussions of managerial insights for publishers when\ndetermining counter-ad-blocking strategies.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 17:04:27 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zhao", "Shuai", ""], ["Kalra", "Achir", ""], ["Borcea", "Cristian", ""], ["Chen", "Yi", ""]]}, {"id": "2002.06638", "submitter": "Feng Feng", "authors": "Feng Feng, Shang Kai, Tony Stockman", "title": "Can rhythm be touched? An evaluation of rhythmic sketch performance with\n  augmented multimodal feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although it has been shown that augmented multimodal feedback has a\nfacilitatory effect on motor performance for motor learning and music training,\nthe functionality of haptic feedback combined with other modalities in rhythmic\nmovement tasks has rarely been explored and analysed. In this paper, we\nevaluate the functionality of visual-haptic feedback in a rhythmic sketch task\nby comparing it with other multimodal conditions. Further, we examine the\npossibility of accessing the quality of task execution through kinematic\nanalysis. Based on participants' speed profiles, we investigate the quality of\nmotor control and movement smoothness under different feedback conditions.\nResults revealed better motor control ability with auditory feedback and\nimproved movement smoothness with haptic feedback. Finally, we propose that\nhaptic feedback can be integrated with other modal stimuli for different\ninteraction purposes, and that kinematic analysis can be a complementary\napproach to gesture analysis as well as providing subjective evaluation of\ninteraction performance.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 18:21:12 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Feng", "Feng", ""], ["Kai", "Shang", ""], ["Stockman", "Tony", ""]]}, {"id": "2002.06642", "submitter": "Tab Memmott", "authors": "Tab Memmott, Aziz Ko\\c{c}anao\\u{g}ullar{\\i}, Matthew Lawhead, Daniel\n  Klee, Shiran Dudy, Melanie Fried-Oken, and Barry Oken", "title": "BciPy: Brain-Computer Interface Software in Python", "comments": "24 pages, 15 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are high technological and software demands associated with conducting\nbrain-computer interface (BCI) research. In order to accelerate the development\nand accessibility of BCI, it is worthwhile to focus on open-source and desired\ntooling. Python, a prominent computer language, has emerged as a language of\nchoice for many research and engineering purposes. In this manuscript, we\npresent BciPy, an open-source, Python-based software for conducting BCI\nresearch. It was developed with a focus on restoring communication using\nevent-related potential (ERP) spelling interfaces, however, it may be used for\nother non-spelling and non-ERP BCI paradigms. Major modules in this system\ninclude support for data acquisition, data queries, stimuli presentation,\nsignal processing, signal viewing and modeling, language modeling, task\nbuilding, and a simple Graphical User Interface (GUI).\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 18:36:43 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Memmott", "Tab", ""], ["Ko\u00e7anao\u011fullar\u0131", "Aziz", ""], ["Lawhead", "Matthew", ""], ["Klee", "Daniel", ""], ["Dudy", "Shiran", ""], ["Fried-Oken", "Melanie", ""], ["Oken", "Barry", ""]]}, {"id": "2002.06655", "submitter": "Feng Feng", "authors": "Feng Feng, Tony Stockman", "title": "Concurrent Crossmodal Feedback Assists Target-searching: Displaying\n  Distance Information Through Visual, Auditory and Haptic Modalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans sense of distance depends on the integration of multi sensory cues.\nThe incoming visual luminance, auditory pitch and tactile vibration could all\ncontribute to the ability of distance judgement. This ability can be enhanced\nif the multimodal cues are associated in a congruent manner, a phenomenon has\nbeen referred to as Crossmodal correspondences. In the context of multi-sensory\ninteraction, whether and how such correspondences influence information\nprocessing with continuous motor engagement, particularly for target searching\nactivities, has rarely been investigated. This paper presents an experimental\nuser study to address this question. We built a target-searching application\nbased on a Table-top, displayed the unimodal and Crossmodal distance cues\nconcurrently responding to peoples searching movement, measured task\nperformance through kinematic evaluation. We find that the Crossmodal display\nan audio display lead to improved searching efficiency and accuracy. More\ninterestingly, this improvement is confirmed by kinematic analysis, which also\nunveiled the underlying movement features that could account for this\nimprovement. We discussed how these findings could shed lights on the design of\nassistive technology and of other multi sensory interaction.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 19:39:32 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Feng", "Feng", ""], ["Stockman", "Tony", ""]]}, {"id": "2002.06669", "submitter": "Feng Feng", "authors": "Feng Feng, Puhong Li, Tony Stockman", "title": "Exploring crossmodal perceptual enhancement and integration in a\n  sequence-reproducing task with cognitive priming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging the perceptual phenomenon of crossmoal correspondence has been\nshown to facilitate peoples information processing and improves sensorimotor\nperformance. However for goal-oriented interactive tasks, the question of how\nto enhance the perception of specific Crossmodal information, and how\nCrossmodal information integration takes place during interaction is still\nunclear. The present paper reports two experiments investigating these\nquestions. In the first experiment, a cognitive priming technique was\nintroduced as a way to enhance the perception of two Crossmodal stimuli, in two\nconditions respectively, and their effect on sensory-motor performance was\nobserved. Based on the results, the second experiment combined the two\nCrossmodal stimuli in the same interfaces in a way that their correspondence\ncongruency was mutually exclusive. The same priming techniques was applied as a\nmanipulating factor to observe the Crossmodal integration process. Results\nshowed that first, the Crossmodal integration during interaction can be\nenhanced by the priming technique, but the effect varies according to the\ncombination of Crossmodal stimuli and the types of priming material. Moreover,\npeoples subjective evaluations towards priming types were in contradiction with\ntheir objective behavioural data. Second, when two Crossmodal sequences can be\nperceived simultaneously, results suggested different perceptual weights are\npossessed by different participants, and the perceptual enhancement effect was\nobserved only on the dominant one, the pitch-elevation. Furthermore, the\nCrossmodal integration tended to be integrated in a selective manner without\npriming. These results contribute design implications for multisensory feedback\nand mindless computing.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 20:15:22 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Feng", "Feng", ""], ["Li", "Puhong", ""], ["Stockman", "Tony", ""]]}, {"id": "2002.06910", "submitter": "Angelos Chatzimparmpas", "authors": "Angelos Chatzimparmpas, Rafael M. Martins, Andreas Kerren", "title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "comments": "This manuscript is published in the IEEE Transactions on\n  Visualization and Computer Graphics Journal (IEEE TVCG)", "journal-ref": "IEEE TVCG 2020, 26(8), 2696-2714", "doi": "10.1109/TVCG.2020.2986996", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of\nmultidimensional data has proven to be a popular approach, with successful\napplications in a wide range of domains. Despite their usefulness, t-SNE\nprojections can be hard to interpret or even misleading, which hurts the\ntrustworthiness of the results. Understanding the details of t-SNE itself and\nthe reasons behind specific patterns in its output may be a daunting task,\nespecially for non-experts in dimensionality reduction. In this work, we\npresent t-viSNE, an interactive tool for the visual exploration of t-SNE\nprojections that enables analysts to inspect different aspects of their\naccuracy and meaning, such as the effects of hyper-parameters, distance and\nneighborhood preservation, densities and costs of specific neighborhoods, and\nthe correlations between dimensions and visual patterns. We propose a coherent,\naccessible, and well-integrated collection of different views for the\nvisualization of t-SNE projections. The applicability and usability of t-viSNE\nare demonstrated through hypothetical usage scenarios with real data sets.\nFinally, we present the results of a user study where the tool's effectiveness\nwas evaluated. By bringing to light information that would normally be lost\nafter running t-SNE, we hope to support analysts in using t-SNE and making its\nresults better understandable.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 12:22:34 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 09:37:40 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 05:12:47 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 20:40:37 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Chatzimparmpas", "Angelos", ""], ["Martins", "Rafael M.", ""], ["Kerren", "Andreas", ""]]}, {"id": "2002.07325", "submitter": "Arash Kalatian", "authors": "Arash Kalatian and Bilal Farooq", "title": "Decoding pedestrian and automated vehicle interactions using immersive\n  virtual reality and interpretable deep learning", "comments": null, "journal-ref": "Transportation Research Part C: Emerging Technologies, 124 (2021)\n  pp. 102962", "doi": "10.1016/j.trc.2020.102962", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure pedestrian friendly streets in the era of automated vehicles,\nreassessment of current policies, practices, design, rules and regulations of\nurban areas is of importance. This study investigates pedestrian crossing\nbehaviour, as an important element of urban dynamics that is expected to be\naffected by the presence of automated vehicles. For this purpose, an\ninterpretable machine learning framework is proposed to explore factors\naffecting pedestrians' wait time before crossing mid-block crosswalks in the\npresence of automated vehicles. To collect rich behavioural data, we developed\na dynamic and immersive virtual reality experiment, with 180 participants from\na heterogeneous population in 4 different locations in the Greater Toronto Area\n(GTA). Pedestrian wait time behaviour is then analyzed using a data-driven Cox\nProportional Hazards (CPH) model, in which the linear combination of the\ncovariates is replaced by a flexible non-linear deep neural network. The\nproposed model achieved a 5% improvement in goodness of fit, but more\nimportantly, enabled us to incorporate a richer set of covariates. A game\ntheoretic based interpretability method is used to understand the contribution\nof different covariates to the time pedestrians wait before crossing. Results\nshow that the presence of automated vehicles on roads, wider lane widths, high\ndensity on roads, limited sight distance, and lack of walking habits are the\nmain contributing factors to longer wait times. Our study suggested that, to\nmove towards pedestrian-friendly urban areas, national level educational\nprograms for children, enhanced safety measures for seniors, promotion of\nactive modes of transportation, and revised traffic rules and regulations\nshould be considered.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 01:30:29 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 20:51:44 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Kalatian", "Arash", ""], ["Farooq", "Bilal", ""]]}, {"id": "2002.07541", "submitter": "Neelesh Kumar", "authors": "Neelesh Kumar and Konstantinos P. Michmizos", "title": "Machine Learning for Motor Learning: EEG-based Continuous Assessment of\n  Cognitive Engagement for Adaptive Rehabilitation Robots", "comments": "6 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although cognitive engagement (CE) is crucial for motor learning, it remains\nunderutilized in rehabilitation robots, partly because its assessment currently\nrelies on subjective and gross measurements taken intermittently. Here, we\npropose an end-to-end computational framework that assesses CE in real-time,\nusing electroencephalography (EEG) signals as objective measurements. The\nframework consists of i) a deep convolutional neural network (CNN) that\nextracts task-discriminative spatiotemporal EEG to predict the level of CE for\ntwo classes -- cognitively engaged vs. disengaged; and ii) a novel sliding\nwindow method that predicts continuous levels of CE in real-time. We evaluated\nour framework on 8 subjects using an in-house Go/No-Go experiment that adapted\nits gameplay parameters to induce cognitive fatigue. The proposed CNN had an\naverage leave-one-out accuracy of 88.13\\%. The CE prediction correlated well\nwith a commonly used behavioral metric based on self-reports taken every 5\nminutes ($\\rho$=0.93). Our results objectify CE in real-time and pave the way\nfor using CE as a rehabilitation parameter for tailoring robotic therapy to\neach patient's needs and skills.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 13:13:39 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 16:59:22 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Kumar", "Neelesh", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "2002.07576", "submitter": "Ren\\'e Peinl", "authors": "Ren\\'e Peinl and Tobias Wirth", "title": "Presence in VR experiences -- an empirical cost-benefit-analysis", "comments": "empirical study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Virtual reality (VR) is on the edge of getting a mainstream platform for\ngaming, education and product design. The feeling of being present in the\nvirtual world is influenced by many factors and even more intriguing a single\nnegative influence can destroy the illusion that was created with a lot of\neffort by other measures. Therefore, it is crucial to have a balance between\nthe influencing factors, know the importance of the factors and have a good\nestimation of how much effort it takes to bring each factor to a certain level\nof fidelity. This paper collects influencing factors discussed in literature,\nanalyses the immersion of current off-the-shelf VR-solutions and presents\nresults from an empirical study on efforts and benefits from certain aspects\ninfluencing presence in VR experiences. It turns out, that sometimes delivering\nhigh fidelity is easier to achieve than medium fidelity and for other aspects\nit is worthwhile investing more effort to achieve higher fidelity to improve\npresence a lot.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 15:17:17 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Peinl", "Ren\u00e9", ""], ["Wirth", "Tobias", ""]]}, {"id": "2002.07671", "submitter": "Jouni Helske", "authors": "Jouni Helske, Satu Helske, Matthew Cooper, Anders Ynnerman, and Lonni\n  Besan\\c{c}on", "title": "Can visualization alleviate dichotomous thinking? Effects of visual\n  representations on the cliff effect", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics. 2021;\n  27(8)", "doi": "10.1109/TVCG.2021.3073466", "report-no": null, "categories": "stat.OT cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Common reporting styles for statistical results in scientific articles, such\nas p-values and confidence intervals (CI), have been reported to be prone to\ndichotomous interpretations, especially with respect to the null hypothesis\nsignificance testing framework. For example when the p-value is small enough or\nthe CIs of the mean effects of a studied drug and a placebo are not\noverlapping, scientists tend to claim significant differences while often\ndisregarding the magnitudes and absolute differences in the effect sizes. This\ntype of reasoning has been shown to be potentially harmful to science.\nTechniques relying on the visual estimation of the strength of evidence have\nbeen recommended to reduce such dichotomous interpretations but their\neffectiveness has also been challenged. We ran two experiments on researchers\nwith expertise in statistical analysis to compare several alternative\nrepresentations of confidence intervals and used Bayesian multilevel models to\nestimate the effects of the representation styles on differences in\nresearchers' subjective confidence in the results. We also asked the\nrespondents' opinions and preferences in representation styles. Our results\nsuggest that adding visual information to classic CI representation can\ndecrease the tendency towards dichotomous interpretations - measured as the\n`cliff effect': the sudden drop in confidence around p-value 0.05 - compared\nwith classic CI visualization and textual representation of the CI with\np-values. All data and analyses are publicly available at\nhttps://github.com/helske/statvis.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 17:21:35 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 07:46:04 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 12:15:10 GMT"}, {"version": "v4", "created": "Fri, 28 May 2021 06:26:22 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Helske", "Jouni", ""], ["Helske", "Satu", ""], ["Cooper", "Matthew", ""], ["Ynnerman", "Anders", ""], ["Besan\u00e7on", "Lonni", ""]]}, {"id": "2002.07927", "submitter": "Sashank Santhanam", "authors": "Sashank Santhanam, Alireza Karduni, Samira Shaikh", "title": "Studying the Effects of Cognitive Biases in Evaluation of Conversational\n  Agents", "comments": "Accepted at CHI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans quite frequently interact with conversational agents. The rapid\nadvancement in generative language modeling through neural networks has helped\nadvance the creation of intelligent conversational agents. Researchers\ntypically evaluate the output of their models through crowdsourced judgments,\nbut there are no established best practices for conducting such studies.\nMoreover, it is unclear if cognitive biases in decision-making are affecting\ncrowdsourced workers' judgments when they undertake these tasks. To\ninvestigate, we conducted a between-subjects study with 77 crowdsourced workers\nto understand the role of cognitive biases, specifically anchoring bias, when\nhumans are asked to evaluate the output of conversational agents. Our results\nprovide insight into how best to evaluate conversational agents. We find\nincreased consistency in ratings across two experimental conditions may be a\nresult of anchoring bias. We also determine that external factors such as time\nand prior experience in similar tasks have effects on inter-rater consistency.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 23:52:39 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 16:27:37 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Santhanam", "Sashank", ""], ["Karduni", "Alireza", ""], ["Shaikh", "Samira", ""]]}, {"id": "2002.07950", "submitter": "R. Jordan Crouser", "authors": "Zhengliang Liu, R. Jordan Crouser, and Alvitta Ottley", "title": "Survey on Individual Differences in Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developments in data visualization research have enabled visualization\nsystems to achieve great general usability and application across a variety of\ndomains. These advancements have improved not only people's understanding of\ndata, but also the general understanding of people themselves, and how they\ninteract with visualization systems. In particular, researchers have gradually\ncome to recognize the deficiency of having one-size-fits-all visualization\ninterfaces, as well as the significance of individual differences in the use of\ndata visualization systems. Unfortunately, the absence of comprehensive surveys\nof the existing literature impedes the development of this research. In this\npaper, we review the research perspectives, as well as the personality traits\nand cognitive abilities, visualizations, tasks, and measures investigated in\nthe existing literature. We aim to provide a detailed summary of existing\nscholarship, produce evidence-based reviews, and spur future inquiry.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 01:14:12 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 22:09:44 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Liu", "Zhengliang", ""], ["Crouser", "R. Jordan", ""], ["Ottley", "Alvitta", ""]]}, {"id": "2002.07968", "submitter": "Mahdi Ebnali", "authors": "Mahdi Ebnali, Richard Lamb, Razieh Fathi", "title": "Familiarization tours for first-time users of highly automated cars:\n  Comparing the effects of virtual environments with different levels of\n  interaction fidelity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in aviation and driving has highlighted the importance of training\nas an effective approach to reduce the costs associated with the supervisory\nrole of the human in automated systems. However, only a few studies have\ninvestigated the effect of pre-trip familiarization tours on highly automated\ndriving. In the present study, a driving simulator experiment compared the\neffectiveness of four familiarization groups, control, video, low fidelity\nvirtual reality (VR), and high fidelity VR on automation trust and driving\nperformance in several critical and non-critical transition tasks. The results\nrevealed the positive impact of familiarization tours on trust, takeover, and\nhandback performance at the first time of measurement. Takeover quality only\nimproved when practice was presented in high-fidelity VR. After three times of\nexposure to transition requests, trust and transition performance of all groups\nconverged to those of the high fidelity VR group, demonstrating that: a)\nexperiencing automation failures during the training may reduce costs\nassociated with first failures in highly automated driving; b) the VR tour with\nhigh level of interaction fidelity is superior to other types of\nfamiliarization tour, and c) uneducated and less-educated drivers learn about\nautomation by experiencing it. Knowledge resulting from this research could\nhelp develop cost-effective familiarization tours for highly automated vehicles\nin dealerships and car rental centers.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 02:44:34 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Ebnali", "Mahdi", ""], ["Lamb", "Richard", ""], ["Fathi", "Razieh", ""]]}, {"id": "2002.08034", "submitter": "Yang Liu", "authors": "Yang Liu, Tom Gedeon, Sabrina Caldwell, Shouxu Lin, and Zi Jin", "title": "Emotion Recognition Through Observer's Physiological Signals", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition based on physiological signals is a hot topic and has a\nwide range of applications, like safe driving, health care and creating a\nsecure society. This paper introduces a physiological dataset PAFEW, which is\nobtained using movie clips from the Acted Facial Expressions in the Wild (AFEW)\ndataset as stimuli. To establish a baseline, we use the electrodermal activity\n(EDA) signals in this dataset and extract 6 features from each signal series\ncorresponding to each movie clip to recognize 7 emotions, i.e., Anger, Disgust,\nFear, Happy, Surprise, Sad and Neutral. Overall, 24 observers participated in\nour collection of the training set, including 19 observers who participated in\nonly one session watching 80 videos from 7 classes and 5 observers who\nparticipated multiple times and watched all the videos. All videos were\npresented in an order balanced fashion. Leave-one-observer-out was employed in\nthis classification task. We report the classification accuracy of our\nbaseline, a three-layer network, on this initial training set while training\nwith signals from all participants, only single participants and only multiple\nparticipants. We also investigate the recognition accuracy of grouping the\ndataset by arousal or valence, which achieves 68.66% and 72.72% separately.\nFinally, we provide a two-step network. The first step is to classify the\nfeatures into high/low arousal or positive/negative valence by a network. Then\nthe arousal/valence middle output of the first step is concatenated with\nfeature sets as input of the second step for emotion recognition. We found that\nadding arousal or valence information can help to improve the classification\naccuracy. In addition, the information of positive/negative valence boosts the\nclassification accuracy to a higher degree on this dataset.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 07:23:17 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Liu", "Yang", ""], ["Gedeon", "Tom", ""], ["Caldwell", "Sabrina", ""], ["Lin", "Shouxu", ""], ["Jin", "Zi", ""]]}, {"id": "2002.08139", "submitter": "Nanna Inie", "authors": "Nanna Inie, Peter Dalsgaard", "title": "How Interaction Designers Use Tools to Manage Ideas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a grounded theory-analysis based on a qualitative study\nof professional interaction designers (n=20) with a focus on how they use tools\nto manage design ideas. Idea management can be understood as a subcategory of\nthe field Personal Information Management, which includes the activities around\nthe capture, organization, retrieval, and use of information. Idea management\npertains then to the management and use of ideas as part of creative\nactivities. The paper identifies tool-supported idea management strategies and\nneeds of professional interaction designers, and discusses the context and\nconsequences of these strategies. Based on our analysis, we identify a\nconceptual framework of ten strategies which are supported by tools: saving,\nexternalizing, advancing, exploring, archiving, clustering, extracting,\nbrowsing, verifying, and collaborating. Finally, we discuss how this framework\ncan be used to characterize and analyze existing and novel idea management\ntools.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 12:40:23 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Inie", "Nanna", ""], ["Dalsgaard", "Peter", ""]]}, {"id": "2002.08320", "submitter": "Diane Staheli", "authors": "Dennis Ross, Arunesh Sinha, Diane Staheli, Bill Streilein", "title": "Proceedings of the Artificial Intelligence for Cyber Security (AICS)\n  Workshop 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The workshop will focus on the application of artificial intelligence to\nproblems in cyber security. AICS 2020 emphasis will be on human-machine teaming\nwithin the context of cyber security problems and will specifically explore\ncollaboration between human operators and AI technologies. The workshop will\naddress applicable areas of AI, such as machine learning, game theory, natural\nlanguage processing, knowledge representation, automated and assistive\nreasoning and human machine interactions. Further, cyber security application\nareas with a particular emphasis on the characterization and deployment of\nhuman-machine teaming will be the focus.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 18:12:00 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 22:03:20 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Ross", "Dennis", ""], ["Sinha", "Arunesh", ""], ["Staheli", "Diane", ""], ["Streilein", "Bill", ""]]}, {"id": "2002.08354", "submitter": "Konstantinos Michmizos", "authors": "Neelesh Kumar and Konstantinos P. Michmizos", "title": "Deep Learning of Movement Intent and Reaction Time for EEG-informed\n  Adaptation of Rehabilitation Robots", "comments": "6 pages, 3 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:2002.07541", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mounting evidence suggests that adaptation is a crucial mechanism for\nrehabilitation robots in promoting motor learning. Yet, it is commonly based on\nrobot-derived movement kinematics, which is a rather subjective measurement of\nperformance, especially in the presence of a sensorimotor impairment. Here, we\npropose a deep convolutional neural network (CNN) that uses\nelectroencephalography (EEG) as an objective measurement of two kinematics\ncomponents that are typically used to assess motor learning and thereby\nadaptation: i) the intent to initiate a goal-directed movement, and ii) the\nreaction time (RT) of that movement. We evaluated our CNN on data acquired from\nan in-house experiment where 13 subjects moved a rehabilitation robotic arm in\nfour directions on a plane, in response to visual stimuli. Our CNN achieved\naverage test accuracies of 80.08% and 79.82% in a binary classification of the\nintent (intent vs. no intent) and RT (slow vs. fast), respectively. Our results\ndemonstrate how individual movement components implicated in distinct types of\nmotor learning can be predicted from synchronized EEG data acquired before the\nstart of the movement. Our approach can, therefore, inform robotic adaptation\nin real-time and has the potential to further improve one's ability to perform\nthe rehabilitation task.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 13:20:46 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Kumar", "Neelesh", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "2002.08356", "submitter": "Takanori Fujiwara", "authors": "Rongchen Guo, Takanori Fujiwara, Yiran Li, Kelly M. Lima, Soman Sen,\n  Nam K. Tran, and Kwan-Liu Ma", "title": "Comparative Visual Analytics for Assessing Medical Records with Sequence\n  Embedding", "comments": "This is the author's version of the article that has been accepted in\n  PacificVis 2020 Visualization Meets AI Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning for data-driven diagnosis has been actively studied in\nmedicine to provide better healthcare. Supporting analysis of a patient cohort\nsimilar to a patient under treatment is a key task for clinicians to make\ndecisions with high confidence. However, such analysis is not straightforward\ndue to the characteristics of medical records: high dimensionality,\nirregularity in time, and sparsity. To address this challenge, we introduce a\nmethod for similarity calculation of medical records. Our method employs event\nand sequence embeddings. While we use an autoencoder for the event embedding,\nwe apply its variant with the self-attention mechanism for the sequence\nembedding. Moreover, in order to better handle the irregularity of data, we\nenhance the self-attention mechanism with consideration of different time\nintervals. We have developed a visual analytics system to support comparative\nstudies of patient records. To make a comparison of sequences with different\nlengths easier, our system incorporates a sequence alignment method. Through\nits interactive interface, the user can quickly identify patients of interest\nand conveniently review both the temporal and multivariate aspects of the\npatient records. We demonstrate the effectiveness of our design and system with\ncase studies using a real-world dataset from the neonatal intensive care unit\nof UC Davis.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 19:29:30 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 20:02:15 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Guo", "Rongchen", ""], ["Fujiwara", "Takanori", ""], ["Li", "Yiran", ""], ["Lima", "Kelly M.", ""], ["Sen", "Soman", ""], ["Tran", "Nam K.", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2002.08434", "submitter": "Vikram Shree", "authors": "Vikram Shree, Wei-Lun Chao and Mark Campbell", "title": "Interactive Natural Language-based Person Search", "comments": "8 pages, 12 figures, Published in IEEE Robotics and Automation\n  Letters (RA-L), \"Dataset at:\n  https://github.com/vikshree/QA_PersonSearchLanguageData\" , Video attachment\n  at: https://www.youtube.com/watch?v=Yyxu8uVUREE&feature=youtu.be", "journal-ref": "in IEEE Robotics and Automation Letters, vol. 5, no. 2, pp.\n  1851-1858, April 2020", "doi": "10.1109/LRA.2020.2969921", "report-no": null, "categories": "cs.RO cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the problem of searching people in an unconstrained\nenvironment, with natural language descriptions. Specifically, we study how to\nsystematically design an algorithm to effectively acquire descriptions from\nhumans. An algorithm is proposed by adapting models, used for visual and\nlanguage understanding, to search a person of interest (POI) in a principled\nway, achieving promising results without the need to re-design another\ncomplicated model. We then investigate an iterative question-answering (QA)\nstrategy that enable robots to request additional information about the POI's\nappearance from the user. To this end, we introduce a greedy algorithm to rank\nquestions in terms of their significance, and equip the algorithm with the\ncapability to dynamically adjust the length of human-robot interaction\naccording to model's uncertainty. Our approach is validated not only on\nbenchmark datasets but on a mobile robot, moving in a dynamic and crowded\nenvironment.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 20:42:19 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Shree", "Vikram", ""], ["Chao", "Wei-Lun", ""], ["Campbell", "Mark", ""]]}, {"id": "2002.08455", "submitter": "Mohsen Parisay", "authors": "Mohsen Parisay, Charalambos Poullis, Marta Kersten", "title": "EyeTAP: A Novel Technique using Voice Inputs to Address the Midas Touch\n  Problem for Gaze-based Interactions", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijhcs.2021.102676", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the main challenges of gaze-based interactions is the ability to\ndistinguish normal eye function from a deliberate interaction with the computer\nsystem, commonly referred to as 'Midas touch'. In this paper we propose, EyeTAP\n(Eye tracking point-and-select by Targeted Acoustic Pulse) a hands-free\ninteraction method for point-and-select tasks. We evaluated the prototype in\ntwo separate user studies, each containing two experiments with 33 participants\nand found that EyeTAP is robust even in presence of ambient noise in the audio\ninput signal with tolerance of up to 70 dB, results in a faster movement time,\nand faster task completion time, and has a lower cognitive workload than voice\nrecognition. In addition, EyeTAP has a lower error rate than the dwell-time\nmethod in a ribbon-shaped experiment. These characteristics make it applicable\nfor users for whom physical movements are restricted or not possible due to a\ndisability. Furthermore, EyeTAP has no specific requirements in terms of user\ninterface design and therefore it can be easily integrated into existing\nsystems with minimal modifications. EyeTAP can be regarded as an acceptable\nalternative to address the Midas touch.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 21:36:25 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 13:41:52 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Parisay", "Mohsen", ""], ["Poullis", "Charalambos", ""], ["Kersten", "Marta", ""]]}, {"id": "2002.08657", "submitter": "Yuki Koyama", "authors": "Yuki Koyama and Takeo Igarashi", "title": "Computational Design with Crowds", "comments": "This book chapter was originally published in Computational\n  Interaction edited by Antti Oulasvirta, Per Ola Kristensson, Xiaojun Bi, and\n  Andrew Howes", "journal-ref": "Computational Interaction (Antti Oulasvirta, Per Ola Kristensson,\n  Xiaojun Bi, and Andrew Howes (Eds.)), chapter 6, pages 153-184. Oxford\n  University Press, 2018", "doi": "10.1093/oso/9780198799603.001.0001", "report-no": null, "categories": "cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational design is aimed at supporting or automating design processes\nusing computational techniques. However, some classes of design tasks involve\ncriteria that are difficult to handle only with computers. For example, visual\ndesign tasks seeking to fulfill aesthetic goals are difficult to handle purely\nwith computers. One promising approach is to leverage human computation; that\nis, to incorporate human input into the computation process. Crowdsourcing\nplatforms provide a convenient way to integrate such human computation into a\nworking system.\n  In this chapter, we discuss such computational design with crowds in the\ndomain of parameter tweaking tasks in visual design. Parameter tweaking is\noften performed to maximize the aesthetic quality of designed objects.\nComputational design powered by crowds can solve this maximization problem by\nleveraging human computation. We discuss the opportunities and challenges of\ncomputational design with crowds with two illustrative examples: (1) estimating\nthe objective function (specifically, preference learning from crowds' pairwise\ncomparisons) to facilitate interactive design exploration by a designer and (2)\ndirectly searching for the optimal parameter setting that maximizes the\nobjective function (specifically, crowds-in-the-loop Bayesian optimization).\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 10:40:13 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Koyama", "Yuki", ""], ["Igarashi", "Takeo", ""]]}, {"id": "2002.08777", "submitter": "Jodie Lobana", "authors": "NIklas Kuhl, Jodie Lobana, and Christian Meske", "title": "Do you comply with AI? -- Personalized explanations of learning\n  algorithms and their impact on employees' compliance behavior", "comments": "Fortieth International Conference on Information Systems (ICIS) 2019,\n  Munich, Germany. All Authors contributed equally in shared first authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine Learning algorithms are technological key enablers for artificial\nintelligence (AI). Due to the inherent complexity, these learning algorithms\nrepresent black boxes and are difficult to comprehend, therefore influencing\ncompliance behavior. Hence, compliance with the recommendations of such\nartifacts, which can impact employees' task performance significantly, is still\nsubject to research - and personalization of AI explanations seems to be a\npromising concept in this regard. In our work, we hypothesize that, based on\nvarying backgrounds like training, domain knowledge and demographic\ncharacteristics, individuals have different understandings and hence mental\nmodels about the learning algorithm. Personalization of AI explanations,\nrelated to the individuals' mental models, may thus be an instrument to affect\ncompliance and therefore employee task performance. Our preliminary results\nalready indicate the importance of personalized explanations in industry\nsettings and emphasize the importance of this research endeavor.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 14:55:20 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Kuhl", "NIklas", ""], ["Lobana", "Jodie", ""], ["Meske", "Christian", ""]]}, {"id": "2002.08956", "submitter": "Eduardo Graells-Garrido", "authors": "Eduardo Graells-Garrido and Vanessa Pe\\~na-Araya", "title": "Toward An Interdisciplinary Methodology to Solve New (Old)\n  Transportation Problems", "comments": "6 pages, 5 figures. To be presented at the Data Science for Social\n  Good workshop at The Web Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rising availability of digital traces provides a fertile ground for new\nsolutions to both, new and old problems in cities. Even though a massive data\nset analyzed with Data Science methods may provide a powerful solution to a\nproblem, its adoption by relevant stakeholders is not guaranteed, due to\nadoption blockers such as lack of interpretability and transparency. In this\ncontext, this paper proposes a preliminary methodology toward bridging two\ndisciplines, Data Science and Transportation, to solve urban problems with\nmethods that are suitable for adoption. The methodology is defined by four\nsteps where people from both disciplines go from algorithm and model definition\nto the building of a potentially adoptable solution. As case study, we describe\nhow this methodology was applied to define a model to infer commuting trips\nwith mode of transportation from mobile phone data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 13:09:04 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Graells-Garrido", "Eduardo", ""], ["Pe\u00f1a-Araya", "Vanessa", ""]]}, {"id": "2002.08972", "submitter": "Onur G\\\"unl\\\"u Dr.-Ing.", "authors": "Efe Bozkir and Onur G\\\"unl\\\"u, Wolfgang Fuhl, Rafael F. Schaefer, and\n  Enkelejda Kasneci", "title": "Differential Privacy for Eye Tracking with Temporal Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New generation head-mounted displays, such as VR and AR glasses, are coming\ninto the market with already integrated eye tracking and are expected to enable\nnovel ways of human-computer interaction in many applications. However, since\neye movement properties contain biometric information, privacy concerns have to\nbe handled properly. Privacy-preservation techniques such as differential\nprivacy mechanisms have recently been applied to the eye movement data obtained\nfrom such displays. Standard differential privacy mechanisms; however, are\nvulnerable to temporal correlations in the eye movement features. In this work,\nwe propose a novel transform-coding based differential privacy mechanism to\nfurther adapt it to the statistics of eye movement feature data by comparing\nvarious low-complexity methods. We extent Fourier Perturbation Algorithm, which\nis a differential privacy mechanism, and correct a scaling mistake in its\nproof. Furthermore, we illustrate significant reductions in sample correlations\nin addition to query sensitivities, which provide the best utility-privacy\ntrade-off in the eye tracking literature. Our results show significantly high\nprivacy without loss in classification accuracies as well.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 19:01:34 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 14:04:54 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Bozkir", "Efe", ""], ["G\u00fcnl\u00fc", "Onur", ""], ["Fuhl", "Wolfgang", ""], ["Schaefer", "Rafael F.", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2002.09029", "submitter": "Maximilian Mackeprang", "authors": "Maximilian Mackeprang, Kim Kern, Thomas Hadler, and Claudia\n  M\\\"uller-Birn", "title": "Seeker or Avoider? User Modeling for Inspiration Deployment in\n  Large-Scale Ideation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  People react differently to inspirations shown to them during brainstorming.\nExisting research on large-scale ideation systems has investigated this\nphenomenon through aspects of timing, inspiration similarity and inspiration\nintegration. However, these approaches do not address people's individual\npreferences. In the research presented, we aim to address this lack with\nregards to inspirations. In a first step, we conducted a co-located\nbrainstorming study with 15 participants, which allowed us to differentiate two\ntypes of ideators: Inspiration seekers and inspiration avoiders. These insights\ninformed the study design of the second step, where we propose a user model for\nclassifying people depending on their ideator types, which was translated into\na rule-based and a random forest-based classifier. We evaluated the validity of\nour user model by conducting an online experiment with 380 participants. The\nresults confirmed our proposed ideator types, showing that, while seekers\nbenefit from the availability of inspiration, avoiders were influenced\nnegatively. The random forest classifier enabled us to differentiate people\nwith a 73 \\% accuracy after only three minutes of ideation. These insights show\nthat the proposed ideator types are a promising user model for large-scale\nideation. In future work, this distinction may help to design more personalized\nlarge-scale ideation systems that recommend inspirations adaptively.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 12:24:11 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Mackeprang", "Maximilian", ""], ["Kern", "Kim", ""], ["Hadler", "Thomas", ""], ["M\u00fcller-Birn", "Claudia", ""]]}, {"id": "2002.09054", "submitter": "Lionel Robert", "authors": "Lionel P. Robert, Casey Pierce, Liz Morris, Sangmi Kim, Rasha Alahmad", "title": "Designing Fair AI for Managing Employees in Organizations: A Review,\n  Critique, and Design Agenda", "comments": "66 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organizations are rapidly deploying artificial intelligence (AI) systems to\nmanage their workers. However, AI has been found at times to be unfair to\nworkers. Unfairness toward workers has been associated with decreased worker\neffort and increased worker turnover. To avoid such problems, AI systems must\nbe designed to support fairness and redress instances of unfairness. Despite\nthe attention related to AI unfairness, there has not been a theoretical and\nsystematic approach to developing a design agenda. This paper addresses the\nissue in three ways. First, we introduce the organizational justice theory,\nthree different fairness types (distributive, procedural, interactional), and\nthe frameworks for redressing instances of unfairness (retributive justice,\nrestorative justice). Second, we review the design literature that specifically\nfocuses on issues of AI fairness in organizations. Third, we propose a design\nagenda for AI fairness in organizations that applies each of the fairness types\nto organizational scenarios. Then, the paper concludes with implications for\nfuture research.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 22:52:43 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Robert", "Lionel P.", ""], ["Pierce", "Casey", ""], ["Morris", "Liz", ""], ["Kim", "Sangmi", ""], ["Alahmad", "Rasha", ""]]}, {"id": "2002.09155", "submitter": "Namwoo Kang", "authors": "Soyoung Yoo, Sunghee Lee, Seongsin Kim, Eunji Kim, Hwan Hwangbo,\n  Namwoo Kang", "title": "A Study on Anxiety about Using Robo-taxis: HMI Design for Anxiety Factor\n  Analysis and Anxiety Relief Based on Field Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the approaching commercialization of robo-taxis, various anxiety\nfactors concerning the safety of autonomous vehicles are expected to form a\nlarge barrier against consumers' use of robo-taxi services. The purpose of this\nstudy is to derive the various internal and external factors that contribute to\nthe anxieties of robo-taxi passengers, and to propose a human-machine interface\n(HMI) concept to resolve such factors, by testing robo-taxi services on real,\ncomplex urban roads. In addition, a remote system for safely testing a\nrobo-taxi in complex downtown areas was constructed, by adopting the Wizard of\nOz (WOZ) methodology. From the results of our tests - conducted upon 28\nsubjects in the central area of Seoul - 19 major anxiety factors arising from\nautonomous driving were identified, and seven HMI functions to resolve such\nfactors were designed. The functions were evaluated and their anxiety reduction\neffects verified. In addition, the various design insights required to increase\nthe reliability of robo-taxis were provided through quantitative and\nqualitative analysis of the user experience surveys and interviews.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 07:02:01 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Yoo", "Soyoung", ""], ["Lee", "Sunghee", ""], ["Kim", "Seongsin", ""], ["Kim", "Eunji", ""], ["Hwangbo", "Hwan", ""], ["Kang", "Namwoo", ""]]}, {"id": "2002.09303", "submitter": "Thomas Deselaers", "authors": "Philippe Gervais and Thomas Deselaers and Emre Aksan and Otmar\n  Hilliges", "title": "The DIDI dataset: Digital Ink Diagram data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We are releasing a dataset of diagram drawings with dynamic drawing\ninformation. The dataset aims to foster research in interactive graphical\nsymbolic understanding. The dataset was obtained using a prompted data\ncollection effort.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 16:16:28 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 11:56:21 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gervais", "Philippe", ""], ["Deselaers", "Thomas", ""], ["Aksan", "Emre", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2002.09595", "submitter": "Andr\\'es P\\'aez", "authors": "Andr\\'es P\\'aez", "title": "The Pragmatic Turn in Explainable Artificial Intelligence (XAI)", "comments": null, "journal-ref": "Minds and Machines, 29(3), 441-459, 2019", "doi": "10.1007/s11023-019-09502-w", "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper I argue that the search for explainable models and\ninterpretable decisions in AI must be reformulated in terms of the broader\nproject of offering a pragmatic and naturalistic account of understanding in\nAI. Intuitively, the purpose of providing an explanation of a model or a\ndecision is to make it understandable to its stakeholders. But without a\nprevious grasp of what it means to say that an agent understands a model or a\ndecision, the explanatory strategies will lack a well-defined goal. Aside from\nproviding a clearer objective for XAI, focusing on understanding also allows us\nto relax the factivity condition on explanation, which is impossible to fulfill\nin many machine learning models, and to focus instead on the pragmatic\nconditions that determine the best fit between a model and the methods and\ndevices deployed to understand it. After an examination of the different types\nof understanding discussed in the philosophical and psychological literature, I\nconclude that interpretative or approximation models not only provide the best\nway to achieve the objectual understanding of a machine learning model, but are\nalso a necessary condition to achieve post-hoc interpretability. This\nconclusion is partly based on the shortcomings of the purely functionalist\napproach to post-hoc interpretability that seems to be predominant in most\nrecent literature.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 01:40:01 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["P\u00e1ez", "Andr\u00e9s", ""]]}, {"id": "2002.09925", "submitter": "Yue Jiang", "authors": "Yue Jiang, Wolfgang Stuerzlinger, Matthias Zwicker, Christof Lutteroth", "title": "ORCSolver: An Efficient Solver for Adaptive GUI Layout with\n  OR-Constraints", "comments": "Published at CHI2020", "journal-ref": null, "doi": "10.1145/3313831.3376610", "report-no": null, "categories": "cs.HC cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OR-constrained (ORC) graphical user interface layouts unify conventional\nconstraint-based layouts with flow layouts, which enables the definition of\nflexible layouts that adapt to screens with different sizes, orientations, or\naspect ratios with only a single layout specification. Unfortunately, solving\nORC layouts with current solvers is time-consuming and the needed time\nincreases exponentially with the number of widgets and constraints. To address\nthis challenge, we propose ORCSolver, a novel solving technique for adaptive\nORC layouts, based on a branch-and-bound approach with heuristic preprocessing.\nWe demonstrate that ORCSolver simplifies ORC specifications at runtime and our\napproach can solve ORC layout specifications efficiently at near-interactive\nrates.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 15:46:59 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Jiang", "Yue", ""], ["Stuerzlinger", "Wolfgang", ""], ["Zwicker", "Matthias", ""], ["Lutteroth", "Christof", ""]]}, {"id": "2002.09949", "submitter": "Marie Destandau", "authors": "Marie Destandau, Olivier Corby, Jean-Daniel Fekete and Alain Giboin", "title": "Path Outlines: Browsing Path-Based Summaries of Knowledge Graphs", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Knowledge Graphs have become a ubiquitous technology powering search engines,\nrecommender systems, connected objects, corporate knowledge management and Open\nData. They rely on small units of information named triples that can be\ncombined to form higher level statements across datasets following information\nneeds. But data producers face a problem: reconstituting chains of triples has\na high cognitive cost, which hinders them from gaining meaningful overviews of\ntheir own datasets. We introduce path outlines: conceptual objects\ncharacterizing sequences of triples with descriptive statistics. We interview\n11 data producers to evaluate their interest. We present Path Outlines, a tool\nto browse path-based summaries, based on coordinated views with 2 novel\nvisualisations. We compare Path Outlines with the current baseline technique in\nan experiment with 36 participants. We show that it is 3 times faster, leads to\nbetter task completion, less errors, that participants prefer it, and find\ntasks easier with it.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 17:29:12 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 16:36:13 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 19:45:26 GMT"}, {"version": "v4", "created": "Thu, 8 Oct 2020 20:12:24 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Destandau", "Marie", ""], ["Corby", "Olivier", ""], ["Fekete", "Jean-Daniel", ""], ["Giboin", "Alain", ""]]}, {"id": "2002.10096", "submitter": "Marian D\\\"ork", "authors": "Philipp Geuder, Marie Claire Leidinger, Martin von Lupin, Marian\n  D\\\"ork, Tobias Schr\\\"oder", "title": "Emosaic: Visualizing Affective Content of Text at Varying Granularity", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Emosaic, a tool for visualizing the emotional tone of\ntext documents, considering multiple dimensions of emotion and varying levels\nof semantic granularity. Emosaic is grounded in psychological research on the\nrelationship between language, affect, and color perception. We capitalize on\nan established three-dimensional model of human emotion: valence (good, nice\nvs. bad, awful), arousal (calm, passive vs. exciting, active) and dominance\n(weak, controlled vs. strong, in control). Previously, multi-dimensional models\nof emotion have been used rarely in visualizations of textual data, due to the\nperceptual challenges involved. Furthermore, until recently most text\nvisualizations remained at a high level, precluding closer engagement with the\ndeep semantic content of the text. Informed by empirical studies, we introduce\na color mapping that translates any point in three-dimensional affective space\ninto a unique color. Emosaic uses affective dictionaries of words annotated\nwith the three emotional parameters of the valence-arousal-dominance model to\nextract emotional meanings from texts and then assigns to them corresponding\ncolor parameters of the hue-saturation-brightness color space. This approach of\nmapping emotion to color is aimed at helping readers to more easily grasp the\nemotional tone of the text. Several features of Emosaic allow readers to\ninteractively explore the affective content of the text in more detail; e.g.,\nin aggregated form as histograms, in sequential form following the order of\ntext, and in detail embedded into the text display itself. Interaction\ntechniques have been included to allow for filtering and navigating of text and\nvisualizations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 07:25:01 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Geuder", "Philipp", ""], ["Leidinger", "Marie Claire", ""], ["von Lupin", "Martin", ""], ["D\u00f6rk", "Marian", ""], ["Schr\u00f6der", "Tobias", ""]]}, {"id": "2002.10119", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez and Javier\n  Ortega-Garcia", "title": "DeepSign: Deep On-Line Signature Verification", "comments": null, "journal-ref": "IEEE Transactions on Biometrics, Behavior, and Identity Science,\n  2021", "doi": "10.1109/TBIOM.2021.3054533", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning has become a breathtaking technology in the last years,\novercoming traditional handcrafted approaches and even humans for many\ndifferent tasks. However, in some tasks, such as the verification of\nhandwritten signatures, the amount of publicly available data is scarce, what\nmakes difficult to test the real limits of deep learning. In addition to the\nlack of public data, it is not easy to evaluate the improvements of novel\nproposed approaches as different databases and experimental protocols are\nusually considered.\n  The main contributions of this study are: i) we provide an in-depth analysis\nof state-of-the-art deep learning approaches for on-line signature\nverification, ii) we present and describe the new DeepSignDB on-line\nhandwritten signature biometric public database, iii) we propose a standard\nexperimental protocol and benchmark to be used for the research community in\norder to perform a fair comparison of novel approaches with the state of the\nart, and iv) we adapt and evaluate our recent deep learning approach named\nTime-Aligned Recurrent Neural Networks (TA-RNNs) for the task of on-line\nhandwritten signature verification. This approach combines the potential of\nDynamic Time Warping and Recurrent Neural Networks to train more robust systems\nagainst forgeries. Our proposed TA-RNN system outperforms the state of the art,\nachieving results even below 2.0% EER when considering skilled forgery\nimpostors and just one training signature per user.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 08:53:11 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 08:44:11 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 15:53:57 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Tolosana", "Ruben", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""], ["Ortega-Garcia", "Javier", ""]]}, {"id": "2002.10313", "submitter": "Maria Karyda", "authors": "Maria Karyda, Merja Ry\\\"oppy, Jacob Buur and Andr\\'es Lucero", "title": "Imagining Data-Objects for Reflective Self-Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While self-tracking data is typically captured real-time in a lived\nexperience, the data is often stored in a manner detached from the context\nwhere it belongs. Research has shown that there is a potential to enhance\npeople's lived experiences with data-objects (artifacts representing\ncontextually relevant data), for individual and collective reflections through\na physical portrayal of data. This paper expands that research by studying how\nto design contextually relevant data-objects based on people's needs. We\nconducted a participatory research project with five households using object\ntheater as a core method to encourage participants to speculate upon\ncombinations of meaningful objects and personal data archives. In this paper,\nwe detail three aspects that seem relevant for designing data-objects: social\nsharing, contextual ambiguity and interaction with the body. We show how an\nexperience-centric view on data-objects can contribute with the contextual,\nsocial and bodily interplay between people, data and objects.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 15:42:28 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Karyda", "Maria", ""], ["Ry\u00f6ppy", "Merja", ""], ["Buur", "Jacob", ""], ["Lucero", "Andr\u00e9s", ""]]}, {"id": "2002.10530", "submitter": "Lucas Layman", "authors": "William Roden, Lucas Layman", "title": "Cry Wolf: Toward an Experimentation Platform and Dataset for Human\n  Factors in Cyber Security Analysis", "comments": "The definitive Version of Record was published in the 2020 ACM\n  Southeast Conference (ACMSE 2020), April 2--4, 2020, Tampa, FL, USA", "journal-ref": null, "doi": "10.1145/3374135.3385301", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer network defense is a partnership between automated systems and human\ncyber security analysts. The system behaviors, for example raising a high\nproportion of false alarms, likely impact cyber analyst performance.\nExperimentation in the analyst-system domain is challenging due to lack of\naccess to security experts, the usability of attack datasets, and the training\nrequired to use security analysis tools. This paper describes Cry Wolf, an open\nsource web application for user studies of cyber security analysis tasks. This\npaper also provides an open-access dataset of 73 true and false Intrusion\nDetection System (IDS) alarms derived from real-world examples of \"impossible\ntravel\" scenarios. Cry Wolf and the impossible travel dataset were used in an\nexperiment on the impact of IDS false alarm rate on analysts' abilities to\ncorrectly classify IDS alerts as true or false alarms. Results from that\nexperiment are used to evaluate the quality of the dataset using difficulty and\ndiscrimination index measures drawn from classical test theory. Many alerts in\nthe dataset provide good discrimination for participants' overall task\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 20:38:23 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Roden", "William", ""], ["Layman", "Lucas", ""]]}, {"id": "2002.10586", "submitter": "Amir Yazdani", "authors": "Amir Yazdani, Roya Sabbagh Novin, Andrew Merryweather, and Tucker\n  Hermans", "title": "Is The Leader Robot an Adequate Sensor for Posture Estimation and\n  Ergonomic Assessment of A Human Teleoperator?", "comments": "Submitted to IEEE CASE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ergonomic assessment of human posture plays a vital role in understanding\nwork-related safety and health. Current posture estimation approaches face\nocclusion challenges in teleoperation and physical human-robot interaction. We\ninvestigate if the leader robot is an adequate sensor for posture estimation in\nteleoperation and we introduce a new probabilistic approach that relies solely\non the trajectory of the leader robot for generating observations. We model the\nhuman using a redundant, partially-observable dynamical system and we infer the\nposture using a standard particle filter. We compare our approach with postures\nfrom a commercial motion capture system and also two least-squares optimization\napproaches for human inverse kinematics. The results reveal that the proposed\napproach successfully estimates human postures and ergonomic risk scores\ncomparable to those estimates from gold-standard motion capture.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 23:20:31 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 22:51:18 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 01:35:29 GMT"}, {"version": "v4", "created": "Fri, 19 Mar 2021 16:59:13 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Yazdani", "Amir", ""], ["Novin", "Roya Sabbagh", ""], ["Merryweather", "Andrew", ""], ["Hermans", "Tucker", ""]]}, {"id": "2002.10593", "submitter": "Jim Samuel", "authors": "Jim Samuel, Richard Holowczak and Alexander Pelaez", "title": "The Effects Of Technology Driven Information Categories On Performance\n  In Electronic Trading Markets", "comments": null, "journal-ref": "Journal of Information Technology Management, 2017, V 28,1-2", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic trading markets have evolved rapidly with continued adoption of\nnew technologies and growing in-formation acquisition and processing\ncapabilities. Traditional perspectives on trading performance adopted a\nmono-lithic view of information. Past research and practitioner heuristics\nposit that adopting new technologies and incorpo-rating more information should\nincrease price efficiency and trading performance uniformity. However, along\nwith technological change, information dynamics have evolved significantly\nresulting in immense growth in data volumes, and increased complexity of\ninformation categories. The present research explores behavioral trading\nperformance under varying information category conditions and argues that\nunfettered technological developments and information consumption will not\nnecessarily lead to consistent improvement in uniformity of trading\nperformance. In this study, we employ an artificial stock market based economic\nexperiment to examine the role of technol-ogy driven information categories in\ninfluencing trading decisions in electronic markets. Financial electronic\nmarkets are used as an information-rich mature markets representation to\nanalyze information category driven trading perfor-mance. The results show that\na variation of information categories can influence trading performance. The\nfindings provide a basis to better understand behavioral phenomena in\nelectronic markets and can be used to explain anomalies as well as to manage\ntrading performance in electronic markets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 23:42:24 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Samuel", "Jim", ""], ["Holowczak", "Richard", ""], ["Pelaez", "Alexander", ""]]}, {"id": "2002.10594", "submitter": "Daniel Freer", "authors": "Daniel Freer, Yao Guo, Fani Deligianni, Guang-Zhong Yang", "title": "On-Orbit Operations Simulator for Workload Measurement during\n  Telerobotic Training", "comments": "8 pages, 8 figures, 2 tables. Revision for RA-L (with IROS option)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training for telerobotic systems often makes heavy use of simulated\nplatforms, which ensure safe operation during the learning process. Outer space\nis one domain in which such a simulated training platform would be useful, as\nOn-Orbit Operations (O3) can be costly, inefficient, or even dangerous if not\nperformed properly. In this paper, we present a new telerobotic training\nsimulator for the Canadarm2 on the International Space Station (ISS), which is\nable to modulate workload through the addition of confounding factors such as\nlatency, obstacles, and time pressure. In addition, multimodal physiological\ndata is collected from subjects as they perform a task from the simulator under\nthese different conditions. As most current workload measures are subjective,\nwe analyse objective measures from the simulator and EEG data that can provide\na reliable measure. ANOVA of task data revealed which simulator-based\nperformance measures could predict the presence of latency and time pressure.\nFurthermore, EEG classification using a Riemannian classifier and\nLeave-One-Subject-Out cross-validation showed promising classification\nperformance and allowed for comparison of different channel configurations and\npreprocessing methods. Additionally, Riemannian distance and beta power of EEG\ndata were investigated as potential cross-trial and continuous workload\nmeasures.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 23:44:17 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 15:09:06 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Freer", "Daniel", ""], ["Guo", "Yao", ""], ["Deligianni", "Fani", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "2002.10696", "submitter": "Markku Suomalainen", "authors": "Israel Becerra, Markku Suomalainen, Eliezer Lozano, Katherine J.\n  Mimnaugh, Rafael Murrieta-Cid and Steven M. LaValle", "title": "Human Perception-Optimized Planning for Comfortable VR-Based\n  Telepresence", "comments": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "journal-ref": null, "doi": "10.1109/LRA.2020.3015191", "report-no": null, "categories": "cs.RO cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an emerging motion planning problem by considering a\nhuman that is immersed into the viewing perspective of a remote robot. The\nchallenge is to make the experience both effective (such as delivering a sense\nof presence) and comfortable (such as avoiding adverse sickness symptoms,\nincluding nausea). We refer to this challenging new area as human\nperception-optimized planning and propose a general multiobjective optimization\nframework that can be instantiated in many envisioned scenarios. We then\nconsider a specific VR telepresence task as a case of human\nperception-optimized planning, in which we simulate a robot that sends 360\nvideo to a remote user to be viewed through a head-mounted display. In this\nparticular task, we plan trajectories that minimize VR sickness (and thereby\nmaximize comfort). An A* type method is used to create a Pareto-optimal\ncollection of piecewise linear trajectories while taking into account criteria\nthat improve comfort. We conducted a study with human subjects touring a\nvirtual museum, in which paths computed by our algorithm are compared against a\nreference RRT-based trajectory. Generally, users suffered less from VR sickness\nand preferred the paths created by the presented algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 06:48:43 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 06:09:48 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Becerra", "Israel", ""], ["Suomalainen", "Markku", ""], ["Lozano", "Eliezer", ""], ["Mimnaugh", "Katherine J.", ""], ["Murrieta-Cid", "Rafael", ""], ["LaValle", "Steven M.", ""]]}, {"id": "2002.10702", "submitter": "Peitong Duan", "authors": "Peitong Duan, Casimir Wierzynski, Lama Nachman", "title": "Optimizing User Interface Layouts via Gradient Descent", "comments": null, "journal-ref": null, "doi": "10.1145/3313831.3376589", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating parts of the user interface (UI) design process has been a\nlongstanding challenge. We present an automated technique for optimizing the\nlayouts of mobile UIs. Our method uses gradient descent on a neural network\nmodel of task performance with respect to the model's inputs to make layout\nmodifications that result in improved predicted error rates and task completion\ntimes. We start by extending prior work on neural network based performance\nprediction to 2-dimensional mobile UIs with an expanded interaction space. We\nthen apply our method to two UIs, including one that the model had not been\ntrained on, to discover layout alternatives with significantly improved\npredicted performance. Finally, we confirm these predictions experimentally,\nshowing improvements up to 9.2 percent in the optimized layouts. This\ndemonstrates the algorithm's efficacy in improving the task performance of a\nlayout, and its ability to generalize and improve layouts of new interfaces.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 07:17:27 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Duan", "Peitong", ""], ["Wierzynski", "Casimir", ""], ["Nachman", "Lama", ""]]}, {"id": "2002.10860", "submitter": "Tomoichi Takahashi", "authors": "Tomoichi Takahashi", "title": "Toward dynamical crowd control to prevent hazardous situations", "comments": "3pages, 7 figures, submitted to PED2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common for large crowds to gather to attend games, exhibitions,\npolitical rallies, and other events. Thus, careful designs and operational\nplans are made to ensure the safe, secure, and efficient movement of people in\nthese crowded environments. However, the congestion created by large crowds has\nresulted in hazardous incidents across the world. Developments in information\ntechnology can provide new means to disseminate public information, thus\nchanging human behavior in situations of danger and duress. In this paper, we\npropose a crowd control and evacuation guidance management system using digital\npromotional signage to demonstrate the effects of crowd control via\nsimulations.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 09:52:05 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Takahashi", "Tomoichi", ""]]}, {"id": "2002.10887", "submitter": "Jonas Oppenlaender", "authors": "Jonas Oppenlaender, Aku Visuri, Kristy Milland, Panos Ipeirotis, Simo\n  Hosio", "title": "What do crowd workers think about creative work?", "comments": "4 pages, accepted at the Workshop on Worker-centered Design (CHI\n  '20). arXiv admin note: text overlap with arXiv:2001.06798", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing platforms are a powerful and convenient means for recruiting\nparticipants in online studies and collecting data from the crowd. As\ninformation work is being more and more automated by Machine Learning\nalgorithms, creativity $-$ that is, a human's ability for divergent and\nconvergent thinking $-$ will play an increasingly important role on online\ncrowdsourcing platforms. However, we lack insights into what crowd workers\nthink about creative work. In studies in Human-Computer Interaction (HCI), the\nability and willingness of the crowd to participate in creative work seems to\nbe largely unquestioned. Insights into the workers' perspective are rare, but\nimportant, as they may inform the design of studies with higher validity. Given\nthat creativity will play an increasingly important role in crowdsourcing, it\nis imperative to develop an understanding of how workers perceive creative\nwork. In this paper, we summarize our recent worker-centered study of creative\nwork on two general-purpose crowdsourcing platforms (Amazon Mechanical Turk and\nProlific). Our study illuminates what creative work is like for crowd workers\non these two crowdsourcing platforms. The work identifies several archetypal\ntypes of workers with different attitudes towards creative work, and discusses\ncommon pitfalls with creative work on crowdsourcing platforms.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 08:56:51 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Oppenlaender", "Jonas", ""], ["Visuri", "Aku", ""], ["Milland", "Kristy", ""], ["Ipeirotis", "Panos", ""], ["Hosio", "Simo", ""]]}, {"id": "2002.10905", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Yao Rong, Enkelejda Kasneci", "title": "Fully Convolutional Neural Networks for Raw Eye Tracking Data\n  Segmentation, Generation, and Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use fully convolutional neural networks for the semantic\nsegmentation of eye tracking data. We also use these networks for\nreconstruction, and in conjunction with a variational auto-encoder to generate\neye movement data. The first improvement of our approach is that no input\nwindow is necessary, due to the use of fully convolutional networks and\ntherefore any input size can be processed directly. The second improvement is\nthat the used and generated data is raw eye tracking data (position X, Y and\ntime) without preprocessing. This is achieved by pre-initializing the filters\nin the first layer and by building the input tensor along the z axis. We\nevaluated our approach on three publicly available datasets and compare the\nresults to the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 06:57:09 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 07:13:46 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 12:22:08 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Rong", "Yao", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2002.10971", "submitter": "Md Navid Akbar", "authors": "Pushyami Kaveti, Md Navid Akbar", "title": "Role of Intrinsic Motivation in User Interface Design to Enhance Worker\n  Performance in Amazon MTurk", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biologists and scientists have been tackling the problem of marine life\nmonitoring and fish stock estimation for many years now. Efforts are now\ndirected to move towards non-intrusive methods, by utilizing specially designed\nunderwater robots to collect images of the marine population. Training machine\nlearning algorithms on the images collected, we can now estimate the\npopulation. This in turn helps to impose regulations to control overfishing. To\ntrain these models, however, we need annotated images. Annotation of large sets\nof images collected over a decade is quite challenging. Hence, we resort to\nAmazon Mechanical Turk (MTurk), a crowdsourcing platform, for the image\nannotation task. Although it is fast to get work done in MTurk, the work\nobtained is often of poor quality. This work aims to understand the human\nfactors in designing Human Intelligence Tasks (HITs), from the perspective of\nthe Self-Determination Theory. Applying elements from the theory, we design an\nHIT to increase the competence and motivation of the workers. Within our\nexperimental framework, we find that the new interface significantly improves\nthe accuracy of worker performance.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 15:34:50 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kaveti", "Pushyami", ""], ["Akbar", "Md Navid", ""]]}, {"id": "2002.10998", "submitter": "Takanori Fujiwara", "authors": "Yiran Li, Takanori Fujiwara, Yong K. Choi, Katherine K. Kim, Kwan-Liu\n  Ma", "title": "A Visual Analytics System for Multi-model Comparison on Clinical Data\n  Predictions", "comments": "This is the author's version of the article that has been accepted to\n  PacificVis 2020 Visualization Meets AI Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing trend of applying machine learning methods to medical\ndatasets in order to predict patients' future status. Although some of these\nmethods achieve high performance, challenges still exist in comparing and\nevaluating different models through their interpretable information. Such\nanalytics can help clinicians improve evidence-based medical decision making.\nIn this work, we develop a visual analytics system that compares multiple\nmodels' prediction criteria and evaluates their consistency. With our system,\nusers can generate knowledge on different models' inner criteria and how\nconfidently we can rely on each model's prediction for a certain patient.\nThrough a case study of a publicly available clinical dataset, we demonstrate\nthe effectiveness of our visual analytics system to assist clinicians and\nresearchers in comparing and quantitatively evaluating different machine\nlearning methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 20:33:04 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 20:08:20 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Li", "Yiran", ""], ["Fujiwara", "Takanori", ""], ["Choi", "Yong K.", ""], ["Kim", "Katherine K.", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2002.11284", "submitter": "Paula Lago", "authors": "Paula Lago, Moe Matsuki, Sozo Inoue", "title": "Achieving Single-Sensor Complex Activity Recognition from Multi-Sensor\n  Training Data", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we propose a method for single sensor-based activity\nrecognition, trained with data from multiple sensors. There is no doubt that\nthe performance of complex activity recognition systems increases when we use\nenough sensors with sufficient quality, however using such rich sensors may not\nbe feasible in real-life situations for various reasons such as user comfort,\nprivacy, battery-preservation, and/or costs. In many cases, only one device\nsuch as a smartphone is available, and it is challenging to achieve high\naccuracy with a single sensor, more so for complex activities. Our method\ncombines representation learning with feature mapping to leverage multiple\nsensor information made available during training while using a single sensor\nduring testing or in real usage. Our results show that the proposed approach\ncan improve the F1-score of the complex activity recognition by up to 17\\%\ncompared to that in training while utilizing the same sensor data in a new user\nscenario.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 03:42:35 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Lago", "Paula", ""], ["Matsuki", "Moe", ""], ["Inoue", "Sozo", ""]]}, {"id": "2002.11458", "submitter": "Pablo Barros", "authors": "Pablo Barros, Alessandra Sciutti, Anne C. Bloem, Inge M. Hootsmans,\n  Lena M. Opheij, Romain H.A. Toebosch, Emilia Barakova", "title": "It's Food Fight! Introducing the Chef's Hat Card Game for\n  Affective-Aware HRI", "comments": "Accepted by the Workshop on Exploring Creative Content in Social\n  Robotics at HRI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Emotional expressions and their changes during an interaction affect heavily\nhow we perceive and behave towards other persons. To design an HRI scenario\nthat makes possible to observe, understand, and model affective interactions\nand generate the appropriate responses or initiations of a robot is a very\nchallenging task. In this paper, we report our efforts in designing such a\nscenario, and to propose a modeling strategy of affective interaction by\nartificial intelligence deployed in autonomous robots. Overall, we present a\nnovel HRI game scenario that was designed to comply with the specific\nrequirements that will allow us to develop the next wave of affective-aware\nsocial robots that provide adequate emotional responses.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 12:27:15 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 11:30:10 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Barros", "Pablo", ""], ["Sciutti", "Alessandra", ""], ["Bloem", "Anne C.", ""], ["Hootsmans", "Inge M.", ""], ["Opheij", "Lena M.", ""], ["Toebosch", "Romain H. A.", ""], ["Barakova", "Emilia", ""]]}, {"id": "2002.11596", "submitter": "Florian Fischer", "authors": "Florian Fischer, Arthur Fleig, Markus Klar, Lars Gruene, Joerg Mueller", "title": "An Optimal Control Model of Mouse Pointing Using the LQR", "comments": "13 pages (including appendix), 11 figures, one table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the Linear-Quadratic Regulator (LQR) to model\nmovement of the mouse pointer. We propose a model in which users are assumed to\nbehave optimally with respect to a certain cost function. Users try to minimize\nthe distance of the mouse pointer to the target smoothly and with minimal\neffort, by simultaneously minimizing the jerk of the movement. We identify\nparameters of our model from a dataset of reciprocal pointing with the mouse.\nWe compare our model to the classical minimum-jerk and second-order lag models\non data from 12 users with a total of 7702 movements. Our results show that our\napproach explains the data significantly better than either of these previous\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 16:28:16 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Fischer", "Florian", ""], ["Fleig", "Arthur", ""], ["Klar", "Markus", ""], ["Gruene", "Lars", ""], ["Mueller", "Joerg", ""]]}, {"id": "2002.11697", "submitter": "Tathagata Chakraborti", "authors": "Tathagata Chakraborti, Sarath Sreedharan, Subbarao Kambhampati", "title": "The Emerging Landscape of Explainable AI Planning and Decision Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a comprehensive outline of the different threads of\nwork in Explainable AI Planning (XAIP) that has emerged as a focus area in the\nlast couple of years and contrast that with earlier efforts in the field in\nterms of techniques, target users, and delivery mechanisms. We hope that the\nsurvey will provide guidance to new researchers in automated planning towards\nthe role of explanations in the effective design of human-in-the-loop systems,\nas well as provide the established researcher with some perspective on the\nevolution of the exciting world of explainable planning.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 18:40:47 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Chakraborti", "Tathagata", ""], ["Sreedharan", "Sarath", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "2002.11701", "submitter": "Siddharth Biswal", "authors": "Siddharth Biswal, Cao Xiao, Lucas M. Glass, M. Brandon Westover, and\n  Jimeng Sun", "title": "CLARA: Clinical Report Auto-completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating clinical reports from raw recordings such as X-rays and\nelectroencephalogram (EEG) is an essential and routine task for doctors.\nHowever, it is often time-consuming to write accurate and detailed reports.\nMost existing methods try to generate the whole reports from the raw input with\nlimited success because 1) generated reports often contain errors that need\nmanual review and correction, 2) it does not save time when doctors want to\nwrite additional information into the report, and 3) the generated reports are\nnot customized based on individual doctors' preference. We propose {\\it\nCL}inic{\\it A}l {\\it R}eport {\\it A}uto-completion (CLARA), an interactive\nmethod that generates reports in a sentence by sentence fashion based on\ndoctors' anchor words and partially completed sentences. CLARA searches for\nmost relevant sentences from existing reports as the template for the current\nreport. The retrieved sentences are sequentially modified by combining with the\ninput feature representations to create the final report. In our experimental\nevaluation, CLARA achieved 0.393 CIDEr and 0.248 BLEU-4 on X-ray reports and\n0.482 CIDEr and 0.491 BLEU-4 for EEG reports for sentence-level generation,\nwhich is up to 35% improvement over the best baseline. Also via our qualitative\nevaluation, CLARA is shown to produce reports which have a significantly higher\nlevel of approval by doctors in a user study (3.74 out of 5 for CLARA vs 2.52\nout of 5 for the baseline).\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 18:45:00 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 13:32:52 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Biswal", "Siddharth", ""], ["Xiao", "Cao", ""], ["Glass", "Lucas M.", ""], ["Westover", "M. Brandon", ""], ["Sun", "Jimeng", ""]]}, {"id": "2002.11754", "submitter": "Matthias Hohmann", "authors": "Matthias R. Hohmann, Lisa Konieczny, Michelle Hackl, Brian Wirth,\n  Talha Zaman, Raffi Enficiaud, Moritz Grosse-Wentrup, Bernhard Sch\\\"olkopf", "title": "MYND: Unsupervised Evaluation of Novel BCI Control Strategies on\n  Consumer Hardware", "comments": "9 pages, 5 figures. Submitted to PNAS. Minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurophysiological studies are typically conducted in laboratories with\nlimited ecological validity, scalability, and generalizability of findings.\nThis is a significant challenge for the development of brain-computer\ninterfaces (BCIs), which ultimately need to function in unsupervised settings\non consumer-grade hardware. We introduce MYND: A framework that couples\nconsumer-grade recording hardware with an easy-to-use application for the\nunsupervised evaluation of BCI control strategies. Subjects are guided through\nexperiment selection, hardware fitting, recording, and data upload in order to\nself-administer multi-day studies that include neurophysiological recordings\nand questionnaires. As a use case, we evaluate two BCI control strategies\n(\"Positive memories\" and \"Music imagery\") in a realistic scenario by combining\nMYND with a four-channel electroencephalogram (EEG). Thirty subjects recorded\n70.4 hours of EEG data with the system at home. The median headset fitting time\nwas 25.9 seconds, and a median signal quality of 90.2% was retained during\nrecordings.Neural activity in both control strategies could be decoded with an\naverage offline accuracy of 68.5% and 64.0% across all days. The repeated\nunsupervised execution of the same strategy affected performance, which could\nbe tackled by implementing feedback to let subjects switch between strategies\nor devise new strategies with the platform.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 19:24:24 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 16:48:25 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hohmann", "Matthias R.", ""], ["Konieczny", "Lisa", ""], ["Hackl", "Michelle", ""], ["Wirth", "Brian", ""], ["Zaman", "Talha", ""], ["Enficiaud", "Raffi", ""], ["Grosse-Wentrup", "Moritz", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2002.11834", "submitter": "Austin Hounsel", "authors": "Agnieszka Dutkowska-Zuk, Austin Hounsel, Andre Xiong, Molly Roberts,\n  Brandon Stewart, Marshini Chetty, Nick Feamster", "title": "Understanding How and Why University Students Use Virtual Private\n  Networks", "comments": "Interview guide, interview summary codebook, survey questions, and\n  additional survey figures included in the appendix document", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how and why university students chose and use VPNs, and whether they\nare aware of the security and privacy risks that VPNs pose. To answer these\nquestions, we conducted 32 in-person interviews and a survey with 349\nrespondents, all university students in the United States. We find students are\nmostly concerned with access to content and privacy concerns were often\nsecondary. They made tradeoffs to achieve a particular goal, such as using a\nfree commercial VPN that may collect their online activities to access an\nonline service in a geographic area. Many users expected that their VPNs were\ncollecting data about them, although they did not understand how VPNs work. We\nconclude with a discussion of ways to help users make choices about VPNs.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 23:02:57 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 20:20:11 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 21:58:18 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2021 23:07:30 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Dutkowska-Zuk", "Agnieszka", ""], ["Hounsel", "Austin", ""], ["Xiong", "Andre", ""], ["Roberts", "Molly", ""], ["Stewart", "Brandon", ""], ["Chetty", "Marshini", ""], ["Feamster", "Nick", ""]]}, {"id": "2002.12007", "submitter": "Sofia Seinfeld", "authors": "Sofia Seinfeld, Tiare Feuchtner, Johannes Pinzek, J\\\"org M\\\"uller", "title": "Impact of Information Placement and User Representations in VR on\n  Performance and Embodiment", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2020.3021342", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human sensory processing is sensitive to the proximity of stimuli to the\nbody. It is therefore plausible that these perceptual mechanisms also modulate\nthe detectability of content in VR, depending on its location. We evaluate this\nin a user study and further explore the impact of the user's representation\nduring interaction. We also analyze how embodiment and motor performance are\ninfluenced by these factors. In a dual-task paradigm, participants executed a\nmotor task, either through virtual hands, virtual controllers, or a keyboard.\nSimultaneously, they detected visual stimuli appearing in different locations.\nWe found that, while actively performing a motor task in the virtual\nenvironment, performance in detecting additional visual stimuli is higher when\npresented near the user's body. This effect is independent of how the user is\nrepresented and only occurs when the user is also engaged in a secondary task.\nWe further found improved motor performance and increased embodiment when\ninteracting through virtual tools and hands in VR, compared to interacting with\na keyboard. This study contributes to better understanding the detectability of\nvisual content in VR, depending on its location in the virtual environment, as\nwell as the impact of different user representations on information processing,\nembodiment, and motor performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 09:59:11 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 19:48:20 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 15:59:30 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Seinfeld", "Sofia", ""], ["Feuchtner", "Tiare", ""], ["Pinzek", "Johannes", ""], ["M\u00fcller", "J\u00f6rg", ""]]}, {"id": "2002.12020", "submitter": "Sofia Seinfeld", "authors": "Sofia Seinfeld and J\\\"org M\\\"uller", "title": "Impact of Visuomotor Feedback on the Embodiment of Virtual Hands\n  Detached from the Body", "comments": null, "journal-ref": "Sci Rep 10, 22427 (2020)", "doi": "10.1038/s41598-020-79255-5", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has been shown that mere observation of body discontinuity leads to\ndiminished body ownership. However, the impact of body discontinuity has mainly\nbeen investigated in conditions where participants observe a collocated static\nvirtual body from a first-person perspective. This study explores the influence\nof body discountinuity on the sense of embodiment, when rich visuomotor\ncorrelations between a real and an artificial virtual body are established. In\ntwo experiments, we evaluated body ownership and motor performance, when\nparticipants interacted in virtual reality either using virtual hands connected\nor disconnected from a body. We found that even under the presence of congruent\nvisuomotor feedback, mere observation of body discontinuity resulted in\ndiminished embodiment. Contradictory evidence was found in relation to motor\nperformance, where further research is needed to understand the role of visual\nbody discontinuity in motor tasks. Preliminary findings on physiological\nreactions to a threat were also assessed, indicating that body visual\ndiscontinuity does not differently impact threat-related skin conductance\nresponses. The present results are in accordance with past evidence showing\nthat body discontinuity negatively impacts embodiment. However, further\nresearch is needed to understand the influence of visuomotor feedback and body\nmorphological congruency on motor performance and threat-related physiological\nreactions.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 10:30:32 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 19:38:28 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 12:19:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Seinfeld", "Sofia", ""], ["M\u00fcller", "J\u00f6rg", ""]]}, {"id": "2002.12228", "submitter": "James Rondinelli", "authors": "M. J. Waters, J. M. Walker, C. T. Nelson, D. Joester, and J. M.\n  Rondinelli", "title": "Exploiting Colorimetry for Fidelity in Data Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cond-mat.mtrl-sci cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in multimodal characterization methods fuel a generation of\nincreasing immense hyper-dimensional datasets. Color mapping is employed for\nconveying higher dimensional data in two-dimensional (2D) representations for\nhuman consumption without relying on multiple projections. How one constructs\nthese color maps, however, critically affects how accurately one perceives\ndata. For simple scalar fields, perceptually uniform color maps and color\nselection have been shown to improve data readability and interpretation across\nresearch fields. Here we review core concepts underlying the design of\nperceptually uniform color map and extend the concepts from scalar fields to\ntwo-dimensional vector fields and three-component composition fields frequently\nfound in materials-chemistry research to enable high-fidelity visualization. We\ndevelop the software tools PAPUC and CMPUC to enable researchers to utilize\nthese colorimetry principles and employ perceptually uniform color spaces for\nrigorously meaningful color mapping of higher dimensional data representations.\nLast, we demonstrate how these approaches deliver immediate improvements in\ndata readability and interpretation in microscopies and spectroscopies\nroutinely used in discerning materials structure, chemistry, and properties.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 16:17:23 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Waters", "M. J.", ""], ["Walker", "J. M.", ""], ["Nelson", "C. T.", ""], ["Joester", "D.", ""], ["Rondinelli", "J. M.", ""]]}, {"id": "2002.12261", "submitter": "Min Hun Lee", "authors": "Min Hun Lee, Daniel P. Siewiorek, Asim Smailagic, Alexandre\n  Bernardino, and Sergi Berm\\'udez i Badia", "title": "Opportunities of a Machine Learning-based Decision Support System for\n  Stroke Rehabilitation Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rehabilitation assessment is critical to determine an adequate intervention\nfor a patient. However, the current practices of assessment mainly rely on\ntherapist's experience, and assessment is infrequently executed due to the\nlimited availability of a therapist. In this paper, we identified the needs of\ntherapists to assess patient's functional abilities (e.g. alternative\nperspective on assessment with quantitative information on patient's exercise\nmotions). As a result, we developed an intelligent decision support system that\ncan identify salient features of assessment using reinforcement learning to\nassess the quality of motion and summarize patient specific analysis. We\nevaluated this system with seven therapists using the dataset from 15 patient\nperforming three exercises. The evaluation demonstrates that our system is\npreferred over a traditional system without analysis while presenting more\nuseful information and significantly increasing the agreement over therapists'\nevaluation from 0.6600 to 0.7108 F1-scores ($p <0.05$). We discuss the\nimportance of presenting contextually relevant and salient information and\nadaptation to develop a human and machine collaborative decision making system.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 17:04:07 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 17:22:42 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lee", "Min Hun", ""], ["Siewiorek", "Daniel P.", ""], ["Smailagic", "Asim", ""], ["Bernardino", "Alexandre", ""], ["Badia", "Sergi Berm\u00fadez i", ""]]}, {"id": "2002.12315", "submitter": "Yi-Chi Liao", "authors": "Yi-Chi Liao, Sunjun Kim, Byungjoo Lee, Antti Oulasvirta", "title": "Press'Em: Simulating Varying Button Tactility via FDVV Models", "comments": "4 pages, CHI'20 EA. arXiv admin note: text overlap with\n  arXiv:2001.04352", "journal-ref": null, "doi": "10.1145/3334480.3383161", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Push-buttons provide rich haptic feedback during a press via mechanical\nstructures. While different buttons have varying haptic qualities, few works\nhave attempted to dynamically render such tactility, which limits designers\nfrom freely exploring buttons' haptic design. We extend the typical\nforce-displacement (FD) model with vibration (V) and velocity-dependence\ncharacteristics (V) to form a novel FDVV model. We then introduce Press'Em, a\n3D-printed prototype capable of simulating button tactility based on FDVV\nmodels. To drive Press'Em, an end-to-end simulation pipeline is presented that\ncovers (1) capturing any physical buttons, (2) controlling the actuation\nsignals, and (3) simulating the tactility. Our system can go beyond replicating\nexisting buttons to enable designers to emulate and test non-existent ones with\ndesired haptic properties. Press'Em aims to be a tool for future research to\nbetter understand and iterate over button designs.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 13:42:41 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Liao", "Yi-Chi", ""], ["Kim", "Sunjun", ""], ["Lee", "Byungjoo", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "2002.12360", "submitter": "Amir Aly", "authors": "Adriana Tapus, Andreea Peca, Amir Aly, Cristina Pop, Lavinia Jisa,\n  Sebastian Pintea, Alina Rusu, and Daniel David", "title": "Social Engagement of Children with Autism during Interaction with a\n  Robot", "comments": "Proceedings of the 2nd International Conference on Innovative\n  Research in Autism (IRIA), France, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation plays an important role in development, being one of the precursors\nof social cognition. Even though some children with autism imitate\nspontaneously and other children with autism can learn to imitate, the dynamics\nof imitation is affected in the large majority of cases. Existing studies from\nthe literature suggest that robots can be used to teach children with autism\nbasic interaction skills like imitation. Based on these findings, in this\nstudy, we investigate if children with autism show more social engagement when\ninteracting with an imitative robot (Fig 1) compared to a human partner in a\nmotor imitation task.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 11:14:24 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Tapus", "Adriana", ""], ["Peca", "Andreea", ""], ["Aly", "Amir", ""], ["Pop", "Cristina", ""], ["Jisa", "Lavinia", ""], ["Pintea", "Sebastian", ""], ["Rusu", "Alina", ""], ["David", "Daniel", ""]]}, {"id": "2002.12387", "submitter": "Daniel J Liebling", "authors": "Daniel J. Liebling, Michal Lahav, Abigail Evans, Aaron Donsbach, Jess\n  Holbrook, Boris Smus, Lindsey Boran", "title": "Unmet Needs and Opportunities for Mobile Translation AI", "comments": "13 pages, 3 figures, to be published in Proceedings of the 2020 CHI\n  Conference on Human Factors in Computing Systems (CHI '20)", "journal-ref": null, "doi": "10.1145/3313831.3376260", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Translation apps and devices are often presented in the context of providing\nassistance while traveling abroad. However, the spectrum of needs for\ncross-language communication is much wider. To investigate these needs, we\nconducted three studies with populations spanning socioeconomic status and\ngeographic regions: (1) United States-based travelers, (2) migrant workers in\nIndia, and (3) immigrant populations in the United States. We compare frequent\ntravelers' perception and actual translation needs with those of the two\nmigrant communities. The latter two, with low language proficiency, have the\ngreatest translation needs to navigate their daily lives. However, current\nmobile translation apps do not meet these needs. Our findings provide new\ninsights on the usage practices and limitations of mobile translation tools.\nFinally, we propose design implications to help apps better serve these unmet\nneeds.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 19:01:08 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Liebling", "Daniel J.", ""], ["Lahav", "Michal", ""], ["Evans", "Abigail", ""], ["Donsbach", "Aaron", ""], ["Holbrook", "Jess", ""], ["Smus", "Boris", ""], ["Boran", "Lindsey", ""]]}, {"id": "2002.12450", "submitter": "Juliana Ferreira J", "authors": "Juliana Jansen Ferreira and Mateus de Souza Monteiro", "title": "Do ML Experts Discuss Explainability for AI Systems? A discussion case\n  in the industry for a domain-specific solution", "comments": "7 pages, IUI workshop on Explainable Smart Systems and Algorithmic\n  Transparency in Emerging Technologies (ExSS-ATEC'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The application of Artificial Intelligence (AI) tools in different domains\nare becoming mandatory for all companies wishing to excel in their industries.\nOne major challenge for a successful application of AI is to combine the\nmachine learning (ML) expertise with the domain knowledge to have the best\nresults applying AI tools. Domain specialists have an understanding of the data\nand how it can impact their decisions. ML experts have the ability to use\nAI-based tools dealing with large amounts of data and generating insights for\ndomain experts. But without a deep understanding of the data, ML experts are\nnot able to tune their models to get optimal results for a specific domain.\nTherefore, domain experts are key users for ML tools and the explainability of\nthose AI tools become an essential feature in that context. There are a lot of\nefforts to research AI explainability for different contexts, users and goals.\nIn this position paper, we discuss interesting findings about how ML experts\ncan express concerns about AI explainability while defining features of an ML\ntool to be developed for a specific domain. We analyze data from two brainstorm\nsessions done to discuss the functionalities of an ML tool to support\ngeoscientists (domain experts) on analyzing seismic data (domain-specific data)\nwith ML resources.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 21:23:27 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Ferreira", "Juliana Jansen", ""], ["Monteiro", "Mateus de Souza", ""]]}, {"id": "2002.12551", "submitter": "EPTCS", "authors": "Ludovic Font (\\'Ecole Polytechnique de Montr\\'eal), S\\'ebastien Cyr\n  (Universit\\'e de Montr\\'eal), Philippe R. Richard (Universit\\'e de\n  Montr\\'eal), Michel Gagnon (\\'Ecole Polytechnique de Montr\\'eal)", "title": "Automating the Generation of High School Geometry Proofs using Prolog in\n  an Educational Context", "comments": "In Proceedings ThEdu'19, arXiv:2002.11895", "journal-ref": "EPTCS 313, 2020, pp. 1-16", "doi": "10.4204/EPTCS.313.1", "report-no": null, "categories": "cs.AI cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working on intelligent tutor systems designed for mathematics education\nand its specificities, an interesting objective is to provide relevant help to\nthe students by anticipating their next steps. This can only be done by\nknowing, beforehand, the possible ways to solve a problem. Hence the need for\nan automated theorem prover that provide proofs as they would be written by a\nstudent. To achieve this objective, logic programming is a natural tool due to\nthe similarity of its reasoning with a mathematical proof by inference. In this\npaper, we present the core ideas we used to implement such a prover, from its\nencoding in Prolog to the generation of the complete set of proofs. However,\nwhen dealing with educational aspects, there are many challenges to overcome.\nWe also present the main issues we encountered, as well as the chosen\nsolutions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 05:23:16 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Font", "Ludovic", "", "\u00c9cole Polytechnique de Montr\u00e9al"], ["Cyr", "S\u00e9bastien", "", "Universit\u00e9 de Montr\u00e9al"], ["Richard", "Philippe R.", "", "Universit\u00e9 de\n  Montr\u00e9al"], ["Gagnon", "Michel", "", "\u00c9cole Polytechnique de Montr\u00e9al"]]}, {"id": "2002.12557", "submitter": "Kyungjun Lee", "authors": "Kyungjun Lee, Abhinav Shrivastava, Hernisa Kacorri", "title": "Hand-Priming in Object Localization for Assistive Egocentric Vision", "comments": "the 2020 Winter Conference on Applications of Computer Vision (WACV\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric vision holds great promises for increasing access to visual\ninformation and improving the quality of life for people with visual\nimpairments, with object recognition being one of the daily challenges for this\npopulation. While we strive to improve recognition performance, it remains\ndifficult to identify which object is of interest to the user; the object may\nnot even be included in the frame due to challenges in camera aiming without\nvisual feedback. Also, gaze information, commonly used to infer the area of\ninterest in egocentric vision, is often not dependable. However, blind users\noften tend to include their hand either interacting with the object that they\nwish to recognize or simply placing it in proximity for better camera aiming.\nWe propose localization models that leverage the presence of the hand as the\ncontextual information for priming the center area of the object of interest.\nIn our approach, hand segmentation is fed to either the entire localization\nnetwork or its last convolutional layers. Using egocentric datasets from\nsighted and blind individuals, we show that the hand-priming achieves higher\nprecision than other approaches, such as fine-tuning, multi-class, and\nmulti-task learning, which also encode hand-object interactions in\nlocalization.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 05:32:36 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Lee", "Kyungjun", ""], ["Shrivastava", "Abhinav", ""], ["Kacorri", "Hernisa", ""]]}]