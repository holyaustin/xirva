[{"id": "1805.00307", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura, Kosuke Tanabe, Issei Tachibana", "title": "Tourist Navigation in Android Smartphone by using Emotion Generating\n  Calculations and Mental State Transition Networks", "comments": "6 pages, 8 figures, Proc. of The 6th International conference on Soft\n  Computing and Intelligent Systems and The 13th International Symposium on\n  Advanced Intelligent Systems(SCIS-ISIS 2012). arXiv admin note: substantial\n  text overlap with arXiv:1804.03994, arXiv:1804.02657, arXiv:1804.04946; text\n  overlap with arXiv:1804.02813", "journal-ref": null, "doi": "10.1109/SCIS-ISIS.2012.6505087", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental State Transition Network which consists of mental states connected to\neach other is a basic concept of approximating to human psychological and\nmental responses. It can represent transition from an emotional state to other\none with stimulus by calculating Emotion Generating Calculations method. A\ncomputer agent can transit a mental state in MSTN based on analysis of emotion\nby EGC method. In this paper, the Andorid EGC which the agent works in Android\nsmartphone can evaluate the feelings in the conversation. The tourist\nnavigation system with the proposed technique in this paper will be expected to\nbe an emotional oriented interface in Android smartphone.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 04:16:45 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Ichimura", "Takumi", ""], ["Tanabe", "Kosuke", ""], ["Tachibana", "Issei", ""]]}, {"id": "1805.00441", "submitter": "Markus Luczak-Roesch", "authors": "Cathal Doyle and Yevgeniya Li and Markus Luczak-Roesch and Dayle\n  Anderson and Brigitte Glasson and Matthew Boucher and Carol Brieseman and\n  Dianne Christenson and Melissa Coton", "title": "What is online citizen science anyway? An educational perspective", "comments": "16 pages, submitted to the 21st ACM Conference on Computer-Supported\n  Cooperative Work and Social Computing (CSCW 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper we seek to contribute to the debate about the nature of citizen\ninvolvement in real scientific projects by the means of online tools that\nfacilitate crowdsourcing and collaboration. We focus on an understudied area,\nthe impact of online citizen science participation on the science education of\nschool age children. We present a binary tree of online citizen science process\nflows and the results of an anonymous survey among primary school teachers in\nNew Zealand that are known advocates of science education. Our findings reveal\nwhy teachers are interested in using online citizen science in classroom\nactivities and what they are looking for when making their choice for a\nparticular project to use. From these characteristics we derive recommendations\nfor the optimal embedding of online citizen science in education related to the\nprocess, the context, and the dissemination of results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 01:49:09 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Doyle", "Cathal", ""], ["Li", "Yevgeniya", ""], ["Luczak-Roesch", "Markus", ""], ["Anderson", "Dayle", ""], ["Glasson", "Brigitte", ""], ["Boucher", "Matthew", ""], ["Brieseman", "Carol", ""], ["Christenson", "Dianne", ""], ["Coton", "Melissa", ""]]}, {"id": "1805.00460", "submitter": "Andrew Shin", "authors": "Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada", "title": "Customized Image Narrative Generation via Interactive Visual Question\n  Generation and Answering", "comments": "To Appear at CVPR 2018 as spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image description task has been invariably examined in a static manner with\nqualitative presumptions held to be universally applicable, regardless of the\nscope or target of the description. In practice, however, different viewers may\npay attention to different aspects of the image, and yield different\ndescriptions or interpretations under various contexts. Such diversity in\nperspectives is difficult to derive with conventional image description\ntechniques. In this paper, we propose a customized image narrative generation\ntask, in which the users are interactively engaged in the generation process by\nproviding answers to the questions. We further attempt to learn the user's\ninterest via repeating such interactive stages, and to automatically reflect\nthe interest in descriptions for new images. Experimental results demonstrate\nthat our model can generate a variety of descriptions from single image that\ncover a wider range of topics than conventional models, while being\ncustomizable to the target user of interaction.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 11:27:45 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Shin", "Andrew", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1805.00789", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Lina Yao, Shuai Zhang, Salil S. Kanhere, Quan Z. Sheng,\n  Yunhao Liu", "title": "Internet of Things Meets Brain-Computer Interface: A Unified Deep\n  Learning Framework for Enabling Human-Thing Cognitive Interactivity", "comments": "Accepted by IEEE Internet of Things Journal (http://ieee-iotj.org/).\n  arXiv admin note: substantial text overlap with arXiv:1804.05493", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Brain-Computer Interface (BCI) acquires brain signals, analyzes and\ntranslates them into commands that are relayed to actuation devices for\ncarrying out desired actions. With the widespread connectivity of everyday\ndevices realized by the advent of the Internet of Things (IoT), BCI can empower\nindividuals to directly control objects such as smart home appliances or\nassistive robots, directly via their thoughts. However, realization of this\nvision is faced with a number of challenges, most importantly being the issue\nof accurately interpreting the intent of the individual from the raw brain\nsignals that are often of low fidelity and subject to noise. Moreover,\npre-processing brain signals and the subsequent feature engineering are both\ntime-consuming and highly reliant on human domain expertise. To address the\naforementioned issues, in this paper, we propose a unified deep learning based\nframework that enables effective human-thing cognitive interactivity in order\nto bridge individuals and IoT objects. We design a reinforcement learning based\nSelective Attention Mechanism (SAM) to discover the distinctive features from\nthe input brain signals. In addition, we propose a modified Long Short-Term\nMemory (LSTM) to distinguish the inter-dimensional information forwarded from\nthe SAM. To evaluate the efficiency of the proposed framework, we conduct\nextensive real-world experiments and demonstrate that our model outperforms a\nnumber of competitive state-of-the-art baselines. Two practical real-time\nhuman-thing cognitive interaction applications are presented to validate the\nfeasibility of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 05:38:21 GMT"}, {"version": "v2", "created": "Sat, 5 May 2018 10:33:38 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 06:28:26 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Zhang", "Xiang", ""], ["Yao", "Lina", ""], ["Zhang", "Shuai", ""], ["Kanhere", "Salil S.", ""], ["Sheng", "Quan Z.", ""], ["Liu", "Yunhao", ""]]}, {"id": "1805.00823", "submitter": "Ran Yu", "authors": "Ran Yu, Ujwal Gadiraju, Peter Holtz, Markus Rokicki, Philipp Kemkes,\n  Stefan Dietze", "title": "Predicting User Knowledge Gain in Informational Search Sessions", "comments": "10 pages, 2 figures, SIGIR18", "journal-ref": null, "doi": "10.1145/3209978.3210064", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web search is frequently used by people to acquire new knowledge and to\nsatisfy learning-related objectives. In this context, informational search\nmissions with an intention to obtain knowledge pertaining to a topic are\nprominent. The importance of learning as an outcome of web search has been\nrecognized. Yet, there is a lack of understanding of the impact of web search\non a user's knowledge state. Predicting the knowledge gain of users can be an\nimportant step forward if web search engines that are currently optimized for\nrelevance can be molded to serve learning outcomes. In this paper, we introduce\na supervised model to predict a user's knowledge state and knowledge gain from\nfeatures captured during the search sessions. To measure and predict the\nknowledge gain of users in informational search sessions, we recruited 468\ndistinct users using crowdsourcing and orchestrated real-world search sessions\nspanning 11 different topics and information needs. By using scientifically\nformulated knowledge tests, we calibrated the knowledge of users before and\nafter their search sessions, quantifying their knowledge gain. Our supervised\nmodels utilise and derive a comprehensive set of features from the current\nstate of the art and compare performance of a range of feature sets and feature\nselection strategies. Through our results, we demonstrate the ability to\npredict and classify the knowledge state and gain using features obtained\nduring search sessions, exhibiting superior performance to an existing baseline\nin the knowledge state prediction task.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 14:02:24 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Yu", "Ran", ""], ["Gadiraju", "Ujwal", ""], ["Holtz", "Peter", ""], ["Rokicki", "Markus", ""], ["Kemkes", "Philipp", ""], ["Dietze", "Stefan", ""]]}, {"id": "1805.00889", "submitter": "Justin Salamon", "authors": "Juan Pablo Bello, Claudio Silva, Oded Nov, R. Luke DuBois, Anish\n  Arora, Justin Salamon, Charles Mydlarz, Harish Doraiswamy", "title": "SONYC: A System for the Monitoring, Analysis and Mitigation of Urban\n  Noise Pollution", "comments": "Accepted May 2018, Communications of the ACM. This is the author's\n  version of the work. It is posted here for your personal use. Not for\n  redistribution. The definitive Version of Record will be published in\n  Communications of the ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CY cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Sounds of New York City (SONYC) project, a smart cities\ninitiative focused on developing a cyber-physical system for the monitoring,\nanalysis and mitigation of urban noise pollution. Noise pollution is one of the\ntopmost quality of life issues for urban residents in the U.S. with proven\neffects on health, education, the economy, and the environment. Yet, most\ncities lack the resources to continuously monitor noise and understand the\ncontribution of individual sources, the tools to analyze patterns of noise\npollution at city-scale, and the means to empower city agencies to take\neffective, data-driven action for noise mitigation. The SONYC project advances\nnovel technological and socio-technical solutions that help address these\nneeds.\n  SONYC includes a distributed network of both sensors and people for\nlarge-scale noise monitoring. The sensors use low-cost, low-power technology,\nand cutting-edge machine listening techniques, to produce calibrated acoustic\nmeasurements and recognize individual sound sources in real time. Citizen\nscience methods are used to help urban residents connect to city agencies and\neach other, understand their noise footprint, and facilitate reporting and\nself-regulation. Crucially, SONYC utilizes big data solutions to analyze,\nretrieve and visualize information from sensors and citizens, creating a\ncomprehensive acoustic model of the city that can be used to identify\nsignificant patterns of noise pollution. These data can be used to drive the\nstrategic application of noise code enforcement by city agencies to optimize\nthe reduction of noise pollution. The entire system, integrating cyber,\nphysical and social infrastructure, forms a closed loop of continuous sensing,\nanalysis and actuation on the environment.\n  SONYC provides a blueprint for the mitigation of noise pollution that can\npotentially be applied to other cities in the US and abroad.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 16:07:39 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 19:23:01 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Bello", "Juan Pablo", ""], ["Silva", "Claudio", ""], ["Nov", "Oded", ""], ["DuBois", "R. Luke", ""], ["Arora", "Anish", ""], ["Salamon", "Justin", ""], ["Mydlarz", "Charles", ""], ["Doraiswamy", "Harish", ""]]}, {"id": "1805.00901", "submitter": "Pier Luca Lanzi", "authors": "Fabrizia Corona, Rocco M. Chiuri, Giovanni Filocamo, Michaela Foa',\n  Pier Luca Lanzi, Amalia Lopopolo, and Antonella Petaccia", "title": "Serious Games for Wrist Rehabilitation in Juvenile Idiopathic Arthritis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rehabilitation is a painful and tiring process involving series of exercises\nthat patients must repeat over a long period. Unfortunately, patients often\ngrow bored, frustrated, and lose motivation making rehabilitation less\neffective. In the recent years video games have been widely used to implement\nrehabilitation protocols so as to make the process more entertaining, engaging\nand to keep patients motivated. In this paper, we present an integrated\nframework we developed for the wrist rehabilitation of patients affected by\nJuvenile Idiopathic Arthritis (JIA) following a therapeutic protocol at the\nClinica Pediatrica G. e D. De Marchi. The framework comprises four video games\nand a set modules that let the therapists tune and control the exercises the\ngames implemented, record all the patients actions, replay and analyze the\nsessions. We present the result of a preliminary validation we performed with\nfour poliarticular JIA patients at the clinic under the supervision of the\ntherapists. Overall, we received good feedback both from the young patients,\nwho enjoyed performing known rehabilitation exercises using video games, and\ntherapists who were satisfied with the framework and its potentials for\nengaging and motivating the patients.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 16:38:01 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Corona", "Fabrizia", ""], ["Chiuri", "Rocco M.", ""], ["Filocamo", "Giovanni", ""], ["Foa'", "Michaela", ""], ["Lanzi", "Pier Luca", ""], ["Lopopolo", "Amalia", ""], ["Petaccia", "Antonella", ""]]}, {"id": "1805.00977", "submitter": "Ludovik Coba <", "authors": "Ludovik Coba, Markus Zanker, Laurens Rook and Panagiotis Symeonidis", "title": "Exploring Users' Perception of Collaborative Explanation Styles", "comments": null, "journal-ref": null, "doi": "10.1109/CBI.2018.00017", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering systems heavily depend on user feedback expressed in\nproduct ratings to select and rank items to recommend. In this study we explore\nhow users value different collaborative explanation styles following the\nuser-based or item-based paradigm. Furthermore, we explore how the\ncharacteristics of these rating summarizations, like the total number of\nratings and the mean rating value, influence the decisions of online users.\nResults, based on a choice-based conjoint experimental design, show that the\nmean indicator has a higher impact compared to the total number of ratings.\nFinally, we discuss how these empirical results can serve as an input to\ndeveloping algorithms that foster items with a, consequently, higher\nprobability of choice based on their rating summarizations or their\nexplainability due to these ratings when ranking recommendations.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 18:45:32 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Coba", "Ludovik", ""], ["Zanker", "Markus", ""], ["Rook", "Laurens", ""], ["Symeonidis", "Panagiotis", ""]]}, {"id": "1805.01129", "submitter": "Palakorn Achananuparp", "authors": "Palakorn Achananuparp, Ee-Peng Lim, Vibhanshu Abhishek", "title": "Does Journaling Encourage Healthier Choices? Analyzing Healthy Eating\n  Behaviors of Food Journalers", "comments": "Published at Digital Health 2018", "journal-ref": null, "doi": "10.1145/3194658.3194663", "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past research has shown the benefits of food journaling in promoting mindful\neating and healthier food choices. However, the links between journaling and\nhealthy eating have not been thoroughly examined. Beyond caloric restriction,\ndo journalers consistently and sufficiently consume healthful diets? How\ndifferent are their eating habits compared to those of average consumers who\ntend to be less conscious about health? In this study, we analyze the healthy\neating behaviors of active food journalers using data from MyFitnessPal.\nSurprisingly, our findings show that food journalers do not eat as healthily as\nthey should despite their proclivity to health eating and their food choices\nresemble those of the general populace. Furthermore, we find that the\njournaling duration is only a marginal determinant of healthy eating outcomes\nand sociodemographic factors, such as gender and regions of residence, are much\nmore predictive of healthy food choices.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 05:59:22 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Achananuparp", "Palakorn", ""], ["Lim", "Ee-Peng", ""], ["Abhishek", "Vibhanshu", ""]]}, {"id": "1805.01138", "submitter": "Brit Youngmann", "authors": "Liron Allerhand, Brit Youngmann, Elad Yom-Tov and David Arkadir", "title": "Detecting Parkinson's Disease from interactions with a search engine: Is\n  expert knowledge sufficient?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's disease (PD) is a slowly progressing neurodegenerative disease\nwith early manifestation of motor signs. Recently, there has been a growing\ninterest in developing automatic tools that can assess motor function in PD\npatients. Here we show that mouse tracking data collected during people's\ninteraction with a search engine can be used to distinguish PD patients from\nsimilar, non-diseased users and present a methodology developed for the\ndiagnosis of PD from these data. A main challenge we address is the extraction\nof informative features from raw mouse tracking data. We do so in two\ncomplementary ways: First, we manually construct expert-recommended informative\nfeatures, aiming to identify abnormalities in motor behaviors. Second, we use\nan unsupervised representation learning technique to map these raw data to\nhigh-level features. Using all the extracted features, a Random Forest\nclassifier is then used to distinguish PD patients from controls, achieving an\nAUC of 0.92, while results using only expert-generated or auto-generated\nfeatures are 0.87 and 0.83, respectively. Our results indicate that mouse\ntracking data can help in detecting users at early stages of the disease, and\nthat both expert-generated features and unsupervised techniques for feature\ngeneration are required to achieve the best possible performance\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 06:54:45 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Allerhand", "Liron", ""], ["Youngmann", "Brit", ""], ["Yom-Tov", "Elad", ""], ["Arkadir", "David", ""]]}, {"id": "1805.01452", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias, Stefanos Zafeiriou", "title": "A Multi-component CNN-RNN Approach for Dimensional Emotion Recognition\n  in-the-wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our approach to the One-Minute Gradual-Emotion\nRecognition (OMG-Emotion) Challenge, focusing on dimensional emotion\nrecognition through visual analysis of the provided emotion videos. The\napproach is based on a Convolutional and Recurrent (CNN-RNN) deep neural\narchitecture we have developed for the relevant large AffWild Emotion Database.\nWe extended and adapted this architecture, by letting a combination of multiple\nfeatures generated in the CNN component be explored by RNN subnets. Our target\nhas been to obtain best performance on the OMG-Emotion visual validation data\nset, while learning the respective visual training data set. Extended\nexperimentation has led to best architectures for the estimation of the values\nof the valence and arousal emotion dimensions over these data sets.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 17:54:44 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 01:01:00 GMT"}, {"version": "v3", "created": "Tue, 8 May 2018 09:43:17 GMT"}, {"version": "v4", "created": "Mon, 12 Nov 2018 23:17:42 GMT"}, {"version": "v5", "created": "Fri, 13 Dec 2019 23:32:41 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1805.01512", "submitter": "Chun-Wei Chiang", "authors": "Chun-Wei Chiang, Eber Betanzos, Saiph Savage", "title": "Blockchain for Trustful Collaborations between Immigrants and\n  Governments", "comments": "6 pages, 6 figures", "journal-ref": "Proceeding CHI EA '18 Extended Abstracts of the 2018 CHI\n  Conference on Human Factors in Computing Systems Paper No. LBW531", "doi": "10.1145/3170427.3188660", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immigrants usually are pro-social towards their hometowns and try to improve\nthem. However, the lack of trust in their government can drive immigrants to\nwork individually. As a result, their pro-social activities are usually limited\nin impact and scope. This paper studies the interface factors that ease\ncollaborations between immigrants and their home governments. We specifically\nfocus on Mexican immigrants in the US who want to improve their rural\ncommunities. We identify that for Mexican immigrants having clear workflows of\nhow their money flows and a sense of control over this workflow is important\nfor collaborating with their government. Based on these findings, we create a\nblockchain based system for building trust between governments and immigrants.\nWe finish by discussing design implications of our work and future directions.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 19:00:32 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Chiang", "Chun-Wei", ""], ["Betanzos", "Eber", ""], ["Savage", "Saiph", ""]]}, {"id": "1805.01515", "submitter": "Yuhang Zhao", "authors": "Yuhang Zhao, Shaomei Wu, Lindsay Reynolds, and Shiri Azenkot", "title": "The Effect of Computer-Generated Descriptions on Photo-Sharing\n  Experiences of People with Visual Impairments", "comments": "CSCW 2018", "journal-ref": "Proc. ACM Hum.-Comput. Interact. 1, CSCW, 121 (November 2017), 22\n  pages", "doi": "10.1145/3134756", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like sighted people, visually impaired people want to share photographs on\nsocial networking services, but find it difficult to identify and select photos\nfrom their albums. We aimed to address this problem by incorporating\nstate-of-the-art computer-generated descriptions into Facebook's photo-sharing\nfeature. We interviewed 12 visually impaired participants to understand their\nphoto-sharing experiences and designed a photo description feature for the\nFacebook mobile application. We evaluated this feature with six participants in\na seven-day diary study. We found that participants used the descriptions to\nrecall and organize their photos, but they hesitated to upload photos without a\nsighted person's input. In addition to basic information about photo content,\nparticipants wanted to know more details about salient objects and people, and\nwhether the photos reflected their personal aesthetics. We discuss these\nfindings from the lens of self-disclosure and self-presentation theories and\npropose new computer vision research directions that will better support visual\ncontent sharing by visually impaired people.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 19:10:38 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Zhao", "Yuhang", ""], ["Wu", "Shaomei", ""], ["Reynolds", "Lindsay", ""], ["Azenkot", "Shiri", ""]]}, {"id": "1805.01819", "submitter": "Ilia Rushkin", "authors": "Ilia Rushkin", "title": "Time-on-Task Estimation with Log-Normal Mixture Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method of estimating a user's time-on-task in an online\nlearning environment. The method is agnostic of the details of the user's\nmental activity and does not rely on any data except timestamps of user's\ninteractions, accounting for individual user differences. The method is\nimplemented in R (the code is open-source) and has been tested in the data from\na large sample of HarvardX MOOCs.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 15:15:01 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Rushkin", "Ilia", ""]]}, {"id": "1805.02249", "submitter": "Ali Sharifara", "authors": "Shawn N. Gieser, Joseph Tompkins, Ali Sharifara, Fillia Makedon", "title": "Using Humanoid Robot to Instruct and Evaluate Performance of a Physical\n  Task", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.32802.73920", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a tool to assess users ability to change tasks. To\ndo this, we use a variation of the Box and Blocks Test. In this version, a\nhumanoid robot instructs a user to perform a task involving the movement of\ncertain colored blocks. The robot changes randomly change the color of blocks\nthat the user is supposed to move. Canny Edge Detection and Hough\nTransformation are used to assess user perform the robot's built-in camera.\nThis will allow the robot to inform the user and keep a log of their progress.\nWe present this method for monitoring user progress by describing how the moved\nblocks are detected. We also present the results of a pilot study where users\nused this system to perform the task. Preliminary results show that users do\nnot perform differently when the task is changed in this scenario.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 17:42:57 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Gieser", "Shawn N.", ""], ["Tompkins", "Joseph", ""], ["Sharifara", "Ali", ""], ["Makedon", "Fillia", ""]]}, {"id": "1805.02399", "submitter": "Nilavra Bhattacharya", "authors": "Nilavra Bhattacharya, Jacek Gwizdka", "title": "Relating Eye-Tracking Measures With Changes In Knowledge on Search Tasks", "comments": "ACM Symposium on Eye Tracking Research and Applications (ETRA), June\n  14-17, 2018, Warsaw, Poland", "journal-ref": "Proceedings of the 2018 ACM Symposium on Eye Tracking Research &\n  Applications", "doi": "10.1145/3204493.3204579", "report-no": null, "categories": "cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conducted an eye-tracking study where 30 participants performed searches\non the web. We measured their topical knowledge before and after each task.\nTheir eye-fixations were labelled as \"reading\" or \"scanning\". The series of\nreading fixations in a line, called \"reading-sequences\" were characterized by\ntheir length in pixels, fixation duration, and the number of fixations making\nup the sequence. We hypothesize that differences in knowledge-change of\nparticipants are reflected in their eye-tracking measures related to reading.\nOur results show that the participants with higher change in knowledge differ\nsignificantly in terms of their total reading-sequence-length,\nreading-sequence-duration, and number of reading fixations, when compared to\nparticipants with lower knowledge-change.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 08:33:59 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Bhattacharya", "Nilavra", ""], ["Gwizdka", "Jacek", ""]]}, {"id": "1805.02489", "submitter": "Jean-Benoit Delbrouck", "authors": "Jean-Benoit Delbrouck", "title": "Transformer for Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the UMONS solution for the OMG-Emotion Challenge. We\nexplore a context-dependent architecture where the arousal and valence of an\nutterance are predicted according to its surrounding context (i.e. the\npreceding and following utterances of the video). We report an improvement when\ntaking into account context for both unimodal and multimodal predictions.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 19:42:57 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 13:03:59 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Delbrouck", "Jean-Benoit", ""]]}, {"id": "1805.02492", "submitter": "Shovan Maity", "authors": "Shovan Maity, Debayan Das, Baibhab Chatterjee, Shreyas Sen", "title": "Characterization and Classification of Human Body Channel as a function\n  of Excitation and Termination Modalities", "comments": "Accepted in 40th International Engineering in Medicine and Biology\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Body Communication (HBC) has recently emerged as an alternative to\nradio frequency transmission for connecting devices on and in the human body\nwith order(s) of magnitude lower energy. The communication between these\ndevices can give rise to different scenarios, which can be classified as\nwearable-wearable, wearable-machine, machine-machine interactions. In this\npaper, for the first time, the human body channel characteristics is measured\nfor a wide range of such possible scenarios (14 vs. a few in previous\nliterature) and classified according to the form-factor of the transmitter and\nreceiver. The effect of excitation/termination configurations on the channel\nloss is also explored, which helps explain the previously unexplained wide\nvariation in HBC Channel measurements. Measurement results show that\nwearable-wearable interaction has the maximum loss (upto -50 dB) followed by\nwearable-machine and machinemachine interaction (min loss of 0.5 dB), primarily\ndue to the small ground size of the wearable devices. Among the excitation\nconfigurations, differential excitation is suitable for small channel length\nwhereas single ended is better for longer channel.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 00:43:32 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 18:32:12 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Maity", "Shovan", ""], ["Das", "Debayan", ""], ["Chatterjee", "Baibhab", ""], ["Sen", "Shreyas", ""]]}, {"id": "1805.02711", "submitter": "Bahador Saket", "authors": "Bahador Saket and Alex Endert", "title": "Evaluation of Visualization by Demonstration and Manual View\n  Specification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an exploratory study comparing the visualization construction and\ndata exploration processes of people using two visualization tools, each\nimplementing a different interaction paradigm. One of the visualization tools\nimplements the manual view specification paradigm (Polestar) and another\nimplements the visualization by demonstration paradigm (VisExemplar). Findings\nof our study indicate that the interaction paradigms implemented in these tools\ninfluence: 1) approaches used for constructing visualizations, 2) how users\nform goals, 3) how many visualization alternatives are considered and created,\nand 4) the feeling of control during the visualization construction process.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 19:30:44 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Saket", "Bahador", ""], ["Endert", "Alex", ""]]}, {"id": "1805.02787", "submitter": "Lex Fridman", "authors": "Julia Kindelsberger, Lex Fridman, Michael Glazer, Bryan Reimer", "title": "Designing Toward Minimalism in Vehicle HMI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose that safe, beautiful, fulfilling vehicle HMI design must start\nfrom a rigorous consideration of minimalist design. Modern vehicles are\nchanging from mechanical machines to mobile computing devices, similar to the\nchange from landline phones to smartphones. We propose the approach of\n\"designing toward minimalism\", where we ask \"why?\" rather than \"why not?\" in\nchoosing what information to display to the driver. We demonstrate this\napproach on an HMI case study of displaying vehicle speed. We first show that\nvehicle speed is what 87.6% of people ask for. We then show, through an online\nstudy with 1,038 subjects and 22,950 videos, that humans can estimate\nego-vehicle speed very well, especially at lower speeds. Thus, despite\nbelieving that we need this information, we may not. In this way, we\ndemonstrate a systematic approach of questioning the fundamental assumptions of\nwhat information is essential for vehicle HMI.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 00:31:09 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Kindelsberger", "Julia", ""], ["Fridman", "Lex", ""], ["Glazer", "Michael", ""], ["Reimer", "Bryan", ""]]}, {"id": "1805.03168", "submitter": "Grigorios Kalogiannis Mr", "authors": "Kalogiannis Gregory, Karampelas Nikolaos, Hassapis George", "title": "A reworked SOBI algorithm based on SCHUR Decomposition for EEG data\n  processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In brain machine interfaces (BMI) that are used to control motor\nrehabilitation devices there is the need to process the monitored brain signals\nwith the purpose of recognizing patient's intentions to move his hands or limbs\nand reject artifact and noise superimposed on these signals. This kind of\nprocessing has to take place within time limits imposed by the on-line control\nrequirements of such devices. A widely-used algorithm is the Second Order Blind\nIdentification (SOBI) independent component analysis (ICA) algorithm. This\nalgorithm, however, presents long processing time and therefor it not suitable\nfor use in the brain-based control of rehabilitation devices. A rework of this\nalgorithm that is presented in this paper and based on SCHUR decomposition\nresults to significantly reduced processing time. This new algorithm is quite\nappropriate for use in brain-based control of rehabilitation devices.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 17:10:51 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Gregory", "Kalogiannis", ""], ["Nikolaos", "Karampelas", ""], ["George", "Hassapis", ""]]}, {"id": "1805.03329", "submitter": "arXiv Admin", "authors": "Michael Jacob, Zack Zheng", "title": "LogIn: Unlock Journaling System for Personal Informatics", "comments": "arXiv admin note: submission has been withdrawn by arXiv\n  administrators due to inappropriate overlap with external sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In situ self-report is widely used in human-computer interaction, ubiquitous\ncomputing, and for assessment and intervention in health and wellness.\nUnfortunately, it remains limited by high burdens. We examine unlock journaling\nas an alternative. Specifically, we build upon recent work to introduce single\nslide unlock journaling gestures appropriate for health and wellness measures.\nWe then present the first field study comparing unlock journaling with\ntraditional diaries and notification based reminders in self report of health\nand wellness measures. We find unlock journaling is less intrusive than\nreminders, dramatically improves frequency of journaling, and can provide equal\nor better timeliness. Where appropriate to broader design needs, unlock\njournaling is thus an overall promising method for in situ self report.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 00:42:15 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 19:14:08 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Jacob", "Michael", ""], ["Zheng", "Zack", ""]]}, {"id": "1805.03491", "submitter": "Markus Schr\\\"oder", "authors": "Markus Schr\\\"oder, Christian Jilek and Andreas Dengel", "title": "Deep Linking Desktop Resources", "comments": "5 pages, 2 figures, ESWC 2018 demo paper", "journal-ref": "The Semantic Web: ESWC 2018 Satellite Events, pp. 202-207,\n  Springer", "doi": "10.1007/978-3-319-98192-5_38", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Linking is the process of referring to a specific piece of web content.\nAlthough users can browse their files in desktop environments, they are unable\nto directly traverse deeper into their content using deep links. In order to\nsolve this issue, we demonstrate \"DeepLinker\", a tool which generates and\ninterprets deep links to desktop resources, thus enabling the reference to a\ncertain location within a file using a simple hyperlink. By default, the\nservice responds with an HTML representation of the resource along with further\nlinks to follow. Additionally, we allow the use of RDF to interlink our deep\nlinks with other resources.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 11:15:54 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Schr\u00f6der", "Markus", ""], ["Jilek", "Christian", ""], ["Dengel", "Andreas", ""]]}, {"id": "1805.03553", "submitter": "Abdullah Al-Dujaili", "authors": "Alex Huang, Abdullah Al-Dujaili, Erik Hemberg, Una-May O'Reilly", "title": "On Visual Hallmarks of Robustness to Adversarial Malware", "comments": "Submitted to the IReDLiA workshop at the Federated Artificial\n  Intelligence Meeting (FAIM) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge of adversarial learning is to interpret the resulting\nhardened model. In this contribution, we ask how robust generalization can be\nvisually discerned and whether a concise view of the interactions between a\nhardened decision map and input samples is possible. We first provide a means\nof visually comparing a hardened model's loss behavior with respect to the\nadversarial variants generated during training versus loss behavior with\nrespect to adversarial variants generated from other sources. This allows us to\nconfirm that the association of observed flatness of a loss landscape with\ngeneralization that is seen with naturally trained models extends to\nadversarially hardened models and robust generalization. To complement these\nmeans of interpreting model parameter robustness we also use self-organizing\nmaps to provide a visual means of superimposing adversarial and natural\nvariants on a model's decision space, thus allowing the model's global\nrobustness to be comprehensively examined.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 14:28:42 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Huang", "Alex", ""], ["Al-Dujaili", "Abdullah", ""], ["Hemberg", "Erik", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "1805.03563", "submitter": "Dominique Vaufreydaz", "authors": "Eleonore Ferrier-Barbut (Chroma), Dominique Vaufreydaz (Pervasive),\n  Jean-Alix David (Chroma), J\\'er\\^ome Lussereau (Chroma), Anne Spalanzani\n  (Chroma)", "title": "Personal space of autonomous car's passengers sitting in the driver's\n  seat", "comments": null, "journal-ref": "The 2018 IEEE Intelligent Vehicles Symposium (IV'18), Jun 2018,\n  Changshu, Suzhou, China", "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the specific context of an autonomous car navigating\nin an urban center within a shared space between pedestrians and cars. The\ndriver delegates the control to the autonomous system while remaining seated in\nthe driver's seat. The proposed study aims at giving a first insight into the\ndefinition of human perception of space applied to vehicles by testing the\nexistence of a personal space around the car.It aims at measuring proxemic\ninformation about the driver's comfort zone in such conditions.Proxemics, or\nhuman perception of space, has been largely explored when applied to humans or\nto robots, leading to the concept of personal space, but poorly when applied to\nvehicles. In this article, we highlight the existence and the characteristics\nof a zone of comfort around the car which is not correlated to the risk of a\ncollision between the car and other road users. Our experiment includes 19\nvolunteers using a virtual reality headset to look at 30 scenarios filmed in\n360{\\textdegree} from the point of view of a passenger sitting in the driver's\nseat of an autonomous car.They were asked to say \"stop\" when they felt\ndiscomfort visualizing the scenarios.As said, the scenarios voluntarily avoid\ncollision effect as we do not want to measure fear but discomfort.The scenarios\ninvolve one or three pedestrians walking past the car at different distances\nfrom the wings of the car, relative to the direction of motion of the car, on\nboth sides. The car is either static or moving straight forward at different\nspeeds.The results indicate the existence of a comfort zone around the car in\nwhich intrusion causes discomfort.The size of the comfort zone is sensitive\nneither to the side of the car where the pedestrian passes nor to the number of\npedestrians. In contrast, the feeling of discomfort is relative to the car's\nmotion (static or moving).Another outcome from this study is an illustration of\nthe usage of first person 360{\\textdegree} video and a virtual reality headset\nto evaluate feelings of a passenger within an autonomous car.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 14:46:40 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Ferrier-Barbut", "Eleonore", "", "Chroma"], ["Vaufreydaz", "Dominique", "", "Pervasive"], ["David", "Jean-Alix", "", "Chroma"], ["Lussereau", "J\u00e9r\u00f4me", "", "Chroma"], ["Spalanzani", "Anne", "", "Chroma"]]}, {"id": "1805.03709", "submitter": "Patrick Stotko", "authors": "Patrick Stotko, Stefan Krumpen, Matthias B. Hullin, Michael Weinmann,\n  Reinhard Klein", "title": "SLAMCast: Large-Scale, Real-Time 3D Reconstruction and Streaming for\n  Immersive Multi-Client Live Telepresence", "comments": "The final version of record is available at\n  http://dx.doi.org/10.1109/TVCG.2019.2899231", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics (2019),\n  25:5(2102-2112)", "doi": "10.1109/TVCG.2019.2899231", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time 3D scene reconstruction from RGB-D sensor data, as well as the\nexploration of such data in VR/AR settings, has seen tremendous progress in\nrecent years. The combination of both these components into telepresence\nsystems, however, comes with significant technical challenges. All approaches\nproposed so far are extremely demanding on input and output devices, compute\nresources and transmission bandwidth, and they do not reach the level of\nimmediacy required for applications such as remote collaboration. Here, we\nintroduce what we believe is the first practical client-server system for\nreal-time capture and many-user exploration of static 3D scenes. Our system is\nbased on the observation that interactive frame rates are sufficient for\ncapturing and reconstruction, and real-time performance is only required on the\nclient site to achieve lag-free view updates when rendering the 3D model.\nStarting from this insight, we extend previous voxel block hashing frameworks\nby overcoming internal dependencies and introducing, to the best of our\nknowledge, the first thread-safe GPU hash map data structure that is robust\nunder massively concurrent retrieval, insertion and removal of entries on a\nthread level. We further propose a novel transmission scheme for volume data\nthat is specifically targeted to Marching Cubes geometry reconstruction and\nenables a 90% reduction in bandwidth between server and exploration clients.\nThe resulting system poses very moderate requirements on network bandwidth,\nlatency and client-side computation, which enables it to rely entirely on\nconsumer-grade hardware, including mobile devices. We demonstrate that our\ntechnique achieves state-of-the-art representation accuracy while providing,\nfor any number of clients, an immersive and fluid lag-free viewing experience\neven during network outages.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 19:54:39 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 15:16:42 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Stotko", "Patrick", ""], ["Krumpen", "Stefan", ""], ["Hullin", "Matthias B.", ""], ["Weinmann", "Michael", ""], ["Klein", "Reinhard", ""]]}, {"id": "1805.04157", "submitter": "Stephen Bonner", "authors": "Nik Khadijah Nik Aznan, Stephen Bonner, Jason D. Connolly, Noura Al\n  Moubayed, Toby P. Breckon", "title": "On the Classification of SSVEP-Based Dry-EEG Signals via Convolutional\n  Neural Networks", "comments": "Accepted as a full paper at the 2018 IEEE International Conference on\n  Systems, Man, and Cybernetics (SMC2018)", "journal-ref": null, "doi": "10.1109/SMC.2018.00631", "report-no": null, "categories": "cs.HC eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Convolutional Neural Network (CNN) approach\nfor the classification of raw dry-EEG signals without any data pre-processing.\nTo illustrate the effectiveness of our approach, we utilise the Steady State\nVisual Evoked Potential (SSVEP) paradigm as our use case. SSVEP can be utilised\nto allow people with severe physical disabilities such as Complete Locked-In\nSyndrome or Amyotrophic Lateral Sclerosis to be aided via BCI applications, as\nit requires only the subject to fixate upon the sensory stimuli of interest.\nHere we utilise SSVEP flicker frequencies between 10 to 30 Hz, which we record\nas subject cortical waveforms via the dry-EEG headset. Our proposed end-to-end\nCNN allows us to automatically and accurately classify SSVEP stimulation\ndirectly from the dry-EEG waveforms. Our CNN architecture utilises a common\nSSVEP Convolutional Unit (SCU), comprising of a 1D convolutional layer, batch\nnormalization and max pooling. Furthermore, we compare several deep learning\nneural network variants with our primary CNN architecture, in addition to\ntraditional machine learning classification approaches. Experimental evaluation\nshows our CNN architecture to be significantly better than competing\napproaches, achieving a classification accuracy of 96% whilst demonstrating\nsuperior cross-subject performance and even being able to generalise well to\nunseen subjects whose data is entirely absent from the training process.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 20:10:02 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 09:47:21 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Aznan", "Nik Khadijah Nik", ""], ["Bonner", "Stephen", ""], ["Connolly", "Jason D.", ""], ["Moubayed", "Noura Al", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1805.04342", "submitter": "Simone Santini", "authors": "Simone Santini", "title": "Semiotic internationalization and localization of computer programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Localization, the process--part of translation studies--of adapting a program\nto a new linguistic community, is often intended in the relatively narrow sense\nof translating the messages and labels of the program into the target language.\nCorrespondingly, internationalization, the discipline--which is part of\nsoftware engineering--of putting in place all the measures that will make\nlocalization easier, is also limited in scope.\n  In this paper we analyze the various systems through which a program\ncommunicates with a person (icons, buttons, actions, interface layout, etc.)\nand find that most of them, far from being iconic, are in reality symbolic\nsemiotic systems related to the culture in which or for which the program was\ndeveloped (typically American programmers of western office workers). Based on\nthese findings, we argue that during the localization process, the translator\nshould have the option to translate them all, that is, to adapt the whole\ninterface and its founding metaphors to the cultural environment in which the\nprogram is deployed.\n  This conclusion will result in a greater role for internationalization in the\nsoftware development process, and we outline a few architectural principles\nthat should be considered when creating a program for a multi-cultural market.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 11:50:47 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Santini", "Simone", ""]]}, {"id": "1805.04366", "submitter": "Bernadette Spieler", "authors": "Bernadette Spieler and Wolfgang Slany", "title": "Female Teenagers and Coding: Create Gender Sensitive and Creative\n  Learning Environments", "comments": "12 pages, 8 figures, Constructioism 2018", "journal-ref": "Constructioism 2018", "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of women in technical fields is far below the average number of\nmales, especially in developed countries. Gender differences in STEM are\nalready present in secondary schools in students aged between 12 to 15 years.\nAdolescence is a critical time for identity formation, and self-attributes are\na source for internal conflicts, especially for female teenagers. It is during\nthis intermediate female adolescence that girls begin to make critical career\nchoices, which therefore makes this a key age to reinforce them and reduce the\ngender disparities in ICT. Computational thinking skills are important from a\nphilosophical point of view, since they allow us to understand the foundations\nof rational thought in a clear, easily understandable, but also inspiring and\nchallenging way. To address the gender bias in schools, one of the goals of the\nEuropean H2020 project No One Left Behind (NOLB) included integrating Pocket\nCode, a free open source app developed by the non-profit project Catrobat, into\ndifferent school lessons. Through game design, Pocket Code allows teenage girls\nto incorporate diversity and inclusiveness, as well as the ability to reflect\ntheir cultural identity, their emotions, their likes, and their ways of\ninteracting and thinking. To evaluate the impact of the use of the app in these\ncourses, we captured the results on engaging girls in design and coding\nactivities. For this paper, the authors present the data of surveys during the\nsecond cycle of the project. With a focus on female teenagers, the results\nallow us to conclude that a suitable classroom setting is significantly more\nimportant for them than the coding tool itself.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 12:53:26 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 11:37:10 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Spieler", "Bernadette", ""], ["Slany", "Wolfgang", ""]]}, {"id": "1805.04458", "submitter": "Bernadette Spieler", "authors": "Helen Boulton and Bernadette Spieler and Anja Petri and Christian\n  Schindler and Wolfgang Slany and Maria Beltran", "title": "The role of game jams in developing informal learning of computational\n  thinking: a cross-european case study", "comments": "10 pages, 4 figures, 2 tables, EduLearn 2016", "journal-ref": "EduLearn 2016", "doi": "10.21125/edulearn.2016", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper will present a cross-European experience of game jams as part of a\nHorizon 2020 funded project: No-one Left Behind (NOLB). The NOLB project was\ncreated to unlock inclusive gaming creation and experiences in formal learning\nsituations from primary to secondary level, particularly for children at risk\nof social exclusion. The project has engendered the concept of game jams,\nevents organised with the aim of designing and creating small games in a short\ntime-frame around a central theme. Game jams can support engagement with\ninformal learning beyond schools across a range of disciplines, resulting in an\nexciting experience associated with strong, positive emotions which can\nsignificantly support learning goals. This paper will disseminate experience of\ntwo cross-European game jams; the first a pilot and the second having over 95\nsubmissions from countries across Europe, America, Canada, Egypt, the\nPhilippians and India. Data collected through these games jams supports that\ncoding, designing, reflection, analysing, creating, debugging, persevering and\napplication, as well as developing computational thinking concepts such as\ndecomposition, using patterns, abstraction and evaluation. The notion of game\njams provides a paradigm for creating both formal and informal learning\nexperiences such as directed learning experience, problem-solving, hands-on\nprojects, working collaboratively, and creative invention, within a\nlearner-centred learning environment where children are creators of their own\nknowledge and learning material.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 15:54:00 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 11:16:57 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Boulton", "Helen", ""], ["Spieler", "Bernadette", ""], ["Petri", "Anja", ""], ["Schindler", "Christian", ""], ["Slany", "Wolfgang", ""], ["Beltran", "Maria", ""]]}, {"id": "1805.04461", "submitter": "Bernadette Spieler", "authors": "Bernadette Spieler and Anja Petri and Christian Schindler and Wolfgang\n  Slany and Maria Betran and Helen Boulton and Eugenio Gaeta and Jonathan Smith", "title": "Pocket Code: a mobile app for game jams to facilitate classroom learning\n  through game creation", "comments": "4 pages, 3 figures, ICGBL 2016", "journal-ref": "International Conference of Game Based Learning 2016", "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game jams are a way to create games under fast-paced conditions and certain\nconstraints. The increase in game jam events all over the world, their engaging\nand creative nature, with the aim of sharing results among players can be seen\nin the high participation rate of such events (2013: 16,705 participants from\n319 jam sites in 63 countries produced 3248 games) . This promising concept can\nbe easily transferred to a classroom setting.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 16:02:20 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 11:05:38 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Spieler", "Bernadette", ""], ["Petri", "Anja", ""], ["Schindler", "Christian", ""], ["Slany", "Wolfgang", ""], ["Betran", "Maria", ""], ["Boulton", "Helen", ""], ["Gaeta", "Eugenio", ""], ["Smith", "Jonathan", ""]]}, {"id": "1805.04462", "submitter": "Bernadette Spieler", "authors": "Anja Petri and Christian Schindler and Wolfgang Slany and Bernadette\n  Spieler and Jonathan Smith", "title": "Pocket Game Jams: a Constructionist Approach at Schools", "comments": "5 pages, MobiHCI 2015", "journal-ref": "International Conference on Human-Computer Interaction with Mobile\n  Devices and Services Adjunct 2015", "doi": "10.1145/2786567.2801610", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constructionist approach is more interested in constructing personal\nexperience than about acquiring information. It states that learning is most\neffective when building knowledge through active engagement. Experiential and\ndiscovery learning by challenges inspire creativity, and projects allow\nindependent thinking and new ways of learning information. This paper describes\nhow the \"No One Left Behind\" (NOLB) project plans to integrate this approach\ninto school curricula using two concepts. The first one is to enable students\nto create their own games with Pocket Code by using its easy-to-learn visual\nprogramming language. The second concept is to foster collaboration and\nteamwork through hands-on sessions by conducting Game Jams using Pocket Code,\nso called Pocket Game Jams. We present insights into such a Pocket Game Jam and\ngive an outlook on how we will use this concept.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 16:02:38 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 10:58:01 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Petri", "Anja", ""], ["Schindler", "Christian", ""], ["Slany", "Wolfgang", ""], ["Spieler", "Bernadette", ""], ["Smith", "Jonathan", ""]]}, {"id": "1805.04465", "submitter": "Bernadette Spieler", "authors": "Bernadette Spieler and Christian Schindler and Wolfgang Slany and\n  Olena Mashkina", "title": "App creation in schools for different curricula subjects - lesson\n  learned", "comments": "10 pages, 5 tables EduLearn 2017", "journal-ref": "EduLearn 2017 https://library.iated.org/publications/EDULEARN17", "doi": "10.21125/edulearn.2017.2315", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next generation of jobs will be characterized by an increased demand for\npeople with computational and problem solving skills. In Austria, computer\nscience topics are underrepresented in school curricula hence teaching time for\nthese topics is limited. From primary through secondary school, only a few\nopportunities exist for young students to explore programming. Furthermore,\ntoday's teachers are rarely trained in computer science, which impairs their\npotential to motivate students in these courses. Within the \"No One Left\nBehind\" (NOLB) project, teachers were supported to guide and assist their\nstudents in their learning processes by constructing ideas through game making.\nThus, students created games that referred to different subject areas by using\nthe programming tool Pocket Code, an app developed at Graz University of\nTechnology (TU-Graz). This tool helps students to take control of their own\neducation, becoming more engaged, interested, and empowered as a result. To\nensure an optimal integration of the app in diverse subjects the different\nbackgrounds (technical and non-technical) of teachers must be considered as\nwell. First, teachers were supported to use Pocket Code in the different\nsubjects in school within the feasibility study of the project. Observed\nchallenges and difficulties using the app have been gathered. Second, we\nconducted interviews with teachers and students to underpin our onsite\nobservations. As a result, it was possible to validate Pocket Codes' potential\nto be used in a diverse range of subjects. Third, we focused especially on\nthose teachers who were not technically trained to provide them with a\nframework for Pocket Code units, e.g., with the help of structured lesson plans\nand predefined templates.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 16:05:01 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 10:21:05 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Spieler", "Bernadette", ""], ["Schindler", "Christian", ""], ["Slany", "Wolfgang", ""], ["Mashkina", "Olena", ""]]}, {"id": "1805.04517", "submitter": "Bernadette Spieler", "authors": "Bernadette Spieler and Christian Schindler and Wolfgang Slany and\n  Olena Mashkina and Maria Eugenia Beltran and Helen Boulton and David Brown", "title": "Evaluation of Game Templates to support Programming Activities in\n  Schools", "comments": "10 pages, 5 figures, 4 tables European Conference of game based\n  learning (ECGBL)", "journal-ref": "11th European Conference on Game Based Learning isbn =\n  9781510850446 issn = 2049-0992", "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game creation challenges in schools potentially provide engaging,\ngoal-oriented, and interactive experiences in classes; thereby supporting the\ntransfer of knowledge for learning in a fun and pedagogic manner. A key element\nof the ongoing European project No One Left Behind (NOLB) is to integrate a\ngame-making teaching framework (GMTF) into the educational app Pocket Code.\nPocket Code allows learners to create programs in a visual Lego-style way to\nfacilitate learning how to code at secondary high schools. The concept of the\nNOLB GMTF is based on principles of the Universal Design for Learning (UDL)\nmodel. This framework provides a coherent approach to learning and teaching by\nintegrating leisure oriented gaming methods into multi-discipline curricula.\nOne output of this framework is the integration of game-based methods via game\ntemplates that refer to didactical scenarios that include a refined set of\ngenres, assets, rules, challenges, and strategies. These templates allows: 1)\nteachers to start with a well-structured program, and 2) pupils to add content\nand adjust the code to integrate their own ideas. During the project game\ngenres such as adventure, action, and quiz, as well as rewards or victory point\nmechanisms, have been embedded into different subjects, e.g., science,\nmathematics, and arts. The insights gained during the class hours were used to\ngenerate 13 game templates, which are integrated in Create@School (a new\nversion of the Pocket Code app which targets schools). To test the efficiency\nof these templates, user experience (UX) tests were conducted during classes to\ncompare games created by pupils who used templates and those who started to\ncreate a game from scratch. Preliminary results showed that these templates\nallow learners to focus on subject-relevant problem solving activities rather\nthan on understanding the functionality of the app.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 15:51:18 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 10:35:45 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Spieler", "Bernadette", ""], ["Schindler", "Christian", ""], ["Slany", "Wolfgang", ""], ["Mashkina", "Olena", ""], ["Beltran", "Maria Eugenia", ""], ["Boulton", "Helen", ""], ["Brown", "David", ""]]}, {"id": "1805.04594", "submitter": "Arunesh Mathur", "authors": "Arunesh Mathur, Nathan Malkin, Marian Harbach, Eyal Peer, Serge\n  Egelman", "title": "Quantifying Users' Beliefs about Software Updates", "comments": "7 pages, 3 figures, NDSS Workshop on Usable Security (USEC 2018)", "journal-ref": "NDSS Workshop on Usable Security, San Diego (USEC 2018)", "doi": "10.14722/usec.2018.23036", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software updates are critical to the performance, compatibility, and security\nof software systems. However, users do not always install updates, leaving\ntheir machines vulnerable to attackers' exploits. While recent studies have\nhighlighted numerous reasons why users ignore updates, little is known about\nhow prevalent each of these beliefs is. Gaining a better understanding of the\nprevalence of each belief may help software designers better target their\nefforts in understanding what specific user concerns to address when developing\nand deploying software updates. In our study, we performed a survey to quantify\nthe prevalence of users' reasons for not updating uncovered by previous\nstudies. We used this data to derive three factors underlying these beliefs:\nupdate costs, update necessity, and update risks. Based on our results, we\nprovide recommendations for how software developers can better improve users'\nsoftware updating experiences, thereby increasing compliance and, with it,\nsecurity.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 21:20:09 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Mathur", "Arunesh", ""], ["Malkin", "Nathan", ""], ["Harbach", "Marian", ""], ["Peer", "Eyal", ""], ["Egelman", "Serge", ""]]}, {"id": "1805.04737", "submitter": "Dongrui Wu", "authors": "Dongrui Wu, Vernon J. Lawhern, Stephen Gordon, Brent J. Lance,\n  Chin-Teng Lin", "title": "Offline EEG-Based Driver Drowsiness Estimation Using Enhanced Batch-Mode\n  Active Learning (EBMAL) for Regression", "comments": null, "journal-ref": "IEEE Int'l. Conf. on Systems, Man and Cybernetics, pp. 730-736,\n  Budapest, Hungary, 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many important regression problems in real-world brain-computer\ninterface (BCI) applications, e.g., driver drowsiness estimation from EEG\nsignals. This paper considers offline analysis: given a pool of unlabeled EEG\nepochs recorded during driving, how do we optimally select a small number of\nthem to label so that an accurate regression model can be built from them to\nlabel the rest? Active learning is a promising solution to this problem, but\ninterestingly, to our best knowledge, it has not been used for regression\nproblems in BCI so far. This paper proposes a novel enhanced batch-mode active\nlearning (EBMAL) approach for regression, which improves upon a baseline active\nlearning algorithm by increasing the reliability, representativeness and\ndiversity of the selected samples to achieve better regression performance. We\nvalidate its effectiveness using driver drowsiness estimation from EEG signals.\nHowever, EBMAL is a general approach that can also be applied to many other\noffline regression problems beyond BCI.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 15:36:05 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Wu", "Dongrui", ""], ["Lawhern", "Vernon J.", ""], ["Gordon", "Stephen", ""], ["Lance", "Brent J.", ""], ["Lin", "Chin-Teng", ""]]}, {"id": "1805.04740", "submitter": "Dongrui Wu", "authors": "Dongrui Wu, Vernon J. Lawhern, Stephen Gordon, Brent J. Lance,\n  Chin-Teng Lin", "title": "Agreement Rate Initialized Maximum Likelihood Estimator for Ensemble\n  Classifier Aggregation and Its Application in Brain-Computer Interface", "comments": null, "journal-ref": "IEEE Int'l. Conf. on Systems, Man and Cybernetics, pp. 724-729,\n  Budapest, Hungary, 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning is a powerful approach to construct a strong learner from\nmultiple base learners. The most popular way to aggregate an ensemble of\nclassifiers is majority voting, which assigns a sample to the class that most\nbase classifiers vote for. However, improved performance can be obtained by\nassigning weights to the base classifiers according to their accuracy. This\npaper proposes an agreement rate initialized maximum likelihood estimator\n(ARIMLE) to optimally fuse the base classifiers. ARIMLE first uses a simplified\nagreement rate method to estimate the classification accuracy of each base\nclassifier from the unlabeled samples, then employs the accuracies to\ninitialize a maximum likelihood estimator (MLE), and finally uses the\nexpectation-maximization algorithm to refine the MLE. Extensive experiments on\nvisually evoked potential classification in a brain-computer interface\napplication show that ARIMLE outperforms majority voting, and also achieves\nbetter or comparable performance with several other state-of-the-art classifier\ncombination approaches.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 15:43:36 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Wu", "Dongrui", ""], ["Lawhern", "Vernon J.", ""], ["Gordon", "Stephen", ""], ["Lance", "Brent J.", ""], ["Lin", "Chin-Teng", ""]]}, {"id": "1805.04986", "submitter": "Jing Jin", "authors": "Zhaoyang Qiu, Shugeng Chen, Ian Daly, Jie Jia, Xingyu Wang, Jing Jin", "title": "BCI-Based Strategies on Stroke Rehabilitation with Avatar and FES\n  Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stroke is the leading cause of serious and long-term disability worldwide.\nSome studies have shown that motor imagery (MI) based BCI has a positive effect\nin poststroke rehabilitation. It could help patients promote the reorganization\nprocesses in the damaged brain regions. However, offline motor imagery and\nconventional online motor imagery with feedback (such as rewarding sounds and\nmovements of an avatar) could not reflect the true intention of the patients.\nIn this study, both virtual limbs and functional electrical stimulation (FES)\nwere used as feedback to provide patients a closed-loop sensorimotor\nintegration for motor rehabilitation. The FES system would activate if the user\nwas imagining hand movement of instructed side. Ten stroke patients (7 male,\naged 22-70 years, mean 49.5+-15.1) were involved in this study. All of them\nparticipated in BCI-FES rehabilitation training for 4 weeks.The average motor\nimagery accuracies of the ten patients in the last week were 71.3%, which has\nimproved 3% than that in the first week. Five patients' Fugl-Meyer Assessment\n(FMA) scores have been raised. Patient 6, who has have suffered from stroke\nover two years, achieved the greatest improvement after rehabilitation training\n(pre FMA: 20, post FMA: 35). In the aspect of brain patterns, the active\npatterns of the five patients gradually became centralized and shifted to\nsensorimotor areas (channel C3 and C4) and premotor area (channel FC3 and\nFC4).In this study, motor imagery based BCI and FES system were combined to\nprovided stoke patients with a closed-loop sensorimotor integration for motor\nrehabilitation. Result showed evidences that the BCI-FES system is effective in\nrestoring upper extremities motor function in stroke. In future work, more\ncases are needed to demonstrate its superiority over conventional therapy and\nexplore the potential role of MI in poststroke rehabilitation.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 02:06:02 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Qiu", "Zhaoyang", ""], ["Chen", "Shugeng", ""], ["Daly", "Ian", ""], ["Jia", "Jie", ""], ["Wang", "Xingyu", ""], ["Jin", "Jing", ""]]}, {"id": "1805.05200", "submitter": "Shovan Maity", "authors": "Shovan Maity, Mingxuan He, Mayukh Nath, Debayan Das, Baibhab\n  Chatterjee, Shreyas Sen", "title": "BioPhysical Modeling, Characterization and Optimization of\n  Electro-Quasistatic Human Body Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Body Communication (HBC) has emerged as an alternative to radio wave\ncommunication for connecting low power, miniaturized wearable and implantable\ndevices in, on and around the human body which uses the human body as the\ncommunication channel. Previous studies characterizing the human body channel\nhas reported widely varying channel response much of which has been attributed\nto the variation in measurement setup. This calls for the development of a\nunifying bio physical model of HBC supported by in depth analysis and an\nunderstanding of the effect of excitation, termination modality on HBC\nmeasurements. This paper characterizes the human body channel up to 1MHz\nfrequency to evaluate it as a medium for broadband communication. A lumped bio\nphysical model of HBC is developed, supported by experimental validations that\nprovides insight into some of the key discrepancies found in previous studies.\nVoltage loss measurements are carried out both with an oscilloscope and a\nminiaturized wearable prototype to capture the effects of non common ground.\nResults show that the channel loss is strongly dependent on the termination\nimpedance at the receiver end, with up to 4dB variation in average loss for\ndifferent termination in an oscilloscope and an additional 9 dB channel loss\nwith wearable prototype compared to an oscilloscope measurement. The measured\nchannel response with capacitive termination reduces low frequency loss and\nallows flat band transfer function down to 13 KHz, establishing the human body\nas a broadband communication channel. Analysis of the measured results and the\nsimulation model shows that (1) high impedance (2) capacitive termination\nshould be used at the receiver end for accurate voltage mode loss measurements\nof the HBC channel at low frequencies.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 14:42:35 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Maity", "Shovan", ""], ["He", "Mingxuan", ""], ["Nath", "Mayukh", ""], ["Das", "Debayan", ""], ["Chatterjee", "Baibhab", ""], ["Sen", "Shreyas", ""]]}, {"id": "1805.05287", "submitter": "Zhibing Zhao", "authors": "Zhibing Zhao, Haoming Li, Junming Wang, Jeffrey Kephart, Nicholas\n  Mattei, Hui Su, Lirong Xia", "title": "A Cost-Effective Framework for Preference Elicitation and Aggregation", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a cost-effective framework for preference elicitation and\naggregation under the Plackett-Luce model with features. Given a budget, our\nframework iteratively computes the most cost-effective elicitation questions in\norder to help the agents make a better group decision.\n  We illustrate the viability of the framework with experiments on Amazon\nMechanical Turk, which we use to estimate the cost of answering different types\nof elicitation questions. We compare the prediction accuracy of our framework\nwhen adopting various information criteria that evaluate the expected\ninformation gain from a question. Our experiments show carefully designed\ninformation criteria are much more efficient, i.e., they arrive at the correct\nanswer using fewer queries, than randomly asking questions given the budget\nconstraint.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 16:57:36 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 13:50:56 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Zhao", "Zhibing", ""], ["Li", "Haoming", ""], ["Wang", "Junming", ""], ["Kephart", "Jeffrey", ""], ["Mattei", "Nicholas", ""], ["Su", "Hui", ""], ["Xia", "Lirong", ""]]}, {"id": "1805.05345", "submitter": "Cristian Danescu-Niculescu-Mizil", "authors": "Justine Zhang, Jonathan P. Chang, Cristian Danescu-Niculescu-Mizil,\n  Lucas Dixon, Yiqing Hua, Nithum Thain, Dario Taraborelli", "title": "Conversations Gone Awry: Detecting Early Signs of Conversational Failure", "comments": "To appear in the Proceedings of ACL 2018, 15 pages, 1 figure. Data,\n  quiz, code and additional information at\n  http://www.cs.cornell.edu/~cristian/Conversations_gone_awry.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges online social systems face is the prevalence of\nantisocial behavior, such as harassment and personal attacks. In this work, we\nintroduce the task of predicting from the very start of a conversation whether\nit will get out of hand. As opposed to detecting undesirable behavior after the\nfact, this task aims to enable early, actionable prediction at a time when the\nconversation might still be salvaged.\n  To this end, we develop a framework for capturing pragmatic devices---such as\npoliteness strategies and rhetorical prompts---used to start a conversation,\nand analyze their relation to its future trajectory. Applying this framework in\na controlled setting, we demonstrate the feasibility of detecting early warning\nsigns of antisocial behavior in online discussions.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 18:00:03 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Zhang", "Justine", ""], ["Chang", "Jonathan P.", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""], ["Dixon", "Lucas", ""], ["Hua", "Yiqing", ""], ["Thain", "Nithum", ""], ["Taraborelli", "Dario", ""]]}, {"id": "1805.05543", "submitter": "Aniket Bera", "authors": "Aniket Bera, Tanmay Randhavane, Emily Kubin, Austin Wang, Dinesh\n  Manocha and Kurt Gray", "title": "The Socially Invisible Robot: Navigation in the Social World using Robot\n  Entitativity", "comments": null, "journal-ref": "Proceedings of the IEEE/RSJ International Conference on\n  Intelligent Robots and Systems 2018", "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a real-time, data-driven algorithm to enhance the\nsocial-invisibility of robots within crowds. Our approach is based on prior\npsychological research, which reveals that people notice\nand--importantly--react negatively to groups of social actors when they have\nhigh entitativity, moving in a tight group with similar appearances and\ntrajectories. In order to evaluate that behavior, we performed a user study to\ndevelop navigational algorithms that minimize entitativity. This study\nestablishes a mapping between emotional reactions and multi-robot trajectories\nand appearances and further generalizes the finding across various\nenvironmental conditions. We demonstrate the applicability of our entitativity\nmodeling for trajectory computation for active surveillance and dynamic\nintervention in simulated robot-human interaction scenarios. Our approach\nempirically shows that various levels of entitative robots can be used to both\navoid and influence pedestrians while not eliciting strong emotional reactions,\ngiving multi-robot systems socially-invisibility.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 03:11:12 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 15:10:00 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Bera", "Aniket", ""], ["Randhavane", "Tanmay", ""], ["Kubin", "Emily", ""], ["Wang", "Austin", ""], ["Manocha", "Dinesh", ""], ["Gray", "Kurt", ""]]}, {"id": "1805.05781", "submitter": "Dongrui Wu", "authors": "Dongrui Wu", "title": "Active Semi-supervised Transfer Learning (ASTL) for Offline BCI\n  Calibration", "comments": "IEEE Int'l. Conf. on Systems, Man and Cybernetics, Banff, Canada,\n  2017. arXiv admin note: substantial text overlap with arXiv:1702.02897", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-trial classification of event-related potentials in\nelectroencephalogram (EEG) signals is a very important paradigm of\nbrain-computer interface (BCI). Because of individual differences, usually some\nsubject-specific calibration data are required to tailor the classifier for\neach subject. Transfer learning has been extensively used to reduce such\ncalibration data requirement, by making use of auxiliary data from\nsimilar/relevant subjects/tasks. However, all previous research assumes that\nall auxiliary data have been labeled. This paper considers a more general\nscenario, in which part of the auxiliary data could be unlabeled. We propose\nactive semi-supervised transfer learning (ASTL) for offline BCI calibration,\nwhich integrates active learning, semi-supervised learning, and transfer\nlearning. Using a visual evoked potential oddball task and three different EEG\nheadsets, we demonstrate that ASTL can achieve consistently good performance\nacross subjects and headsets, and it outperforms some state-of-the-art\napproaches in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 15:27:23 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Wu", "Dongrui", ""]]}, {"id": "1805.06031", "submitter": "Noah Apthorpe", "authors": "Noah Apthorpe, Yan Shvartzshnaider, Arunesh Mathur, Dillon Reisman,\n  Nick Feamster", "title": "Discovering Smart Home Internet of Things Privacy Norms Using Contextual\n  Integrity", "comments": "23 pages, 5 figures, 3 tables", "journal-ref": "Proceedings of the ACM on Interactive, Mobile, Wearable and\n  Ubiquitous Technologies, Vol. 2, No. 2, Article 59. June 2018", "doi": "10.1145/3214262", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of Internet of Things (IoT) devices for consumer \"smart\"\nhomes raises concerns about user privacy. We present a survey method based on\nthe Contextual Integrity (CI) privacy framework that can quickly and\nefficiently discover privacy norms at scale. We apply the method to discover\nprivacy norms in the smart home context, surveying 1,731 American adults on\nAmazon Mechanical Turk. For $2,800 and in less than six hours, we measured the\nacceptability of 3,840 information flows representing a combinatorial space of\nsmart home devices sending consumer information to first and third-party\nrecipients under various conditions. Our results provide actionable\nrecommendations for IoT device manufacturers, including design best practices\nand instructions for adopting our method for further research.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 20:52:49 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Apthorpe", "Noah", ""], ["Shvartzshnaider", "Yan", ""], ["Mathur", "Arunesh", ""], ["Reisman", "Dillon", ""], ["Feamster", "Nick", ""]]}, {"id": "1805.06242", "submitter": "Chandrakant Bothe", "authors": "Chandrakant Bothe, Sven Magg, Cornelius Weber and Stefan Wermter", "title": "Conversational Analysis using Utterance-level Attention-based\n  Bidirectional Recurrent Neural Networks", "comments": "Proceedings of INTERSPEECH 2018", "journal-ref": null, "doi": "10.21437/Interspeech.2018-2527", "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches for dialogue act recognition have shown that context from\npreceding utterances is important to classify the subsequent one. It was shown\nthat the performance improves rapidly when the context is taken into account.\nWe propose an utterance-level attention-based bidirectional recurrent neural\nnetwork (Utt-Att-BiRNN) model to analyze the importance of preceding utterances\nto classify the current one. In our setup, the BiRNN is given the input set of\ncurrent and preceding utterances. Our model outperforms previous models that\nuse only preceding utterances as context on the used corpus. Another\ncontribution of the article is to discover the amount of information in each\nutterance to classify the subsequent one and to show that context-based\nlearning not only improves the performance but also achieves higher confidence\nin the classification. We use character- and word-level features to represent\nthe utterances. The results are presented for character and word feature\nrepresentations and as an ensemble model of both representations. We found that\nwhen classifying short utterances, the closest preceding utterances contributes\nto a higher degree.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 10:51:56 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 12:11:59 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Bothe", "Chandrakant", ""], ["Magg", "Sven", ""], ["Weber", "Cornelius", ""], ["Wermter", "Stefan", ""]]}, {"id": "1805.06270", "submitter": "Ali Shafti", "authors": "Ali Shafti, Ahmad Ataka, Beatriz Urbistondo Lazpita, Ali Shiva, Helge\n  A. Wurdemann, Kaspar Althoefer", "title": "Real-time Robot-assisted Ergonomics", "comments": "6 pages, accepted and to be presented at IEEE ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach in human robot interaction driven by\nergonomics. With a clear focus on optimising ergonomics, the approach proposed\nhere continuously observes a human user's posture and by invoking appropriate\ncooperative robot movements, the user's posture is, whenever required, brought\nback to an ergonomic optimum. Effectively, the new protocol optimises the\nhuman-robot relative position and orientation as a function of human\nergonomics. An RGB-D camera is used to calculate and monitor human joint angles\nin real-time and to determine the current ergonomics state. A total of 6 main\ncauses of low ergonomic states are identified, leading to 6 universal robot\nresponses to allow the human to return to an optimal ergonomics state. The\nalgorithmic framework identifies these 6 causes and controls the cooperating\nrobot to always adapt the environment (e.g. change the pose of the workpiece)\nin a way that is ergonomically most comfortable for the interacting user.\nHence, human-robot interaction is continuously re-evaluated optimizing\nergonomics states. The approach is validated through an experimental study,\nbased on established ergonomic methods and their adaptation for real-time\napplication. The study confirms improved ergonomics using the new approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 12:26:37 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 10:30:44 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 11:28:11 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Shafti", "Ali", ""], ["Ataka", "Ahmad", ""], ["Lazpita", "Beatriz Urbistondo", ""], ["Shiva", "Ali", ""], ["Wurdemann", "Helge A.", ""], ["Althoefer", "Kaspar", ""]]}, {"id": "1805.06280", "submitter": "Chandrakant Bothe", "authors": "Chandrakant Bothe, Cornelius Weber, Sven Magg, and Stefan Wermter", "title": "A Context-based Approach for Dialogue Act Recognition using Simple\n  Recurrent Neural Networks", "comments": "Proceedings of the Eleventh International Conference on Language\n  Resources and Evaluation (LREC 2018)", "journal-ref": null, "doi": null, "report-no": "id:525, pages:1952--1957", "categories": "cs.CL cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue act recognition is an important part of natural language\nunderstanding. We investigate the way dialogue act corpora are annotated and\nthe learning approaches used so far. We find that the dialogue act is\ncontext-sensitive within the conversation for most of the classes.\nNevertheless, previous models of dialogue act classification work on the\nutterance-level and only very few consider context. We propose a novel\ncontext-based learning method to classify dialogue acts using a character-level\nlanguage model utterance representation, and we notice significant improvement.\nWe evaluate this method on the Switchboard Dialogue Act corpus, and our results\nshow that the consideration of the preceding utterances as a context of the\ncurrent utterance improves dialogue act detection.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 12:58:18 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Bothe", "Chandrakant", ""], ["Weber", "Cornelius", ""], ["Magg", "Sven", ""], ["Wermter", "Stefan", ""]]}, {"id": "1805.06284", "submitter": "Milan Jain", "authors": "Milan Jain", "title": "Energy-Efficient Thermostats for Room-Level Air Conditioning", "comments": "8 pages, 5 figures, SmartObjects '18, in conjunction with CHI '18,\n  Montreal, Canada", "journal-ref": null, "doi": null, "report-no": "Vol-2082/paper-2", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Room-level air conditioners (also referred as ACs) consume a significant\nproportion of total energy in residential and small-scale commercial buildings.\nIn a typical AC, occupants specify their comfort requirements by manually\nsetting the desired temperature on the thermostat. Though commercial\nthermostats (such as Tado) provide basic energy-saving features, they neither\nconsider the influence of external factors (such as weather) to set the\nthermostat temperature nor offer advanced features such as monitoring the\nfitness level of AC. In this paper, we discuss grey-box modeling techniques to\nenhance existing thermostats for energy-efficient control of the ACs and\nprovide actionable and corrective feedback to the users. Our study indicates\nthat the enhancements can reduce occupants' discomfort by 23% when maximising\nthe user experience, and reduce AC energy consumption by 26% during the\npower-saving mode.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 04:58:50 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Jain", "Milan", ""]]}, {"id": "1805.06427", "submitter": "Vinay Jayaram", "authors": "Vinay Jayaram, Alexandre Barachant", "title": "MOABB: Trustworthy algorithm benchmarking for BCIs", "comments": null, "journal-ref": null, "doi": "10.1088/1741-2552/aadea0", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BCI algorithm development has long been hampered by two major issues: small\nsample sets and a lack of reproducibility. We offer a solution to both of these\nproblems via a software suite that streamlines both the issues of finding and\npreprocessing data in a reliable manner, as well as that of using a consistent\ninterface for machine learning methods. By building on recent advances in\nsoftware for signal analysis implemented in the MNE toolkit, and the unified\nframework for machine learning offered by the scikit-learn project, we offer a\nsystem that can improve BCI algorithm development. This system is fully\nopen-source under the BSD licence and available at\nhttps://github.com/NeuroTechX/moabb. To validate our efforts, we analyze a set\nof state-of-the-art decoding algorithms across 12 open access datasets, with\nover 250 subjects. Our analysis confirms that different datasets can result in\nvery different results for identical processing pipelines, highlighting the\nneed for trustworthy algorithm benchmarking in the field of BCIs, and further\nthat many previously validated methods do not hold up when applied across\ndifferent datasets, which has wide-reaching implications for practical BCIs.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 17:02:50 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Jayaram", "Vinay", ""], ["Barachant", "Alexandre", ""]]}, {"id": "1805.06542", "submitter": "Elissa Marie Redmiles", "authors": "Elissa M. Redmiles, Michelle L. Mazurek, John P. Dickerson", "title": "Dancing Pigs or Externalities? Measuring the Rationality of Security\n  Decisions", "comments": null, "journal-ref": "2018 ACM Conference on Economics and Computation", "doi": "10.1145/3219166.3219185", "report-no": null, "categories": "cs.GT cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately modeling human decision-making in security is critical to thinking\nabout when, why, and how to recommend that users adopt certain secure\nbehaviors. In this work, we conduct behavioral economics experiments to model\nthe rationality of end-user security decision-making in a realistic online\nexperimental system simulating a bank account. We ask participants to make a\nfinancially impactful security choice, in the face of transparent risks of\naccount compromise and benefits offered by an optional security behavior\n(two-factor authentication). We measure the cost and utility of adopting the\nsecurity behavior via measurements of time spent executing the behavior and\nestimates of the participant's wage. We find that more than 50% of our\nparticipants made rational (e.g., utility optimal) decisions, and we find that\nparticipants are more likely to behave rationally in the face of higher risk.\nAdditionally, we find that users' decisions can be modeled well as a function\nof past behavior (anchoring effects), knowledge of costs, and to a lesser\nextent, users' awareness of risks and context (R2=0.61). We also find evidence\nof endowment effects, as seen in other areas of economic and psychological\ndecision-science literature, in our digital-security setting. Finally, using\nour data, we show theoretically that a \"one-size-fits\"-all emphasis on security\ncan lead to market losses, but that adoption by a subset of users with higher\nrisks or lower costs can lead to market gains.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 22:22:37 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Redmiles", "Elissa M.", ""], ["Mazurek", "Michelle L.", ""], ["Dickerson", "John P.", ""]]}, {"id": "1805.06606", "submitter": "Woo Yong Choi", "authors": "Chan Woo Lee, Kyu Ye Song, Jihoon Jeong, Woo Yong Choi", "title": "Convolutional Attention Networks for Multimodal Emotion Recognition from\n  Speech and Text Data", "comments": "Inaccurate scientific facts listed in document", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition has become a popular topic of interest, especially in the\nfield of human computer interaction. Previous works involve unimodal analysis\nof emotion, while recent efforts focus on multi-modal emotion recognition from\nvision and speech. In this paper, we propose a new method of learning about the\nhidden representations between just speech and text data using convolutional\nattention networks. Compared to the shallow model which employs simple\nconcatenation of feature vectors, the proposed attention model performs much\nbetter in classifying emotion from speech and text data contained in the\nCMU-MOSEI dataset.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 05:51:00 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 07:04:40 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Lee", "Chan Woo", ""], ["Song", "Kyu Ye", ""], ["Jeong", "Jihoon", ""], ["Choi", "Woo Yong", ""]]}, {"id": "1805.06652", "submitter": "Jonny O'Dwyer", "authors": "Jonny O'Dwyer, Niall Murray, Ronan Flynn", "title": "Affective computing using speech and eye gaze: a review and bimodal\n  system proposal for continuous affect prediction", "comments": "Submitted to International Journal of Human-Computer Studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech has been a widely used modality in the field of affective computing.\nRecently however, there has been a growing interest in the use of multi-modal\naffective computing systems. These multi-modal systems incorporate both verbal\nand non-verbal features for affective computing tasks. Such multi-modal\naffective computing systems are advantageous for emotion assessment of\nindividuals in audio-video communication environments such as teleconferencing,\nhealthcare, and education. From a review of the literature, the use of eye gaze\nfeatures extracted from video is a modality that has remained largely\nunexploited for continuous affect prediction. This work presents a review of\nthe literature within the emotion classification and continuous affect\nprediction sub-fields of affective computing for both speech and eye gaze\nmodalities. Additionally, continuous affect prediction experiments using speech\nand eye gaze modalities are presented. A baseline system is proposed using open\nsource software, the performance of which is assessed on a publicly available\naudio-visual corpus. Further system performance is assessed in a cross-corpus\nand cross-lingual experiment. The experimental results suggest that eye gaze is\nan effective supportive modality for speech when used in a bimodal continuous\naffect prediction system. The addition of eye gaze to speech in a simple\nfeature fusion framework yields a prediction improvement of 6.13% for valence\nand 1.62% for arousal.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 08:34:49 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["O'Dwyer", "Jonny", ""], ["Murray", "Niall", ""], ["Flynn", "Ronan", ""]]}, {"id": "1805.07064", "submitter": "Jeremy Frey", "authors": "Emmanuel Christophe (PRISM), J\\'er\\'emy Frey, Richard\n  Kronland-Martinet (PRISM), Jean-Arthur Micoulaud-Franchi, Jelena Mladenovi\\'c\n  (Potioc), Ga\\\"elle Mougin (PG, HCP), Jean Vion-Dury (PRISM), Solvi Ystad\n  (PRISM), Mitsuko Aramaki (PRISM)", "title": "Evaluation of a congruent auditory feedback for Motor Imagery BCI", "comments": null, "journal-ref": "International BCI meeting, May 2018, Asilomar, United States.\n  http://bcisociety.org/", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a feedback that helps participants to achieve higher performances\nis an important concern in brain-computer interface (BCI) research. In a pilot\nstudy, we demonstrate how a congruent auditory feedback could improve\nclassification in a electroencephalography (EEG) motor imagery BCI. This is a\npromising result for creating alternate feedback modality.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 06:35:21 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 08:26:23 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Christophe", "Emmanuel", "", "PRISM"], ["Frey", "J\u00e9r\u00e9my", "", "PRISM"], ["Kronland-Martinet", "Richard", "", "PRISM"], ["Micoulaud-Franchi", "Jean-Arthur", "", "Potioc"], ["Mladenovi\u0107", "Jelena", "", "Potioc"], ["Mougin", "Ga\u00eblle", "", "PG, HCP"], ["Vion-Dury", "Jean", "", "PRISM"], ["Ystad", "Solvi", "", "PRISM"], ["Aramaki", "Mitsuko", "", "PRISM"]]}, {"id": "1805.07233", "submitter": "Kaixuan Chen", "authors": "Kaixuan Chen, Lina Yao, Xianzhi Wang, Dalin Zhang, Tao Gu, Zhiwen Yu,\n  Zheng Yang", "title": "Interpretable Parallel Recurrent Neural Networks with Convolutional\n  Attentions for Multi-Modality Activity Modeling", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.07661", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal features play a key role in wearable sensor-based human activity\nrecognition (HAR). Selecting the most salient features adaptively is a\npromising way to maximize the effectiveness of multimodal sensor data. In this\nregard, we propose a \"collect fully and select wisely\" principle as well as an\ninterpretable parallel recurrent model with convolutional attentions to improve\nthe recognition performance. We first collect modality features and the\nrelations between each pair of features to generate activity frames, and then\nintroduce an attention mechanism to select the most prominent regions from\nactivity frames precisely. The selected frames not only maximize the\nutilization of valid features but also reduce the number of features to be\ncomputed effectively. We further analyze the accuracy and interpretability of\nthe proposed model based on extensive experiments. The results show that our\nmodel achieves competitive performance on two benchmarked datasets and works\nwell in real life scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 02:43:02 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Chen", "Kaixuan", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Zhang", "Dalin", ""], ["Gu", "Tao", ""], ["Yu", "Zhiwen", ""], ["Yang", "Zheng", ""]]}, {"id": "1805.07343", "submitter": "Rommel Salas", "authors": "Cesar Rommel Salas", "title": "The impact of binaural white noise with oscillations of 100 to 750hz in\n  the short-term visual working memory and the reactivity of alpha and beta\n  cerebral waves", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  According to some researchers, noise is typically conceived as a detrimental\nfactor in cognitive performance affecting perception, decision making, and\nmotor function. However, in recent studies it is associated with white noise\nwith concentration and calm, therefore, this research seeks to establish the\nimpact of binaural white noise on the performance of short-term visual and\nworking memory, the alpha - beta brain activity, attention - meditation,\nthrough the use of two auditory stimuli with frequency ranges of (100 to 450hz)\nand (100 to 750hz). This study was conducted in the city of Montes Claros, the\nRepublic of Brazil, where seven participants were evaluated (n = 7) with an\naverage age of 36.71, and two age groups (GP1) 21 to 30 and (GP2) 41 50, with\nuniversity studies. Within the experimental process, the short-term visual\nmemory tests were performed using the cognitive assessment battery CAB of\nCogniFit, and the recording of brain activities through the use of monopolar\nelectroencephalogram and the eSense algorithms. With the results obtained and\nthrough the use of statistical tests, we can infer that the binaural white\nnoise with oscillations of 100 to 750 Hz contributed to the performance of\nvisual work memory in the short term\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 13:57:41 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Salas", "Cesar Rommel", ""]]}, {"id": "1805.07793", "submitter": "Christoph Anderson", "authors": "Christoph Anderson, Clara Heissler, Sandra Ohly, Klaus David", "title": "Assessment of Social Roles for Interruption Management: A New Concept in\n  the Field of Interruptibility", "comments": "6 pages, ACM International Joint Conference on Pervasive and\n  Ubiquitous Computing: Adjunct", "journal-ref": "Proceedings of the 2016 ACM International Joint Conference on\n  Pervasive and Ubiquitous Computing: Adjunct, Heidelberg, Germany, 2016, pp.\n  1530-1535", "doi": "10.1145/2968219.2968544", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining and identifying opportune moments for interruptions is a\nchallenging task in Ubiquitous Computing and Human-Computer-Interaction. The\ncurrent state-of-the-art approaches do this by identifying breakpoints either\nin user tasks, activities or by processing social relationships and contents of\ninterruptions. However, from a psychological perspective, not all of these\nbreakpoints represent opportune moments for interruptions. In this paper, we\npropose a new concept in the field of interruptibility. The concept is based on\nrole theory and psychological interruption research. In particular, we argue\nthat social roles which define sets of norms, expectations, rules and\nbehaviours can provide useful information about the user's current context that\ncan be used to enhance interruption management systems. Based on this concept,\nwe propose a prototype system architecture that uses social roles to detect\nopportune moments for interruptions.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 16:37:22 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Anderson", "Christoph", ""], ["Heissler", "Clara", ""], ["Ohly", "Sandra", ""], ["David", "Klaus", ""]]}, {"id": "1805.07888", "submitter": "Weixuan Chen", "authors": "Weixuan Chen, Daniel McDuff", "title": "DeepPhys: Video-Based Physiological Measurement Using Convolutional\n  Attention Networks", "comments": "Accepted paper at ECCV 2018. 16 pages, 3 figures, supplementary\n  materials in the ancillary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-contact video-based physiological measurement has many applications in\nhealth care and human-computer interaction. Practical applications require\nmeasurements to be accurate even in the presence of large head rotations. We\npropose the first end-to-end system for video-based measurement of heart and\nbreathing rate using a deep convolutional network. The system features a new\nmotion representation based on a skin reflection model and a new attention\nmechanism using appearance information to guide motion estimation, both of\nwhich enable robust measurement under heterogeneous lighting and major motions.\nOur approach significantly outperforms all current state-of-the-art methods on\nboth RGB and infrared video datasets. Furthermore, it allows spatial-temporal\ndistributions of physiological signals to be visualized via the attention\nmechanism.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 04:42:42 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 19:25:30 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Chen", "Weixuan", ""], ["McDuff", "Daniel", ""]]}, {"id": "1805.07907", "submitter": "Joy Bose", "authors": "Kushal Singla, Joy Bose", "title": "IoT2Vec: Identification of Similar IoT Devices via Activity Footprints", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.NE cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a smart home or smart office environment with a number of IoT\ndevices connected and passing data between one another. The footprints of the\ndata transferred can provide valuable information about the devices, which can\nbe used to (a) identify the IoT devices and (b) in case of failure, to identify\nthe correct replacements for these devices. In this paper, we generate the\nembeddings for IoT devices in a smart home using Word2Vec, and explore the\npossibility of having a similar concept for IoT devices, aka IoT2Vec. These\nembeddings can be used in a number of ways, such as to find similar devices in\nan IoT device store, or as a signature of each type of IoT device. We show\nresults of a feasibility study on the CASAS dataset of IoT device activity\nlogs, using our method to identify the patterns in embeddings of various types\nof IoT devices in a household.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 06:31:52 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Singla", "Kushal", ""], ["Bose", "Joy", ""]]}, {"id": "1805.08014", "submitter": "Birgitta Dresp-Langley", "authors": "Chiara Silvestri, Rene Motro, Bernard Maurin, Birgitta Dresp-Langley", "title": "Visual spatial learning of complex object morphologies through\n  interaction with virtual and real-world data", "comments": null, "journal-ref": "2010, Design Studies, 31, 363-381", "doi": "10.1016/j.destud.2010.03.001", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conceptual design relies on extensive manipulation of morphological\nproperties of real or virtual objects.This study investigates the nature of the\nperceptual information that could be retrieved from different representation\nmodalities to reproduce structural properties of a complex object by drawing .\nThe abstract and complex object (tensegrity simplex) was presented to two study\npopulations (design experts/architects and non-experts) in three different\nrepresentation modalities (2D image view explored visually only, digital 3D\nmodel explored visually using a computer mouse, the real object explored\nvisually and manually. After viewing and exploring, observers had to draw the\nmost critical parts of the structure by hand into a 2D reference frame. The\nresults reveal a considerable performance advantage of digital 3D model\nexploration compared with real-world 3D object manipulation in the expert\npopulation.The results are discussed in terms of the specific nature of\nmorphological cues made available through the different representation\nmodalities.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 12:26:02 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Silvestri", "Chiara", ""], ["Motro", "Rene", ""], ["Maurin", "Bernard", ""], ["Dresp-Langley", "Birgitta", ""]]}, {"id": "1805.08367", "submitter": "Kriti Nelavelli", "authors": "Kriti Nelavelli, Thomas Ploetz", "title": "Adaptive App Design by Detecting Handedness", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taller and sleeker smartphone devices are becoming the new norm. More screen\nspace and very responsive touchscreens have made for enjoyable experiences\navailable to us at all times. However, after years of interacting with smaller,\nportable devices, we still try to use these large smartphones on the go, and do\nnot want to change how, where, and when we interact with them. The older\ndevices were easier to use with one hand, when mobile. Now, with bigger\ndevices, users have trouble accessing all parts of the screen with one hand. We\nneed to recognize the limitations in usability due to these large screens. We\nmust start designing user interfaces that are more conducive to one hand usage,\nwhich is the preferred way of interacting with the phone. This paper introduces\nAdaptive App Design, a design methodology that promotes dynamic and adaptive\ninterfaces for one handed usage. We present a novel method of recognizing which\nhand the user is interacting with and suggest how to design friendlier\ninterfaces for them by presenting a set of design guidelines for this\nmethodology.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 02:50:30 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Nelavelli", "Kriti", ""], ["Ploetz", "Thomas", ""]]}, {"id": "1805.08418", "submitter": "Jiangtao Wang", "authors": "Jiangtao Wang, Leye Wang, Yasha Wang, Daqing Zhang, and Linghe Kong", "title": "Task Allocation in Mobile Crowd Sensing: State of the Art and Future\n  Opportunities", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Crowd Sensing (MCS) is the special case of crowdsourcing, which\nleverages the smartphones with various embedded sensors and user's mobility to\nsense diverse phenomenon in a city. Task allocation is a fundamental research\nissue in MCS, which is crucial for the efficiency and effectiveness of MCS\napplications. In this article, we specifically focus on the task allocation in\nMCS systems. We first present the unique features of MCS allocation compared to\ngeneric crowdsourcing, and then provide a comprehensive review for diversifying\nproblem formulation and allocation algorithms together with future research\nopportunities.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 06:21:09 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 07:17:12 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Wang", "Jiangtao", ""], ["Wang", "Leye", ""], ["Wang", "Yasha", ""], ["Zhang", "Daqing", ""], ["Kong", "Linghe", ""]]}, {"id": "1805.08420", "submitter": "Jiangtao Wang", "authors": "Jiangtao Wang, Yasha Wang, Daqing Zhang, Qin Lv, and Chao Chen", "title": "Crowd-Powered Sensing and Actuation in Smart Cities: Current Issues and\n  Future Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of seamless connection of human, machine, and smart things,\nthere is an emerging trend to leverage the power of crowds (e.g., citizens,\nmobile devices, and smart things) to monitor what is happening in a city,\nunderstand how the city is evolving, and further take actions to enable better\nquality of life, which is referred to as Crowd-Powered Smart City (CPSC). In\nthis article, we provide a literature review for CPSC and identify future\nresearch opportunities. Specifically, we first define the concepts with typical\nCPSC applications. Then, we present the main characteristics of CPSC and\nfurther highlight the research issues. In the end, we point out existing\nlimitations which can inform and guide future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 06:26:02 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Wang", "Jiangtao", ""], ["Wang", "Yasha", ""], ["Zhang", "Daqing", ""], ["Lv", "Qin", ""], ["Chen", "Chao", ""]]}, {"id": "1805.08480", "submitter": "Jiangtao Wang", "authors": "Jiangtao Wang, Feng Wang, Yasha Wang, Leye Wang, Zhaopeng Qiu, Daqing\n  Zhang, Bin Guo, Qin Lv", "title": "HyTasker: Hybrid Task Allocation in Mobile Crowd Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task allocation is a major challenge in Mobile Crowd Sensing (MCS). While\nprevious task allocation approaches follow either the opportunistic or\nparticipatory mode, this paper proposes to integrate these two complementary\nmodes in a two-phased hybrid framework called HyTasker. In the offline phase, a\ngroup of workers (called opportunistic workers) are selected, and they complete\nMCS tasks during their daily routines (i.e., opportunistic mode). In the online\nphase, we assign another set of workers (called participatory workers) and\nrequire them to move specifically to perform tasks that are not completed by\nthe opportunistic workers (i.e., participatory mode). Instead of considering\nthese two phases separately, HyTasker jointly optimizes them with a total\nincentive budget constraint. In particular, when selecting opportunistic\nworkers in the offline phase of HyTasker, we propose a novel algorithm that\nsimultaneously considers the predicted task assignment for the participatory\nworkers, in which the density and mobility of participatory workers are taken\ninto account. Experiments on a real-world mobility dataset demonstrate that\nHyTasker outperforms other methods with more completed tasks under the same\nbudget constraint.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 10:10:42 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Wang", "Jiangtao", ""], ["Wang", "Feng", ""], ["Wang", "Yasha", ""], ["Wang", "Leye", ""], ["Qiu", "Zhaopeng", ""], ["Zhang", "Daqing", ""], ["Guo", "Bin", ""], ["Lv", "Qin", ""]]}, {"id": "1805.08525", "submitter": "Jiangtao Wang", "authors": "Jiangtao Wang, Feng Wang, Yasha Wang, Daqing Zhang, Leye Wang,\n  Zhaopeng Qiu", "title": "Social-Network-Assisted Worker Recruitment in Mobile Crowd Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worker recruitment is a crucial research problem in Mobile Crowd Sensing\n(MCS). While previous studies rely on a specified platform with a pre-assumed\nlarge user pool, this paper leverages the influenced propagation on the social\nnetwork to assist the MCS worker recruitment. We first select a subset of users\non the social network as initial seeds and push MCS tasks to them. Then,\ninfluenced users who accept tasks are recruited as workers, and the ultimate\ngoal is to maximize the coverage. Specifically, to select a near-optimal set of\nseeds, we propose two algorithms, named Basic-Selector and Fast-Selector,\nrespectively. Basic-Selector adopts an iterative greedy process based on the\npredicted mobility, which has good performance but suffers from inefficiency\nconcerns. To accelerate the selection, Fast-Selector is proposed, which is\nbased on the interdependency of geographical positions among friends. Empirical\nstudies on two real-world datasets verify that Fast-Selector achieves higher\ncoverage than baseline methods under various settings, meanwhile, it is much\nmore efficient than Basic-Selector while only sacrificing a slight fraction of\nthe coverage.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 11:57:38 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Wang", "Jiangtao", ""], ["Wang", "Feng", ""], ["Wang", "Yasha", ""], ["Zhang", "Daqing", ""], ["Wang", "Leye", ""], ["Qiu", "Zhaopeng", ""]]}, {"id": "1805.08729", "submitter": "Marta Ortin", "authors": "Marta Ortin, Adrian Jarabo, Belen Masia, Diego Gutierrez", "title": "Analyzing Interfaces and Workflows for Light Field Editing", "comments": "10 papers, 14 figures", "journal-ref": "Marta Ortin, Adrian Jarabo, Belen Masia, and Diego Gutierrez.\n  Analyzing Interfaces and Workflows for Light Field Editing. IEEE Journal of\n  Selected Topics in Signal Processing, vol. 11, no. 7, pp. 1162-1172, Oct.\n  2017", "doi": "10.1109/JSTSP.2017.2746263", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing number of available consumer light field cameras, such as\nLytro TM, Raytrix TM, or Pelican Imaging TM, this new form of photography is\nprogressively becoming more common. However, there are still very few tools for\nlight field editing, and the interfaces to create those edits remain largely\nunexplored. Given the extended dimensionality of light field data, it is not\nclear what the most intuitive interfaces and optimal workflows are, in contrast\nwith well-studied 2D image manipulation software. In this work we provide a\ndetailed description of subjects' performance and preferences for a number of\nsimple editing tasks, which form the basis for more complex operations. We\nperform a detailed state sequence analysis and hidden Markov chain analysis\nbased on the sequence of tools and interaction paradigms users employ while\nediting light fields. These insights can aid researchers and designers in\ncreating new light field editing tools and interfaces, thus helping to close\nthe gap between 4D and 2D image editing.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 16:39:12 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Ortin", "Marta", ""], ["Jarabo", "Adrian", ""], ["Masia", "Belen", ""], ["Gutierrez", "Diego", ""]]}, {"id": "1805.09012", "submitter": "Christoph Anderson", "authors": "Christoph Anderson, Isabel Suarez, Yaqian Xu, Klaus David", "title": "An Ontology-Based Reasoning Framework for Context-Aware Applications", "comments": "6 pages, 1 figure, International and Interdisciplinary Conference on\n  Modeling and Using Context", "journal-ref": "Modeling and Using Context, H. Christiansen, I. Stojanovic, and G.\n  A. Papadopoulos, Eds. Cham, Switzerland: Springer International Publishing,\n  2015, pp. 471-476", "doi": "10.1007/978-3-319-25591-0_34", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-aware applications process context information to support users in\ntheir daily tasks and routines. These applications can adapt their\nfunctionalities by aggregating context information through machine-learning and\ndata processing algorithms, supporting users with recommendations or services\nbased on their current needs. In the last years, smartphones have been used in\nthe field of context-awareness due to their embedded sensors and various\ncommunication interfaces such as Bluetooth, WiFi, NFC or cellular. However,\nbuilding context-aware applications for smartphones can be a challenging and\ntime-consuming task. In this paper, we describe an ontology-based reasoning\nframework to create context-aware applications. The framework is based on an\nontology as well as micro-services to aggregate, process and represent context\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 08:36:28 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Anderson", "Christoph", ""], ["Suarez", "Isabel", ""], ["Xu", "Yaqian", ""], ["David", "Klaus", ""]]}, {"id": "1805.09109", "submitter": "Jeremy Frey", "authors": "Jelena Mladenovi\\'c (Potioc, CRNL), J\\'er\\'emy Frey, Emmanuel Maby\n  (CRNL), Mateus Joffily (GATE Lyon Saint-\\'Etienne), Fabien Lotte (Potioc),\n  Jeremie Mattout (CRNL)", "title": "Active Inference for Adaptive BCI: application to the P300 Speller", "comments": null, "journal-ref": "International BCI meeting, May 2018, Asilomar, United States.\n  2018, http://bcisociety.org/", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive Brain-Computer interfaces (BCIs) have shown to improve performance,\nhowever a general and flexible framework to implement adaptive features is\nstill lacking. We appeal to a generic Bayesian approach, called Active\nInference (AI), to infer user's intentions or states and act in a way that\noptimizes performance. In realistic P300-speller simulations, AI outperforms\ntraditional algorithms with an increase in bit rate between 18% and 59%, while\noffering a possibility of unifying various adaptive implementations within one\ngeneric framework.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 08:20:26 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Mladenovi\u0107", "Jelena", "", "Potioc, CRNL"], ["Frey", "J\u00e9r\u00e9my", "", "CRNL"], ["Maby", "Emmanuel", "", "CRNL"], ["Joffily", "Mateus", "", "GATE Lyon Saint-\u00c9tienne"], ["Lotte", "Fabien", "", "Potioc"], ["Mattout", "Jeremie", "", "CRNL"]]}, {"id": "1805.09192", "submitter": "Birgitta Dresp-Langley", "authors": "Yasmine Boumenir, Fanny Georges, Jeremy Valentin, Guy Rebillard,\n  Birgitta Dresp-Langley", "title": "Wayfinding through an unfamiliar environment", "comments": null, "journal-ref": "2010, Perceptual and Motor Skills, 111(3):829-47", "doi": "10.2466/04.22.23.27.PMS.111.6.829-847", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strategies for finding one's way through an unfamiliar environment may be\nhelped by computer generated 2D maps, 3D virtual environments, or other\nnavigation aids. The relative effectiveness of 2D and 3D virtual navigation\naids was investigated. The wayfinding experiments (navigation tests) were\nconducted in a large, park-like environment. 24 participants (12 men, 12 women;\nage range = 22-50 years; M=32, SD = 7.4) were divided into three groups of four\nindividuals per gender, who 1) explored a computer generated 2D map of the\ngiven route prior to navigation, 2) received a silent guided tour by means of\nan interactive 3D virtual representation, or 3) acquired direct experience of\nthe real-world route through a silent guided tour where they were accompanied\nby a human individual who had expert knowledge of all the routes in the park.\nParticipants from the different preparation groups then had to find the same\nroute again on their own. 12 observers (six men and six women) were given a\n\"simple\" route with only one critical turn, and the other 12 a \"complex\" route\nwith six critical turns. Navigation performances were compared with those of\nthree experts who were highly familiar with all the routes of the park. Those\namong the naive participants who had benefitted from a direct experience\n(guided tour) prior to navigation, all found their way again on the simple and\ncomplex routes. Those who had explored the interactive 3D virtual environment\nwere all unable to find their way on the complex route. The relative scale\nrepresentation in the virtual 3D environment may have given incorrect\nimpressions of relative distances between salient objects along the itinerary,\nrendering important landmark information useless.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 14:20:47 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Boumenir", "Yasmine", ""], ["Georges", "Fanny", ""], ["Valentin", "Jeremy", ""], ["Rebillard", "Guy", ""], ["Dresp-Langley", "Birgitta", ""]]}, {"id": "1805.09322", "submitter": "Grigorios Kalogiannis Mr", "authors": "Kalogiannis Gregory, Hassapis George", "title": "Reworked Second Order Blind Identification and Support Vector Machine\n  technique towards imagery movement identification from EEG signals", "comments": "1 page, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During imagery motor movements tasks, the so called mu and beta event related\ndesynchronization (ERD) and synchronization (ERS) are taking place, allowing us\nto determine human patient imagery movement. However, initial recordings of\nelectroencephalography (EEG) signals contain system and environmental noise as\nwell as interference that must be ejected in order to separate the ERS/ERD\nevents from the rest of the signal. This paper presents a new technique based\non a reworked Second Order Blind Identification (SOBI) algorithm for noise\nremoval while imagery movement classification is implemented using Support\nVector Machine (SVM) technique.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 14:27:05 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Gregory", "Kalogiannis", ""], ["George", "Hassapis", ""]]}, {"id": "1805.09454", "submitter": "Aryabrata Basu", "authors": "Aryabrata Basu, Kyle Johnsen", "title": "Navigating a maze differently - a user study", "comments": "13 pages, pre-print to potential VR journals such as Presence, IJVR,\n  VR Springer, etc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigating spaces is an embodied experience. Examples can vary from rescue\nworkers trying to save people from natural disasters; a tourist finding their\nway to the nearest coffee shop, or a gamer solving a maze. Virtual reality\nallows these experiences to be simulated in a controlled virtual environment.\nHowever, virtual reality users remain anchored in the real world and the\nconventions by which the virtual environment is deployed influence user\nperformance. There is currently a need to evaluate the degree of influence\nimposed by extrinsic factors and virtual reality hardware on its users.\nTraditionally, virtual reality experiences have been deployed using\nHead-Mounted Displays with powerful computers rendering the graphical content\nof the virtual environment; however, user input has been facilitated using an\narray of human interface devices including Keyboards, Mice, Trackballs,\nTouchscreens, Joysticks, Gamepads, Motion detecting cameras and Webcams. Some\nof these HIDs have also been introduced for non-immersive video games and\ngeneral computing. Due to this fact, a subset of virtual reality users has\ngreater familiarity than others in using these HIDs. Virtual reality\nexperiences that utilize gamepads (controllers) to navigate virtual\nenvironments may introduce a bias towards usability among virtual reality users\npreviously exposed to video-gaming.\n  This article presents an evaluative user study conducted using our ubiquitous\nvirtual reality framework with general audiences. Among our findings, we reveal\na usability bias among virtual reality users who are predominantly video\ngamers. Beyond this, we found a statistical difference in user behavior between\nuntethered immersive virtual reality experiences compared to untethered\nnon-immersive virtual reality experiences.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 23:41:42 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 05:08:36 GMT"}, {"version": "v3", "created": "Fri, 14 Sep 2018 20:37:39 GMT"}, {"version": "v4", "created": "Thu, 20 Sep 2018 19:33:14 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Basu", "Aryabrata", ""], ["Johnsen", "Kyle", ""]]}, {"id": "1805.09511", "submitter": "Weixuan Chen", "authors": "Weixuan Chen, Javier Hernandez, Rosalind W. Picard", "title": "Estimating Carotid Pulse and Breathing Rate from Near-infrared Video of\n  the Neck", "comments": "21 pages, 15 figures", "journal-ref": null, "doi": "10.1088/1361-6579/aae625", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Non-contact physiological measurement is a growing research area\nthat allows capturing vital signs such as heart rate (HR) and breathing rate\n(BR) comfortably and unobtrusively with remote devices. However, most of the\napproaches work only in bright environments in which subtle\nphotoplethysmographic and ballistocardiographic signals can be easily analyzed\nand/or require expensive and custom hardware to perform the measurements.\n  Approach: This work introduces a low-cost method to measure subtle motions\nassociated with the carotid pulse and breathing movement from the neck using\nnear-infrared (NIR) video imaging. A skin reflection model of the neck was\nestablished to provide a theoretical foundation for the method. In particular,\nthe method relies on template matching for neck detection, Principal Component\nAnalysis for feature extraction, and Hidden Markov Models for data smoothing.\n  Main Results: We compared the estimated HR and BR measures with ones provided\nby an FDA-cleared device in a 12-participant laboratory study: the estimates\nachieved a mean absolute error of 0.36 beats per minute and 0.24 breaths per\nminute under both bright and dark lighting.\n  Significance: This work advances the possibilities of non-contact\nphysiological measurement in real-life conditions in which environmental\nillumination is limited and in which the face of the person is not readily\navailable or needs to be protected. Due to the increasing availability of NIR\nimaging devices, the described methods are readily scalable.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 05:15:18 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Chen", "Weixuan", ""], ["Hernandez", "Javier", ""], ["Picard", "Rosalind W.", ""]]}, {"id": "1805.09635", "submitter": "Anneli Heimb\\\"urger Dr. Tech.", "authors": "Anneli Heimb\\\"urger", "title": "When Cultures Meet: Modelling Cross-Cultural Knowledge Spaces", "comments": null, "journal-ref": "Frontiers in Artificial Intelligence and Applications, 2008, Vol.\n  166, Information Modelling and Knowledge Bases XIX. Amsterdam: IOS Press. Pp.\n  314-321", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross cultural research projects are becoming a norm in our global world.\nMore and more projects are being executed using teams from eastern and western\ncultures. Cultural competence might help project managers to achieve project\ngoals and avoid potential risks in cross cultural project environments and\nwould also support them to promote creativity and motivation through flexible\nleadership. In our paper we introduce an idea for constructing an information\nsystem, a cross cultural knowledge space, which could support cross cultural\ncommunication, collaborative learning experiences and time based project\nmanagement functions. The case cultures in our project are Finnish and\nJapanese. The system can be used both in virtual and in physical spaces for\nexample to clarify cultural business etiquette. The core of our system design\nwill be based on cross cultural ontology, and the system implementation on XML\ntechnologies. Our approach is a practical, step by step example of constructive\nresearch. In our paper we shortly describe Hofstede's dimensions for assessing\ncultures as one example of a larger framework for our study. We also discuss\nthe concept of time in cultural context.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 12:40:47 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Heimb\u00fcrger", "Anneli", ""]]}, {"id": "1805.09676", "submitter": "Robert Bridges", "authors": "Robert A. Bridges, Maria A. Vincent, Kelly M. T. Huffer, John R.\n  Goodall, Jessie D. Jamieson, Zachary Burch", "title": "Forming IDEAS Interactive Data Exploration & Analysis System", "comments": "4 page short paper on IDEAS System, 4 figures", "journal-ref": "Workshop on Information Security Workers, USENIX SOUPS 2018", "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern cyber security operations collect an enormous amount of logging and\nalerting data. While analysts have the ability to query and compute simple\nstatistics and plots from their data, current analytical tools are too simple\nto admit deep understanding. To detect advanced and novel attacks, analysts\nturn to manual investigations. While commonplace, current investigations are\ntime-consuming, intuition-based, and proving insufficient. Our hypothesis is\nthat arming the analyst with easy-to-use data science tools will increase their\nwork efficiency, provide them with the ability to resolve hypotheses with\nscientific inquiry of their data, and support their decisions with evidence\nover intuition. To this end, we present our work to build IDEAS (Interactive\nData Exploration and Analysis System). We present three real-world use-cases\nthat drive the system design from the algorithmic capabilities to the user\ninterface. Finally, a modular and scalable software architecture is discussed\nalong with plans for our pilot deployment with a security operation command.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 13:54:24 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 19:50:20 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Bridges", "Robert A.", ""], ["Vincent", "Maria A.", ""], ["Huffer", "Kelly M. T.", ""], ["Goodall", "John R.", ""], ["Jamieson", "Jessie D.", ""], ["Burch", "Zachary", ""]]}, {"id": "1805.10664", "submitter": "Rick Chang", "authors": "Jen-Hao Rick Chang, B. V. K. Vijaya Kumar, Aswin C. Sankaranarayanan", "title": "Towards Multifocal Displays with Dense Focal Stacks", "comments": null, "journal-ref": null, "doi": "10.1145/3272127.3275015", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a virtual reality display that is capable of generating a dense\ncollection of depth/focal planes. This is achieved by driving a focus-tunable\nlens to sweep a range of focal lengths at a high frequency and, subsequently,\ntracking the focal length precisely at microsecond time resolutions using an\noptical module. Precise tracking of the focal length, coupled with a high-speed\ndisplay, enables our lab prototype to generate 1600 focal planes per second.\nThis enables a novel first-of-its-kind virtual reality multifocal display that\nis capable of resolving the vergence-accommodation conflict endemic to today's\ndisplays.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 17:51:07 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 07:51:47 GMT"}, {"version": "v3", "created": "Sat, 22 Sep 2018 22:40:47 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Chang", "Jen-Hao Rick", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Sankaranarayanan", "Aswin C.", ""]]}, {"id": "1805.10723", "submitter": "VIctor Dibia", "authors": "Victor Dibia, Aaron Cox, Justin Weisz", "title": "Designing for Democratization: Introducing Novices to Artificial\n  Intelligence Via Maker Kits", "comments": "Early paper draft - Updated references, author list, figure captions,\n  acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing research highlight the myriad of benefits realized when technology\nis sufficiently democratized and made accessible to non-technical or novice\nusers. However, democratizing complex technologies such as artificial\nintelligence (AI) remains hard. In this work, we draw on theoretical\nunderpinnings from the democratization of innovation, in exploring the design\nof maker kits that help introduce novice users to complex technologies. We\nreport on our work designing TJBot: an open source cardboard robot that can be\nprogrammed using pre-built AI services. We highlight principles we adopted in\nthis process (approachable design, simplicity, extensibility and\naccessibility), insights we learned from showing the kit at workshops (66\nparticipants) and how users interacted with the project on GitHub over a\n12-month period (Nov 2016 - Nov 2017). We find that the project succeeds in\nattracting novice users (40% of users who forked the project are new to GitHub)\nand a variety of demographics are interested in prototyping use cases such as\nhome automation, task delegation, teaching and learning.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 01:30:04 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 19:31:37 GMT"}, {"version": "v3", "created": "Sat, 5 Jan 2019 20:11:55 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Dibia", "Victor", ""], ["Cox", "Aaron", ""], ["Weisz", "Justin", ""]]}, {"id": "1805.10724", "submitter": "Bum Chul Kwon", "authors": "Bum Chul Kwon, Min-Je Choi, Joanne Taery Kim, Edward Choi, Young Bin\n  Kim, Soonwook Kwon, Jimeng Sun, Jaegul Choo", "title": "RetainVis: Visual Analytics with Interpretable and Interactive Recurrent\n  Neural Networks on Electronic Medical Records", "comments": "Accepted at IEEE VIS 2018. To appear in IEEE Transactions on\n  Visualization and Computer Graphics in January 2019", "journal-ref": null, "doi": "10.1109/TVCG.2018.2865027", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have recently seen many successful applications of recurrent neural\nnetworks (RNNs) on electronic medical records (EMRs), which contain histories\nof patients' diagnoses, medications, and other various events, in order to\npredict the current and future states of patients. Despite the strong\nperformance of RNNs, it is often challenging for users to understand why the\nmodel makes a particular prediction. Such black-box nature of RNNs can impede\nits wide adoption in clinical practice. Furthermore, we have no established\nmethods to interactively leverage users' domain expertise and prior knowledge\nas inputs for steering the model. Therefore, our design study aims to provide a\nvisual analytics solution to increase interpretability and interactivity of\nRNNs via a joint effort of medical experts, artificial intelligence scientists,\nand visual analytics researchers. Following the iterative design process\nbetween the experts, we design, implement, and evaluate a visual analytics tool\ncalled RetainVis, which couples a newly improved, interpretable and interactive\nRNN-based model called RetainEX and visualizations for users' exploration of\nEMR data in the context of prediction tasks. Our study shows the effective use\nof RetainVis for gaining insights into how individual medical codes contribute\nto making risk predictions, using EMRs of patients with heart failure and\ncataract symptoms. Our study also demonstrates how we made substantial changes\nto the state-of-the-art RNN model called RETAIN in order to make use of\ntemporal information and increase interactivity. This study will provide a\nuseful guideline for researchers that aim to design an interpretable and\ninteractive visual analytics tool for RNNs.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 01:30:53 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 16:34:35 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 05:31:15 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kwon", "Bum Chul", ""], ["Choi", "Min-Je", ""], ["Kim", "Joanne Taery", ""], ["Choi", "Edward", ""], ["Kim", "Young Bin", ""], ["Kwon", "Soonwook", ""], ["Sun", "Jimeng", ""], ["Choo", "Jaegul", ""]]}, {"id": "1805.10799", "submitter": "Hyemin Ahn", "authors": "Hyemin Ahn, Sungjoon Choi, Nuri Kim, Geonho Cha, Songhwai Oh", "title": "Interactive Text2Pickup Network for Natural Language based Human-Robot\n  Collaboration", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the Interactive Text2Pickup (IT2P) network for\nhuman-robot collaboration which enables an effective interaction with a human\nuser despite the ambiguity in user's commands. We focus on the task where a\nrobot is expected to pick up an object instructed by a human, and to interact\nwith the human when the given instruction is vague. The proposed network\nunderstands the command from the human user and estimates the position of the\ndesired object first. To handle the inherent ambiguity in human language\ncommands, a suitable question which can resolve the ambiguity is generated. The\nuser's answer to the question is combined with the initial command and given\nback to the network, resulting in more accurate estimation. The experiment\nresults show that given unambiguous commands, the proposed method can estimate\nthe position of the requested object with an accuracy of 98.49% based on our\ntest dataset. Given ambiguous language commands, we show that the accuracy of\nthe pick up task increases by 1.94 times after incorporating the information\nobtained from the interaction.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 07:52:42 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Ahn", "Hyemin", ""], ["Choi", "Sungjoon", ""], ["Kim", "Nuri", ""], ["Cha", "Geonho", ""], ["Oh", "Songhwai", ""]]}, {"id": "1805.11352", "submitter": "Christopher Power", "authors": "Jen Beeston, Christopher Power, Paul Cairns, Mark Barlet", "title": "Characteristics and Motivations of Players with Disabilities in Digital\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In research and practice into the accessibility of digital games, much of the\nwork has focused on how to make games accessible to people with disa- bilities.\nWith an increasing number of people with disabilities playing main- stream\ncommercial games, it is important that we understand who they are and how they\nplay in order to take a more user-centered approach as this field grows. We\nconducted a demographic survey of 230 players with disabilities and found that\nthey play mainstream digital games using a variety of assistive tech- nologies,\nuse accessibility options such as key remapping and subtitles, and they\nidentify themselves as gamers who play digital games as their primary hobby.\nThis gives us a richer picture of players with disabilities and indicates that\nthere are opportunities to begin to look at accessible player experiences (APX)\nin games.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 10:52:12 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Beeston", "Jen", ""], ["Power", "Christopher", ""], ["Cairns", "Paul", ""], ["Barlet", "Mark", ""]]}, {"id": "1805.11537", "submitter": "Ludovik Coba <", "authors": "Ludovik Coba, Markus Zanker, Laurens Rook, Panagiotis Symeonidis", "title": "Decision Making of Maximizers and Satisficers Based on Collaborative\n  Explanations", "comments": null, "journal-ref": null, "doi": "10.1145/3301275.3302304", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rating-based summary statistics are ubiquitous in e-commerce, and often are\ncrucial components in personalized recommendation mechanisms. Largely left\nunexplored, however, is the issue to what extent the descriptives of rating\ndistributions influence the decision making of online consumers. We conducted a\nconjoint experiment to explore how different summarizations of rating\ndistributions (i.e., in the form of the number of ratings, mean, variance,\nskewness or the origin of the ratings) impact users' decision making. Results\nfrom over 200 participants indicate that users are primarily guided by the mean\nand the number of ratings and to a lesser degree by the variance, and the\norigin of a rating. We also looked into the maximizing behavioral tendencies of\nour participants, and found that in particular participants scoring high on the\nDecision Difficulty subscale displayed other sensitivities regarding the way in\nwhich rating distributions were summarized than others.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 15:15:15 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Coba", "Ludovik", ""], ["Zanker", "Markus", ""], ["Rook", "Laurens", ""], ["Symeonidis", "Panagiotis", ""]]}, {"id": "1805.11773", "submitter": "Amir Rasouli", "authors": "Amir Rasouli and John K. Tsotsos", "title": "Autonomous Vehicles that Interact with Pedestrians: A Survey of Theory\n  and Practice", "comments": "This work has been submitted to the IEEE Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges that autonomous cars are facing today is driving\nin urban environments. To make it a reality, autonomous vehicles require the\nability to communicate with other road users and understand their intentions.\nSuch interactions are essential between the vehicles and pedestrians as the\nmost vulnerable road users. Understanding pedestrian behavior, however, is not\nintuitive and depends on various factors such as demographics of the\npedestrians, traffic dynamics, environmental conditions, etc. In this paper, we\nidentify these factors by surveying pedestrian behavior studies, both the\nclassical works on pedestrian-driver interaction and the modern ones that\ninvolve autonomous vehicles. To this end, we will discuss various methods of\nstudying pedestrian behavior, and analyze how the factors identified in the\nliterature are interrelated. We will also review the practical applications\naimed at solving the interaction problem including design approaches for\nautonomous vehicles that communicate with pedestrians and visual perception and\nreasoning algorithms tailored to understanding pedestrian intention. Based on\nour findings, we will discuss the open problems and propose future research\ndirections.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 02:01:17 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Rasouli", "Amir", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1805.11878", "submitter": "Dominik Kowald PhD", "authors": "Dominik Kowald", "title": "Modeling Cognitive Processes in Social Tagging to Improve Tag\n  Recommendations", "comments": "PHD-Symposium paper @ WWW2015. Supervised by Prof. Stefanie\n  Lindstaedt, Prof. Tobias Ley and Ass-Prof. Elisabeth Lex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the emergence of Web 2.0, tag recommenders have become important tools,\nwhich aim to support users in finding descriptive tags for their bookmarked\nresources. Although current algorithms provide good results in terms of tag\nprediction accuracy, they are often designed in a data-driven way and thus,\nlack a thorough understanding of the cognitive processes that play a role when\npeople assign tags to resources. This thesis aims at modeling these cognitive\ndynamics in social tagging in order to improve tag recommendations and to\nbetter understand the underlying processes.\n  As a first attempt in this direction, we have implemented an interplay\nbetween individual micro-level (e.g., categorizing resources or temporal\ndynamics) and collective macro-level (e.g., imitating other users' tags)\nprocesses in the form of a novel tag recommender algorithm. The preliminary\nresults for datasets gathered from BibSonomy, CiteULike and Delicious show that\nour proposed approach can outperform current state-of-the-art algorithms, such\nas Collaborative Filtering, FolkRank or Pairwise Interaction Tensor\nFactorization. We conclude that recommender systems can be improved by\nincorporating related principles of human cognition.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 09:33:29 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Kowald", "Dominik", ""]]}, {"id": "1805.12346", "submitter": "Marcos Baez", "authors": "Svetlana Nikitina, Florian Daniel, Marcos Baez, Fabio Casati", "title": "Crowdsourcing for Reminiscence Chatbot Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work-in-progress paper we discuss the challenges in identifying\neffective and scalable crowd-based strategies for designing content,\nconversation logic, and meaningful metrics for a reminiscence chatbot targeted\nat older adults. We formalize the problem and outline the main research\nquestions that drive the research agenda in chatbot design for reminiscence and\nfor relational agents for older adults in general.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 07:06:11 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Nikitina", "Svetlana", ""], ["Daniel", "Florian", ""], ["Baez", "Marcos", ""], ["Casati", "Fabio", ""]]}, {"id": "1805.12376", "submitter": "Marcos Baez", "authors": "Jorge Ramirez, Evgeny Krivosheev, Marcos Baez, Fabio Casati, Boualem\n  Benatallah", "title": "CrowdRev: A platform for Crowd-based Screening of Literature Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper and demo we present a crowd and crowd+AI based system, called\nCrowdRev, supporting the screening phase of literature reviews and achieving\nthe same quality as author classification at a fraction of the cost, and\nnear-instantly. CrowdRev makes it easy for authors to leverage the crowd, and\nensures that no money is wasted even in the face of difficult papers or\ncriteria: if the system detects that the task is too hard for the crowd, it\njust gives up trying (for that paper, or for that criteria, or altogether),\nwithout wasting money and never compromising on quality.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 08:32:37 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Ramirez", "Jorge", ""], ["Krivosheev", "Evgeny", ""], ["Baez", "Marcos", ""], ["Casati", "Fabio", ""], ["Benatallah", "Boualem", ""]]}, {"id": "1805.12473", "submitter": "Abd\\\"ulkadir \\c{C}ak{\\i}r", "authors": "Abd\\\"ulkadir \\c{C}akir and \\\"Umm\\\"u\\c{s}an \\c{C}itak", "title": "Simulation Of Logic Circuit Tests On Android-Based Mobile Devices", "comments": "13 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, an application that can run on Android and Windows-based\nmobile devices was developed to allow students attending such classes as\nNumerical/Digital Electronics, Logic Circuits, Basic Electronics Measurement\nand Electronic Systems in Turkey's Vocation and Technical Education Schools to\neasily carry out the simulation of logic gates, as well as logic circuit tests\nperformed using logic gates. A 2D-mobile application that runs on both\nplatforms was developed using the C# language on the Unity3D editor. To assess\nthe usability of the mobile application, a one-hour training session was\nadministered in March of the 2017-2018 academic year to two groups of students\nfrom a single class in the sixth grade of an Imam Hatip Secondary School\naffiliated to the Ministry of National Education. Each of the two groups\ncontained 12 students who were assumed to be equivalent, and who had no prior\nknowledge of the subject. The training of the first group began with a lecture\non basic logic gates using a blackboard, and involved no simulations, while the\nsecond group, in addition to being given the same the lecture, received\nadditional training involving demonstrations of the developed mobile\napplication and its simulations. Following the lectures, a written exam was\napplied to both groups. An evaluation of the exam results revealed that 83\npercent of the students who had been given demonstrations of the mobile\napplication were able to perform the circuit task completely, whereas only 50\npercent of the other were able to complete the task. It was concluded that the\napplication was both useful and facilitating for to the students, and it was\nalso noted that students who were supported by the mobile application had\ngained a better grasp of the topic by being able to see and practice the\nsimulations first hand.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 21:14:03 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["\u00c7akir", "Abd\u00fclkadir", ""], ["\u00c7itak", "\u00dcmm\u00fc\u015fan", ""]]}, {"id": "1805.12475", "submitter": "Michael Green", "authors": "Gabriella A. B. Barros, Michael Cerny Green, Antonios Liapis, and\n  Julian Togelius", "title": "Data-driven Design: A Case for Maximalist Game Design", "comments": "9 pages, 2 Figures, Accepted in ICCC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximalism in art refers to drawing on and combining multiple different\nsources for art creation, embracing the resulting collisions and heterogeneity.\nThis paper discusses the use of maximalism in game design and particularly in\ndata games, which are games that are generated partly based on open data. Using\nData Adventures, a series of generators that create adventure games from data\nsources such as Wikipedia and OpenStreetMap, as a lens we explore several\ntradeoffs and issues in maximalist game design. This includes the tension\nbetween transformation and fidelity, between decorative and functional content,\nand legal and ethical issues resulting from this type of generativity. This\npaper sketches out the design space of maximalist data-driven games, a design\nspace that is mostly unexplored.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 00:43:03 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Barros", "Gabriella A. B.", ""], ["Green", "Michael Cerny", ""], ["Liapis", "Antonios", ""], ["Togelius", "Julian", ""]]}, {"id": "1805.12539", "submitter": "Adam Aviv", "authors": "Flynn Wolf and Adam J. Aviv and Ravi Kuber", "title": "Classifying Eyes-Free Mobile Authentication Techniques", "comments": "A version of this paper is to appear in the Journal of Information\n  Security and Applications (JISA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile device users avoiding observational attacks and coping with\nsituational impairments may employ techniques for eyes-free mobile unlock\nauthentication, where a user enters his/her passcode without looking at the\ndevice. This study supplies an initial description of user accu- racy in\nperforming this authentication behavior with PIN and pattern passcodes, with\nvarying lengths and visual characteristics. Additionally, we inquire if\ntactile-only feedback can provide assistive spatialization, finding that\norientation cues prior to unlocking do not help. Measure- ments of edit\ndistance and dynamic time warping accuracy were collected, using a\nwithin-group, randomized study of 26 participants. 1,021 passcode entry\ngestures were collected and classified, identifying six user strategies for\nusing the pre-entry tactile feedback, and ten codes for types of events and\nerrors that occurred during entry. We found that users who focused on orienting\nthemselves to position the first digit of the passcode using the tactile\nfeedback performed better in the task. These results could be applied to better\ndefine eyes-free behavior in further research, and to design better and more\nsecure methods for eyes-free authentication.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 16:20:54 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Wolf", "Flynn", ""], ["Aviv", "Adam J.", ""], ["Kuber", "Ravi", ""]]}]