[{"id": "1710.00069", "submitter": "Souneil Park", "authors": "Souneil Park, Aleksandar Matic, Kamini Garg, Nuria Oliver", "title": "When Simpler Data Does Not Imply Less Information: A Study of User\n  Profiling Scenarios with Constrained View of Mobile HTTP(S) Traffic", "comments": null, "journal-ref": "ACM Trans. Web Volume 12 Issue 2, January 2018", "doi": "10.1145/3143402", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential growth in smartphone adoption is contributing to the\navailability of vast amounts of human behavioral data. This data enables the\ndevelopment of increasingly accurate data-driven user models that facilitate\nthe delivery of personalized services which are often free in exchange for the\nuse of its customers' data. Although such usage conventions have raised many\nprivacy concerns, the increasing value of personal data is motivating diverse\nentities to aggressively collect and exploit the data. In this paper, we unfold\nprofiling scenarios around mobile HTTP(S) traffic, focusing on those that have\nlimited but meaningful segments of the data. The capability of the scenarios to\nprofile personal information is examined with real user data, collected\nin-the-wild from 61 mobile phone users for a minimum of 30 days. Our study\nattempts to model heterogeneous user traits and interests, including\npersonality, boredom proneness, demographics, and shopping interests. Based on\nour modeling results, we discuss various implications to personalization,\nprivacy, and personal data rights.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 19:42:36 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Park", "Souneil", ""], ["Matic", "Aleksandar", ""], ["Garg", "Kamini", ""], ["Oliver", "Nuria", ""]]}, {"id": "1710.00171", "submitter": "Mara Brandt", "authors": "Mara Brandt, Britta Wrede, Franz Kummert, Lars Schillingmann", "title": "Confirmation detection in human-agent interaction using non-lexical\n  speech cues", "comments": "6 pages, Symposium on Natural Communication for Human-Robot\n  Collaboration, AAAI Fall Symposium Series 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even if only the acoustic channel is considered, human communication is\nhighly multi-modal. Non-lexical cues provide a variety of information such as\nemotion or agreement. The ability to process such cues is highly relevant for\nspoken dialog systems, especially in assistance systems. In this paper we focus\non the recognition of non-lexical confirmations such as \"mhm\", as they enhance\nthe system's ability to accurately interpret human intent in natural\ncommunication. The architecture uses a Support Vector Machine to detect\nconfirmations based on acoustic features. In a systematic comparison, several\nfeature sets were evaluated for their performance on a corpus of human-agent\ninteraction in a setting with naive users including elderly and cognitively\nimpaired people. Our results show that using stacked formants as features yield\nan accuracy of 84% outperforming regular formants and MFCC or pitch based\nfeatures for online classification.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 09:32:23 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Brandt", "Mara", ""], ["Wrede", "Britta", ""], ["Kummert", "Franz", ""], ["Schillingmann", "Lars", ""]]}, {"id": "1710.00274", "submitter": "Michael Wollowski", "authors": "Michael Wollowski, Carlotta Berry, Ryder Winck, Alan Jern, David\n  Voltmer, Alan Chiu, Yosi Shibberu", "title": "A Data-driven Approach Towards Human-robot Collaborative Problem Solving\n  in a Shared Space", "comments": "2017 AAAI Fall Symposium on Natural Communication for Human-Robot\n  Collaboration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are developing a system for human-robot communication that enables people\nto communicate with robots in a natural way and is focused on solving problems\nin a shared space. Our strategy for developing this system is fundamentally\ndata-driven: we use data from multiple input sources and train key components\nwith various machine learning techniques. We developed a web application that\nis collecting data on how two humans communicate to accomplish a task, as well\nas a mobile laboratory that is instrumented to collect data on how two humans\ncommunicate to accomplish a task in a physically shared space. The data from\nthese systems will be used to train and fine-tune the second stage of our\nsystem, in which the robot will be simulated through software. A physical robot\nwill be used in the final stage of our project. We describe these instruments,\na test-suite and performance metrics designed to evaluate and automate the data\ngathering process as well as evaluate an initial data set.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 00:27:20 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Wollowski", "Michael", ""], ["Berry", "Carlotta", ""], ["Winck", "Ryder", ""], ["Jern", "Alan", ""], ["Voltmer", "David", ""], ["Chiu", "Alan", ""], ["Shibberu", "Yosi", ""]]}, {"id": "1710.00366", "submitter": "Fabio Calefato", "authors": "Fabio Calefato, Giuseppe Iaffaldano, Filippo Lanubile", "title": "Collaboration Success Factors in an Online Music Community", "comments": "GROUP 2018, January 7-10, 2018, Sanibel Island, FL, USA, 10 pages", "journal-ref": "Proc. of GROUP 2018, January 7-10, 2018, Sanibel Island, FL, USA", "doi": "10.1145/3148330.3148346", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online communities have been able to develop large, open-source software\n(OSS) projects like Linux and Firefox throughout the successful collaborations\ncarried out by their members over the Internet. However, online communities\nalso involve creative arts domains such as animation, video games, and music.\nDespite their growing popularity, the factors that lead to successful\ncollaborations in these communities are not entirely understood. In this paper,\nwe present a study on creative collaboration in a music community where authors\nwrite songs together by 'overdubbing,' that is, by mixing a new track with an\nexisting audio recording. We analyzed the relationship between song- and\nauthor-related measures and the likelihood of a song being overdubbed. We found\nthat recent songs, as well as songs with many reactions, are more likely to be\noverdubbed; authors with a high status in the community and a recognizable\nidentity write songs that the community tends to build upon.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 15:24:52 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 14:31:11 GMT"}, {"version": "v3", "created": "Thu, 12 Oct 2017 19:55:45 GMT"}, {"version": "v4", "created": "Wed, 18 Oct 2017 13:58:08 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Calefato", "Fabio", ""], ["Iaffaldano", "Giuseppe", ""], ["Lanubile", "Filippo", ""]]}, {"id": "1710.00397", "submitter": "Lukas Vermeer", "authors": "Timo Kluck and Lukas Vermeer", "title": "Leaky Abstraction In Online Experimentation Platforms: A Conceptual\n  Framework To Categorize Common Challenges", "comments": "Presented at the 2015 Conference on Digital Experimentation\n  (CODE@MIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online experimentation platforms abstract away many of the details of\nexperimental design, ensuring experimenters do not have to worry about\nsampling, randomisation, subject tracking, data collection, metric definition\nand interpretation of results. The recent success and rapid adoption of these\nplatforms in the industry might in part be attributed to the ease-of-use these\nabstractions provide. Previous authors have pointed out there are common\npitfalls to avoid when running controlled experiments on the web and emphasised\nthe need for experts familiar with the entire software stack to be involved in\nthe process.\n  In this paper, we argue that these pitfalls and the need to understand the\nunderlying complexity are not the result of shortcomings specific to existing\nplatforms which might be solved by better platform design. We postulate that\nthey are a direct consequence of what is commonly referred to as \"the law of\nleaky abstractions\". That is, it is an inherent feature of any software\nplatform that details of its implementation leak to the surface, and that in\ncertain situations, the platform's consumers necessarily need to understand\ndetails of underlying systems in order to make proficient use of it.\n  We present several examples of this concept, including examples from\nliterature, and suggest some possible mitigation strategies that can be\nemployed to reduce the impact of abstraction leakage. The conceptual framework\nput forward in this paper allows us to explicitly categorize experimentation\npitfalls in terms of which specific abstraction is leaking, thereby aiding\nimplementers and users of these platforms to better understand and tackle the\nchallenges they face.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 19:35:49 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Kluck", "Timo", ""], ["Vermeer", "Lukas", ""]]}, {"id": "1710.00763", "submitter": "Doris Jung-Lin Lee", "authors": "Doris Jung-Lin Lee, John Lee, Tarique Siddiqui, Jaewoo Kim, Karrie\n  Karahalios, Aditya Parameswaran", "title": "You can't always sketch what you want: Understanding Sensemaking in\n  Visual Query Systems", "comments": "Accepted for presentation at IEEE VAST 2019, to be held October 20-25\n  in Vancouver, Canada. Paper will also be published in a special issue of IEEE\n  Transactions on Visualization and Computer Graphics (TVCG) IEEE VIS\n  (InfoVis/VAST/SciVis) 2019 ACM 2012 CCS - Human-centered computing,\n  Visualization, Visualization design and evaluation methods", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934666", "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual query systems (VQSs) empower users to interactively search for line\ncharts with desired visual patterns, typically specified using intuitive\nsketch-based interfaces. Despite decades of past work on VQSs, these efforts\nhave not translated to adoption in practice, possibly because VQSs are largely\nevaluated in unrealistic lab-based settings. To remedy this gap in adoption, we\ncollaborated with experts from three diverse domains---astronomy, genetics, and\nmaterial science---via a year-long user-centered design process to develop a\nVQS that supports their workflow and analytical needs, and evaluate how VQSs\ncan be used in practice. Our study results reveal that ad-hoc sketch-only\nquerying is not as commonly used as prior work suggests, since analysts are\noften unable to precisely express their patterns of interest. In addition, we\ncharacterize three essential sensemaking processes supported by our enhanced\nVQS. We discover that participants employ all three processes, but in different\nproportions, depending on the analytical needs in each domain. Our findings\nsuggest that all three sensemaking processes must be integrated in order to\nmake future VQSs useful for a wide range of analytical inquiries.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 16:31:24 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 02:06:10 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 17:56:33 GMT"}, {"version": "v4", "created": "Tue, 9 Oct 2018 15:53:41 GMT"}, {"version": "v5", "created": "Sun, 30 Dec 2018 03:38:58 GMT"}, {"version": "v6", "created": "Tue, 16 Jul 2019 19:51:27 GMT"}, {"version": "v7", "created": "Thu, 3 Oct 2019 19:19:02 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Lee", "Doris Jung-Lin", ""], ["Lee", "John", ""], ["Siddiqui", "Tarique", ""], ["Kim", "Jaewoo", ""], ["Karahalios", "Karrie", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1710.00808", "submitter": "Mathias Unberath", "authors": "Long Qian and Mathias Unberath and Kevin Yu and Bernhard Fuerst and\n  Alex Johnson and Nassir Navab and Greg Osgood", "title": "Technical Note: Towards Virtual Monitors for Image Guided Interventions\n  - Real-time Streaming to Optical See-Through Head-Mounted Displays", "comments": "6 pages, 2 Figures. Under review at Medical Physics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Image guidance is crucial for the success of many interventions.\nImages are displayed on designated monitors that cannot be positioned optimally\ndue to sterility and spatial constraints. This indirect visualization causes\npotential occlusion, hinders hand-eye coordination, leads to increased\nprocedure duration and surgeon load. Methods: We propose a virtual monitor\nsystem that displays medical images in a mixed reality visualization using\noptical see-through head-mounted displays. The system streams high-resolution\nmedical images from any modality to the head-mounted display in real-time that\nare blended with the surgical site. It allows for mixed reality visualization\nof images in head-, world-, or body-anchored mode and can thus be adapted to\nspecific procedural needs. Results: For typical image sizes, the proposed\nsystem exhibits an average end-to-end delay and refresh rate of 214 +- 30 ms\nand 41:4 +- 32:0 Hz, respectively. Conclusions: The proposed virtual monitor\nsystem is capable of real-time mixed reality visualization of medical images.\nIn future, we seek to conduct first pre-clinical studies to quantitatively\nassess the impact of the system on standard image guided procedures.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 17:39:07 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Qian", "Long", ""], ["Unberath", "Mathias", ""], ["Yu", "Kevin", ""], ["Fuerst", "Bernhard", ""], ["Johnson", "Alex", ""], ["Navab", "Nassir", ""], ["Osgood", "Greg", ""]]}, {"id": "1710.00888", "submitter": "Jose Berengueres Ph.D", "authors": "Jose Berengueres and Dani Castro", "title": "Sentiment Perception of Readers and Writers in Emoji use", "comments": "8 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research has traditionally analyzed emoji sentiment from the point\nof view of the reader of the content not the author. Here, we analyze emoji\nsentiment from the point of view of the author and present a emoji sentiment\nbenchmark that was built from an employee happiness dataset where emoji happen\nto be annotated with daily happiness of the author of the comment. The data\nspans over 3 years, and 4k employees of 56 companies based in Barcelona. We\ncompare sentiment of writers to readers. Results indicate that, there is an 82%\nagreement in how emoji sentiment is perceived by readers and writers. Finally,\nwe report that when authors use emoji they report higher levels of happiness.\nEmoji use was not found to be correlated with differences in author moodiness.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 20:07:18 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 23:26:06 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Berengueres", "Jose", ""], ["Castro", "Dani", ""]]}, {"id": "1710.00992", "submitter": "Rebecca Faust", "authors": "Rebecca Faust, David Glickenstein, Carlos Scheidegger", "title": "DimReader: Axis lines that explain non-linear projections", "comments": "13 Pages, 12 Figures", "journal-ref": "IEEE transactions on visualization and computer graphics 25.1\n  (2018): 481-490", "doi": "10.1109/TVCG.2018.2865194", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear dimensionality reduction (NDR) methods such as LLE and t-SNE are\npopular with visualization researchers and experienced data analysts, but\npresent serious problems of interpretation. In this paper, we present\nDimReader, a technique that recovers readable axes from such techniques.\nDimReader is based on analyzing infinitesimal perturbations of the dataset with\nrespect to variables of interest. The perturbations define exactly how we want\nto change each point in the original dataset and we measure the effect that\nthese changes have on the projection. The recovered axes are in direct analogy\nwith the axis lines (grid lines) of traditional scatterplots. We also present\nmethods for discovering perturbations on the input data that change the\nprojection the most. The calculation of the perturbations is efficient and\neasily integrated into programs written in modern programming languages. We\npresent results of DimReader on a variety of NDR methods and datasets both\nsynthetic and real-life, and show how it can be used to compare different NDR\nmethods. Finally, we discuss limitations of our proposal and situations where\nfurther research is needed.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 05:28:46 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 02:33:39 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Faust", "Rebecca", ""], ["Glickenstein", "David", ""], ["Scheidegger", "Carlos", ""]]}, {"id": "1710.01370", "submitter": "Jens Grubert", "authors": "Travis Gesslein, Daniel Scherer and Jens Grubert", "title": "BodyDigitizer: An Open Source Photogrammetry-based 3D Body Scanner", "comments": "changed template, minor modifications for camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rising popularity of Augmented and Virtual Reality, there is a need\nfor representing humans as virtual avatars in various application domains\nranging from remote telepresence, games to medical applications. Besides\nexplicitly modelling 3D avatars, sensing approaches that create person-specific\navatars are becoming popular. However, affordable solutions typically suffer\nfrom a low visual quality and professional solution are often too expensive to\nbe deployed in nonprofit projects.\n  We present an open-source project, BodyDigitizer, which aims at providing\nboth build instructions and configuration software for a high-resolution\nphotogrammetry-based 3D body scanner. Our system encompasses up to 96 Rasperry\nPI cameras, active LED lighting, a sturdy frame construction and open-source\nconfiguration software. %We demonstrate the applicability of the body scanner\nin a nonprofit Mixed Reality health project. The detailed build instruction and\nsoftware are available at http://www.bodydigitizer.org.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 20:10:10 GMT"}, {"version": "v2", "created": "Sat, 28 Oct 2017 19:28:06 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Gesslein", "Travis", ""], ["Scherer", "Daniel", ""], ["Grubert", "Jens", ""]]}, {"id": "1710.01541", "submitter": "Martin Cooney", "authors": "Martin Cooney, Sepideh Pashami, Yuantao Fan, Anita Sant'Anna, Yinrong\n  Ma, Tianyi Zhang, Yuwei Zhao, Wolfgang Hotze, Jeremy Heyne, Cristofer\n  Englund, Achim J. Lilienthal, and Tom Ziemke", "title": "Exploring home robot capabilities by medium fidelity prototyping", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for autonomous robots to be able to support people's well-being in\nhomes and everyday environments, new interactive capabilities will be required,\nas exemplified by the soft design used for Disney's recent robot character\nBaymax in popular fiction. Home robots will be required to be easy to interact\nwith and intelligent--adaptive, fun, unobtrusive and involving little effort to\npower and maintain--and capable of carrying out useful tasks both on an\neveryday level and during emergencies. The current article adopts an\nexploratory medium fidelity prototyping approach for testing some new robotic\ncapabilities in regard to recognizing people's activities and intentions and\nbehaving in a way which is transparent to people. Results are discussed with\nthe aim of informing next designs.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 10:45:27 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 10:08:34 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Cooney", "Martin", ""], ["Pashami", "Sepideh", ""], ["Fan", "Yuantao", ""], ["Sant'Anna", "Anita", ""], ["Ma", "Yinrong", ""], ["Zhang", "Tianyi", ""], ["Zhao", "Yuwei", ""], ["Hotze", "Wolfgang", ""], ["Heyne", "Jeremy", ""], ["Englund", "Cristofer", ""], ["Lilienthal", "Achim J.", ""], ["Ziemke", "Tom", ""]]}, {"id": "1710.01772", "submitter": "Yunfeng Zhang", "authors": "Yedendra B. Shrinivasan, Yunfeng Zhang", "title": "CELIO: An application development framework for interactive spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing applications for interactive space is different from developing\ncross-platform applications for personal computing. Input, output, and\narchitectural variations in each interactive space introduce big overhead in\nterms of cost and time for developing, deploying and maintaining applications\nfor interactive spaces. Often, these applications become on-off experience tied\nto the deployed spaces. To alleviate this problem and enable rapid responsive\nspace design applications similar to responsive web design, we present CELIO\napplication development framework for interactive spaces. The framework is\nmicro services based and neatly decouples application and design specifications\nfrom hardware and architecture specifications of an interactive space. In this\npaper, we describe this framework and its implementation details. Also, we\nbriefly discuss the use cases developed using this framework.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 19:19:39 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Shrinivasan", "Yedendra B.", ""], ["Zhang", "Yunfeng", ""]]}, {"id": "1710.01778", "submitter": "Yunfeng Zhang", "authors": "Yunfeng Zhang", "title": "Combining absolute and relative pointing for fast and accurate distant\n  interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional relative pointing devices such as mice and trackpads are\nunsuitable for pointing at distant displays, because they encumber the users by\nrequiring either a flat surface to operate on or being held by two hands. Past\nresearch has examined many new pointing methods, but few could surpass the\nspeed and accuracy of mice and trackpads. This paper introduces a new pointing\nsystem that is developed based on HTC Vive, a relatively low-cost virtual\nreality system, and proposes two methods of combining absolute and relative\npointing. The proposed methods were compared against single-mode pointing\nmethods (i.e., pure absolute pointing and pure relative pointing) in a Fitts'\nlaw study. The results show that with only a short period of practice, one\nhybrid pointing technique enabled faster and more accurate pointing than both\nsingle-mode pointing techniques, which included a trackpad.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 19:42:07 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Zhang", "Yunfeng", ""]]}, {"id": "1710.01832", "submitter": "Marco Cavallo", "authors": "Marco Cavallo, \\c{C}a\\u{g}atay Demiralp", "title": "Track Xplorer: A System for Visual Analysis of Sensor-based Motor\n  Activity Predictions", "comments": "My co-author has submitted the same paper to Arxiv himself, so we\n  have a duplicate arxiv link for the same work. See arXiv:1806.09256", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting motor activities from sensor datasets is becoming increasingly\ncommon in a wide range of applications with the rapid commoditization of\nwearable sensors. To detect activities, data scientists iteratively experiment\nwith different classifiers before deciding on a single model. Evaluating,\ncomparing, and reasoning about prediction results of alternative classifiers is\na crucial step in the process of iterative model development. However, standard\naggregate performance metrics (such as accuracy score) and textual display of\nindividual event sequences have limited granularity and scalability to\neffectively perform this critical step.\n  To ameliorate these limitations, we introduce Track Xplorer, an interactive\nvisualization system to query, analyze and compare the classification output of\nactivity detection in multi-sensor data. Track Xplorer visualizes the results\nof different classifiers as well as the ground truth labels and the video of\nactivities as temporally-aligned linear tracks. Through coordinated track\nvisualizations, Track Xplorer enables users to interactively explore and\ncompare the results of different classifiers, assess their accuracy with\nrespect to the ground truth labels and video. Users can brush arbitrary regions\nof any classifier track, zoom in and out with ease, and playback the\ncorresponding video segment to contextualize the performance of the classifier\nwithin the selected region.\n  Track Xplorer also contributes an algebra over track representations to\nfilter, compose, and compare classification outputs, enabling users to\neffectively reason about the performance of classifiers. We demonstrate how our\ntool helps data scientists debug misclassifications and improve the prediction\nperformance in developing activity classifiers for real-world, multi-sensor\ndata gathered from Parkinson's patients.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 00:20:20 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 00:52:35 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Cavallo", "Marco", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1710.01842", "submitter": "Oren Lederman", "authors": "Oren Lederman, Dan Calacci, Angus MacMullen, Daniel C. Fehder, Fiona\n  E.Murray, Alex 'Sandy' Pentland", "title": "Open Badges: A Low-Cost Toolkit for Measuring Team Communication and\n  Dynamics", "comments": null, "journal-ref": "Lederman, O., Calacci, D., MacMullen, A., Fehder, D. C., Murray,\n  F., & Pentland, A. S. (2016). Open badges: A low-cost toolkit for measuring\n  team communication and dynamics. In Social, Cultural, and Behavioral Modeling", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Open Badges, an open-source framework an toolkit for measuring and\nshaping face-to-face social interactions using either custom hardware devices\nor smart phones, and real-time web-based visualizations. Open Badges is a\nmodular system that allows researchers to monitor and collect interaction data\nfrom people engaged in real-life social settings. In this paper we describe the\ntechnical aspects of the Open Badges project and the motivation for its\ncreation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 00:58:19 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Lederman", "Oren", ""], ["Calacci", "Dan", ""], ["MacMullen", "Angus", ""], ["Fehder", "Daniel C.", ""], ["Murray", "Fiona E.", ""], ["Pentland", "Alex 'Sandy'", ""]]}, {"id": "1710.01916", "submitter": "Luiza Mici", "authors": "Luiza Mici, German I. Parisi, Stefan Wermter", "title": "A self-organizing neural network architecture for learning human-object\n  interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The visual recognition of transitive actions comprising human-object\ninteractions is a key component for artificial systems operating in natural\nenvironments. This challenging task requires jointly the recognition of\narticulated body actions as well as the extraction of semantic elements from\nthe scene such as the identity of the manipulated objects. In this paper, we\npresent a self-organizing neural network for the recognition of human-object\ninteractions from RGB-D videos. Our model consists of a hierarchy of\nGrow-When-Required (GWR) networks that learn prototypical representations of\nbody motion patterns and objects, accounting for the development of\naction-object mappings in an unsupervised fashion. We report experimental\nresults on a dataset of daily activities collected for the purpose of this\nstudy as well as on a publicly available benchmark dataset. In line with\nneurophysiological studies, our self-organizing architecture exhibits higher\nneural activation for congruent action-object pairs learned during training\nsessions with respect to synthetically created incongruent ones. We show that\nour unsupervised model shows competitive classification results on the\nbenchmark dataset with respect to strictly supervised approaches.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 08:40:24 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 09:00:41 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Mici", "Luiza", ""], ["Parisi", "German I.", ""], ["Wermter", "Stefan", ""]]}, {"id": "1710.01966", "submitter": "Erick Peirson", "authors": "B. R. Erick Peirson, Erin Bottino, Julia L. Damerow, Manfred D.\n  Laubichler", "title": "Quantitative Perspectives on Fifty Years of the Journal of the History\n  of Biology", "comments": "45 pages, 14 figures, 4 tables", "journal-ref": null, "doi": "10.1007/s10739-017-9499-2", "report-no": null, "categories": "cs.DL cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Journal of the History of Biology provides a fifty-year long record for\nexamining the evolution of the history of biology as a scholarly discipline. In\nthis paper, we present a new dataset and preliminary quantitative analysis of\nthe thematic content of JHB from the perspectives of geography, organisms, and\nthematic fields. The geographic diversity of authors whose work appears in JHB\nhas increased steadily since 1968, but the geographic coverage of the content\nof JHB articles remains strongly lopsided toward the United States, United\nKingdom, and western Europe and has diversified much less dramatically over\ntime. The taxonomic diversity of organisms discussed in JHB increased steadily\nbetween 1968 and the late 1990s but declined in later years, mirroring broader\npatterns of diversification previously reported in the biomedical research\nliterature. Finally, we used a combination of topic modeling and nonlinear\ndimensionality reduction techniques to develop a model of multi-article fields\nwithin JHB. We found evidence for directional changes in the representation of\nfields on multiple scales. The diversity of JHB with regard to the\nrepresentation of thematic fields has increased overall, with most of that\ndiversification occurring in recent years. Drawing on the dataset generated in\nthe course of this analysis, as well as web services in the emerging digital\nhistory and philosophy of science ecosystem, we have developed an interactive\nweb platform for exploring the content of JHB, and we provide a brief overview\nof the platform in this article. As a whole, the data and analyses presented\nhere provide a starting-place for further critical reflection on the evolution\nof the history of biology over the past half-century.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 11:13:16 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Peirson", "B. R. Erick", ""], ["Bottino", "Erin", ""], ["Damerow", "Julia L.", ""], ["Laubichler", "Manfred D.", ""]]}, {"id": "1710.02173", "submitter": "Cagatay Demiralp", "authors": "\\c{C}a\\u{g}atay Demiralp", "title": "Clustrophile: A Tool for Visual Clustering Analysis", "comments": "KDD IDEA'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While clustering is one of the most popular methods for data mining, analysts\nlack adequate tools for quick, iterative clustering analysis, which is\nessential for hypothesis generation and data reasoning. We introduce\nClustrophile, an interactive tool for iteratively computing discrete and\ncontinuous data clusters, rapidly exploring different choices of clustering\nparameters, and reasoning about clustering instances in relation to data\ndimensions. Clustrophile combines three basic visualizations -- a table of raw\ndatasets, a scatter plot of planar projections, and a matrix diagram (heatmap)\nof discrete clusterings -- through interaction and intermediate visual\nencoding. Clustrophile also contributes two spatial interaction techniques,\n$\\textit{forward projection}$ and $\\textit{backward projection}$, and a\nvisualization method, $\\textit{prolines}$, for reasoning about two-dimensional\nprojections obtained through dimensionality reductions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 18:27:56 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1710.02599", "submitter": "Pulkit Budhiraja", "authors": "Pulkit Budhiraja, Mark Roman Miller, Abhishek K Modi, David Forsyth", "title": "Rotation Blurring: Use of Artificial Blurring to Reduce Cybersickness in\n  Virtual Reality First Person Shooters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of Virtual Reality (VR) systems often experience vection, the\nperception of self-motion in the absence of any physical movement. While\nvection helps to improve presence in VR, it often leads to a form of motion\nsickness called cybersickness. Cybersickness is a major deterrent to large\nscale adoption of VR.\n  Prior work has discovered that changing vection (changing the perceived speed\nor moving direction) causes more severe cybersickness than steady vection\n(walking at a constant speed or in a constant direction). Based on this idea,\nwe try to reduce the cybersickness caused by character movements in a First\nPerson Shooter (FPS) game in VR. We propose Rotation Blurring (RB), uniformly\nblurring the screen during rotational movements to reduce cybersickness. We\nperformed a user study to evaluate the impact of RB in reducing cybersickness.\nWe found that the blurring technique led to an overall reduction in sickness\nlevels of the participants and delayed its onset. Participants who experienced\nacute levels of cybersickness benefited significantly from this technique.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 22:02:00 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Budhiraja", "Pulkit", ""], ["Miller", "Mark Roman", ""], ["Modi", "Abhishek K", ""], ["Forsyth", "David", ""]]}, {"id": "1710.02609", "submitter": "Shuo Zhang", "authors": "Shuo Zhang, Khaled Alanezi, Mike Gartrell, Richard Han, Qin Lv,\n  Shivakaht Mishra", "title": "Understanding Group Event Scheduling via the OutWithFriendz Mobile\n  Application", "comments": "accepted for publication in ACM IMWUT (Ubicomp) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide adoption of smartphones and mobile applications has brought\nsignificant changes to not only how individuals behave in the real world, but\nalso how groups of users interact with each other when organizing group events.\nUnderstanding how users make event decisions as a group and identifying the\ncontributing factors can offer important insights for social group studies and\nmore effective system and application design for group event scheduling.\n  In this work, we have designed a new mobile application called\nOutWithFriendz, which enables users of our mobile app to organize group events,\ninvite friends, suggest and vote on event time and venue. We have deployed\nOutWithFriendz at both Apple App Store and Google Play, and conducted a\nlarge-scale user study spanning over 500 users and 300 group events. Our\nanalysis has revealed several important observations regarding group event\nplanning process including the importance of user mobility, individual\npreferences, host preferences, and group voting process.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 23:46:40 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Zhang", "Shuo", ""], ["Alanezi", "Khaled", ""], ["Gartrell", "Mike", ""], ["Han", "Richard", ""], ["Lv", "Qin", ""], ["Mishra", "Shivakaht", ""]]}, {"id": "1710.02862", "submitter": "Mahsa Mirzargar", "authors": "Mahsa Mirzargar and Ross T. Whitaker and Robert M. Kirby", "title": "Exploration of Heterogeneous Data Using Robust Similarity", "comments": "Presented at Visualization in Data Science (VDS at IEEE VIS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous data pose serious challenges to data analysis tasks, including\nexploration and visualization. Current techniques often utilize dimensionality\nreductions, aggregation, or conversion to numerical values to analyze\nheterogeneous data. However, the effectiveness of such techniques to find\nsubtle structures such as the presence of multiple modes or detection of\noutliers is hindered by the challenge to find the proper subspaces or prior\nknowledge to reveal the structures. In this paper, we propose a generic\nsimilarity-based exploration technique that is applicable to a wide variety of\ndatatypes and their combinations, including heterogeneous ensembles. The\nproposed concept of similarity has a close connection to statistical analysis\nand can be deployed for summarization, revealing fine structures such as the\npresence of multiple modes, and detection of anomalies or outliers. We then\npropose a visual encoding framework that enables the exploration of a\nheterogeneous dataset in different levels of detail and provides insightful\ninformation about both global and local structures. We demonstrate the utility\nof the proposed technique using various real datasets, including ensemble data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 17:43:23 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Mirzargar", "Mahsa", ""], ["Whitaker", "Ross T.", ""], ["Kirby", "Robert M.", ""]]}, {"id": "1710.03088", "submitter": "Mahdi Miraz", "authors": "Mohammed Fakrudeen, Sufian Yousef, Mahdi H. Miraz and AbdelRahman\n  Hamza Hussein", "title": "Finger Based Techniques for Nonvisual Touchscreen Text Entry", "comments": "arXiv admin note: substantial text overlap with arXiv:1708.05073", "journal-ref": "IAENG Transactions on Engineering Sciences, Chapter 28, pp.\n  372-386, April 2015", "doi": "10.1142/9789814667364_0028", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research proposes Finger Based Technique (FBT) for non-visual touch\nscreen device interaction designed for blind users. Based on the proposed\ntechnique, the blind user can access virtual keys based on finger holding\npositions. Three different models have been proposed. They are Single Digit\nFinger-Digit Input (FDI), Double Digit FDI for digital text entry, and\nFinger-Text Input (FTI) for normal text entry. All the proposed models were\nimplemented with voice feedback while enabling touch as the input gesture. The\nmodels were evaluated with 7 blind participants with Samsung Galaxy S2\napparatus. The results show that Single Digit FDI is substantially faster and\nmore accurate than Double Digit FDI and iPhone voice-over. FTI also looks\npromising for text entry. Our study also reveals 11 accessible regions to place\nwidgets for quick access by blind users in flat touch screen based smartphones.\nIdentification of these accessible regions will promote dynamic interactions\nfor blind users and serve as a usability design framework for touch screen\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 12:36:49 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Fakrudeen", "Mohammed", ""], ["Yousef", "Sufian", ""], ["Miraz", "Mahdi H.", ""], ["Hussein", "AbdelRahman Hamza", ""]]}, {"id": "1710.03299", "submitter": "Sasan Tavakkol", "authors": "Roshanak Alialy, Sasan Tavakkol, Elham Tavakkol, Amir\n  Ghorbani-Aghbologhi, Alireza Ghaffarieh, Seon Ho Kim, Cyrus Shahabi", "title": "A Review on the Applications of Crowdsourcing in Human Pathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of the digital pathology has introduced new avenues of diagnostic\nmedicine. Among them, crowdsourcing has attracted researchers' attention in the\nrecent years, allowing them to engage thousands of untrained individuals in\nresearch and diagnosis. While there exist several articles in this regard,\nprior works have not collectively documented them. We, therefore, aim to review\nthe applications of crowdsourcing in human pathology in a semi-systematic\nmanner. We firstly, introduce a novel method to do a systematic search of the\nliterature. Utilizing this method, we, then, collect hundreds of articles and\nscreen them against a pre-defined set of criteria. Furthermore, we crowdsource\npart of the screening process, to examine another potential application of\ncrowdsourcing. Finally, we review the selected articles and characterize the\nprior uses of crowdsourcing in pathology.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 20:18:19 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 22:17:12 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Alialy", "Roshanak", ""], ["Tavakkol", "Sasan", ""], ["Tavakkol", "Elham", ""], ["Ghorbani-Aghbologhi", "Amir", ""], ["Ghaffarieh", "Alireza", ""], ["Kim", "Seon Ho", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "1710.03320", "submitter": "Alina Striner", "authors": "Alina Striner, Sasha Azad, Chris Martens", "title": "A Common Framework for Audience Interactivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audience interactivity is interpreted differently across domains. This\nresearch develops a framework to describe audience interactivity across a broad\nrange of experiences. We build on early work characterizing child audience\ninteractivity experiences, expanding on these findings with an extensive review\nof literature in theater, games, and theme parks, paired with expert interviews\nin those domains. The framework scaffolds interactivity as nested spheres of\naudience influence, and comprises a series of dimensions of audience\ninteractivity including a Spectrum of Audience Interactivity. This framework\naims to develop a common taxonomy for researchers and practitioners working\nwith audience interactivity experiences.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 21:20:00 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 22:45:25 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Striner", "Alina", ""], ["Azad", "Sasha", ""], ["Martens", "Chris", ""]]}, {"id": "1710.03755", "submitter": "Anindya Maiti", "authors": "Nisha Vinayaga-Sureshkanth, Anindya Maiti, Murtuza Jadliwala, Kirsten\n  Crager, Jibo He, Heena Rathore", "title": "Towards a Practical Pedestrian Distraction Detection Framework using\n  Wearables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pedestrian safety continues to be a significant concern in urban communities\nand pedestrian distraction is emerging as one of the main causes of grave and\nfatal accidents involving pedestrians. The advent of sophisticated mobile and\nwearable devices, equipped with high-precision on-board sensors capable of\nmeasuring fine-grained user movements and context, provides a tremendous\nopportunity for designing effective pedestrian safety systems and applications.\nAccurate and efficient recognition of pedestrian distractions in real-time\ngiven the memory, computation and communication limitations of these devices,\nhowever, remains the key technical challenge in the design of such systems.\nEarlier research efforts in pedestrian distraction detection using data\navailable from mobile and wearable devices have primarily focused only on\nachieving high detection accuracy, resulting in designs that are either\nresource intensive and unsuitable for implementation on mainstream mobile\ndevices, or computationally slow and not useful for real-time pedestrian safety\napplications, or require specialized hardware and less likely to be adopted by\nmost users. In the quest for a pedestrian safety system that achieves a\nfavorable balance between computational efficiency, detection accuracy, and\nenergy consumption, this paper makes the following main contributions: (i)\ndesign of a novel complex activity recognition framework which employs motion\ndata available from users' mobile and wearable devices and a lightweight\nfrequency matching approach to accurately and efficiently recognize complex\ndistraction related activities, and (ii) a comprehensive comparative evaluation\nof the proposed framework with well-known complex activity recognition\ntechniques in the literature with the help of data collected from human subject\npedestrians and prototype implementations on commercially-available mobile and\nwearable devices.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 15:50:16 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Vinayaga-Sureshkanth", "Nisha", ""], ["Maiti", "Anindya", ""], ["Jadliwala", "Murtuza", ""], ["Crager", "Kirsten", ""], ["He", "Jibo", ""], ["Rathore", "Heena", ""]]}, {"id": "1710.03888", "submitter": "Nalin Asanka Gamagedara Arachchilage", "authors": "Nicholas Micallef and Nalin Asanka Gamagedara Arachchilage", "title": "Involving Users in the Design of a Serious Game for Security Questions\n  Education", "comments": "10; International Symposium on Human Aspects of Information Security\n  & Assurance (HAISA), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using security questions most users still trade-off security for the\nconvenience of memorability. This happens because most users find strong\nanswers to security questions difficult to remember. Previous research in\nsecurity education was successful in motivating users to change their behaviour\ntowards security issues, through the use of serious games (i.e. games designed\nfor a primary purpose other than pure entertainment). Hence, in this paper we\nevaluate the design of a serious game, to investigate the features and\nfunctionalities that users would find desirable in a game that aims to educate\nthem to provide strong and memorable answers to security questions. Our\nfindings reveal that: (1) even for security education games, rewards seem to\nmotivate users to have a better learning experience; (2) functionalities which\ncontain a social element (e.g. getting help from other players) do not seem\nappropriate for serious games related to security questions, because users fear\nthat their acquaintances could gain access to their security questions; (3)\neven users who do not usually play games would seem to prefer to play security\neducation games on a mobile device.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 03:00:31 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Micallef", "Nicholas", ""], ["Arachchilage", "Nalin Asanka Gamagedara", ""]]}, {"id": "1710.03889", "submitter": "Kazuki Otao", "authors": "Yoichi Ochiai, Kazuki Otao, Hiroyuki Osone", "title": "Air Mounted Eyepiece: Design Methods for Aerial Optical Functions of\n  Near-Eye and See-Through Display using Transmissive Mirror Device", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to implement an optical see-through head mounted\ndisplay which renders real aerial images with a wide viewing angle, called an\nAir Mounted Eyepiece (AME). To achieve the AMD design, we employ an\noff-the-shelf head mounted display and Transmissive Mirror Device (TMD) which\nis usually used in aerial real imaging systems. In the proposed method, we\nreplicate the function of the head mounted display (HMD) itself, which is used\nin the air by using the TMD and presenting a real image of eyepiece in front of\nthe eye. Moreover, it can realize a wide viewing angle 3D display by placing a\nvirtual lens in front of the eye without wearing an HMD. In addition to\nenhancing the experience of mixed reality and augmented reality, our proposed\nmethod can be used as a 3D imaging method for use in other applications such as\nin automobiles and desktop work. We aim to contribute to the field of\nhuman-computer interaction and the research on eyepiece interfaces by\ndiscussing the advantages and the limitations of this near-eye optical system.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 03:08:02 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Ochiai", "Yoichi", ""], ["Otao", "Kazuki", ""], ["Osone", "Hiroyuki", ""]]}, {"id": "1710.04044", "submitter": "Bj\\\"orn Ross", "authors": "Stefan Stieglitz, Florian Brachten, Bj\\\"orn Ross and Anna-Katharina\n  Jung", "title": "Do Social Bots Dream of Electric Sheep? A Categorisation of Social Media\n  Bot Accounts", "comments": "Accepted for publication in the Proceedings of the Australasian\n  Conference on Information Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So-called 'social bots' have garnered a lot of attention lately. Previous\nresearch showed that they attempted to influence political events such as the\nBrexit referendum and the US presidential elections. It remains, however,\nsomewhat unclear what exactly can be understood by the term 'social bot'. This\npaper addresses the need to better understand the intentions of bots on social\nmedia and to develop a shared understanding of how 'social' bots differ from\nother types of bots. We thus describe a systematic review of publications that\nresearched bot accounts on social media. Based on the results of this\nliterature review, we propose a scheme for categorising bot accounts on social\nmedia sites. Our scheme groups bot accounts by two dimensions - Imitation of\nhuman behaviour and Intent.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 13:02:05 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Stieglitz", "Stefan", ""], ["Brachten", "Florian", ""], ["Ross", "Bj\u00f6rn", ""], ["Jung", "Anna-Katharina", ""]]}, {"id": "1710.04203", "submitter": "Giannis Haralabopoulos", "authors": "Giannis Haralabopoulos and Elena Simperl", "title": "Crowdsourcing for Beyond Polarity Sentiment Analysis A Pure Emotion\n  Lexicon", "comments": "Keywords: Beyond Polarity, Pure Sentiment, Crowdsourcing, Sentiment\n  Analysis, Lexicon Acquisition, Reddit, Twitter, Brexit [19 pages, 6 figures,\n  4 tables]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis aims to uncover emotions conveyed through information. In\nits simplest form, it is performed on a polarity basis, where the goal is to\nclassify information with positive or negative emotion. Recent research has\nexplored more nuanced ways to capture emotions that go beyond polarity. For\nthese methods to work, they require a critical resource: a lexicon that is\nappropriate for the task at hand, in terms of the range of emotions it captures\ndiversity. In the past, sentiment analysis lexicons have been created by\nexperts, such as linguists and behavioural scientists, with strict rules.\nLexicon evaluation was also performed by experts or gold standards. In our\npaper, we propose a crowdsourcing method for lexicon acquisition, which is\nscalable, cost-effective, and doesn't require experts or gold standards. We\nalso compare crowd and expert evaluations of the lexicon, to assess the overall\nlexicon quality, and the evaluation capabilities of the crowd.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 21:38:48 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Haralabopoulos", "Giannis", ""], ["Simperl", "Elena", ""]]}, {"id": "1710.04205", "submitter": "Jalal Mahmud", "authors": "Bin Xu, Liang Gou, Anbang Xu, Jalal Mahmud, Dan Cosley", "title": "Raising Awareness of Conveyed Personality In Social Media Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users' persistent social media contents like posts on Facebook Timeline are\npresented as an \"exhibition\" about the person to others, and managing these\nexhibitional contents for impression management needs intentional and manual\nefforts. To raise awareness of and facilitate impression management around past\ncontents, we developed a prototype called PersonalityInsight. The system\nemploys computational psycho-linguistic analysis to help users visualize the\nway their past text posts might convey impressions of their personality and\nallowed users to modify their posts based on these visualizations. We conducted\na user study to evaluate the design; users overall found that such a tool\nraised awareness of the fact and the ways personality might be conveyed through\ntheir past content as one aspect of impression management, but that it needs\ndesign improvement to offer action-able suggestions for content modification,\nas well as careful thinking about impression management as one of many values\npeople have about their digital past.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 03:52:04 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Xu", "Bin", ""], ["Gou", "Liang", ""], ["Xu", "Anbang", ""], ["Mahmud", "Jalal", ""], ["Cosley", "Dan", ""]]}, {"id": "1710.04486", "submitter": "Dominique Vaufreydaz", "authors": "Thomas Guntz (LIG), Raffaella Balzarini (LIG), Dominique Vaufreydaz\n  (LIG, UGA), James L. Crowley (Grenoble INP, LIG)", "title": "Multimodal Observation and Interpretation of Subjects Engaged in Problem\n  Solving", "comments": null, "journal-ref": "1st Workshop on \"Behavior, Emotion and Representation: Building\n  Blocks of Interaction'', Oct 2017, Bielefeld, Germany. 2017", "doi": null, "report-no": null, "categories": "cs.HC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the first results of a pilot experiment in the\ncapture and interpretation of multimodal signals of human experts engaged in\nsolving challenging chess problems. Our goal is to investigate the extent to\nwhich observations of eye-gaze, posture, emotion and other physiological\nsignals can be used to model the cognitive state of subjects, and to explore\nthe integration of multiple sensor modalities to improve the reliability of\ndetection of human displays of awareness and emotion. We observed chess players\nengaged in problems of increasing difficulty while recording their behavior.\nSuch recordings can be used to estimate a participant's awareness of the\ncurrent situation and to predict ability to respond effectively to challenging\nsituations. Results show that a multimodal approach is more accurate than a\nunimodal one. By combining body posture, visual attention and emotion, the\nmultimodal approach can reach up to 93% of accuracy when determining player's\nchess expertise while unimodal approach reaches 86%. Finally this experiment\nvalidates the use of our equipment as a general and reproducible tool for the\nstudy of participants engaged in screen-based interaction and/or problem\nsolving.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 12:59:42 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Guntz", "Thomas", "", "LIG"], ["Balzarini", "Raffaella", "", "LIG"], ["Vaufreydaz", "Dominique", "", "LIG, UGA"], ["Crowley", "James L.", "", "Grenoble INP, LIG"]]}, {"id": "1710.04608", "submitter": "Yue Zhao", "authors": "Yue Zhao, Zhongtian Qiu, Yiqing Yang, Weiwei Li, Mingming Fan", "title": "An empirical study of touch-based authentication methods on smartwatches", "comments": "ISWC '17, Proceedings of the 2017 ACM International Symposium on\n  Wearable Computers, 122-125, ACM New York, NY, USA", "journal-ref": null, "doi": "10.1145/3123021.3123049", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of smartwatches poses new challenges to information security.\nAlthough there are mature touch-based authentication methods for smartphones,\nthe effectiveness of using these methods on smartwatches is still unclear. We\nconducted a user study (n=16) to evaluate how authentication methods (PIN and\nPattern), UIs (Square and Circular), and display sizes (38mm and 42mm) affect\nauthentication accuracy, speed, and security. Circular UIs are tailored to\nsmartwatches with fewer UI elements. Results show that 1) PIN is more accurate\nand secure than Pattern; 2) Pattern is much faster than PIN; 3) Square UIs are\nmore secure but less accurate than Circular UIs; 4) display size does not\naffect accuracy or speed, but security; 5) Square PIN is the most secure method\nof all. The study also reveals a security concern that participants' favorite\nmethod is not the best in any of the measures. We finally discuss implications\nfor future touch-based smartwatch authentication design.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 16:46:27 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Zhao", "Yue", ""], ["Qiu", "Zhongtian", ""], ["Yang", "Yiqing", ""], ["Li", "Weiwei", ""], ["Fan", "Mingming", ""]]}, {"id": "1710.04881", "submitter": "Pedram Daee", "authors": "Pedram Daee, Tomi Peltola, Aki Vehtari, Samuel Kaski", "title": "User Modelling for Avoiding Overfitting in Interactive Knowledge\n  Elicitation for Prediction", "comments": "9 pages, 2 figures. The paper is published in the proceedings of IUI\n  2018. Codes and data available at\n  https://github.com/HIIT/human-overfitting-in-IML", "journal-ref": null, "doi": "10.1145/3172944.3172989", "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human-in-the-loop machine learning, the user provides information beyond\nthat in the training data. Many algorithms and user interfaces have been\ndesigned to optimize and facilitate this human--machine interaction; however,\nfewer studies have addressed the potential defects the designs can cause.\nEffective interaction often requires exposing the user to the training data or\nits statistics. The design of the system is then critical, as this can lead to\ndouble use of data and overfitting, if the user reinforces noisy patterns in\nthe data. We propose a user modelling methodology, by assuming simple rational\nbehaviour, to correct the problem. We show, in a user study with 48\nparticipants, that the method improves predictive performance in a sparse\nlinear regression sentiment analysis task, where graded user knowledge on\nfeature relevance is elicited. We believe that the key idea of inferring user\nknowledge with probabilistic user models has general applicability in guarding\nagainst overfitting and improving interactive machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 11:52:19 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 02:06:27 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Daee", "Pedram", ""], ["Peltola", "Tomi", ""], ["Vehtari", "Aki", ""], ["Kaski", "Samuel", ""]]}, {"id": "1710.05044", "submitter": "Youngjun Cho", "authors": "Youngjun Cho, Nadia Bianchi-Berthouze, Simon J. Julier, Nicolai\n  Marquardt", "title": "ThermSense: Smartphone-based Breathing Sensing Platform using Noncontact\n  Low-Cost Thermal Camera", "comments": "The Seventh AAAC International Conference on Affective Computing and\n  Intelligent Interaction Workshops and Demos", "journal-ref": null, "doi": "10.1109/ACIIW.2017.8272593", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of sensing breathing is becoming an increasingly important\nfunction for technology that aims at supporting both psychological and physical\nwellbeing. We demonstrate ThermSense, a new breathing sensing platform based on\nsmartphone technology and low-cost thermal camera, which allows a user to\nmeasure his/her breathing pattern in a contact-free manner. With the designed\nkey functions of Thermal Voxel Integration-based breathing estimation and\nrespiration variability spectrogram (RVS, bi-dimensional representation of\nbreathing dynamics), the developed platform provides scalability and\nflexibility for gathering respiratory physiological measurements ubiquitously.\nThe functionality could be used for a variety of applications from stress\nmonitoring to respiration training.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 18:14:27 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Cho", "Youngjun", ""], ["Bianchi-Berthouze", "Nadia", ""], ["Julier", "Simon J.", ""], ["Marquardt", "Nicolai", ""]]}, {"id": "1710.05319", "submitter": "Amit Milstein", "authors": "Amit Milstein, Tzvi Ganel, Sigal Berman, and Ilana Nisky", "title": "Human-centered transparency of grasping via a robot-assisted minimally\n  invasive surgery system", "comments": "10 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate grasping of rigid objects in unilateral robot-assisted\nminimally invasive surgery (RAMIS) in this paper. We define a human-centered\ntransparency that quantifies natural action and perception in RAMIS. We\ndemonstrate this human-centered transparency analysis for different values of\ngripper scaling - the scaling between the grasp aperture of the surgeon-side\nmanipulator and the aperture of the surgical instrument grasper. Thirty-one\nparticipants performed teleoperated grasping and perceptual assessment of rigid\nobjects in one of three gripper scaling conditions (fine, normal, and quick,\ntrading off precision and responsiveness). Psychophysical analysis of the\nvariability of maximal grasping aperture during prehension and of the reported\nsize of the object revealed that in normal and quick (but not in the fine)\ngripper scaling conditions, teleoperated grasping with our system was similar\nto natural grasping, and therefore, human-centered transparent. We anticipate\nthat using motor control and psychophysics for human-centered optimizing of\nteleoperation control will eventually improve the usability of RAMIS.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 11:38:20 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 05:34:18 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Milstein", "Amit", ""], ["Ganel", "Tzvi", ""], ["Berman", "Sigal", ""], ["Nisky", "Ilana", ""]]}, {"id": "1710.05604", "submitter": "Mariano Rico", "authors": "Mariano Rico, Jos\\'e Manuel G\\'omez-P\\'erez, Rafael Gonzalez, Aleix\n  Garrido, Oscar Corcho", "title": "Collaboration Spheres: a Visual Metaphor to Share and Reuse Research\n  Objects", "comments": "The URL to the web app does not work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research Objects (ROs) are semantically enhanced aggregations of resources\nassociated to scientific experiments, such as data, provenance of these data,\nthe scientific workflow used to run the experiment, intermediate results, logs\nand the interpretation of the results. As the number of ROs increases, it is\nbecoming difficult to find ROs to be used, reused or re-purposed. New search\nand retrieval techniques are required to find the most appropriate ROs for a\ngiven researcher, paying attention to provide an intuitive user interface. In\nthis paper we show CollabSpheres, a user interface that provides a new visual\nmetaphor to find ROs by means of a recommendation system that takes advantage\nof the social aspects of ROs. The experimental evaluation of this tool shows\nthat users perceive high values of usability, user satisfaction, usefulness and\nease of use. From the analysis of these results we argue that users perceive\nthe simplicity, intuitiveness and cleanness of this tool, as well as this tool\nincreases collaboration and reuse of research objects.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 10:19:45 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Rico", "Mariano", ""], ["G\u00f3mez-P\u00e9rez", "Jos\u00e9 Manuel", ""], ["Gonzalez", "Rafael", ""], ["Garrido", "Aleix", ""], ["Corcho", "Oscar", ""]]}, {"id": "1710.05913", "submitter": "Szymon Wasik", "authors": "Szymon Wasik (1 and 2), Maciej Antczak (1), Jan Badura (1), Artur\n  Laskowski (1), Tomasz Sternal (1) ((1) Institute of Computing Science, Poznan\n  University of Technology, (2) Institute of Bioorganic Chemistry, Polish\n  Academy of Sciences)", "title": "A Survey on Online Judge Systems and Their Applications", "comments": "Authors pre-print of the article accepted for publication in ACM\n  Computing Surveys (accepted on 19-Sep-2017)", "journal-ref": "ACM Computing Surveys 51(1), April 2018, Article No. 3", "doi": "10.1145/3143560", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online judges are systems designed for the reliable evaluation of algorithm\nsource code submitted by users, which is next compiled and tested in a\nhomogeneous environment. Online judges are becoming popular in various\napplications. Thus, we would like to review the state of the art for these\nsystems. We classify them according to their principal objectives into systems\nsupporting organization of competitive programming contests, enhancing\neducation and recruitment processes, facilitating the solving of data mining\nchallenges, online compilers and development platforms integrated as components\nof other custom systems. Moreover, we introduce a formal definition of an\nonline judge system and summarize the common evaluation methodology supported\nby such systems. Finally, we briefly discuss an Optil.io platform as an example\nof an online judge system, which has been proposed for the solving of complex\noptimization problems. We also analyze the competition results conducted using\nthis platform. The competition proved that online judge systems, strengthened\nby crowdsourcing concepts, can be successfully applied to accurately and\nefficiently solve complex industrial- and science-driven challenges.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 14:29:19 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Wasik", "Szymon", "", "1 and 2"], ["Antczak", "Maciej", ""], ["Badura", "Jan", ""], ["Laskowski", "Artur", ""], ["Sternal", "Tomasz", ""]]}, {"id": "1710.06291", "submitter": "Andreas Mathisen", "authors": "Andreas Mathisen (1) and Kaj Gr{\\o}nb{\\ae}k (1) ((1) Department of\n  Computer Science, Aarhus University)", "title": "Clear Visual Separation of Temporal Event Sequences", "comments": "In Proceedings of the 3rd IEEE Symposium on Visualization in Data\n  Science (VDS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting and visualizing informative insights from temporal event sequences\nbecomes increasingly difficult when data volume and variety increase. Besides\ndealing with high event type cardinality and many distinct sequences, it can be\ndifficult to tell whether it is appropriate to combine multiple events into one\nor utilize additional information about event attributes. Existing approaches\noften make use of frequent sequential patterns extracted from the dataset,\nhowever, these patterns are limited in terms of interpretability and utility.\nIn addition, it is difficult to assess the role of absolute and relative time\nwhen using pattern mining techniques.\n  In this paper, we present methods that addresses these challenges by\nautomatically learning composite events which enables better aggregation of\nmultiple event sequences. By leveraging event sequence outcomes, we present\nappropriate linked visualizations that allow domain experts to identify\ncritical flows, to assess validity and to understand the role of time.\nFurthermore, we explore information gain and visual complexity metrics to\nidentify the most relevant visual patterns. We compare composite event learning\nwith two approaches for extracting event patterns using real world company\nevent data from an ongoing project with the Danish Business Authority.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 14:01:55 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Mathisen", "Andreas", ""], ["Gr\u00f8nb\u00e6k", "Kaj", ""]]}, {"id": "1710.06406", "submitter": "Claire Bonial", "authors": "Claire Bonial, Matthew Marge, Ron artstein, Ashley Foots, Felix\n  Gervits, Cory J. Hayes, Cassidy Henry, Susan G. Hill, Anton Leuski, Stephanie\n  M. Lukin, Pooja Moolchandani, Kimberly A. Pollard, David Traum, Clare R. Voss", "title": "Laying Down the Yellow Brick Road: Development of a Wizard-of-Oz\n  Interface for Collecting Human-Robot Dialogue", "comments": "7 pages, 2 figures, accepted for oral presentation at the Symposium\n  on Natural Communication for Human-Robot Collaboration, AAAI Fall Symposium\n  Series, November 9-11, 2017, https://www.aaai.org/ocs/index.php/FSS/FSS17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the adaptation and refinement of a graphical user interface\ndesigned to facilitate a Wizard-of-Oz (WoZ) approach to collecting human-robot\ndialogue data. The data collected will be used to develop a dialogue system for\nrobot navigation. Building on an interface previously used in the development\nof dialogue systems for virtual agents and video playback, we add templates\nwith open parameters which allow the wizard to quickly produce a wide variety\nof utterances. Our research demonstrates that this approach to data collection\nis viable as an intermediate step in developing a dialogue system for physical\nrobots in remote locations from their users - a domain in which the human and\nrobot need to regularly verify and update a shared understanding of the\nphysical environment. We show that our WoZ interface and the fixed set of\nutterances and templates therein provide for a natural pace of dialogue with\ngood coverage of the navigation domain.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 17:34:31 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Bonial", "Claire", ""], ["Marge", "Matthew", ""], ["artstein", "Ron", ""], ["Foots", "Ashley", ""], ["Gervits", "Felix", ""], ["Hayes", "Cory J.", ""], ["Henry", "Cassidy", ""], ["Hill", "Susan G.", ""], ["Leuski", "Anton", ""], ["Lukin", "Stephanie M.", ""], ["Moolchandani", "Pooja", ""], ["Pollard", "Kimberly A.", ""], ["Traum", "David", ""], ["Voss", "Clare R.", ""]]}, {"id": "1710.06615", "submitter": "Christian Tominski", "authors": "Davide Ceneda, Theresia Gschwandtner, Thorsten May, Silvia Miksch,\n  Hans-J\\\"org Schulz, Marc Streit, Christian Tominski", "title": "Amending the Characterization of Guidance in Visual Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At VAST 2016, a characterization of guidance has been presented. It includes\na definition of guidance and a model of guidance based on van Wijk's model of\nvisualization. This note amends the original characterization of guidance in\ntwo aspects. First, we provide a clarification of what guidance actually is\n(and is not). Second, we insert into the model a conceptually relevant link\nthat was missing in the original version.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 08:24:07 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Ceneda", "Davide", ""], ["Gschwandtner", "Theresia", ""], ["May", "Thorsten", ""], ["Miksch", "Silvia", ""], ["Schulz", "Hans-J\u00f6rg", ""], ["Streit", "Marc", ""], ["Tominski", "Christian", ""]]}, {"id": "1710.06654", "submitter": "Zachary Pardos", "authors": "Zachary A. Pardos, Lev Horodyskyj", "title": "Analysis of Student Behaviour in Habitable Worlds Using Continuous\n  Representation Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to visualizing temporal clickstream behaviour\nin the context of a degree-satisfying online course, Habitable Worlds, offered\nthrough Arizona State University. The current practice for visualizing\nbehaviour within a digital learning environment has been to generate plots\nbased on hand engineered or coded features using domain knowledge. While this\napproach has been effective in relating behaviour to known phenomena, features\ncrafted from domain knowledge are not likely well suited to make unfamiliar\nphenomena salient and thus can preclude discovery. We introduce a methodology\nfor organically surfacing behavioural regularities from clickstream data,\nconducting an expert in-the-loop hyperparameter search, and identifying\nanticipated as well as newly discovered patterns of behaviour. While these\nvisualization techniques have been used before in the broader machine learning\ncommunity to better understand neural networks and relationships between word\nvectors, we apply them to online behavioural learner data and go a step\nfurther; exploring the impact of the parameters of the model on producing\ntangible, non-trivial observations of behaviour that are suggestive of\npedagogical improvement to the course designers and instructors. The\nmethodology introduced in this paper led to an improved understanding of\npassing and non-passing student behaviour in the course and is widely\napplicable to other datasets of clickstream activity where investigators and\nstakeholders wish to organically surface principal patterns of behaviour.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 10:05:19 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 03:28:42 GMT"}, {"version": "v3", "created": "Tue, 25 Dec 2018 08:48:14 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Pardos", "Zachary A.", ""], ["Horodyskyj", "Lev", ""]]}, {"id": "1710.06785", "submitter": "Ramviyas Parasuraman", "authors": "Ramviyas Parasuraman, Sergio Caccamo, Fredrik B{\\aa}berg, Petter\n  \\\"Ogren and Mark Neerincx", "title": "A New UGV Teleoperation Interface for Improved Awareness of Network\n  Connectivity and Physical Surroundings", "comments": "Accepted for publication in the Journal of Human-Robot Interaction\n  (JHRI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reliable wireless connection between the operator and the teleoperated\nUnmanned Ground Vehicle (UGV) is critical in many Urban Search and Rescue\n(USAR) missions. Unfortunately, as was seen in e.g. the Fukushima disaster, the\nnetworks available in areas where USAR missions take place are often severely\nlimited in range and coverage. Therefore, during mission execution, the\noperator needs to keep track of not only the physical parts of the mission,\nsuch as navigating through an area or searching for victims, but also the\nvariations in network connectivity across the environment. In this paper, we\npropose and evaluate a new teleoperation User Interface (UI) that includes a\nway of estimating the Direction of Arrival (DoA) of the Radio Signal Strength\n(RSS) and integrating the DoA information in the interface. The evaluation\nshows that using the interface results in more objects found, and less aborted\nmissions due to connectivity problems, as compared to a standard interface. The\nproposed interface is an extension to an existing interface centered around the\nvideo stream captured by the UGV. But instead of just showing the network\nsignal strength in terms of percent and a set of bars, the additional\ninformation of DoA is added in terms of a color bar surrounding the video feed.\nWith this information, the operator knows what movement directions are safe,\neven when moving in regions close to the connectivity threshold.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 21:39:53 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 17:50:45 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 08:57:19 GMT"}, {"version": "v4", "created": "Tue, 7 Nov 2017 19:34:45 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Parasuraman", "Ramviyas", ""], ["Caccamo", "Sergio", ""], ["B\u00e5berg", "Fredrik", ""], ["\u00d6gren", "Petter", ""], ["Neerincx", "Mark", ""]]}, {"id": "1710.06925", "submitter": "Jessica Hair", "authors": "Tim Sodergren, Jessica Hair, Jeff M. Phillips, Bei Wang", "title": "Visualizing Sensor Network Coverage with Location Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interactive visualization system for exploring the coverage in\nsensor networks with uncertain sensor locations. We consider a simple case of\nuncertainty where the location of each sensor is confined to a discrete number\nof points sampled uniformly at random from a region with a fixed radius.\nEmploying techniques from topological data analysis, we model and visualize\nnetwork coverage by quantifying the uncertainty defined on its simplicial\ncomplex representations. We demonstrate the capabilities and effectiveness of\nour tool via the exploration of randomly distributed sensor networks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 20:26:37 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Sodergren", "Tim", ""], ["Hair", "Jessica", ""], ["Phillips", "Jeff M.", ""], ["Wang", "Bei", ""]]}, {"id": "1710.07029", "submitter": "Daniel Seebacher", "authors": "Daniel Seebacher, Johannes H\\\"au{\\ss}ler, Michael Hundt, Manuel Stein,\n  Hannes M\\\"uller, Ulrich Engelke, Daniel Keim", "title": "Visual Analysis of Spatio-Temporal Event Predictions: Investigating the\n  Spread Dynamics of Invasive Species", "comments": null, "journal-ref": "Symposium on Visualization in Data Science (VDS) at IEEE VIS 2017", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invasive species are a major cause of ecological damage and commercial\nlosses. A current problem spreading in North America and Europe is the vinegar\nfly Drosophila suzukii. Unlike other Drosophila, it infests non-rotting and\nhealthy fruits and is therefore of concern to fruit growers, such as vintners.\nConsequently, large amounts of data about infestations have been collected in\nrecent years. However, there is a lack of interactive methods to investigate\nthis data. We employ ensemble-based classification to predict areas susceptible\nto infestation by D. suzukii and bring them into a spatio-temporal context\nusing maps and glyph-based visualizations. Following the information-seeking\nmantra, we provide a visual analysis system Drosophigator for spatio-temporal\nevent prediction, enabling the investigation of the spread dynamics of invasive\nspecies. We demonstrate the usefulness of this approach in two use cases.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 07:56:56 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Seebacher", "Daniel", ""], ["H\u00e4u\u00dfler", "Johannes", ""], ["Hundt", "Michael", ""], ["Stein", "Manuel", ""], ["M\u00fcller", "Hannes", ""], ["Engelke", "Ulrich", ""], ["Keim", "Daniel", ""]]}, {"id": "1710.07322", "submitter": "Bruno Schneider", "authors": "Bruno Schneider, Dominik J\\\"ackle, Florian Stoffel, Alexandra Diehl,\n  Johannes Fuchs and Daniel Keim", "title": "Visual Integration of Data and Model Space in Ensemble Learning", "comments": "8 pages, 7 pictures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of classifier models typically deliver superior performance and can\noutperform single classifier models given a dataset and classification task at\nhand. However, the gain in performance comes together with the lack in\ncomprehensibility, posing a challenge to understand how each model affects the\nclassification outputs and where the errors come from. We propose a tight\nvisual integration of the data and the model space for exploring and combining\nclassifier models. We introduce a workflow that builds upon the visual\nintegration and enables the effective exploration of classification outputs and\nmodels. We then present a use case in which we start with an ensemble\nautomatically selected by a standard ensemble selection algorithm, and show how\nwe can manipulate models and alternative combinations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 19:10:16 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Schneider", "Bruno", ""], ["J\u00e4ckle", "Dominik", ""], ["Stoffel", "Florian", ""], ["Diehl", "Alexandra", ""], ["Fuchs", "Johannes", ""], ["Keim", "Daniel", ""]]}, {"id": "1710.07562", "submitter": "Florian Brachten", "authors": "Florian Brachten, Stefan Stieglitz, Lennart Hofeditz, Katharina\n  Kloppenborg, Annette Reimann", "title": "Strategies and Influence of Social Bots in a 2017 German state election\n  - A case study on Twitter", "comments": "Accepted for publication in the Proceedings of the Australasian\n  Conference on Information Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As social media has permeated large parts of the population it simultaneously\nhas become a way to reach many people e.g. with political messages. One way to\nefficiently reach those people is the application of automated computer\nprograms that aim to simulate human behaviour - so called social bots. These\nbots are thought to be able to potentially influence users' opinion about a\ntopic. To gain insight in the use of these bots in the run-up to the German\nBundestag elections, we collected a dataset from Twitter consisting of tweets\nregarding a German state election in May 2017. The strategies and influence of\nsocial bots were analysed based on relevant features and network visualization.\n61 social bots were identified. Possibly due to the concentration on German\nlanguage as well as the elections regionality, identified bots showed no signs\nof collective political strategies and low to none influence. Implications are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 14:59:54 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Brachten", "Florian", ""], ["Stieglitz", "Stefan", ""], ["Hofeditz", "Lennart", ""], ["Kloppenborg", "Katharina", ""], ["Reimann", "Annette", ""]]}, {"id": "1710.07631", "submitter": "Alok Hota", "authors": "Alok Hota, Mohammad Raji, Tanner Hobson, Jian Huang", "title": "A Space-Efficient Method for Navigable Ensemble Analysis and\n  Visualization", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists increasingly rely on simulation runs of complex models in lieu of\ncost-prohibitive or infeasible experimentation. The data output of many\ncontrolled simulation runs, the ensemble, is used to verify correctness and\nquantify uncertainty. However, due to their size and complexity, ensembles are\ndifficult to visually analyze because the working set often exceeds strict\nmemory limitations. We present a navigable ensemble analysis tool, NEA, for\ninteractive exploration of ensembles. NEA's pre-processing component takes\nadvantage of the data similarity characteristics of ensembles to represent the\ndata in a new, spatially-efficient data structure which does not require fully\nreconstructing the original data at visualization time. This data structure\nallows a fine degree of control in working set management, which enables\ninteractive ensemble exploration while fitting within memory limitations.\nScientists can also gain new insights from the data-similarity analysis in the\npre-processing component.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 19:25:04 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Hota", "Alok", ""], ["Raji", "Mohammad", ""], ["Hobson", "Tanner", ""], ["Huang", "Jian", ""]]}, {"id": "1710.07757", "submitter": "Abhishek Verma Dr.", "authors": "Abhishek Verma and B\\'er\\'enice Mettler", "title": "Human Learning of Unknown Environments in Agile Guidance Tasks", "comments": "49 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trained human pilots or operators still stand out through their efficient,\nrobust, and versatile skills in guidance tasks such as driving agile vehicles\nin spatial environments or performing complex surgeries. This research studies\nhow humans learn a task environment for agile behavior. The hypothesis is that\nsensory-motor primitives previously described as interaction patterns and\nproposed as units of behavior for organization and planning of behavior provide\nelements of memory structure needed to efficiently learn task environments. The\npaper presents a modeling and analysis framework using the interaction patterns\nto formulate learning as a graph learning process and apply the framework to\ninvestigate and evaluate human learning and decision-making while operating in\nunknown environments. This approach emphasizes the effects of agent-environment\ndynamics (e.g., a vehicle controlled by a human operator), which is not\nemphasized in existing environment learning studies. The framework is applied\nto study human data collected from simulated first-person guidance experiments\nin an obstacle field. Subjects were asked to perform multiple trials and find\nminimum-time routes between prespecified start and goal locations without\npriori knowledge of the environment.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 05:55:23 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Verma", "Abhishek", ""], ["Mettler", "B\u00e9r\u00e9nice", ""]]}, {"id": "1710.07899", "submitter": "Amar Ranjan Dash", "authors": "Manas Ranjan Patra, Amar Ranjan Dash", "title": "Accessibility analysis of some Indian educational web portals", "comments": "15 pages, 13 figures, 2 tables", "journal-ref": "International Journal of Computer Science Engineering and\n  Applications (IJCSEA), AIRCC, Vol. 7(4), 2017, pp. 9-24, DOI:\n  10.5121/ijcsea.2017.7404", "doi": "10.5121/ijcsea.2017.7404", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web portals are being considered as excellent means for conducting teaching\nand learning activities electronically. The number of online services such as\ncourse enrollment, tutoring through online course materials, evaluation and\neven certification through web portals is increasing day by day. However, the\neffectiveness of an educational web portal depends on its accessibility to a\nwide range of students irrespective of their age, and physical abilities.\nAccessibility of web portals largely depends on their userfriendliness in terms\nof design, contents, assistive features, and online support. In this paper, we\nhave critically analyzed the web portals of thirty Indian Universities of\ndifferent categories based on the WCAG 2.0 guidelines. The purpose of this\nstudy is to point out the deficiencies that are commonly observed in web\nportals and help web designers to remove such deficiencies from the academic\nweb portals with a view to enhance their accessibility.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 07:18:02 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Patra", "Manas Ranjan", ""], ["Dash", "Amar Ranjan", ""]]}, {"id": "1710.07941", "submitter": "Zhifeng Kong", "authors": "Qi Lyu, Zhifeng Kong, Chao Shen, Tianwei Yue", "title": "WristAuthen: A Dynamic Time Wrapping Approach for User Authentication by\n  Hand-Interaction through Wrist-Worn Devices", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing trend of using wearable devices for context-aware computing and\npervasive sensing systems has raised its potentials for quick and reliable\nauthentication techniques. Since personal writing habitats differ from each\nother, it is possible to realize user authentication through writing. This is\nof great significance as sensible information is easily collected by these\ndevices. This paper presents a novel user authentication system through\nwrist-worn devices by analyzing the interaction behavior with users, which is\nboth accurate and efficient for future usage. The key feature of our approach\nlies in using much more effective Savitzky-Golay filter and Dynamic Time\nWrapping method to obtain fine-grained writing metrics for user authentication.\nThese new metrics are relatively unique from person to person and independent\nof the computing platform. Analyses are conducted on the wristband-interaction\ndata collected from 50 users with diversity in gender, age, and height.\nExtensive experimental results show that the proposed approach can identify\nusers in a timely and accurate manner, with a false-negative rate of 1.78\\%,\nfalse-positive rate of 6.7\\%, and Area Under ROC Curve of 0.983 . Additional\nexamination on robustness to various mimic attacks, tolerance to training data,\nand comparisons to further analyze the applicability.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 13:58:57 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Lyu", "Qi", ""], ["Kong", "Zhifeng", ""], ["Shen", "Chao", ""], ["Yue", "Tianwei", ""]]}, {"id": "1710.08217", "submitter": "Lukas Vermeer", "authors": "Raphael Lopez Kaufman, Jegar Pitchforth, Lukas Vermeer", "title": "Democratizing online controlled experiments at Booking.com", "comments": "Presented at the 2017 Conference on Digital Experimentation\n  (CODE@MIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an extensive literature about online controlled experiments, both on\nthe statistical methods available to analyze experiment results as well as on\nthe infrastructure built by several large scale Internet companies but also on\nthe organizational challenges of embracing online experiments to inform product\ndevelopment. At Booking.com we have been conducting evidenced based product\ndevelopment using online experiments for more than ten years. Our methods and\ninfrastructure were designed from their inception to reflect Booking.com\nculture, that is, with democratization and decentralization of experimentation\nand decision making in mind.\n  In this paper we explain how building a central repository of successes and\nfailures to allow for knowledge sharing, having a generic and extensible code\nlibrary which enforces a loose coupling between experimentation and business\nlogic, monitoring closely and transparently the quality and the reliability of\nthe data gathering pipelines to build trust in the experimentation\ninfrastructure, and putting in place safeguards to enable anyone to have end to\nend ownership of their experiments have allowed such a large organization as\nBooking.com to truly and successfully democratize experimentation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 11:50:11 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Kaufman", "Raphael Lopez", ""], ["Pitchforth", "Jegar", ""], ["Vermeer", "Lukas", ""]]}, {"id": "1710.08390", "submitter": "Dirk Lewandowski", "authors": "Sebastian Suenkler, Dirk Lewandowski", "title": "Does it matter which search engine is used? A user study using post-task\n  relevance judgments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this research was to find out how the two search engines\nGoogle and Bing perform when users work freely on pre-defined tasks, and judge\nthe relevance of the results immediately after finishing their search session.\nIn a user study, 64 participants conducted two search tasks each, and then\njudged the results on the following: (1) The quality of the results they\nselected in their search sessions, (2) The quality of the results they were\npresented with in their search sessions (but which they did not click on), (3)\nThe quality of the results from the competing search engine for their queries\n(which they did not see in their search session). We found that users heavily\nrelied on Google, that Google produced more relevant results than Bing, that\nusers were well able to select relevant results from the results lists, and\nthat users judged the relevance of results lower when they regarded a task as\ndifficult and did not find the correct information.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 17:03:10 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Suenkler", "Sebastian", ""], ["Lewandowski", "Dirk", ""]]}, {"id": "1710.08788", "submitter": "Amar Ranjan Dash", "authors": "Manas Ranjan Patra, Amar Ranjan Dash, Prasanna Kumar Mishra", "title": "A Quantitative Analysis of WCAG 2.0 Compliance For Some Indian Web\n  Portals", "comments": "15 Pages, 15 figure, 2 Table", "journal-ref": "International Journal of Computer Science, Engineering and\n  Applications (IJCSEA), AIRCC, Vol. 4(1), pp. 9-24, DOI:\n  10.5121/ijcsea.2014.4102", "doi": "10.5121/ijcsea.2014.4102", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web portals have served as an excellent medium to facilitate user centric\nservices for organizations irrespective of the type, size, and domain of\noperation. The objective of these portals has been to deliver a plethora of\nservices such as information dissemination, transactional services, and\ncustomer feedback. Therefore, the design of a web portal is crucial in order\nthat it is accessible to a wide range of user community irrespective of age\ngroup, physical abilities, and level of literacy. In this paper, we have\nstudied the compliance of WCAG 2.0 by three different categories of Indian web\nsites which are most frequently accessed by a large section of user community.\nWe have provided a quantitative evaluation of different aspects of\naccessibility which we believe can pave the way for better design of web sites\nby taking care of the deficiencies inherent in the web portals.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 14:11:04 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Patra", "Manas Ranjan", ""], ["Dash", "Amar Ranjan", ""], ["Mishra", "Prasanna Kumar", ""]]}, {"id": "1710.08867", "submitter": "Mao Mao", "authors": "Mao Mao, Alan F. Blackwell, David A. Good", "title": "Retirement Transition in the Digital Ecology: Reflecting on Identity\n  Reconstruction and Technology Appropriation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a qualitative study of retirees' social and personal\npractices via digital music technologies in the context of community music. We\nconducted a diary study, and interviewed retired community musicians who are\nexperiencing transition to retirement. Amongst challenges due to ageing and\nretirement, retirees participating in community music often experience\ndiscontinuity of identity caused by the lack of social and personal support\nafter retirement, and also report lack of interest in using new technologies.\nLife transition theory was used to understand retirees' perception and\nstrategies of identity navigation, informing the design of community-oriented\nonline music services. We deepened our understanding of retirement transitions\nwith technologies by showing how retirees participating in community music make\nsense of new rules and norms after retirement. A key finding is that retirees\nreconstruct identities by connecting with music communities, through which they\ncan develop an understanding of unfamiliar patterns of the retired life, and\ngain support musically and socially. Technologies act as 'boundary objects' for\ncommunication between digital and physical artefacts, personal and social\nrelationships. We highlight the importance of managing artefactual and\ninterpersonal boundaries when designing online services for individual and\ncommunities in transitions.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 11:12:30 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Mao", "Mao", ""], ["Blackwell", "Alan F.", ""], ["Good", "David A.", ""]]}, {"id": "1710.09139", "submitter": "Martin V\\\"olker", "authors": "Martin V\\\"olker, Robin T. Schirrmeister, Lukas D. J. Fiederer, Wolfram\n  Burgard, Tonio Ball", "title": "Deep Transfer Learning for Error Decoding from Non-Invasive EEG", "comments": "6 pages, 9 figures, The 6th International Winter Conference on\n  Brain-Computer Interface 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recorded high-density EEG in a flanker task experiment (31 subjects) and\nan online BCI control paradigm (4 subjects). On these datasets, we evaluated\nthe use of transfer learning for error decoding with deep convolutional neural\nnetworks (deep ConvNets). In comparison with a regularized linear discriminant\nanalysis (rLDA) classifier, ConvNets were significantly better in both intra-\nand inter-subject decoding, achieving an average accuracy of 84.1 % within\nsubject and 81.7 % on unknown subjects (flanker task). Neither method was,\nhowever, able to generalize reliably between paradigms. Visualization of\nfeatures the ConvNets learned from the data showed plausible patterns of brain\nactivity, revealing both similarities and differences between the different\nkinds of errors. Our findings indicate that deep learning techniques are useful\nto infer information about the correctness of action in BCI applications,\nparticularly for the transfer of pre-trained classifiers to new recording\nsessions or subjects.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 09:47:04 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 16:46:48 GMT"}, {"version": "v3", "created": "Wed, 10 Jan 2018 09:13:24 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["V\u00f6lker", "Martin", ""], ["Schirrmeister", "Robin T.", ""], ["Fiederer", "Lukas D. J.", ""], ["Burgard", "Wolfram", ""], ["Ball", "Tonio", ""]]}, {"id": "1710.09789", "submitter": "Hazel Murray", "authors": "Hazel Murray and David Malone", "title": "Evaluating Password Advice", "comments": "6 pages, 4 tables, 2 figures, conference paper", "journal-ref": "Murray, H. and Malone, D., 2017, June. Evaluating password advice.\n  In Signals and Systems Conference (ISSC), 2017 28th Irish (pp. 1-6). IEEE", "doi": "10.1109/ISSC.2017.7983609", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Password advice is constantly circulated by standards agencies, companies,\nwebsites and specialists. But there appears to be great diversity in terms of\nthe advice that is given. Users have noticed that different websites are\nenforcing different restrictions. For example, requiring different combinations\nof uppercase and lowercase letters, numbers and special characters. We\ncollected password advice and found that the advice distributed by one\norganization can directly contradict advice given by another. Our paper aims to\nilluminate interesting characteristics for a sample of the password advice\ndistributed. We also create a framework for identifying the costs associated\nwith implementing password advice. In doing so we identify a reason for why\npassword advice is often both derided and ignored.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 16:19:43 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Murray", "Hazel", ""], ["Malone", "David", ""]]}, {"id": "1710.09901", "submitter": "Qunwei Li", "authors": "Qunwei Li and Pramod K. Varshney", "title": "Optimal Crowdsourced Classification with a Reject Option in the Presence\n  of Spammers", "comments": "submitted to ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the design of an effective crowdsourcing system for an $M$-ary\nclassification task. Crowd workers complete simple binary microtasks whose\nresults are aggregated to give the final decision. We consider the scenario\nwhere the workers have a reject option so that they are allowed to skip\nmicrotasks when they are unable to or choose not to respond to binary\nmicrotasks. We present an aggregation approach using a weighted majority voting\nrule, where each worker's response is assigned an optimized weight to maximize\ncrowd's classification performance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 20:15:46 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Li", "Qunwei", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1710.11429", "submitter": "Efthimios Bothos", "authors": "Efthimios Bothos, Babis Magoutas, Brian Caulfield, Athena Tsirimpa,\n  Maria Kamargianni, Panagiotis Georgakis and Gregoris Mentzas", "title": "Behavioural Change Support Intelligent Transportation Applications", "comments": "Intelligent Transportation Systems Conference (ITSC) 2017 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This workshop invites researchers and practitioners to participate in\nexploring behavioral change support intelligent transportation applications. We\nwelcome submissions that explore intelligent transportation systems (ITS),\nwhich interact with travelers in order to persuade them or nudge them towards\nsustainable transportation behaviors and decisions. Emerging opportunities\nincluding the use of data and information generated by ITS and users' mobile\ndevices in order to render personalized, contextualized and timely transport\nbehavioral change interventions are in our focus. We invite submissions and\nideas from domains of ITS including, but not limited to, multi-modal journey\nplanners, advanced traveler information systems and in-vehicle systems. The\nexpected outcome will be a deeper understanding of the challenges and future\nresearch directions with respect to behavioral change support through ITS.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 12:23:05 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Bothos", "Efthimios", ""], ["Magoutas", "Babis", ""], ["Caulfield", "Brian", ""], ["Tsirimpa", "Athena", ""], ["Kamargianni", "Maria", ""], ["Georgakis", "Panagiotis", ""], ["Mentzas", "Gregoris", ""]]}, {"id": "1710.11597", "submitter": "Anton Muehlemann", "authors": "Anton Muehlemann", "title": "Sentiment Protocol: A Decentralized Protocol Leveraging Crowd Sourced\n  Wisdom", "comments": "13 pages, 3 figures", "journal-ref": "Ledger Journal, 2018", "doi": "10.5195/ledger.2018.113", "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wisdom of the crowd is a valuable asset in today's society. It is not\nonly important in predicting elections but also plays an essential role in\nmarketing and the financial industry. Having a trustworthy source of opinion\ncan make forecasts more accurate and markets predictable. Until now, a\nfundamental problem of surveys is the lack of incentives for participants to\nprovide accurate information. Classical solutions like small monetary rewards\nor the chance of winning a prize are often not very attractive for\nparticipants. More attractive solutions, such as prediction markets, face the\nissue of illegality and are often unavailable. In this work, we present a\nsolution that unites the advantages from classical polling and prediction\nmarkets via a customizable incentivization framework. Apart from predicting\nevents, this framework can also be used to govern decentralized autonomous\norganizations.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 17:17:25 GMT"}, {"version": "v2", "created": "Sat, 5 May 2018 17:55:25 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Muehlemann", "Anton", ""]]}]