[{"id": "1404.0934", "submitter": "Jiyi Li", "authors": "Jiyi Li", "title": "Map Route Ranking with Weighted Distance using Environmental Factors", "comments": "2 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When users search for the routes between two places using map based services,\nthese services compute and provide the top candidate routes based on shortest\ngeometric distances or ideal time consuming. However, other real factors like\nphysical exertion and practical time consuming will influence user experience,\nand the environmental factors like steep slope and traffic jam that result in\nthese real factors need to be considered. For example, when users travel on\nfoot or by bicycle, if there are many steep slopes on the routes, it will be\ndifficult or easy to be tired. In this paper, we propose an approach computing\nweighted distance considering these environmental factors. We rank the\ncandidate route results generated by Google Map using elevation information. We\nintegrate the elevation information in the route results to assist users to\nmake decision. The solution can also be used in other scenarios that need to\nconsider environmental factors.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 12:28:04 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Li", "Jiyi", ""]]}, {"id": "1404.1320", "submitter": "Robert LiKamWa", "authors": "Robert LiKamWa, Zhen Wang, Aaron Carroll, Felix Xiaozhu Lin, Lin Zhong", "title": "Draining our Glass: An Energy and Heat Characterization of Google Glass", "comments": null, "journal-ref": null, "doi": null, "report-no": "Rice University ECE Technical Report 2014-03-23", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Google Glass is a mobile device designed to be worn as eyeglasses. This\nform factor enables new usage possibilities, such as hands-free video chats and\ninstant web search. However, its shape also hampers its potential: (1) battery\nsize, and therefore lifetime, is limited by a need for the device to be\nlightweight, and (2) high-power processing leads to significant heat, which\nshould be limited, due to the Glass' compact form factor and close proximity to\nthe user's skin. We use the Glass in a case study of the power and thermal\ncharacteristics of optical head-mounted display devices. We share insights and\nimplications to limit power consumption to increase the safety and utility of\nhead-mounted devices.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 05:09:26 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["LiKamWa", "Robert", ""], ["Wang", "Zhen", ""], ["Carroll", "Aaron", ""], ["Lin", "Felix Xiaozhu", ""], ["Zhong", "Lin", ""]]}, {"id": "1404.1664", "submitter": "Basant Agarwal", "authors": "Namita Mittal, Basant Agarwal, Ajay Gupta, Hemant Madhur", "title": "Icon Based Information Retrieval and Disease Identification in\n  Agriculture", "comments": "Iconic Interface, Image Processing, Pattern Recognition, Data Mining,\n  Information Retrieval", "journal-ref": "International Journal of Advanced Studies in Computer Science &\n  Engineering IJASCSE, Volume 3, Issue 3, 2014", "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in the ICT industry in past few decades has enabled the\nquick and easy access to the information available on the internet. But,\ndigital literacy is the pre-requisite for its use. The main purpose of this\npaper is to provide an interface for digitally illiterate users, especially\nfarmers to efficiently and effectively retrieve information through Internet.\nIn addition, to enable the farmers to identify the disease in their crop, its\ncause and symptoms using digital image processing and pattern recognition\ninstantly without waiting for an expert to visit the farms and identify the\ndisease.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 06:20:53 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Mittal", "Namita", ""], ["Agarwal", "Basant", ""], ["Gupta", "Ajay", ""], ["Madhur", "Hemant", ""]]}, {"id": "1404.1669", "submitter": "Shafi'i Muhammad Abdulhamid Mr", "authors": "Olawale S. Adebayo, Shafii M. Abdulhamid and Andrew Fluck", "title": "The Prospects for e-Examinations in Nigeria and Australia", "comments": "13 pages, 3 figures, 1 table", "journal-ref": "International Journal of Advances in Management, Technology and\n  Engineering Sciences, 2013", "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares the e-Examination system in Nigeria with that of\nAustralia. We consider the experiences of working with commercial firms such as\nElectronic Testing Company (eTC) and using open-source software. It is\nimportant to foster good relationships with accreditation authorities (such as\nUniversity Authorities, West African Examination Council (WAEC), Joint\nAdmissions and Matriculation Board (JAMB) etc. and the Tasmanian Qualifications\nAuthority) to assist in the transition from paperbased assessment to post-paper\nassessment. The paper also considers the relative convenience for students,\nadministrators and lecturer/assessors; and to gauges the reliability and\nsecurity of the two systems in use. It examines the challenges in conducting\ne-Examinations in both countries by juxtaposing the systems in the two\ncountries and suggests ways of developing more acceptable e-Examination\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 07:15:40 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Adebayo", "Olawale S.", ""], ["Abdulhamid", "Shafii M.", ""], ["Fluck", "Andrew", ""]]}, {"id": "1404.1799", "submitter": "Harris Kyriakou", "authors": "Harris Kyriakou, Jeffrey V. Nickerson", "title": "Collective Innovation in Open Source Hardware", "comments": "To appear in the proceedings of Collective Intelligence 2014 (Boston,\n  June 10-12 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing community that shares digital 3D designs has created an opportunity\nto study, encourage and stimulate innovation. This remix community allows\npeople not only to prototype at a minimal cost but also to work on projects\nthey are genuinely interested in. Participants free of the limitations\ntypically imposed by formal organizations develop products driven by their own\ninterest.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 14:27:16 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Kyriakou", "Harris", ""], ["Nickerson", "Jeffrey V.", ""]]}, {"id": "1404.1911", "submitter": "Bahador Saket", "authors": "Bahador Saket, Paolo Simonetto, Stephen Kobourov and Katy Borner", "title": "Node, Node-Link, and Node-Link-Group Diagrams: An Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effectively showing the relationships between objects in a dataset is one of\nthe main tasks in information visualization. Typically there is a well-defined\nnotion of distance between pairs of objects, and traditional approaches such as\nprincipal component analysis or multi-dimensional scaling are used to place the\nobjects as points in 2D space, so that similar objects are close to each other.\nIn another typical setting, the dataset is visualized as a network graph, where\nrelated nodes are connected by links. More recently, datasets are also\nvisualized as maps, where in addition to nodes and links, there is an explicit\nrepresentation of groups and clusters. We consider these three Techniques,\ncharacterized by a progressive increase of the amount of encoded information:\nnode diagrams, node-link diagrams and node-link-group diagrams. We assess these\nthree types of diagrams with a controlled experiment that covers nine different\ntasks falling broadly in three categories: node-based tasks, network-based\ntasks and group-based tasks. Our findings indicate that adding links, or links\nand group representations, does not negatively impact performance (time and\naccuracy) of node-based tasks. Similarly, adding group representations does not\nnegatively impact the performance of network-based tasks. Node-link-group\ndiagrams outperform the others on group-based tasks. These conclusions\ncontradict results in other studies, in similar but subtly different settings.\nTaken together, however, such results can have significant implications for the\ndesign of standard and domain specific visualizations tools.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 20:01:40 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Saket", "Bahador", ""], ["Simonetto", "Paolo", ""], ["Kobourov", "Stephen", ""], ["Borner", "Katy", ""]]}, {"id": "1404.2163", "submitter": "Timur Mirzoev", "authors": "Dr. Timur Mirzoev, Lawton Sack", "title": "Webpage Load Speed: ASP.NET vs. PHP", "comments": null, "journal-ref": "i-managers Journal on Information Technology, Vol. 2, No. 2, March\n  May 2013", "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data transmission speeds over the Internet continue to increase, it is\nnecessary to research and identify technologies that can take advantage of the\nincreased speeds by enhancing the loading speed of webpages. One area of\nconsideration is found in the type of framework that is utilized for a website.\nThere are numerous frameworks that can be chosen from to be used to support a\nwebsite, each with their distinctive advantages. There are many different\nopinions that have been tested on which framework should be used to fully\nrealize the optimization of page load speed. This manuscript examines and\nimplements testing methods for two popular frameworks, Active Server Pages .net\nand PHP, to make a final determination of which framework is most beneficial\nfor webpage load speeds.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 14:51:22 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Mirzoev", "Dr. Timur", ""], ["Sack", "Lawton", ""]]}, {"id": "1404.2364", "submitter": "Rashmi Jain", "authors": "Dr Manju Kaushik and Rashmi Jain", "title": "Gesture Based Interaction NUI: An Overview", "comments": "4 pages.\"Published with International Journal of Engineering Trends\n  and Technology (IJETT)\"", "journal-ref": "International Journal of Engineering Trends and Technology (IJETT)\n  9(12), March 2014. Published by Seventh Sense Research Group", "doi": "10.14445/22315381/IJETT-V9P319", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Touch,face,voice recognition and movement sensors all are part of an emerging\nfield of computing often called natural user interface, or NUI. Interacting\nwith technology in these humanistic ways is no longer limited to high tech\nsecret agents. Gesture Touch, face, voice recognition and movement sensors all\nare part of an emerging field of computing often called natural user interface,\nor NUI. Interacting with technology in these humanistic ways is no longer\nlimited to high tech secret agents. Gesture recognition is the process by which\ngestures formed by a user are made known to the system. In completely immersive\nVR environments, the keyboard is generally not included, Technology\nincorporates face, voice, gesture, and object recognition to give users a\nvariety of ways to interact with the console, all without needing a controller.\nThis paper focuses on the emerging way of human computer interaction, Gesture\nrecognition concept and gesture types.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 04:43:27 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Kaushik", "Dr Manju", ""], ["Jain", "Rashmi", ""]]}, {"id": "1404.2742", "submitter": "Sumit Kumar Dey", "authors": "Sumit Kumar Dey and Shubham Anand", "title": "Algorithm For Multi-Hand Finger Counting: An Easy Approach", "comments": "8 pages, 4 sets of figures", "journal-ref": "Advances in Vision Computing: An International Journal(AVC) Vol.1,\n  No.1, March 2014", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an easy algorithm for real time hand finger counting\ninvolving one or more than one hand. Hand finger counting is a simple medium\nfor Human-Computer Interface which can prove to be a convenient input method\nfor driving interactive menus, small applications and interactive games. Here,\nwe also calculate the orientation of the hand which can be used to provide\ninputs for the direction and/or speed control of a robot, controlling mouse\ncursor or slide-show presentations. If being used by a single person, with his\nor her two hands, the person can trigger ten different commands with fingers in\naddition to the orientation of the hand. We tried to use very simple algorithm\nusing very basic mathematics of 2D coordinate geometry and avoided the\nconventional use of calculus, contours and convex hull. Anyone seeking for an\neasy to implement hand finger counting algorithm can refer to it.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 09:09:11 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Dey", "Sumit Kumar", ""], ["Anand", "Shubham", ""]]}, {"id": "1404.3920", "submitter": "Aske Plaat", "authors": "Catholijn Jonker, Joost Broekens, Aske Plaat", "title": "Virtual Reflexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Virtual Reality is used successfully to treat people for regular phobias. A\nnew challenge is to develop Virtual Reality Exposure Training for social\nskills. Virtual actors in such systems have to show appropriate social behavior\nincluding emotions, gaze, and keeping distance. The behavior must be realistic\nand real-time. Current approaches consist of four steps: 1) trainee social\nsignal detection, 2) cognitive-affective interpretation, 3) determination of\nthe appropriate bodily responses, and 4) actuation. The \"cognitive\" detour of\nsuch approaches does not match the directness of human bodily reflexes and\ncauses unrealistic responses and delay. Instead, we propose virtual reflexes as\nconcurrent sensory-motor processes to control virtual actors. Here we present a\nvirtual reflexes architecture, explain how emotion and cognitive modulation are\nembedded, detail its workings, and give an example description of an aggression\ntraining application.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 14:07:09 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Jonker", "Catholijn", ""], ["Broekens", "Joost", ""], ["Plaat", "Aske", ""]]}, {"id": "1404.3958", "submitter": "Tomasz Rutkowski", "authors": "Chisaki Nakaizumi, Toshie Matsui, Koichi Mori, Shoji Makino, and\n  Tomasz M. Rutkowski", "title": "Head-related Impulse Response-based Spatial Auditory Brain-computer\n  Interface", "comments": "5 pages, 1 figure, submitted to 6th International Brain-Computer\n  Interface Conference 2014, Graz, Austria", "journal-ref": null, "doi": "10.3217/978-3-85125-378-8-20", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This study provides a comprehensive test of the head-related impulse response\n(HRIR) to an auditory spatial speller brain-computer interface (BCI) paradigm,\nincluding a comparison with a conventional virtual headphone-based spatial\nauditory modality. Five BCI-naive users participated in an experiment based on\nfive Japanese vowels. The auditory evoked potentials obtained produced\nencouragingly good and stable P300-responses in online BCI experiments. Our\ncase study indicates that the auditory HRIR spatial sound paradigm reproduced\nwith headphones could be a viable alternative to established multi-loudspeaker\nsurround sound BCI-speller applications.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 15:40:14 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Nakaizumi", "Chisaki", ""], ["Matsui", "Toshie", ""], ["Mori", "Koichi", ""], ["Makino", "Shoji", ""], ["Rutkowski", "Tomasz M.", ""]]}, {"id": "1404.4184", "submitter": "Tomasz Rutkowski", "authors": "Katsuhiko Hamada, Hiromu Mori, Hiroyuki Shinoda, and Tomasz M.\n  Rutkowski", "title": "Airborne Ultrasonic Tactile Display Brain-computer Interface Paradigm", "comments": "5 pages, 3 figures, submitted to 6th International Brain-Computer\n  Interface Conference 2014, Graz, Austria", "journal-ref": null, "doi": "10.3217/978-3-85125-378-8-18", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We study the extent to which contact-less and airborne ultrasonic tactile\ndisplay (AUTD) stimuli delivered to the palms of a user can serve as a platform\nfor a brain computer interface (BCI) paradigm. Six palm positions are used to\nevoke combined somatosensory brain responses, in order to define a novel\ncontact-less tactile BCI. A comparison is made with classical attached\nvibrotactile transducers. Experiment results of subjects performing online\nexperiments validate the novel BCI paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 10:10:25 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Hamada", "Katsuhiko", ""], ["Mori", "Hiromu", ""], ["Shinoda", "Hiroyuki", ""], ["Rutkowski", "Tomasz M.", ""]]}, {"id": "1404.4226", "submitter": "Tomasz Rutkowski", "authors": "Takumi Kodama, Shoji Makino, and Tomasz M. Rutkowski", "title": "Spatial Tactile Brain-Computer Interface Paradigm Applying Vibration\n  Stimuli to Large Areas of User's Back", "comments": "5 pages, 4 figures, submitted to 6th International Brain-Computer\n  Interface Conference 2014, Graz, Austria", "journal-ref": null, "doi": "10.3217/978-3-85125-378-8-32", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We aim at an augmentation of communication abilities of amyotrophic lateral\nsclerosis (ALS) patients by creating a brain-computer interface (BCI) which can\ncontrol a computer or other device by using only brain activity. As a method,\nwe use a stimulus-driven BCI based on vibration stimuli delivered via a gaming\npad to the user's back. We identify P300 responses from brain activity data in\nresponse to the vibration stimuli. The user's intentions are classified\naccording to the P300 responses recorded in the EEG. From the results of the\npsychophysical and online BCI experiments, we are able to classify the P300\nresponses very accurately, which proves the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 12:51:08 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Kodama", "Takumi", ""], ["Makino", "Shoji", ""], ["Rutkowski", "Tomasz M.", ""]]}, {"id": "1404.4607", "submitter": "Olivier Aubert", "authors": "Olivier Aubert, Yannick Pri\\'e, Camila Canellas", "title": "Leveraging video annotations in video-based e-learning", "comments": "7th International Conference on Computer Supported Education (CSEDU),\n  Barcelone : Spain (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The e-learning community has been producing and using video content for a\nlong time, and in the last years, the advent of MOOCs greatly relied on video\nrecordings of teacher courses. Video annotations are information pieces that\ncan be anchored in the temporality of the video so as to sustain various\nprocesses ranging from active reading to rich media editing. In this position\npaper we study how video annotations can be used in an e-learning context -\nespecially MOOCs - from the triple point of view of pedagogical processes,\ncurrent technical platforms functionalities, and current challenges. Our\nanalysis is that there is still plenty of room for leveraging video annotations\nin MOOCs beyond simple active reading, namely live annotation, performance\nannotation and annotation for assignment; and that new developments are needed\nto accompany this evolution.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 15:13:21 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Aubert", "Olivier", ""], ["Pri\u00e9", "Yannick", ""], ["Canellas", "Camila", ""]]}, {"id": "1404.5248", "submitter": "Meddeb Mohamed", "authors": "M. Meddeb, H. Karray and Adel M. Alimi", "title": "Intelligent Remote Control for TV Program based on Emotion in Arabic\n  Speech", "comments": "6 pages, 3 figures", "journal-ref": "International Journal of Scientific Research & Engineering\n  Technology (IJSET), ISSN: (2277-1581) volume 1, 2014", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems for TV program have been studied for the realization of\npersonalized TV Electronic Program Guides. In this paper, we propose automatic\nemotion Arabic speech recognition in order to achieve an intelligent remote\ncontrol. In addition, the TV can estimate our interests and preferences by\nobserving our behavior to watch and have a conversation on topics that might be\ninteresting to us.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 17:25:15 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Meddeb", "M.", ""], ["Karray", "H.", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1404.5708", "submitter": "Qi Xuan", "authors": "Qi Xuan, Premkumar T Devanbu, Vladimir Filkov", "title": "Converging Work-Talk Patterns in Online Task-Oriented Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of what we do is accomplished by working collaboratively with others,\nand a large portion of our lives are spent working and talking; the patterns\nembodied in the alternation of working and talking can provide much useful\ninsight into task-oriented social behaviors. The available electronic traces of\nthe different kinds of human activities in online communities are an empirical\ngoldmine that can enable the holistic study and understanding of these social\nsystems. Open Source Software projects are prototypical examples of\ncollaborative, task-oriented communities, depending on volunteers for\nhigh-quality work. Here, we use sequence analysis methods to identify the\nwork-talk patterns of software developers in these online communities.\n  We find that software developers prefer to persist in same kinds of\nactivities, i.e., a string of work activities followed by a string of talk\nactivities and so forth, rather than switch them frequently; this tendency\nstrengthens with time, suggesting that developers become more efficient, and\ncan work longer with fewer interruptions. This process is accompanied by the\nformation of community culture: developers' patterns in the same communities\nget closer with time while different communities get relatively more different.\nThe emergence of community culture is apparently driven by both \"talk\" and\n\"work\". Finally, we also find that workers with good balance between \"work\" and\n\"talk\" tend to produce just as much work as those that focus strongly on\n\"work\"; however, the former appear to be more likely to continue to be active\ncontributors in the communities.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 05:29:57 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Xuan", "Qi", ""], ["Devanbu", "Premkumar T", ""], ["Filkov", "Vladimir", ""]]}, {"id": "1404.6222", "submitter": "Jeremy Frey", "authors": "J\\'er\\'emy Frey (INRIA Bordeaux - Sud-Ouest, LaBRI), L\\'eonard\n  Pommereau (INRIA Bordeaux - Sud-Ouest), Fabien Lotte (INRIA Bordeaux -\n  Sud-Ouest, LaBRI), Martin Hachet (INRIA Bordeaux - Sud-Ouest, LaBRI)", "title": "Assessing the Zone of Comfort in Stereoscopic Displays using EEG", "comments": null, "journal-ref": "ACM SIGCHI Conference on Human Factors in Computing Systems (2014)", "doi": "10.1145/2559206.2581191", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conflict between vergence (eye movement) and accommodation (crystalline\nlens deformation) occurs in every stereoscopic display. It could cause\nimportant stress outside the \"zone of comfort\", when stereoscopic effect is too\nstrong. This conflict has already been studied using questionnaires, during\nviewing sessions of several minutes. The present pilot study describes an\nexperimental protocol which compares two different comfort conditions using\nelectroencephalography (EEG) over short viewing sequences. Analyses showed\nsignificant differences both in event-related potentials (ERP) and in frequency\nbands power. An uncomfortable stereoscopy correlates with a weaker negative\ncomponent and a delayed positive component in ERP. It also induces a power\ndecrease in the alpha band and increases in theta and beta bands. With fast\nresponses to stimuli, EEG is likely to enable the conception of adaptive\nsystems, which could tune the stereoscopic experience according to each viewer.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 18:43:14 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Frey", "J\u00e9r\u00e9my", "", "INRIA Bordeaux - Sud-Ouest, LaBRI"], ["Pommereau", "L\u00e9onard", "", "INRIA Bordeaux - Sud-Ouest"], ["Lotte", "Fabien", "", "INRIA Bordeaux -\n  Sud-Ouest, LaBRI"], ["Hachet", "Martin", "", "INRIA Bordeaux - Sud-Ouest, LaBRI"]]}, {"id": "1404.6384", "submitter": "Pierre de Buyl", "authors": "Jinook Oh", "title": "CATOS: Computer Aided Training/Observing System", "comments": "Part of the Proceedings of the 6th European Conference on Python in\n  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2013-03", "categories": "cs.CE cs.HC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In animal behavioral biology, there are several cases in which an autonomous\nobserving/training system would be useful. 1) Observation of certain species\ncontinuously, or for documenting specific events, which happen irregularly; 2)\nLongterm intensive training of animals in preparation for behavioral\nexperiments; and 3) Training and testing of animals without human interference,\nto eliminate potential cues and biases induced by humans. The primary goal of\nthis study is to build a system named CATOS (Computer Aided Training/Observing\nSystem) that could be used in the above situations. As a proof of concept, the\nsystem was built and tested in a pilot experiment, in which cats were trained\nto press three buttons differently in response to three different sounds (human\nspeech) to receive food rewards. The system was built in use for about 6\nmonths, successfully training two cats. One cat learned to press a particular\nbutton, out of three buttons, to obtain the food reward with over 70 percent\ncorrectness.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:53:33 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Oh", "Jinook", ""]]}, {"id": "1404.6602", "submitter": "EPTCS", "authors": "K. Rustan M. Leino (Microsoft Research, Redmond, WA, USA), Valentin\n  W\\\"ustholz (ETH Zurich, Department of Computer Science, Switzerland)", "title": "The Dafny Integrated Development Environment", "comments": "In Proceedings F-IDE 2014, arXiv:1404.5785", "journal-ref": "EPTCS 149, 2014, pp. 3-15", "doi": "10.4204/EPTCS.149.2", "report-no": null, "categories": "cs.PL cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, program verifiers and interactive theorem provers have\nbecome more powerful and more suitable for verifying large programs or proofs.\nThis has demonstrated the need for improving the user experience of these tools\nto increase productivity and to make them more accessible to non-experts. This\npaper presents an integrated development environment for Dafny-a programming\nlanguage, verifier, and proof assistant-that addresses issues present in most\nstate-of-the-art verifiers: low responsiveness and lack of support for\nunderstanding non-obvious verification failures. The paper demonstrates several\nnew features that move the state-of-the-art closer towards a verification\nenvironment that can provide verification feedback as the user types and can\npresent more helpful information about the program or failed verifications in a\ndemand-driven and unobtrusive way.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 05:32:07 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Leino", "K. Rustan M.", "", "Microsoft Research, Redmond, WA, USA"], ["W\u00fcstholz", "Valentin", "", "ETH Zurich, Department of Computer Science, Switzerland"]]}, {"id": "1404.6745", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on Adaptable Menu: A TRIZ based analysis", "comments": "10 pages, 6 figures", "journal-ref": "Mishra, Umakant, Inventions on Adaptable Menu: A TRIZ Based\n  Analysis. December 6, 2006, Available at SSRN:\n  http://ssrn.com/abstract=949236", "doi": "10.2139/ssrn.949236", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The menu is one of the most widely used elements of a graphical user\ninterface. The objective of a menu system is to provide various commands and\nfunctions to the user in an easy way so that the user can just select the\ndesired operation from a given list instead of typing a complex command in the\ncommand prompt.\n  In a conventional menu system the menu items or options are hard-coded in the\ncomputer program. The programmer or developer composes menu items at the time\nof development. The developer tries to include all options that he feels may be\nrequired by the user in future. Although the items are decided from\n\"requirement analysis\" and other studies, it is difficult to know the exact\nneed of a user at a future period of time. This leads to inclusion of a lot of\nitems in the menu, which leads to user confusion and frustration.\n  Thus there is a need for adaptable menu that can be changed according to user\nrequirement. The items of the adaptable menu should change from user to user\nand from time to time depending on the program context and likelihood of user\nselection.\n  This article defines the Ideal Final Result of a dropdown menu system,\ndefines the desirable functions of an adaptable menu, finds and solves the\ncontradictions faced in achieving the desirable functions, and illustrates six\nselected cases on adaptable menu from US patent database.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 13:36:52 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6747", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "10 inventions on Improving Toolbars: A TRIZ based analysis", "comments": "15 pages", "journal-ref": "Mishra, Umakant, 10 Inventions on Improving Toolbars: A TRIZ Based\n  Analysis (September 7, 2007). Available at SSRN:\n  http://ssrn.com/abstract=1264683", "doi": "10.2139/ssrn.1264683", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toolbar is one among the popular elements of a graphical user interface. The\nother popular elements of graphical user interface are buttons, menu,\nscrollbar, dialog box etc., all of which provide easy access to various\nfunctions of a GUI System.\n  A toolbar often does a similar function as the menu but with certain\ndifferences. A menu has the advantage of holding a large number of items\nwithout needing any additional screen space. In contrast, each button on the\ntoolbar permanently occupies some space on the screen. It is not possible to\nimplement large number of functions through a toolbar, as they will occupy more\nand more valuable screen space. However, the toolbar has an advantage as it\ngives a single click access to any function unlike a menu system where the user\nhas to navigate through sub-menus to ultimate discover the item he is looking\nfor.\n  This article explores the desired features of a toolbar and the ideal\nfeatures of an advanced toolbar. The contradictions are described from a TRIZ\nperspective and solutions are derived using Inventive principles. Besides the\narticle illustrates 10 inventions on improving Toolbars selected from US patent\ndatabase.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 13:44:37 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6750", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "10 Inventions on Command Buttons in a Graphical User Interface", "comments": "12 pages", "journal-ref": "Mishra, Umakant, 10 Inventions on Command Buttons in a Graphical\n  User Interface, (December 6, 2006) Available at SSRN:\n  http://ssrn.com/abstract=949240", "doi": "10.2139/ssrn.949240", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A command button may contain a textual label or a graphic image or both. It\nmay be static or animated. There can be many different features to make a\ncommand button attractive and effective. As command button is a typical GUI\nelement, most improvement on GUI in general will also be applicable to command\nbuttons. Besides, there are also inventions to improve various aspects of\ncommand buttons in specific. This article illustrates 10 selected inventions\nfrom US patent database. Each invention is followed by a TRIZ based analysis in\nbrief.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 13:57:05 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6752", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "10 Inventions on scrolling and scrollbars in Graphical User Interface", "comments": "13 pages", "journal-ref": "Mishra, Umakant, 10 Inventions on Scrolling and Scrollbars in\n  Graphical User Interface, (December 6, 2006), Available at SSRN:\n  http://ssrn.com/abstract=949243", "doi": "10.2139/ssrn.949243", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scrolling mechanism is used to display and edit large documents in a limited\nscreen space or window. The scrolling mechanism may include a vertical scroll\nbar or a horizontal scroll bar or both to move the contents of the documents up\nand down or left and right. There may be navigation buttons on the screen\nrepresenting the navigation keys on the keyboard. The user can click these\nbuttons to scroll the screen.\n  There may be very different methods of scrolling such as by using the\n\"thumb\", as popularly used with a PDF document. Scrolling may be achieved\nthrough eyeball tracking in a hands-free environment where the user does not\nhave hands or wants to use hands for other activities. This article illustrates\nten interesting inventions on scrolling selected from US patent database.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 14:02:40 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6754", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on dialog boxes used in GUI", "comments": "Available at SSRN: http://ssrn.com/abstract=949247. Mishra, Umakant,\n  Inventions on Dialog Boxes Used in GUI, (December 6, 2006)", "journal-ref": null, "doi": "10.2139/ssrn.949247", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dialog boxes are useful in case of displaying warnings, errors,\nconfirmations etc. in special situations. A typical dialog box is displayed in\na small window with some text message along with a few options for the user to\nselect. However, there are certain difficulties associated in programming and\nimplementing a conventional dialog box, such as, severe programming effort,\nrigidity of the hard coded message, obscuring screen space and so on. There is\na need to overcome these difficulties of the dialog box to make them more\nefficient and useful.\n  The modality of the dialog boxes also creates some limitations. While modal\ndialog boxes needs to be closed explicitly by the user, modeless dialog boxes\ncan grow in number and become difficult to control. Thus, an ideal dialog box\nshould be deprived of all the above-mentioned drawbacks. The dialog box should\nnot obscure the screen. The user should be able open multiple dialog boxes but\nwithout obscuring the screen.\n  This article analyses 5 interesting inventions on dialog boxes selected from\nUS Patent database. Each invention tries to overcome some limitations of a\nconventional dialog box and provides some innovative features. Each solution is\nalso analyzed from a TRIZ perspective.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 14:12:20 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6756", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on Tree Navigators used in Graphical User Interface", "comments": "7 pages, 4 figures. Umakant Mishra, Inventions on Tree Navigators\n  Used in Graphical User Interface. (December 6, 2006), Available at SSRN:\n  http://ssrn.com/abstract=949244", "journal-ref": null, "doi": "10.2139/ssrn.949244", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tree view or tree navigator is used to display hierarchical data organized\nin the form of a tree. In a tree structure there are parent and child nodes.\nThe child nodes may further have descendants to n levels.\n  There are many methods to make the navigation easy. Some of these are\nexpanding and collapsing branches, splitting the tree, displaying a parent node\nin a separate tree, zooming branches, scrolling in various directions etc. It\nis still a difficult exercise to handle large trees efficiently. The effort\nstill continues to manage large number of nodes with faster speed, greater\ncontrol, user friendliness and aesthetics.\n  This article illustrates five inventions on tree navigators selected from US\npatent database. Each of them tries to solve various problems relating to the\ntree navigator in different ways. Each invention is also analyzed from a TRIZ\nperspective.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 14:22:02 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6757", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on expressing emotions In Graphical User Interface", "comments": "7 pages, 4 figures. Umakant Mishra, Inventions on Expressing Emotions\n  in Graphical User Interface, (December 6, 2006), Available at SSRN:\n  http://ssrn.com/abstract=949250", "journal-ref": null, "doi": "10.2139/ssrn.949250", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional GUI is more mechanical and does not recognize or communicate\nemotions. The modern GUIs are trying to infer the likely emotional state and\npersonality of the user and communicate through a corresponding emotional\nstate.\n  Emotions are expressed in graphical icons, sounds, pictures and other means.\nThe emotions are found to be useful in especially in communication software,\ninteractive learning systems, robotics and other adaptive environments. Various\nmechanisms have been developed to express emotions through graphical user\ninterfaces. This article illustrates some interesting inventions selected from\nUS patent database.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 14:29:01 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6761", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on GUI for Touch Sensitive Screens", "comments": "6 pages, 4 figures", "journal-ref": "Umakant Mishra, Inventions on GUI for Touch Sensitive Screens\n  (September 7, 2007). Available at SSRN: http://ssrn.com/abstract=1264684 or\n  http://dx.doi.org/10.2139/ssrn.1264684", "doi": "10.2139/ssrn.1264684", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A touch sensitive screen displays the information on the screen and also\nreceives the input by sensing a user's touch on the same screen. This mechanism\nfacilitates system interaction directly through the screen without needing a\nmouse or keyboard. This method has the advantage to make the system compact by\nremoving keyboard, mouse and similar interactive device.\n  However there are certain difficulties to implement a touch screen interface.\nThe display screens of portable devices are becoming smaller thereby leaving\nlesser space for display of data, menu or touch screen interaction. Besides\nsome screens need to display so much of information that they hardly can afford\nany space to display touch screen buttons. This article illustrates various\ninventions which have successfully eliminated these difficulties by applying\nappropriate Inventive principles.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 15:05:44 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6764", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on presenting textual items in Graphical User Interface", "comments": "9 pages, 6 figures", "journal-ref": "Umakant Mishra, Inventions on Presenting Textual Items in\n  Graphical User Interface (September 7, 2007). Available at SSRN:\n  http://ssrn.com/abstract=1264685 or http://dx.doi.org/10.2139/ssrn.1264685", "doi": "10.2139/ssrn.1264685", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a GUI largely replaces textual descriptions by graphical icons, the\ntextual items are not completely removed. The textual items are inevitably used\nin window titles, message boxes, help items, menu items and popup items.\nTextual items are necessary for communicating messages that are beyond the\nlimitation of graphical messages.\n  However, it is necessary to harness the textual items on the graphical\ninterface in such a way that they complement each other to produce the best\neffect. One has to keep various considerations in mind while applying textual\nitems in Graphical User Interface. This article illustrates a few inventions on\npresenting textual items in a Graphical user Interface.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 15:12:56 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6765", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on GUI for Eye Cursor Controls Systems", "comments": "6 pages, 4 figures", "journal-ref": "Mishra, Umakant, Inventions on GUI for Eye Cursor Control Systems\n  (September 7, 2007), Available at SSRN: http://ssrn.com/abstract=1264687 or\n  http://dx.doi.org/10.2139/ssrn.1264687", "doi": "10.2139/ssrn.1264687", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operating a GUI through eyeball is a complex mechanism and not used as often\nas mouse or trackball. But there are situations where eye-mouse devices can\nplay a tremendous role especially where the hands of the user are not available\nor busy to perform other activities. The difficulties of implementing an\neye-cursor control system are many. The article illustrates some inventions on\neye-cursor control system, which attempt to eliminate the difficulties of the\nprior art mechanisms.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 15:18:54 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6771", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on using sound and speech in GUI", "comments": "9 pages, 5 figures. Mishra, Umakant, Inventions on Using Sound and\n  Speech in GUI (September 7, 2007). Available at SSRN:\n  http://ssrn.com/abstract=1264688 or http://dx.doi.org/10.2139/ssrn.1264688", "journal-ref": null, "doi": "10.2139/ssrn.1264688", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice Recognition (VR) facilitates a human interaction with the machine. VR\nmay be used to replace the manual task of pushing buttons on a wireless\ntelephone keypad. This is particularly useful when the hands of the user are\nbusy with other activities like driving a car.\n  However, the VRS system has several limitations. The VRS requires lot of\ntraining and customization in order to be effectively used by individual users\nas each individual falls into different voice patterns. Besides the voice\ninterface is complex and is not as reliable as the keyboard or mouse. This\narticle illustrates some interesting inventions on using sound and voice in\nGraphical User Interfaces.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 15:32:41 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6773", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on Using Colors in Graphical User Interfaces", "comments": "7 pages, 3 figures. Umakant Mishra, Inventions on Color Selections in\n  Graphical User Interfaces (September 7, 2007), Available at SSRN:\n  http://ssrn.com/abstract=1264689 or http://dx.doi.org/10.2139/ssrn.1264689", "journal-ref": null, "doi": "10.2139/ssrn.1264689", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color is an important aspect of any graphical user interface (GUI). Color is\nused to make a GUI attractive and meaningful. But there are difficulties in\nusing colors too. Improper use of color can result in adverse effects. Wrong\ncolors at wrong place can make the GUI look clumsy and confusing. Apart from\nthe aesthetics issues there are many other issues involved with colors too.\n  One of the contradictions relating to usage of color is \"The color of the GUI\nshould be customizable to suit user preference. But at the same time it should\nnot be customizable, as that would cause annoyance and confusion to other\nusers.\"\n  Another contraction relating to using color is \"The user should be displayed\nall 16 million colors to select the desired color precisely, But from another\nangle the user should not be displayed all 16 million colors as that would\ncreate confusion and difficulty in selection.\"\n  This article analyses some inventions selected from US Patent database and\nillustrates how the inventors have been able to solve various contradictions\nrelating to usage of colors in Graphical User Interface.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 15:43:19 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6776", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on GUI Aesthetics", "comments": "9 pages, 7 figures", "journal-ref": "Umakant Mishra, Inventions on GUI Aesthetics (September 7, 2007).\n  Available at SSRN: http://ssrn.com/abstract=1264690 or\n  http://dx.doi.org/10.2139/ssrn.1264690", "doi": "10.2139/ssrn.1264690", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aesthetics or \"look and feel\" is one of the most important features of any\ngraphical user interface. Better aesthetics makes the interface user-friendlier\nand more popular. Better aesthetics helps the user to understand the meaning of\nvarious components and memorize the navigation paths. A better look and feel\nultimately makes a GUI more efficient and effective. Various methods are\nadopted to improve the aesthetics of a GUI, such as, by using colors, using 3D\ngraphics, using pictorial icons, using sound etc.\n  It is important to provide links to all the important features on a desktop\nor on a quick access panel. But too many icons or buttons sometimes creates\nconfusion. Hence it is important to restrict the temptation of putting\neverything on the first screen or load the rarely used buttons on the toolbar.\nOne should ensure that the aesthetics of a GUI is not compromising with its\naccessibility and other important features. This article illustrates some\ninventions made on GUI aesthetics.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 15:51:51 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6779", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on selecting GUI elements", "comments": "8 pages, 5 figures", "journal-ref": "Mishra, Umakant, Inventions on Selecting GUI Elements (September\n  7, 2007), Available at SSRN: http://ssrn.com/abstract=1264692 or\n  http://dx.doi.org/10.2139/ssrn.1264692", "doi": "10.2139/ssrn.1264692", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting an object or element is a fundamental operation in any graphic user\ninterface. It is necessary to select an object before doing any operation (such\nas, dragging, copying, opening, deleting etc.) on that object. The GUI may\nprovide features to select any single object or even multiple objects. The\nfeature of selecting multiple objects can provides tremendous power to the GUI\nas the user can do complex operations on multiple objects in one go.\n  However, the process of selection is not as simple as it appears to the user\nof a GUI. The internal logic of a selection mechanism can be very complex in\nsome situations. The article describes some fundamental difficulties associated\nwith the selection mechanism, and illustrates the solutions provided by\ndifferent inventions selected from US patent database.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 16:06:23 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6782", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on Displaying and Resizing Windows", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": "10.2139/ssrn.1264693", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Windows are used quite frequently in a GUI environment. The greatest\nadvantage of using windows is that each window creates a virtual screen space.\nHence, although the physical screen space is limited to a few inches, use of\nwindows can create unlimited screen space to display innumerable items.\n  The use of windows facilitates the user to open and interact with multiple\nprograms or documents simultaneously in different windows. Sometimes a single\nprogram may also open multiple windows to display various items. The user can\nresize the windows and move their location time to time as desired.\n  However, there are several concerns of a window relating to its size,\nappearance, positioning, color, visibility, resizability etc. For example, the\nwindow should have a minimum and a maximum size, dragging and resizing the\nwindow should be easy, one window should not be obscured by another window,\nwindows should adjust their size and location in order to match with the\nchanges in the resolution and display environment etc.\n  This article illustrates six patents from US Patent database solving problems\nrelating to displaying and resizing of windows. The inventions include\nautomatic resizing and relocating of windows, alternative modes of displaying\nwindows to accommodate within the limited display area, combining both spatial\nand temporal methods to resize the window, locking and minimizing windows and\nmaking the windows invisible on specific situations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 16:14:19 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.6938", "submitter": "Marcin Skowron", "authors": "Marcin Skowron, Stefan Rank, Aleksandra \\'Swiderska, Dennis K\\\"uster,\n  Arvid Kappas", "title": "Applying a Text-Based Affective Dialogue System in Psychological\n  Research: Case Studies on the Effects of System Behavior, Interaction Context\n  and Social Exclusion", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": "10.1007/s12559-014-9271-2", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents two studies conducted with an affective dialogue system\nin which text-based system-user communication was used to model, generate, and\npresent different affective and social interaction scenarios. We specifically\ninvestigated the influence of interaction context and roles assigned to the\nsystem and the participants, as well as the impact of pre-structured social\ninteraction patterns that were modelled to mimic aspects of 'social exclusion'\nscenarios. The results of the first study demonstrate that both the social\ncontext of the interaction and the roles assigned to the system influence the\nsystem evaluation, interaction patterns, textual expressions of affective\nstates, as well as emotional self-reports. The results observed for the second\nstudy show the system's ability to partially exclude a participant from a\ntriadic conversation without triggering significantly different affective\nreactions or a more negative system evaluation. The experimental evidence\nprovides insights on the perception, modelling and generation of affective and\nsocial cues in artificial systems that can be realized in different modalities,\nincluding the text modality, thus delivering valuable input for applying\naffective dialogue systems as tools for studying affect and social aspects in\nonline communication.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 11:36:17 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Skowron", "Marcin", ""], ["Rank", "Stefan", ""], ["\u015awiderska", "Aleksandra", ""], ["K\u00fcster", "Dennis", ""], ["Kappas", "Arvid", ""]]}, {"id": "1404.7120", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on Menu and Toolbar Coordination", "comments": "7 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1404.6747", "journal-ref": "Umakant Mishra, Inventions on Menu and Toolbar Coordination,\n  (December 6, 2006 ) Available at SSRN: http://ssrn.com/abstract=949239", "doi": "10.2139/ssrn.949239", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both toolbar and dropdown menu are used popularly in a graphical user\ninterface with a similar objective of providing easy access to the internal\nfunctions. Often the same functions are provided through both menu and toolbar.\n  Both toolbar and dropdown menu have their own advantages and disadvantages. A\nmenu can provide more options occupying less real estate, while toolbar can\nprovide a single click access without navigating through trees and branches.\n  As a menu and toolbar system shares many common objectives, it is often\nuseful maintain some relationship to coordinate between both the elements of a\nGUI system. The relationships can be easy as both of them often share the same\ninternal function. For example, the print option in a menu will (most likely)\ncall the same function as the print button on the toolbar.\n  This article discusses the similarities and differences between a dropdown\nmenu and toolbar. Five inventions trying to focus on both menu and toolbar are\nillustrated in the article.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 13:50:54 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.7121", "submitter": "Umakant Mishra", "authors": "Umakant Mishra", "title": "Inventions on Drag and Drop in GUI", "comments": "7 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1404.6752", "journal-ref": "Umakant Mishra, Inventions on Drag and Drop in GUI (September 7,\n  2007). Available at SSRN:http://ssrn.com/abstract=1264691 or\n  http://dx.doi.org/10.2139/ssrn.1264691", "doi": "10.2139/ssrn.1264691", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drag and drop operation is one of the key capabilities of any Graphical User\nInterface. The user can do quite complex operations simply by visually dragging\nand dropping objects from one location to another. It saves user from\nremembering and typing a lot of commands.\n  The result of a drag and drop operation may vary depending the type of source\nobject and type of destination object. For example dragging a file and dropping\non a folder may copy or move the file to the destination folder, dropping that\nfile to a remote FTP location may upload that file using internet, dropping\nthat file on a printer icon may print that file, dropping that file on the\ntrash can may delete that file, and dropping that file on an executable may\nplay or open or compute or manipulate that file.\n  Although a drag and drop operation prima facie seems to be a simple\noperation, it can become extremely complicated depending on the type of source\nobjects dragged and the type of destination objects selected for dropping.\nThere are many limitations of a conventional drag and drop operation. This\narticle points out the difficulties of a drag and drop operation and\nillustrates the solutions disclosed by various inventions to overcome those\ndifficulties.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 16:00:55 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Mishra", "Umakant", ""]]}, {"id": "1404.7247", "submitter": "Maria Spichkova", "authors": "Maria Spichkova", "title": "Human Factors of Formal Methods", "comments": "Preprint. Final version published in Proceedings of IADIS\n  International Conference Interfaces and Human Computer Interaction 2012 (IHCI\n  2012), 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a brief introduction to the work that aims to apply the\nachievements within the area of engineering psychology to the area of formal\nmethods, focusing on the specification phase of a system development process.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 06:27:57 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Spichkova", "Maria", ""]]}]