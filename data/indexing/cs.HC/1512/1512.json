[{"id": "1512.00296", "submitter": "Vinay Jayaram", "authors": "Vinay Jayaram, Morteza Alamgir, Yasemin Altun, Bernhard Sch\\\"olkopf,\n  Moritz Grosse-Wentrup", "title": "Transfer Learning in Brain-Computer Interfaces", "comments": "To be published in IEEE Computational Intelligence Magazine, special\n  BCI issue on January 15th online", "journal-ref": null, "doi": "10.1109/MCI.2015.2501545", "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of brain-computer interfaces (BCIs) improves with the amount\nof available training data, the statistical distribution of this data, however,\nvaries across subjects as well as across sessions within individual subjects,\nlimiting the transferability of training data or trained models between them.\nIn this article, we review current transfer learning techniques in BCIs that\nexploit shared structure between training data of multiple subjects and/or\nsessions to increase performance. We then present a framework for transfer\nlearning in the context of BCIs that can be applied to any arbitrary feature\nspace, as well as a novel regression estimation method that is specifically\ndesigned for the structure of a system based on the electroencephalogram (EEG).\nWe demonstrate the utility of our framework and method on subject-to-subject\ntransfer in a motor-imagery paradigm as well as on session-to-session transfer\nin one patient diagnosed with amyotrophic lateral sclerosis (ALS), showing that\nit is able to outperform other comparable methods on an identical dataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 15:33:24 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Jayaram", "Vinay", ""], ["Alamgir", "Morteza", ""], ["Altun", "Yasemin", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1512.01058", "submitter": "Anke Brock", "authors": "Anke Brock (Potioc), Christophe Jouffrais (CNRS, IRIT)", "title": "Interactive audio-tactile maps for visually impaired people", "comments": "\\&lt;http://dl.acm.org/citation.cfm?id=J956\\&CFID=730680571\\&CFTOKEN=17044974\\&gt;.\n  \\&lt;10.1145/2850440.2850441\\&gt", "journal-ref": "ACM SIGACCESS Accessibility and Computing (ACM Digital Library),\n  Association for Computing Machinery (ACM), 2015, pp.3-12.", "doi": "10.1145/2850440.2850441", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually impaired people face important challenges related to orientation and\nmobility. Indeed, 56% of visually impaired people in France declared having\nproblems concerning autonomous mobility. These problems often mean that\nvisually impaired people travel less, which influences their personal and\nprofessional life and can lead to exclusion from society. Therefore this issue\npresents a social challenge as well as an important research area. Accessible\ngeographic maps are helpful for acquiring knowledge about a city's or\nneighborhood's configuration, as well as selecting a route to reach a\ndestination. Traditionally, raised-line paper maps with braille text have been\nused. These maps have proved to be efficient for the acquisition of spatial\nknowledge by visually impaired people. Yet, these maps possess significant\nlimitations. For instance, due to the specificities of the tactile sense only a\nlimited amount of information can be displayed on a single map, which\ndramatically increases the number of maps that are needed. For the same reason,\nit is difficult to represent specific information such as distances. Finally,\nbraille labels are used for textual descriptions but only a small percentage of\nthe visually impaired population reads braille. In France 15% of blind people\nare braille readers and only 10% can read and write. In the United States,\nfewer than 10% of the legally blind people are braille readers and only 10% of\nblind children actually learn braille. Recent technological advances have\nenabled the design of interactive maps with the aim to overcome these\nlimitations. Indeed, interactive maps have the potential to provide a broad\nspectrum of the population with spatial knowledge, irrespective of age,\nimpairment, skill level, or other factors. To this regard, they might be an\nefficient means for providing visually impaired people with access to\ngeospatial information. In this paper we give an overview of our research on\nmaking geographic maps accessible to visually impaired people.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 12:33:54 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Brock", "Anke", "", "Potioc"], ["Jouffrais", "Christophe", "", "CNRS, IRIT"]]}, {"id": "1512.01124", "submitter": "Peter Sunehag", "authors": "Peter Sunehag, Richard Evans, Gabriel Dulac-Arnold, Yori Zwols, Daniel\n  Visentin and Ben Coppin", "title": "Deep Reinforcement Learning with Attention for Slate Markov Decision\n  Processes with High-Dimensional States and Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world problems come with action spaces represented as feature\nvectors. Although high-dimensional control is a largely unsolved problem, there\nhas recently been progress for modest dimensionalities. Here we report on a\nsuccessful attempt at addressing problems of dimensionality as high as $2000$,\nof a particular form. Motivated by important applications such as\nrecommendation systems that do not fit the standard reinforcement learning\nframeworks, we introduce Slate Markov Decision Processes (slate-MDPs). A\nSlate-MDP is an MDP with a combinatorial action space consisting of slates\n(tuples) of primitive actions of which one is executed in an underlying MDP.\nThe agent does not control the choice of this executed action and the action\nmight not even be from the slate, e.g., for recommendation systems for which\nall recommendations can be ignored. We use deep Q-learning based on feature\nrepresentations of both the state and action to learn the value of whole\nslates. Unlike existing methods, we optimize for both the combinatorial and\nsequential aspects of our tasks. The new agent's superiority over agents that\neither ignore the combinatorial or sequential long-term value aspect is\ndemonstrated on a range of environments with dynamics from a real-world\nrecommendation system. Further, we use deep deterministic policy gradients to\nlearn a policy that for each position of the slate, guides attention towards\nthe part of the action space in which the value is the highest and we only\nevaluate actions in this area. The attention is used within a sequentially\ngreedy procedure leveraging submodularity. Finally, we show how introducing\nrisk-seeking can dramatically improve the agents performance and ability to\ndiscover more far reaching strategies.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 15:51:30 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 17:34:55 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Sunehag", "Peter", ""], ["Evans", "Richard", ""], ["Dulac-Arnold", "Gabriel", ""], ["Zwols", "Yori", ""], ["Visentin", "Daniel", ""], ["Coppin", "Ben", ""]]}, {"id": "1512.01260", "submitter": "Simone Fiori", "authors": "Andrea Civita, Simone Fiori, Giuseppe Romani", "title": "A Smartphone-Based Acquisition System for Hips Rotation Fluency\n  Assessment", "comments": "Technical report of the iSpLab, Department of Information\n  Engineering, Universita' Politecnica delle Marche (Italy)", "journal-ref": "Information (MDPI, Special issue on \"eHealth and Artificial\n  Intelligence\"). Vol. 9, No. Article No. 321, December 2018", "doi": "10.3390/info9120321", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present contribution is motivated by recent studies on the assessment of\nthe fluency of body movements during complex motor tasks. In particular, we\nfocus on the estimation of the Cartesian kinematic jerk (namely, the derivative\nof the acceleration) of the hips' orientation during a full three-dimensional\nmovement. The kinematic jerk index is estimated on the basis of gyroscopic\nsignals acquired through a smartphone. A specific free mobile application\navailable for the Android mobile operating system, HyperIMU, is used to acquire\nthe gyroscopic signals and to transmit them to a personal computer via a User\nDatagram Protocol (UDP) through a wireless network. The personal computer\nelaborates the acquired data through a MATLAB script, either in real time or\noffline, and returns the kinematic jerk index associated to a motor task.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 21:36:37 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Civita", "Andrea", ""], ["Fiori", "Simone", ""], ["Romani", "Giuseppe", ""]]}, {"id": "1512.01325", "submitter": "Babak Saleh", "authors": "Babak Saleh, Ahmed Elgammal, Jacob Feldman, Ali Farhadi", "title": "Toward a Taxonomy and Computational Models of Abnormalities in Images", "comments": "To appear in the Thirtieth AAAI Conference on Artificial Intelligence\n  (AAAI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system can spot an abnormal image, and reason about what\nmakes it strange. This task has not received enough attention in computer\nvision. In this paper we study various types of atypicalities in images in a\nmore comprehensive way than has been done before. We propose a new dataset of\nabnormal images showing a wide range of atypicalities. We design human subject\nexperiments to discover a coarse taxonomy of the reasons for abnormality. Our\nexperiments reveal three major categories of abnormality: object-centric,\nscene-centric, and contextual. Based on this taxonomy, we propose a\ncomprehensive computational model that can predict all different types of\nabnormality in images and outperform prior arts in abnormality recognition.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 06:29:53 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Saleh", "Babak", ""], ["Elgammal", "Ahmed", ""], ["Feldman", "Jacob", ""], ["Farhadi", "Ali", ""]]}, {"id": "1512.01872", "submitter": "Pranav Rajpurkar", "authors": "Pranav Rajpurkar, Toki Migimatsu, Jeff Kiske, Royce Cheng-Yue, Sameep\n  Tandon, Tao Wang, Andrew Ng", "title": "Driverseat: Crowdstrapping Learning Tasks for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While emerging deep-learning systems have outclassed knowledge-based\napproaches in many tasks, their application to detection tasks for autonomous\ntechnologies remains an open field for scientific exploration. Broadly, there\nare two major developmental bottlenecks: the unavailability of comprehensively\nlabeled datasets and of expressive evaluation strategies. Approaches for\nlabeling datasets have relied on intensive hand-engineering, and strategies for\nevaluating learning systems have been unable to identify failure-case\nscenarios. Human intelligence offers an untapped approach for breaking through\nthese bottlenecks. This paper introduces Driverseat, a technology for embedding\ncrowds around learning systems for autonomous driving. Driverseat utilizes\ncrowd contributions for (a) collecting complex 3D labels and (b) tagging\ndiverse scenarios for ready evaluation of learning systems. We demonstrate how\nDriverseat can crowdstrap a convolutional neural network on the lane-detection\ntask. More generally, crowdstrapping introduces a valuable paradigm for any\ntechnology that can benefit from leveraging the powerful combination of human\nand computer intelligence.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 01:34:23 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Rajpurkar", "Pranav", ""], ["Migimatsu", "Toki", ""], ["Kiske", "Jeff", ""], ["Cheng-Yue", "Royce", ""], ["Tandon", "Sameep", ""], ["Wang", "Tao", ""], ["Ng", "Andrew", ""]]}, {"id": "1512.02921", "submitter": "Misha Sra", "authors": "Misha Sra, Chris Schmandt", "title": "Design Strategies for Playful Technologies to Support Light-intensity\n  Physical Activity in the Workplace", "comments": "11 pages, 5 figures. Video:\n  http://living.media.mit.edu/projects/see-saw/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moderate to vigorous intensity physical activity has an established\npreventative role in obesity, cardiovascular disease, and diabetes. However\nrecent evidence suggests that sitting time affects health negatively\nindependent of whether adults meet prescribed physical activity guidelines.\nSince many of us spend long hours daily sitting in front of a host of\nelectronic screens, this is cause for concern. In this paper, we describe a set\nof three prototype digital games created for encouraging light-intensity\nphysical activity during short breaks at work. The design of these kinds of\ngames is a complex process that must consider motivation strategies,\ninteraction methodology, usability and ludic aspects. We present design\nguidelines for technologies that encourage physical activity in the workplace\nthat we derived from a user evaluation using the prototypes. Although the\ndesign guidelines can be seen as general principles, we conclude that they have\nto be considered differently for different workplace cultures and workspaces.\nOur study was conducted with users who have some experience playing casual\ngames on their mobile devices and were able and willing to increase their\nphysical activity.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 16:13:05 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Sra", "Misha", ""], ["Schmandt", "Chris", ""]]}, {"id": "1512.02922", "submitter": "Misha Sra", "authors": "Misha Sra, Chris Schmandt", "title": "MetaSpace II: Object and full-body tracking for interaction and\n  navigation in social VR", "comments": "10 pages, 9 figures. Video:\n  http://living.media.mit.edu/projects/metaspace-ii/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MetaSpace II (MS2) is a social Virtual Reality (VR) system where multiple\nusers can not only see and hear but also interact with each other, grasp and\nmanipulate objects, walk around in space, and get tactile feedback. MS2 allows\nwalking in physical space by tracking each user's skeleton in real-time and\nallows users to feel by employing passive haptics i.e., when users touch or\nmanipulate an object in the virtual world, they simultaneously also touch or\nmanipulate a corresponding object in the physical world. To enable these\nelements in VR, MS2 creates a correspondence in spatial layout and object\nplacement by building the virtual world on top of a 3D scan of the real world.\nThrough the association between the real and virtual world, users are able to\nwalk freely while wearing a head-mounted device, avoid obstacles like walls and\nfurniture, and interact with people and objects. Most current virtual reality\n(VR) environments are designed for a single user experience where interactions\nwith virtual objects are mediated by hand-held input devices or hand gestures.\nAdditionally, users are only shown a representation of their hands in VR\nfloating in front of the camera as seen from a first person perspective. We\nbelieve, representing each user as a full-body avatar that is controlled by\nnatural movements of the person in the real world (see Figure 1d), can greatly\nenhance believability and a user's sense immersion in VR.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 16:13:34 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Sra", "Misha", ""], ["Schmandt", "Chris", ""]]}, {"id": "1512.03199", "submitter": "Dominic van der Zypen", "authors": "Michael Mayer, Dominic van der Zypen", "title": "Graph-theoretic autofill", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine a website that asks the user to fill in a web form and -- based on\nthe input values -- derives a relevant figure, for instance an expected salary,\na medical diagnosis or the market value of a house. How to deal with missing\ninput values at run-time? Besides using fixed defaults, a more sophisticated\napproach is to use predefined dependencies (logical or correlational) between\ndifferent fields to autofill missing values in an iterative way. Directed\nloopless graphs (in which cycles are allowed) are the ideal mathematical model\nto formalize these dependencies. We present two new graph-theoretic approaches\nto filling missing values at run-time.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 10:30:19 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Mayer", "Michael", ""], ["van der Zypen", "Dominic", ""]]}, {"id": "1512.04042", "submitter": "Shixia Liu", "authors": "Shixia Liu, Jialun Yin, Xiting Wang, Weiwei Cui, Kelei Cao, Jian Pei", "title": "Online Visual Analytics of Text Streams", "comments": "IEEE TVCG 2016", "journal-ref": null, "doi": "10.1109/TVCG.2015.2509990", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online visual analytics approach to helping users explore and\nunderstand hierarchical topic evolution in high-volume text streams. The key\nidea behind this approach is to identify representative topics in incoming\ndocuments and align them with the existing representative topics that they\nimmediately follow (in time). To this end, we learn a set of streaming tree\ncuts from topic trees based on user-selected focus nodes. A dynamic Bayesian\nnetwork model has been developed to derive the tree cuts in the incoming topic\ntrees to balance the fitness of each tree cut and the smoothness between\nadjacent tree cuts. By connecting the corresponding topics at different times,\nwe are able to provide an overview of the evolving hierarchical topics. A\nsedimentation-based visualization has been designed to enable the interactive\nanalysis of streaming text data from global patterns to local details. We\nevaluated our method on real-world datasets and the results are generally\nfavorable.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 12:22:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Liu", "Shixia", ""], ["Yin", "Jialun", ""], ["Wang", "Xiting", ""], ["Cui", "Weiwei", ""], ["Cao", "Kelei", ""], ["Pei", "Jian", ""]]}, {"id": "1512.04334", "submitter": "Yunde Jia", "authors": "Yunde Jia, Bin Xu, Jiajun Shen, Mintao Pei, Zhen Dong, Jingyi Hou, Min\n  Yang", "title": "Telepresence Interaction by Touching Live Video Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a telepresence interaction framework based on touchscreen\nand telepresence-robot technologies. The core of the framework is a new user\ninterface, Touchable live video Image based User Interface, called TIUI. The\nTIUI allows a remote operator to not just drive the telepresence robot but\noperate and interact with real objects by touching their live video images on a\npad with finger touch gestures. We implemented a telepresence interaction\nsystem which is composed of a telepresence robot and tele-interactive objects\nlocated in a local space, the TIUI of a pad located in a remote space, and the\nwireless networks connecting the two spaces. Our system can be a perfect\nembodiment of a remote operator to do most of daily living tasks, such as\nopening a door, drawing a curtain, pushing a wheelchair, and other like tasks.\nThe evaluation and demonstration results show the effectiveness and promising\napplications of our system.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 14:24:12 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 14:19:51 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Jia", "Yunde", ""], ["Xu", "Bin", ""], ["Shen", "Jiajun", ""], ["Pei", "Mintao", ""], ["Dong", "Zhen", ""], ["Hou", "Jingyi", ""], ["Yang", "Min", ""]]}, {"id": "1512.04582", "submitter": "Jan Egger", "authors": "Jan Egger, Harald Busse, Philipp Brandmaier, Daniel Seider, Matthias\n  Gawlitza, Steffen Strocka, Philip Voglreiter, Mark Dokter, Michael Hofmann,\n  Bernhard Kainz, Alexander Hann, Xiaojun Chen, Tuomas Alhonnoro, Mika Pollari,\n  Dieter Schmalstieg, Michael Moche", "title": "Interactive Volumetry Of Liver Ablation Zones", "comments": "18 pages, 15 figures, 8 tables, 57 references", "journal-ref": "Sci. Rep. 5, 15373; doi: 10.1038/srep15373 (2015)", "doi": "10.1038/srep15373", "report-no": null, "categories": "cs.CV cs.GR cs.HC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Percutaneous radiofrequency ablation (RFA) is a minimally invasive technique\nthat destroys cancer cells by heat. The heat results from focusing energy in\nthe radiofrequency spectrum through a needle. Amongst others, this can enable\nthe treatment of patients who are not eligible for an open surgery. However,\nthe possibility of recurrent liver cancer due to incomplete ablation of the\ntumor makes post-interventional monitoring via regular follow-up scans\nmandatory. These scans have to be carefully inspected for any conspicuousness.\nWithin this study, the RF ablation zones from twelve post-interventional CT\nacquisitions have been segmented semi-automatically to support the visual\ninspection. An interactive, graph-based contouring approach, which prefers\nspherically shaped regions, has been applied. For the quantitative and\nqualitative analysis of the algorithm's results, manual slice-by-slice\nsegmentations produced by clinical experts have been used as the gold standard\n(which have also been compared among each other). As evaluation metric for the\nstatistical validation, the Dice Similarity Coefficient (DSC) has been\ncalculated. The results show that the proposed tool provides lesion\nsegmentation with sufficient accuracy much faster than manual segmentation. The\nvisual feedback and interactivity make the proposed tool well suitable for the\nclinical workflow.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 08:14:32 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Egger", "Jan", ""], ["Busse", "Harald", ""], ["Brandmaier", "Philipp", ""], ["Seider", "Daniel", ""], ["Gawlitza", "Matthias", ""], ["Strocka", "Steffen", ""], ["Voglreiter", "Philip", ""], ["Dokter", "Mark", ""], ["Hofmann", "Michael", ""], ["Kainz", "Bernhard", ""], ["Hann", "Alexander", ""], ["Chen", "Xiaojun", ""], ["Alhonnoro", "Tuomas", ""], ["Pollari", "Mika", ""], ["Schmalstieg", "Dieter", ""], ["Moche", "Michael", ""]]}, {"id": "1512.05220", "submitter": "Luke Bashford", "authors": "Luke Bashford and Carsten Mehring", "title": "Ownership and Agency of an Independent Supernumerary Hand Induced by an\n  Imitation Brain-Computer Interface", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0156591", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study body ownership and control, illusions that elicit these feelings in\nnon-body objects are widely used. Classically introduced with the Rubber Hand\nIllusion, these illusions have been replicated more recently in virtual reality\nand by using brain-computer interfaces. Traditionally these illusions\ninvestigate the replacement of a body part by an artificial counterpart,\nhowever as brain-computer interface research develops it offers us the\npossibility to explore the case where non-body objects are controlled in\naddition to movements of our own limbs. Therefore we propose a new illusion\ndesigned to test the feeling of ownership and control of an independent\nsupernumerary hand. Subjects are under the impression they control a virtual\nreality hand via a brain-computer interface, but in reality there is no causal\nconnection between brain activity and virtual hand movement but correct\nmovements are observed with 80% probability. These imitation brain-computer\ninterface trials are interspersed with movements in both the subjects' real\nhands, which are in view throughout the experiment. We show that subjects\ndevelop strong feelings of ownership and control over the third hand, despite\nonly receiving visual feedback with no causal link to the actual brain signals.\nOur illusion is crucially different from previously reported studies as we\ndemonstrate independent ownership and control of the third hand without loss of\nownership in the real hands.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 15:55:14 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 14:47:34 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Bashford", "Luke", ""], ["Mehring", "Carsten", ""]]}, {"id": "1512.05471", "submitter": "Christian Sandor", "authors": "Christian Sandor, Martin Fuchs, Alvaro Cassinelli, Hao Li, Richard\n  Newcombe, Goshiro Yamamoto, Steven Feiner", "title": "Breaking the Barriers to True Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Augmented Reality (AR) and Virtual Reality (VR) have gained\nconsiderable commercial traction, with Facebook acquiring Oculus VR for \\$2\nbillion, Magic Leap attracting more than \\$500 million of funding, and\nMicrosoft announcing their HoloLens head-worn computer. Where is humanity\nheaded: a brave new dystopia-or a paradise come true?\n  In this article, we present discussions, which started at the symposium\n\"Making Augmented Reality Real\", held at Nara Institute of Science and\nTechnology in August 2014. Ten scientists were invited to this three-day event,\nwhich started with a full day of public presentations and panel discussions\n(video recordings are available at the event web page), followed by two days of\nroundtable discussions addressing the future of AR and VR.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 05:57:06 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Sandor", "Christian", ""], ["Fuchs", "Martin", ""], ["Cassinelli", "Alvaro", ""], ["Li", "Hao", ""], ["Newcombe", "Richard", ""], ["Yamamoto", "Goshiro", ""], ["Feiner", "Steven", ""]]}, {"id": "1512.05497", "submitter": "Per B{\\ae}kgaard", "authors": "Per B{\\ae}kgaard, Michael Kai Petersen, Jakob Eg Larsen", "title": "Assessing Levels of Attention using Low Cost Eye Tracking", "comments": "12 pages, 6 figures, 2 tables. The final publication will be\n  available at Springer via http://dx.doi.org/DOIxxx, when published as part of\n  the HCI International 2016 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of mobile eye trackers embedded in next generation smartphones\nor VR displays will make it possible to trace not only what objects we look at\nbut also the level of attention in a given situation. Exploring whether we can\nquantify the engagement of a user interacting with a laptop, we apply mobile\neye tracking in an in-depth study over 2 weeks with nearly 10.000 observations\nto assess pupil size changes, related to attentional aspects of alertness,\norientation and conflict resolution. Visually presenting conflicting cues and\ntargets we hypothesize that it's feasible to measure the allocated effort when\nresponding to confusing stimuli. Although such experiments are normally carried\nout in a lab, we are able to differentiate between sustained alertness and\ncomplex decision making even with low cost eye tracking \"in the wild\". From a\nquantified self perspective of individual behavioral adaptation, the\ncorrelations between the pupil size and the task dependent reaction time and\nerror rates may longer term provide a foundation for modifying smartphone\ncontent and interaction to the users perceived level of attention.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 09:04:50 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 11:24:06 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["B\u00e6kgaard", "Per", ""], ["Petersen", "Michael Kai", ""], ["Larsen", "Jakob Eg", ""]]}, {"id": "1512.05742", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Laurent Charlin,\n  Joelle Pineau", "title": "A Survey of Available Corpora for Building Data-Driven Dialogue Systems", "comments": "56 pages including references and appendix, 5 tables and 1 figure;\n  Under review for the Dialogue & Discourse journal. Update: paper has been\n  rewritten and now includes several new datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, several areas of speech and language understanding\nhave witnessed substantial breakthroughs from the use of data-driven models. In\nthe area of dialogue systems, the trend is less obvious, and most practical\nsystems are still built through significant engineering and expert knowledge.\nNevertheless, several recent results suggest that data-driven approaches are\nfeasible and quite promising. To facilitate research in this area, we have\ncarried out a wide survey of publicly available datasets suitable for\ndata-driven learning of dialogue systems. We discuss important characteristics\nof these datasets, how they can be used to learn diverse dialogue strategies,\nand their other potential uses. We also examine methods for transfer learning\nbetween datasets and the use of external knowledge. Finally, we discuss\nappropriate choice of evaluation metrics for the learning objective.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 19:52:39 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 04:58:05 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 01:15:32 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Lowe", "Ryan", ""], ["Henderson", "Peter", ""], ["Charlin", "Laurent", ""], ["Pineau", "Joelle", ""]]}, {"id": "1512.05949", "submitter": "Tim Jungnickel", "authors": "Tim Jungnickel and Tobias Herb", "title": "TP1-valid Transformation Functions for Operations on ordered n-ary Trees", "comments": "Extension/Report for the work \"Simultaneous Editing of JSON Objects\n  via Operational Transformation\" in ACM SAC '16", "journal-ref": null, "doi": "10.1145/2851613.2852003", "report-no": null, "categories": "cs.LO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative work on shared documents was revolutionized by web services\nlike Google Docs or Etherpad. Multiple users can work on the same document in a\ncomfortable and distributed way. For the synchronization of the changes a\nreplication system named Operational Transformation is used. Such a system\nconsists of a control algorithm and a transformation function. In essence, a\ntransformation function solves the conflicts that arise when multiple users\nchange the document at the same time. In this work we investigate on the\ncorrectness of such transformation functions. We introduce transformation\nfunctions n-ary trees that we designed especially for the purpose of\nsynchronization changes on JSON objects. We provide a detailed proof of the\nnecessary property: the Transformation Property 1.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 13:41:10 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Jungnickel", "Tim", ""], ["Herb", "Tobias", ""]]}, {"id": "1512.07487", "submitter": "Emilio Leonardi", "authors": "Alessandro Nordio, Alberto Tarable, Emilio Leonardi, Marco Ajmone\n  Marsan", "title": "Selecting the top-quality item through crowd scoring", "comments": "To be published, ACM TOMPECS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate crowdsourcing algorithms for finding the top-quality item\nwithin a large collection of objects with unknown intrinsic quality values.\nThis is an important problem with many relevant applications, for example in\nnetworked recommendation systems. The core of the algorithms is that objects\nare distributed to crowd workers, who return a noisy and biased evaluation. All\nreceived evaluations are then combined, to identify the top-quality object. We\nfirst present a simple probabilistic model for the system under investigation.\nThen, we devise and study a class of efficient adaptive algorithms to assign in\nan effective way objects to workers. We compare the performance of several\nalgorithms, which correspond to different choices of the design\nparameters/metrics. In the simulations we show that some of the algorithms\nachieve near optimal performance for a suitable setting of the system\nparameters.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 14:23:15 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 08:50:48 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Nordio", "Alessandro", ""], ["Tarable", "Alberto", ""], ["Leonardi", "Emilio", ""], ["Marsan", "Marco Ajmone", ""]]}, {"id": "1512.07563", "submitter": "Didier Fass", "authors": "Didier Fass (MOSEL)", "title": "Affordances and Safe Design of Assistance Wearable Virtual Environment\n  of Gesture", "comments": null, "journal-ref": "Tared Ahram, Waldemar Karwowski, Dylan Schmorrow. Human Factors\n  and Egonomics (AHFE 2015), Jul 2015, Las Vegas, United States. Elsivier,\n  Procedia Manufacturing, pp.8, 2015, 6th International Conference on Applied\n  Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE\n  2015", "doi": "10.1016/j.promfg.2015.07.343", "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety and reliability are the main issues for designing assistance wearable\nvirtual environment of technical gesture in aerospace, or health application\ndomains. That needs the integration in the same isomorphic engineering\nframework of human requirements, systems requirements and the rationale of\ntheir relation to the natural and artifactual environment.To explore coupling\nintegration and design functional organization of support technical gesture\nsystems, firstly ecological psychologyprovides usa heuristicconcept: the\naffordance. On the other hand mathematical theory of integrative physiology\nprovides us scientific concepts: the stabilizing auto-association principle and\nfunctional interaction.After demonstrating the epistemological consistence of\nthese concepts, we define an isomorphic framework to describe and model human\nsystems integration dedicated to human in-the-loop system engineering.We\npresent an experimental approach of safe design of assistance wearable virtual\nenvironment of gesture based in laboratory and parabolic flights. On the\nresults, we discuss the relevance of our conceptual approach and the\napplications to future assistance of gesture wearable systems engineering.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 17:57:23 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Fass", "Didier", "", "MOSEL"]]}, {"id": "1512.07592", "submitter": "Kin Gwn Lore", "authors": "Kin Gwn Lore, Nicholas Sweet, Kundan Kumar, Nisar Ahmed, Soumik Sarkar", "title": "Deep Value of Information Estimators for Collaborative Human-Machine\n  Information Gathering", "comments": "10 pages, to appear in ICCPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective human-machine collaboration can significantly improve many learning\nand planning strategies for information gathering via fusion of 'hard' and\n'soft' data originating from machine and human sensors, respectively. However,\ngathering the most informative data from human sensors without task overloading\nremains a critical technical challenge. In this context, Value of Information\n(VOI) is a crucial decision-theoretic metric for scheduling interaction with\nhuman sensors. We present a new Deep Learning based VOI estimation framework\nthat can be used to schedule collaborative human-machine sensing with\ncomputationally efficient online inference and minimal policy hand-tuning.\nSupervised learning is used to train deep convolutional neural networks (CNNs)\nto extract hierarchical features from 'images' of belief spaces obtained via\ndata fusion. These features can be associated with soft data query choices to\nreliably compute VOI for human interaction. The CNN framework is described in\ndetail, and a performance comparison to a feature-based POMDP scheduling policy\nis provided. The practical feasibility of our method is also demonstrated on a\nmobile robotic search problem with language-based semantic human sensor inputs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 19:14:38 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Lore", "Kin Gwn", ""], ["Sweet", "Nicholas", ""], ["Kumar", "Kundan", ""], ["Ahmed", "Nisar", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1512.08321", "submitter": "Jooyeon Kim", "authors": "Jooyeon Kim, Brian C. Keegan, Sungjoon Park, Alice Oh", "title": "The Proficiency-Congruency Dilemma: Virtual Team Design and Performance\n  in Multiplayer Online Games", "comments": "To appear In Proceedings of the 34th Annual ACM Conference on Human\n  Factors in Computing Systems (CHI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplayer online battle arena games provide an excellent opportunity to\nstudy team performance. When designing a team, players must negotiate a\n\\textit{proficiency-congruency dilemma} between selecting roles that best match\ntheir experience and roles that best complement the existing roles on the team.\nWe adopt a mixed-methods approach to explore how users negotiate this dilemma.\nUsing data from \\textit{League of Legends}, we define a similarity space to\noperationalize team design constructs about role proficiency, generality, and\ncongruency. We collect publicly available data from 3.36 million users to test\nthe influence of these constructs on team performance. We also conduct focus\ngroups with novice and elite players to understand how players' team design\npractices vary with expertise. We find that player proficiency increases team\nperformance more than team congruency. These findings have implications for\nplayers, designers, and theorists about how to recommend team designs that\njointly prioritize individuals' expertise and teams' compatibility.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 05:55:37 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Kim", "Jooyeon", ""], ["Keegan", "Brian C.", ""], ["Park", "Sungjoon", ""], ["Oh", "Alice", ""]]}, {"id": "1512.08799", "submitter": "Hao Wu", "authors": "Hao Wu, Maoyuan Sun, Peng Mi, Nikolaj Tatti, Chris North, Naren\n  Ramakrishnan", "title": "Interactive Discovery of Coordinated Relationship Chains with Maximum\n  Entropy Models", "comments": "The journal version of paper is submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern visual analytic tools promote human-in-the-loop analysis but are\nlimited in their ability to direct the user toward interesting and promising\ndirections of study. This problem is especially acute when the analysis task is\nexploratory in nature, e.g., the discovery of potentially coordinated\nrelationships in massive text datasets. Such tasks are very common in domains\nlike intelligence analysis and security forensics where the goal is to uncover\nsurprising coalitions bridging multiple types of relations. We introduce new\nmaximum entropy models to discover surprising chains of relationships\nleveraging count data about entity occurrences in documents. These models are\nembedded in a visual analytic system called MERCER that treats relationship\nbundles as first class objects and directs the user toward promising lines of\ninquiry. We demonstrate how user input can judiciously direct analysis toward\nvalid conclusions whereas a purely algorithmic approach could be led astray.\nExperimental results on both synthetic and real datasets from the intelligence\ncommunity are presented.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 21:27:05 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Wu", "Hao", ""], ["Sun", "Maoyuan", ""], ["Mi", "Peng", ""], ["Tatti", "Nikolaj", ""], ["North", "Chris", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1512.09163", "submitter": "Gordon Love", "authors": "Paul V. Johnson, Jared A.Q. Parnell, Joowan Kim, Christopher D.\n  Saunter, Gordon D. Love, Martin S. Banks", "title": "Dynamic lens and monovision 3D displays to improve viewer comfort", "comments": null, "journal-ref": null, "doi": "10.1364/OE.24.011808", "report-no": null, "categories": "cs.HC physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereoscopic 3D (S3D) displays provide an additional sense of depth compared\nto non-stereoscopic displays by sending slightly different images to the two\neyes. But conventional S3D displays do not reproduce all natural depth cues. In\nparticular, focus cues are incorrect causing mismatches between accommodation\nand vergence: The eyes must accommodate to the display screen to create sharp\nretinal images even when binocular disparity drives the eyes to converge to\nother distances. This mismatch causes visual discomfort and reduces visual\nperformance. We propose and assess two new techniques that are designed to\nreduce the vergence-accommodation conflict and thereby decrease discomfort and\nincrease visual performance. These techniques are much simpler to implement\nthan previous conflict-reducing techniques.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 21:59:14 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Johnson", "Paul V.", ""], ["Parnell", "Jared A. Q.", ""], ["Kim", "Joowan", ""], ["Saunter", "Christopher D.", ""], ["Love", "Gordon D.", ""], ["Banks", "Martin S.", ""]]}]