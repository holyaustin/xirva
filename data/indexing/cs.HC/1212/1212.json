[{"id": "1212.0083", "submitter": "Laurent Bougrain", "authors": "Laurent Bougrain (INRIA Nancy - Grand Est / LORIA), Olivier Rochel\n  (INRIA), Octave Boussaton (INRIA Nancy - Grand Est / LORIA), Lionel Havet\n  (INRIA Nancy - Grand Est / LORIA)", "title": "From the decoding of cortical activities to the control of a JACO\n  robotic arm: a whole processing chain", "comments": null, "journal-ref": "CAR - Control Architecture of Robots - 2012 (2012)", "doi": null, "report-no": null, "categories": "cs.NE cs.HC cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a complete processing chain for decoding intracranial\ndata recorded in the cortex of a monkey and replicates the associated movements\non a JACO robotic arm by Kinova. We developed specific modules inside the\nOpenViBE platform in order to build a Brain-Machine Interface able to read the\ndata, compute the position of the robotic finger and send this position to the\nrobotic arm. More pre- cisely, two client/server protocols have been tested to\ntransfer the finger positions: VRPN and a light protocol based on TCP/IP\nsockets. According to the requested finger position, the server calls the\nassoci- ated functions of an API by Kinova to move the fin- gers properly.\nFinally, we monitor the gap between the requested and actual fingers positions.\nThis chain can be generalized to any movement of the arm or wrist.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 08:27:24 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Bougrain", "Laurent", "", "INRIA Nancy - Grand Est / LORIA"], ["Rochel", "Olivier", "", "INRIA"], ["Boussaton", "Octave", "", "INRIA Nancy - Grand Est / LORIA"], ["Havet", "Lionel", "", "INRIA Nancy - Grand Est / LORIA"]]}, {"id": "1212.0169", "submitter": "Marko Horvat", "authors": "Marko Horvat, Sini\\v{s}a Popovi\\'c, Kre\\v{s}imir \\'Cosi\\'c", "title": "Towards semantic and affective coupling in emotionally annotated\n  databases", "comments": "6 pages, 6 figures", "journal-ref": "Proceedings of the 35nd International Convention MIPRO 2012 (2012)\n  1003-1008", "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotionally annotated databases are repositories of multimedia documents with\nannotated affective content that elicit emotional responses in exposed human\nsubjects. They are primarily used in research of human emotions, attention and\ndevelopment of stress-related mental disorders. This can be successfully\nexploited in larger processes like selection, evaluation and training of\npersonnel for occupations involving high stress levels. Emotionally annotated\ndatabases are also used in multimodal affective user interfaces to facilitate\nricher and more intuitive human-computer interaction. Multimedia documents in\nemotionally annotated databases must have maximum personal ego relevance to be\nthe most effective in all these applications. For this reason flexible\nconstruction of subject-specific of emotionally annotated databases is\nimperative. But current construction process is lengthy and labor intensive\nbecause it inherently includes an elaborate tagging experiment involving a team\nof human experts. This is unacceptable since the creation of new databases or\nmodification of the existing ones becomes slow and difficult. We identify a\npositive correlation between the affect and semantics in the existing\nemotionally annotated databases and propose to exploit this feature with an\ninteractive relevance feedback for a more efficient construction of emotionally\nannotated databases. Automatic estimation of affective annotations from\nexisting semantics enhanced with information refinement processes may lead to\nan efficient construction of high-quality emotionally annotated databases.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 23:36:14 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Horvat", "Marko", ""], ["Popovi\u0107", "Sini\u0161a", ""], ["\u0106osi\u0107", "Kre\u0161imir", ""]]}, {"id": "1212.0469", "submitter": "Po T. Wang", "authors": "Po T. Wang, Christine E. King, An H. Do, Zoran Nenadic", "title": "Pushing the Communication Speed Limit of a Noninvasive BCI Speller", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Electroencephalogram (EEG) based brain-computer interfaces (BCI) may provide\na means of communication for those affected by severe paralysis. However, the\nrelatively low information transfer rates (ITR) of these systems, currently\nlimited to 1 bit/sec, present a serious obstacle to their widespread adoption\nin both clinical and non-clinical applications. Here, we report on the\ndevelopment of a novel noninvasive BCI communication system that achieves ITRs\nthat are severalfold higher than those previously reported with similar\nsystems. Using only 8 EEG channels, 6 healthy subjects with little to no prior\nBCI experience selected characters from a virtual keyboard with sustained,\nerror-free, online ITRs in excess of 3 bit/sec. By factoring in the time spent\nto notify the subjects of their selection, practical, error-free typing rates\nas high as 12.75 character/min were achieved, which allowed subjects to\ncorrectly type a 44-character sentence in less than 3.5 minutes. We hypothesize\nthat ITRs can be further improved by optimizing the parameters of the\ninterface, while practical typing rates can be significantly improved by\nshortening the selection notification time. These results provide compelling\nevidence that the ITR limit of noninvasive BCIs has not yet been reached and\nthat further investigation into this matter is both justified and necessary.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 17:59:52 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2013 17:02:21 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Wang", "Po T.", ""], ["King", "Christine E.", ""], ["Do", "An H.", ""], ["Nenadic", "Zoran", ""]]}, {"id": "1212.0647", "submitter": "Mohammadi Akheela Khanum Mrs", "authors": "Mohammadi Akheela Khanum, Munesh Chandra Trivedi", "title": "Take Care: A Study on Usability Evaluation Methods for Children", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays whenever a user buys any gadget, apart from the price his focus\nwould also be on how easy is the functionality of the gadget. This means users\nare more concerned towards the usability of the gadget. Therefore, this study\nset to explore usability evaluations methods for children in order to analyze\ntheir roles in the development of technology. Usability evaluation methods\nwhich are successfully tested on the adults are investigated to find out how\nsuccessfully they can also be applied to children. The results of the review\nindicate that usability evaluation with children is more challenging than with\nadults. The study found that depending on one type of usability evaluation\nmethod would not be an appropriate decision.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 09:22:31 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Khanum", "Mohammadi Akheela", ""], ["Trivedi", "Munesh Chandra", ""]]}, {"id": "1212.1849", "submitter": "Rukshan Alexander", "authors": "A. Rukshan and A. Baravalle", "title": "Automated Usability Testing: Analysing Asia Web Sites", "comments": "13 pages, 6 Tables, and 3 Figures", "journal-ref": "International Conference on Business and Information 2012 (ICBI\n  2012), Faculty of Commerce and Management Studies of University of Kelaniya,\n  Sri Lanka", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web usability is continuing to be a pressing problem. For number of years\nresearchers have been developed tools for doing automatic web usability\ntesting. This study uses our own PHP, and MySQL based tool AWebHUT: Automated\nWeb Homepage Usability Tester to evaluate web usability of full Dmoz\n(www.dmoz.org) Asia web sites (45126 on time stamp 2011-12-03 04:12:46 GMT).\nThe tool uses an extensive automated quantitative analysis of XHTML source code\nof homepages against seventeen organised web usability guidelines. The\nautomated quantitative approach is effective on large scale to achieve better\nusability. The AWebHUT uses four web usability levels such as N: Neutral, V:\nViolate, R: Respect, and E: Error to evaluate web usability. The main objective\nof the study is to produce data which is used to answer research questions, (1)\nAre there any categories of web sites which have usability problems? Which\nones? and (2) Are there any categories in which the usability is typically\nhigher? Why? The findings were indicated that all Asia categories have\nusability problems. Furthermore, there are four web sites which have highest\nweb usability problem with violation percentage 71. One step further, the Asia\ncategory: Weather has highest usability problems with 42.2819 as the average of\nthe violation percentage. The category Weather uses tables and images,\nconsiderable amount of those were not satisfying web usability guidelines which\nrelates to tables and images. One step further, the Asia wants to get the same\nlevel of usability as North America, Europe, and Australia therefore it is\nessential to have an automated web usability evaluation in Asia web sites to\nidentify web usability problems which are important for improving Asia web\nsites.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2012 02:20:00 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Rukshan", "A.", ""], ["Baravalle", "A.", ""]]}, {"id": "1212.1986", "submitter": "Lee Worden", "authors": "Lee Worden", "title": "WorkingWiki: a MediaWiki-based platform for collaborative research", "comments": "11 pages, 3 figures. Presented at ITP 2011 Workshop on Mathematical\n  Wikis. Source code archived with revision history at\n  <http://lalashan.mcmaster.ca/theobio/projects/index.php/Paper_for_MathWikis-2011>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  WorkingWiki is a software extension for the popular MediaWiki platform that\nmakes a wiki into a powerful environment for collaborating on\npublication-quality manuscripts and software projects. Developed in Jonathan\nDushoff's theoretical biology lab at McMaster University and available as free\nsoftware, it allows wiki users to work together on anything that can be done by\nusing UNIX commands to transform textual \"source code\" into output. Researchers\ncan use it to collaborate on programs written in R, python, C, or any other\nlanguage, and there are special features to support easy work on LaTeX\ndocuments. It develops the potential of the wiki medium to serve as a\ncombination collaborative text editor, development environment, revision\ncontrol system, and publishing platform. Its potential uses are open-ended -\nits processing is controlled by makefiles that are straightforward to customize\n- and its modular design is intended to allow parts of it to be adapted to\nother purposes.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 07:10:51 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Worden", "Lee", ""]]}, {"id": "1212.3540", "submitter": "Dima Kagan", "authors": "Yehonatan Bitton, Michael Fire, Dima Kagan, Bracha Shapira, Lior\n  Rokach, Judit Bar-Ilan", "title": "Social Network Based Search for Experts", "comments": "Participated in HCIR 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our system illustrates how information retrieved from social networks can be\nused for suggesting experts for specific tasks. The system is designed to\nfacilitate the task of finding the appropriate person(s) for a job, as a\nconference committee member, an advisor, etc. This short description will\ndemonstrate how the system works in the context of the HCIR2012 published\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 17:35:31 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Bitton", "Yehonatan", ""], ["Fire", "Michael", ""], ["Kagan", "Dima", ""], ["Shapira", "Bracha", ""], ["Rokach", "Lior", ""], ["Bar-Ilan", "Judit", ""]]}, {"id": "1212.6250", "submitter": "Miao Song", "authors": "Miao Song", "title": "Computer-Assisted Interactive Documentary and Performance Arts in\n  Illimitable Space", "comments": "PhD thesis copy; 272 pages, 83 figures, 6 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CY cs.GR cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This major component of the research described in this thesis is 3D computer\ngraphics, specifically the realistic physics-based softbody simulation and\nhaptic responsive environments. Minor components include advanced\nhuman-computer interaction environments, non-linear documentary storytelling,\nand theatre performance. The journey of this research has been unusual because\nit requires a researcher with solid knowledge and background in multiple\ndisciplines; who also has to be creative and sensitive in order to combine the\npossible areas into a new research direction. [...] It focuses on the advanced\ncomputer graphics and emerges from experimental cinematic works and theatrical\nartistic practices. Some development content and installations are completed to\nprove and evaluate the described concepts and to be convincing. [...] To\nsummarize, the resulting work involves not only artistic creativity, but\nsolving or combining technological hurdles in motion tracking, pattern\nrecognition, force feedback control, etc., with the available documentary\nfootage on film, video, or images, and text via a variety of devices [....] and\nprogramming, and installing all the needed interfaces such that it all works in\nreal-time. Thus, the contribution to the knowledge advancement is in solving\nthese interfacing problems and the real-time aspects of the interaction that\nhave uses in film industry, fashion industry, new age interactive theatre,\ncomputer games, and web-based technologies and services for entertainment and\neducation. It also includes building up on this experience to integrate Kinect-\nand haptic-based interaction, artistic scenery rendering, and other forms of\ncontrol. This research work connects all the research disciplines, seemingly\ndisjoint fields of research, such as computer graphics, documentary film,\ninteractive media, and theatre performance together.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2012 20:49:45 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Song", "Miao", ""]]}, {"id": "1212.6273", "submitter": "John-John Cabibihan", "authors": "John-John Cabibihan, Wing-Chee So, and Soumo Pramanik", "title": "Human-Recognizable Robotic Gestures", "comments": "21 pages, 5 figures", "journal-ref": "Autonomous Mental Development, IEEE Transactions, 2012, 4(4),\n  305-314", "doi": "10.1109/TAMD.2012.2208962", "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  For robots to be accommodated in human spaces and in humans daily activities,\nrobots should be able to understand messages from the human conversation\npartner. In the same light, humans must also understand the messages that are\nbeing communicated by robots, including the non-verbal ones. We conducted a\nweb-based video study wherein participants gave interpretations on the iconic\ngestures and emblems that were produced by an anthropomorphic robot. Out of the\n15 gestures presented, we found 6 robotic gestures that can be accurately\nrecognized by the human observer. These were nodding, clapping, hugging,\nexpressing anger, walking, and flying. We reviewed these gestures for their\nmeaning from literatures in human and animal behavior. We conclude by\ndiscussing the possible implications of these gestures for the design of social\nrobots that are aimed to have engaging interactions with humans.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2012 22:10:14 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Cabibihan", "John-John", ""], ["So", "Wing-Chee", ""], ["Pramanik", "Soumo", ""]]}]