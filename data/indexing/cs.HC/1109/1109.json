[{"id": "1109.0132", "submitter": "Sushma Yalamanchili", "authors": "Sushma Yalamanchili, M.Kameswara Rao", "title": "A Framework for Devanagari Script-based Captcha", "comments": "10 pages, 8 Figures, CCSEA 2011 - First International Conference,\n  Chennai, July 15-17, 2011", "journal-ref": "International Journal of Advanced Information Technology, Vol. 1,\n  No. 4, August, pp. 47-57, 2011", "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Interactive Proofs (HIPs) are automatic reverse Turing tests designed\nto distinguish between various groups of users. Completely Automatic Public\nTuring test to tell Computers and Humans Apart (CAPTCHA) is a HIP system that\ndistinguish between humans and malicious computer programs. Many CAPTCHAs have\nbeen proposed in the literature that text-graphical based, audio-based,\npuzzle-based and mathematical questions-based. The design and implementation of\nCAPTCHAs fall in the realm of Artificial Intelligence. We aim to utilize\nCAPTCHAs as a tool to improve the security of Internet based applications. In\nthis paper we present a framework for a text-based CAPTCHA based on Devanagari\nscript which can exploit the difference in the reading proficiency between\nhumans and computer programs. Our selection of Devanagari script-based CAPTCHA\nis based on the fact that it is used by a large number of Indian languages\nincluding Hindi which is the third most spoken language. There is potential for\nan exponential rise in the applications that are likely to be developed in that\nscript thereby making it easy to secure Indian language based applications.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 09:04:56 GMT"}], "update_date": "2011-09-02", "authors_parsed": [["Yalamanchili", "Sushma", ""], ["Rao", "M. Kameswara", ""]]}, {"id": "1109.1454", "submitter": "Anis Ismail", "authors": "Anis Ismail, Abd El Salam AL Hajjar and Mohammad Hajjar", "title": "A Prototype System for Controlling a Computer by Head Movements and\n  Voice Commands", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new prototype system for controlling a PC by head\nmovements and also with voice commands. Our system is a multimodal interface\nconcerned with controlling the computer. The selected modes of interaction are\nspeech and gestures. We are seeing the revolutionary of computers and\ninformation technologies into daily practice. Healthy people use keyboard,\nmouse, trackball, or touchpad for controlling the PC. However these peripheries\nare usually not suitable for handicapped people. They may have problems using\nthese standard peripheries, for example when they suffer from myopathy, or\ncannot move their hands after an injury. Our system has been developed to\nprovide computer access for people with severe disabilities. This system tracks\nthe computer user's Head movements with a video camera and translates them into\nthe movements of the mouse pointer on the screen and the voice as button\npresses. Therefore we are coming with a proposal system that can be used with\nhandicapped people to control the PC.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 10:26:17 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Ismail", "Anis", ""], ["Hajjar", "Abd El Salam AL", ""], ["Hajjar", "Mohammad", ""]]}, {"id": "1109.2114", "submitter": "Daniel Kharitonov", "authors": "Daniel Kharitonov", "title": "Net-Centric World: Lifestyle of the 21st Century", "comments": "Preprint for ITU-T Kaleidoscope 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we research the potential of information communication\ntechnologies (ICTs) for changing our society from a commute-centric to a\nnetwork-centric environment. We propose to formalize the key attributes of\nICT-based telecommuting experiences from both economic and human interactivity\nperspective. We introduce the notion of network-eligible transactions and\ndisclose the link between degree of network centricity and worker settlement\nradius, postulating that media-rich network services have a strong potential to\nincrease the physical distance between work and home locations. We also\nhighlight notable technology challenges and opportunities of migration from\nlocation-based to mobile living, signifying the needs for new services and\nstandards development.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 19:22:42 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Kharitonov", "Daniel", ""]]}, {"id": "1109.2997", "submitter": "Sergey Andreyev", "authors": "Sergey Andreyev", "title": "Rejecting Adaptive Interface", "comments": "14 pages, 3 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs have to be designed in such a way as to make them looking good and\nbeing handy for all users. Adaptive interface, with all the numerous\nachievements throughout 30 years of its history, contains and in reality is\nbased on one fundamental flaw - on the assumption that designer knows better\nthan anyone else what is good for users in each and all cases. Programs of the\nnew type - user-driven applications - still deliver to users the best results\nof developers' work but at the same time give users the full control over\napplications and in this way really allow each user to change an application in\nsuch a way as he wants it to look at each particular moment. Users can move and\nresize each and all of the screen objects while an application is running, and\nthis changes the whole programming philosophy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2011 05:53:45 GMT"}], "update_date": "2011-09-15", "authors_parsed": [["Andreyev", "Sergey", ""]]}, {"id": "1109.5034", "submitter": "Piotr Gawron jr.", "authors": "Micha{\\l} Romaszewski and Przemys{\\l}aw G{\\l}omb and Piotr Gawron", "title": "Natural hand gestures for human identification in a Human-Computer\n  Interface", "comments": "13 pages, 3 figures, This is a major rewrite of previous version of\n  the paper. The same dataset as in previous version was used. The analysis is\n  now focused on application of the gestures classification methods to human\n  identification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is the identification of humans based on motion data in\nthe form of natural hand gestures. In this paper, the identification problem is\nformulated as classification with classes corresponding to persons' identities,\nbased on recorded signals of performed gestures. The identification performance\nis examined with a database of twenty-two natural hand gestures recorded with\ntwo types of hardware and three state-of-art classifiers: Linear Discrimination\nAnalysis (LDA), Support Vector machines (SVM) and k-Nearest Neighbour (k-NN).\nResults show that natural hand gestures allow for an effective human\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2011 11:16:25 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 18:42:02 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Romaszewski", "Micha\u0142", ""], ["G\u0142omb", "Przemys\u0142aw", ""], ["Gawron", "Piotr", ""]]}, {"id": "1109.5323", "submitter": "Jeremy Lee", "authors": "Jeremy Lee", "title": "Squiggle - A Glyph Recognizer for Gesture Input", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Squiggle is a template-based glyph recognizer in the lineage of `$1\nRecognizer' and `Protractor'. It seeks a good fit linear affine mapping between\nthe input and template glyphs which are represented as a list of milestone\npoints along the glyph path. The algorithm can recognize input glyphs invariant\nof rotation, scaling, skew, and reflection symmetries. In practice the\nalgorithm is fast and robust enough to recognize user-generated glyphs as they\nare being drawn in real time, and to project `shadows' of the matching\ntemplates as feedback.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2011 04:41:35 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Lee", "Jeremy", ""]]}, {"id": "1109.6288", "submitter": "Angelo Gargantini", "authors": "Angelo Gargantini", "title": "Using Stereoscopic 3D Technologies for the Diagnosis and Treatment of\n  Amblyopia in Children", "comments": "Extended version of the HEALTHINF 2011 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D4Amb project aims at developing a system based on the stereoscopic 3D\ntechonlogy, like the NVIDIA 3D Vision, for the diagnosis and treatment of\namblyopia in young children. It exploits the active shutter technology to\nprovide binocular vision, i.e. to show different images to the amblyotic (or\nlazy) and the normal eye. It would allow easy diagnosis of amblyopia and its\ntreatment by means of interactive games or other entertainment activities. It\nshould not suffer from the compliance problems of the classical treatment, it\nis suitable to domestic use, and it could at least partially substitute\nocclusion or patching of the normal eye.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 18:35:51 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Gargantini", "Angelo", ""]]}]