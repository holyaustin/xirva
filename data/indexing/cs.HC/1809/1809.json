[{"id": "1809.00172", "submitter": "Norbert B\\'atfai Ph.D.", "authors": "Norbert B\\'atfai, D\\'avid Papp, Ren\\'at\\'o Besenczi, Gerg\\H{o}\n  Bogacsovics, D\\'avid Veres", "title": "Benchmarking Cognitive Abilities of the Brain with Computer Games", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the players have experienced the feeling of temporarily losing their\ncharacter in a given gameplay situation when they cannot control the character,\nsimply because they temporarily cannot see it. The main reasons for this\nfeeling may be due to the interplay of the following factors: (1) the visual\ncomplexity of the game is unexpectedly increased compared with the previous\ntime period as more and more game objects and effects are rendered on the\ndisplay; (2) and/or the game is lagging; (3) and finally, it is also possible\nthat the players have no sufficient experience with controlling the character.\nThis paper focuses on the first reason. We have developed a benchmark program\nwhich allows its user to experience the feeling of losing character. While the\nuser can control the character well the benchmark program will increase the\nvisual complexity of the display. Otherwise, if the user lost the character\nthen the program will decrease the complexity until the user will find the\ncharacter again, and so on. The complexity is measured based on the number of\nchanged pixels between two consecutive display images. Our measurements show\nthat the average of bit per second values of losing and finding pairs describes\nthe user well. The final goal of this research is to further develop our\nbenchmark to a standard psychological test.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 13:17:03 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["B\u00e1tfai", "Norbert", ""], ["Papp", "D\u00e1vid", ""], ["Besenczi", "Ren\u00e1t\u00f3", ""], ["Bogacsovics", "Gerg\u0151", ""], ["Veres", "D\u00e1vid", ""]]}, {"id": "1809.00270", "submitter": "Vahan Yoghourdjian", "authors": "Vahan Yoghourdjian, Daniel Archambault, Stephan Diehl, Tim Dwyer,\n  Karsten Klein, Helen C. Purchase, Hsiang-Yun Wu", "title": "Exploring the Limits of Complexity: A Survey of Empirical Studies on\n  Graph Visualisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades, researchers in information visualisation and graph drawing have\nfocused on developing techniques for the layout and display of very large and\ncomplex networks. Experiments involving human participants have also explored\nthe readability of different styles of layout and representations for such\nnetworks. In both bodies of literature, networks are frequently referred to as\nbeing 'large' or 'complex', yet these terms are relative. From a human-centred,\nexperiment point-of-view, what constitutes 'large' (for example) depends on\nseveral factors, such as data complexity, visual complexity, and the technology\nused. In this paper, we survey the literature on human-centred experiments to\nunderstand how, in practice, different features and characteristics of\nnode-link diagrams affect visual complexity.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 00:03:54 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Yoghourdjian", "Vahan", ""], ["Archambault", "Daniel", ""], ["Diehl", "Stephan", ""], ["Dwyer", "Tim", ""], ["Klein", "Karsten", ""], ["Purchase", "Helen C.", ""], ["Wu", "Hsiang-Yun", ""]]}, {"id": "1809.00346", "submitter": "Saumya Kumaar Saksena", "authors": "Ravi M. Vishwanath, Saumya Kumaar and S N Omkar", "title": "A Real-time Control Approach for Unmanned Aerial Vehicles using\n  Brain-computer Interface", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interfacing (BCI) is a technology that is almost four decades\nold and it was developed solely for the purpose of developing and enhancing the\nimpact of neuroprosthetics. However, in the recent times, with the\ncommercialization of non-invasive electroencephalogram (EEG) headsets, the\ntechnology has seen a wide variety of applications like home automation,\nwheelchair control, vehicle steering etc. One of the latest developed\napplications is the mind-controlled quadrotor unmanned aerial vehicle. These\napplications, how- ever, do not require a very high-speed response and give\nsatisfactory results when standard classification methods like Support Vector\nMachine (SVM) and Multi-Layer Perceptron (MLPC). Issues are faced when there is\na requirement for high-speed control in the case of fixed-wing unmanned aerial\nvehicles where such methods are rendered unreliable due to the low speed of\nclassification. Such an application requires the system to classify data at\nhigh speeds in order to retain the con- trollability of the vehicle. This paper\nproposes a novel method of classification which uses a combination of Common\nSpatial Paradigm and Linear Discriminant Analysis that provides an improved\nclassification accuracy in real time. A non-linear SVM based classification\ntechnique has also been discussed. Further, this paper discusses the\nimplementation of the proposed method on a fixed-wing and VTOL unmanned aerial\nvehicles.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 14:29:43 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Vishwanath", "Ravi M.", ""], ["Kumaar", "Saumya", ""], ["Omkar", "S N", ""]]}, {"id": "1809.00375", "submitter": "Shourya Pratap Singh", "authors": "Shourya Pratap Singh, Ankit Kumar Panda, Susobhit Panigrahi, Ajaya\n  Kumar Dash, Debi Prosad Dogra", "title": "PlutoAR: An Inexpensive, Interactive And Portable Augmented Reality\n  Based Interpreter For K-10 Curriculum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regular K-10 curriculums often do not get the necessary of affordable\ntechnology involving interactive ways of teaching the prescribed curriculum\nwith effective analytical skill building. In this paper, we present \"PlutoAR\",\na paper-based augmented reality interpreter which is scalable, affordable,\nportable and can be used as a platform for skill building for the kids. PlutoAR\nmanages to overcome the conventional albeit non-interactive ways of teaching by\nincorporating augmented reality (AR) through an interactive toolkit to provide\nstudents with the best of both worlds. Students cut out paper \"tiles\" and place\nthese tiles one by one on a larger paper surface called \"Launchpad\" and use the\nPlutoAR mobile application which runs on any Android device with a camera and\nuses augmented reality to output each step of the program like an interpreter.\nPlutoAR has inbuilt AR experiences like stories, maze solving using conditional\nloops, simple elementary mathematics and the intuition of gravity.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 19:05:26 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 04:43:42 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Singh", "Shourya Pratap", ""], ["Panda", "Ankit Kumar", ""], ["Panigrahi", "Susobhit", ""], ["Dash", "Ajaya Kumar", ""], ["Dogra", "Debi Prosad", ""]]}, {"id": "1809.00395", "submitter": "Alborz Rezazadeh Sereshkeh", "authors": "Alborz Rezazadeh Sereshkeh, Rozhin Yousefi, Andrew T Wong, Tom Chau", "title": "Online classification of imagined speech using functional near-infrared\n  spectroscopy signals", "comments": null, "journal-ref": null, "doi": "10.1088/1741-2552/aae4b9", "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most brain-computer interfaces (BCIs) based on functional near-infrared\nspectroscopy (fNIRS) require that users perform mental tasks such as motor\nimagery, mental arithmetic, or music imagery to convey a message or to answer\nsimple yes or no questions. These cognitive tasks usually have no direct\nassociation with the communicative intent, which makes them difficult for users\nto perform. In this paper, a 3-class intuitive BCI is presented which enables\nusers to directly answer yes or no questions by covertly rehearsing the word\n'yes' or 'no' for 15 s. The BCI also admits an equivalent duration of\nunconstrained rest which constitutes the third discernable task. Twelve\nparticipants each completed one offline block and six online blocks over the\ncourse of 2 sessions. The mean value of the change in oxygenated hemoglobin\nconcentration during a trial was calculated for each channel and used to train\na regularized linear discriminant analysis (RLDA) classifier. By the final\nonline block, 9 out of 12 participants were performing above chance (p<0.001),\nwith a 3-class accuracy of 83.8+9.4%. Even when considering all participants,\nthe average online 3-class accuracy over the last 3 blocks was 64.1+20.6%, with\nonly 3 participants scoring below chance (p<0.001). For most participants,\nchannels in the left temporal and temporoparietal cortex provided the most\ndiscriminative information. To our knowledge, this is the first report of an\nonline fNIRS 3-class imagined speech BCI. Our findings suggest that imagined\nspeech can be used as a reliable activation task for selected users for the\ndevelopment of more intuitive BCIs for communication.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 21:27:19 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Sereshkeh", "Alborz Rezazadeh", ""], ["Yousefi", "Rozhin", ""], ["Wong", "Andrew T", ""], ["Chau", "Tom", ""]]}, {"id": "1809.00620", "submitter": "Arunesh Mathur", "authors": "Arunesh Mathur, Arvind Narayanan, Marshini Chetty", "title": "Endorsements on Social Media: An Empirical Study of Affiliate Marketing\n  Disclosures on YouTube and Pinterest", "comments": "26 pages, 6 figures, ACM Conference on Computer-Supported Cooperative\n  Work and Social Computing (CSCW 2018)", "journal-ref": "Proceedings of the ACM on Human- Computer Interaction, Vol. 2,\n  CSCW, Article 119 (November 2018)", "doi": "10.1145/3274388", "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online advertisements that masquerade as non-advertising content pose\nnumerous risks to users. Such hidden advertisements appear on social media\nplatforms when content creators or \"influencers\" endorse products and brands in\ntheir content. While the Federal Trade Commission (FTC) requires content\ncreators to disclose their endorsements in order to prevent deception and harm\nto users, we do not know whether and how content creators comply with the FTC's\nguidelines. In this paper, we studied disclosures within affiliate marketing,\nan endorsement-based advertising strategy used by social media content\ncreators. We examined whether content creators follow the FTC's disclosure\nguidelines, how they word the disclosures, and whether these disclosures help\nusers identify affiliate marketing content as advertisements. To do so, we\nfirst measured the prevalence of and identified the types of disclosures in\nover 500,000 YouTube videos and 2.1 million Pinterest pins. We then conducted a\nuser study with 1,791 participants to test the efficacy of these disclosures.\nOur findings reveal that only about 10% of affiliate marketing content on both\nplatforms contains any disclosures at all. Further, users fail to understand\nshorter, non-explanatory disclosures. Based on our findings, we make various\ndesign and policy suggestions to help improve advertising disclosure practices\non social media platforms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 14:57:02 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 22:31:41 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Mathur", "Arunesh", ""], ["Narayanan", "Arvind", ""], ["Chetty", "Marshini", ""]]}, {"id": "1809.00661", "submitter": "Gleb Gusev", "authors": "Roman Budylin, Alexey Drutsa, Gleb Gusev, Pavel Serdyukov, Igor\n  Yashkov", "title": "Online Evaluation for Effective Web Service Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of the majority of the leading web services and software products\ntoday is generally guided by data-driven decisions based on evaluation that\nensures a steady stream of updates, both in terms of quality and quantity.\nLarge internet companies use online evaluation on a day-to-day basis and at a\nlarge scale. The number of smaller companies using A/B testing in their\ndevelopment cycle is also growing. Web development across the board strongly\ndepends on quality of experimentation platforms. In this tutorial, we overview\nstate-of-the-art methods underlying everyday evaluation pipelines at some of\nthe leading Internet companies. Software engineers, designers, analysts,\nservice or product managers --- beginners, advanced specialists, and\nresearchers --- can learn how to make web service development data-driven and\ndo it effectively.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 17:14:47 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Budylin", "Roman", ""], ["Drutsa", "Alexey", ""], ["Gusev", "Gleb", ""], ["Serdyukov", "Pavel", ""], ["Yashkov", "Igor", ""]]}, {"id": "1809.00740", "submitter": "Maria Glenski", "authors": "Maria Glenski, Greg Stoddard, Paul Resnick, and Tim Weninger", "title": "GuessTheKarma: A Game to Assess Social Rating Systems", "comments": "15 pages, 7 figures, accepted to CSCW 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Popularity systems, like Twitter retweets, Reddit upvotes, and Pinterest pins\nhave the potential to guide people toward posts that others liked. That,\nhowever, creates a feedback loop that reduces their informativeness: items\nmarked as more popular get more attention, so that additional upvotes and\nretweets may simply reflect the increased attention and not independent\ninformation about the fraction of people that like the items. How much\ninformation remains? For example, how confident can we be that more people\nprefer item A to item B if item A had hundreds of upvotes on Reddit and item B\nhad only a few? We investigate using an Internet game called GuessTheKarma that\ncollects independent preference judgments (N=20,674) for 400 pairs of images,\napproximately 50 per pair. Unlike the rating systems that dominate social media\nservices, GuessTheKarma is devoid of social and ranking effects that influence\nratings. Overall, Reddit scores were not very good predictors of the true\npopulation preferences for items as measured by GuessTheKarma: the image with\nhigher score was preferred by a majority of independent raters only 68% of the\ntime. However, when one image had a low score and the other was one of the\nhighest scoring in its subreddit, the higher scoring image was preferred nearly\n90% of the time by the majority of independent raters. Similarly, Imgur view\ncounts for the images were poor predictors except when there were orders of\nmagnitude differences between the pairs. We conclude that popularity systems\nmarked by feedback loops may convey a strong signal about population\npreferences, but only when comparing items that received vastly different\npopularity scores.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 23:02:11 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Glenski", "Maria", ""], ["Stoddard", "Greg", ""], ["Resnick", "Paul", ""], ["Weninger", "Tim", ""]]}, {"id": "1809.00901", "submitter": "Hye Won Chung", "authors": "Hye Won Chung, Ji Oon Lee, Doyeon Kim, Alfred O. Hero", "title": "Parity Queries for Binary Classification", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.HC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a query-based data acquisition problem that aims to recover the\nvalues of $k$ binary variables from parity (XOR) measurements of chosen subsets\nof the variables. Assume the response model where only a randomly selected\nsubset of the measurements is received. We propose a method for designing a\nsequence of queries so that the variables can be identified with high\nprobability using as few ($n$) measurements as possible. We define the query\ndifficulty $\\bar{d}$ as the average size of the query subsets and the sample\ncomplexity $n$ as the minimum number of measurements required to attain a given\nrecovery accuracy. We obtain fundamental trade-offs between recovery accuracy,\nquery difficulty, and sample complexity. In particular, the necessary and\nsufficient sample complexity required for recovering all $k$ variables with\nhigh probability is $n = c_0 \\max\\{k, (k \\log k)/\\bar{d}\\}$ and the sample\ncomplexity for recovering a fixed proportion $(1-\\delta)k$ of the variables for\n$\\delta=o(1)$ is $n = c_1\\max\\{k, (k \\log(1/\\delta))/\\bar{d}\\}$, where $c_0,\nc_1>0$.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 11:43:20 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 03:03:49 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Chung", "Hye Won", ""], ["Lee", "Ji Oon", ""], ["Kim", "Doyeon", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1809.00928", "submitter": "Jirapat Likitlersuang", "authors": "Jirapat Likitlersuang, Elizabeth R. Sumitro, Tianshi Cao, Ryan J.\n  Visee, Sukhvinder Kalsi-Ryan, and Jose Zariffa", "title": "Egocentric Video: A New Tool for Capturing Hand Use of Individuals with\n  Spinal Cord Injury at Home", "comments": "13 pages, 4 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current upper extremity outcome measures for persons with cervical spinal\ncord injury (cSCI) lack the ability to directly collect quantitative\ninformation in home and community environments. A wearable first-person\n(egocentric) camera system is presented that can monitor functional hand use\noutside of clinical settings. The system is based on computer vision algorithms\nthat detect the hand, segment the hand outline, distinguish the user's left or\nright hand, and detect functional interactions of the hand with objects during\nactivities of daily living. The algorithm was evaluated using egocentric video\nrecordings from 9 participants with cSCI, obtained in a home simulation\nlaboratory. The system produces a binary hand-object interaction decision for\neach video frame, based on features reflecting motion cues of the hand, hand\nshape and colour characteristics of the scene. This output was compared with a\nmanual labelling of the video, yielding F1-scores of 0.74 $\\pm$ 0.15 for the\nleft hand and 0.73 $\\pm$ 0.15 for the right hand. From the resulting\nframe-by-frame binary data, functional hand use measures were extracted: the\namount of total interaction as a percentage of testing time, the average\nduration of interactions in seconds, and the number of interactions per hour.\nModerate and significant correlations were found when comparing these output\nmeasures to the results of the manual labelling, with $\\rho$ = 0.40, 0.54 and\n0.55 respectively. These results demonstrate the potential of a wearable\negocentric camera for capturing quantitative measures of hand use at home.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 20:31:20 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 20:02:31 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Likitlersuang", "Jirapat", ""], ["Sumitro", "Elizabeth R.", ""], ["Cao", "Tianshi", ""], ["Visee", "Ryan J.", ""], ["Kalsi-Ryan", "Sukhvinder", ""], ["Zariffa", "Jose", ""]]}, {"id": "1809.00929", "submitter": "Dongrui Wu", "authors": "Yuqi Cui and Dongrui Wu", "title": "EEG-Based Driver Drowsiness Estimation Using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, including convolutional neural networks (CNNs), has started\nfinding applications in brain-computer interfaces (BCIs). However, so far most\nsuch approaches focused on BCI classification problems. This paper extends\nEEGNet, a 3-layer CNN model for BCI classification, to BCI regression, and also\nutilizes a novel spectral meta-learner for regression (SMLR) approach to\naggregate multiple EEGNets for improved performance. Our model uses the power\nspectral density (PSD) of EEG signals as the input. Compared with raw EEG\ninputs, the PSD inputs can reduce the computational cost significantly, yet\nachieve much better regression performance. Experiments on driver drowsiness\nestimation from EEG signals demonstrate the outstanding performance of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 23:01:43 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Cui", "Yuqi", ""], ["Wu", "Dongrui", ""]]}, {"id": "1809.00947", "submitter": "Kleomenis Katevas", "authors": "Kleomenis Katevas, Katrin H\\\"ansel, Richard Clegg, Ilias Leontiadis,\n  Hamed Haddadi, Laurissa Tokarchuk", "title": "Finding Dory in the Crowd: Detecting Social Interactions using\n  Multi-Modal Mobile Sensing", "comments": "21 pages, 6 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remembering our day-to-day social interactions is challenging even if you\naren't a blue memory challenged fish. The ability to automatically detect and\nremember these types of interactions is not only beneficial for individuals\ninterested in their behavior in crowded situations, but also of interest to\nthose who analyze crowd behavior. Currently, detecting social interactions is\noften performed using a variety of methods including ethnographic studies,\ncomputer vision techniques and manual annotation-based data analysis. However,\nmobile phones offer easier means for data collection that is easy to analyze\nand can preserve the user's privacy. In this work, we present a system for\ndetecting stationary social interactions inside crowds, leveraging multi-modal\nmobile sensing data such as Bluetooth Smart (BLE), accelerometer and gyroscope.\nTo inform the development of such system, we conducted a study with 24\nparticipants, where we asked them to socialize with each other for 45 minutes.\nWe built a machine learning system based on gradient-boosted trees that\npredicts both 1:1 and group interactions with 77.8% precision and 86.5% recall,\na 30.2% performance increase compared to a proximity-based approach. By\nutilizing a community detection-based method, we further detected the various\ngroup formation that exist within the crowd. Using mobile phone sensors already\ncarried by the majority of people in a crowd makes our approach particularly\nwell suited to real-life analysis of crowd behavior and influence strategies.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 11:30:19 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 09:11:52 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Katevas", "Kleomenis", ""], ["H\u00e4nsel", "Katrin", ""], ["Clegg", "Richard", ""], ["Leontiadis", "Ilias", ""], ["Haddadi", "Hamed", ""], ["Tokarchuk", "Laurissa", ""]]}, {"id": "1809.00949", "submitter": "Idris Jeelani", "authors": "Idris Jeelani, Kevin Han and Alex Albert", "title": "Automating Analysis of Construction Workers Viewing Patterns for\n  Personalized Safety Training and Management", "comments": "ISARC 2018 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unrecognized hazards increase the likelihood of workplace fatalities and\ninjuries substantially. However, recent research has demonstrated that a large\nproportion of hazards remain unrecognized in dynamic construction environments.\nRecent studies have suggested a strong correlation between viewing patterns of\nworkers and their hazard recognition performance. Hence, it is important to\nstudy and analyze the viewing patterns of workers to gain a better\nunderstanding of their hazard recognition performance. The objective of this\nexploratory research is to explore hazard recognition as a visual search\nprocess to identifying various visual search factors that affect the process of\nhazard recognition. Further, the study also proposes a framework to develop a\nvision based tool capable of recording and analyzing viewing patterns of\nconstruction workers and generate feedback for personalized training and\nproactive safety management.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 18:15:53 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Jeelani", "Idris", ""], ["Han", "Kevin", ""], ["Albert", "Alex", ""]]}, {"id": "1809.01017", "submitter": "Tamara Mchedlidze David", "authors": "Moritz Klammler and Tamara Mchedlidze and Alexey Pak", "title": "Aesthetic Discrimination of Graph Layouts", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the following basic question: given two layouts of the\nsame graph, which one is more aesthetically pleasing? We propose a neural\nnetwork-based discriminator model trained on a labeled dataset that decides\nwhich of two layouts has a higher aesthetic quality. The feature vectors used\nas inputs to the model are based on known graph drawing quality metrics,\nclassical statistics, information-theoretical quantities, and two-point\nstatistics inspired by methods of condensed matter physics. The large corpus of\nlayout pairs used for training and testing is constructed using force-directed\ndrawing algorithms and the layouts that naturally stem from the process of\ngraph generation. It is further extended using data augmentation techniques.\nThe mean prediction accuracy of our model is 95.70%, outperforming\ndiscriminators based on stress and on the linear combination of popular quality\nmetrics by a statistically significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 14:19:43 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Klammler", "Moritz", ""], ["Mchedlidze", "Tamara", ""], ["Pak", "Alexey", ""]]}, {"id": "1809.01563", "submitter": "Jessie Taft", "authors": "Jevan Hutson, Jessie G. Taft, Solon Barocas, Karen Levy", "title": "Debiasing Desire: Addressing Bias & Discrimination on Intimate Platforms", "comments": null, "journal-ref": "Jevan Hutson, Jessie G. Taft, Solon Barocas, and Karen Levy. 2018.\n  Debiasing Desire: Addressing Bias & Discrimination on Intimate Platforms.\n  Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article 73 (November 2018), 18\n  pages", "doi": "10.1145/3274342", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Designing technical systems to be resistant to bias and discrimination\nrepresents vital new terrain for researchers, policymakers, and the\nanti-discrimination project more broadly. We consider bias and discrimination\nin the context of popular online dating and hookup platforms in the United\nStates, which we call intimate platforms. Drawing on work in\nsocial-justice-oriented and Queer HCI, we review design features of popular\nintimate platforms and their potential role in exacerbating or mitigating\ninterpersonal bias. We argue that focusing on platform design can reveal\nopportunities to reshape troubling patterns of intimate contact without\noverriding users' decisional autonomy. We identify and address the difficult\nethical questions that nevertheless come along with such intervention, while\nurging the social computing community to engage more deeply with issues of\nbias, discrimination, and exclusion in the study and design of intimate\nplatforms.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 15:02:46 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 12:59:40 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Hutson", "Jevan", ""], ["Taft", "Jessie G.", ""], ["Barocas", "Solon", ""], ["Levy", "Karen", ""]]}, {"id": "1809.01581", "submitter": "Setareh Nasihati", "authors": "Setareh Nasihati Gilani, David Traum, Arcangelo Merla, Eugenia Hee,\n  Zoey Walker, Barbara Manini, Grady Gallagher, Laura-Ann Petitto", "title": "Multimodal Dialogue Management for Multiparty Interaction with Infants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present dialogue management routines for a system to engage in multiparty\nagent-infant interaction. The ultimate purpose of this research is to help\ninfants learn a visual sign language by engaging them in naturalistic and\nsocially contingent conversations during an early-life critical period for\nlanguage development (ages 6 to 12 months) as initiated by an artificial agent.\nAs a first step, we focus on creating and maintaining agent-infant engagement\nthat elicits appropriate and socially contingent responses from the baby. Our\nsystem includes two agents, a physical robot and an animated virtual human. The\nsystem's multimodal perception includes an eye-tracker (measures attention) and\na thermal infrared imaging camera (measures patterns of emotional arousal). A\ndialogue policy is presented that selects individual actions and planned\nmultiparty sequences based on perceptual inputs about the baby's internal\nchanging states of emotional engagement. The present version of the system was\nevaluated in interaction with 8 babies. All babies demonstrated spontaneous and\nsustained engagement with the agents for several minutes, with patterns of\nconversationally relevant and socially contingent behaviors. We further\nperformed a detailed case-study analysis with annotation of all agent and baby\nbehaviors. Results show that the baby's behaviors were generally relevant to\nagent conversations and contained direct evidence for socially contingent\nresponses by the baby to specific linguistic samples produced by the avatar.\nThis work demonstrates the potential for language learning from agents in very\nyoung babies and has especially broad implications regarding the use of\nartificial agents with babies who have minimal language exposure in early life.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 15:36:53 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Gilani", "Setareh Nasihati", ""], ["Traum", "David", ""], ["Merla", "Arcangelo", ""], ["Hee", "Eugenia", ""], ["Walker", "Zoey", ""], ["Manini", "Barbara", ""], ["Gallagher", "Grady", ""], ["Petitto", "Laura-Ann", ""]]}, {"id": "1809.01587", "submitter": "Minsuk Kahng", "authors": "Minsuk Kahng, Nikhil Thorat, Duen Horng Chau, Fernanda Vi\\'egas,\n  Martin Wattenberg", "title": "GAN Lab: Understanding Complex Deep Generative Models using Interactive\n  Visual Experimentation", "comments": "This paper will be published in the IEEE Transactions on\n  Visualization and Computer Graphics, 25(1), January 2019, and presented at\n  IEEE VAST 2018", "journal-ref": null, "doi": "10.1109/TVCG.2018.2864500", "report-no": null, "categories": "cs.HC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent success in deep learning has generated immense interest among\npractitioners and students, inspiring many to learn about this new technology.\nWhile visual and interactive approaches have been successfully developed to\nhelp people more easily learn deep learning, most existing tools focus on\nsimpler models. In this work, we present GAN Lab, the first interactive\nvisualization tool designed for non-experts to learn and experiment with\nGenerative Adversarial Networks (GANs), a popular class of complex deep\nlearning models. With GAN Lab, users can interactively train generative models\nand visualize the dynamic training process's intermediate results. GAN Lab\ntightly integrates an model overview graph that summarizes GAN's structure, and\na layered distributions view that helps users interpret the interplay between\nsubmodels. GAN Lab introduces new interactive experimentation features for\nlearning complex deep learning models, such as step-by-step training at\nmultiple levels of abstraction for understanding intricate training dynamics.\nImplemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web\nbrowsers, without the need for installation or specialized hardware, overcoming\na major practical challenge in deploying interactive tools for deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 15:51:50 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Kahng", "Minsuk", ""], ["Thorat", "Nikhil", ""], ["Chau", "Duen Horng", ""], ["Vi\u00e9gas", "Fernanda", ""], ["Wattenberg", "Martin", ""]]}, {"id": "1809.01807", "submitter": "Kory W Mathewson", "authors": "Kory W. Mathewson and Piotr Mirowski", "title": "Improbotics: Exploring the Imitation Game using Machine Intelligence in\n  Improvised Theatre", "comments": "8 pages, 6 figures, AAAI Publications, 2018 Artificial Intelligence\n  and Interactive Digital Entertainment Conference (AIIDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theatrical improvisation (impro or improv) is a demanding form of live,\ncollaborative performance. Improv is a humorous and playful artform built on an\nopen-ended narrative structure which simultaneously celebrates effort and\nfailure. It is thus an ideal test bed for the development and deployment of\ninteractive artificial intelligence (AI)-based conversational agents, or\nartificial improvisors. This case study introduces an improv show experiment\nfeaturing human actors and artificial improvisors. We have previously developed\na deep-learning-based artificial improvisor, trained on movie subtitles, that\ncan generate plausible, context-based, lines of dialogue suitable for theatre\n(Mathewson and Mirowski 2017). In this work, we have employed it to control\nwhat a subset of human actors say during an improv performance. We also give\nhuman-generated lines to a different subset of performers. All lines are\nprovided to actors with headphones and all performers are wearing headphones.\nThis paper describes a Turing test, or imitation game, taking place in a\ntheatre, with both the audience members and the performers left to guess who is\na human and who is a machine. In order to test scientific hypotheses about the\nperception of humans versus machines we collect anonymous feedback from\nvolunteer performers and audience members. Our results suggest that rehearsal\nincreases proficiency and possibility to control events in the performance.\nThat said, consistency with real world experience is limited by the interface\nand the mechanisms used to perform the show. We also show that human-generated\nlines are shorter, more positive, and have less difficult words with more\ngrammar and spelling mistakes than the artificial improvisor generated lines.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 03:46:01 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Mathewson", "Kory W.", ""], ["Mirowski", "Piotr", ""]]}, {"id": "1809.01952", "submitter": "Biswarup Mukherjee", "authors": "Nima Akhlaghi, Ananya Dhawan, Amir A. Khan, Biswarup Mukherjee, Cecile\n  Truong and Siddhartha Sikdar", "title": "Sparsity Analysis of a Sonomyographic Muscle-Computer Interface", "comments": null, "journal-ref": null, "doi": "10.1109/TBME.2019.2919488", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The objectives of this paper are to determine the optimal location\nfor ultrasound transducer placement on the anterior forearm for imaging maximum\nmuscle deformations during different hand motions and to investigate the effect\nof using a sparse set of ultrasound scanlines for motion classification for\nultrasound-based muscle computer interfaces (MCIs). Methods: The optimal\nplacement of the ultrasound transducer along the forearm is identified using\nfreehand 3D reconstructions of the muscle thickness during rest and motion\ncompletion. From the ultrasound images acquired from the optimally placed\ntransducer, we determine classification accuracy with equally spaced scanlines\nacross the cross-sectional field-of-view (FOV). Furthermore, we investigated\nthe unique contribution of each scanline to class discrimination using Fisher\ncriteria (FC) and mutual information (MI) with respect to motion\ndiscriminability. Results: Experiments with 5 able-bodied subjects show that\nthe maximum muscle deformation occurred between 30-50% of the forearm length\nfor multiple degrees-of-freedom. The average classification accuracy was 94.6%\nwith the entire 128 scanline image and 94.5% with 4 equally-spaced scanlines.\nHowever, no significant improvement in classification accuracy was observed\nwith optimal scanline selection using FC and MI. Conclusion: For an optimally\nplaced transducer, a small subset of ultrasound scanlines can be used instead\nof a full imaging array without sacrificing performance in terms of\nclassification accuracy for multiple degrees-of-freedom. Significance: The\nselection of a small subset of transducer elements can enable the reduction of\ncomputation, and simplification of the instrumentation and power consumption of\nwearable sonomyographic MCIs particularly for rehabilitation and gesture\nrecognition applications.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 12:41:05 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Akhlaghi", "Nima", ""], ["Dhawan", "Ananya", ""], ["Khan", "Amir A.", ""], ["Mukherjee", "Biswarup", ""], ["Truong", "Cecile", ""], ["Sikdar", "Siddhartha", ""]]}, {"id": "1809.02869", "submitter": "Pedram Daee", "authors": "Tomi Peltola, Mustafa Mert \\c{C}elikok, Pedram Daee, Samuel Kaski", "title": "Machine Teaching of Active Sequential Learners", "comments": "24 pages, 16 figures. This version focuses more on machine teaching\n  while the previous version focused more on human-computer interaction and\n  user modelling. The title has been updated accordingly. Code and data\n  available at\n  https://github.com/AaltoPML/machine-teaching-of-active-sequential-learners .\n  NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine teaching addresses the problem of finding the best training data that\ncan guide a learning algorithm to a target model with minimal effort. In\nconventional settings, a teacher provides data that are consistent with the\ntrue data distribution. However, for sequential learners which actively choose\ntheir queries, such as multi-armed bandits and active learners, the teacher can\nonly provide responses to the learner's queries, not design the full data. In\nthis setting, consistent teachers can be sub-optimal for finite horizons. We\nformulate this sequential teaching problem, which current techniques in machine\nteaching do not address, as a Markov decision process, with the dynamics\nnesting a model of the learner and the actions being the teacher's responses.\nFurthermore, we address the complementary problem of learning from a teacher\nthat plans: to recognise the teaching intent of the responses, the learner is\nendowed with a model of the teacher. We test the formulation with multi-armed\nbandit learners in simulated experiments and a user study. The results show\nthat learning is improved by (i) planning teaching and (ii) the learner having\na model of the teacher. The approach gives tools to taking into account\nstrategic (planning) behaviour of users of interactive intelligent systems,\nsuch as recommendation engines, by considering them as boundedly optimal\nteachers.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 20:39:31 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 10:40:50 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 06:30:11 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Peltola", "Tomi", ""], ["\u00c7elikok", "Mustafa Mert", ""], ["Daee", "Pedram", ""], ["Kaski", "Samuel", ""]]}, {"id": "1809.03398", "submitter": "Sebastian Baltes", "authors": "Natalie Stors and Sebastian Baltes", "title": "Constructing Urban Tourism Space Digitally: A Study of Airbnb Listings\n  in Two Berlin Neighborhoods", "comments": "29 pages, 4 figures, Proceedings of the ACM on Human-Computer\n  Interaction, Vol. 2, Issue CSCW, Article 166 (PACMHCI/CSCW 2018), ACM, 2018", "journal-ref": null, "doi": "10.1145/3274435", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, Airbnb has emerged as the most popular platform for\nrenting out single rooms or whole apartments. The impact of Airbnb listings on\nlocal neighborhoods has been controversially discussed in many cities around\nthe world. The platform's widespread adoption led to changes in urban life, and\nin particular urban tourism. We argue that urban tourism space can no longer be\nunderstood as a fixed, spatial entity. Instead, we follow a constructionist\napproach and argue that urban tourism space is (re-)produced digitally and\ncollaboratively on online platforms such as Airbnb. We relate our work to a\nCSCW research direction that is concerned with the role of digital technologies\nin the production and appropriation of urban space and use the concept of\nrepresentations as a theoretical lens for our empirical study. In that study,\nwe qualitatively analyzed how the two Berlin neighborhoods Kreuzk\\\"olln and\nCity West are digitally constructed by Airbnb hosts in their listing\ndescriptions. Moreover, we quantitatively investigated to what extend mentioned\nplaces differ between Airbnb hosts and visitBerlin, the city's destination\nmanagement organization (DMO). In our qualitative analysis, we found that hosts\nprimarily focus on facilities and places in close proximity to their apartment.\nIn the traditional urban tourism hotspot City West, hosts referred to many\nplaces also mentioned by the DMO. In the neighborhood of Kreuzk\\\"olln, in\ncontrast, hosts reframed everyday places such as parks or an immigrant food\nmarket as the must sees in the area. We discuss how Airbnb hosts contribute to\nthe discursive production of urban neighborhoods and thus co-produce them as\ntourist destinations. With the emergence of online platforms such as Airbnb,\npower relations in the construction of tourism space might shift from DMOs\ntowards local residents who are now producing tourism space collaboratively.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 11:28:38 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Stors", "Natalie", ""], ["Baltes", "Sebastian", ""]]}, {"id": "1809.03539", "submitter": "Maarten Wijntjes", "authors": "Maarten W.A. Wijntjes", "title": "Annotating shadows, highlights and faces: the contribution of a 'human\n  in the loop' for digital art history", "comments": "Presented at the \"1st KDD Workshop on Data Science for Digital Art\n  History: tackling big data Challenges, Algorithms, and Systems\", see\n  http://dsdah2018.blogs.dsv.su.se for more info. Manuscript should eventually\n  be published in Journal of Digital Art History (www.dah-journal.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While automatic computational techniques appear to reveal novel insights in\ndigital art history, a complementary approach seems to get less attention: that\nof human annotation. We argue and exemplify that a 'human in the loop' can\nreveal insights that may be difficult to detect automatically. Specifically, we\nfocussed on perceptual aspects within pictorial art. Using rather simple\nannotation tasks (e.g. delineate human lengths, indicate highlights and\nclassify gaze direction) we could both replicate earlier findings and reveal\nnovel insights into pictorial conventions. We found that Canaletto depicted\nhuman figures in rather accurate perspective, varied viewpoint elevation\nbetween approximately 3 and 9 meters and highly preferred light directions\nparallel to the projection plane. Furthermore, we found that taking the\naveraged images of leftward looking faces reveals a woman, and for rightward\nlooking faces showed a male, confirming earlier accounts on lateral gender bias\nin pictorial art. Lastly, we confirmed and refined the well-known\nlight-from-the-left bias. Together, the annotations, analyses and results\nexemplify how human annotation can contribute and complement to technical and\ndigital art history.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 18:34:22 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Wijntjes", "Maarten W. A.", ""]]}, {"id": "1809.03650", "submitter": "Seong-Eun Moon", "authors": "Seong-Eun Moon, Soobeom Jang, Jong-Seok Lee", "title": "Evaluation of Preference of Multimedia Content using Deep Neural\n  Networks for Electroencephalography", "comments": "Accepted for the 10th International Conference on Quality of\n  Multimedia Experience (QoMEX 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of quality of experience (QoE) based on electroencephalography\n(EEG) has received great attention due to its capability of real-time QoE\nmonitoring of users. However, it still suffers from rather low recognition\naccuracy. In this paper, we propose a novel method using deep neural networks\ntoward improved modeling of EEG and thereby improved recognition accuracy. In\nparticular, we aim to model spatio-temporal characteristics relevant for QoE\nanalysis within learning models. The results demonstrate the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 01:51:24 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 01:14:00 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Moon", "Seong-Eun", ""], ["Jang", "Soobeom", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1809.03920", "submitter": "Benjamin Finley", "authors": "Benjamin Finley, Tapio Soikkeli", "title": "Multidevice mobile sessions: A first look", "comments": "Accepted Manuscript", "journal-ref": "B. Finley, T. Soikkeli, Multidevice mobile sessions: A first look,\n  Pervasive and Mobile Computing, Volume 39, 2017, Pages 267-283", "doi": "10.1016/j.pmcj.2016.11.001", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing number of users with multiple mobile devices underscores the\nimportance of understanding how users interact, often simultaneously, with\nthese multiple devices. However, most device based monitoring studies have\nfocused only on a single device type. In contrast, we study the multidevice\nusage of a US-based panel through device based monitoring on panelist's\nsmartphone and tablet devices. We present a broad range of results from\ncharacterizing individual multidevice sessions to estimating device usage\nsubstitution. For example, we find that for panelists, 50% of all device\ninteraction time can be considered multidevice usage.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 14:22:25 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Finley", "Benjamin", ""], ["Soikkeli", "Tapio", ""]]}, {"id": "1809.04103", "submitter": "Jack Murtagh", "authors": "Jack Murtagh, Kathryn Taylor, George Kellaris, Salil Vadhan", "title": "Usable Differential Privacy: A Case Study with PSI", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a promising framework for addressing the privacy\nconcerns in sharing sensitive datasets for others to analyze. However\ndifferential privacy is a highly technical area and current deployments often\nrequire experts to write code, tune parameters, and optimize the trade-off\nbetween the privacy and accuracy of statistical releases. For differential\nprivacy to achieve its potential for wide impact, it is important to design\nusable systems that enable differential privacy to be used by ordinary data\nowners and analysts. PSI is a tool that was designed for this purpose, allowing\nresearchers to release useful differentially private statistical information\nabout their datasets without being experts in computer science, statistics, or\nprivacy. We conducted a thorough usability study of PSI to test whether it\naccomplishes its goal of usability by non-experts. The usability test\nilluminated which features of PSI are most user-friendly and prompted us to\nimprove aspects of the tool that caused confusion. The test also highlighted\nsome general principles and lessons for designing usable systems for\ndifferential privacy, which we discuss in depth.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 18:42:41 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Murtagh", "Jack", ""], ["Taylor", "Kathryn", ""], ["Kellaris", "George", ""], ["Vadhan", "Salil", ""]]}, {"id": "1809.04177", "submitter": "Yohan Jo", "authors": "Yohan Jo, Keith Maki, Gaurav Tomar", "title": "Time Series Analysis of Clickstream Logs from Online Courses", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rapidly rising popularity of Massive Open Online Courses (MOOCs),\nthere is a growing demand for scalable automated support technologies for\nstudent learning. Transferring traditional educational resources to online\ncontexts has become an increasingly relevant problem in recent years. For\nlearning science theories to be applicable, educators need a way to identify\nlearning behaviors of students which contribute to learning outcomes, and use\nthem to design and provide personalized intervention support to the students.\nClick logs are an important source of information about students' learning\nbehaviors, however current literature has limited understanding of how these\nbehaviors are represented within click logs. In this project, we have exploited\nthe temporal dynamics of student behaviors both to do behavior modeling via\ngraphical modeling approaches and to do performance prediction via recurrent\nneural network approaches in order to first identify student behaviors and then\nuse them to predict their final outcome in the course. Our experiments showed\nthat the long short-term memory (LSTM) model is capable of learning long-term\ndependencies in a sequence and outperforms other strong baselines in the\nprediction task. Further, these sequential approaches to click log analysis can\nbe successfully imported to other courses when used with results obtained from\ngraphical model behavior modeling.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 21:31:06 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Jo", "Yohan", ""], ["Maki", "Keith", ""], ["Tomar", "Gaurav", ""]]}, {"id": "1809.04208", "submitter": "Seong-Eun Moon", "authors": "Seong-Eun Moon, Soobeom Jang, Jong-Seok Lee", "title": "Convolutional Neural Network Approach for EEG-based Emotion Recognition\n  using Brain Connectivity and its Spatial Information", "comments": "Accepted for the 2018 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition based on electroencephalography (EEG) has received\nattention as a way to implement human-centric services. However, there is still\nmuch room for improvement, particularly in terms of the recognition accuracy.\nIn this paper, we propose a novel deep learning approach using convolutional\nneural networks (CNNs) for EEG-based emotion recognition. In particular, we\nemploy brain connectivity features that have not been used with deep learning\nmodels in previous studies, which can account for synchronous activations of\ndifferent brain regions. In addition, we develop a method to effectively\ncapture asymmetric brain activity patterns that are important for emotion\nrecognition. Experimental results confirm the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 00:56:32 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Moon", "Seong-Eun", ""], ["Jang", "Soobeom", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1809.04254", "submitter": "Seong-Eun Moon", "authors": "Seong-Eun Moon, Jong-Seok Lee", "title": "Implicit Analysis of Perceptual Multimedia Experience Based on\n  Physiological Response: A Review", "comments": "Published in IEEE Transactions on Multimedia", "journal-ref": "S.-E. Moon and J.-S. Lee, \"Implicit Analysis of Perceptual\n  Multimedia Experience Based on Physiological Response: A Review,\" in IEEE\n  Transactions on Multimedia, vol. 19, no. 2, pp. 340-353, Feb. 2017", "doi": "10.1109/TMM.2016.2614880", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential growth of popularity of multimedia has led to needs for\nuser-centric adaptive applications that manage multimedia content more\neffectively. Implicit analysis, which examines users' perceptual experience of\nmultimedia by monitoring physiological or behavioral cues, has potential to\nsatisfy such demands. Particularly, physiological signals categorized into\ncerebral physiological signals (electroencephalography, functional magnetic\nresonance imaging, and functional near-infrared spectroscopy) and peripheral\nphysiological signals (heart rate, respiration, skin temperature, etc.) have\nrecently received attention along with notable development of wearable\nphysiological sensors. In this paper, we review existing studies on\nphysiological signal analysis exploring perceptual experience of multimedia.\nFurthermore, we discuss current trends and challenges.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 04:48:56 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Moon", "Seong-Eun", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1809.04359", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias, Stefanos Zafeiriou", "title": "Training Deep Neural Networks with Different Datasets In-the-wild: The\n  Emotion Recognition Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel procedure is presented in this paper, for training a deep\nconvolutional and recurrent neural network, taking into account both the\navailable training data set and some information extracted from similar\nnetworks trained with other relevant data sets. This information is included in\nan extended loss function used for the network training, so that the network\ncan have an improved performance when applied to the other data sets, without\nforgetting the learned knowledge from the original data set. Facial expression\nand emotion recognition in-the-wild is the test bed application that is used to\ndemonstrate the improved performance achieved using the proposed approach. In\nthis framework, we provide an experimental study on categorical emotion\nrecognition using datasets from a very recent related emotion recognition\nchallenge.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 11:16:31 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1809.04507", "submitter": "Viral Parekh", "authors": "Viral Parekh, Maneesh Bilalpur, Sharavan Kumar, Stefan Winkler, C V\n  Jawahar, Ramanathan Subramanian", "title": "Investigating the generalizability of EEG-based Cognitive Load\n  Estimation Across Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine if EEG-based cognitive load (CL) estimation is generalizable\nacross the character, spatial pattern, bar graph and pie chart-based\nvisualizations for the nback~task. CL is estimated via two recent approaches:\n(a) Deep convolutional neural network, and (b) Proximal support vector\nmachines. Experiments reveal that CL estimation suffers across visualizations\nmotivating the need for effective machine learning techniques to benchmark\nvisual interface usability for a given analytic task.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 15:13:44 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Parekh", "Viral", ""], ["Bilalpur", "Maneesh", ""], ["Kumar", "Sharavan", ""], ["Winkler", "Stefan", ""], ["Jawahar", "C V", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1809.04549", "submitter": "Hojin Lee", "authors": "Hojin Lee, Hyoungkyun Kim, Seungmoon Choi", "title": "Driving Skill Modeling Using Neural Networks for Performance-based\n  Haptic Assistance", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": "10.1109/THMS.2021.3061409", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a data-driven framework, modeling expert driving skills\nfor performance-based haptic assistance using neural networks (NNs). We have\nbuilt a haptic driving training simulator to collect expert driving data and to\nprovide proper haptic feedback. We establish an expert driving skill model by\ntraining NNs with the collected data. Then, the skill model is applied to\nperformance-based haptic assistance to provide optimized references of the\nsteering/pedaling movements. We evaluate the skill model and its application to\nperformance-based haptic assistance in two user experiments. The results of the\nfirst experiment demonstrate that our skill model has appropriately captured\nexperts' steering/pedaling skills. The results of the second experiment show\nthat our performance-based haptic assistance can help novice drivers perform\nsteering as expert drivers, but cannot assist their pedaling performance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 16:39:30 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 23:43:34 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 11:50:40 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lee", "Hojin", ""], ["Kim", "Hyoungkyun", ""], ["Choi", "Seungmoon", ""]]}, {"id": "1809.04777", "submitter": "Seong-Eun Moon", "authors": "Seong-Eun Moon, Jong-Seok Lee", "title": "Perceptual Experience Analysis for Tone-mapped HDR Videos based on EEG\n  and Peripheral Physiological Signals", "comments": "Published in IEEE Transactions on Autonomous Mental Development", "journal-ref": "S.-E. Moon and J.-S. Lee, \"Perceptual Experience Analysis for\n  Tone-mapped HDR Videos Based on EEG and Peripheral Physiological Signals,\" in\n  IEEE Transactions on Autonomous Mental Development, vol. 7, no. 3, pp.\n  236-247, Sept. 2015", "doi": "10.1109/TAMD.2015.2449553", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dynamic range (HDR) imaging has been attracting much attention as a\ntechnology that can provide immersive experience. Its ultimate goal is to\nprovide better quality of experience (QoE) via enhanced contrast. In this\npaper, we analyze perceptual experience of tone-mapped HDR videos both\nexplicitly by conducting a subjective questionnaire assessment and implicitly\nby using EEG and peripheral physiological signals. From the results of the\nsubjective assessment, it is revealed that tone-mapped HDR videos are more\ninteresting and more natural, and give better quality than low dynamic range\n(LDR) videos. Physiological signals were recorded during watching tone-mapped\nHDR and LDR videos, and classification systems are constructed to explore\nperceptual difference captured by the physiological signals. Significant\ndifference in the physiological signals is observed between tone-mapped HDR and\nLDR videos in the classification under both a subject-dependent and a\nsubject-independent scenarios. Also, significant difference in the signals\nbetween high versus low perceived contrast and overall quality is detected via\nclassification under the subject-dependent scenario. Moreover, it is shown that\nfeatures extracted from the gamma frequency band are effective for\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 05:08:36 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Moon", "Seong-Eun", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1809.04794", "submitter": "Jelena Mladenovic", "authors": "Jelena Mladenovi\\'c (Potioc, CRNL, LaBRI)", "title": "Considering Gut Biofeedback for Emotion Regulation", "comments": null, "journal-ref": "UERMMI - UbiComp/ISWC'18 Adjunct, Oct 2018, Sinapore, Singapore.\n  http://ubicomp.org/ubicomp2018/", "doi": "10.1145/3267305.3267686", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in the enteric nervous system, sometimes called the second\nbrain, has revealed potential of the digestive system in predicting emotion.\nEven though people regularly experience changes in their gastrointestinal (GI)\ntract which influence their mood and behavior multiple times per day, robust\nmeasurements and wearable devices are not quite developed for such phenomena.\nHowever, other manifestations of the autonomic nervous system such as\nelectrodermal activity, heart rate, and facial muscle movement have been\nextensively used as measures of emotions or in biofeedback applications, while\nneglecting the gut. We expose electrogastrography (EGG), i.e., recordings of\nthe myoelectric activity of the GI tract, as a possible measure for inferring\nhuman emotions. In this paper, we also wish to bring into light some\nfundamental questions about emotions, which are often taken for granted in the\nfield of Human Computer Interaction, but are still a great debate in the fields\nof cognitive neuroscience and psychology.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 06:34:05 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Mladenovi\u0107", "Jelena", "", "Potioc, CRNL, LaBRI"]]}, {"id": "1809.04857", "submitter": "Ovidiu Banias", "authors": "Camil Octavian Milincu, Otilia Alexandra Tudoran, Paul Florin Tarce,\n  Ovidiu Banias", "title": "Hybrid design tools - making of a digitally augmented blackboard", "comments": null, "journal-ref": "5th International Multidisciplinary Scientific Conference on\n  Social Sciences and Arts SGEM 2018", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way that design is being taught is continuously changing under the\npressure of the transition from analogical to digital environments. This\nbecomes even more important as the novelty and the alleged superiority of the\ndigital world is used as a marketing tool by competing universities. Even\nthough in some fields of application this approach is desirable, some\nparticular aspects of teaching design and architecture make this transition\ndebatable. The advantages of drawing on blackboards over drawing on whiteboard\nsurfaces in regards of line aesthetic and expression possibilities were\npreviously identified, along with the complementary necessary features for\nimprovement. This study showcases a proof of concept in digitally augmenting a\nblackboard surface. The system allows the capturing, processing and making real\ntime projections of images over the blackboard surface as trace references.\nSuch a hybrid system, along with providing support for design and architecture\nrelated presentations and discussions could also mediate the contradictory\nrelation towards technology that students and teachers have.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 09:40:49 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Milincu", "Camil Octavian", ""], ["Tudoran", "Otilia Alexandra", ""], ["Tarce", "Paul Florin", ""], ["Banias", "Ovidiu", ""]]}, {"id": "1809.04931", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency", "title": "Multimodal Local-Global Ranking Fusion for Emotion Recognition", "comments": "ACM International Conference on Multimodal Interaction (ICMI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition is a core research area at the intersection of artificial\nintelligence and human communication analysis. It is a significant technical\nchallenge since humans display their emotions through complex idiosyncratic\ncombinations of the language, visual and acoustic modalities. In contrast to\ntraditional multimodal fusion techniques, we approach emotion recognition from\nboth direct person-independent and relative person-dependent perspectives. The\ndirect person-independent perspective follows the conventional emotion\nrecognition approach which directly infers absolute emotion labels from\nobserved multimodal features. The relative person-dependent perspective\napproaches emotion recognition in a relative manner by comparing partial video\nsegments to determine if there was an increase or decrease in emotional\nintensity. Our proposed model integrates these direct and relative prediction\nperspectives by dividing the emotion recognition task into three easier\nsubtasks. The first subtask involves a multimodal local ranking of relative\nemotion intensities between two short segments of a video. The second subtask\nuses local rankings to infer global relative emotion ranks with a Bayesian\nranking algorithm. The third subtask incorporates both direct predictions from\nobserved multimodal behaviors and relative emotion ranks from local-global\nrankings for final emotion prediction. Our approach displays excellent\nperformance on an audio-visual emotion recognition benchmark and improves over\nother algorithms for multimodal fusion.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 09:44:01 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Liang", "Paul Pu", ""], ["Zadeh", "Amir", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1809.04945", "submitter": "Ingmar Steiner", "authors": "Eran Raveh, Ingmar Steiner, Iona Gessinger, Bernd M\\\"obius", "title": "Studying Mutual Phonetic Influence with a Web-Based Spoken Dialogue\n  System", "comments": "Proc. 20th International Conference on Speech and Computer (SPECOM)", "journal-ref": null, "doi": "10.1007/978-3-319-99579-3_57", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study on mutual speech variation influences in a\nhuman-computer setting. The study highlights behavioral patterns in data\ncollected as part of a shadowing experiment, and is performed using a novel\nend-to-end platform for studying phonetic variation in dialogue. It includes a\nspoken dialogue system capable of detecting and tracking the state of phonetic\nfeatures in the user's speech and adapting accordingly. It provides visual and\nnumeric representations of the changes in real time, offering a high degree of\ncustomization, and can be used for simulating or reproducing speech variation\nscenarios. The replicated experiment presented in this paper along with the\nanalysis of the relationship between the human and non-human interlocutors lays\nthe groundwork for a spoken dialogue system with personalized speaking style,\nwhich we expect will improve the naturalness and efficiency of human-computer\ninteraction.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 13:35:37 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Raveh", "Eran", ""], ["Steiner", "Ingmar", ""], ["Gessinger", "Iona", ""], ["M\u00f6bius", "Bernd", ""]]}, {"id": "1809.05352", "submitter": "Ahmed Arif", "authors": "Monwen Shen, Gulnar Rakhmetulla, Ahmed Sabbir Arif", "title": "Put a Ring on It: Text Entry Performance on a Grip Ring Attached\n  Smartphone", "comments": null, "journal-ref": "MobileHCI 2018 Workshop on Socio-Technical Aspects of Text Entry\n  (September 3, 2018). Barcelona, Spain, CEUR-WS.org/Vol-2183, 6-10", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents results of a study investing effects of grip rings on\ntext entry. Results revealed that grip rings do not affect text entry\nperformance in terms of speed, accuracy, or keystrokes per character. It then\nreflects on future research directions based on the results and observations\nfrom the study. The purpose of this work is to stress the necessity of\nclassifying and evaluating low-cost mobile phone accessories.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 11:05:21 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Shen", "Monwen", ""], ["Rakhmetulla", "Gulnar", ""], ["Arif", "Ahmed Sabbir", ""]]}, {"id": "1809.05369", "submitter": "Chris Norval", "authors": "Chris Norval, Jennifer Cobbe, Heleen Janssen, Jatinder Singh", "title": "Reclaiming Data: Overcoming app identification barriers for exercising\n  data protection rights", "comments": "Author preprint (accepted 20-Aug-18) To appear in the proceedings of\n  the 4th Workshop on Legal and Technical Issues in Cloud and Pervasive\n  Computing (IoT) [CLaw-18], UbiComp/ISWC'18 Adjunct,\n  https://doi.org/10.1145/3267305.3274153", "journal-ref": null, "doi": "10.1145/3267305.3274153", "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data protection regulations generally afford individuals certain rights over\ntheir personal data, including the rights to access, rectify, and delete the\ndata held on them. Exercising such rights naturally requires those with data\nmanagement obligations (service providers) to be able to match an individual\nwith their data. However, many mobile apps collect personal data, without\nrequiring user registration or collecting details of a user's identity (email\naddress, names, phone number, and so forth). As a result, a user's ability to\nexercise their rights will be hindered without means for an individual to link\nthemselves with this 'nameless' data. Current approaches often involve those\nseeking to exercise their legal rights having to give the app's provider more\npersonal information, or even to register for a service; both of which seem\ncontrary to the spirit of data protection law. This paper explores these\nconcerns, and indicates simple means for facilitating data subject rights\nthrough both application and mobile platform (OS) design.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 12:05:05 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Norval", "Chris", ""], ["Cobbe", "Jennifer", ""], ["Janssen", "Heleen", ""], ["Singh", "Jatinder", ""]]}, {"id": "1809.05491", "submitter": "Xiuming Zhang", "authors": "Xiuming Zhang, Tali Dekel, Tianfan Xue, Andrew Owens, Qiurui He,\n  Jiajun Wu, Stefanie Mueller, William T. Freeman", "title": "MoSculp: Interactive Visualization of Shape and Time", "comments": "UIST 2018. Project page: http://mosculp.csail.mit.edu/", "journal-ref": null, "doi": "10.1145/3242587.3242592", "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that allows users to visualize complex human motion via\n3D motion sculptures---a representation that conveys the 3D structure swept by\na human body as it moves through space. Given an input video, our system\ncomputes the motion sculptures and provides a user interface for rendering it\nin different styles, including the options to insert the sculpture back into\nthe original video, render it in a synthetic scene or physically print it.\n  To provide this end-to-end workflow, we introduce an algorithm that estimates\nthat human's 3D geometry over time from a set of 2D images and develop a\n3D-aware image-based rendering approach that embeds the sculpture back into the\nscene. By automating the process, our system takes motion sculpture creation\nout of the realm of professional artists, and makes it applicable to a wide\nrange of existing video material.\n  By providing viewers with 3D information, motion sculptures reveal space-time\nmotion information that is difficult to perceive with the naked eye, and allow\nviewers to interpret how different parts of the object interact over time. We\nvalidate the effectiveness of this approach with user studies, finding that our\nmotion sculpture visualizations are significantly more informative about motion\nthan existing stroboscopic and space-time visualization methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 16:27:08 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 17:56:47 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Zhang", "Xiuming", ""], ["Dekel", "Tali", ""], ["Xue", "Tianfan", ""], ["Owens", "Andrew", ""], ["He", "Qiurui", ""], ["Wu", "Jiajun", ""], ["Mueller", "Stefanie", ""], ["Freeman", "William T.", ""]]}, {"id": "1809.05500", "submitter": "Marco Cavallo", "authors": "Marco Cavallo, Angus G. Forbes", "title": "CAVE-AR: A VR Authoring System to Interactively Design, Simulate, and\n  Debug Multi-user AR Experiences", "comments": "UIC EVL Laboratory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite advances in augmented reality (AR), the process of creating\nmeaningful experiences with this technology is still extremely challenging. Due\nto different tracking implementations and hardware constraints, developing AR\napplications either requires low-level programming skills, or is done through\nspecific authoring tools that largely sacrifice the possibility of customizing\nthe AR experience. Existing development workflows also do not support\npreviewing or simulating the AR experience, requiring a lengthy process of\ntrial and error by which content creators deploy and physically test\napplications in each iteration. To mitigate these limitations, we propose\nCAVE-AR, a novel virtual reality system for authoring, simulating and debugging\ncustom AR experiences. Available both as a standalone or a plug-in tool,\nCAVE-AR is based on the concept of representing in the same global reference\nsystem both in AR content and tracking information, mixing geographical\ninformation, architectural features, and sensor data to simulate the context of\nan AR experience. Thanks to its novel abstraction of existing tracking\ntechnologies, CAVE-AR operates independently of users' devices, and integrates\nwith existing programming tools to provide maximum flexibility. Our VR\napplication provides designers with ways to create and modify an AR\napplication, even while others are in the midst of using it. CAVE-AR further\nallows the designer to track how users are behaving, preview what they are\ncurrently seeing, and interact with them through several different channels. To\nillustrate our proposed development workflow and demonstrate the advantages of\nour authoring system, we introduce two CAVEAR use cases in which an augmented\nreality application is created and tested. We compare the CAVE-AR workflow to\ntraditional development methods and demonstrate the importance of simulation\nand live application debugging.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 16:52:06 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 19:23:42 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Cavallo", "Marco", ""], ["Forbes", "Angus G.", ""]]}, {"id": "1809.05502", "submitter": "Eunjeong Koh", "authors": "Eunjeong Stella Koh and Shahrokh Yadegari", "title": "Mugeetion: Musical Interface Using Facial Gesture and Emotion", "comments": "4 pages, accepted to ICMC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  People feel emotions when listening to music. However, emotions are not\ntangible objects that can be exploited in the music composition process as they\nare difficult to capture and quantify in algorithms. We present a novel musical\ninterface, Mugeetion, designed to capture occurring instances of emotional\nstates from users' facial gestures and relay that data to associated musical\nfeatures. Mugeetion can translate qualitative data of emotional states into\nquantitative data, which can be utilized in the sound generation process. We\nalso presented and tested this work in the exhibition of sound installation,\nHearing Seascape, using the audiences' facial expressions. Audiences heard\nchanges in the background sound based on their emotional state. The process\ncontributes multiple research areas, such as gesture tracking systems,\nemotion-sound modeling, and the connection between sound and facial gesture.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 16:52:47 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 23:34:51 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Koh", "Eunjeong Stella", ""], ["Yadegari", "Shahrokh", ""]]}, {"id": "1809.05518", "submitter": "David Portugal", "authors": "David Portugal, Lu\\'is Santos, Pedro Trindade, Christophoros\n  Christophorou, Panayiotis Andreou, Dimosthenis Georgiadis, Marios Belk,\n  Jo\\~ao Freire, Paulo Alvito, George Samaras, Eleni Christodoulou, and Jorge\n  Dias", "title": "SocialRobot: Towards a Personalized Elderly Care Mobile Robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SocialRobot is a collaborative European project, which focuses on providing a\npractical and interactive solution to improve the quality of life of elderly\npeople. Having this in mind, a state of the art robotic mobile platform has\nbeen integrated with virtual social care technology to meet the elderly\nindividual needs and requirements, following a human centered approach. In this\nshort paper, we make an overview of SocialRobot, the developed architecture and\nthe human-robot interactive scenarios being prepared and tested in the\nframework of the project for dissemination and exploitation purposes.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 17:41:59 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Portugal", "David", ""], ["Santos", "Lu\u00eds", ""], ["Trindade", "Pedro", ""], ["Christophorou", "Christophoros", ""], ["Andreou", "Panayiotis", ""], ["Georgiadis", "Dimosthenis", ""], ["Belk", "Marios", ""], ["Freire", "Jo\u00e3o", ""], ["Alvito", "Paulo", ""], ["Samaras", "George", ""], ["Christodoulou", "Eleni", ""], ["Dias", "Jorge", ""]]}, {"id": "1809.05585", "submitter": "Philipp Jordan", "authors": "Philipp Jordan, Brett Oppegaard", "title": "Media Accessibility Policy in Theory and Reality: Empirical Outreach to\n  Audio Description Users in the United States", "comments": "10 pages, 7 figures including subfigures, 3 tables including\n  subtables. This paper has been accepted for the upcoming 52nd Hawaii\n  International Conference on System Sciences (HICSS-52). Publication in the\n  Conference Proceedings is pending on author's presentation of this paper at\n  the conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio description, a form of trans-modal media translation, allows people who\nare blind or visually impaired access to visually-oriented, socio-cultural, or\nhistorical public discourse alike. Although audio description has gained more\nprominence in media policy and research lately, it rarely has been studied\nempirically. Yet this paper presents quantitative and qualitative survey data\non its challenges and opportunities, through the analysis of responses from 483\nparticipants in a national sample, with 334 of these respondents being blind.\nOur results give insight into audio description use in broadcast TV, streaming\nservices, for physical media, such as DVDs, and in movie theaters. We further\ndiscover a multiplicity of barriers and hindrances which prevent a better\nadoption and larger proliferation of audio description. In our discussion, we\npresent a possible answer to these problems - the UniDescription Project - a\nmedia ecosystem for the creation, curation, and dissemination of audio\ndescription for multiple media platforms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 20:49:51 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Jordan", "Philipp", ""], ["Oppegaard", "Brett", ""]]}, {"id": "1809.05635", "submitter": "Ozan Ozdenizci", "authors": "Ozan Ozdenizci, Sezen Yagmur Gunay, Fernando Quivira, Deniz Erdogmus", "title": "Hierarchical Graphical Models for Context-Aware Hybrid Brain-Machine\n  Interfaces", "comments": "40th International Engineering in Medicine and Biology Conference\n  (EMBC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hierarchical graphical model based context-aware hybrid\nbrain-machine interface (hBMI) using probabilistic fusion of\nelectroencephalographic (EEG) and electromyographic (EMG) activities. Based on\nexperimental data collected during stationary executions and subsequent\nimageries of five different hand gestures with both limbs, we demonstrate\nfeasibility of the proposed hBMI system through within session and online\nacross sessions classification analyses. Furthermore, we investigate the\ncontext-aware extent of the model by a simulated probabilistic approach and\nhighlight potential implications of our work in the field of\nneurophysiologically-driven robotic hand prosthetics.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 02:34:58 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Ozdenizci", "Ozan", ""], ["Gunay", "Sezen Yagmur", ""], ["Quivira", "Fernando", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "1809.05720", "submitter": "Nicholas Mattei", "authors": "Avinash Balakrishnan, Djallel Bouneffouf, Nicholas Mattei, Francesca\n  Rossi", "title": "Incorporating Behavioral Constraints in Online AI Systems", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI systems that learn through reward feedback about the actions they take are\nincreasingly deployed in domains that have significant impact on our daily\nlife. However, in many cases the online rewards should not be the only guiding\ncriteria, as there are additional constraints and/or priorities imposed by\nregulations, values, preferences, or ethical principles. We detail a novel\nonline agent that learns a set of behavioral constraints by observation and\nuses these learned constraints as a guide when making decisions in an online\nsetting while still being reactive to reward feedback. To define this agent, we\npropose to adopt a novel extension to the classical contextual multi-armed\nbandit setting and we provide a new algorithm called Behavior Constrained\nThompson Sampling (BCTS) that allows for online learning while obeying\nexogenous constraints. Our agent learns a constrained policy that implements\nthe observed behavioral constraints demonstrated by a teacher agent, and then\nuses this constrained policy to guide the reward-based online exploration and\nexploitation. We characterize the upper bound on the expected regret of the\ncontextual bandit algorithm that underlies our agent and provide a case study\nwith real world data in two application domains. Our experiments show that the\ndesigned agent is able to act within the set of behavior constraints without\nsignificantly degrading its overall reward performance.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 14:24:37 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Balakrishnan", "Avinash", ""], ["Bouneffouf", "Djallel", ""], ["Mattei", "Nicholas", ""], ["Rossi", "Francesca", ""]]}, {"id": "1809.05833", "submitter": "Kyudong Park", "authors": "Kyudong Park, Dohyeon Kim, Sung H. Han", "title": "Usability of the Size, Spacing, and Depth of Virtual Buttons on\n  Head-Mounted Displays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality (VR) allows users to see and manipulate virtual scenes and\nitems through input devices, like head-mounted displays. In this study, the\neffects of button size, spacing, and depth on the usability of virtual buttons\nin VR environments were investigated. Task completion time, number of errors,\nand subjective preferences were collected to test different levels of the\nbutton size, spacing, and depth. The experiment was conducted in a desktop\nsetting with Oculus Rift and Leap motion. A total of 18 subjects performed a\nbutton selection task. The optimal levels of button size and spacing within the\nexperimental conditions are 25 mm and between 5 mm and 9 mm, respectively.\nButton sizes of 15 mm with 1-mm spacing were too small to be used in VR\nenvironments. A trend of decreasing task completion time and the number of\nerrors was observed as button size and spacing increased. However, large size\nand spacing may cause fatigue, due to continuous extension of the arms. For\ndepth effects, the touch method took a shorter task completion time. However,\nthe push method recorded a smaller number of errors, owing to the visual\npush-feedback. In this paper, we discuss advantages and disadvantages in\ndetail. The results can be applied to many different application areas with VR\nHMD.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 08:01:11 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Park", "Kyudong", ""], ["Kim", "Dohyeon", ""], ["Han", "Sung H.", ""]]}, {"id": "1809.05837", "submitter": "Zhenyi He", "authors": "Zhenyi He, Fengyuan Zhu, Ken Perlin and Xiaojuan Ma", "title": "Manifest the Invisible: Design for Situational Awareness of Physical\n  Environments in Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) provides immersive experiences in the virtual world, but\nit may reduce users' awareness of physical surroundings and cause safety\nconcerns and psychological discomfort. Hence, there is a need of an ambient\ninformation design to increase users' situational awareness (SA) of physical\nelements when they are immersed in VR environment. This is challenging, since\nthere is a tradeoff between the awareness in reality and the interference with\nusers' experience in virtuality. In this paper, we design five representations\n(indexical, symbolic, and iconic with three emotions) based on two dimensions\n(vividness and emotion) to address the problem. We conduct an empirical study\nto evaluate participants' SA, perceived breaks in presence (BIPs), and\nperceived engagement through VR tasks that require movement in space. Results\nshow that designs with higher vividness evoke more SA, designs that are more\nconsistent with the virtual environment can mitigate the BIP issue, and\nemotion-evoking designs are more engaging.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 08:43:32 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["He", "Zhenyi", ""], ["Zhu", "Fengyuan", ""], ["Perlin", "Ken", ""], ["Ma", "Xiaojuan", ""]]}, {"id": "1809.05839", "submitter": "Gautham Krishna Gudur", "authors": "Gautham Krishna G, Karthik Subramanian Nathan, Yogesh Kumar B, Ankith\n  A Prabhu, Ajay Kannan, Vineeth Vijayaraghavan", "title": "A Generic Multi-modal Dynamic Gesture Recognition System using Machine\n  Learning", "comments": "Accepted at IEEE Future of Information and Communications Conference\n  (FICC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human computer interaction facilitates intelligent communication between\nhumans and computers, in which gesture recognition plays a prominent role. This\npaper proposes a machine learning system to identify dynamic gestures using\ntri-axial acceleration data acquired from two public datasets. These datasets,\nuWave and Sony, were acquired using accelerometers embedded in Wii remotes and\nsmartwatches, respectively. A dynamic gesture signed by the user is\ncharacterized by a generic set of features extracted across time and frequency\ndomains. The system was analyzed from an end-user perspective and was modelled\nto operate in three modes. The modes of operation determine the subsets of data\nto be used for training and testing the system. From an initial set of seven\nclassifiers, three were chosen to evaluate each dataset across all modes\nrendering the system towards mode-neutrality and dataset-independence. The\nproposed system is able to classify gestures performed at varying speeds with\nminimum preprocessing, making it computationally efficient. Moreover, this\nsystem was found to run on a low-cost embedded platform - Raspberry Pi Zero\n(USD 5), making it economically viable.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 08:51:05 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["G", "Gautham Krishna", ""], ["Nathan", "Karthik Subramanian", ""], ["B", "Yogesh Kumar", ""], ["Prabhu", "Ankith A", ""], ["Kannan", "Ajay", ""], ["Vijayaraghavan", "Vineeth", ""]]}, {"id": "1809.05869", "submitter": "Kyudong Park", "authors": "Kyudong Park and Sung H. Han and Hojin Lee", "title": "A Study on Shared Steering Control in Driving Experience Perspective:\n  How Strong and How Soon Should Intervention Be?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lane keeping assistance system (LKAS), a representative of the advanced\ndriver assistance system (ADAS), comprises a shared control that cooperates\nwith the driver to achieve a common goal. The experience of the driver through\nthe steering wheel may vary significantly depending on the steering control\nstrategy of the system. In this study, we examine how driving experience\nchanges according to various steering control strategies. Based on the\npreliminary study and typical LKAS parameters, nine control strategies (3\ntorque amounts (TOR) x 3 deviations to start control (DEV)) were designed as a\nprototype. Eighteen participants participated in evaluating each strategy in a\nhighway environment provided by a driving simulator. Two-way repeated measure\nANOVA was used to assess the effects of the system. Both the objective measures\n(standard deviation of lane position, steering reversal rate, and root mean\nsquare of lateral speed) and subjective measures (pleasure and arousal of\nemotion, trust, disturbance, and satisfaction) are analyzed. The experimental\nresults demonstrate that all dependent measures are significant. As the TOR\nincreased, SDLP decreased. However, no difference is observed between the 2-Nm\nand 3-Nm TOR in terms of trust and satisfaction. The high disturbance and\nnegative emotion in 3 Nm appear to be the cause. In terms of the DEV, the high\nlevel of the root mean square of the lateral speed is observed at 0.8 m.\nFurther, negative effects are found in pleasure, trust, and satisfaction. There\nis little difference at all dependent measures between 0.0-m and 0.4-m DEV. In\nthe regression model analyzed from the aspect of satisfaction, the 2.32-Nm TOR\nand 0.27-m DEV are the optimal values. We expect our research on shared\nsteering control with an assistance system to be applied to the experience\ndesign of a lateral semi-autonomous vehicle.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 13:14:50 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Park", "Kyudong", ""], ["Han", "Sung H.", ""], ["Lee", "Hojin", ""]]}, {"id": "1809.05904", "submitter": "Jason R.C. Nurse Dr", "authors": "Aastha Madaan and Jason R.C. Nurse and David De Roure and Kieron\n  O'Hara and Wendy Hall and Sadie Creese", "title": "A Storm in an IoT Cup: The Emergence of Cyber-Physical Social Machines", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "SSRN-3250383", "categories": "cs.CY cs.AI cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of social machines is increasingly being used to characterise\nvarious socio-cognitive spaces on the Web. Social machines are human\ncollectives using networked digital technology which initiate real-world\nprocesses and activities including human communication, interactions and\nknowledge creation. As such, they continuously emerge and fade on the Web. The\nrelationship between humans and machines is made more complex by the adoption\nof Internet of Things (IoT) sensors and devices. The scale, automation,\ncontinuous sensing, and actuation capabilities of these devices add an extra\ndimension to the relationship between humans and machines making it difficult\nto understand their evolution at either the systemic or the conceptual level.\nThis article describes these new socio-technical systems, which we term\nCyber-Physical Social Machines, through different exemplars, and considers the\nassociated challenges of security and privacy.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 16:00:02 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 14:47:47 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Madaan", "Aastha", ""], ["Nurse", "Jason R. C.", ""], ["De Roure", "David", ""], ["O'Hara", "Kieron", ""], ["Hall", "Wendy", ""], ["Creese", "Sadie", ""]]}, {"id": "1809.05911", "submitter": "Zhishuai Han", "authors": "Zhishuai Han, Xiaojuan Ban, Xiaokun Wang and Di wu", "title": "Robust and customized methods for real-time hand gesture recognition\n  under object-occlusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic hand tracking and gesture recognition is a hard task since there are\nmany joints on the fingers and each joint owns many degrees of freedom.\nBesides, object occlusion is also a thorny issue in finger tracking and posture\nrecognition. Therefore, we propose a robust and customized system for realtime\nhand tracking and gesture recognition under occlusion environment. First, we\nmodel the angles between hand keypoints and encode their relative coordinate\nvectors, then we introduce GAN to generate raw discrete sequence dataset.\nSecondly we propose a time series forecasting method in the prediction of\ndefined hand keypoint location. Finally, we define a sliding window matching\nmethod to complete gesture recognition. We analyze 11 kinds of typical gestures\nand show how to perform gesture recognition with the proposed method. Our work\ncan reach state of the art results and contribute to build a framework to\nimplement customized gesture recognition task.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 16:41:26 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Han", "Zhishuai", ""], ["Ban", "Xiaojuan", ""], ["Wang", "Xiaokun", ""], ["wu", "Di", ""]]}, {"id": "1809.05970", "submitter": "Brian Cohn", "authors": "Brian A. Cohn, Dilan D. Shah, Ali Marjaninejad, Martin Shapiro, Serhan\n  Ulkumen, Christopher M. Laine, Francisco J. Valero-Cuevas, Kenneth H.\n  Hayashida, Sarah Ingersoll", "title": "Quantifying and attenuating pathologic tremor in virtual reality", "comments": "3 pages; 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a virtual reality (VR) experience that creates a research-grade\nbenchmark in assessing patients with active upper-limb tremor, while\nsimultaneously offering the opportunity for patients to engage with VR\nexperiences without their pathologic tremor. Accurate and precise use of\nhandheld motion controllers in VR gaming applications may be limited for\npatients with upper limb tremor. In parallel, objective tools measuring tremor\nare not in widespread, routine clinical use. We used a commercially available\nVR system and designed a challenging virtual-balloon-popping test mimicking a\ncommon nose-to-target pointing task used by medical practitioners to\nsubjectively evaluate tremor in the exam room. Within our VR experience, we\noffer a software mode which uses a low-pass filter to adjust hand position and\npointing orientation over a series of past data points. This digital filter\ncreates a smoothing function for hand movement which effectively removes the\npatient's tremor in the VR representation. While the patient completes trials\nof the reaching task, quantitative data on the pathologic tremor is digitally\nrecorded. With speed, accuracy, and the tremor components computed across three\naxes of movement, patients can be evaluated for their tremor amplitudes in a\nquantitative, replicable, and enjoyable manner. Removal of tremor in digital\nspace may allow patients having significant upper limb tremor to have both an\nobjective clinical measurement of symptoms while providing patients positive\nfeedback and interaction.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 22:23:04 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Cohn", "Brian A.", ""], ["Shah", "Dilan D.", ""], ["Marjaninejad", "Ali", ""], ["Shapiro", "Martin", ""], ["Ulkumen", "Serhan", ""], ["Laine", "Christopher M.", ""], ["Valero-Cuevas", "Francisco J.", ""], ["Hayashida", "Kenneth H.", ""], ["Ingersoll", "Sarah", ""]]}, {"id": "1809.06172", "submitter": "Fabio Calefato", "authors": "Fabio Calefato and Giuseppe Iaffaldano and Filippo Lanubile and\n  Federico Maiorano", "title": "Investigating Crowd Creativity in Online Music Communities", "comments": "Proceedings of the ACM on Human-Computer Interaction, Vol. 2, No.\n  CSCW, Article 27, Publication date: November 2018", "journal-ref": "Proceedings of the ACM on Human-Computer Interaction, Vol. 2, No.\n  CSCW, Article 27, Publication date: November 2018", "doi": "10.1145/3274296", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd creativity is typically associated with peer-production communities\nfocusing on artistic products like animations, video games, and music, but less\nfrequently to Open Source Software (OSS), despite the fact that also developers\nmust be creative to come up with new solutions to their technical challenges.\nIn this paper, we conduct a study to further the understanding of which factors\nfrom prior work in both OSS and art communities are predictive of successful\ncollaboration - defined as reuse of previous songs - in three different\nsongwriting communities, namely Songtree, Splice, and ccMixter. The main\nfindings from this study confirm that the success of collaborations is\nassociated with high community status of recognizable authors and low degree of\nderivativity of songs.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 05:17:07 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 08:39:27 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Calefato", "Fabio", ""], ["Iaffaldano", "Giuseppe", ""], ["Lanubile", "Filippo", ""], ["Maiorano", "Federico", ""]]}, {"id": "1809.06320", "submitter": "Armin Becher", "authors": "Armin Becher, Jens Angerer, Thomas Grauschopf", "title": "Novel Approach to Measure Motion-To-Photon and Mouth-To-Ear Latency in\n  Distributed Virtual Reality Systems", "comments": "GI VR/AR Workshop 2018, Virtuelle und Erweiterte Realit\\\"at - 15.\n  Workshop der GI-Fachgruppe VR/AR, D\\\"usseldorf, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Virtual Reality systems enable globally dispersed users to\ninteract with each other in a shared virtual environment. In such systems,\ndifferent types of latencies occur. For a good VR experience, they need to be\ncontrolled. The time delay between the user's head motion and the corresponding\ndisplay output of the VR system might lead to adverse effects such as a reduced\nsense of presence or motion sickness. Additionally, high network latency among\nworldwide locations makes collaboration between users more difficult and leads\nto misunderstandings. To evaluate the performance and optimize dispersed VR\nsolutions it is therefore important to measure those delays. In this work, a\nnovel, easy to set up, and inexpensive method to measure local and remote\nsystem latency will be described. The measuring setup consists of a\nmicrocontroller, a microphone, a piezo buzzer, a photosensor, and a\npotentiometer. With these components, it is possible to measure\nmotion-to-photon and mouth-to-ear latency of various VR systems. By using\nGPS-receivers for timecode-synchronization it is also possible to obtain the\nend-to-end delays between different worldwide locations. The described system\nwas used to measure local and remote latencies of two HMD based distributed VR\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 16:48:28 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Becher", "Armin", ""], ["Angerer", "Jens", ""], ["Grauschopf", "Thomas", ""]]}, {"id": "1809.06535", "submitter": "Kyudong Park", "authors": "Kyudong Park, Jiyoung Kwahk, Sung H. Han, Minseok Song, Dong Gu Choi,\n  Hyeji Jang, Dohyeon Kim, Young Deok Won, In Sub Jeong", "title": "Modelling the Intrusive feelings of advanced driver assistance systems\n  based on vehicle activity log data: a case study for the lane keeping\n  assistance system", "comments": null, "journal-ref": "International Journal of Automotive Technology, 2019,\n  20(3):455-463", "doi": "10.1007/s12239-019-0043-6", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the automotive industry has been among the sectors that\nbest-understands the importance of drivers' affect, the focus of design and\nresearch in the automotive field has long emphasized the visceral aspects of\nexterior and interior design. With the adoption of Advanced Driver Assistance\nSystems (ADAS), endowing 'semi-autonomy' to the vehicles, however, the scope of\naffective design should be expanded to include the behavioural aspects of the\nvehicle. In such a 'shared-control' system wherein the vehicle can intervene in\nthe human driver's operations, a certain degree of 'intrusive feelings' are\nunavoidable. For example, when the Lane Keeping Assistance System (LKAS), one\nof the most popular examples of ADAS, operates the steering wheel in a\ndangerous situation, the driver may feel interrupted or surprised because of\nthe abrupt torque generated by LKAS. This kind of unpleasant experience can\nlead to prolonged negative feelings such as irritation, anxiety, and distrust\nof the system. Therefore, there are increasing needs of investigating the\ndriver's affective responses towards the vehicle's dynamic behaviour. In this\nstudy, four types of intrusive feelings caused by LKAS were identified to be\nproposed as a quantitative performance indicator in designing the affectively\nsatisfactory behaviour of LKAS. A metric as well as a statistical data analysis\nmethod to quantitatively measure the intrusive feelings through the vehicle\nsensor log data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 05:13:21 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 14:24:00 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Park", "Kyudong", ""], ["Kwahk", "Jiyoung", ""], ["Han", "Sung H.", ""], ["Song", "Minseok", ""], ["Choi", "Dong Gu", ""], ["Jang", "Hyeji", ""], ["Kim", "Dohyeon", ""], ["Won", "Young Deok", ""], ["Jeong", "In Sub", ""]]}, {"id": "1809.06673", "submitter": "Zehong Cao Dr.", "authors": "Zehong Cao, Chin-Teng Lin, Kuan-Lin Lai, Li-Wei Ko, Jung-Tai King,\n  Jong-Ling Fuh, Shuu-Jiun Wang", "title": "Extraction of SSVEPs-based Inherent Fuzzy Entropy Using a Wearable\n  Headband EEG in Migraine Patients", "comments": "The revised manuscript is submitting to IEEE Transactions on Fuzzy\n  Systems", "journal-ref": "IEEE Transactions on Fuzzy Systems (18 March 2019)", "doi": "10.1109/TFUZZ.2019.2905823", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inherent fuzzy entropy is an objective measurement of electroencephalography\n(EEG) complexity, reflecting the robustness of brain systems. In this study, we\npresent a novel application of multi-scale relative inherent fuzzy entropy\nusing repetitive steady-state visual evoked potentials (SSVEPs) to investigate\nEEG complexity change between two migraine phases, i.e. inter-ictal (baseline)\nand pre-ictal (before migraine attacks) phases. We used a wearable headband EEG\ndevice with O1, Oz, O2 and Fpz electrodes to collect EEG signals from 80\nparticipants (40 migraine patients and 40 healthy controls [HCs]) under the\nfollowing two conditions: during resting state and SSVEPs with five 15-Hz\nphotic stimuli. We found a significant enhancement in occipital EEG entropy\nwith increasing stimulus times in both HCs and patients in the inter-ictal\nphase but a reverse trend in patients in the pre-ictal phase. In the 1st SSVEP\n, occipital EEG entropy of the HCs was significantly higher than that of\npatents in the pre-ictal phase (FDR-adjusted p < 0.05). Regarding the\ntransitional variance of EEG entropy between the 1st and 5th SSVEPs, patients\nin the pre-ictal phase exhibited significantly lower values than patients in\nthe inter-ictal phase (FDR-adjusted p < 0.05). Furthermore, in the\nclassification model, the AdaBoost ensemble learning showed an accuracy of 81%\nand AUC of 0.87 for classifying inter-ictal and pre-ictal phases. In contrast,\nthere were no differences in EEG entropy among groups or sessions by using\nother competing entropy models, including approximate entropy, sample entropy\nand fuzzy entropy on the same dataset. In conclusion, inherent fuzzy entropy\noffers novel applications in visual stimulus environments and may have the\npotential to provide a pre-ictal alert to migraine patients.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 12:38:31 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 07:07:56 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Cao", "Zehong", ""], ["Lin", "Chin-Teng", ""], ["Lai", "Kuan-Lin", ""], ["Ko", "Li-Wei", ""], ["King", "Jung-Tai", ""], ["Fuh", "Jong-Ling", ""], ["Wang", "Shuu-Jiun", ""]]}, {"id": "1809.06675", "submitter": "Zehong Cao Prof.", "authors": "Chun-Hsiang Chuang, Zehong Cao, Po-Tsang Chen, Chih-Sheng Huang,\n  Nikhil R. Pal, Chin-Teng Lin", "title": "Dynamically Weighted Ensemble-based Prediction System for Adaptively\n  Modeling Driver Reaction Time", "comments": "Revision submitted to IEEE Transactions on Biomedical Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting a driver's cognitive state, or more specifically, modeling a\ndriver's reaction time (RT) in response to the appearance of a potential hazard\nwarrants urgent research. In the last two decades, the electric field that is\ngenerated by the activities in the brain, monitored by an electroencephalogram\n(EEG), has been proven to be a robust physiological indicator of human\nbehavior. However, mapping the human brain can be extremely challenging,\nespecially owing to the variability in human beings over time, both within and\namong individuals. Factors such as fatigue, inattention and stress can induce\nhomeostatic changes in the brain, which affect the observed relationship\nbetween brain dynamics and behavioral performance, and thus make the existing\nsystems for predicting RT difficult to generalize. To solve this problem, an\nensemble-based weighted prediction system is presented herein. This system\ncomprises a set of prediction submodels that are individually trained using\ngroups of data with similar EEG-RT relationships. To obtain a final prediction,\nthe prediction outcomes of the sub-models are then multiplied by weights that\nare derived from the EEG alpha coherences of 10 channels plus theta band powers\nof 30 channels, whose changes were found to be indicators of variations in the\nEEG-RT relationship. The results thus obtained reveal that the proposed system\nwith a time-varying adaptive weighting mechanism significantly outperforms the\nconventional system in modeling a driver's RT. The adaptive design of the\nproposed system demonstrates its feasibility in coping with the variability in\nthe brain-behavior relationship. In this contribution surprisingly simple\nEEG-based adaptive methods are used in combination with an ensemble scheme to\nsignificantly increase system performance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 12:49:08 GMT"}, {"version": "v2", "created": "Sun, 26 May 2019 11:30:26 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Chuang", "Chun-Hsiang", ""], ["Cao", "Zehong", ""], ["Chen", "Po-Tsang", ""], ["Huang", "Chih-Sheng", ""], ["Pal", "Nikhil R.", ""], ["Lin", "Chin-Teng", ""]]}, {"id": "1809.07166", "submitter": "Karl Rosenberg", "authors": "Ken Perlin, Zhenyi He, Karl Rosenberg", "title": "Chalktalk : A Visualization and Communication Language -- As a Tool in\n  the Domain of Computer Science Education", "comments": "SPLASH LIVE workshop 2018, Nov. 2018, Boston, MA, USA. See\n  https://2018.splashcon.org/event/live-2018-papers-chalktalk-a-visualization-and-communication-language-as-a-tool-in-the-domain-of-computer-science-education", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of a classroom lesson, concepts must be visualized and\norganized in many ways depending on the needs of the teacher and students.\nTraditional presentation media such as the blackboard or electronic whiteboard\nallow for static hand-drawn images, and slideshow software may be used to\ngenerate linear sequences of text and pre-animated images. However, none of\nthese media support the creation of dynamic visualizations that can be\nmanipulated, combined, or re-animated in real-time, and so demonstrating new\nconcepts or adapting to changes in the requirements of a presentation is a\nchallenge. Thus, we propose Chalktalk as a solution. Chalktalk is an\nopen-source presentation and visualization tool in which the user's drawings\nare recognized as animated and interactive \"sketches,\" which the user controls\nvia mouse gestures. Sketches help users demonstrate and experiment with complex\nideas (e.g. computer graphics, procedural animation, logic) during a live\npresentation without needing to create and structure all content ahead of time.\nBecause sketches can interoperate and be programmed to represent underlying\ndata in multiple ways, Chalktalk presents the opportunity to visualize key\nconcepts in computer science: especially data structures, whose data and form\nchange over time due to the variety of interactions within a computer system.\nTo show Chalktalk's capabilities, we have prototyped sketch implementations for\nbinary search tree (BST) and stack (LIFO) data structures, which take advantage\nof sketches' ability to interact and change at run-time. We discuss these\nprototypes and conclude with considerations for future research using the\nChalktalk platform.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 13:14:54 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Perlin", "Ken", ""], ["He", "Zhenyi", ""], ["Rosenberg", "Karl", ""]]}, {"id": "1809.07424", "submitter": "Besmira Nushi", "authors": "Besmira Nushi, Ece Kamar, Eric Horvitz", "title": "Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing\n  System Failure", "comments": null, "journal-ref": "AAAI Conference on Human Computation and Crowdsourcing 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning systems move from computer-science laboratories into the\nopen world, their accountability becomes a high priority problem.\nAccountability requires deep understanding of system behavior and its failures.\nCurrent evaluation methods such as single-score error metrics and confusion\nmatrices provide aggregate views of system performance that hide important\nshortcomings. Understanding details about failures is important for identifying\npathways for refinement, communicating the reliability of systems in different\nsettings, and for specifying appropriate human oversight and engagement.\nCharacterization of failures and shortcomings is particularly complex for\nsystems composed of multiple machine learned components. For such systems,\nexisting evaluation methods have limited expressiveness in describing and\nexplaining the relationship among input content, the internal states of system\ncomponents, and final output quality. We present Pandora, a set of hybrid\nhuman-machine methods and tools for describing and explaining system failures.\nPandora leverages both human and system-generated observations to summarize\nconditions of system malfunction with respect to the input content and system\narchitecture. We share results of a case study with a machine learning pipeline\nfor image captioning that show how detailed performance views can be beneficial\nfor analysis and debugging.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 22:53:46 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Nushi", "Besmira", ""], ["Kamar", "Ece", ""], ["Horvitz", "Eric", ""]]}, {"id": "1809.07593", "submitter": "Boris Bogaerts", "authors": "Boris Bogaerts, Seppe Sels, Steve Vanlanduit and Rudi Penne", "title": "Interactive Camera Network Design using a Virtual Reality Interface", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional literature on camera network design focuses on constructing\nautomated algorithms. These require problem specific input from experts in\norder to produce their output. The nature of the required input is highly\nunintuitive leading to an unpractical workflow for human operators. In this\nwork we focus on developing a virtual reality user interface allowing human\noperators to manually design camera networks in an intuitive manner. From real\nworld practical examples we conclude that the camera networks designed using\nthis interface are highly competitive with, or superior to those generated by\nautomated algorithms, but the associated workflow is much more intuitive and\nsimple. The competitiveness of the human-generated camera networks is\nremarkable because the structure of the optimization problem is a well known\ncombinatorial NP-hard problem. These results indicate that human operators can\nbe used in challenging geometrical combinatorial optimization problems given an\nintuitive visualization of the problem.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 12:33:30 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Bogaerts", "Boris", ""], ["Sels", "Seppe", ""], ["Vanlanduit", "Steve", ""], ["Penne", "Rudi", ""]]}, {"id": "1809.07720", "submitter": "Hongze Li", "authors": "Hongze Li", "title": "A Visual Query System for Scholar Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scholar networks is quite popular in the academic domain, like Aminer.\nIt offers to display the academic social network, including profile search,\nexpert finding, conference analysis, course search, sub-graph search, topic\nbrowser, academic ranks and user management. Usually the search results are\nlisted as items, while the relations among them are hidden to the users.\nVisualization is a feasible way to help users explore the hidden relations and\ndiscover more useful information. This article aim to visualize the search\nresults in Aminer in a more user-friendly way and help them better utilize the\ntool. We provided three different designs to visualize the results and tested\nthem in user study. The empirical results of our research show that the\ndesigned graphs help users better understand the area they intend to know and\nmake their search more effective.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 10:34:38 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Li", "Hongze", ""]]}, {"id": "1809.07829", "submitter": "Jo\\~ao Rufino Mr", "authors": "Vanessa Martins, Jo\\~ao Rufino, Bruno Fernandes, Lu\\'is Silva, Jo\\~ao\n  Almeida, Joaquim Ferreira, Jos\\'e Fonseca", "title": "Personal Virtual Traffic Light Systems", "comments": "7 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic control management at intersections, a challenging and complex field\nof study, aims to attain a balance between safety and efficient traffic\ncontrol. Nowadays, traffic control at intersections is typically done by\ntraffic light systems which are not optimal and exhibit several drawbacks, e.g.\npoor efficiency and real-time adaptability. With the advent of Intelligent\nTransportation Systems (ITS), vehicles are being equipped with state-of-the-art\ntechnology, enabling cooperative decision-making which will certainly overwhelm\nthe available traffic control systems. This solution strongly penalizes users\nwithout such capabilities, namely pedestrians, cyclists and other legacy\nvehicles. Therefore, in this work, a prototype based on an alternative\ntechnology to the standard vehicular communications, BLE, is presented. The\nproposed framework aims to integrate legacy and modern vehicular communication\nsystems into a cohesive management system. In this framework, the movements of\nusers at intersections are managed by a centralized controller which, through\nthe use of networked retransmitters deployed at intersections, broadcasts\nalerts and virtual light signalization orders. Users receive the aforementioned\ninformation on their own smart devices, discarding the need for dedicated light\nsignalization infrastructures. Field tests, carried-out with a real-world\nimplementation, validate the correct operation of the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 19:55:48 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Martins", "Vanessa", ""], ["Rufino", "Jo\u00e3o", ""], ["Fernandes", "Bruno", ""], ["Silva", "Lu\u00eds", ""], ["Almeida", "Jo\u00e3o", ""], ["Ferreira", "Joaquim", ""], ["Fonseca", "Jos\u00e9", ""]]}, {"id": "1809.07948", "submitter": "Michael Fulton", "authors": "Michael Fulton, Chelsey Edge, Junaed Sattar", "title": "Robot Communication Via Motion: Closing the Underwater Human-Robot\n  Interaction Loop", "comments": "Under review for ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for underwater robot-to-human\ncommunication using the motion of the robot as \"body language\". To evaluate\nthis system, we develop simulated examples of the system's body language\ngestures, called kinemes, and compare them to a baseline system using flashing\ncolored lights through a user study. Our work shows evidence that motion can be\nused as a successful communication vector which is accurate, easy to learn, and\nquick enough to be used, all without requiring any additional hardware to be\nadded to our platform. We thus contribute to \"closing the loop\" for human-robot\ninteraction underwater by proposing and testing this system, suggesting a\nlibrary of possible body language gestures for underwater robots, and offering\ninsight on the design of nonverbal robot-to-human communication methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 05:22:58 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Fulton", "Michael", ""], ["Edge", "Chelsey", ""], ["Sattar", "Junaed", ""]]}, {"id": "1809.08095", "submitter": "Ali Shafti", "authors": "Ali Shafti, Pavel Orlov and A. Aldo Faisal", "title": "Gaze-based, Context-aware Robotic System for Assisted Reaching and\n  Grasping", "comments": "7 pages, 7 figures, 4 tables. Submitted to IEEE ICRA 2019 - under\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assistive robotic systems endeavour to support those with movement\ndisabilities, enabling them to move again and regain functionality. Main issue\nwith these systems is the complexity of their low-level control, and how to\ntranslate this to simpler, higher level commands that are easy and intuitive\nfor a human user to interact with. We have created a multi-modal system,\nconsisting of different sensing, decision making and actuating modalities,\nleading to intuitive, human-in-the-loop assistive robotics. The system takes\nits cue from the user's gaze, to decode their intentions and implement\nlow-level motion actions to achieve high-level tasks. This results in the user\nsimply having to look at the objects of interest, for the robotic system to\nassist them in reaching for those objects, grasping them, and using them to\ninteract with other objects. We present our method for 3D gaze estimation, and\ngrammars-based implementation of sequences of action with the robotic system.\nThe 3D gaze estimation is evaluated with 8 subjects, showing an overall\naccuracy of $4.68\\pm0.14cm$. The full system is tested with 5 subjects, showing\nsuccessful implementation of $100\\%$ of reach to gaze point actions and full\nimplementation of pick and place tasks in 96\\%, and pick and pour tasks in\n$76\\%$ of cases. Finally we present a discussion on our results and what future\nwork is needed to improve the system.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 13:30:03 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 11:46:00 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Shafti", "Ali", ""], ["Orlov", "Pavel", ""], ["Faisal", "A. Aldo", ""]]}, {"id": "1809.08177", "submitter": "Jennifer Rogers", "authors": "Jennifer Rogers, Nicholas Spina, Ashley Neese, Rachel Hess, Darrel\n  Brodke, Alexander Lex (University of Utah)", "title": "Composer: Visual Cohort Analysis of Patient Outcomes", "comments": "Preprint for Applied Clinical Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective: Visual cohort analysis utilizing electronic health record data has\nbecome an important tool in clinical assessment of patient outcomes. In this\npaper, we introduce Composer, a visual analysis tool for orthopedic surgeons to\ncompare changes in physical functions of a patient cohort following various\nspinal procedures. The goal of our project is to help researchers analyze\noutcomes of procedures and facilitate informed decision-making about treatment\noptions between patient and clinician. Methods: In collaboration with\nOrthopedic surgeons and researchers, we defined domain-specific user\nrequirements to inform the design. We developed the tool in an iterative\nprocess with our collaborators to develop and refine functionality. With\nComposer, analysts can dynamically define a patient cohort using demographic\ninformation, clinical parameters, and events in patient medical histories and\nthen analyze patient-reported outcome scores for the cohort over time, as well\nas compare it to other cohorts. Using Composer's current iteration, we provide\na usage scenario for use of the tool in a clinical setting. Conclusion: We have\ndeveloped a prototype cohort analysis tool to help clinicians assess patient\ntreatment options by analyzing prior cases with similar characteristics. Though\nComposer was designed using patient data specific to Orthopedic research, we\nbelieve the tool is generalizable to other healthcare domains. A long term goal\nfor Composer is to develop the application into a shared decision-making tool\nthat allows translation of comparison and analysis from a clinician facing\ninterface into visual representations to communicate treatment options to\npatients.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 15:44:48 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 23:44:36 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Rogers", "Jennifer", "", "University of Utah"], ["Spina", "Nicholas", "", "University of Utah"], ["Neese", "Ashley", "", "University of Utah"], ["Hess", "Rachel", "", "University of Utah"], ["Brodke", "Darrel", "", "University of Utah"], ["Lex", "Alexander", "", "University of Utah"]]}, {"id": "1809.08559", "submitter": "Oscar Karnalim", "authors": "Oscar Karnalim and Lisan Sulistiani", "title": "Which Source Code Plagiarism Detection Approach is More Humane?", "comments": "The 9th International Conference on Awareness Science and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes in developing source code plagiarism detection that is\nmore aligned with human perspective. Three evaluation mechanisms that directly\nrelate human perspective with evaluated approaches are proposed: think-aloud,\naspect-oriented, and empirical mechanism. Using those mechanisms, a comparative\nstudy toward attribute-and structure-based plagiarism detection approach (i.e.,\ntwo popular approach categories in source code plagiarism detection) is\nconducted. According to that study, structure-based approach is more effective\nthan the attribute-based one; its signature aspect and resulted similarity\ndegrees are more related to human preferences. In addition, such approach is\nrelated to most human-oriented aspects for suspecting source code plagiarism.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 09:14:05 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Karnalim", "Oscar", ""], ["Sulistiani", "Lisan", ""]]}, {"id": "1809.08585", "submitter": "Haruna Isah", "authors": "Tiffany Leung, Farhana Zulkernine, Haruna Isah", "title": "The use of Virtual Reality in Enhancing Interdisciplinary Research and\n  Education", "comments": "6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Virtual Reality (VR) is increasingly being recognized for its educational\npotential and as an effective way to convey new knowledge to people, it\nsupports interactive and collaborative activities. Affordable VR powered by\nmobile technologies is opening a new world of opportunities that can transform\nthe ways in which we learn and engage with others. This paper reports our study\nregarding the application of VR in stimulating interdisciplinary communication.\nIt investigates the promises of VR in interdisciplinary education and research.\nThe main contributions of this study are (i) literature review of theories of\nlearning underlying the justification of the use of VR systems in education,\n(ii) taxonomy of the various types and implementations of VR systems and their\napplication in supporting education and research (iii) evaluation of\neducational applications of VR from a broad range of disciplines, (iv)\ninvestigation of how the learning process and learning outcomes are affected by\nVR systems, and (v) comparative analysis of VR and traditional methods of\nteaching in terms of quality of learning. This study seeks to inspire and\ninform interdisciplinary researchers and learners about the ways in which VR\nmight support them and also VR software developers to push the limits of their\ncraft.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 12:22:11 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Leung", "Tiffany", ""], ["Zulkernine", "Farhana", ""], ["Isah", "Haruna", ""]]}, {"id": "1809.08632", "submitter": "Linxing Jiang", "authors": "Linxing Preston Jiang, Andrea Stocco, Darby M. Losey, Justin A.\n  Abernethy, Chantel S. Prat, Rajesh P. N. Rao", "title": "BrainNet: A Multi-Person Brain-to-Brain Interface for Direct\n  Collaboration Between Brains", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-019-41895-7", "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BrainNet which, to our knowledge, is the first multi-person\nnon-invasive direct brain-to-brain interface for collaborative problem solving.\nThe interface combines electroencephalography (EEG) to record brain signals and\ntranscranial magnetic stimulation (TMS) to deliver information noninvasively to\nthe brain. The interface allows three human subjects to collaborate and solve a\ntask using direct brain-to-brain communication. Two of the three subjects are\n\"Senders\" whose brain signals are decoded using real-time EEG data analysis to\nextract decisions about whether to rotate a block in a Tetris-like game before\nit is dropped to fill a line. The Senders' decisions are transmitted via the\nInternet to the brain of a third subject, the \"Receiver,\" who cannot see the\ngame screen. The decisions are delivered to the Receiver's brain via magnetic\nstimulation of the occipital cortex. The Receiver integrates the information\nreceived and makes a decision using an EEG interface about either turning the\nblock or keeping it in the same position. A second round of the game gives the\nSenders one more chance to validate and provide feedback to the Receiver's\naction. We evaluated the performance of BrainNet in terms of (1) Group-level\nperformance during the game; (2) True/False positive rates of subjects'\ndecisions; (3) Mutual information between subjects. Five groups of three\nsubjects successfully used BrainNet to perform the Tetris task, with an average\naccuracy of 0.813. Furthermore, by varying the information reliability of the\nSenders by artificially injecting noise into one Sender's signal, we found that\nReceivers are able to learn which Sender is more reliable based solely on the\ninformation transmitted to their brains. Our results raise the possibility of\nfuture brain-to-brain interfaces that enable cooperative problem solving by\nhumans using a \"social network\" of connected brains.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 16:59:55 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 23:10:22 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 20:14:47 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Jiang", "Linxing Preston", ""], ["Stocco", "Andrea", ""], ["Losey", "Darby M.", ""], ["Abernethy", "Justin A.", ""], ["Prat", "Chantel S.", ""], ["Rao", "Rajesh P. N.", ""]]}, {"id": "1809.08640", "submitter": "Adam Aviv", "authors": "Adam J. Aviv and Flynn Wolf and Ravi Kuber", "title": "Comparing Video Based Shoulder Surfing with Live Simulation", "comments": "This article appears in the 2018 Annual Computer Security\n  Applications Conference. https://doi.org/10.1145/3274694.3274702", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We analyze the claims that video recreations of shoulder surfing attacks\noffer a suitable alternative and a baseline, as compared to evaluation in a\nlive setting. We recreated a subset of the factors of a prior video-simulation\nexperiment conducted by Aviv et al. (ACSAC 2017), and model the same scenario\nusing live participants ($n=36$) instead (i.e., the victim and attacker were\nboth present). The live experiment confirmed that for Android's graphical\npatterns video simulation is consistent with the live setting for attacker\nsuccess rates. However, both 4- and 6-digit PINs demonstrate statistically\nsignificant differences in attacker performance, with live attackers performing\nas much 1.9x better than in the video simulation. The security benefits gained\nfrom removing feedback lines in Android's graphical patterns are also greatly\ndiminished in the live setting, particularly under multiple attacker\nobservations, but overall, the data suggests that video recreations can provide\na suitable baseline measure for attacker success rate. However, we caution that\nresearchers should consider that these baselines may greatly underestimate the\nthreat of an attacker in live settings.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 17:23:08 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Aviv", "Adam J.", ""], ["Wolf", "Flynn", ""], ["Kuber", "Ravi", ""]]}, {"id": "1809.08817", "submitter": "Christian Tiefenau", "authors": "Christian Tiefenau and Emanuel von Zezschwitz", "title": "The Struggle is Real: Analyzing Ground Truth Data of TLS\n  (Mis-)Configurations", "comments": "Poster presented at the 14th Symposium on Usable Privacy and Security\n  (SOUPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of today, TLS is the most commonly used protocol to protect communication\ncontent. To provide good security, it is of central importance, that\nadministrators know how to configure their services correctly. For this\npurpose, services like, e.g., Qualys SSL Server Test can be leveraged to test\nthe correctness of a given web server configuration. We analyzed the\nutilization of this service over a period of 2.5 months and found two major\nusage-patterns. In addition, there is a relation between the number of\ntest-runs and the resulting quality (i.e., security) of a TLS configuration.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 09:45:33 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Tiefenau", "Christian", ""], ["von Zezschwitz", "Emanuel", ""]]}, {"id": "1809.08884", "submitter": "Ralf Teusner", "authors": "Ralf Teusner, Kai-Adrian Rollmann, Jan Renz", "title": "Taking Informed Action on Student Activity in MOOCs", "comments": null, "journal-ref": null, "doi": "10.1145/3051457.3053971", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to understand specific student behavior\nin MOOCs. Instructors currently perceive participants only as one homogeneous\ngroup. In order to improve learning outcomes, they encourage students to get\nactive in the discussion forum and remind them of approaching deadlines. While\nthese actions are most likely helpful, their actual impact is often not\nmeasured. Additionally, it is uncertain whether such generic approaches\nsometimes cause the opposite effect, as some participants are bothered with\nirrelevant information. On the basis of fine granular events emitted by our\nlearning platform, we derive metrics and enable teachers to employ clustering,\nin order to divide the vast field of participants into meaningful subgroups to\nbe addressed individually.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 09:59:03 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Teusner", "Ralf", ""], ["Rollmann", "Kai-Adrian", ""], ["Renz", "Jan", ""]]}, {"id": "1809.08888", "submitter": "Anca Dumitrache", "authors": "Anca Dumitrache, Oana Inel, Benjamin Timmermans, Carlos Ortiz,\n  Robert-Jan Sips, Lora Aroyo, Chris Welty", "title": "Empirical Methodology for Crowdsourcing Ground Truth", "comments": "in publication at the Semantic Web Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The process of gathering ground truth data through human annotation is a\nmajor bottleneck in the use of information extraction methods for populating\nthe Semantic Web. Crowdsourcing-based approaches are gaining popularity in the\nattempt to solve the issues related to volume of data and lack of annotators.\nTypically these practices use inter-annotator agreement as a measure of\nquality. However, in many domains, such as event detection, there is ambiguity\nin the data, as well as a multitude of perspectives of the information\nexamples. We present an empirically derived methodology for efficiently\ngathering of ground truth data in a diverse set of use cases covering a variety\nof domains and annotation tasks. Central to our approach is the use of\nCrowdTruth metrics that capture inter-annotator disagreement. We show that\nmeasuring disagreement is essential for acquiring a high quality ground truth.\nWe achieve this by comparing the quality of the data aggregated with CrowdTruth\nmetrics with majority vote, over a set of diverse crowdsourcing tasks: Medical\nRelation Extraction, Twitter Event Identification, News Event Extraction and\nSound Interpretation. We also show that an increased number of crowd workers\nleads to growth and stabilization in the quality of annotations, going against\nthe usual practice of employing a small number of annotators.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 13:04:56 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Dumitrache", "Anca", ""], ["Inel", "Oana", ""], ["Timmermans", "Benjamin", ""], ["Ortiz", "Carlos", ""], ["Sips", "Robert-Jan", ""], ["Aroyo", "Lora", ""], ["Welty", "Chris", ""]]}, {"id": "1809.08893", "submitter": "Faruk Diblen", "authors": "Faruk Diblen, Jisk Attema, Rena Bakhshi, Sascha Caron, Luc Hendriks,\n  Bob Stienen", "title": "SPOT: Open Source framework for scientific data repository and\n  interactive visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SPOT is an open source and free visual data analytics tool for\nmulti-dimensional data-sets. Its web-based interface allows a quick analysis of\ncomplex data interactively. The operations on data such as aggregation and\nfiltering are implemented. The generated charts are responsive and OpenGL\nsupported. It follows FAIR principles to allow reuse and comparison of the\npublished data-sets. The software also support PostgreSQL database for\nscalability.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 18:04:32 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Diblen", "Faruk", ""], ["Attema", "Jisk", ""], ["Bakhshi", "Rena", ""], ["Caron", "Sascha", ""], ["Hendriks", "Luc", ""], ["Stienen", "Bob", ""]]}, {"id": "1809.08927", "submitter": "Zixing Zhang", "authors": "Jing Han, Zixing Zhang, Nicholas Cummins, and Bj\\\"orn Schuller", "title": "Adversarial Training in Affective Computing and Sentiment Analysis:\n  Recent Advances and Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, adversarial training has become an extremely active\nresearch topic and has been successfully applied to various Artificial\nIntelligence (AI) domains. As a potentially crucial technique for the\ndevelopment of the next generation of emotional AI systems, we herein provide a\ncomprehensive overview of the application of adversarial training to affective\ncomputing and sentiment analysis. Various representative adversarial training\nalgorithms are explained and discussed accordingly, aimed at tackling diverse\nchallenges associated with emotional AI systems. Further, we highlight a range\nof potential future research directions. We expect that this overview will help\nfacilitate the development of adversarial training for affective computing and\nsentiment analysis in both the academic and industrial communities.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 08:27:01 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Han", "Jing", ""], ["Zhang", "Zixing", ""], ["Cummins", "Nicholas", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1809.09293", "submitter": "Vaneet Aggarwal", "authors": "Vaneet Aggarwal and Hamed Asadi and Mayank Gupta and Jae Joong Lee and\n  Denny Yu", "title": "Covfefe: A Computer Vision Approach For Estimating Force Exertion", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cumulative exposure to repetitive and forceful activities may lead to\nmusculoskeletal injuries which not only reduce workers' efficiency and\nproductivity, but also affect their quality of life. Thus, widely accessible\ntechniques for reliable detection of unsafe muscle force exertion levels for\nhuman activity is necessary for their well-being. However, measurement of force\nexertion levels is challenging and the existing techniques pose a great\nchallenge as they are either intrusive, interfere with human-machine interface,\nand/or subjective in the nature, thus are not scalable for all workers. In this\nwork, we use face videos and the photoplethysmography (PPG) signals to classify\nforce exertion levels of 0\\%, 50\\%, and 100\\% (representing rest, moderate\neffort, and high effort), thus providing a non-intrusive and scalable approach.\nEfficient feature extraction approaches have been investigated, including\nstandard deviation of the movement of different landmarks of the face,\ndistances between peaks and troughs in the PPG signals. We note that the PPG\nsignals can be obtained from the face videos, thus giving an efficient\nclassification algorithm for the force exertion levels using face videos. Based\non the data collected from 20 subjects, features extracted from the face videos\ngive 90\\% accuracy in classification among the 100\\% and the combination of 0\\%\nand 50\\% datasets. Further combining the PPG signals provide 81.7\\% accuracy.\nThe approach is also shown to be robust to the correctly identify force level\nwhen the person is talking, even though such datasets are not included in the\ntraining.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 02:45:19 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Aggarwal", "Vaneet", ""], ["Asadi", "Hamed", ""], ["Gupta", "Mayank", ""], ["Lee", "Jae Joong", ""], ["Yu", "Denny", ""]]}, {"id": "1809.09328", "submitter": "Jevin West", "authors": "Carl T. Bergstrom and Jevin D. West", "title": "Why scatter plots suggest causality, and what we can do about it", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scatter plots carry an implicit if subtle message about causality. Whether we\nlook at functions of one variable in pure mathematics, plots of experimental\nmeasurements as a function of the experimental conditions, or scatter plots of\npredictor and response variables, the value plotted on the vertical axis is by\nconvention assumed to be determined or influenced by the value on the\nhorizontal axis. This is a problem for the public understanding of scientific\nresults and perhaps also for professional scientists' interpretations of\nscatter plots. To avoid suggesting a causal relationship between the x and y\nvalues in a scatter plot, we propose a new type of data visualization, the\ndiamond plot. Diamond plots are essentially 45 degree rotations of ordinary\nscatter plots; by visually jarring the viewer they clearly indicate that she\nshould not draw the usual distinction between independent/predictor variable\nand dependent/response variable. Instead, she should see the relationship as\npurely correlative.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 05:43:56 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Bergstrom", "Carl T.", ""], ["West", "Jevin D.", ""]]}, {"id": "1809.09417", "submitter": "Miriah Meyer", "authors": "Jason Dykes and Miriah Meyer", "title": "Reflection On Reflection In Design Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visualization design study research methodologies emphasize the need for\nreflection to generate knowledge. And yet, there is very little guidance in the\nliterature specifying what reflection in the context of design studies actually\ninvolves. We initiated a community discussion on this topic through a panel at\nthe 2017 IEEE VIS Conference - this report documents the panel discussion. We\nanalyze the panel content through the lense of our own reflective experiences\nand propose several priorities for ongoing thinking on reflection in applied\nvisualization research.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 11:44:27 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Dykes", "Jason", ""], ["Meyer", "Miriah", ""]]}, {"id": "1809.09664", "submitter": "Alvitta Ottley", "authors": "Ran Wan, Roman Garnett, and Alvitta Ottley", "title": "Learning and Anticipating Future Actions During Exploratory Data\n  Analysis", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of visual analytics is to create a symbiosis between human and\ncomputer by leveraging their unique strengths. While this model has\ndemonstrated immense success, we are yet to realize the full potential of such\na human-computer partnership. In a perfect collaborative mixed-initiative\nsystem, the computer must possess skills for learning and anticipating the\nusers' needs. Addressing this gap, we propose a framework for inferring focus\nareas from passive observations of the user's actions, thereby allowing\naccurate predictions of future events. We evaluate this technique with a crime\nmap and demonstrate that users' clicks appear in our prediction set 95% - 97%\nof the time. Further analysis shows that we can achieve high prediction\naccuracy typically after three clicks. Altogether, we show that passive\nobservations of interaction data can reveal valuable information that will\nallow the system to learn and anticipate future events, laying the foundation\nfor next-generation tools.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 19:03:28 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Wan", "Ran", ""], ["Garnett", "Roman", ""], ["Ottley", "Alvitta", ""]]}, {"id": "1809.09738", "submitter": "Lucy Fortson", "authors": "Lucy Fortson, Darryl Wright, Chris Lintott, Laura Trouille", "title": "Optimizing the Human-Machine Partnership with Zooniverse", "comments": "3 pages, 1 figure, proceedings for 2018 ACM Collective Intelligence\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, Citizen Science has become a proven method of\ndistributed data analysis, enabling research teams from diverse domains to\nsolve problems involving large quantities of data with complexity levels which\nrequire human pattern recognition capabilities. With over 120 projects built\nreaching nearly 1.7 million volunteers, the Zooniverse.org platform has led the\nway in the application of Citizen Science as a method for closing the Big Data\nanalysis gap. Since the launch in 2007 of the Galaxy Zoo project, the\nZooniverse platform has enabled significant contributions across many\ndisciplines; e.g., in ecology, humanities, and astronomy. Citizen science as an\napproach to Big Data combines the twin advantages of the ability to scale\nanalysis to the size of modern datasets with the ability of humans to make\nserendipitous discoveries. To cope with the larger datasets looming on the\nhorizon such as astronomy's Large Synoptic Survey Telescope (LSST) or the 100's\nof TB from ecology projects annually, Zooniverse has been researching a system\ndesign that is optimized for efficiency in task assignment and incorporating\nhuman and machine classifiers into the classification engine. By making\nefficient use of smart task assignment and the combination of human and machine\nclassifiers, we can achieve greater accuracy and flexibility than has been\npossible to date. We note that creating the most efficient system must consider\nhow best to engage and retain volunteers as well as make the most efficient use\nof their classifications. Our work thus focuses on understanding the factors\nthat optimize efficiency of the combined human-machine system. This paper\nsummarizes some of our research to date on integration of machine learning with\nZooniverse, while also describing new infrastructure developed on the\nZooniverse platform to carry out this research.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 21:55:56 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Fortson", "Lucy", ""], ["Wright", "Darryl", ""], ["Lintott", "Chris", ""], ["Trouille", "Laura", ""]]}, {"id": "1809.09745", "submitter": "Nitish Nag", "authors": "Nitish Nag, Vaibhav Pandey, Aishwarya Manjunath, Avinash Vaka, Ramesh\n  Jain", "title": "Surface Type Estimation from GPS Tracked Bicycle Activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road conditions affect both machine and human powered modes of\ntransportation. In the case of human powered transportation, poor road\nconditions increase the work for the individual to travel. Previous estimates\nfor these parameters have used computationally expensive analysis of satellite\nimages. In this work, we use a computationally inexpensive and simple method by\nusing only GPS data from a human powered cyclist. By estimating if the road\ntaken by the user has high or low variations in their directional vector, we\nclassify if the user is on a paved road or on an unpaved trail. In order to do\nthis, three methods were adopted, changes in frequency of the direction of\nslope in a given path segment, fitting segments of the path, and finding the\nfirst derivative and the number of points of zero crossings of each segment.\nMachine learning models such as support vector machines, K-nearest neighbors,\nand decision trees were used for the classification of the path. We show in our\nmethods, the decision trees performed the best with an accuracy of 86\\%.\nEstimation of the type of surface can be used for many applications such as\nunderstanding rolling resistance for power estimation estimation or building\nexercise recommendation systems by user profiling as described in detail in the\npaper.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 22:24:12 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Nag", "Nitish", ""], ["Pandey", "Vaibhav", ""], ["Manjunath", "Aishwarya", ""], ["Vaka", "Avinash", ""], ["Jain", "Ramesh", ""]]}, {"id": "1809.09846", "submitter": "Bing Zhai", "authors": "Bing Zhai, Stuart Nicholson, Kyle Montague, Yu Guan, Patrick Olivier,\n  Jason Ellis", "title": "Co-sleep: Designing a workplace-based wellness program for sleep\n  deprivation", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sleep deprivation is a public health issue. Awareness of sleep deprivation\nhas not been widely investigated in workplace-based wellness programmes. This\nstudy adopted a three-stage design process with nine participants from a local\nmanufacturing company to help raise awareness of sleep deprivation. The common\ncauses of sleep deprivation were identified through the deployment of\ntechnology probes and participant interviews. The study contributes smart\nInternet of things(IoT) workplace-based design concepts for activity tracking\nthat may aid sleep and explore ways of sharing personal sleep data within the\nworkplace. Through the use of co-design methods, the study also highlights\nprominent privacy concerns relating to use of personal data from different\nstakeholders' perspectives, including the unexpected use of sleep data by\norganisations for fatigue risk management and the evaluation of employee\nperformance. The Actigrahy and sleep diary data can be accessed online through\nhttps://github.com/famousgrouse/pervasivehealth/\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 08:32:58 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 21:40:24 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Zhai", "Bing", ""], ["Nicholson", "Stuart", ""], ["Montague", "Kyle", ""], ["Guan", "Yu", ""], ["Olivier", "Patrick", ""], ["Ellis", "Jason", ""]]}, {"id": "1809.09948", "submitter": "Ozan Ozdenizci", "authors": "Ozan Ozdenizci, Catalina Cumpanasoiu, Carla Mazefsky, Matthew Siegel,\n  Deniz Erdogmus, Stratis Ioannidis, Matthew S. Goodwin", "title": "Time-Series Prediction of Proximal Aggression Onset in Minimally-Verbal\n  Youth with Autism Spectrum Disorder Using Physiological Biosignals", "comments": "40th International Engineering in Medicine and Biology Conference\n  (EMBC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been suggested that changes in physiological arousal precede\npotentially dangerous aggressive behavior in youth with autism spectrum\ndisorder (ASD) who are minimally verbal (MV-ASD). The current work tests this\nhypothesis through time-series analyses on biosignals acquired prior to\nproximal aggression onset. We implement ridge-regularized logistic regression\nmodels on physiological biosensor data wirelessly recorded from 15 MV-ASD youth\nover 64 independent naturalistic observations in a hospital inpatient unit. Our\nresults demonstrate proof-of-concept, feasibility, and incipient validity\npredicting aggression onset 1 minute before it occurs using global,\nperson-dependent, and hybrid classifier models.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 02:50:32 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Ozdenizci", "Ozan", ""], ["Cumpanasoiu", "Catalina", ""], ["Mazefsky", "Carla", ""], ["Siegel", "Matthew", ""], ["Erdogmus", "Deniz", ""], ["Ioannidis", "Stratis", ""], ["Goodwin", "Matthew S.", ""]]}, {"id": "1809.10266", "submitter": "John Frens", "authors": "John Frens, Erin Walker, Gary Hsieh", "title": "Supporting Answerers with Feedback in Social Q&A", "comments": "Published in Proceedings of the Fifth Annual ACM Conference on\n  Learning at Scale, Article No. 10, London, United Kingdom. June 26 - 28, 2018", "journal-ref": null, "doi": "10.1145/3231644.3231653", "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior research has examined the use of Social Question and Answer (Q&A)\nwebsites for answer and help seeking. However, the potential for these websites\nto support domain learning has not yet been realized. Helping users write\neffective answers can be beneficial for subject area learning for both\nanswerers and the recipients of answers. In this study, we examine the utility\nof crowdsourced, criteria-based feedback for answerers on a student-centered\nQ&A website, Brainly.com. In an experiment with 55 users, we compared\nperceptions of the current rating system against two feedback designs with\nexplicit criteria (Appropriate, Understandable, and Generalizable). Contrary to\nour hypotheses, answerers disagreed with and rejected the criteria-based\nfeedback. Although the criteria aligned with answerers' goals, and crowdsourced\nratings were found to be objectively accurate, the norms and expectations for\nanswers on Brainly conflicted with our design. We conclude with implications\nfor the design of feedback in social Q&A.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 23:36:53 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Frens", "John", ""], ["Walker", "Erin", ""], ["Hsieh", "Gary", ""]]}, {"id": "1809.10782", "submitter": "Dylan Cashman", "authors": "Dylan Cashman (1), Shah Rukh Humayoun (1), Florian Heimerl (2),\n  Kendall Park (2), Subhajit Das (3), John Thompson (3), Bahador Saket (3),\n  Abigail Mosca (1), John Stasko (3), Alex Endert (3), Michael Gleicher (2),\n  Remco Chang (1) ((1) Tufts University, (2) University of Wisconsin - Madison,\n  (3) Georgia Tech)", "title": "A User-based Visual Analytics Workflow for Exploratory Model Analysis", "comments": null, "journal-ref": "Computer Graphics Forum 38(3) 2019, The Eurographics Association\n  and John Wiley & Sons Ltd", "doi": "10.1111/cgf.13681", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many visual analytics systems allow users to interact with machine learning\nmodels towards the goals of data exploration and insight generation on a given\ndataset. However, in some situations, insights may be less important than the\nproduction of an accurate predictive model for future use. In that case, users\nare more interested in generating of diverse and robust predictive models,\nverifying their performance on holdout data, and selecting the most suitable\nmodel for their usage scenario. In this paper, we consider the concept of\nExploratory Model Analysis (EMA), which is defined as the process of\ndiscovering and selecting relevant models that can be used to make predictions\non a data source. We delineate the differences between EMA and the well-known\nterm exploratory data analysis in terms of the desired outcome of the analytic\nprocess: insights into the data or a set of deployable models. The\ncontributions of this work are a visual analytics system workflow for EMA, a\nuser study, and two use cases validating the effectiveness of the workflow. We\nfound that our system workflow enabled users to generate complex models, to\nassess them for various qualities, and to select the most relevant model for\ntheir task.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 22:18:51 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 17:15:16 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 16:08:41 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Cashman", "Dylan", ""], ["Humayoun", "Shah Rukh", ""], ["Heimerl", "Florian", ""], ["Park", "Kendall", ""], ["Das", "Subhajit", ""], ["Thompson", "John", ""], ["Saket", "Bahador", ""], ["Mosca", "Abigail", ""], ["Stasko", "John", ""], ["Endert", "Alex", ""], ["Gleicher", "Michael", ""], ["Chang", "Remco", ""]]}, {"id": "1809.10944", "submitter": "Jun Zhao Dr", "authors": "Jun Zhao", "title": "Are Children Well-Supported by Their Parents Concerning Online Privacy\n  Risks, and Who Supports the Parents?", "comments": "20 pages, 1 figure, report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Tablet computers are becoming ubiquitously available at home or school for\nyoung children to complement education or entertainment. However, parents of\nchildren aged 6-11 often believe that children are too young to face or\ncomprehend online privacy issues, and often take a protective approach to\nrestrict or monitor what children can access online, instead of discussing\nprivacy issues with children. Parents work hard to protect their children's\nonline safety. However, little is known how much parents are aware of the risks\nassociated with the implicit personal data collection by the first- or\nthird-party companies behind the mobile `apps' used by their children, and\nhence how well parents can safeguard their children from this kind of risks.\n  Parents have always been playing a pivotal role in mitigating children's\ninteractions with digital technologies --- from TV to game consoles, to\npersonal computers --- but the rapidly changing technologies are posing\nchallenges for parents to keep up with. There is a pressing need to understand\nhow much parents are aware of privacy risks concerning the use of tablets and\nhow they are managing them for their primary school-aged young children. At the\nsame time, we must also reach out to the children themselves, who are on the\nfrontline of these technologies, to learn how capable they are to recognise\nrisks and how well they are supported by their parents to cope with these\nrisks. Therefore, in the summer of 2017, we conducted face-to-face interviews\nwith 12 families in Oxfordshire and an online survey with 250 parents. This\nreport summarises our key findings of these two studies.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 10:17:44 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Zhao", "Jun", ""]]}]