[{"id": "2011.00101", "submitter": "Dongrui Wu", "authors": "Lubin Meng, Jian Huang, Zhigang Zeng, Xue Jiang, Shan Yu, Tzyy-Ping\n  Jung, Chin-Teng Lin, Ricardo Chavarriaga, Dongrui Wu", "title": "EEG-Based Brain-Computer Interfaces Are Vulnerable to Backdoor Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research and development of electroencephalogram (EEG) based brain-computer\ninterfaces (BCIs) have advanced rapidly, partly due to deeper understanding of\nthe brain and wide adoption of sophisticated machine learning approaches for\ndecoding the EEG signals. However, recent studies have shown that machine\nlearning algorithms are vulnerable to adversarial attacks. This article\nproposes to use narrow period pulse for poisoning attack of EEG-based BCIs,\nwhich is implementable in practice and has never been considered before. One\ncan create dangerous backdoors in the machine learning model by injecting\npoisoning samples into the training set. Test samples with the backdoor key\nwill then be classified into the target class specified by the attacker. What\nmost distinguishes our approach from previous ones is that the backdoor key\ndoes not need to be synchronized with the EEG trials, making it very easy to\nimplement. The effectiveness and robustness of the backdoor attack approach is\ndemonstrated, highlighting a critical security concern for EEG-based BCIs and\ncalling for urgent attention to address it.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 20:49:42 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 23:16:26 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Meng", "Lubin", ""], ["Huang", "Jian", ""], ["Zeng", "Zhigang", ""], ["Jiang", "Xue", ""], ["Yu", "Shan", ""], ["Jung", "Tzyy-Ping", ""], ["Lin", "Chin-Teng", ""], ["Chavarriaga", "Ricardo", ""], ["Wu", "Dongrui", ""]]}, {"id": "2011.00165", "submitter": "Esmaeil Seraj", "authors": "Esmaeil Seraj, Xiyang Wu, Matthew Gombolay", "title": "FireCommander: An Interactive, Probabilistic Multi-agent Environment for\n  Joint Perception-Action Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this tutorial is to help individuals use the\n\\underline{FireCommander} game environment for research applications. The\nFireCommander is an interactive, probabilistic joint perception-action\nreconnaissance environment in which a composite team of agents (e.g., robots)\ncooperate to fight dynamic, propagating firespots (e.g., targets). In\nFireCommander game, a team of agents must be tasked to optimally deal with a\nwildfire situation in an environment with propagating fire areas and some\nfacilities such as houses, hospitals, power stations, etc. The team of agents\ncan accomplish their mission by first sensing (e.g., estimating fire states),\ncommunicating the sensed fire-information among each other and then taking\naction to put the firespots out based on the sensed information (e.g., dropping\nwater on estimated fire locations). The FireCommander environment can be useful\nfor research topics spanning a wide range of applications from Reinforcement\nLearning (RL) and Learning from Demonstration (LfD), to Coordination,\nPsychology, Human-Robot Interaction (HRI) and Teaming. There are four important\nfacets of the FireCommander environment that overall, create a non-trivial\ngame: (1) Complex Objectives: Multi-objective Stochastic Environment,\n(2)Probabilistic Environment: Agents' actions result in probabilistic\nperformance, (3) Hidden Targets: Partially Observable Environment and, (4)\nUni-task Robots: Perception-only and Action-only agents. The FireCommander\nenvironment is first-of-its-kind in terms of including Perception-only and\nAction-only agents for coordination. It is a general multi-purpose game that\ncan be useful in a variety of combinatorial optimization problems and\nstochastic games, such as applications of Reinforcement Learning (RL), Learning\nfrom Demonstration (LfD) and Inverse RL (iRL).\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 02:06:07 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Seraj", "Esmaeil", ""], ["Wu", "Xiyang", ""], ["Gombolay", "Matthew", ""]]}, {"id": "2011.00191", "submitter": "Hancheng Cao", "authors": "Hancheng Cao, Zhilong Chen, Mengjie Cheng, Shuling Zhao, Tao Wang,\n  Yong Li", "title": "You Recommend, I Buy: How and Why People Engage in Instant Messaging\n  Based Social Commerce", "comments": "Hancheng Cao and Zhilong Chen contributed equally to this work;\n  Accepted to CSCW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emerging business phenomenon especially in China, instant messaging\n(IM) based social commerce is growing increasingly popular, attracting hundreds\nof millions of users and is becoming one important way where people make\neveryday purchases. Such platforms embed shopping experiences within IM apps,\ne.g., WeChat, WhatsApp, where real-world friends post and recommend products\nfrom the platforms in IM group chats and quite often form lasting\nrecommending/buying relationships. How and why do users engage in IM based\nsocial commerce? Do such platforms create novel experiences that are distinct\nfrom prior commerce? And do these platforms bring changes to user social lives\nand relationships? To shed light on these questions, we launched a qualitative\nstudy where we carried out semi-structured interviews on 12 instant messaging\nbased social commerce users in China. We showed that IM based social commerce:\n1) enables more reachable, cost-reducing, and immersive user shopping\nexperience, 2) shapes user decision-making process in shopping through\npre-existing social relationship, mutual trust, shared identity, and community\nnorm, and 3) creates novel social interactions, which can contribute to new tie\nformation while maintaining existing social relationships. We demonstrate that\nall these unique aspects link closely to the characteristics of IM platforms,\nas well as the coupling of user social and economic lives under such business\nmodel. Our study provides important research and design implications for social\ncommerce, and decentralized, trusted socio-technical systems in general.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 05:21:59 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 05:42:09 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Cao", "Hancheng", ""], ["Chen", "Zhilong", ""], ["Cheng", "Mengjie", ""], ["Zhao", "Shuling", ""], ["Wang", "Tao", ""], ["Li", "Yong", ""]]}, {"id": "2011.00264", "submitter": "Sebastian Cmentowski", "authors": "Sebastian Cmentowski and Jens Kr\\\"uger", "title": "Playing With Friends -- The Importance of Social Play During the\n  COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": "10.1145/3383668.3419911", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In early 2020, the virus SARS-CoV-2 evolved into a new pandemic, forcing\ngovernments worldwide to establish social distancing measures. Consequently,\npeople had to switch to online media, such as social networks or\nvideotelephony, to keep in touch with friends and family. In this context,\nonline games, combining entertainment with social interactions, also\nexperienced a notable growth. In our work, we focused on the potential of games\nas a replacement for social contacts in the COVID-19 crisis. Our online survey\nresults indicate that the value of games for social needs depends on individual\ngaming habits. Participants playing mostly multiplayer games increased their\nplaytime and mentioned social play as a key motivator. Contrarily, non-players\nwere not motivated to add games as communication channels. We deduce that such\ncrises mainly catalyze existing gaming habits.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 13:14:28 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Cmentowski", "Sebastian", ""], ["Kr\u00fcger", "Jens", ""]]}, {"id": "2011.00329", "submitter": "Zona Kostic", "authors": "Zona Kostic, Jared Jessup, Jeffrey Baglioni, Nathan Weeks, Johann\n  Philipp Dreessen, Ning Chen, Tianyu Liu", "title": "Visual Companion for Booklovers", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An innumerable number of individual choices go into discovering a new book.\nThere are unmistakably two groups of booklovers: those who like to search\nonline, follow other people's latest readings, or simply react to a system's\nrecommendations; and those who love to wander between library stacks, lose\nthemselves behind bookstore shelves, or simply hide behind piles of\n(un)organized books. Depending on which group a person may fall into, there are\ntwo distinct and corresponding mediums that inform his or her choices: digital,\nthat provides efficient retrieval of information online, and physical, a more\ntactile pursuit that leads to unexpected discoveries and promotes serendipity.\nHow could we possibly bridge the gap between these seemingly disparate mediums\ninto an integrated system that can amplify the benefits they both offer? In\nthis paper, we present the BookVIS application, which uses book-related data\nand generates personalized visualizations to follow users in their quest for a\nnew book. In this new redesigned version, the app brings associative visual\nconnections to support intuitive exploration of easily retrieved digital\ninformation and its relationship with the physical book in hand. BookVIS keeps\ntrack of the user's reading preferences and generates a dataSelfie as an\nindividual snapshot of a personal taste that grows over time. Usability testing\nhas also been conducted and has demonstrated the app's ability to identify\ndistinguishable patterns in readers' tastes that could be further used to\ncommunicate personal preferences in new \"shelf-browsing\" iterations. By\nefficiently supplementing the user's cognitive information needs while still\nsupporting the spontaneity and enjoyment of the book browsing experience,\nBookVIS bridges the gap between real and online realms, and maximizes the\nengagement of personalized mobile visual clues.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 18:16:50 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kostic", "Zona", ""], ["Jessup", "Jared", ""], ["Baglioni", "Jeffrey", ""], ["Weeks", "Nathan", ""], ["Dreessen", "Johann Philipp", ""], ["Chen", "Ning", ""], ["Liu", "Tianyu", ""]]}, {"id": "2011.00553", "submitter": "Guoliang Liu Prof. Dr.", "authors": "Guoliang Liu, Qinghui Zhang, Yichao Cao, Junwei Li, Hao Wu and Guohui\n  Tian", "title": "Memory Group Sampling Based Online Action Recognition Using Kinetic\n  Skeleton Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online action recognition is an important task for human centered intelligent\nservices, which is still difficult to achieve due to the varieties and\nuncertainties of spatial and temporal scales of human actions. In this paper,\nwe propose two core ideas to handle the online action recognition problem.\nFirst, we combine the spatial and temporal skeleton features to depict the\nactions, which include not only the geometrical features, but also multi-scale\nmotion features, such that both the spatial and temporal information of the\naction are covered. Second, we propose a memory group sampling method to\ncombine the previous action frames and current action frames, which is based on\nthe truth that the neighbouring frames are largely redundant, and the sampling\nmechanism ensures that the long-term contextual information is also considered.\nFinally, an improved 1D CNN network is employed for training and testing using\nthe features from sampled frames. The comparison results to the state of the\nart methods using the public datasets show that the proposed method is fast and\nefficient, and has competitive performance\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 16:43:08 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 05:09:10 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Liu", "Guoliang", ""], ["Zhang", "Qinghui", ""], ["Cao", "Yichao", ""], ["Li", "Junwei", ""], ["Wu", "Hao", ""], ["Tian", "Guohui", ""]]}, {"id": "2011.00665", "submitter": "Tadashi Okoshi", "authors": "Tadashi Okoshi, Wataru Sasaki, Hiroshi Kawane, Kota Tsubouchi", "title": "NationalMood: Large-scale Estimation of People's Mood from Web Search\n  Query and Mobile Sensor Data", "comments": "submitted to The Web Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to estimate current affective statuses of web users has\nconsiderable potential towards the realization of user-centric opportune\nservices. However, determining the type of data to be used for such estimation\nas well as collecting the ground truth of such affective statuses are difficult\nin the real world situation. We propose a novel way of such estimation based on\na combinational use of user's web search queries and mobile sensor data. Our\nlarge-scale data analysis with about 11,000,000 users and 100 recent\nadvertisement log revealed (1) the existence of certain class of advertisement\nto which mood-status-based delivery would be significantly effective, (2) that\nour \"National Mood Score\" shows the ups and downs of people's moods in COVID-19\npandemic that inversely correlated to the number of patients, as well as the\nweekly mood rhythm of people.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 01:15:04 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 02:37:02 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Okoshi", "Tadashi", ""], ["Sasaki", "Wataru", ""], ["Kawane", "Hiroshi", ""], ["Tsubouchi", "Kota", ""]]}, {"id": "2011.00733", "submitter": "Sergey Alyaev", "authors": "Sergey Alyaev, Reidar Brumer Bratvold, Sofija Ivanova, Andrew\n  Holsaeter, Morten Bendiksen", "title": "An interactive sequential-decision benchmark from geosteering", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.08916", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geosteering workflows are increasingly based on the quantification of\nsubsurface uncertainties during real-time operations. As a consequence\noperational decision making is becoming both better informed and more complex.\nThis paper presents an experimental web-based decision support system, which\ncan be used to both aid expert decisions under uncertainty or further develop\ndecision optimization algorithms in controlled environment. A user of the\nsystem (either human or AI) controls the decisions to steer the well or stop\ndrilling. Whenever a user drills ahead, the system produces simulated\nmeasurements along the selected well trajectory which are used to update the\nuncertainty represented by model realizations using the ensemble Kalman filter.\nTo enable informed decisions the system is equipped with functionality to\nevaluate the value of the selected trajectory under uncertainty with respect to\nthe objectives of the current experiment.\n  To illustrate the utility of the system as a benchmark, we present the\ninitial experiment, in which we compare the decision skills of geoscientists\nwith those of a recently published automatic decision support algorithm. The\nexperiment and the survey after it showed that most participants were able to\nuse the interface and complete the three test rounds. At the same time, the\nautomated algorithm outperformed 28 out of 29 qualified human participants.\n  Such an experiment is not sufficient to draw conclusions about practical\ngeosteering, but is nevertheless useful for geoscience. First, this\ncommunication-by-doing made 76% of respondents more curious about and/or\nconfident in the presented technologies. Second, the system can be further used\nas a benchmark for sequential decisions under uncertainty. This can accelerate\ndevelopment of algorithms and improve the training for decision making.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 12:54:11 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Alyaev", "Sergey", ""], ["Bratvold", "Reidar Brumer", ""], ["Ivanova", "Sofija", ""], ["Holsaeter", "Andrew", ""], ["Bendiksen", "Morten", ""]]}, {"id": "2011.00876", "submitter": "Berkay Kopru", "authors": "Berkay K\\\"opr\\\"u, Engin Erzin", "title": "Multimodal Continuous Emotion Recognition using Deep Multi-Task Learning\n  with Correlation Loss", "comments": "6 pages,letter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we focus on continuous emotion recognition using body motion\nand speech signals to estimate Activation, Valence, and Dominance (AVD)\nattributes. Semi-End-To-End network architecture is proposed where both\nextracted features and raw signals are fed, and this network is trained using\nmulti-task learning (MTL) rather than the state-of-the-art single task learning\n(STL). Furthermore, correlation losses, Concordance Correlation Coefficient\n(CCC) and Pearson Correlation Coefficient (PCC), are used as an optimization\nobjective during the training. Experiments are conducted on CreativeIT and\nRECOLA database, and evaluations are performed using the CCC metric. To\nhighlight the effect of MTL, correlation losses and multi-modality, we\nrespectively compare the performance of MTL against STL, CCC loss against root\nmean square error (MSE) loss and, PCC loss, multi-modality against single\nmodality. We observe significant performance improvements with MTL training\nover STL, especially for estimation of the valence. Furthermore, the CCC loss\nachieves more than 7% CCC improvements on CreativeIT, and 13% improvements on\nRECOLA against MSE loss.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 10:30:22 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["K\u00f6pr\u00fc", "Berkay", ""], ["Erzin", "Engin", ""]]}, {"id": "2011.00895", "submitter": "Leonard G\\\"oke", "authors": "Leonard G\\\"oke", "title": "AnyMOD.jl: A Julia package for creating energy system models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AnyMOD.jl is a Julia framework for creating large-scale energy system models\nwith multiple periods of capacity expansion. It applies a novel graph-based\napproach that was developed to address the challenges in modeling high levels\nof intermittent generation and sectoral integration. Created models are\nformulated as linear optimization problems using JuMP.jl as a backend. To\nenable modelers to work more efficiently, the framework provides additional\nfeatures that help to visualize results, streamline the read-in of input data,\nand rescale optimization problems to increase solver performance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 11:14:59 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["G\u00f6ke", "Leonard", ""]]}, {"id": "2011.00958", "submitter": "Anton Smerdov", "authors": "Anton Smerdov, Bo Zhou, Paul Lukowicz, Andrey Somov", "title": "Collection and Validation of Psycophysiological Data from Professional\n  and Amateur Players: a Multimodal eSports Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper training and analytics in eSports require accurately collected and\nannotated data. Most eSports research focuses exclusively on in-game data\nanalysis, and there is a lack of prior work involving eSports athletes'\npsychophysiological data. In this paper, we present a dataset collected from\nprofessional and amateur teams in 22 matches in League of Legends video game.\nRecorded data include the players' physiological activity, e.g. movements,\npulse, saccades, obtained from various sensors, self-reported after-match\nsurvey, and in-game data. An important feature of the dataset is simultaneous\ndata collection from five players, which facilitates the analysis of sensor\ndata on a team level. Upon the collection of dataset we carried out its\nvalidation. In particular, we demonstrate that stress and concentration levels\nfor professional players are less correlated, meaning more independent\nplaystyle. Also, we show that the absence of team communication does not affect\nthe professional players as much as amateur ones. To investigate other possible\nuse cases of the dataset, we have trained classical machine learning algorithms\nfor skill prediction and player re-identification using 3-minute sessions of\nsensor data. Best models achieved 0.856 and 0.521 (0.10 for a chance level)\naccuracy scores on a validation set for skill prediction and player re-id\nproblems, respectively. The dataset is available at\nhttps://github.com/asmerdov/eSports_Sensors_Dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:25:11 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Smerdov", "Anton", ""], ["Zhou", "Bo", ""], ["Lukowicz", "Paul", ""], ["Somov", "Andrey", ""]]}, {"id": "2011.01056", "submitter": "Basel Alhaji", "authors": "Basel Alhaji, Andreas Rausch, Michael Prilla", "title": "Toward Mutual Trust Modeling in Human-Robot Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent revolution of intelligent systems made it possible for robots and\nautonomous systems to work alongside humans, collaborating with them and\nsupporting them in many domains. It is undeniable that this interaction can\nhave huge benefits for humans if it is designed properly. However,\ncollaboration with humans requires a high level of cognition and social\ncapabilities in order to gain humans acceptance. In all-human teams, mutual\ntrust is the engine for successful collaboration. This applies to human-robot\ncollaboration as well. Trust in this interaction controls over- and\nunder-reliance. It can also mitigate some risk. Therefore, an appropriate trust\nlevel is essential for this new form of teamwork. Most research in this area\nhas looked at trust of humans in machines, neglecting the mutuality of trust\namong collaboration partners. In this paper, we propose a trust model that\nincorporates this mutuality captures trust levels of both the human and the\nrobot in real-time, so that robot can base actions on this, allowing for\nsmoother, more natural interactions. This increases the human autonomy since\nthe human does not need to monitor the robot behavior all the time.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 15:38:24 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Alhaji", "Basel", ""], ["Rausch", "Andreas", ""], ["Prilla", "Michael", ""]]}, {"id": "2011.01446", "submitter": "Mingdong Zhang", "authors": "Mingdong Zhang, Li Chen, Xiaoru Yuan, Renpei Huang, Shuang Liu, and\n  Junhai Yong", "title": "Visualization of Technical and Tactical Characteristics in Fencing", "comments": null, "journal-ref": "Journal of Visualization, 2019, 22(1): 109-124", "doi": "10.1007/s12650-018-0521-3", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fencing is a sport that relies heavily on the use of tactics. However, most\nexisting methods for analyzing fencing data are based on statistical models in\nwhich hidden patterns are difficult to discover. Unlike sequential games, such\nas tennis and table tennis, fencing is a type of simultaneous game. Thus, the\nexisting methods on the sports visualization do not operate well for fencing\nmatches. In this study, we cooperated with experts to analyze the technical and\ntactical characteristics of fencing competitions. To meet the requirements of\nthe fencing experts, we designed and implemented FencingVis, an interactive\nvisualization system for fencing competition data.The action sequences in the\nbout are first visualized by modified bar charts to reveal the actions of\nfootworks and bladeworks of both fencers. Then an interactive technique is\nprovided for exploring the patterns of behavior of fencers. The different\ncombinations of tactical behavior patterns are further mapped to the graph\nmodel and visualized by a tactical flow graph. This graph can reveal the\ndifferent strategies adopted by both fencers and their mutual influence in one\nbout. We also provided a number of well-coordinated views to supplement the\ntactical flow graph and display the information of the fencing competition from\ndifferent perspectives. The well-coordinated views are meant to organically\nintegrate with the tactical flow graph through consistent visual style and view\ncoordination. We demonstrated the usability and effectiveness of the proposed\nsystem with three case studies. On the basis of expert feedback, FencingVis can\nhelp analysts find not only the tactical patterns hidden in fencing bouts, but\nalso the technical and tactical characteristics of the contestant.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 03:26:10 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zhang", "Mingdong", ""], ["Chen", "Li", ""], ["Yuan", "Xiaoru", ""], ["Huang", "Renpei", ""], ["Liu", "Shuang", ""], ["Yong", "Junhai", ""]]}, {"id": "2011.01644", "submitter": "Juan Quintero", "authors": "Juan Quintero and Zinaida Benenson", "title": "Understanding Usability and User Acceptance of Usage-Based Insurance\n  from Users' View", "comments": null, "journal-ref": null, "doi": "10.1145/3366750.3366759", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent Transportation Systems (ITS) cover a variety of services related\nto topics such as traffic control and safe driving, among others. In the\ncontext of car insurance, a recent application for ITS is known as Usage-Based\nInsurance (UBI). UBI refers to car insurance policies that enable insurance\ncompanies to collect individual driving data using a telematics device.\nCollected data is analysed and used to offer individual discounts based on\ndriving behaviour and to provide feedback on driving performance. Although\nthere are plenty of advertising materials about the benefits of UBI, the user\nacceptance and the usability of UBI systems have not received research\nattention so far. To this end, we conduct two user studies: semi-structured\ninterviews with UBI users and a qualitative analysis of 186 customer inquiries\nfrom a web forum of a German insurance company. We find that under certain\ncircumstances, UBI provokes dangerous driving behaviour. These situations could\nbe mitigated by making UBI transparent and the feedback customisable by\ndrivers. Moreover, the country driving conditions, the policy conditions, and\nthe perceived driving style influence UBI acceptance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 11:44:27 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Quintero", "Juan", ""], ["Benenson", "Zinaida", ""]]}, {"id": "2011.01774", "submitter": "Scott Friedman", "authors": "Scott E. Friedman, Robert P. Goldman, Richard G. Freedman, Ugur Kuter,\n  Christopher Geib, Jeffrey Rye", "title": "Provenance-Based Assessment of Plans in Context", "comments": "9 pages, 7 figures, including in Proceedings of the 2020 ICAPS\n  Workshop on Explainable AI Planning (XAIP)", "journal-ref": "Proceedings of the 2020 ICAPS Workshop on Explainable AI Planning", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world planning domains involve diverse information sources,\nexternal entities, and variable-reliability agents, all of which may impact the\nconfidence, risk, and sensitivity of plans. Humans reviewing a plan may lack\ncontext about these factors; however, this information is available during the\ndomain generation, which means it can also be interwoven into the planner and\nits resulting plans. This paper presents a provenance-based approach to\nexplaining automated plans. Our approach (1) extends the SHOP3 HTN planner to\ngenerate dependency information, (2) transforms the dependency information into\nan established PROV-O representation, and (3) uses graph propagation and\nTMS-inspired algorithms to support dynamic and counter-factual assessment of\ninformation flow, confidence, and support. We qualified our approach's\nexplanatory scope with respect to explanation targets from the automated\nplanning literature and the information analysis literature, and we demonstrate\nits ability to assess a plan's pertinence, sensitivity, risk, assumption\nsupport, diversity, and relative confidence.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 15:13:54 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Friedman", "Scott E.", ""], ["Goldman", "Robert P.", ""], ["Freedman", "Richard G.", ""], ["Kuter", "Ugur", ""], ["Geib", "Christopher", ""], ["Rye", "Jeffrey", ""]]}, {"id": "2011.01931", "submitter": "Haihan Lin", "authors": "Haihan Lin, Ryan A. Metcalf, Jack Wilburn, Alexander Lex", "title": "Sanguine: Visual Analysis for Patient Blood Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Blood transfusion is a frequently performed medical procedure in surgical and\nnonsurgical contexts. Although it is frequently necessary or even life-saving,\nit has been identified as one of the most overused procedures in hospitals.\nUnnecessary transfusions not only waste resources but can also be detrimental\nto patient outcomes. Patient blood management (PBM) is the clinical practice of\noptimizing transfusions and associated outcomes. In this paper, we introduce\nSanguine, a visual analysis tool for transfusion data and related patient\nmedical records. Sanguine was designed with two user groups in mind: PBM\nexperts and clinicians who conduct transfusions. PBM experts use Sanguine to\nexplore and analyze transfusion practices and its associated medical outcomes.\nThey can compare individual surgeons, or compare outcomes or time periods, such\nas before and after an intervention regarding transfusion practices. PBM\nexperts then curate and annotate views for communication with clinicians, with\nthe goal of improving their transfusion practices. Such a review session could\nbe in person or through a shared link. We validate the utility and\neffectiveness of Sanguine through case studies.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:59:49 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 15:44:22 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 19:18:51 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lin", "Haihan", ""], ["Metcalf", "Ryan A.", ""], ["Wilburn", "Jack", ""], ["Lex", "Alexander", ""]]}, {"id": "2011.01969", "submitter": "JiHyun Jeong", "authors": "JiHyun Jeong and Guy Hoffman", "title": "Face-work for Human-Agent Joint Decision-Making", "comments": "In Proceedings of the AAAI 2020 Fall Symposium Series on Trust and\n  Explainability in Artificial Intelligence for Human-Robot Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to integrate face-work, a common social ritual related to\ntrust, into a decision-making agent that works collaboratively with a human.\nFace-work is a set of trust-building behaviors designed to \"save face\" or\nprevent others from \"losing face.\" This paper describes the design of a\ndecision-making process that explicitly considers face-work as part of its\naction selection. We also present a simulated robot arm deployed in an online\nenvironment that can be used to evaluate the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:24:06 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Jeong", "JiHyun", ""], ["Hoffman", "Guy", ""]]}, {"id": "2011.02149", "submitter": "Chuan Wang", "authors": "Chuan Wang and Kwan-Liu Ma", "title": "HypperSteer: Hypothetical Steering and Data Perturbation in Sequence\n  Prediction with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Recurrent Neural Networks (RNN) continues to find success in predictive\ndecision-making with temporal event sequences. Recent studies have shown the\nimportance and practicality of visual analytics in interpreting deep learning\nmodels for real-world applications. However, very limited work enables\ninteractions with deep learning models and guides practitioners to form\nhypotheticals towards the desired prediction outcomes, especially for sequence\nprediction. Specifically, no existing work has addressed the what-if analysis\nand value perturbation along different time-steps for sequence outcome\nprediction. We present a model-agnostic visual analytics tool, HypperSteer,\nthat steers hypothetical testing and allows users to perturb data for sequence\npredictions interactively. We showcase how HypperSteer helps in steering\npatient data to achieve desired treatment outcomes and discuss how HypperSteer\ncan serve as a comprehensive solution for other practical scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 06:26:58 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 01:55:39 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Wang", "Chuan", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2011.02340", "submitter": "Khouloud Hwerbi", "authors": "Khouloud Hwerbi", "title": "An ontology-based chatbot for crises management: use case coronavirus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today is the era of intelligence in machines. With the advances in Artificial\nIntelligence, machines have started to impersonate different human traits, a\nchatbot is the next big thing in the domain of conversational services. A\nchatbot is a virtual person who is capable to carry out a natural conversation\nwith people. They can include skills that enable them to converse with the\nhumans in audio, visual, or textual formats. Artificial intelligence\nconversational entities, also called chatbots, conversational agents, or\ndialogue system, are an excellent example of such machines. Obtaining the right\ninformation at the right time and place is the key to effective disaster\nmanagement. The term \"disaster management\" encompasses both natural and\nhuman-caused disasters. To assist citizens, our project is to create a COVID\nAssistant to provide the need of up to date information to be available 24\nhours. With the growth in the World Wide Web, it is quite intelligible that\nusers are interested in the swift and relatedly correct information for their\nhunt. A chatbot can be seen as a question-and-answer system in which experts\nprovide knowledge to solicit users. This master thesis is dedicated to discuss\nCOVID Assistant chatbot and explain each component in detail. The design of the\nproposed chatbot is introduced by its seven components: Ontology, Web Scraping\nmodule, DB, State Machine, keyword Extractor, Trained chatbot, and User\nInterface.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 09:30:51 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Hwerbi", "Khouloud", ""]]}, {"id": "2011.02418", "submitter": "David Kou\\v{r}il", "authors": "David Kou\\v{r}il, Ond\\v{r}ej Strnad, Peter Mindek, Sarkis Halladjian,\n  Tobias Isenberg, M. Eduard Gr\\\"oller, Ivan Viola", "title": "Molecumentary: Scalable Narrated Documentaries Using Molecular\n  Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a method for producing documentary-style content using real-time\nscientific visualization. We produce molecumentaries, i.e., molecular\ndocumentaries featuring structural models from molecular biology. We employ\nscalable methods instead of the rigid traditional production pipeline. Our\nmethod is motivated by the rapid evolution of interactive scientific\nvisualization, which shows great potential in science dissemination. Without\nsome form of explanation or guidance, however, novices and lay-persons often\nfind it difficult to gain insights from the visualization itself. We integrate\nsuch knowledge using the verbal channel and provide it along an engaging visual\npresentation. To realize the synthesis of a molecumentary, we provide technical\nsolutions along two major production steps: 1) preparing a story structure and\n2) turning the story into a concrete narrative. In the first step, information\nabout the model from heterogeneous sources is compiled into a story graph.\nLocal knowledge is combined with remote sources to complete the story graph and\nenrich the final result. In the second step, a narrative, i.e., story elements\npresented in sequence, is synthesized using the story graph. We present a\nmethod for traversing the story graph and generating a virtual tour, using\nautomated camera and visualization transitions. Texts written by domain experts\nare turned into verbal representations using text-to-speech functionality and\nprovided as a commentary. Using the described framework we synthesize automatic\nfly-throughs with descriptions that mimic a manually authored documentary.\nFurthermore, we demonstrate a second scenario: guiding the documentary\nnarrative by a textual input.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 17:18:26 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Kou\u0159il", "David", ""], ["Strnad", "Ond\u0159ej", ""], ["Mindek", "Peter", ""], ["Halladjian", "Sarkis", ""], ["Isenberg", "Tobias", ""], ["Gr\u00f6ller", "M. Eduard", ""], ["Viola", "Ivan", ""]]}, {"id": "2011.02555", "submitter": "Manuel Olgu\\'in Mu\\~noz", "authors": "M. Olgu\\'in Mu\\~noz (1), R. Klatzky (2), J. Wang (3), P. Pillai (4),\n  M. Satyanarayanan (3), J. Gross (1) ((1) School of Electrical Engineering and\n  Computer Science, KTH Royal Institute of Technology, (2) Department of\n  Psychology, Carnegie Mellon University, (3) School of Computer Science,\n  Carnegie Mellon University, (4) Intel Labs, Pittsburgh)", "title": "Impact of delayed response on Wearable Cognitive Assistance", "comments": "27 pages, 14 figures. Submitted to PLOS ONE on October 15, 2020", "journal-ref": null, "doi": "10.1371/journal.pone.0248690", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wearable Cognitive Assistants (WCA) are anticipated to become a widely-used\napplication class, in conjunction with emerging network infrastructures like 5G\nthat incorporate edge computing capabilities. While prototypical studies of\nsuch applications exist today, the relationship between infrastructure service\nprovisioning and its implication for WCA usability is largely unexplored\ndespite the relevance that these applications have for future networks. This\npaper presents an experimental study assessing how WCA users react to varying\nend-to-end delays induced by the application pipeline or infrastructure.\nParticipants interacted directly with an instrumented task-guidance WCA as\ndelays were introduced into the system in a controllable fashion. System and\ntask state were tracked in real time, and biometric data from wearable sensors\non the participants were recorded. Our results show that periods of extended\nsystem delay cause users to correspondingly (and substantially) slow down in\ntheir guided task execution, an effect that persists for a time after the\nsystem returns to a more responsive state. Furthermore, the slow-down in task\nexecution is correlated with a personality trait, neuroticism, associated with\nintolerance for time delays. We show that our results implicate impaired\ncognitive planning, as contrasted with resource depletion or emotional arousal,\nas the reason for slowed user task executions under system delay. The findings\nhave several implications for the design and operation of WCA applications as\nwell as computational and communication infrastructure, and additionally for\nthe development of performance analysis tools for WCA.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 22:02:37 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Mu\u00f1oz", "M. Olgu\u00edn", ""], ["Klatzky", "R.", ""], ["Wang", "J.", ""], ["Pillai", "P.", ""], ["Satyanarayanan", "M.", ""], ["Gross", "J.", ""]]}, {"id": "2011.02731", "submitter": "Lukas Hindemith", "authors": "Lukas Hindemith, Anna-Lisa Vollmer, Jan Phillip G\\\"opfert, Christiane\n  B. Wiebel-Herboth, Britta Wrede", "title": "Why robots should be technical: Correcting mental models through\n  technical architecture concepts", "comments": "Submitted to Interaction Studies Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in social robotics is commonly focused on designing robots that\nimitate human behavior. While this might increase a user's satisfaction and\nacceptance of robots at first glance, it does not automatically aid a\nnon-expert user in naturally interacting with robots, and might actually hurt\ntheir ability to correctly anticipate a robot's capabilities. We argue that a\nfaulty mental model, that the user has of the robot, is one of the main sources\nof confusion. In this work we investigate how communicating technical concepts\nof robotic systems to users affects their mental models, and how this can\nincrease the quality of human-robot interaction. We conducted an online study\nand investigated possible ways of improving users' mental models. Our results\nunderline that communicating technical concepts can form an improved mental\nmodel. Consequently, we show the importance of consciously designing robots\nthat express their capabilities and limitations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 09:56:34 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Hindemith", "Lukas", ""], ["Vollmer", "Anna-Lisa", ""], ["G\u00f6pfert", "Jan Phillip", ""], ["Wiebel-Herboth", "Christiane B.", ""], ["Wrede", "Britta", ""]]}, {"id": "2011.02804", "submitter": "Jorge Ramirez", "authors": "Jorge Ram\\'irez, Marcos Baez, Fabio Casati, Luca Cernuzzi, Boualem\n  Benatallah", "title": "Challenges and strategies for running controlled crowdsourcing\n  experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on the challenges and lessons we learned while running\ncontrolled experiments in crowdsourcing platforms. Crowdsourcing is becoming an\nattractive technique to engage a diverse and large pool of subjects in\nexperimental research, allowing researchers to achieve levels of scale and\ncompletion times that would otherwise not be feasible in lab settings. However,\nthe scale and flexibility comes at the cost of multiple and sometimes unknown\nsources of bias and confounding factors that arise from technical limitations\nof crowdsourcing platforms and from the challenges of running controlled\nexperiments in the \"wild\". In this paper, we take our experience in running\nsystematic evaluations of task design as a motivating example to explore,\ndescribe, and quantify the potential impact of running uncontrolled\ncrowdsourcing experiments and derive possible coping strategies. Among the\nchallenges identified, we can mention sampling bias, controlling the assignment\nof subjects to experimental conditions, learning effects, and reliability of\ncrowdsourcing results. According to our empirical studies, the impact of\npotential biases and confounding factors can amount to a 38\\% loss in the\nutility of the data collected in uncontrolled settings; and it can\nsignificantly change the outcome of experiments. These issues ultimately\ninspired us to implement CrowdHub, a system that sits on top of major\ncrowdsourcing platforms and allows researchers and practitioners to run\ncontrolled crowdsourcing projects.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 13:20:45 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Ram\u00edrez", "Jorge", ""], ["Baez", "Marcos", ""], ["Casati", "Fabio", ""], ["Cernuzzi", "Luca", ""], ["Benatallah", "Boualem", ""]]}, {"id": "2011.02891", "submitter": "Jorge Ramirez", "authors": "Jorge Ram\\'irez, Marcos Baez, Fabio Casati, Luca Cernuzzi, Boualem\n  Benatallah, Ekaterina A. Taran, Veronika A. Malanina", "title": "On the impact of predicate complexity in crowdsourced classification\n  tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores and offers guidance on a specific and relevant problem in\ntask design for crowdsourcing: how to formulate a complex question used to\nclassify a set of items. In micro-task markets, classification is still among\nthe most popular tasks. We situate our work in the context of information\nretrieval and multi-predicate classification, i.e., classifying a set of items\nbased on a set of conditions. Our experiments cover a wide range of tasks and\ndomains, and also consider crowd workers alone and in tandem with machine\nlearning classifiers. We provide empirical evidence into how the resulting\nclassification performance is affected by different predicate formulation\nstrategies, emphasizing the importance of predicate formulation as a task\ndesign dimension in crowdsourcing.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 15:04:10 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 23:53:25 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Ram\u00edrez", "Jorge", ""], ["Baez", "Marcos", ""], ["Casati", "Fabio", ""], ["Cernuzzi", "Luca", ""], ["Benatallah", "Boualem", ""], ["Taran", "Ekaterina A.", ""], ["Malanina", "Veronika A.", ""]]}, {"id": "2011.02933", "submitter": "Abdallah Namoun", "authors": "Abdallah Namoun and Ahmad B. Alkhodre", "title": "Towards Usability Guidelines for the Design of Effective Arabic\n  Websites: Design Practices and Lessons Focusing on Font and Image usage", "comments": "10 pages, 17 figures, 2 tables, journal", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), 10(4), (2019), 585-594", "doi": "10.14569/IJACSA.2019.0100472", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Arabic websites constitute 1% of the web content with more than 225\nmillion viewers and 41% Internet penetration. However, there is a lack of\ndesign guidelines related to the selection and use of appropriate font type and\nsize and images in Arabic websites. Both text and images are vital multimedia\ncomponents of websites and thereby were selected for investigation in this\nstudy. The herein paper performed an indepth inspection of font and image\ndesign practices within 73 most visited Arabic websites in Saudi Arabia\naccording to Alexa Internet ranking in the first quarter of 2019. Our\nexhaustive analysis showed discrepancies between the international design\nrecommendations and the actual design of Arabic websites. There was a\nconsiderable variation and inconsistency in using font types and sizes between\nand within the Arabic websites. Arabic Droid Kufi was used mostly for styling\npage titles and navigation menus, whilst Tahoma was used for styling\nparagraphs. The font size of the Arabic text ranged from 12 to 16 pixels, which\nmay lead to poor readability. Images were used heavily in the Arabic websites\ncausing prolonged site loading times. Moreover, the images strongly reflected\nthe dimensions of the Saudi culture, especially collectivism and masculinity.\nCurrent Arabic web design practices are compared against the findings from past\nstudies about international designs and lessons aiming at ameliorating the\nArabic web design are inferred.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 15:54:47 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Namoun", "Abdallah", ""], ["Alkhodre", "Ahmad B.", ""]]}, {"id": "2011.03209", "submitter": "Youjia Zhou", "authors": "Youjia Zhou, Nithin Chalapathi, Archit Rathore, Yaodong Zhao, Bei Wang", "title": "Mapper Interactive: A Scalable, Extendable, and Interactive Toolbox for\n  the Visual Exploration of High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mapper algorithm is a popular tool from topological data analysis for\nextracting topological summaries of high-dimensional datasets. In this paper,\nwe present Mapper Interactive, a web-based framework for the interactive\nanalysis and visualization of high-dimensional point cloud data. It implements\nthe mapper algorithm in an interactive, scalable, and easily extendable way,\nthus supporting practical data analysis. In particular, its command-line API\ncan compute mapper graphs for 1 million points of 256 dimensions in about 3\nminutes (4 times faster than the vanilla implementation). Its visual interface\nallows on-the-fly computation and manipulation of the mapper graph based on\nuser-specified parameters and supports the addition of new analysis modules\nwith a few lines of code. Mapper Interactive makes the mapper algorithm\naccessible to nonspecialists and accelerates topological analytics workflows.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 06:55:59 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 06:03:35 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Zhou", "Youjia", ""], ["Chalapathi", "Nithin", ""], ["Rathore", "Archit", ""], ["Zhao", "Yaodong", ""], ["Wang", "Bei", ""]]}, {"id": "2011.03371", "submitter": "Samaneh Saadat", "authors": "Samaneh Saadat, Gita Sukthankar", "title": "Explaining Differences in Classes of Discrete Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there are many machine learning methods to classify and cluster\nsequences, they fail to explain what are the differences in groups of sequences\nthat make them distinguishable. Although in some cases having a black box model\nis sufficient, there is a need for increased explainability in research areas\nfocused on human behaviors. For example, psychologists are less interested in\nhaving a model that predicts human behavior with high accuracy and more\nconcerned with identifying differences between actions that lead to divergent\nhuman behavior. This paper presents techniques for understanding differences\nbetween classes of discrete sequences. Approaches introduced in this paper can\nbe utilized to interpret black box machine learning models on sequences. The\nfirst approach compares k-gram representations of sequences using the\nsilhouette score. The second method characterizes differences by analyzing the\ndistance matrix of subsequences. As a case study, we trained black box\nsupervised learning methods to classify sequences of GitHub teams and then\nutilized our sequence analysis techniques to measure and characterize\ndifferences between event sequences of teams with bots and teams without bots.\nIn our second case study, we classified Minecraft event sequences to infer\ntheir high-level actions and analyzed differences between low-level event\nsequences of actions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 14:13:30 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Saadat", "Samaneh", ""], ["Sukthankar", "Gita", ""]]}, {"id": "2011.03413", "submitter": "Mathias Wolfgang Jesse", "authors": "Mathias Jesse and Dietmar Jannach", "title": "Digital Nudging with Recommender Systems: Survey and Future Directions", "comments": null, "journal-ref": "JIn Computers in Human Behavior Reports 3, p. 100052 (2021)", "doi": "10.1016/j.chbr.2020.100052", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are nowadays a pervasive part of our online user\nexperience, where they either serve as information filters or provide us with\nsuggestions for additionally relevant content. These systems thereby influence\nwhich information is easily accessible to us and thus affect our\ndecision-making processes though the automated selection and ranking of the\npresented content. Automated recommendations can therefore be seen as digital\nnudges, because they determine different aspects of the choice architecture for\nusers.\n  In this work, we examine the relationship between digital nudging and\nrecommender systems, topics that so far were mostly investigated in isolation.\nThrough a systematic literature search, we first identified 87 nudging\nmechanisms, which we categorize in a novel taxonomy. A subsequent analysis then\nshows that only a small part of these nudging mechanisms was previously\ninvestigated in the context of recommender systems. This indicates that there\nis a huge potential to develop future recommender systems that leverage the\npower of digital nudging in order to influence the decision-making of users. In\nthis work, we therefore outline potential ways of integrating nudging\nmechanisms into recommender systems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 15:08:43 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 11:50:25 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Jesse", "Mathias", ""], ["Jannach", "Dietmar", ""]]}, {"id": "2011.03570", "submitter": "Diane Hosfelt", "authors": "Diane Hosfelt, Jessica Outlaw, Tyesha Snow, Sara Carbonneau", "title": "Look Before You Leap: Trusted User Interfaces for the Immersive Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part of what makes the web successful is that anyone can publish content and\nbrowsers maintain certain safety guarantees. For example, it's safe to travel\nbetween links and make other trust decisions on the web because users can\nalways identify the location they are at. If we want virtual and augmented\nreality to be successful, we need that same safety. On the traditional,\ntwo-dimensional (2D) web, this user interface (UI) is provided by the browser\nbars and borders (also known as the chrome). However, the immersive,\nthree-dimensional (3D) web has no concept of a browser chrome, preventing\nroutine user inspection of URLs. In this paper, we discuss the unique\nchallenges that fully immersive head-worn computing devices provide to this\nmodel, evaluate three different strategies for trusted immersive UI, and make\nspecific recommendations to increase user safety and reduce the risks of\nspoofing.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 19:36:39 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Hosfelt", "Diane", ""], ["Outlaw", "Jessica", ""], ["Snow", "Tyesha", ""], ["Carbonneau", "Sara", ""]]}, {"id": "2011.03675", "submitter": "Dusanka Boskovic", "authors": "Selma Rizvic, Dusanka Boskovic, Vensada Okanovic, Sanda Sljivo, Merima\n  Zukic", "title": "Interactive digital storytelling: bringing cultural heritage in a\n  classroom", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Interactive digital storytelling becomes a popular choice for information\npresentation in many fields. Its application spans from media industry and\nbusiness information visualization, through digital cultural heritage, serious\ngames, education, to contemporary theater and visual arts. The benefits of this\nform of multimedia presentation in education are generally recognized, and\nseveral studies exploring and supporting the opinion are conducted. In addition\nto discussing the benefits, we wanted to address the challenges in introducing\ninteractive digital storytelling and serious games in the classroom. The\nchallenge of inherent ambiguity of edutainment, due to opposing features of\neducation and entertainment is augmented with different viewpoints of\nmultidisciplinary team members. We specifically address the opposing views on\nartistic liberty, at one side, and technical constraints and historic facts, on\nthe other side. In this paper we present the first findings related to these\nquestions and to initiate furthering discussions in this area.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 19:28:23 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Rizvic", "Selma", ""], ["Boskovic", "Dusanka", ""], ["Okanovic", "Vensada", ""], ["Sljivo", "Sanda", ""], ["Zukic", "Merima", ""]]}, {"id": "2011.03676", "submitter": "Aleksandar Miladinovic", "authors": "Aleksandar Miladinovi\\'c, Milo\\v{s} Aj\\v{c}evi\\'c, Pierpaolo Busan,\n  Joanna Jarmolowska, Giulia Silveri, Manuela Deodato, Sussana Mezzarobba,\n  Piero Paolo Battaglini, Agostino Accardo", "title": "Evaluation of Motor Imagery-Based BCI methods in neurorehabilitation of\n  Parkinson's Disease patients", "comments": null, "journal-ref": "2020 42nd Annual International Conference of the IEEE Engineering\n  in Medicine & Biology Society (EMBC)", "doi": "10.1109/EMBC44109.2020.9176651", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study reports the performance of Parkinson's disease (PD) patients to\noperate Motor-Imagery based Brain-Computer Interface (MI-BCI) and compares\nthree selected pre-processing and classification approaches. The experiment was\nconducted on 7 PD patients who performed a total of 14 MI-BCI sessions\ntargeting lower extremities. EEG was recorded during the initial calibration\nphase of each session, and the specific BCI models were produced by using\nSpectrally weighted Common Spatial Patterns (SpecCSP), Source Power\nComodulation (SPoC) and Filter-Bank Common Spatial Patterns (FBCSP) methods.\nThe results showed that FBCSP outperformed SPoC in terms of accuracy, and both\nSPoC and SpecCSP in terms of the false-positive ratio. The study also\ndemonstrates that PD patients were capable of operating MI-BCI, although with\nlower accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 10:31:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Miladinovi\u0107", "Aleksandar", ""], ["Aj\u010devi\u0107", "Milo\u0161", ""], ["Busan", "Pierpaolo", ""], ["Jarmolowska", "Joanna", ""], ["Silveri", "Giulia", ""], ["Deodato", "Manuela", ""], ["Mezzarobba", "Sussana", ""], ["Battaglini", "Piero Paolo", ""], ["Accardo", "Agostino", ""]]}, {"id": "2011.03844", "submitter": "Miguel Altamirano Cabrera", "authors": "Miguel Altamirano-Cabrera, Igor Usachev, Juan Heredia, Jonathan\n  Tirado, Aleksey Fedoseev, Dzmitry Tsetserukou", "title": "MaskBot: Real-time Robotic Projection Mapping with Head Motion Tracking", "comments": "Accepted to the ACM SIGGRAPH ASIA 2020 conference (Emerging\n  Technologies section), ACM copyright, 2 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The projection mapping systems on the human face is limited by the latency\nand the movement of the users. The area of the projection is restricted by the\nposition of the projectors and the cameras. We are introducing MaskBot, a\nreal-time projection mapping system operated by a 6 Degrees of Freedom (DoF)\ncollaborative robot. The collaborative robot locates the projector and camera\nin normal position to the face of the user to increase the projection area and\nto reduce the latency of the system. A webcam is used to detect the face and to\nsense the robot-user distance to modify the projection size and orientation.\nMaskBot projects different images on the face of the user, such as face\nmodifications, make-up, and logos. In contrast to the existing methods, the\npresented system is the first that introduces a robotic projection mapping. One\nof the prospective applications is to acquire a dataset of adversarial images\nto challenge face detection DNN systems, such as Face ID.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 20:21:32 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Altamirano-Cabrera", "Miguel", ""], ["Usachev", "Igor", ""], ["Heredia", "Juan", ""], ["Tirado", "Jonathan", ""], ["Fedoseev", "Aleksey", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "2011.03850", "submitter": "Anahid Basiri Prof", "authors": "Anahid Basiri", "title": "Open Area Path Finding to Improve Wheelchair Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Navigation is one of the most widely used applications of the Location Based\nServices (LBS) which have become part of our digitally informed daily lives.\nNavigation services, however, have generally been designed for drivers rather\nthan other users such as pedestrians or wheelchair users. For these users the\ndirected networks of streets and roads do not limit their movements, but their\nmovements may have other limitations, including lower speed of movement, and\nbeing more dependent on weather and the pavement surface conditions. This paper\nproposes and implements a novel path finding algorithm for open areas, i.e.\nareas with no network of pathways such as grasslands and parks where the\nconventional graph-based algorithms fail to calculate a practically traversable\npath. The new method provides multimodality, a higher level of performance,\nefficiency, and user satisfaction in comparison with currently available\nsolutions. The proposed algorithm creates a new graph in the open area, which\ncan consider the obstacles and barriers and calculate the path based on the\nfactors that are important for wheelchair users. Factors, including slope,\nwidth, and surface condition of the routes, are recognised by mining the actual\ntrajectories of wheelchairs users using trajectory mining and machine learning\ntechniques. Unlike raster-based techniques, a graph-based open area path\nfinding algorithm allows the routing to be fully compatible with current\ntransportation routing services, and enables a full multimodal routing service.\nThe implementations and tests show at least a 76.4% similarity between the\nproposed algorithm outputs and actual wheelchair users trajectories.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 21:20:32 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Basiri", "Anahid", ""]]}, {"id": "2011.03969", "submitter": "Marcos Baez", "authors": "Mla{\\dj}an Jovanovi\\'c, Marcos Baez, Fabio Casati", "title": "Chatbots as conversational healthcare services", "comments": null, "journal-ref": null, "doi": "10.1109/MIC.2020.3037151", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chatbots are emerging as a promising platform for accessing and delivering\nhealthcare services. The evidence is in the growing number of publicly\navailable chatbots aiming at taking an active role in the provision of\nprevention, diagnosis, and treatment services. This article takes a closer look\nat how these emerging chatbots address design aspects relevant to healthcare\nservice provision, emphasizing the Human-AI interaction aspects and the\ntransparency in AI automation and decision making.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 12:35:52 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Jovanovi\u0107", "Mla\u0111an", ""], ["Baez", "Marcos", ""], ["Casati", "Fabio", ""]]}, {"id": "2011.03983", "submitter": "Sharath Chandra Guntuku", "authors": "Roshan Santosh, H. Andrew Schwartz, Johannes C. Eichstaedt, Lyle H.\n  Ungar, Sharath C. Guntuku", "title": "Detecting Emerging Symptoms of COVID-19 using Context-based Twitter\n  Embeddings", "comments": "In proceedings of EMNLP 2020 (Empirical Methods in NLP) workshop on\n  COVID-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present an iterative graph-based approach for the detection\nof symptoms of COVID-19, the pathology of which seems to be evolving. More\ngenerally, the method can be applied to finding context-specific words and\ntexts (e.g. symptom mentions) in large imbalanced corpora (e.g. all tweets\nmentioning #COVID-19). Given the novelty of COVID-19, we also test if the\nproposed approach generalizes to the problem of detecting Adverse Drug Reaction\n(ADR). We find that the approach applied to Twitter data can detect symptom\nmentions substantially before being reported by the Centers for Disease Control\n(CDC).\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 13:56:05 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Santosh", "Roshan", ""], ["Schwartz", "H. Andrew", ""], ["Eichstaedt", "Johannes C.", ""], ["Ungar", "Lyle H.", ""], ["Guntuku", "Sharath C.", ""]]}, {"id": "2011.04016", "submitter": "Scott Friedman", "authors": "Scott Friedman, Jeff Rye, David LaVergne, Dan Thomsen, Matthew Allen,\n  Kyle Tunis", "title": "Provenance-Based Interpretation of Multi-Agent Information Analysis", "comments": "6 pages, 5 figures, appears in Proceedings of TaPP 2020", "journal-ref": "Proceedings of TaPP 2020", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytic software tools and workflows are increasing in capability,\ncomplexity, number, and scale, and the integrity of our workflows is as\nimportant as ever. Specifically, we must be able to inspect the process of\nanalytic workflows to assess (1) confidence of the conclusions, (2) risks and\nbiases of the operations involved, (3) sensitivity of the conclusions to\nsources and agents, (4) impact and pertinence of various sources and agents,\nand (5) diversity of the sources that support the conclusions. We present an\napproach that tracks agents' provenance with PROV-O in conjunction with agents'\nappraisals and evidence links (expressed in our novel DIVE ontology). Together,\nPROV-O and DIVE enable dynamic propagation of confidence and counter-factual\nrefutation to improve human-machine trust and analytic integrity. We\ndemonstrate representative software developed for user interaction with that\nprovenance, and discuss key needs for organizations adopting such approaches.\nWe demonstrate all of these assessments in a multi-agent analysis scenario,\nusing an interactive web-based information validation UI.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 16:43:34 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Friedman", "Scott", ""], ["Rye", "Jeff", ""], ["LaVergne", "David", ""], ["Thomsen", "Dan", ""], ["Allen", "Matthew", ""], ["Tunis", "Kyle", ""]]}, {"id": "2011.04322", "submitter": "Wei Jeng", "authors": "Hsu-Chun Hsiao, Chun-Ying Huang, Bing-Kai Hong, Shin-Ming Cheng,\n  Hsin-Yuan Hu, Chia-Chien Wu, Jian-Sin Lee, Shih-Hong Wang, Wei Jeng", "title": "An Empirical Evaluation of Bluetooth-based Decentralized Contact Tracing\n  in Crowds", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital contact tracing is being used by many countries to help contain\nCOVID-19's spread in a post-lockdown world. Among the various available\ntechniques, decentralized contact tracing that uses Bluetooth received signal\nstrength indication (RSSI) to detect proximity is considered less of a privacy\nrisk than approaches that rely on collecting absolute locations via GPS,\ncellular-tower history, or QR-code scanning. As of October 2020, there have\nbeen millions of downloads of such Bluetooth-based contract-tracing apps, as\nmore and more countries officially adopt them. However, the effectiveness of\nthese apps in the real world remains unclear due to a lack of empirical\nresearch that includes realistic crowd sizes and densities. This study aims to\nfill that gap, by empirically investigating the effectiveness of\nBluetooth-based contact tracing in crowd environments with a total of 80\nparticipants, emulating classrooms, moving lines, and other types of real-world\ngatherings. The results confirm that Bluetooth RSSI is unreliable for detecting\nproximity, and that this inaccuracy worsens in environments that are especially\ncrowded. In other words, this technique may be least useful when it is most in\nneed, and that it is fragile when confronted by low-cost jamming. Moreover,\ntechnical problems such as high energy consumption and phone overheating caused\nby the contact-tracing app were found to negatively influence users'\nwillingness to adopt it. On the bright side, however, Bluetooth RSSI may still\nbe useful for detecting coarse-grained contact events, for example, proximity\nof up to 20m lasting for an hour. Based on our findings, we recommend that\nexisting contact-tracing apps can be re-purposed to focus on coarse-grained\nproximity detection, and that future ones calibrate distance estimates and\nadjust broadcast frequencies based on auxiliary information.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 10:44:03 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 02:32:07 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Hsiao", "Hsu-Chun", ""], ["Huang", "Chun-Ying", ""], ["Hong", "Bing-Kai", ""], ["Cheng", "Shin-Ming", ""], ["Hu", "Hsin-Yuan", ""], ["Wu", "Chia-Chien", ""], ["Lee", "Jian-Sin", ""], ["Wang", "Shih-Hong", ""], ["Jeng", "Wei", ""]]}, {"id": "2011.04812", "submitter": "Kejun Li", "authors": "Kejun Li, Maegan Tucker, Erdem B{\\i}y{\\i}k, Ellen Novoseller, Joel W.\n  Burdick, Yanan Sui, Dorsa Sadigh, Yisong Yue, Aaron D. Ames", "title": "ROIAL: Region of Interest Active Learning for Characterizing Exoskeleton\n  Gait Preference Landscapes", "comments": "6 pages + 1 page of references; 7 figures; To Appear at ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing what types of exoskeleton gaits are comfortable for users, and\nunderstanding the science of walking more generally, require recovering a\nuser's utility landscape. Learning these landscapes is challenging, as walking\ntrajectories are defined by numerous gait parameters, data collection from\nhuman trials is expensive, and user safety and comfort must be ensured. This\nwork proposes the Region of Interest Active Learning (ROIAL) framework, which\nactively learns each user's underlying utility function over a region of\ninterest that ensures safety and comfort. ROIAL learns from ordinal and\npreference feedback, which are more reliable feedback mechanisms than absolute\nnumerical scores. The algorithm's performance is evaluated both in simulation\nand experimentally for three non-disabled subjects walking inside of a\nlower-body exoskeleton. ROIAL learns Bayesian posteriors that predict each\nexoskeleton user's utility landscape across four exoskeleton gait parameters.\nThe algorithm discovers both commonalities and discrepancies across users' gait\npreferences and identifies the gait parameters that most influenced user\nfeedback. These results demonstrate the feasibility of recovering gait utility\nlandscapes from limited human trials.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 22:45:58 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 22:59:19 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Li", "Kejun", ""], ["Tucker", "Maegan", ""], ["B\u0131y\u0131k", "Erdem", ""], ["Novoseller", "Ellen", ""], ["Burdick", "Joel W.", ""], ["Sui", "Yanan", ""], ["Sadigh", "Dorsa", ""], ["Yue", "Yisong", ""], ["Ames", "Aaron D.", ""]]}, {"id": "2011.05039", "submitter": "Ben Cobley", "authors": "Bennet Cobley, David Boyle", "title": "OnionBot: A System for Collaborative Computational Cooking", "comments": "6 pages, 7 figures. Open source repository is available at\n  https://github.com/onionbot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unsolved challenge in cooking automation is designing for shared kitchen\nworkspaces. In particular, robots struggle with dexterity in the unstructured\nand dynamic kitchen environment. We propose that human-machine collaboration\ncan be achieved without robotic manipulation. We describe a novel system design\nusing computer vision to inform intelligent cooking interventions. This\nhuman-centered approach does not require actuators and promotes dynamic,\nnatural collaboration. We show that automation that assists user-led actions\ncan offer meaningful cooking assistance and can generate the image databases\nneeded for fully autonomous robotic systems of the future. We provide an open\nsource implementation of our work and encourage the research community to build\nupon it.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 11:07:33 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Cobley", "Bennet", ""], ["Boyle", "David", ""]]}, {"id": "2011.05228", "submitter": "Pantelis Pappas Mr.", "authors": "Pantelis Pappas, Manolis Chiou, Georgios-Theofanis Epsimos, Grigoris\n  Nikolaou and Rustam Stolkin", "title": "VFH+ based shared control for remotely operated mobile robots", "comments": "8 pages,6 figures", "journal-ref": "2020 IEEE International Symposium on Safety, Security, and Rescue\n  Robotics (SSRR), 2020", "doi": "10.1109/SSRR50563.2020.9292585", "report-no": "pp. 366-373", "categories": "cs.RO cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of safe and efficient navigation in remotely\ncontrolled robots operating in hazardous and unstructured environments; or\nconducting other remote robotic tasks. A shared control method is presented\nwhich blends the commands from a VFH+ obstacle avoidance navigation module with\nthe teleoperation commands provided by an operator via a joypad. The presented\napproach offers several advantages such as flexibility allowing for a\nstraightforward adaptation of the controller's behaviour and easy integration\nwith variable autonomy systems; as well as the ability to cope with dynamic\nenvironments. The advantages of the presented controller are demonstrated by an\nexperimental evaluation in a disaster response scenario. More specifically,\npresented evidence show a clear performance increase in terms of safety and\ntask completion time compared to a pure teleoperation approach, as well as an\nability to cope with previously unobserved obstacles.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 16:42:14 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Pappas", "Pantelis", ""], ["Chiou", "Manolis", ""], ["Epsimos", "Georgios-Theofanis", ""], ["Nikolaou", "Grigoris", ""], ["Stolkin", "Rustam", ""]]}, {"id": "2011.05533", "submitter": "Matthew Marge", "authors": "Matthew Marge, Carol Espy-Wilson, Nigel Ward", "title": "Spoken Language Interaction with Robots: Research Issues and\n  Recommendations, Report from the NSF Future Directions Workshop", "comments": "35 pages, 6 figures, Final report from the NSF Future Directions\n  Workshop on Speech for Robotics, held in October 2019, College Park, MD.\n  Workshop website: https://isr.umd.edu/2019-SFRW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With robotics rapidly advancing, more effective human-robot interaction is\nincreasingly needed to realize the full potential of robots for society. While\nspoken language must be part of the solution, our ability to provide spoken\nlanguage interaction capabilities is still very limited. The National Science\nFoundation accordingly convened a workshop, bringing together speech, language,\nand robotics researchers to discuss what needs to be done. The result is this\nreport, in which we identify key scientific and engineering advances needed.\n  Our recommendations broadly relate to eight general themes. First, meeting\nhuman needs requires addressing new challenges in speech technology and user\nexperience design. Second, this requires better models of the social and\ninteractive aspects of language use. Third, for robustness, robots need\nhigher-bandwidth communication with users and better handling of uncertainty,\nincluding simultaneous consideration of multiple hypotheses and goals. Fourth,\nmore powerful adaptation methods are needed, to enable robots to communicate in\nnew environments, for new tasks, and with diverse user populations, without\nextensive re-engineering or the collection of massive training data. Fifth,\nsince robots are embodied, speech should function together with other\ncommunication modalities, such as gaze, gesture, posture, and motion. Sixth,\nsince robots operate in complex environments, speech components need access to\nrich yet efficient representations of what the robot knows about objects,\nlocations, noise sources, the user, and other humans. Seventh, since robots\noperate in real time, their speech and language processing components must\nalso. Eighth, in addition to more research, we need more work on infrastructure\nand resources, including shareable software modules and internal interfaces,\ninexpensive hardware, baseline systems, and diverse corpora.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 03:45:34 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Marge", "Matthew", ""], ["Espy-Wilson", "Carol", ""], ["Ward", "Nigel", ""]]}, {"id": "2011.05600", "submitter": "Will Crichton", "authors": "Will Crichton", "title": "Documentation Generation as Information Visualization", "comments": "To appear at PLATEAU @ SPLASH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic documentation generation tools, or auto docs, are widely used to\nvisualize information about APIs. However, each auto doc tool comes with its\nown unique representation of API information. In this paper, I use an\ninformation visualization analysis of auto docs to generate potential design\nprinciples for improving their usability. Developers use auto docs as a\nreference by looking up relevant API primitives given partial information, or\nleads, about its name, type, or behavior. I discuss how auto docs can better\nsupport searching and scanning on these leads, e.g. by providing more\ninformation-dense visualizations of method signatures.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 07:04:30 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Crichton", "Will", ""]]}, {"id": "2011.05861", "submitter": "Jing Mu", "authors": "Jing Mu, Ying Tan, David B. Grayden, and Denny Oetomo", "title": "Multi-Frequency Canonical Correlation Analysis (MFCCA): An Extended\n  Decoding Algorithm for Multi-Frequency SSVEP", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stimulation methods that utilise more than one stimulation frequency have\nbeen developed in steady-state visual evoked potential (SSVEP) brain-computer\ninterfaces (BCIs) for the purpose of increasing the number of targets that can\nbe presented simultaneously. However, there is no unified decoding algorithm\nthat can be applied to a large class of multi-frequency stimulated SSVEP\nsettings. This paper extends the widely used canonical correlation analysis\n(CCA) decoder to explicitly accommodate multi-frequency SSVEP by exploiting the\ninteractions between the multiple stimulation frequencies. A concept \"order\"\nwas defined as the sum of absolute values of the coefficients in the linear\ninteraction. The probability distribution of the order in the resulting SSVEP\nresponse was then used to improve decoding accuracy. Results show that,\ncompared to the standard CCA formulation, the proposed multi-frequency CCA\n(MFCCA) has a 20% improvement in decoding accuracy on average at order 2.\nAlthough the proposed methods were only tested with two input frequencies, the\ntechnique is capable of handling more than two simultaneous input frequencies.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 09:02:03 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Mu", "Jing", ""], ["Tan", "Ying", ""], ["Grayden", "David B.", ""], ["Oetomo", "Denny", ""]]}, {"id": "2011.05863", "submitter": "Birgitta Dresp-Langley", "authors": "Birgitta Dresp-Langley", "title": "Wearable Sensors for Individual Grip Force Profiling", "comments": null, "journal-ref": "In S. Y. Yurish (Ed.) Advances in Biosensors: Reviews, Vol.3,\n  International Frequency Sensor Association (IFSA), 2020, pp. 107-122", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biosensors and wearable sensor systems with transmitting capabilities are\ncurrently developed and used for the monitoring of health data, exercise\nactivities, and other performance data. Unlike conventional approaches, these\ndevices enable convenient, continuous, and/or unobtrusive monitoring of user\nbehavioral signals in real time. Examples include signals relative to body\nmotion, body temperature, blood flow parameters and a variety of biological or\nbiochemical markers and, as will be shown in this chapter here, individual grip\nforce data that directly translate into spatiotemporal grip force profiles for\ndifferent locations on the fingers and palm of the hand. Wearable sensor\nsystems combine innovation in sensor design, electronics, data transmission,\npower management, and signal processing for statistical analysis, as will be\nfurther shown herein. The first section of this chapter will provide an\noverview of the current state of the art in grip force profiling to highlight\nimportant functional aspects to be considered. In the next section, the\ncontribution of wearable sensor technology in the form of sensor glove systems\nfor the real-time monitoring of surgical task skill evolution in novices\ntraining in a simulator task will be described on the basis of recent examples.\nIn the discussion, advantages and limitations will be weighed against each\nother.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 15:56:21 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Dresp-Langley", "Birgitta", ""]]}, {"id": "2011.05978", "submitter": "Samuel L\\\"aubli", "authors": "Samuel L\\\"aubli, Patrick Simianer, Joern Wuebker, Geza Kovacs, Rico\n  Sennrich, Spence Green", "title": "The Impact of Text Presentation on Translator Performance", "comments": "Accepted for publication in Target", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Widely used computer-aided translation (CAT) tools divide documents into\nsegments such as sentences and arrange them in a side-by-side, spreadsheet-like\nview. We present the first controlled evaluation of these design choices on\ntranslator performance, measuring speed and accuracy in three experimental text\nprocessing tasks. We find significant evidence that sentence-by-sentence\npresentation enables faster text reproduction and within-sentence error\nidentification compared to unsegmented text, and that a top-and-bottom\narrangement of source and target sentences enables faster text reproduction\ncompared to a side-by-side arrangement. For revision, on the other hand, our\nresults suggest that presenting unsegmented text results in the highest\naccuracy and time efficiency. Our findings have direct implications for best\npractices in designing CAT tools.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 18:50:18 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["L\u00e4ubli", "Samuel", ""], ["Simianer", "Patrick", ""], ["Wuebker", "Joern", ""], ["Kovacs", "Geza", ""], ["Sennrich", "Rico", ""], ["Green", "Spence", ""]]}, {"id": "2011.06007", "submitter": "Yugyeong Kim", "authors": "Yugyeong Kim, Sudip Vhaduri, and Christian Poellabauer", "title": "Understanding College Students' Phone Call Behaviors Towards a\n  Sustainable Mobile Health and Wellbeing Solution", "comments": "Accepted for publication in the 3rd International Conference on\n  Systems Engineering (CIIS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the transition from high school to on-campus college life, a student\nleaves home and starts facing enormous life changes, including meeting new\npeople, more responsibilities, being away from family, and academic challenges.\nThese recent changes lead to an elevation of stress and anxiety, affecting a\nstudent's health and wellbeing. With the help of smartphones and their rich\ncollection of sensors, we can continuously monitor various factors that affect\nstudents' behavioral patterns, such as communication behaviors associated with\ntheir health, wellbeing, and academic success. In this work, we try to assess\ncollege students' communication patterns (in terms of phone call duration and\nfrequency) that vary across various geographical contexts (e.g., dormitories,\nclasses, dining) during different times (e.g., epochs of a day, days of a week)\nusing visualization techniques. Findings from this work will help foster the\ndesign and delivery of smartphone-based health interventions; thereby, help the\nstudents adapt to the changes in life.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 19:00:13 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kim", "Yugyeong", ""], ["Vhaduri", "Sudip", ""], ["Poellabauer", "Christian", ""]]}, {"id": "2011.06087", "submitter": "Gareth W. Young Dr", "authors": "Gareth W. Young and Siobh\\'an Mannion and Sara Wentworth", "title": "Evoking Places from Spaces. The application of multimodal narrative\n  techniques in the creation of \"U Modified\"", "comments": "5 pages", "journal-ref": "15th Sound and Music Computing Conference (SMC2018)", "doi": "10.5281/zenodo.1408596", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal diegetic narrative tools, as applied in multimedia arts practices,\npossess the ability to cross the spaces that exist between the physical world\nand the imaginary. Within this paper we present the findings of a\nmultidiscipline practice based research project that explored the potential of\nan audiovisual art performance to purposefully interact with an audience's\nperception of narrative place. To achieve this goal, research was undertaken to\ninvestigate the function of multimodal diegetic practices as applied in the\ncontext of a sonic art narrative. This project direction was undertaken to\nfacilitate the transformation of previous experiences of place through the\ncreative amalgamation and presentation of collected audio and visual footage\nfrom real world spaces. Through the presentation of multimedia relating to\nfamiliar geographical spatial features, the audience were affected to evoke\nmemories of place and to construct and manipulate their own narrative.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 20:50:53 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Young", "Gareth W.", ""], ["Mannion", "Siobh\u00e1n", ""], ["Wentworth", "Sara", ""]]}, {"id": "2011.06171", "submitter": "Will Crichton", "authors": "Will Crichton", "title": "The Usability of Ownership", "comments": "To appear at HATRA @ SPLASH '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ownership is the concept of tracking aliases and mutations to data, useful\nfor both memory safety and system design. The Rust programming language\nimplements ownership via the borrow checker, a static analyzer that extends the\ncore type system. The borrow checker is a notorious learning barrier for new\nRust users. In this paper, I focus on the gap between understanding ownership\nin theory versus its implementation in the borrow checker. As a sound and\nincomplete analysis, compiler errors may arise from either ownership-unsound\nbehavior or limitations of the analyzer. Understanding this distinction is\nessential for fixing ownership errors. But how are users actually supposed to\nmake the correct inference? Drawing on my experience with using and teaching\nRust, I explore the many challenges in interpreting and responding to ownership\nerrors. I also suggest educational and automated interventions that could\nimprove the usability of ownership.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 02:39:03 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Crichton", "Will", ""]]}, {"id": "2011.06237", "submitter": "Bhanu Prakash Reddy Guda", "authors": "Samarth Aggarwal, Rohin Garg, Abhilasha Sancheti, Bhanu Prakash Reddy\n  Guda, Iftikhar Ahamath Burhanuddin", "title": "Goal-driven Command Recommendations for Analysts", "comments": "14th ACM Conference on Recommender Systems (RecSys 2020)", "journal-ref": null, "doi": "10.1145/3383313.3412255", "report-no": null, "categories": "cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent times have seen data analytics software applications become an\nintegral part of the decision-making process of analysts. The users of these\nsoftware applications generate a vast amount of unstructured log data. These\nlogs contain clues to the user's goals, which traditional recommender systems\nmay find difficult to model implicitly from the log data. With this assumption,\nwe would like to assist the analytics process of a user through command\nrecommendations. We categorize the commands into software and data categories\nbased on their purpose to fulfill the task at hand. On the premise that the\nsequence of commands leading up to a data command is a good predictor of the\nlatter, we design, develop, and validate various sequence modeling techniques.\nIn this paper, we propose a framework to provide goal-driven data command\nrecommendations to the user by leveraging unstructured logs. We use the log\ndata of a web-based analytics software to train our neural network models and\nquantify their performance, in comparison to relevant and competitive\nbaselines. We propose a custom loss function to tailor the recommended data\ncommands according to the goal information provided exogenously. We also\npropose an evaluation metric that captures the degree of goal orientation of\nthe recommendations. We demonstrate the promise of our approach by evaluating\nthe models with the proposed metric and showcasing the robustness of our models\nin the case of adversarial examples, where the user activity is misaligned with\nselected goal, through offline evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 07:26:52 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Aggarwal", "Samarth", ""], ["Garg", "Rohin", ""], ["Sancheti", "Abhilasha", ""], ["Guda", "Bhanu Prakash Reddy", ""], ["Burhanuddin", "Iftikhar Ahamath", ""]]}, {"id": "2011.06456", "submitter": "Birgitta Dresp-Langley", "authors": "Birgitta Dresp-Langley, Marie Monfouga", "title": "Combining visual contrast information with sound can produce faster\n  decisions", "comments": null, "journal-ref": "Information 2019, 10, 346", "doi": "10.3390/info10110346", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pierons and Chocholles seminal psychophysical work predicts that human\nresponse time to information relative to visual contrast and sound frequency\ndecreases when contrast intensity or sound frequency increases. The goal of\nthis study is to bring to the fore the ability of individuals to use visual\ncontrast intensity and sound frequency in combination for faster perceptual\ndecisions of relative depth in planar object configurations on the basis of\nphysical variations in luminance contrast. Computer controlled images with two\nabstract patterns of varying contrast intensity, one on the left and one on the\nright, preceded or not by a pure tone of varying frequency, were shown to\nhealthy young humans in controlled experimental sequences. Their task was to\ndecide as quickly as possible which of two patterns, the left or the right one,\nin a given image appeared to stand out as if it were nearer in terms of\napparent or subjective visual depth. The results show that the combinations of\nvarying relative visual contrast with sounds of varying frequency exploited\nhere produced an additive effect on choice response times in terms of\nfacilitation, where a stronger visual contrast combined with a higher sound\nfrequency produced shorter forced choice response times. This new effect is\npredicted by crossmodal audiovisual probability summation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 15:56:57 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Dresp-Langley", "Birgitta", ""], ["Monfouga", "Marie", ""]]}, {"id": "2011.06473", "submitter": "Tae Woo Hong", "authors": "Freddie Hong, Connor Myant, David Boyle", "title": "Thermoformed Circuit Boards: Fabrication of highly conductive freeform\n  3D printed circuit boards with heat bending", "comments": "10 pages, 17 figures, 2021 ACM CHI Conference on Human Factors in\n  Computing Systems", "journal-ref": null, "doi": "10.1145/3411764.3445469", "report-no": null, "categories": "cs.ET cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fabricating 3D printed electronics using desktop printers has become more\naccessible with recent developments in conductive thermoplastic filaments.\nBecause of their high resistance and difficulties in printing traces in\nvertical directions, most applications are restricted to capacitive sensing. In\nthis paper, we introduce Thermoformed Circuit Board (TCB), a novel approach\nthat employs the thermoformability of the 3D printed plastics to construct\nvarious double-sided, rigid and highly conductive freeform circuit boards that\ncan withstand high current applications through copper electroplating. To\nillustrate the capability of the TCB, we showcase a range of examples with\nvarious shapes, electrical characteristics and interaction mechanisms. We also\ndemonstrate a new design tool extension to an existing CAD environment that\nallows users to parametrically draw the substrate and conductive trace, and\nexport 3D printable files. TCB is an inexpensive and highly accessible\nfabrication technique intended to broaden HCI researcher participation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 16:28:40 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 14:17:48 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Hong", "Freddie", ""], ["Myant", "Connor", ""], ["Boyle", "David", ""]]}, {"id": "2011.06529", "submitter": "Samiha Samrose", "authors": "Samiha Samrose, Reza Rawassizadeh, Ehsan Hoque", "title": "Immediate or Reflective?: Effects of Real-timeFeedback on Group\n  Discussions over Videochat", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Having a group discussion with the members holding conflicting viewpoints is\ndifficult. It is especially challenging for machine-mediated discussions in\nwhich the subtle social cues are hard to notice. We present a fully automated\nvideochat framework that can automatically analyze audio-video data of the\nparticipants and provide real-time feedback on participation, interruption,\nvolume, and facial emotion. In a heated discourse, these features are\nespecially aligned with the undesired characteristics of dominating the\nconversation without taking turns, interrupting constantly, raising voice, and\nexpressing negative emotion. We conduct a treatment-control user study with 40\nparticipants having 20 sessions in total. We analyze the immediate and the\nreflective effects of real-time feedback on participants. Our findings show\nthat while real-time feedback can make the ongoing discussion significantly\nless spontaneous, its effects propagate to successive sessions bringing\nsignificantly more expressiveness to the team. Our explorations with instant\nand propagated impacts of real-time feedback can be useful for developing\ndesign strategies for various collaborative environments.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:41:22 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Samrose", "Samiha", ""], ["Rawassizadeh", "Reza", ""], ["Hoque", "Ehsan", ""]]}, {"id": "2011.06791", "submitter": "Giulia Cisotto", "authors": "Giulia Bressan, Selina C. Wriessnegger, Giulia Cisotto", "title": "Deep learning-based classification of fine hand movements from low\n  frequency EEG", "comments": null, "journal-ref": null, "doi": "10.3390/fi13050103", "report-no": null, "categories": "eess.SP cs.AI cs.HC cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of different fine hand movements from EEG signals\nrepresents a relevant research challenge, e.g., in brain-computer interface\napplications for motor rehabilitation. Here, we analyzed two different datasets\nwhere fine hand movements (touch, grasp, palmar and lateral grasp) were\nperformed in a self-paced modality. We trained and tested a newly proposed\nconvolutional neural network (CNN), and we compared its classification\nperformance into respect to two well-established machine learning models,\nnamely, a shrinked-LDA and a Random Forest. Compared to previous literature, we\ntook advantage of the knowledge of the neuroscience field, and we trained our\nCNN model on the so-called Movement Related Cortical Potentials (MRCPs)s. They\nare EEG amplitude modulations at low frequencies, i.e., (0.3, 3) Hz, that have\nbeen proved to encode several properties of the movements, e.g., type of grasp,\nforce level and speed. We showed that CNN achieved good performance in both\ndatasets and they were similar or superior to the baseline models. Also,\ncompared to the baseline, our CNN requires a lighter and faster pre-processing\nprocedure, paving the way for its possible use in an online modality, e.g., for\nmany brain-computer interface applications.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 07:16:06 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 08:45:45 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Bressan", "Giulia", ""], ["Wriessnegger", "Selina C.", ""], ["Cisotto", "Giulia", ""]]}, {"id": "2011.06896", "submitter": "Zijun Han", "authors": "Zijun Han, Zhaoming Lu, Xiangming Wen, Wei Zheng, Jingbo Zhao and\n  Lingchao Guo", "title": "CentiTrack: Towards Centimeter-Level Passive Gesture Tracking with\n  Commodity WiFi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gesture awareness plays a crucial role in promoting human-computer interface.\nPrevious works either depend on customized hardware or need a priori learning\nof wireless signal patterns, facing downsides in terms of the privacy concern,\navailability and reliability. In this paper, we propose CentiTrack, the first\ncentimeter-level passive gesture tracking system that works with only three\ncommodityWiFi devices, without any extra hardware modifications or wearable\nsensors. To this end, we first identify the Channel State Information (CSI)\nmeasurement error sources in the physical layer process, and then denoise CSI\nby the complex ratio between adjacent antennas. Principal Component Analysis\n(PCA) is further adopted to separate the reflected signals from noises.\nBenchmark experiments are conducted to verify that the phase changes of\ndenoised CSI are proportional to the length changes of dynamic path reflected\noff the hand. In addition, we adopt the Multiple Signal Classification (MUSIC)\nalgorithm to estimate the Angle-of-Arrivals (AoAs) of dynamic paths, and then\nlocate the initial position of hands with triangulation. We also propose a\nnovel static componnets elimination algorithm for tracking correction by\neliminating the components unrelated to motion. A prototype of CentiTrack is\nfully realized and evaluated in various real scenarios. Extensive experiments\nshow that CentiTrack is superior in terms of tracking accuracy, sensing range\nand device cost, compared with the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 13:24:58 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Han", "Zijun", ""], ["Lu", "Zhaoming", ""], ["Wen", "Xiangming", ""], ["Zheng", "Wei", ""], ["Zhao", "Jingbo", ""], ["Guo", "Lingchao", ""]]}, {"id": "2011.06916", "submitter": "Amanda Fern\\'andez-Fontelo Dr.", "authors": "Amanda Fern\\'andez-Fontelo, Pascal J. Kieslich, Felix Henninger,\n  Frauke Kreuter and Sonja Greven", "title": "Predicting respondent difficulty in web surveys: A machine-learning\n  approach based on mouse movement features", "comments": "40 pages, 2 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central goal of survey research is to collect robust and reliable data from\nrespondents. However, despite researchers' best efforts in designing\nquestionnaires, respondents may experience difficulty understanding questions'\nintent and therefore may struggle to respond appropriately. If it were possible\nto detect such difficulty, this knowledge could be used to inform real-time\ninterventions through responsive questionnaire design, or to indicate and\ncorrect measurement error after the fact. Previous research in the context of\nweb surveys has used paradata, specifically response times, to detect\ndifficulties and to help improve user experience and data quality. However,\nricher data sources are now available, in the form of the movements respondents\nmake with the mouse, as an additional and far more detailed indicator for the\nrespondent-survey interaction. This paper uses machine learning techniques to\nexplore the predictive value of mouse-tracking data with regard to respondents'\ndifficulty. We use data from a survey on respondents' employment history and\ndemographic information, in which we experimentally manipulate the difficulty\nof several questions. Using features derived from the cursor movements, we\npredict whether respondents answered the easy or difficult version of a\nquestion, using and comparing several state-of-the-art supervised learning\nmethods. In addition, we develop a personalization method that adjusts for\nrespondents' baseline mouse behavior and evaluate its performance. For all\nthree manipulated survey questions, we find that including the full set of\nmouse movement features improved prediction performance over response-time-only\nmodels in nested cross-validation. Accounting for individual differences in\nmouse movements led to further improvements.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 10:54:33 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Fern\u00e1ndez-Fontelo", "Amanda", ""], ["Kieslich", "Pascal J.", ""], ["Henninger", "Felix", ""], ["Kreuter", "Frauke", ""], ["Greven", "Sonja", ""]]}, {"id": "2011.07065", "submitter": "Homayoon Beigi", "authors": "Amith Ananthram, Kailash Karthik Saravanakumar, Jessica Huynh, and\n  Homayoon Beigi", "title": "Multi-Modal Emotion Detection with Transfer Learning", "comments": "11 pages, 7 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": "RTI-20201113-01", "categories": "eess.AS cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated emotion detection in speech is a challenging task due to the\ncomplex interdependence between words and the manner in which they are spoken.\nIt is made more difficult by the available datasets; their small size and\nincompatible labeling idiosyncrasies make it hard to build generalizable\nemotion detection systems. To address these two challenges, we present a\nmulti-modal approach that first transfers learning from related tasks in speech\nand text to produce robust neural embeddings and then uses these embeddings to\ntrain a pLDA classifier that is able to adapt to previously unseen emotions and\ndomains. We begin by training a multilayer TDNN on the task of speaker\nidentification with the VoxCeleb corpora and then fine-tune it on the task of\nemotion identification with the Crema-D corpus. Using this network, we extract\nspeech embeddings for Crema-D from each of its layers, generate and concatenate\ntext embeddings for the accompanying transcripts using a fine-tuned BERT model\nand then train an LDA - pLDA classifier on the resulting dense representations.\nWe exhaustively evaluate the predictive power of every component: the TDNN\nalone, speech embeddings from each of its layers alone, text embeddings alone\nand every combination thereof. Our best variant, trained on only VoxCeleb and\nCrema-D and evaluated on IEMOCAP, achieves an EER of 38.05%. Including a\nportion of IEMOCAP during training produces a 5-fold averaged EER of 25.72%\n(For comparison, 44.71% of the gold-label annotations include at least one\nannotator who disagrees).\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 18:58:59 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Ananthram", "Amith", ""], ["Saravanakumar", "Kailash Karthik", ""], ["Huynh", "Jessica", ""], ["Beigi", "Homayoon", ""]]}, {"id": "2011.07105", "submitter": "Florian Fischer", "authors": "Florian Fischer, Miroslav Bachinski, Markus Klar, Arthur Fleig, J\\\"org\n  M\\\"uller", "title": "Reinforcement Learning Control of a Biomechanical Model of the Upper\n  Extremity", "comments": "20 pages, 7 figures, 6 supporting figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the question whether the assumptions of signal-dependent and\nconstant motor noise in a full skeletal model of the human upper extremity,\ntogether with the objective of movement time minimization, can predict reaching\nmovements. We learn a control policy using a motor babbling approach based on\nreinforcement learning, using aimed movements of the tip of the right index\nfinger towards randomly placed 3D targets of varying size. The reward signal is\nthe negative time to reach the target, implying movement time minimization. Our\nbiomechanical model of the upper extremity uses the skeletal structure of the\nUpper Extremity Dynamic Model, including thorax, right shoulder, arm, and hand.\nThe model has 7 actuated degrees of freedom, including shoulder rotation,\nelevation and elevation plane, elbow flexion, forearm rotation, and wrist\nflexion and deviation. To deal with the curse of dimensionality, we use a\nsimplified second-order muscle model acting at each joint instead of individual\nmuscles. We address the lack of gradient provided by the simple reward function\nthrough an adaptive learning curriculum. Our results demonstrate that the\nassumptions of signal-dependent and constant motor noise, together with the\nobjective of movement time minimization, are sufficient for a state-of-the-art\nskeletal model of the human upper extremity to reproduce complex phenomena of\nhuman movement such as Fitts' Law and the 2/3 Power Law. This result supports\nthe idea that the control of the complex human biomechanical system is\nplausible to be determined by a set of simple assumptions and can be easily\nlearned.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 19:49:29 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Fischer", "Florian", ""], ["Bachinski", "Miroslav", ""], ["Klar", "Markus", ""], ["Fleig", "Arthur", ""], ["M\u00fcller", "J\u00f6rg", ""]]}, {"id": "2011.07130", "submitter": "Adam Johs", "authors": "Adam J. Johs, Denise E. Agosto, Rosina O. Weber", "title": "Qualitative Investigation in Explainable Artificial Intelligence: A Bit\n  More Insight from Social Science", "comments": "Accepted to the AAAI 2021 Explainable Agency in Artificial\n  Intelligence Workshop, 10 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a focused analysis of user studies in explainable artificial\nintelligence (XAI) entailing qualitative investigation. We draw on social\nscience corpora to suggest ways for improving the rigor of studies where XAI\nresearchers use observations, interviews, focus groups, and/or questionnaires\nto capture qualitative data. We contextualize the presentation of the XAI\npapers included in our analysis according to the components of rigor described\nin the qualitative research literature: 1) underlying theories or frameworks,\n2) methodological approaches, 3) data collection methods, and 4) data analysis\nprocesses. The results of our analysis support calls from others in the XAI\ncommunity advocating for collaboration with experts from social disciplines to\nbolster rigor and effectiveness in user studies.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 21:02:16 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 22:22:13 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Johs", "Adam J.", ""], ["Agosto", "Denise E.", ""], ["Weber", "Rosina O.", ""]]}, {"id": "2011.07303", "submitter": "Sophia Duan", "authors": "Mark Frost, Sophia Xiaoxia Duan", "title": "Rethinking the Role of Technology in Virtual Teams in Light of COVID-19", "comments": "Australasian Conference on Information Systems (ACIS2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of virtual teams by organisations has grown tremendously as a\nstrategic response to COVID-19. However, the concept of virtual teams is not\nsomething new, with many businesses over the past three decades gradually\nincorporating virtual and/or dispersed teams into their processes. Research on\nvirtual teams has followed that of co-located face-to-face teams through lenses\nsuch as trust, communication, teamwork, leadership and collaboration. This\npaper introduces a new paradigm for examining the development of virtual teams,\narguably one that would facilitate the consideration of technology as part of a\nvirtual team rather than simply as an alternate to face-to-face teams. That is,\nviewing the development of virtual teams with embedded technology within an\norganisation through an innovation framework.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 13:50:05 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Frost", "Mark", ""], ["Duan", "Sophia Xiaoxia", ""]]}, {"id": "2011.07424", "submitter": "Zheng Wang", "authors": "Zhanhong Yan, Kaiming Yang, Zheng Wang, Bo Yang, Tsutomu Kaizuka,\n  Kimihiko Nakano", "title": "Intention-Based Lane Changing and Lane Keeping Haptic Guidance Steering\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haptic guidance in a shared steering assistance system has drawn significant\nattention in intelligent vehicle fields, owing to its mutual communication\nability for vehicle control. By exerting continuous torque on the steering\nwheel, both the driver and support system can share lateral control of the\nvehicle. However, current haptic guidance steering systems demonstrate some\ndeficiencies in assisting lane changing. This study explored a new steering\ninteraction method, including the design and evaluation of an intention-based\nhaptic shared steering system. Such an intention-based method can support both\nlane keeping and lane changing assistance, by detecting a driver lane change\nintention. By using a deep learning-based method to model a driver decision\ntiming regarding lane crossing, an adaptive gain control method was proposed\nfor realizing a steering control system. An intention consistency method was\nproposed to detect whether the driver and the system were acting towards the\nsame target trajectories and to accurately capture the driver intention. A\ndriving simulator experiment was conducted to test the system performance.\nParticipants were required to perform six trials with assistive methods and one\ntrial without assistance. The results demonstrated that the supporting system\ndecreased the lane departure risk in the lane keeping tasks and could support a\nfast and stable lane changing maneuver.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 00:55:09 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yan", "Zhanhong", ""], ["Yang", "Kaiming", ""], ["Wang", "Zheng", ""], ["Yang", "Bo", ""], ["Kaizuka", "Tsutomu", ""], ["Nakano", "Kimihiko", ""]]}, {"id": "2011.07510", "submitter": "Niek Mulleners", "authors": "Niek Mulleners, Johan Jeuring and Bastiaan Heeren", "title": "Model-Driven Synthesis for Programming Tutors", "comments": "presented at HATRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  When giving automated feedback to a student working on a beginner's exercise,\nmany programming tutors run into a completeness problem. On the one hand, we\nwant a student to experiment freely. On the other hand, we want a student to\nwrite her program in such a way that we can provide constructive feedback. We\npropose to investigate how we can overcome this problem by using program\nsynthesis, which we use to generate correct solutions that closely match a\nstudent program, and give feedback based on the results.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 12:18:47 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Mulleners", "Niek", ""], ["Jeuring", "Johan", ""], ["Heeren", "Bastiaan", ""]]}, {"id": "2011.07532", "submitter": "Michael Aupetit", "authors": "Michael Aupetit", "title": "Aquanims -- Area-Preserving Animated Transitions based on a Hydraulic\n  Metaphor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose \"Aquanims\" as new design metaphors for animated transitions that\npreserve displayed areas during the transformation. As liquids are\nincompressible fluids, we use a hydraulic metaphor to convey the sense of area\npreservation during animated transitions. We study the design space of Aquanims\nfor rectangle-based charts.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 14:00:27 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Aupetit", "Michael", ""]]}, {"id": "2011.07586", "submitter": "Umang Bhatt", "authors": "Umang Bhatt, Javier Antor\\'an, Yunfeng Zhang, Q. Vera Liao, Prasanna\n  Sattigeri, Riccardo Fogliato, Gabrielle Gauthier Melan\\c{c}on, Ranganath\n  Krishnan, Jason Stanley, Omesh Tickoo, Lama Nachman, Rumi Chunara, Madhulika\n  Srikumar, Adrian Weller, Alice Xiang", "title": "Uncertainty as a Form of Transparency: Measuring, Communicating, and\n  Using Uncertainty", "comments": "AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society\n  (AIES) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic transparency entails exposing system properties to various\nstakeholders for purposes that include understanding, improving, and contesting\npredictions. Until now, most research into algorithmic transparency has\npredominantly focused on explainability. Explainability attempts to provide\nreasons for a machine learning model's behavior to stakeholders. However,\nunderstanding a model's specific behavior alone might not be enough for\nstakeholders to gauge whether the model is wrong or lacks sufficient knowledge\nto solve the task at hand. In this paper, we argue for considering a\ncomplementary form of transparency by estimating and communicating the\nuncertainty associated with model predictions. First, we discuss methods for\nassessing uncertainty. Then, we characterize how uncertainty can be used to\nmitigate model unfairness, augment decision-making, and build trustworthy\nsystems. Finally, we outline methods for displaying uncertainty to stakeholders\nand recommend how to collect information required for incorporating uncertainty\ninto existing ML pipelines. This work constitutes an interdisciplinary review\ndrawn from literature spanning machine learning, visualization/HCI, design,\ndecision-making, and fairness. We aim to encourage researchers and\npractitioners to measure, communicate, and use uncertainty as a form of\ntransparency.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 17:26:14 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 14:11:01 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 10:33:03 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Bhatt", "Umang", ""], ["Antor\u00e1n", "Javier", ""], ["Zhang", "Yunfeng", ""], ["Liao", "Q. Vera", ""], ["Sattigeri", "Prasanna", ""], ["Fogliato", "Riccardo", ""], ["Melan\u00e7on", "Gabrielle Gauthier", ""], ["Krishnan", "Ranganath", ""], ["Stanley", "Jason", ""], ["Tickoo", "Omesh", ""], ["Nachman", "Lama", ""], ["Chunara", "Rumi", ""], ["Srikumar", "Madhulika", ""], ["Weller", "Adrian", ""], ["Xiang", "Alice", ""]]}, {"id": "2011.07591", "submitter": "Matteo Macchini", "authors": "Matteo Macchini, Jan Frogg, Fabrizio Schiano and Dario Floreano", "title": "Does spontaneous motion lead to intuitive Body-Machine Interfaces? A\n  fitness study of different body segments for wearable telerobotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Robot Interfaces (HRIs) represent a crucial component in telerobotic\nsystems. Body-Machine Interfaces (BoMIs) based on body motion can feel more\nintuitive than standard HRIs for naive users as they leverage humans' natural\ncontrol capability over their movements. Among the different methods used to\nmap human gestures into robot commands, data-driven approaches select a set of\nbody segments and transform their motion into commands for the robot based on\nthe users' spontaneous motion patterns. Despite being a versatile and generic\nmethod, there is no scientific evidence that implementing an interface based on\nspontaneous motion maximizes its effectiveness. In this study, we compare a set\nof BoMIs based on different body segments to investigate this aspect. We\nevaluate the interfaces in a teleoperation task of a fixed-wing drone and\nobserve users' performance and feedback. To this aim, we use a framework that\nallows a user to control the drone with a single Inertial Measurement Unit\n(IMU) and without prior instructions. We show through a user study that\nselecting the body segment for a BoMI based on spontaneous motion can lead to\nsub-optimal performance. Based on our findings, we suggest additional metrics\nbased on biomechanical and behavioral factors that might improve data-driven\nmethods for the design of HRIs.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 17:45:33 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Macchini", "Matteo", ""], ["Frogg", "Jan", ""], ["Schiano", "Fabrizio", ""], ["Floreano", "Dario", ""]]}, {"id": "2011.07656", "submitter": "Vidhi Jain", "authors": "Vidhi Jain, Rohit Jena, Huao Li, Tejus Gupta, Dana Hughes, Michael\n  Lewis, Katia Sycara", "title": "Predicting Human Strategies in Simulated Search and Rescue Task", "comments": "Accepted at NeurIPS 2020; Workshop on Artificial Intelligence for\n  Humanitarian Assistance and Disaster Response (AI+HADR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a search and rescue scenario, rescuers may have different knowledge of the\nenvironment and strategies for exploration. Understanding what is inside a\nrescuer's mind will enable an observer agent to proactively assist them with\ncritical information that can help them perform their task efficiently. To this\nend, we propose to build models of the rescuers based on their trajectory\nobservations to predict their strategies. In our efforts to model the rescuer's\nmind, we begin with a simple simulated search and rescue task in Minecraft with\nhuman participants. We formulate neural sequence models to predict the triage\nstrategy and the next location of the rescuer. As the neural networks are\ndata-driven, we design a diverse set of artificial \"faux human\" agents for\ntraining, to test them with limited human rescuer trajectory data. To evaluate\nthe agents, we compare it to an evidence accumulation method that explicitly\nincorporates all available background knowledge and provides an intended upper\nbound for the expected performance. Further, we perform experiments where the\nobserver/predictor is human. We show results in terms of prediction accuracy of\nour computational approaches as compared with that of human observers.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 23:24:23 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 23:26:39 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Jain", "Vidhi", ""], ["Jena", "Rohit", ""], ["Li", "Huao", ""], ["Gupta", "Tejus", ""], ["Hughes", "Dana", ""], ["Lewis", "Michael", ""], ["Sycara", "Katia", ""]]}, {"id": "2011.07713", "submitter": "Shalabh Gupta", "authors": "Jing Yang and James P. Wilson and Shalabh Gupta", "title": "DARE: AI-based Diver Action Recognition System using Multi-Channel CNNs\n  for AUV Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of sensing, control and robotic technologies, autonomous\nunderwater vehicles (AUVs) have become useful assistants to human divers for\nperforming various underwater operations. In the current practice, the divers\nare required to carry expensive, bulky, and waterproof keyboards or\njoystick-based controllers for supervision and control of AUVs. Therefore,\ndiver action-based supervision is becoming increasingly popular because it is\nconvenient, easier to use, faster, and cost effective. However, the various\nenvironmental, diver and sensing uncertainties present underwater makes it\nchallenging to train a robust and reliable diver action recognition system. In\nthis regard, this paper presents DARE, a diver action recognition system, that\nis trained based on Cognitive Autonomous Driving Buddy (CADDY) dataset, which\nis a rich set of data containing images of different diver gestures and poses\nin several different and realistic underwater environments. DARE is based on\nfusion of stereo-pairs of camera images using a multi-channel convolutional\nneural network supported with a systematically trained tree-topological deep\nneural network classifier to enhance the classification performance. DARE is\nfast and requires only a few milliseconds to classify one stereo-pair, thus\nmaking it suitable for real-time underwater implementation. DARE is\ncomparatively evaluated against several existing classifier architectures and\nthe results show that DARE supersedes the performance of all classifiers for\ndiver action recognition in terms of overall as well as individual class\naccuracies and F1-scores.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 04:05:32 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yang", "Jing", ""], ["Wilson", "James P.", ""], ["Gupta", "Shalabh", ""]]}, {"id": "2011.07901", "submitter": "Mla{\\dj}an Jovanovi\\'c Dr", "authors": "Jasna Petrovic, Mladjan Jovanovic", "title": "Conversational agents for learning foreign languages -- a survey", "comments": "Sinteza 2020 - International Scientific Conference on Information\n  Technology and Data Related Research, Belgrade, Singidunum University, Serbia", "journal-ref": null, "doi": "10.15308/Sinteza-2020-14-22", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversational practice, while crucial for all language learners, can be\nchallenging to get enough of and very expensive. Chatbots are computer programs\ndeveloped to engage in conversations with humans. They are designed as software\navatars with limited, but growing conversational capability. The most natural\nand potentially powerful application of chatbots is in line with their\nfundamental nature - language practice. However, their role and outcomes within\n(in)formal language learning are currently tangential at best. Existing\nresearch in the area has generally focused on chatbots' comprehensibility and\nthe motivation they inspire in their users. In this paper, we provide an\noverview of the chatbots for learning languages, critically analyze existing\napproaches, and discuss the major challenges for future work.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 12:27:02 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Petrovic", "Jasna", ""], ["Jovanovic", "Mladjan", ""]]}, {"id": "2011.07926", "submitter": "Patrick Saalfeld", "authors": "Patrick Saalfeld and Anna Schmeier and Wolfgang D'Hanis and\n  Hermann-Josef Rothk\\\"otter and Bernhard Preim", "title": "Student and Teacher Meet in a Shared Virtual Reality: A one-on-one\n  Tutoring System for Anatomy Education", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a Virtual Reality (VR) one-on-one tutoring system to support\nanatomy education. A student uses a fully immersive VR headset to explore the\nanatomy of the base of the human skull. A teacher guides the student by using\nthe semi-immersive zSpace. Both systems are connected via network and each\naction is synchronized between both systems.\n  The teacher is provided with various features to direct the student through\nthe immersive learning experience. She can influence the student's navigation\nor provide annotations on the fly and, hereby, improve the students learning\nexperience. The system is implemented using the \\textit{Unity} game engine. A\nqualitative user study demonstrates that the one-on-one tutoring approach is\nfeasible and sets a solid base for future research in the area of shared\nvirtual environments for anatomy education.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:17:52 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Saalfeld", "Patrick", ""], ["Schmeier", "Anna", ""], ["D'Hanis", "Wolfgang", ""], ["Rothk\u00f6tter", "Hermann-Josef", ""], ["Preim", "Bernhard", ""]]}, {"id": "2011.08130", "submitter": "Thomas Zimmermann", "authors": "Paige Rodeghero, Thomas Zimmermann, Brian Houck, Denae Ford", "title": "Please Turn Your Cameras On: Remote Onboarding of Software Developers\n  during a Pandemic", "comments": "10 pages. Final version of the paper accepted at ICSE 2021 in the\n  SEIP track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has impacted the way that software development teams\nonboard new hires. Previously, most software developers worked in physical\noffices and new hires onboarded to their teams in the physical office,\nfollowing a standard onboarding process. However, when companies transitioned\nemployees to work from home due to the pandemic, there was little to no time to\ndevelop new onboarding procedures. In this paper, we present a survey of 267\nnew hires at Microsoft that onboarded to software development teams during the\npandemic. We explored their remote onboarding process, including the challenges\nthat the new hires encountered and their social connectedness with their teams.\nWe found that most developers onboarded remotely and never had an opportunity\nto meet their teammates in person. This leads to one of the biggest challenges\nfaced by these new hires, building a strong social connection with their team.\nWe use these results to provide recommendations for onboarding remote hires.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 17:52:03 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 03:33:28 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Rodeghero", "Paige", ""], ["Zimmermann", "Thomas", ""], ["Houck", "Brian", ""], ["Ford", "Denae", ""]]}, {"id": "2011.08302", "submitter": "Varun Mishra", "authors": "Varun Mishra, Florian K\\\"unzler, Jan-Niklas Kramer, Elgar Fleisch,\n  Tobias Kowatsch, David Kotz", "title": "Detecting Receptivity for mHealth Interventions in the Natural\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JITAI is an emerging technique with great potential to support health\nbehavior by providing the right type and amount of support at the right time. A\ncrucial aspect of JITAIs is properly timing the delivery of interventions, to\nensure that a user is receptive and ready to process and use the support\nprovided. Some prior works have explored the association of context and some\nuser-specific traits on receptivity, and have built post-study machine-learning\nmodels to detect receptivity. For effective intervention delivery, however, a\nJITAI system needs to make in-the-moment decisions about a user's receptivity.\nTo this end, we conducted a study in which we deployed machine-learning models\nto detect receptivity in the natural environment, i.e., in free-living\nconditions.\n  We leveraged prior work regarding receptivity to JITAIs and deployed a\nchatbot-based digital coach~-- Ally~-- that provided physical-activity\ninterventions and motivated participants to achieve their step goals. We\nextended the original Ally~app to include two types of machine-learning model\nthat used contextual information about a person to predict when a person is\nreceptive: a \\textit{static model\\/} that was built before the study started\nand remained constant for all participants and an \\textit{adaptive model\\/}\nthat continuously learned the receptivity of individual participants and\nupdated itself as the study progressed. For comparison, we included a\n\\textit{control model\\/} that sent intervention messages at random times. The\napp randomly selected a delivery model for each intervention message. We\nobserved that the machine-learning models led up to a 40\\% improvement in\nreceptivity as compared to the control model. Further, we evaluated the\ntemporal dynamics of the different models and observed that receptivity to\nmessages from the adaptive model increased over the course of the study.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 21:53:00 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 00:09:14 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mishra", "Varun", ""], ["K\u00fcnzler", "Florian", ""], ["Kramer", "Jan-Niklas", ""], ["Fleisch", "Elgar", ""], ["Kowatsch", "Tobias", ""], ["Kotz", "David", ""]]}, {"id": "2011.08311", "submitter": "Cristina Gena", "authors": "Cristina Gena, Claudio Mattutino, Davide Cellie and Enrico Mosca", "title": "Educational robotics for children and their teachers", "comments": "3 pages extracted from an unpublished work presented at Child-Robot\n  Interaction workshop: Present and Future Relationships", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a Google Educator funded project devoted to the training\nof teachers (primary and secondary school) through an e-learning platform that\nwill introduce them to educational robotics using Wolly, a social, educational\nand affective robot.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 22:18:29 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 23:02:10 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Gena", "Cristina", ""], ["Mattutino", "Claudio", ""], ["Cellie", "Davide", ""], ["Mosca", "Enrico", ""]]}, {"id": "2011.08324", "submitter": "Alicia Nobles", "authors": "Rachel Dorn, Alicia L. Nobles, Masoud Rouhizadeh, Mark Dredze", "title": "Examining the Feasibility of Off-the-Shelf Algorithms for Masking\n  Directly Identifiable Information in Social Media Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The identification and removal/replacement of protected information from\nsocial media data is an understudied problem, despite being desirable from an\nethical and legal perspective. This paper identifies types of potentially\ndirectly identifiable information (inspired by protected health information in\nclinical texts) contained in tweets that may be readily removed using\noff-the-shelf algorithms, introduces an English dataset of tweets annotated for\nidentifiable information, and compiles these off-the-shelf algorithms into a\ntool (Nightjar) to evaluate the feasibility of using Nightjar to remove\ndirectly identifiable information from the tweets. Nightjar as well as the\nannotated data can be retrieved from https://bitbucket.org/mdredze/nightjar.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 22:55:49 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Dorn", "Rachel", ""], ["Nobles", "Alicia L.", ""], ["Rouhizadeh", "Masoud", ""], ["Dredze", "Mark", ""]]}, {"id": "2011.08334", "submitter": "Michael Wessel", "authors": "Michael Wessel, Edgar Kalns, Girish Acharya, Andreas Kathol", "title": "Widening the Dialogue Workflow Modeling Bottleneck in Ontology-Based\n  Personal Assistants", "comments": "7 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to dialogue specification for Virtual Personal\nAssistants (VPAs) based on so-called dialogue workflow graphs, with several\ndemonstrated advantages over current ontology-based methods. Our new dialogue\nspecification language (DSL) enables customers to more easily participate in\nthe VPA modeling process due to a user-friendly modeling framework. Resulting\nmodels are also significantly more compact. VPAs can be developed much more\nrapidly. The DSL is a new modeling layer on top of our ontology-based Dialogue\nManagement (DM) framework OntoVPA. We explain the rationale and benefits behind\nthe new language and support our claims with concrete reduced Level-of-Effort\n(LOE) numbers from two recent OntoVPA projects.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 23:32:43 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wessel", "Michael", ""], ["Kalns", "Edgar", ""], ["Acharya", "Girish", ""], ["Kathol", "Andreas", ""]]}, {"id": "2011.08398", "submitter": "Tong Wang", "authors": "Tong Wang and Maytal Saar-Tsechansky", "title": "Augmented Fairness: An Interpretable Model Augmenting Decision-Makers'\n  Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model-agnostic approach for mitigating the prediction bias of a\nblack-box decision-maker, and in particular, a human decision-maker. Our method\ndetects in the feature space where the black-box decision-maker is biased and\nreplaces it with a few short decision rules, acting as a \"fair surrogate\". The\nrule-based surrogate model is trained under two objectives, predictive\nperformance and fairness. Our model focuses on a setting that is common in\npractice but distinct from other literature on fairness. We only have black-box\naccess to the model, and only a limited set of true labels can be queried under\na budget constraint. We formulate a multi-objective optimization for building a\nsurrogate model, where we simultaneously optimize for both predictive\nperformance and bias. To train the model, we propose a novel training algorithm\nthat combines a nondominated sorting genetic algorithm with active learning. We\ntest our model on public datasets where we simulate various biased \"black-box\"\nclassifiers (decision-makers) and apply our approach for interpretable\naugmented fairness.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 03:25:44 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wang", "Tong", ""], ["Saar-Tsechansky", "Maytal", ""]]}, {"id": "2011.08409", "submitter": "Kaiping Chen", "authors": "Kaiping Chen, Anfan Chen, Jingwen Zhang, Jingbo Meng, Cuihua Shen", "title": "Conspiracy and debunking narratives about COVID-19 origination on\n  Chinese social media: How it started and who is to blame", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper studies conspiracy and debunking narratives about COVID-19\norigination on a major Chinese social media platform, Weibo, from January to\nApril 2020. Popular conspiracies about COVID-19 on Weibo, including that the\nvirus is human-synthesized or a bioweapon, differ substantially from those in\nthe US. They attribute more responsibility to the US than to China, especially\nfollowing Sino-US confrontations. Compared to conspiracy posts, debunking posts\nare associated with lower user participation but higher mobilization. Debunking\nnarratives can be more engaging when they come from women and influencers and\ncite scientists. Our findings suggest that conspiracy narratives can carry\nhighly cultural and political orientations. Correction efforts should consider\npolitical motives and identify important stakeholders to reconstruct\ninternational dialogues toward intercultural understanding.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 03:54:06 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Chen", "Kaiping", ""], ["Chen", "Anfan", ""], ["Zhang", "Jingwen", ""], ["Meng", "Jingbo", ""], ["Shen", "Cuihua", ""]]}, {"id": "2011.08690", "submitter": "Aniket Bera", "authors": "Pooja Guhan and Manas Agarwal and Naman Awasthi and Gloria Reeves and\n  Dinesh Manocha and Aniket Bera", "title": "ABC-Net: Semi-Supervised Multimodal GAN-based Engagement Detection using\n  an Affective, Behavioral and Cognitive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ABC-Net, a novel semi-supervised multimodal GAN framework to\ndetect engagement levels in video conversations based on psychology literature.\nWe use three constructs: behavioral, cognitive, and affective engagement, to\nextract various features that can effectively capture engagement levels. We\nfeed these features to our semi-supervised GAN network that does regression\nusing these latent representations to obtain the corresponding valence and\narousal values, which are then categorized into different levels of\nengagements. We demonstrate the efficiency of our network through experiments\non the RECOLA database. To evaluate our method, we analyze and compare our\nperformance on RECOLA and report a relative performance improvement of more\nthan 5% over the baseline methods. To the best of our knowledge, our approach\nis the first method to classify engagement based on a multimodal\nsemi-supervised network.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:18:38 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Guhan", "Pooja", ""], ["Agarwal", "Manas", ""], ["Awasthi", "Naman", ""], ["Reeves", "Gloria", ""], ["Manocha", "Dinesh", ""], ["Bera", "Aniket", ""]]}, {"id": "2011.09001", "submitter": "Dusanka Boskovic", "authors": "Dusanka Boskovic, Nihad Borovina, Merima Zukic", "title": "Stimulating Entrepreneurship in Teaching Human Computer Interaction", "comments": "4 pages, 3 figures, conference paper, presented at the 29th\n  International Electrotechnical and Computer Science Conference ERK 2020 -\n  Portotoz, Slovenia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Software development requires understanding of users, user needs, user tasks\nand context in which they are operating. These skills are familiar to\nentrepreneurs, product managers, and marketing experts. However, our teaching\nexperience suggests that students generally find these topics less attractive\nas they perceive them to be far too theoretical and thus, not as useful. During\nthe years of teaching the Human Computer Interaction course we have noticed\nstudents' preferences for learning technology oriented methods, or what we\nrefer to topics belonging to solution domain. The changes in the modernized HCI\ncourse introduced Product Market Fit canvas in order to bridge the gap between\n'theoretical' and 'practical' part of the course.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 23:31:18 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Boskovic", "Dusanka", ""], ["Borovina", "Nihad", ""], ["Zukic", "Merima", ""]]}, {"id": "2011.09004", "submitter": "Rebecca Russell", "authors": "Aastha Acharya, Rebecca Russell, Nisar R. Ahmed", "title": "Explaining Conditions for Reinforcement Learning Behaviors from Real and\n  Imagined Data", "comments": "Accepted to the Workshop on Challenges of Real-World RL at NeurIPS\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of reinforcement learning (RL) in the real world comes with\nchallenges in calibrating user trust and expectations. As a step toward\ndeveloping RL systems that are able to communicate their competencies, we\npresent a method of generating human-interpretable abstract behavior models\nthat identify the experiential conditions leading to different task execution\nstrategies and outcomes. Our approach consists of extracting experiential\nfeatures from state representations, abstracting strategy descriptors from\ntrajectories, and training an interpretable decision tree that identifies the\nconditions most predictive of different RL behaviors. We demonstrate our method\non trajectory data generated from interactions with the environment and on\nimagined trajectory data that comes from a trained probabilistic world model in\na model-based RL setting.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 23:40:47 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Acharya", "Aastha", ""], ["Russell", "Rebecca", ""], ["Ahmed", "Nisar R.", ""]]}, {"id": "2011.09088", "submitter": "Michael Lyons", "authors": "Michael J. Lyons and Daniel Kluender", "title": "Three Patterns to Support Empathy in Computer-Mediated Human Interaction", "comments": "6 pages, 4 figures", "journal-ref": "ACM CHI'04 Workshop Human-Computer-Human Interaction Patterns:\n  Workshop on the Human Role in HCI Patterns Vienna, Austria, April 25-26, 2004", "doi": "10.5281/zenodo.4278447", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present three patterns for computer-mediated interaction which we\ndiscovered during the design and development of a platform for remote teaching\nand learning of kanji, the Chinese characters used in written Japanese. Our aim\nin developing this system was to provide a basis for embodiment in remote\ninteraction, and in particular to support the experience of empathy by both\nteacher and student. From this study, the essential elements are abstracted and\nsuggested as design patterns for other computer-mediated interaction systems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 05:02:20 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Lyons", "Michael J.", ""], ["Kluender", "Daniel", ""]]}, {"id": "2011.09130", "submitter": "Claudio Di Ciccio", "authors": "Anton Yeshchenko, Claudio Di Ciccio, Jan Mendling, Artem Polyvyanyy", "title": "Visual Drift Detection for Sequence Data Analysis of Business Processes", "comments": "arXiv admin note: text overlap with arXiv:1907.06386", "journal-ref": null, "doi": "10.1109/TVCG.2021.3050071", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event sequence data is increasingly available in various application domains,\nsuch as business process management, software engineering, or medical pathways.\nProcesses in these domains are typically represented as process diagrams or\nflow charts. So far, various techniques have been developed for automatically\ngenerating such diagrams from event sequence data. An open challenge is the\nvisual analysis of drift phenomena when processes change over time. In this\npaper, we address this research gap. Our contribution is a system for\nfine-granular process drift detection and corresponding visualizations for\nevent logs of executed business processes. We evaluated our system both on\nsynthetic and real-world data. On synthetic logs, we achieved an average\nF-score of 0.96 and outperformed all the state-of-the-art methods. On\nreal-world logs, we identified all types of process drifts in a comprehensive\nmanner. Finally, we conducted a user study highlighting that our visualizations\nare easy to use and useful as perceived by process mining experts. In this way,\nour work contributes to research on process mining, event sequence analysis,\nand visualization of temporal data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 17:14:33 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 10:55:41 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 16:00:07 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2021 18:50:02 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yeshchenko", "Anton", ""], ["Di Ciccio", "Claudio", ""], ["Mendling", "Jan", ""], ["Polyvyanyy", "Artem", ""]]}, {"id": "2011.09138", "submitter": "Stefan Langer", "authors": "Markus Friedrich, Stefan Langer, Fabian Frey", "title": "Combining Gesture and Voice Control for Mid-Air Manipulation of CAD\n  Models in VR Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling 3D objects in domains like Computer Aided Design (CAD) is\ntime-consuming and comes with a steep learning curve needed to master the\ndesign process as well as tool complexities. In order to simplify the modeling\nprocess, we designed and implemented a prototypical system that leverages the\nstrengths of Virtual Reality (VR) hand gesture recognition in combination with\nthe expressiveness of a voice-based interface for the task of 3D modeling.\nFurthermore, we use the Constructive Solid Geometry (CSG) tree representation\nfor 3D models within the VR environment to let the user manipulate objects from\nthe ground up, giving an intuitive understanding of how the underlying basic\nshapes connect. The system uses standard mid-air 3D object manipulation\ntechniques and adds a set of voice commands to help mitigate the deficiencies\nof current hand gesture recognition techniques. A user study was conducted to\nevaluate the proposed prototype. The combination of our hybrid input paradigm\nshows to be a promising step towards easier to use CAD modeling.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 07:26:29 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Friedrich", "Markus", ""], ["Langer", "Stefan", ""], ["Frey", "Fabian", ""]]}, {"id": "2011.09143", "submitter": "Quinn Jarvis-Holland", "authors": "Quinn Jarvis-Holland, Crystal Cortez, Nathan (Station) Gamill,\n  Francisco Botello", "title": "Expanding Access to Music Technology -- Rapid Prototyping Accessible\n  Instrument Solutions For Musicians With Intellectual Disabilities", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using open-source and creative coding frameworks, a teamof artist-engineers\nfrom Portland Community College work-ing with artists who experience\nIntellectual/Developmentaldisabilities prototyped an ensemble of adapted\ninstrumentsand synthesizers that facilitate real-time in-key collabora-tion.\nThe instruments employ a variety of sensors, send-ing the resulting musical\ncontrols to software sound gener-ators via MIDI. Careful consideration was\ngiven to the bal-ance between freedom of expression, and curating the pos-sible\nsonic outcomes as adaptation. Evaluation of adaptedinstrument design may differ\ngreatly from frameworks forevaluating traditional instruments or products\nintended formass-market, though the results of such focused and indi-vidualised\ndesign have a variety of possible applications.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 07:48:12 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Jarvis-Holland", "Quinn", "", "Station"], ["Cortez", "Crystal", "", "Station"], ["Nathan", "", "", "Station"], ["Gamill", "", ""], ["Botello", "Francisco", ""]]}, {"id": "2011.09201", "submitter": "Martin Pichlmair", "authors": "Martin Pichlmair and Mads Johansen", "title": "Designing Game Feel. A Survey", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": "10.1109/TG.2021.3072241", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game feel design is the intentional design of the affective impact of\nmoment-to-moment interaction with games. In this paper we survey academic\nresearch and publications by practitioners to give a complete overview of the\nstate of research concerning this aspect of game design, including context from\nrelated areas. We analysed over 200 sources and categorised their content\naccording to the design purpose presented. This resulted in three different\ndomains of intended player experiences: physicality, amplification, and\nsupport. In these domains, the act of polishing that determines game feel,\ntakes the shape of tuning, juicing, and streamlining respectively. Tuning the\nphysicality of game objects creates cohesion, predictability, and the resulting\nmovement informs many other design aspects. Juicing is the act of polishing\namplification and it results in empowerment and provides clarity of feedback by\ncommunicating the importance of game events. Streamlining allows a game to act\non the intention of the player, supporting the execution of actions in the\ngame. These three design intents are the main means through which designers\ncontrol minute details of interactivity and inform the player's reaction. This\nframework and its nuanced vocabulary can lead to an understanding of game feel\nthat is shared between practitioners and researchers as highlighted in the\nconcluding future research section.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 10:39:18 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Pichlmair", "Martin", ""], ["Johansen", "Mads", ""]]}, {"id": "2011.09239", "submitter": "Christian Janiesch", "authors": "Christian Janiesch, Marcus Fischer, Axel Winkelmann, Valentin Nentwich", "title": "Specifying Autonomy in the Internet of Things: The Autonomy Model and\n  Notation", "comments": "Authors Accepted Manuscript", "journal-ref": "Inf Syst E-Bus Manage 17, 159-194 (2019)", "doi": "10.1007/s10257-018-0379-x", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Driven by digitization in society and industry, automating behavior in an\nautonomous way substantially alters industrial value chains in the smart\nservice world. As processes are enhanced with sensor and actuator technology,\nthey become digitally interconnected and merge into an Internet of Things (IoT)\nto form cyber-physical systems (CPS). Using these automated systems,\nenterprises can improve the performance and quality of their operations.\nHowever, currently it is neither feasible nor reasonable to equip any machine\nwith full autonomy when networking with other machines or people. It is\nnecessary to specify rules for machine behavior that also determine an adequate\ndegree of autonomy to realize the potential benefits of the IoT. Yet, there is\na lack of methodologies and guidelines to support the design and implementation\nof machines as explicit autonomous agents such that many designs only consider\nautonomy implicitly. To address this research gap, we perform a comprehensive\nliterature review to extract 12 requirements for the design of autonomous\nagents in the IoT. We introduce a set of constitutive characteristics for\nagents and introduce a classification framework for interactions in multi-agent\nsystems. We integrate our findings by developing a conceptual modeling language\nconsisting of a meta model and a notation that facilitates the specification\nand design of autonomous agents within the IoT as well as CPS: the Autonomy\nModel and Notation. We illustrate and discuss the approach and its limitations.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 12:24:27 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Janiesch", "Christian", ""], ["Fischer", "Marcus", ""], ["Winkelmann", "Axel", ""], ["Nentwich", "Valentin", ""]]}, {"id": "2011.09696", "submitter": "Rui Zhang", "authors": "Rui Zhang, Kai Yin, Li Li", "title": "Towards Emotion-Aware User Simulator for Task-Oriented Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of a task-completion dialogue agent usually affects the user\nexperience: when the conversation system yields an unreasonable response, users\nmay feel dissatisfied. Besides, early termination often occurs in disappointing\nconversations. However, existing off-the-shelf user simulators generally assume\nan ideal and cooperative user, which is somewhat different from a real user,\nand inevitably lead to a sub-optimal dialogue policy. In this paper, we propose\nan emotion-aware user simulation framework for task-oriented dialogue, which is\nbased on the OCC emotion model to update user emotions and drive user actions,\nto generate simulated behaviors that more similar to real users. We present a\nlinear implementation (The source code will be released soon.) that is easy to\nunderstand and extend, and evaluate it on two domain-specific datasets. The\nexperimental results show that the emotional simulation results of our proposed\nframework conform to common sense and have good versatility for different\ndomains. Meanwhile, our framework provides us with another perspective to\nunderstand the improvement process of the dialogue policy model based on\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 07:21:07 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Zhang", "Rui", ""], ["Yin", "Kai", ""], ["Li", "Li", ""]]}, {"id": "2011.09714", "submitter": "Michael Gastner", "authors": "Ian K. Duncan, Shi Tingsheng, Simon T. Perrault and Michael T. Gastner", "title": "Task-Based Effectiveness of Interactive Contiguous Area Cartograms", "comments": "18 pages, 5 figures. Supplemental material available by clicking on\n  \"Other formats\". To be published in IEEE Transactions on Visualization and\n  Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2020.3041745", "report-no": null, "categories": "cs.HC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartograms are map-based data visualizations in which the area of each map\nregion is proportional to an associated numeric data value (e.g., population or\ngross domestic product). A cartogram is called contiguous if it conforms to\nthis area principle while also keeping neighboring regions connected. Because\nof their distorted appearance, contiguous cartograms have been criticized as\ndifficult to read. Some authors have suggested that cartograms may be more\nlegible if they are accompanied by interactive features (e.g., animations,\nlinked brushing, or infotips). We conducted an experiment to evaluate this\nclaim. Participants had to perform visual analysis tasks with interactive and\nnoninteractive contiguous cartograms. The task types covered various aspects of\ncartogram readability, ranging from elementary lookup tasks to synoptic tasks\n(i.e., tasks in which participants had to summarize high-level differences\nbetween two cartograms). Elementary tasks were carried out equally well with\nand without interactivity. Synoptic tasks, by contrast, were more difficult\nwithout interactive features. With access to interactivity, however, most\nparticipants answered even synoptic questions correctly. In a subsequent\nsurvey, participants rated the interactive features as \"easy to use\" and\n\"helpful.\" Our study suggests that interactivity has the potential to make\ncontiguous cartograms accessible even for those readers who are unfamiliar with\ninteractive computer graphics or do not have a prior affinity to working with\nmaps. Among the interactive features, animations had the strongest positive\neffect, so we recommend them as a minimum of interactivity when contiguous\ncartograms are displayed on a computer screen.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 08:32:16 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Duncan", "Ian K.", ""], ["Tingsheng", "Shi", ""], ["Perrault", "Simon T.", ""], ["Gastner", "Michael T.", ""]]}, {"id": "2011.09749", "submitter": "Cristina Gena", "authors": "Cristina Gena, Fabiana Vernero, Claudio Mattutino, Giorgia Persichella\n  and Lomanto Luca", "title": "Experimenting Touchless Gestural Interaction for a University Public\n  Web-based Display", "comments": "Adjunt Proceedings of the 13th Biannual Conference of the Italian\n  SIGCHI Chapter (CHItaly 2019), 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interest in and development of touchless gestural interfaces has recently\nexploded, fueled by the diffusion of both commercial midair gesture platforms\nand public interactive displays. This paper focuses on an application based on\nMicrosoft Kinect that allows students to browse a university website, hosted on\na public display, through simple gestures. We present two empirical evaluations\nwhere we evaluated how users react to this new way of interaction. In addition\nto confirming the current lack of standards, our results provide some\ninspiration for the design of touchless interaction.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 09:58:03 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Gena", "Cristina", ""], ["Vernero", "Fabiana", ""], ["Mattutino", "Claudio", ""], ["Persichella", "Giorgia", ""], ["Luca", "Lomanto", ""]]}, {"id": "2011.09811", "submitter": "Bing Liu", "authors": "Bing Liu and Chuhe Mei", "title": "Lifelong Knowledge Learning in Rule-based Dialogue Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the main weaknesses of current chatbots or dialogue systems is that\nthey do not learn online during conversations after they are deployed. This is\na major loss of opportunity. Clearly, each human user has a great deal of\nknowledge about the world that may be useful to others. If a chatbot can learn\nfrom their users during chatting, it will greatly expand its knowledge base and\nserve its users better. This paper proposes to build such a learning capability\nin a rule-based chatbot so that it can continuously acquire new knowledge in\nits chatting with users. This work is useful because many real-life deployed\nchatbots are rule-based.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 13:33:12 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Liu", "Bing", ""], ["Mei", "Chuhe", ""]]}, {"id": "2011.09831", "submitter": "Javier Fumanal-Idocin Mr.", "authors": "Javier Fumanal-Idocin, Zdenko Tak\\'a\\v{c}, Javier Fern\\'andez Jose\n  Antonio Sanz, Harkaitz Goyena, Ching-Teng Lin, Yu-Kai Wang, Humberto Bustince", "title": "Interval-valued aggregation functions based on moderate deviations\n  applied to Motor-Imagery-Based Brain Computer Interface", "comments": null, "journal-ref": null, "doi": "10.1109/TFUZZ.2021.3092824", "report-no": null, "categories": "cs.HC cs.CV cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we study the use of moderate deviation functions to measure\nsimilarity and dissimilarity among a set of given interval-valued data. To do\nso, we introduce the notion of interval-valued moderate deviation function and\nwe study in particular those interval-valued moderate deviation functions which\npreserve the width of the input intervals. Then, we study how to apply these\nfunctions to construct interval-valued aggregation functions. We have applied\nthem in the decision making phase of two Motor-Imagery Brain Computer Interface\nframeworks, obtaining better results than those obtained using other numerical\nand intervalar aggregations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 14:10:29 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 07:09:17 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Fumanal-Idocin", "Javier", ""], ["Tak\u00e1\u010d", "Zdenko", ""], ["Sanz", "Javier Fern\u00e1ndez Jose Antonio", ""], ["Goyena", "Harkaitz", ""], ["Lin", "Ching-Teng", ""], ["Wang", "Yu-Kai", ""], ["Bustince", "Humberto", ""]]}, {"id": "2011.09896", "submitter": "Nikolaus Piccolotto", "authors": "Nikolaus Piccolotto, Markus B\\\"ogl, Theresia Gschwandtner, Christoph\n  Muehlmann, Klaus Nordhausen, Peter Filzmoser and Silvia Miksch", "title": "TBSSvis: Visual Analytics for Temporal Blind Source Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Blind Source Separation (TBSS) is used to obtain the true,\nunderlying processes from noisy temporal multivariate data, such as\nelectrocardiograms. While these algorithms are widely used, the involved tasks\nare not well supported in current visualization tools, which offer only\ntext-based interactions and static images. Analysts are limited in analyzing\nand comparing obtained results, which consist of diverse data such as matrices\nand ensembles of time series. Additionally, parameters have a big impact on\nseparation performance, but as a consequence of improper tooling analysts\ncurrently do not consider the whole parameter space. We propose to solve these\nproblems by applying visual analytics (VA) principles. To this end, we\ndeveloped a task abstraction and visualization design in a user-centered design\nprocess. We present TBSSvis, an interactive web-based VA prototype, which we\nevaluated in two qualitative user studies. Feedback and observations from these\nstudies show that TBSSvis supports the actual workflow and combination of\ninteractive visualizations that facilitate the tasks involved in analyzing TBBS\nresults. It also provides guidance to facilitate informed parameter selection\nand the analysis of the data at hand.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 15:29:16 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Piccolotto", "Nikolaus", ""], ["B\u00f6gl", "Markus", ""], ["Gschwandtner", "Theresia", ""], ["Muehlmann", "Christoph", ""], ["Nordhausen", "Klaus", ""], ["Filzmoser", "Peter", ""], ["Miksch", "Silvia", ""]]}, {"id": "2011.09988", "submitter": "Adam Coscia", "authors": "Adam Coscia (1), Duen Horng Chau (1), Alex Endert (1) ((1) Georgia\n  Tech)", "title": "Toward a Bias-Aware Future for Mixed-Initiative Visual Analytics", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mixed-initiative visual analytics systems incorporate well-established design\nprinciples that improve users' abilities to solve problems. As these systems\nconsider whether to take initiative towards achieving user goals, many current\nsystems address the potential for cognitive bias in human initiatives\nstatically, relying on fixed initiatives they can take instead of identifying,\ncommunicating and addressing the bias as it occurs. We argue that\nmixed-initiative design principles can and should incorporate cognitive bias\nmitigation strategies directly through development of mitigation techniques\nembedded in the system to address cognitive biases in situ. We identify domain\nexperts in machine learning adopting visual analytics techniques and systems\nthat incorporate existing mixed-initiative principles and examine their\npotential to support bias mitigation strategies. This examination considers the\nunique perspective these experts bring to visual analytics and is situated in\nexisting user-centered systems that make exemplary use of design principles\ninformed by cognitive theory. We then suggest informed opportunities for domain\nexperts to take initiative toward addressing cognitive biases in light of their\nexisting contributions to the field. Finally, we contribute open questions and\nresearch directions for designers seeking to adopt visual analytics techniques\nthat incorporate bias-aware initiatives in future systems.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 17:38:21 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Coscia", "Adam", ""], ["Chau", "Duen Horng", ""], ["Endert", "Alex", ""]]}, {"id": "2011.10118", "submitter": "Rogerio Bonatti", "authors": "Rogerio Bonatti, Arthur Bucker, Sebastian Scherer, Mustafa Mukadam and\n  Jessica Hodgins", "title": "Batteries, camera, action! Learning a semantic control space for\n  expressive robot cinematography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aerial vehicles are revolutionizing the way film-makers can capture shots of\nactors by composing novel aerial and dynamic viewpoints. However, despite great\nadvancements in autonomous flight technology, generating expressive camera\nbehaviors is still a challenge and requires non-technical users to edit a large\nnumber of unintuitive control parameters. In this work, we develop a\ndata-driven framework that enables editing of these complex camera positioning\nparameters in a semantic space (e.g. calm, enjoyable, establishing). First, we\ngenerate a database of video clips with a diverse range of shots in a\nphoto-realistic simulator, and use hundreds of participants in a crowd-sourcing\nframework to obtain scores for a set of semantic descriptors for each clip.\nNext, we analyze correlations between descriptors and build a semantic control\nspace based on cinematography guidelines and human perception studies. Finally,\nwe learn a generative model that can map a set of desired semantic video\ndescriptors into low-level camera trajectory parameters. We evaluate our system\nby demonstrating that our model successfully generates shots that are rated by\nparticipants as having the expected degrees of expression for each descriptor.\nWe also show that our models generalize to different scenes in both simulation\nand real-world experiments. Data and video found at:\nhttps://sites.google.com/view/robotcam.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 21:56:53 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 21:15:21 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bonatti", "Rogerio", ""], ["Bucker", "Arthur", ""], ["Scherer", "Sebastian", ""], ["Mukadam", "Mustafa", ""], ["Hodgins", "Jessica", ""]]}, {"id": "2011.10174", "submitter": "Tai Wang", "authors": "Tai Wang, Conghui He, Zhe Wang, Jianping Shi, Dahua Lin", "title": "FLAVA: Find, Localize, Adjust and Verify to Annotate LiDAR-Based Point\n  Clouds", "comments": "Full technical report for the UIST 2020 Poster version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent years have witnessed the rapid progress of perception algorithms on\ntop of LiDAR, a widely adopted sensor for autonomous driving systems. These\nLiDAR-based solutions are typically data hungry, requiring a large amount of\ndata to be labeled for training and evaluation. However, annotating this kind\nof data is very challenging due to the sparsity and irregularity of point\nclouds and more complex interaction involved in this procedure. To tackle this\nproblem, we propose FLAVA, a systematic approach to minimizing human\ninteraction in the annotation process. Specifically, we divide the annotation\npipeline into four parts: find, localize, adjust and verify. In addition, we\ncarefully design the UI for different stages of the annotation procedure, thus\nkeeping the annotators to focus on the aspects that are most important to each\nstage. Furthermore, our system also greatly reduces the amount of interaction\nby introducing a light-weight yet effective mechanism to propagate the\nannotation results. Experimental results show that our method can remarkably\naccelerate the procedure and improve the annotation quality.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 02:22:36 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Wang", "Tai", ""], ["He", "Conghui", ""], ["Wang", "Zhe", ""], ["Shi", "Jianping", ""], ["Lin", "Dahua", ""]]}, {"id": "2011.10363", "submitter": "David Hanson", "authors": "David Hanson, Frankie Storm, Wenwei Huang, Vytas Krisciunas, Tiger\n  Darrow, Audrey Brown, Mengna Lei, Matthew Aylett, Adam Pickrell, Sophia the\n  Robot", "title": "SophiaPop: Experiments in Human-AI Collaboration on Popular Music", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A diverse team of engineers, artists, and algorithms, collaborated to create\nsongs for SophiaPop, via various neural networks, robotics technologies, and\nartistic tools, and animated the results on Sophia the Robot, a robotic\ncelebrity and animated character. Sophia is a platform for arts, research, and\nother uses. To advance the art and technology of Sophia, we combine various AI\nwith a fictional narrative of her burgeoning career as a popstar. Her actual\nAI-generated pop lyrics, music, and paintings, and animated conversations\nwherein she interacts with humans real-time in narratives that discuss her\nexperiences. To compose the music, SophiaPop team built corpora from human and\nAI-generated Sophia character personality content, along with pop music song\nforms, to train and provide seeds for a number of AI algorithms including\nexpert models, and custom-trained transformer neural networks, which then\ngenerated original pop-song lyrics and melodies. Our musicians including\nFrankie Storm, Adam Pickrell, and Tiger Darrow, then performed interpretations\nof the AI-generated musical content, including singing and instrumentation. The\nhuman-performed singing data then was processed by a neural-network-based\nSophia voice, which was custom-trained from human performances by Cereproc.\nThis AI then generated the unique Sophia voice singing of the songs. Then we\nanimated Sophia to sing the songs in music videos, using a variety of animation\ngenerators and human-generated animations. Being algorithms and humans, working\ntogether, SophiaPop represents a human-AI collaboration, aspiring toward human\nAI symbiosis. We believe that such a creative convergence of multiple\ndisciplines with humans and AI working together, can make AI relevant to human\nculture in new and exciting ways, and lead to a hopeful vision for the future\nof human-AI relations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 12:05:50 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Hanson", "David", ""], ["Storm", "Frankie", ""], ["Huang", "Wenwei", ""], ["Krisciunas", "Vytas", ""], ["Darrow", "Tiger", ""], ["Brown", "Audrey", ""], ["Lei", "Mengna", ""], ["Aylett", "Matthew", ""], ["Pickrell", "Adam", ""], ["Robot", "Sophia the", ""]]}, {"id": "2011.10433", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina and Adria Perez-Rovira and Wieying Kuo and Harm A.\n  W. M. Tiddens and Marleen de Bruijne", "title": "Crowdsourcing Airway Annotations in Chest Computed Tomography Images", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0249580", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring airways in chest computed tomography (CT) scans is important for\ncharacterizing diseases such as cystic fibrosis, yet very time-consuming to\nperform manually. Machine learning algorithms offer an alternative, but need\nlarge sets of annotated scans for good performance. We investigate whether\ncrowdsourcing can be used to gather airway annotations. We generate image\nslices at known locations of airways in 24 subjects and request the crowd\nworkers to outline the airway lumen and airway wall. After combining multiple\ncrowd workers, we compare the measurements to those made by the experts in the\noriginal scans. Similar to our preliminary study, a large portion of the\nannotations were excluded, possibly due to workers misunderstanding the\ninstructions. After excluding such annotations, moderate to strong correlations\nwith the expert can be observed, although these correlations are slightly lower\nthan inter-expert correlations. Furthermore, the results across subjects in\nthis study are quite variable. Although the crowd has potential in annotating\nairways, further development is needed for it to be robust enough for gathering\nannotations in practice. For reproducibility, data and code are available\nonline: \\url{http://github.com/adriapr/crowdairway.git}.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 14:54:32 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Perez-Rovira", "Adria", ""], ["Kuo", "Wieying", ""], ["Tiddens", "Harm A. W. M.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "2011.10653", "submitter": "Abe Leite", "authors": "Abe Leite and Sa\\'ul A. Blanco", "title": "Effects of Human vs. Automatic Feedback on Students' Understanding of AI\n  Concepts and Programming Style", "comments": "Published in SIGCSE '20: Proceedings of the 51st ACM Technical\n  Symposium on Computer Science Education", "journal-ref": "SIGCSE '20: Proceedings of the 51st ACM Technical Symposium on\n  Computer Science Education (Feb 2020) 44-50", "doi": "10.1145/3328778.3366921", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of automatic grading tools has become nearly ubiquitous in large\nundergraduate programming courses, and recent work has focused on improving the\nquality of automatically generated feedback. However, there is a relative lack\nof data directly comparing student outcomes when receiving computer-generated\nfeedback and human-written feedback. This paper addresses this gap by splitting\none 90-student class into two feedback groups and analyzing differences in the\ntwo cohorts' performance. The class is an intro to AI with programming HW\nassignments. One group of students received detailed computer-generated\nfeedback on their programming assignments describing which parts of the\nalgorithms' logic was missing; the other group additionally received\nhuman-written feedback describing how their programs' syntax relates to issues\nwith their logic, and qualitative (style) recommendations for improving their\ncode. Results on quizzes and exam questions suggest that human feedback helps\nstudents obtain a better conceptual understanding, but analyses found no\ndifference between the groups' ability to collaborate on the final project. The\ncourse grade distribution revealed that students who received human-written\nfeedback performed better overall; this effect was the most pronounced in the\nmiddle two quartiles of each group. These results suggest that feedback about\nthe syntax-logic relation may be a primary mechanism by which human feedback\nimproves student outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 21:40:32 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Leite", "Abe", ""], ["Blanco", "Sa\u00fal A.", ""]]}, {"id": "2011.10754", "submitter": "Libu\\v{s}e Hannah Vep\\v{r}ek", "authors": "Libu\\v{s}e Hannah Vep\\v{r}ek, Patricia Seymour, Pietro Michelucci", "title": "Human computation requires and enables a new approach to ethical review", "comments": "8 pages, 4 figures. This is a pre-publication draft submitted to 34th\n  Conference on Neural Information Processing Systems (NeurIPS 2020),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With humans increasingly serving as computational elements in distributed\ninformation processing systems and in consideration of the profit-driven\nmotives and potential inequities that might accompany the emerging thinking\neconomy[1], we recognize the need for establishing a set of related ethics to\nensure the fair treatment and wellbeing of online cognitive laborers and the\nconscientious use of the capabilities to which they contribute. Toward this\nend, we first describe human-in-the-loop computing in context of the new\nconcerns it raises that are not addressed by traditional ethical research\nstandards. We then describe shortcomings in the traditional approach to ethical\nreview and introduce a dynamic approach for sustaining an ethical framework\nthat can continue to evolve within the rapidly shifting context of disruptive\nnew technologies.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 09:44:29 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Vep\u0159ek", "Libu\u0161e Hannah", ""], ["Seymour", "Patricia", ""], ["Michelucci", "Pietro", ""]]}, {"id": "2011.10917", "submitter": "Bei Wang", "authors": "Avishan Bagherinezhad, Michael Young, Bei Wang, Masood Parvania", "title": "Spatio-Temporal Visualization of Interdependent Battery Bus Transit and\n  Power Distribution Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high penetration of transportation electrification and its associated\ncharging requirements magnify the interdependency of the transportation and\npower distribution systems. The emergent interdependency requires that system\noperators fully understand the status of both systems. To this end, a\nvisualization tool is presented to illustrate the interdependency of battery\nbus transit and power distribution systems and the associated components. The\ntool aims at monitoring components from both systems, such as the locations of\nelectric buses, the state of charge of batteries, the price of electricity,\nvoltage, current, and active/reactive power flow. The results showcase the\nsuccess of the visualization tool in monitoring the bus transit and power\ndistribution components to determine a reliable cost-effective scheme for\nspatio-temporal charging of electric buses.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 02:55:45 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Bagherinezhad", "Avishan", ""], ["Young", "Michael", ""], ["Wang", "Bei", ""], ["Parvania", "Masood", ""]]}, {"id": "2011.11048", "submitter": "Zhihua Jin", "authors": "Zhihua Jin, Yong Wang, Qianwen Wang, Yao Ming, Tengfei Ma, Huamin Qu", "title": "GNNVis: A Visual Analytics Approach for Prediction Error Diagnosis of\n  Graph Neural Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) aim to extend deep learning techniques to graph\ndata and have achieved significant progress in graph analysis tasks (e.g., node\nclassification) in recent years. However, similar to other deep neural networks\nlike Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs),\nGNNs behave like a black box with their details hidden from model developers\nand users. It is therefore difficult to diagnose possible errors of GNNs.\nDespite many visual analytics studies being done on CNNs and RNNs, little\nresearch has addressed the challenges for GNNs. This paper fills the research\ngap with an interactive visual analysis tool, GNNVis, to assist model\ndevelopers and users in understanding and analyzing GNNs. Specifically,\nParallel Sets View and Projection View enable users to quickly identify and\nvalidate error patterns in the set of wrong predictions; Graph View and Feature\nMatrix View offer a detailed analysis of individual nodes to assist users in\nforming hypotheses about the error patterns. Since GNNs jointly model the graph\nstructure and the node features, we reveal the relative influences of the two\ntypes of information by comparing the predictions of three models: GNN,\nMulti-Layer Perceptron (MLP), and GNN Without Using Features (GNNWUF). Two case\nstudies and interviews with domain experts demonstrate the effectiveness of\nGNNVis in facilitating the understanding of GNN models and their errors.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 16:09:08 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 03:57:35 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 11:28:14 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Jin", "Zhihua", ""], ["Wang", "Yong", ""], ["Wang", "Qianwen", ""], ["Ming", "Yao", ""], ["Ma", "Tengfei", ""], ["Qu", "Huamin", ""]]}, {"id": "2011.11317", "submitter": "Jochen Meyer", "authors": "Jochen Meyer, Thomas Fr\\\"ohlich, Kai von Holdt", "title": "Corona-Warn-App: Erste Ergebnisse einer Onlineumfrage zur\n  (Nicht-)Nutzung und Gebrauch", "comments": "in German. In the original version, there was a minor bug in\n  calculating percentages for reasons of non-use (page 6), resulting in wrong\n  figures. These have been corrected. Now the figures are correct", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this study, the German \"Corona-Warn-App\" of the German Federal Government\nand the Robert-Koch-Institute is examined by means of a non-representative\nonline survey with 1482 participants for reasons of use and non-use. The study\nprovides insights into user behavior with the app during the Corona pandemic,\nhighlights the topic of data protection and how the app is used in general. Our\nresults show that the app is often not used due to privacy concerns, but that\nthere are also technical problems and doubts about its usefulness. In addition,\nthe app is mainly used due to altruistic reasons and is often opened to view\nthe own risk assessment and to ensure its functionality. To better understand\nthe results, we compare our results with a sample of infas 360 with 10553\nparticipants. It is shown that the results of this study can be compared to a\nlarger population. Finally, the results are discussed and recommendations for\naction are derived.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 10:38:28 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 16:15:04 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Meyer", "Jochen", ""], ["Fr\u00f6hlich", "Thomas", ""], ["von Holdt", "Kai", ""]]}, {"id": "2011.11375", "submitter": "Anders Sundnes L{\\o}vlie", "authors": "Mette Muxoll Schou and Anders Sundnes L{\\o}vlie", "title": "The Diary of Niels: Affective engagement through tangible interaction\n  with museum artifacts", "comments": "Conference: EuroMed 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a research through design exploration using tangible\ninteractions in order to seamlessly integrate technology in a historical house\nmuseum. The study addresses a longstanding concern in museum exhibition design\nthat interactive technologies may distract from the artifacts on display.\nThrough an iterative design process including user studies, a co-creation\nworkshop with museum staff and several prototypes, we developed an interactive\ninstallation called The Diary of Niels that combines physical objects, RFID\nsensors and an elaborate fiction in order to facilitate increased visitor\nengagement. Insights from the research process and user tests indicate that the\nintegration of technology and artifacts is meaningful and engaging for users,\nand helps introduce museum visitors to the historic theme of the exhibition and\nthe meaning of the artifacts. The study also points to continued challenges in\nintegrating such hybrid experiences fully with the rest of the exhibition.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 13:02:00 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Schou", "Mette Muxoll", ""], ["L\u00f8vlie", "Anders Sundnes", ""]]}, {"id": "2011.11383", "submitter": "Atis Elsts", "authors": "Maksims Ivanovs, Roberts Kadikis, Martins Lulla, Aleksejs Rutkovskis,\n  and Atis Elsts", "title": "Automated Quality Assessment of Hand Washing Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Washing hands is one of the most important ways to prevent infectious\ndiseases, including COVID-19. Unfortunately, medical staff does not always\nfollow the World Health Organization (WHO) hand washing guidelines in their\neveryday work. To this end, we present neural networks for automatically\nrecognizing the different washing movements defined by the WHO. We train the\nneural network on a part of a large (2000+ videos) real-world labeled dataset\nwith the different washing movements. The preliminary results show that using\npre-trained neural network models such as MobileNetV2 and Xception for the\ntask, it is possible to achieve >64 % accuracy in recognizing the different\nwashing movements. We also describe the collection and the structure of the\nabove open-access dataset created as part of this work. Finally, we describe\nhow the neural network can be used to construct a mobile phone application for\nautomatic quality control and real-time feedback for medical professionals.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 13:22:53 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 16:05:27 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ivanovs", "Maksims", ""], ["Kadikis", "Roberts", ""], ["Lulla", "Martins", ""], ["Rutkovskis", "Aleksejs", ""], ["Elsts", "Atis", ""]]}, {"id": "2011.11386", "submitter": "Anders Sundnes L{\\o}vlie", "authors": "Karin Ryding, Jocelyn Spence, Anders Sundnes L{\\o}vlie, Steve Benford", "title": "Interpersonalizing Intimate Museum Experiences", "comments": "Provisionally accepted for publication in the International Journal\n  of Human-Computer Interaction (IJHCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We reflect on two museum visiting experiences that adopted the strategy of\ninterpersonalization in which one visitor creates an experience for another. In\nthe Gift app, visitors create personal mini-tours for specific others. In Never\nlet me go, one visitor controls the experience of another by sending them\nremote instructions as they follow them around the museum. By reflecting on the\ndesign of these experiences and their deployment in museums we show how\ninterpersonalization can deliver engaging social visits in which visitors make\ntheir own interpretations. We contrast the approach to previous research in\ncustomization and algorithmic personalization. We reveal how these experiences\nrelied on intimacy between pairs of visitors but also between visitors and the\nmuseum. We propose that interpersonalization requires museums to step back to\nmake space for interpretation, but that this then raises the challenge of how\nto reintroduce the museum's own perspective. Finally, we articulate strategies\nand challenges for applying this approach.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 13:28:37 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ryding", "Karin", ""], ["Spence", "Jocelyn", ""], ["L\u00f8vlie", "Anders Sundnes", ""], ["Benford", "Steve", ""]]}, {"id": "2011.11398", "submitter": "Anders Sundnes L{\\o}vlie", "authors": "Anders Sundnes L{\\o}vlie, Lina Eklund, Annika Waern, Karin Ryding,\n  Paulina Rajkowska", "title": "Designing for Interpersonal Museum Experiences", "comments": "In press, to appear in Graham Black (2021) Museums and the Challenge\n  of Change: old institutions in a new world, Routledge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  What does the age of participation look like from the perspective of a museum\nvisitor? Arguably, the concept of participative experiences is already so\ndeeply ingrained in our culture that we may not even think about it as\nparticipation. Museum visitors engage in a number of activities, of which\nobserving the exhibits is only one part. Since most visitors come to the museum\ntogether with someone else, they spend time and attention on the people they\ncame with, and often the needs of the group are given priority over individual\npreferences. How can museums tap into these activities - and make themselves\nrelevant to visitors? In this chapter we will try to approach this\nconstructively, as a design opportunity. Could it be productive for the museum\nto consider itself not only as a disseminator of knowledge, but also as the\nfacilitator of participative activities between visitors?\n  In what follows, we will outline a range of practical design projects that\nserve as examples of this approach. These projects were part of the European\nUnion funded Horizon2020 project GIFT, a cross-disciplinary collaboration\nbetween researchers, artists, designers and many international museums and\nheritage organisations, exploring the concept of interpersonal museum\nexperiences (see https://gifting.digital/). What the projects have in common is\nthat they build on visitors co-creating and sharing their own narratives in the\nmuseum context. We suggest that these projects demonstrate a spectrum of\npossibilities: From experiences that take place almost without any museum\ninvolvement, to those that give museums a role in curating these narratives.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 13:44:48 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 09:02:31 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["L\u00f8vlie", "Anders Sundnes", ""], ["Eklund", "Lina", ""], ["Waern", "Annika", ""], ["Ryding", "Karin", ""], ["Rajkowska", "Paulina", ""]]}, {"id": "2011.11704", "submitter": "Xiumin Shang", "authors": "Xiumin Shang, Ahmed Sabbir Arif, Marcelo Kallmann", "title": "Evaluating Feedback Strategies for Virtual Human Trainers", "comments": "A video demonstrating our work is available at:\n  https://drive.google.com/file/d/103iz2sUeLRX4KY457wucLZnhXhfiQZNv/view?usp=sharing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address feedback strategies for an autonomous virtual\ntrainer. First, a pilot study was conducted to identify and specify feedback\nstrategies for assisting participants in performing a given task. The task\ninvolved sorting virtual cubes according to areas of countries displayed on\nthem. Two feedback strategies were specified. The first provides correctness\nfeedback by fully correcting user responses at each stage of the task, and the\nsecond provides suggestive feedback by only notifying if and how a response can\nbe corrected. Both strategies were implemented in a virtual training system and\nempirically evaluated. The correctness feedback strategy was preferred by the\nparticipants, was more effective time-wise, and was more effective in improving\ntask performance skills. The overall system was also rated comparable to\nhypothetically performing the same task with real interactions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:13:34 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Shang", "Xiumin", ""], ["Arif", "Ahmed Sabbir", ""], ["Kallmann", "Marcelo", ""]]}, {"id": "2011.11749", "submitter": "Thomas Gross", "authors": "Thomas Gro{\\ss}", "title": "Validity and Reliability of the Scale Internet Users' Information\n  Privacy Concern (IUIPC) [Extended Version]", "comments": "Open Science Framework: https://osf.io/5pywm, 59 pages. This work was\n  supported by the ERC Grant CASCAde (GA no 716980)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet Users' Information Privacy Concerns (IUIPC-10) is one of the most\nendorsed privacy concern scales. It is widely used in the evaluation of human\nfactors of PETs and the investigation of the privacy paradox. Even though its\npredecessor Concern For Information Privacy (CFIP) has been evaluated\nindependently and the instrument itself seen some scrutiny, we are still\nmissing a dedicated confirmation of IUIPC-10, itself. We aim at closing this\ngap by systematically analyzing IUIPC's construct validity and reliability. We\nobtained three mutually independent samples with a total of $N = 1031$\nparticipants. We conducted a confirmatory factor analysis (CFA) on our main\nsample. Having found weaknesses, we established further factor analyses to\nassert the dimensionality of IUIPC-10. We proposed a respecified instrument\nIUIPC-8 with improved psychometric properties. Finally, we validated our\nfindings on a validation sample. While we could confirm the overall\nthree-dimensionality of IUIPC-10, we found that IUIPC-10 consistently failed\nconstruct validity and reliability evaluations, calling into question the\nunidimensionality of its sub-scales Awareness and Control. Our respecified\nscale IUIPC-8 offers a statistically significantly better model and outperforms\nIUIPC-10's construct validity and reliability. The disconfirming evidence on\nthe construct validity raises doubts how well IUIPC-10 measures the latent\nvariable information privacy concern. The sub-par reliability could yield\nspurious and erratic results as well as attenuate relations with other latent\nvariables, such as behavior. Thereby, the instrument could confound studies of\nhuman factors of PETs or the privacy paradox, in general.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:49:47 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Gro\u00df", "Thomas", ""]]}, {"id": "2011.12125", "submitter": "Sara Johansson Fernstad", "authors": "Sara Johansson Fernstad and Jimmy Johansson", "title": "To Explore What Isn't There -- Glyph-based Visualization for Analysis of\n  Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes a novel visualization method, Missingness Glyph, for\nanalysis and exploration of missing values in data. Missing values are a common\nchallenge in most data generating domains and may cause a range of analysis\nissues. Missingness in data may indicate potential problems in data collection\nand pre-processing, or highlight important data characteristics. While the\ndevelopment and improvement of statistical methods for dealing with missing\ndata is a research area in its own right, mainly focussing on replacing missing\nvalues with estimated values, considerably less focus has been put on\nvisualization of missing values. Nonetheless, visualization and explorative\nanalysis has great potential to support understanding of missingness in data,\nand to enable gaining of novel insights into patterns of missingness in a way\nthat statistical methods are unable to. The Missingness Glyph supports\nidentification of relevant missingness patterns in data, and is evaluated and\ncompared to two other visualization methods in context of the missingness\npatterns. The results are promising and confirms that the Missingness Glyph in\nseveral cases perform better than the alternative visualization methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 14:32:26 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Fernstad", "Sara Johansson", ""], ["Johansson", "Jimmy", ""]]}, {"id": "2011.12758", "submitter": "Emily Saltz", "authors": "Emily Saltz, Claire Leibowicz, Claire Wardle", "title": "Encounters with Visual Misinformation and Labels Across Platforms: An\n  Interview and Diary Study to Inform Ecosystem Approaches to Misinformation\n  Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since 2016, the amount of academic research with the keyword \"misinformation\"\nhas more than doubled [2]. This research often focuses on article headlines\nshown in artificial testing environments, yet misinformation largely spreads\nthrough images and video posts shared in highly-personalized platform contexts.\nA foundation of qualitative research is necessary to begin filling this gap to\nensure platforms' visual misinformation interventions are aligned with users'\nneeds and understanding of information in their personal contexts, across\nplatforms. In two studies, we combined in-depth interviews (n=15) with diary\nand co-design methods (n=23) to investigate how a broad mix of Americans\nexposed to misinformation during COVID-19 understand their visual information\nenvironments, including encounters with interventions such as Facebook\nfact-checking labels. Analysis reveals a deep division in user attitudes about\nplatform labeling interventions for visual information which are perceived by\nmany as overly paternalistic, biased, and punitive. Alongside these findings,\nwe discuss our methods as a model for continued independent qualitative\nresearch on cross-platform user experiences of misinformation that inform\ninterventions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:22:36 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 01:13:08 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Saltz", "Emily", ""], ["Leibowicz", "Claire", ""], ["Wardle", "Claire", ""]]}, {"id": "2011.13079", "submitter": "Fnu Shilpika", "authors": "Fnu Shilpika, Takanori Fujiwara, Naohisa Sakamoto, Jorji Nonaka,\n  Kwan-Liu Ma", "title": "A Visual Analytics Approach to Monitor Time-Series Data with Incremental\n  and Progressive Functional Data Analysis", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications involve analyzing time-dependent phenomena,\nwhich are intrinsically functional---consisting of curves varying over a\ncontinuum, which is time in this case. When analyzing continuous data,\nfunctional data analysis (FDA) provides substantial benefits, such as the\nability to study the derivatives and to restrict the ordering of data. However,\ncontinuous data inherently has infinite dimensions, and FDA methods often\nsuffer from high computational costs. This is even more critical when we have\nnew incoming data and want to update the FDA results in real-time. In this\npaper, we present a visual analytics approach to consecutively monitor and\nreview the changing time-series data with a focus on identifying outliers by\nusing FDA. To perform such an analysis while addressing the computational\nproblem, we introduce new incremental and progressive algorithms that promptly\ngenerate the magnitude-shape (MS) plot, which reveals both the functional\nmagnitude and shape outlyingness of time-series data. In addition, by using an\nMS plot in conjunction with an FDA version of principal component analysis, we\nenhance the analyst's ability to investigate the visually-identified outliers.\nWe illustrate the effectiveness of our approach with three case studies using\nreal-world and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 00:48:06 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Shilpika", "Fnu", ""], ["Fujiwara", "Takanori", ""], ["Sakamoto", "Naohisa", ""], ["Nonaka", "Jorji", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2011.13194", "submitter": "Morteza Hosseini", "authors": "Morteza Hosseini, Haoran Ren, Hasib-Al Rashid, Arnab Neelim Mazumder,\n  Bharat Prakash, and Tinoosh Mohsenin", "title": "Neural Networks for Pulmonary Disease Diagnosis using Auditory and\n  Demographic Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pulmonary diseases impact millions of lives globally and annually. The recent\noutbreak of the pandemic of the COVID-19, a novel pulmonary infection, has more\nthan ever brought the attention of the research community to the machine-aided\ndiagnosis of respiratory problems. This paper is thus an effort to exploit\nmachine learning for classification of respiratory problems and proposes a\nframework that employs as much correlated information (auditory and demographic\ninformation in this work) as a dataset provides to increase the sensitivity and\nspecificity of a diagnosing system. First, we use deep convolutional neural\nnetworks (DCNNs) to process and classify a publicly released pulmonary auditory\ndataset, and then we take advantage of the existing demographic information\nwithin the dataset and show that the accuracy of the pulmonary classification\nincreases by 5% when trained on the auditory information in conjunction with\nthe demographic information. Since the demographic data can be extracted using\ncomputer vision, we suggest using another parallel DCNN to estimate the\ndemographic information of the subject under test visioned by the processing\ncomputer. Lastly, as a proposition to bring the healthcare system to users'\nfingertips, we measure deployment characteristics of the auditory DCNN model\nonto processing components of an NVIDIA TX2 development board.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 09:14:40 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hosseini", "Morteza", ""], ["Ren", "Haoran", ""], ["Rashid", "Hasib-Al", ""], ["Mazumder", "Arnab Neelim", ""], ["Prakash", "Bharat", ""], ["Mohsenin", "Tinoosh", ""]]}, {"id": "2011.13347", "submitter": "Catarina Lopes-Dias", "authors": "Catarina Lopes-Dias, Andreea I. Sburlea, Katharina Breitegger, Daniela\n  Wyss, Harald Drescher, Renate Wildburger and Gernot R. M\u007f\\\"uller-Putz", "title": "Online asynchronous detection of error-related potentials in\n  participants with a spinal cord injury using a generic classifier", "comments": null, "journal-ref": "J. Neural Eng. 18 046022 (2021)", "doi": "10.1088/1741-2552/abd1eb", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A BCI user awareness of an error is associated with a cortical signature\nnamed error-related potential (ErrP). The incorporation of ErrPs' detection in\nBCIs can improve BCIs' performance. This work is three-folded. First, we\ninvestigate if an ErrP classifier is transferable from able-bodied participants\nto participants with spinal cord injury (SCI). Second, we test this generic\nErrP classifier with SCI and control participants, in an online experiment\nwithout offline calibration. Third, we investigate the morphology of ErrPs in\nboth groups of participants. We used previously recorded\nelectroencephalographic (EEG) data from able-bodied participants to train an\nErrP classifier. We tested the classifier asynchronously, in an online\nexperiment with 16 new participants: 8 participants with SCI and 8 able-bodied\ncontrol participants. The experiment had no offline calibration and\nparticipants received feedback regarding the ErrPs' detection from its start.\nThe generic classifier was not trained with the user's brain signals. Still,\nits performance was optimized during the online experiment with the use of\npersonalized decision thresholds. Participants with SCI presented a\nnon-homogenous ErrP morphology, and four of them did not present clear ErrP\nsignals. The generic classifier performed above chance level in participants\nwith clear ErrP signals, independently of the SCI (11 out of 16 participants).\nThree out of the five participants that obtained chance level results with the\ngeneric classifier would have not benefited from the use of a personalized\nclassifier. This work shows the feasibility of transferring an ErrP classifier\nfrom able-bodied participants to participants with SCI, for asynchronous\ndetection of ErrPs in an online experiment without offline calibration, which\nprovided immediate feedback to the users.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 15:24:41 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 08:08:22 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 16:15:12 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Lopes-Dias", "Catarina", ""], ["Sburlea", "Andreea I.", ""], ["Breitegger", "Katharina", ""], ["Wyss", "Daniela", ""], ["Drescher", "Harald", ""], ["Wildburger", "Renate", ""], ["M\u007f\u00fcller-Putz", "Gernot R.", ""]]}, {"id": "2011.13374", "submitter": "Huy Kang Kim", "authors": "Eunji Park, Kyung Ho Park, Huy Kang Kim", "title": "Understand Watchdogs: Discover How Game Bot Get Discovered", "comments": "9 pages, 3 figures, 3 tables, this paper is accepted in ICAART 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The game industry has long been troubled by malicious activities utilizing\ngame bots. The game bots disturb other game players and destroy the\nenvironmental system of the games. For these reasons, the game industry put\ntheir best efforts to detect the game bots among players' characters using the\nlearning-based detections. However, one problem with the detection\nmethodologies is that they do not provide rational explanations about their\ndecisions. To resolve this problem, in this work, we investigate the\nexplainabilities of the game bot detection. We develop the XAI model using a\ndataset from the Korean MMORPG, AION, which includes game logs of human players\nand game bots. More than one classification model has been applied to the\ndataset to be analyzed by applying interpretable models. This provides us\nexplanations about the game bots' behavior, and the truthfulness of the\nexplanations has been evaluated. Besides, interpretability contributes to\nminimizing false detection, which imposes unfair restrictions on human players.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 16:34:31 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 12:29:53 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Park", "Eunji", ""], ["Park", "Kyung Ho", ""], ["Kim", "Huy Kang", ""]]}, {"id": "2011.13487", "submitter": "Federico Visi", "authors": "Federico Ghelli Visi and Atau Tanaka", "title": "Interactive Machine Learning of Musical Gesture", "comments": "Author's accepted manuscript, to appear as a chapter in \"Handbook of\n  Artificial Intelligence for Music: Foundations, Advanced Approaches, and\n  Developments for Creativity\", edited by E. R. Miranda. Cham: Springer Nature,\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This chapter presents an overview of Interactive Machine Learning (IML)\ntechniques applied to the analysis and design of musical gestures. We go\nthrough the main challenges and needs related to capturing, analysing, and\napplying IML techniques to human bodily gestures with the purpose of performing\nwith sound synthesis systems. We discuss how different algorithms may be used\nto accomplish different tasks, including interacting with complex synthesis\ntechniques and exploring interaction possibilities by means of Reinforcement\nLearning (RL) in an interaction paradigm we developed called Assisted\nInteractive Machine Learning (AIML). We conclude the chapter with a description\nof how some of these techniques were employed by the authors for the\ndevelopment of four musical pieces, thus outlining the implications that IML\nhave for musical practice.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 22:44:54 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Visi", "Federico Ghelli", ""], ["Tanaka", "Atau", ""]]}, {"id": "2011.13638", "submitter": "Nadeem Kafi", "authors": "Nadeem Kafi, Zubair Ahmed Shaikh, and Muhammad Shahid Shaikh", "title": "Human Computations in Citizen Crowds: A Knowledge Management Solution\n  Framework", "comments": null, "journal-ref": "Mehran University Research Journal of Engineering & Technology,\n  Vol. 37, No. 3, 513-528 July 2018, p-ISSN: 0254-7821, e-ISSN: 2413-7219", "doi": "10.22581/muet1982.1803.06", "report-no": null, "categories": "cs.HC cs.CY cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  KG (Knowledge Generation) and understanding have traditionally been a\nHuman-centric activity. KE (Knowledge Engineering) and KM (Knowledge\nManagement) have tried to augment human knowledge on two separate planes: the\nfirst deals with machine interpretation of knowledge while the later explore\ninteractions in human networks for KG and understanding. However, both remain\ncomputer-centric. Crowdsourced HC (Human Computations) have recently utilized\nhuman cognition and memory to generate diverse knowledge streams on specific\ntasks, which are mostly easy for humans to solve but remain challenging for\nmachine algorithms. Literature shows little work on KM frameworks for citizen\ncrowds, which gather input from the diverse category of Humans, organize that\nknowledge concerning tasks and knowledge categories and recreate new knowledge\nas a computer-centric activity. In this paper, we present an attempt to create\na framework by implementing a simple solution, called ExamCheck, to focus on\nthe generation of knowledge, feedback on that knowledge and recording the\nresults of that knowledge in academic settings. Our solution, based on HC,\nshows that a structured KM framework can address a complex problem in a context\nthat is important for participants themselves.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 10:18:01 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Kafi", "Nadeem", ""], ["Shaikh", "Zubair Ahmed", ""], ["Shaikh", "Muhammad Shahid", ""]]}, {"id": "2011.13741", "submitter": "Aditya Jyoti Paul", "authors": "Aditya Jyoti Paul, Puranjay Mohan, Stuti Sehgal", "title": "Rethinking Generalization in American Sign Language Prediction for Edge\n  Devices with Extremely Low Memory Footprint", "comments": "6 pages, Published in IEEE RAICS 2020, see https://raics.in", "journal-ref": "2020 IEEE Recent Advances in Intelligent Computational Systems\n  (RAICS), 2020, pp. 147-152", "doi": "10.1109/RAICS51191.2020.9332480", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the boom in technical compute in the last few years, the world has\nseen massive advances in artificially intelligent systems solving diverse\nreal-world problems. But a major roadblock in the ubiquitous acceptance of\nthese models is their enormous computational complexity and memory footprint.\nHence efficient architectures and training techniques are required for\ndeployment on extremely low resource inference endpoints. This paper proposes\nan architecture for detection of alphabets in American Sign Language on an ARM\nCortex-M7 microcontroller having just 496 KB of framebuffer RAM. Leveraging\nparameter quantization is a common technique that might cause varying drops in\ntest accuracy. This paper proposes using interpolation as augmentation amongst\nother techniques as an efficient method of reducing this drop, which also helps\nthe model generalize well to previously unseen noisy data. The proposed model\nis about 185 KB post-quantization and inference speed is 20 frames per second.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 14:05:42 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 10:24:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Paul", "Aditya Jyoti", ""], ["Mohan", "Puranjay", ""], ["Sehgal", "Stuti", ""]]}, {"id": "2011.13779", "submitter": "Tasos Spiliotopoulos", "authors": "Tasos Spiliotopoulos, Ian Oakley", "title": "An Integrated Approach Towards the Construction of an HCI Methodological\n  Framework", "comments": "In Proceedings of the 1st European Workshop on HCI Design and\n  Evaluation, Limassol, Cyprus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a methodological framework aiming at the support of HCI\npractitioners and researchers in selecting and applying the most appropriate\ncombination of HCI methods for particular problems. We highlight the need for a\nclear and effective overview of methods and provide further discussion on\npossible extensions that can support recent trends and needs, such as the focus\non specific application domains.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 15:27:36 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Spiliotopoulos", "Tasos", ""], ["Oakley", "Ian", ""]]}, {"id": "2011.13785", "submitter": "Tasos Spiliotopoulos", "authors": "Tasos Spiliotopoulos, Ian Oakley", "title": "Urban Twitter Networks and Communities: A Case Study of Microblogging in\n  Athens", "comments": "Published in the proceedings of Hybrid City Conference 2013, Athens,\n  Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper examines the community formed by the Twitter users that used a\ncity-level hashtag. In particular, we provide a network perspective of the city\nof Athens, Greece, as demonstrated by the analysis and visualization of the\nrelevant Twitter hashtag data, in order to present both an overview and deeper\ninsights at the microblogging practices of this geographic local network.\nFurther analysis suggests that the Twitter community defined by the members of\nthe network shows strong signs of a real-life community.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 15:38:42 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Spiliotopoulos", "Tasos", ""], ["Oakley", "Ian", ""]]}, {"id": "2011.13801", "submitter": "Vicente Amorim", "authors": "Vicente J. P. Amorim, Ricardo A. O. Oliveira, Mauricio Jose da Silva", "title": "Recent Trends in Wearable Computing Research: A Systematic Review", "comments": "39 pages, 21 figures, 21 tablet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wearable devices are a trending topic in both commercial and academic areas.\nIncreasing demand for innovation has led to increased research and new\nproducts, addressing new challenges and creating profitable opportunities.\nHowever, despite a number of reviews and surveys on wearable computing, a study\noutlining how this area has recently evolved, which provides a broad and\nobjective view of the main topics addressed by scientists, is lacking. The\nsystematic review of literature presented in this paper investigates recent\ntrends in wearable computing studies, taking into account a set of constraints\napplied to relevant studies over a window of ten years. The extracted articles\nwere considered as a means to extract valuable information, creating a useful\ndata set to represent the current status. Results of this study faithfully\nportray evolving interests in wearable devices. The analysis conducted here\ninvolving studies made over the past ten years allows evaluation of the areas,\nresearch focus, and technologies that are currently at the forefront of\nwearable device development. Conclusions presented in this review aim to assist\nscientists to better perceive recent demand trends and how wearable technology\ncan further evolve. Finally, this study should assist in outlining the next\nsteps in current and future development.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 15:54:44 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Amorim", "Vicente J. P.", ""], ["Oliveira", "Ricardo A. O.", ""], ["da Silva", "Mauricio Jose", ""]]}, {"id": "2011.13802", "submitter": "Tasos Spiliotopoulos", "authors": "Tasos Spiliotopoulos, Ian Oakley", "title": "Post or Tweet: Lessons from a Study of Facebook and Twitter Usage", "comments": "Published in the CHI 2016 Workshop on Following user pathways: Using\n  cross platform and mixed methods analysis in social media studies, San Jose,\n  CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This workshop paper reports on an ongoing mixed-methods study on the two\narguably most popular social network sites, Facebook and Twitter, for the same\nusers. The overarching goal of the study is to shed light into the nuances of\nsocial media selection and cross-platform use by combining survey data about\nparticipants' motivations with usage data collected via API extraction. We\ndescribe the set-up of the study and focus our discussion on the challenges and\ninsights relating to participant recruiting and data collection, handling and\ndimensionalizing usage data, and comparing usage data across sites.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 15:55:02 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Spiliotopoulos", "Tasos", ""], ["Oakley", "Ian", ""]]}, {"id": "2011.13979", "submitter": "Enis Ulqinaku", "authors": "Ivo Sluganovic and Enis Ulqinaku and Aritra Dhar and Daniele Lain and\n  Srdjan Capkun and Ivan Martinovic", "title": "IntegriScreen: Visually Supervising Remote User Interactions on\n  Compromised Clients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote services and applications that users access via their local clients\n(laptops or desktops) usually assume that, following a successful user\nauthentication at the beginning of the session, all subsequent communication\nreflects the user's intent. However, this is not true if the adversary gains\ncontrol of the client and can therefore manipulate what the user sees and what\nis sent to the remote server.\n  To protect the user's communication with the remote server despite a\npotentially compromised local client, we propose the concept of continuous\nvisual supervision by a second device equipped with a camera. Motivated by the\nrapid increase of the number of incoming devices with front-facing cameras,\nsuch as augmented reality headsets and smart home assistants, we build upon the\ncore idea that the user's actual intended input is what is shown on the\nclient's screen, despite what ends up being sent to the remote server. A\nstatically positioned camera enabled device can, therefore, continuously\nanalyze the client's screen to enforce that the client behaves honestly despite\npotentially being malicious.\n  We evaluate the present-day feasibility and deployability of this concept by\ndeveloping a fully functional prototype, running a host of experimental tests\non three different mobile devices, and by conducting a user study in which we\nanalyze participants' use of the system during various simulated attacks.\nExperimental evaluation indeed confirms the feasibility of the concept of\nvisual supervision, given that the system consistently detects over 98% of\nevaluated attacks, while study participants with little instruction detect the\nremaining attacks with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 20:05:29 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Sluganovic", "Ivo", ""], ["Ulqinaku", "Enis", ""], ["Dhar", "Aritra", ""], ["Lain", "Daniele", ""], ["Capkun", "Srdjan", ""], ["Martinovic", "Ivan", ""]]}, {"id": "2011.14370", "submitter": "Sidhartha Narayan S", "authors": "Sarah, S.Sidhartha Narayan, Irfaan Arif, Hrithwik Shalu, Juned\n  Kadiwala", "title": "A smartphone based multi input workflow for non-invasive estimation of\n  haemoglobin levels using machine learning techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a low cost, non invasive healthcare system that measures\nhaemoglobin levels in patients and can be used as a preliminary diagnostic test\nfor anaemia. A combination of image processing, machine learning and deep\nlearning techniques are employed to develop predictive models to measure\nhaemoglobin levels. This is achieved through the color analysis of the\nfingernail beds, palpebral conjunctiva and tongue of the patients. This\npredictive model is then encapsulated in a healthcare application. This\napplication expedites data collection and facilitates active learning of the\nmodel. It also incorporates personalized calibration of the model for each\npatient, assisting in the continual monitoring of the haemoglobin levels of the\npatient. Upon validating this framework using data, it can serve as a highly\naccurate preliminary diagnostic test for anaemia.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 13:57:09 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Sarah", "", ""], ["Narayan", "S. Sidhartha", ""], ["Arif", "Irfaan", ""], ["Shalu", "Hrithwik", ""], ["Kadiwala", "Juned", ""]]}, {"id": "2011.14535", "submitter": "Sarah Radway", "authors": "Sarah Radway, Anthony Luo, Carmine Elvezio, Jenny Cha, Sophia Kolak,\n  Elijah Zulu, Sad Adib", "title": "Beyond LunAR: An augmented reality UI for deep-space exploration\n  missions", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As space exploration efforts shift to deep space missions, new challenges\nemerge regarding astronaut communication and task completion. While the round\ntrip propagation delay for lunar communications is 2.6 seconds, the time delay\nincreases to nearly 22 minutes for Mars missions. This creates a need for\nastronaut independence from earth-based assistance, and places greater\nsignificance upon the limited communications that are able to be delivered. To\naddress this issue, we prototyped an augmented reality user interface for the\nnew xEMU spacesuit, intended for use on planetary surface missions. This user\ninterface assists with functions that would usually be completed by flight\ncontrollers in Mission Control, or are currently completed in manners that are\nunnecessarily difficult. We accomplish this through features such as AR\nmodel-based task instruction, sampling task assistance, note taking, and\ntelemetry monitoring and display.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 04:16:19 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Radway", "Sarah", ""], ["Luo", "Anthony", ""], ["Elvezio", "Carmine", ""], ["Cha", "Jenny", ""], ["Kolak", "Sophia", ""], ["Zulu", "Elijah", ""], ["Adib", "Sad", ""]]}, {"id": "2011.14754", "submitter": "Mostafa Haghi Kashani", "authors": "Sepideh Bazzaz Abkenar, Mostafa Haghi Kashani, Mohammad Akbari,\n  Ebrahim Mahdipour", "title": "Twitter Spam Detection: A Systematic Review", "comments": "18 pages, 12 figures, 14 tables, 91 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, with the rise of Internet access and mobile devices around the\nglobe, more people are using social networks for collaboration and receiving\nreal-time information. Twitter, the microblogging that is becoming a critical\nsource of communication and news propagation, has grabbed the attention of\nspammers to distract users. So far, researchers have introduced various defense\ntechniques to detect spams and combat spammer activities on Twitter. To\novercome this problem, in recent years, many novel techniques have been offered\nby researchers, which have greatly enhanced the spam detection performance.\nTherefore, it raises a motivation to conduct a systematic review about\ndifferent approaches of spam detection on Twitter. This review focuses on\ncomparing the existing research techniques on Twitter spam detection\nsystematically. Literature review analysis reveals that most of the existing\nmethods rely on Machine Learning-based algorithms. Among these Machine Learning\nalgorithms, the major differences are related to various feature selection\nmethods. Hence, we propose a taxonomy based on different feature selection\nmethods and analyses, namely content analysis, user analysis, tweet analysis,\nnetwork analysis, and hybrid analysis. Then, we present numerical analyses and\ncomparative studies on current approaches, coming up with open challenges that\nhelp researchers develop solutions in this topic.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:10:24 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 11:31:06 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Abkenar", "Sepideh Bazzaz", ""], ["Kashani", "Mostafa Haghi", ""], ["Akbari", "Mohammad", ""], ["Mahdipour", "Ebrahim", ""]]}, {"id": "2011.14825", "submitter": "Yixian Wu", "authors": "Yixian Wu", "title": "App Limits Bar: A Progress of App Limits for Overcoming Smartphone\n  Overuse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smartphone overuse has many negative effects on human beings. The function\nApp Limits in our phones, which belongs to behavior reinforcement strategies\nfor overcoming problematic smartphone overuse, constrains users by shutting\nthem completely out of an app after a certain period of time. While it has the\neffectiveness to some extent, lacking procedural detection of problematic\nbehavior always brings about anxiety and depression for users. We proposed App\nLimits Bar (ALB) to progress the traditional App Limits, aiming to mitigate the\nnegative feelings about using App Limits and enhance users' abilities to manage\ntheir time on addictive apps. Meanwhile, we took a three-phase user study to\nanswer whether ALB helps users be more content with using the app limits\nfunction and whether ALB performs better than traditional App Limits in terms\nof reducing usage time and open times on addictive apps. Future work will study\nthe best shape of the edge screen for visualization when the front screen faces\nus.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:18:58 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wu", "Yixian", ""]]}, {"id": "2011.15050", "submitter": "Ivan Stelmakh", "authors": "Ivan Stelmakh, Nihar B. Shah, Aarti Singh, and Hal Daum\\'e III", "title": "A Novice-Reviewer Experiment to Address Scarcity of Qualified Reviewers\n  in Large Conferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conference peer review constitutes a human-computation process whose\nimportance cannot be overstated: not only it identifies the best submissions\nfor acceptance, but, ultimately, it impacts the future of the whole research\narea by promoting some ideas and restraining others. A surge in the number of\nsubmissions received by leading AI conferences has challenged the\nsustainability of the review process by increasing the burden on the pool of\nqualified reviewers which is growing at a much slower rate. In this work, we\nconsider the problem of reviewer recruiting with a focus on the scarcity of\nqualified reviewers in large conferences. Specifically, we design a procedure\nfor (i) recruiting reviewers from the population not typically covered by major\nconferences and (ii) guiding them through the reviewing pipeline. In\nconjunction with ICML 2020 -- a large, top-tier machine learning conference --\nwe recruit a small set of reviewers through our procedure and compare their\nperformance with the general population of ICML reviewers. Our experiment\nreveals that a combination of the recruiting and guiding mechanisms allows for\na principled enhancement of the reviewer pool and results in reviews of\nsuperior quality compared to the conventional pool of reviews as evaluated by\nsenior members of the program committee (meta-reviewers).\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:48:55 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Stelmakh", "Ivan", ""], ["Shah", "Nihar B.", ""], ["Singh", "Aarti", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "2011.15083", "submitter": "Ivan Stelmakh", "authors": "Ivan Stelmakh, Charvi Rastogi, Nihar B. Shah, Aarti Singh, and Hal\n  Daum\\'e III", "title": "A Large Scale Randomized Controlled Trial on Herding in Peer-Review\n  Discussions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer review is the backbone of academia and humans constitute a cornerstone\nof this process, being responsible for reviewing papers and making the final\nacceptance/rejection decisions. Given that human decision making is known to be\nsusceptible to various cognitive biases, it is important to understand which\n(if any) biases are present in the peer-review process and design the pipeline\nsuch that the impact of these biases is minimized. In this work, we focus on\nthe dynamics of between-reviewers discussions and investigate the presence of\nherding behaviour therein. In that, we aim to understand whether reviewers and\nmore senior decision makers get disproportionately influenced by the first\nargument presented in the discussion when (in case of reviewers) they form an\nindependent opinion about the paper before discussing it with others.\nSpecifically, in conjunction with the review process of ICML 2020 -- a large,\ntop tier machine learning conference -- we design and execute a randomized\ncontrolled trial with the goal of testing for the conditional causal effect of\nthe discussion initiator's opinion on the outcome of a paper.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:23:07 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Stelmakh", "Ivan", ""], ["Rastogi", "Charvi", ""], ["Shah", "Nihar B.", ""], ["Singh", "Aarti", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "2011.15096", "submitter": "Etienne Richan", "authors": "Etienne Richan, Jean Rouat", "title": "A proposal and evaluation of new timbre visualisation methods for audio\n  sample browsers", "comments": "14 pages. Personal and Ubiquitous Computing (2020)", "journal-ref": null, "doi": "10.1007/s00779-020-01388-1", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Searching through vast libraries of sound samples can be a daunting and\ntime-consuming task. Modern audio sample browsers use mappings between acoustic\nproperties and visual attributes to visually differentiate displayed items.\nThere are few studies focused on how well these mappings help users search for\na specific sample. We propose new methods for generating textural labels and\npositioning samples based on perceptual representations of timbre. We perform a\nseries of studies to evaluate the benefits of using shape, color or texture as\nlabels in a known-item search task. We describe the motivation and\nimplementation of the study, and present an in-depth analysis of results. We\nfind that shape significantly improves task performance, while color and\ntexture have little effect. We also compare results between in-person and\nonline participants and propose research directions for further studies.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:30:26 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Richan", "Etienne", ""], ["Rouat", "Jean", ""]]}]