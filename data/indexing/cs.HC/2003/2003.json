[{"id": "2003.00369", "submitter": "Daniel Freer", "authors": "Daniel Freer, Guang-Zhong Yang", "title": "MIndGrasp: A New Training and Testing Framework for Motor Imagery Based\n  3-Dimensional Assistive Robotic Control", "comments": "8 pages, 5 figures, submitted to IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing global age and disability assistive robots are becoming more\nnecessary, and brain computer interfaces (BCI) are often proposed as a solution\nto understanding the intent of a disabled person that needs assistance. Most\nframeworks for electroencephalography (EEG)-based motor imagery (MI) BCI\ncontrol rely on the direct control of the robot in Cartesian space. However,\nfor 3-dimensional movement, this requires 6 motor imagery classes, which is a\ndifficult distinction even for more experienced BCI users. In this paper, we\npresent a simulated training and testing framework which reduces the number of\nmotor imagery classes to 4 while still grasping objects in three-dimensional\nspace. This is achieved through semi-autonomous eye-in-hand vision-based\ncontrol of the robotic arm, while the user-controlled BCI achieves movement to\nthe left and right, as well as movement toward and away from the object of\ninterest. Additionally, the framework includes a method of training a BCI\ndirectly on the assistive robotic system, which should be more easily\ntransferrable to a real-world assistive robot than using a standard training\nprotocol such as Graz-BCI. Presented results do not consider real human EEG\ndata, but are rather shown as a baseline for comparison with future human data\nand other improvements on the system.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 00:41:11 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Freer", "Daniel", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "2003.00380", "submitter": "Jieshan Chen", "authors": "Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xiwei Xu, Liming Zhu,\n  Guoqiang Li, and Jinshui Wang", "title": "Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI\n  Components by Deep Learning", "comments": "Accepted to 42nd International Conference on Software Engineering", "journal-ref": null, "doi": "10.1145/3377811.3380327", "report-no": null, "categories": "cs.HC cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the World Health Organization(WHO), it is estimated that\napproximately 1.3 billion people live with some forms of vision impairment\nglobally, of whom 36 million are blind. Due to their disability, engaging these\nminority into the society is a challenging problem. The recent rise of smart\nmobile phones provides a new solution by enabling blind users' convenient\naccess to the information and service for understanding the world. Users with\nvision impairment can adopt the screen reader embedded in the mobile operating\nsystems to read the content of each screen within the app, and use gestures to\ninteract with the phone. However, the prerequisite of using screen readers is\nthat developers have to add natural-language labels to the image-based\ncomponents when they are developing the app. Unfortunately, more than 77% apps\nhave issues of missing labels, according to our analysis of 10,408 Android\napps. Most of these issues are caused by developers' lack of awareness and\nknowledge in considering the minority. And even if developers want to add the\nlabels to UI components, they may not come up with concise and clear\ndescription as most of them are of no visual issues. To overcome these\nchallenges, we develop a deep-learning based model, called LabelDroid, to\nautomatically predict the labels of image-based buttons by learning from\nlarge-scale commercial apps in Google Play. The experimental results show that\nour model can make accurate predictions and the generated labels are of higher\nquality than that from real Android developers.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 02:31:26 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 11:38:28 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Chen", "Jieshan", ""], ["Chen", "Chunyang", ""], ["Xing", "Zhenchang", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""], ["Li", "Guoqiang", ""], ["Wang", "Jinshui", ""]]}, {"id": "2003.00400", "submitter": "Lingfeng Tao", "authors": "Lingfeng Tao, Michael Bowman, Jiucai Zhang, Xiaoli Zhang", "title": "Learn Task First or Learn Human Partner First: A Hierarchical Task\n  Decomposition Method for Human-Robot Cooperation", "comments": "Submitted to SMC2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying Deep Reinforcement Learning (DRL) to Human-Robot Cooperation (HRC)\nin dynamic control problems is promising yet challenging as the robot needs to\nlearn the dynamics of the controlled system and dynamics of the human partner.\nIn existing research, the robot powered by DRL adopts coupled observation of\nthe environment and the human partner to learn both dynamics simultaneously.\nHowever, such a learning strategy is limited in terms of learning efficiency\nand team performance. This work proposes a novel task decomposition method with\na hierarchical reward mechanism that enables the robot to learn the\nhierarchical dynamic control task separately from learning the human partner's\nbehavior. The method is validated with a hierarchical control task in a\nsimulated environment with human subject experiments. Our method also provides\ninsight into the design of the learning strategy for HRC. The results show that\nthe robot should learn the task first to achieve higher team performance and\nlearn the human first to achieve higher learning efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 04:41:49 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 15:37:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tao", "Lingfeng", ""], ["Bowman", "Michael", ""], ["Zhang", "Jiucai", ""], ["Zhang", "Xiaoli", ""]]}, {"id": "2003.00689", "submitter": "HaiLong Liu", "authors": "Hailong Liu, Takatsugu Hirayama, Luis Yoichi Morales, Hiroshi Murase", "title": "What Timing for an Automated Vehicle to Make Pedestrians Understand Its\n  Driving Intentions for Improving Their Perception of Safety?", "comments": "Accepted by IEEE ITSC 2020, 7 pages, 9 figures, 1 table", "journal-ref": null, "doi": "10.1109/ITSC45102.2020.9294696", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although automated driving systems have been used frequently, they are still\nunpopular in society. To increase the popularity of automated vehicles (AVs),\nassisting pedestrians to accurately understand the driving intentions and\nimproving their perception of safety when interacting with AVs are considered\neffective. Therefore, the AV should send information about its driving\nintention to pedestrians when they interact with each other. However, the\nfollowing questions should be answered regarding how the AV sends the\ninformation to them: 1) What timing for an AV to make pedestrians understand\nits driving intentions after being noticed by them? 2) What timing for an AV to\nmake pedestrians feel safe after being noticed by them? Thirteen participants\nwere invited to interact with a manually driven vehicle and an AV in an\nexperiment. The participants' gaze information and a subjective evaluation of\ntheir understanding of the driving intention as well as their perception of\nsafety were collected. By analyzing the participants' gaze duration on the\nvehicle with their subjective evaluations, we found that the AV should enable\nthe pedestrian to accurately understand its driving intention within 0.5~6.5\n[s] and make the pedestrian feel safe within 0.5~8.0 [s] while the pedestrian\nis gazing at it.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 06:31:03 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 06:12:43 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 04:56:31 GMT"}, {"version": "v4", "created": "Fri, 12 Jun 2020 07:22:44 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Liu", "Hailong", ""], ["Hirayama", "Takatsugu", ""], ["Morales", "Luis Yoichi", ""], ["Murase", "Hiroshi", ""]]}, {"id": "2003.00809", "submitter": "Indigo Orton", "authors": "Indigo J. D. Orton", "title": "Vision based body gesture meta features for Affective Computing", "comments": "MPhil thesis; 74 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Early detection of psychological distress is key to effective treatment.\nAutomatic detection of distress, such as depression, is an active area of\nresearch. Current approaches utilise vocal, facial, and bodily modalities. Of\nthese, the bodily modality is the least investigated, partially due to the\ndifficulty in extracting bodily representations from videos, and partially due\nto the lack of viable datasets. Existing body modality approaches use automatic\ncategorization of expressions to represent body language as a series of\nspecific expressions, much like words within natural language. In this\ndissertation I present a new type of feature, within the body modality, that\nrepresents meta information of gestures, such as speed, and use it to predict a\nnon-clinical depression label. This differs to existing work by representing\noverall behaviour as a small set of aggregated meta features derived from a\nperson's movement. In my method I extract pose estimation from videos, detect\ngestures within body parts, extract meta information from individual gestures,\nand finally aggregate these features to generate a small feature vector for use\nin prediction tasks. I introduce a new dataset of 65 video recordings of\ninterviews with self-evaluated distress, personality, and demographic labels.\nThis dataset enables the development of features utilising the whole body in\ndistress detection tasks. I evaluate my newly introduced meta-features for\npredicting depression, anxiety, perceived stress, somatic stress, five standard\npersonality measures, and gender. A linear regression based classifier using\nthese features achieves a 82.70% F1 score for predicting depression within my\nnovel dataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 14:38:16 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Orton", "Indigo J. D.", ""]]}, {"id": "2003.00832", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Yunsheng Ma, Yang Gu, Jufeng Yang, Tengfei Xing, Pengfei\n  Xu, Runbo Hu, Hua Chai, Kurt Keutzer", "title": "An End-to-End Visual-Audio Attention Network for Emotion Recognition in\n  User-Generated Videos", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition in user-generated videos plays an important role in\nhuman-centered computing. Existing methods mainly employ traditional two-stage\nshallow pipeline, i.e. extracting visual and/or audio features and training\nclassifiers. In this paper, we propose to recognize video emotions in an\nend-to-end manner based on convolutional neural networks (CNNs). Specifically,\nwe develop a deep Visual-Audio Attention Network (VAANet), a novel architecture\nthat integrates spatial, channel-wise, and temporal attentions into a visual 3D\nCNN and temporal attentions into an audio 2D CNN. Further, we design a special\nclassification loss, i.e. polarity-consistent cross-entropy loss, based on the\npolarity-emotion hierarchy constraint to guide the attention generation.\nExtensive experiments conducted on the challenging VideoEmotion-8 and Ekman-6\ndatasets demonstrate that the proposed VAANet outperforms the state-of-the-art\napproaches for video emotion recognition. Our source code is released at:\nhttps://github.com/maysonma/VAANet.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:33:59 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zhao", "Sicheng", ""], ["Ma", "Yunsheng", ""], ["Gu", "Yang", ""], ["Yang", "Jufeng", ""], ["Xing", "Tengfei", ""], ["Xu", "Pengfei", ""], ["Hu", "Runbo", ""], ["Chai", "Hua", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2003.00902", "submitter": "Memo Akten", "authors": "Memo Akten, Rebecca Fiebrink, Mick Grierson", "title": "Learning to See: You Are What You See", "comments": "Presented as an Art Paper at SIGGRAPH 2019", "journal-ref": "ACM SIGGRAPH 2019 Art Gallery July 2019 Article No 13 Pages 1 to 6", "doi": "10.1145/3306211.3320143", "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors present a visual instrument developed as part of the creation of\nthe artwork Learning to See. The artwork explores bias in artificial neural\nnetworks and provides mechanisms for the manipulation of specifically trained\nfor real-world representations. The exploration of these representations acts\nas a metaphor for the process of developing a visual understanding and/or\nvisual vocabulary of the world. These representations can be explored and\nmanipulated in real time, and have been produced in such a way so as to reflect\nspecific creative perspectives that call into question the relationship between\nhow both artificial neural networks and humans may construct meaning.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 07:12:52 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Akten", "Memo", ""], ["Fiebrink", "Rebecca", ""], ["Grierson", "Mick", ""]]}, {"id": "2003.00910", "submitter": "Memo Akten", "authors": "Memo Akten, Rebecca Fiebrink, Mick Grierson", "title": "Deep Meditations: Controlled navigation of latent space", "comments": "Presented at the 2nd Workshop on Machine Learning for Creativity and\n  Design at the Neural Information Processing Systems (NeurIPS) 2018 conference\n  in Montreal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method which allows users to creatively explore and navigate\nthe vast latent spaces of deep generative models. Specifically, our method\nenables users to \\textit{discover} and \\textit{design} \\textit{trajectories} in\nthese high dimensional spaces, to construct stories, and produce time-based\nmedia such as videos---\\textit{with meaningful control over narrative}. Our\ngoal is to encourage and aid the use of deep generative models as a medium for\ncreative expression and story telling with meaningful human control. Our method\nis analogous to traditional video production pipelines in that we use a\nconventional non-linear video editor with proxy clips, and conform with arrays\nof latent space vectors. Examples can be seen at\n\\url{http://deepmeditations.ai}.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 21:19:44 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Akten", "Memo", ""], ["Fiebrink", "Rebecca", ""], ["Grierson", "Mick", ""]]}, {"id": "2003.00919", "submitter": "Mohammad Amin Alipour", "authors": "Soodeh Atefi, Andrew Truelove, Matheus Rheinschmitt, Eduardo Almeida,\n  Iftekhar Ahmed and Amin Alipour", "title": "Examining user reviews of conversational systems: a case study of Alexa\n  skills", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational systems use spoken language to interact with their users.\nAlthough conversational systems, such as Amazon Alexa, are becoming common and\nafford interesting functionalities, there is little known about the issues\nusers of these systems face.\n  In this paper, we study user reviews of more than 2,800 Alexa skills to\nunderstand the characteristics of the reviews and issues that are raised in\nthem. Our results suggest that most skills receive less than 50 reviews. Our\nqualitative study of user reviews using open coding resulted in identifying 16\ntypes of issues in the user reviews. Issues related to the content, integration\nwith online services and devices, error, and regression are top issues raised\nby the users. Our results also indicate differences in volume and types of\ncomplaints by users when compared with more traditional mobile applications. We\ndiscuss the implication of our results for practitioners and researchers.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 13:52:05 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Atefi", "Soodeh", ""], ["Truelove", "Andrew", ""], ["Rheinschmitt", "Matheus", ""], ["Almeida", "Eduardo", ""], ["Ahmed", "Iftekhar", ""], ["Alipour", "Amin", ""]]}, {"id": "2003.00921", "submitter": "Teus Kappen", "authors": "Teus H. Kappen, Mirko Noordegraaf, Wilton A. van Klei, Karel G.M.\n  Moons and Cor J. Kalkman", "title": "Decision Support in the Context of a Complex Decision Situation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of a clinical decision support tool is to reduce the complexity of\nclinical decisions. However, when decision support tools are poorly implemented\nthey may actually confuse physicians and complicate clinical care. This paper\nargues that information from decision support tools is often removed from the\nclinical context of the targeted decisions. Physicians largely depend on\nclinical context to handle the complexity of their day-to-day decisions.\nClinical context enables them to take into account all ambiguous information\nand patient preferences. Decision support tools that provide analytic\ninformation to physicians, without its context, may then complicate the\ndecision process of physicians. It is likely that the joint forces of\nphysicians and technology will produce better decisions than either of them\nexclusively: after all, they do have different ways of dealing with the\ncomplexity of a decision and are thus complementary. Therefore, the future\nchallenges of decision support do not only reside in the optimization of the\npredictive value of the underlying models and algorithms, but equally in the\neffective communication of information and its context to doctors.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 13:59:53 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Kappen", "Teus H.", ""], ["Noordegraaf", "Mirko", ""], ["van Klei", "Wilton A.", ""], ["Moons", "Karel G. M.", ""], ["Kalkman", "Cor J.", ""]]}, {"id": "2003.00954", "submitter": "Cara Nunez", "authors": "Cara M. Nunez, Bryce N. Huerta, Allison M. Okamura, and Heather\n  Culbertson", "title": "Investigating Social Haptic Illusions for Tactile Stroking (SHIFTS)", "comments": "To be published in IEEE Haptics Symposium 2020", "journal-ref": null, "doi": "10.1109/HAPTICS45997.2020.ras.HAP20.35.f631355d", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common and effective form of social touch is stroking on the forearm. We\nseek to replicate this stroking sensation using haptic illusions. This work\ncompares two methods that provide sequential discrete stimulation: sequential\nnormal indentation and sequential lateral skin-slip using discrete actuators.\nOur goals are to understand which form of stimulation more effectively creates\na continuous stroking sensation, and how many discrete contact points are\nneeded. We performed a study with 20 participants in which they rated\nsensations from the haptic devices on continuity and pleasantness. We found\nthat lateral skin-slip created a more continuous sensation, and decreasing the\nnumber of contact points decreased the continuity. These results inform the\ndesign of future wearable haptic devices and the creation of haptic signals for\neffective social communication.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 14:56:08 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Nunez", "Cara M.", ""], ["Huerta", "Bryce N.", ""], ["Okamura", "Allison M.", ""], ["Culbertson", "Heather", ""]]}, {"id": "2003.00970", "submitter": "Cody Buntain", "authors": "Cody Buntain and Richard Bonneau and Jonathan Nagler and Joshua A.\n  Tucker", "title": "YouTube Recommendations and Effects on Sharing Across Online Social\n  Platforms", "comments": null, "journal-ref": null, "doi": "10.1145/3449085", "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In January 2019, YouTube announced it would exclude potentially harmful\ncontent from video recommendations but allow such videos to remain on the\nplatform. While this step intends to reduce YouTube's role in propagating such\ncontent, continued availability of these videos in other online spaces makes it\nunclear whether this compromise actually reduces their spread. To assess this\nimpact, we apply interrupted time series models to measure whether different\ntypes of YouTube sharing in Twitter and Reddit changed significantly in the\neight months around YouTube's announcement. We evaluate video sharing across\nthree curated sets of potentially harmful, anti-social content: a set of\nconspiracy videos that have been shown to experience reduced recommendations in\nYouTube, a larger set of videos posted by conspiracy-oriented channels, and a\nset of videos posted by alternative influence network (AIN) channels. As a\ncontrol, we also evaluate effects on video sharing in a dataset of videos from\nmainstream news channels. Results show conspiracy-labeled and AIN videos that\nhave evidence of YouTube's de-recommendation experience a significant\ndecreasing trend in sharing on both Twitter and Reddit. For videos from\nconspiracy-oriented channels, however, we see no significant effect in Twitter\nbut find a significant increase in the level of conspiracy-channel sharing in\nReddit. For mainstream news sharing, we actually see an increase in trend on\nboth platforms, suggesting YouTube's suppressing particular content types has a\ntargeted effect. This work finds evidence that reducing exposure to anti-social\nvideos within YouTube, without deletion, has potential pro-social,\ncross-platform effects. At the same time, increases in the level of\nconspiracy-channel sharing raise concerns about content producers' responses to\nthese changes, and platform transparency is needed to evaluate these effects\nfurther.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 15:32:55 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 16:44:22 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 01:17:05 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Buntain", "Cody", ""], ["Bonneau", "Richard", ""], ["Nagler", "Jonathan", ""], ["Tucker", "Joshua A.", ""]]}, {"id": "2003.00975", "submitter": "Jean-Daniel Fekete", "authors": "Caillou Philippe, Renault Jonas, Fekete Jean-Daniel, Letournel\n  Anne-Catherine, Sebag Mich\\`ele", "title": "Cartolabe: A Web-Based Scalable Visualization of Large Document\n  Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe CARTOLABE, a web-based multi-scale system for visualizing and\nexploring large textual corpora based on topics, introducing a novel mechanism\nfor the progressive visualization of filtering queries. Initially designed to\nrepresent and navigate through scientific publications in different\ndisciplines, CARTOLABE has evolved to become a generic framework and\naccommodate various corpora, ranging from Wikipedia (4.5M entries) to the\nFrench National Debate (4.3M entries). CARTOLABE is made of two modules: the\nfirst relies on Natural Language Processing methods, converting a corpus and\nits entities (documents, authors, concepts) into high-dimensional vectors,\ncomputing their projection on the 2D plane, and extracting meaningful labels\nfor regions of the plane. The second module is a web-based visualization,\ndisplaying tiles computed from the multidimensional projection of the corpus\nusing the U MAP projection method. This visualization module aims at enabling\nusers with no expertise in visualization and data analysis to get an overview\nof their corpus, and to interact with it: exploring, querying, filtering,\npanning and zooming on regions of semantic interest. Three use cases are\ndiscussed to illustrate CARTOLABE's versatility and ability to bring large\nscale textual corpus visualization and exploration to a wide audience.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 15:50:53 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 20:27:24 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Philippe", "Caillou", ""], ["Jonas", "Renault", ""], ["Jean-Daniel", "Fekete", ""], ["Anne-Catherine", "Letournel", ""], ["Mich\u00e8le", "Sebag", ""]]}, {"id": "2003.01052", "submitter": "Pavel Chebotarev", "authors": "Pavel Chebotarev and Dmitry Gubanov", "title": "How to choose the most appropriate centrality measure?", "comments": "26 pages, 1 table, 1 algorithm, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.HC cs.LG cs.SI math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to select the most appropriate network centrality\nmeasure based on the user's opinion on how such a measure should work on a set\nof simple graphs. The method consists in: (1) forming a set $\\cal F$ of\ncandidate measures; (2) generating a sequence of sufficiently simple graphs\nthat distinguish all measures in $\\cal F$ on some pairs of nodes; (3) compiling\na survey with questions on comparing the centrality of test nodes; (4)\ncompleting this survey, which provides a centrality measure consistent with all\nuser responses. The developed algorithms make it possible to implement this\napproach for any finite set $\\cal F$ of measures. This paper presents its\nrealization for a set of 40 centrality measures. The proposed method called\nculling can be used for rapid analysis or combined with a normative approach by\ncompiling a survey on the subset of measures that satisfy certain normative\nconditions (axioms). In the present study, the latter was done for the subsets\ndetermined by the Self-consistency or Bridge axioms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 17:38:38 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 18:13:18 GMT"}, {"version": "v3", "created": "Sat, 21 Mar 2020 17:41:31 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Chebotarev", "Pavel", ""], ["Gubanov", "Dmitry", ""]]}, {"id": "2003.01062", "submitter": "Aniket Bera", "authors": "Venkatraman Narayanan, Bala Murali Manoghar, Vishnu Sashank Dorbala,\n  Dinesh Manocha, Aniket Bera", "title": "ProxEmo: Gait-based Emotion Learning and Multi-view Proxemic Fusion for\n  Socially-Aware Robot Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ProxEmo, a novel end-to-end emotion prediction algorithm for\nsocially aware robot navigation among pedestrians. Our approach predicts the\nperceived emotions of a pedestrian from walking gaits, which is then used for\nemotion-guided navigation taking into account social and proxemic constraints.\nTo classify emotions, we propose a multi-view skeleton graph convolution-based\nmodel that works on a commodity camera mounted onto a moving robot. Our emotion\nrecognition is integrated into a mapless navigation scheme and makes no\nassumptions about the environment of pedestrian motion. It achieves a mean\naverage emotion prediction precision of 82.47% on the Emotion-Gait benchmark\ndataset. We outperform current state-of-art algorithms for emotion recognition\nfrom 3D gaits. We highlight its benefits in terms of navigation in indoor\nscenes using a Clearpath Jackal robot.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 17:47:49 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 15:38:09 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Narayanan", "Venkatraman", ""], ["Manoghar", "Bala Murali", ""], ["Dorbala", "Vishnu Sashank", ""], ["Manocha", "Dinesh", ""], ["Bera", "Aniket", ""]]}, {"id": "2003.01092", "submitter": "Dimitrios Chamzas", "authors": "Dimitrios Chamzas and Konstantinos Moustakas", "title": "3D Augmented Reality Tangible User Interface using Commodity Hardware", "comments": "15th International Joint Conference on Computer Vision, Imaging and\n  Computer Graphics Theory and Applications (GRAPP), Valletta, Malta, 25-27\n  February 2020", "journal-ref": null, "doi": "10.5220/0009173303840391", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last years, the emerging field of Augmented and Virtual Reality\n(AR-VR) has seen tremendous growth. An interface that has also become very\npopular for the AR systems is the tangible interface or passive-haptic\ninterface. Specifically, an interface where users can manipulate digital\ninformation with input devices that are physical objects. This work presents a\nlow cost Augmented Reality system with a tangible interface that offers\ninteraction between the real and the virtual world. The system estimates in\nreal-time the 3D position of a small colored ball (input device), it maps it to\nthe 3D virtual world and then uses it to control the AR application that runs\nin a mobile device. Using the 3D position of our \"input\" device, it allows us\nto implement more complicated interactivity compared to a 2D input device.\nFinally, we present a simple, fast and robust algorithm that can estimate the\ncorners of a convex quadrangle. The proposed algorithm is suitable for the fast\nregistration of markers and significantly improves performance compared to the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 18:29:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chamzas", "Dimitrios", ""], ["Moustakas", "Konstantinos", ""]]}, {"id": "2003.01238", "submitter": "Andr\\'es P\\'aez", "authors": "Andr\\'es P\\'aez", "title": "Robot Mindreading and the Problem of Trust", "comments": "2020 Convention of the Society for the Study of Artificial\n  Intelligence and Simulation of Behavior", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper raises three questions regarding the attribution of beliefs,\ndesires, and intentions to robots. The first one is whether humans in fact\nengage in robot mindreading. If they do, this raises a second question: does\nrobot mindreading foster trust towards robots? Both of these questions are\nempirical, and I show that the available evidence is insufficient to answer\nthem. Now, if we assume that the answer to both questions is affirmative, a\nthird and more important question arises: should developers and engineers\npromote robot mindreading in view of their stated goal of enhancing\ntransparency? My worry here is that by attempting to make robots more\nmind-readable, they are abandoning the project of understanding automatic\ndecision processes. Features that enhance mind-readability are prone to make\nthe factors that determine automatic decisions even more opaque than they\nalready are. And current strategies to eliminate opacity do not enhance\nmind-readability. The last part of the paper discusses different ways to\nanalyze this apparent trade-off and suggests that a possible solution must\nadopt tolerable degrees of opacity that depend on pragmatic factors connected\nto the level of trust required for the intended uses of the robot.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 22:55:42 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["P\u00e1ez", "Andr\u00e9s", ""]]}, {"id": "2003.01274", "submitter": "Devi Parikh", "authors": "Devi Parikh", "title": "Predicting A Creator's Preferences In, and From, Interactive Generative\n  Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a lay user creates an art piece using an interactive generative art tool,\nwhat, if anything, do the choices they make tell us about them and their\npreferences? These preferences could be in the specific generative art form\n(e.g., color palettes, density of the piece, thickness or curvatures of any\nlines in the piece); predicting them could lead to a smarter interactive tool.\nOr they could be preferences in other walks of life (e.g., music, fashion,\nfood, interior design, paintings) or attributes of the person (e.g.,\npersonality type, gender, artistic inclinations); predicting them could lead to\nimproved personalized recommendations for products or experiences.\n  To study this research question, we collect preferences from 311 subjects,\nboth in a specific generative art form and in other walks of life. We analyze\nthe preferences and train machine learning models to predict a subset of\npreferences from the remaining. We find that preferences in the generative art\nform we studied cannot predict preferences in other walks of life better than\nchance (and vice versa). However, preferences within the generative art form\nare reliably predictive of each other.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 01:05:45 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Parikh", "Devi", ""]]}, {"id": "2003.01304", "submitter": "Aditeya Pandey", "authors": "Aditeya Pandey, Yixuan Zhang, John A. Guerra-Gomez, Andrea G. Parker,\n  Michelle A. Borkin", "title": "Digital Collaborator: Augmenting Task Abstraction in Visualization\n  Design with Artificial Intelligence", "comments": "This paper has been accepted at CHI 2020 workshop - Workshop on\n  Artificial Intelligence for HCI: A Modern Approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the task abstraction phase of the visualization design process, including\nin \"design studies\", a practitioner maps the observed domain goals to\ngeneralizable abstract tasks using visualization theory in order to better\nunderstand and address the users needs. We argue that this manual task\nabstraction process is prone to errors due to designer biases and a lack of\ndomain background and knowledge. Under these circumstances, a collaborator can\nhelp validate and provide sanity checks to visualization practitioners during\nthis important task abstraction stage. However, having a human collaborator is\nnot always feasible and may be subject to the same biases and pitfalls. In this\npaper, we first describe the challenges associated with task abstraction. We\nthen propose a conceptual Digital Collaborator: an artificial intelligence\nsystem that aims to help visualization practitioners by augmenting their\nability to validate and reason about the output of task abstraction. We also\ndiscuss several practical design challenges of designing and implementing such\nsystems\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 02:53:34 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Pandey", "Aditeya", ""], ["Zhang", "Yixuan", ""], ["Guerra-Gomez", "John A.", ""], ["Parker", "Andrea G.", ""], ["Borkin", "Michelle A.", ""]]}, {"id": "2003.01318", "submitter": "Jessica Van Brummelen", "authors": "Jessica Van Brummelen, Kevin Weng, Phoebe Lin, Catherine Yeo", "title": "Convo: What does conversational programming need? An exploration of\n  machine learning interface design", "comments": "9 pages, 7 figures, submitted to VL/HCC 2020, for associated user\n  study video: https://youtu.be/TC5P3OO5exo", "journal-ref": null, "doi": "10.1109/VL/HCC50065.2020.9127277", "report-no": null, "categories": "cs.HC cs.AI cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vast improvements in natural language understanding and speech recognition\nhave paved the way for conversational interaction with computers. While\nconversational agents have often been used for short goal-oriented dialog, we\nknow little about agents for developing computer programs. To explore the\nutility of natural language for programming, we conducted a study ($n$=45)\ncomparing different input methods to a conversational programming system we\ndeveloped. Participants completed novice and advanced tasks using voice-based,\ntext-based, and voice-or-text-based systems. We found that users appreciated\naspects of each system (e.g., voice-input efficiency, text-input precision) and\nthat novice users were more optimistic about programming using voice-input than\nadvanced users. Our results show that future conversational programming tools\nshould be tailored to users' programming experience and allow users to choose\ntheir preferred input mode. To reduce cognitive load, future interfaces can\nincorporate visualizations and possess custom natural language understanding\nand speech recognition models for programming.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 03:39:37 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Van Brummelen", "Jessica", ""], ["Weng", "Kevin", ""], ["Lin", "Phoebe", ""], ["Yeo", "Catherine", ""]]}, {"id": "2003.01425", "submitter": "Chaehan So", "authors": "Chaehan So", "title": "Understanding the Prediction Mechanism of Sentiments by XAI\n  Visualization", "comments": "This is the author's prefinal version be published in conference\n  proceedings: 4th International Conference on Natural Language Processing and\n  Information Retrieval, Sejong, South Korea, 26-28 June, 2020, ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People often rely on online reviews to make purchase decisions. The present\nwork aimed to gain an understanding of a machine learning model's prediction\nmechanism by visualizing the effect of sentiments extracted from online hotel\nreviews with explainable AI (XAI) methodology. Study 1 used the extracted\nsentiments as features to predict the review ratings by five machine learning\nalgorithms (knn, CART decision trees, support vector machines, random forests,\ngradient boosting machines) and identified random forests as best algorithm.\nStudy 2 analyzed the random forests model by feature importance and revealed\nthe sentiments joy, disgust, positive and negative as the most predictive\nfeatures. Furthermore, the visualization of additive variable attributions and\ntheir prediction distribution showed correct prediction in direction and effect\nsize for the 5-star rating but partially wrong direction and insufficient\neffect size for the 1-star rating. These prediction details were corroborated\nby a what-if analysis for the four top features. In conclusion, the prediction\nmechanism of a machine learning model can be uncovered by visualization of\nparticular observations. Comparing instances of contrasting ground truth values\ncan draw a differential picture of the prediction mechanism and inform\ndecisions for model improvement.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 10:25:50 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["So", "Chaehan", ""]]}, {"id": "2003.01525", "submitter": "Juliana Ferreira J", "authors": "Juliana Jansen Ferreira and Mateus de Souza Monteiro", "title": "Evidence-based explanation to promote fairness in AI systems", "comments": "Fair & Responsible AI Workshop @ CHI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As Artificial Intelligence (AI) technology gets more intertwined with every\nsystem, people are using AI to make decisions on their everyday activities. In\nsimple contexts, such as Netflix recommendations, or in more complex context\nlike in judicial scenarios, AI is part of people's decisions. People make\ndecisions and usually, they need to explain their decision to others or in some\nmatter. It is particularly critical in contexts where human expertise is\ncentral to decision-making. In order to explain their decisions with AI\nsupport, people need to understand how AI is part of that decision. When\nconsidering the aspect of fairness, the role that AI has on a decision-making\nprocess becomes even more sensitive since it affects the fairness and the\nresponsibility of those people making the ultimate decision. We have been\nexploring an evidence-based explanation design approach to 'tell the story of a\ndecision'. In this position paper, we discuss our approach for AI systems using\nfairness sensitive cases in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 14:22:11 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Ferreira", "Juliana Jansen", ""], ["Monteiro", "Mateus de Souza", ""]]}, {"id": "2003.02093", "submitter": "Xiao Ma", "authors": "Xiao Ma, Taylor W. Brown", "title": "AI-Mediated Exchange Theory", "comments": "For workshop \"Human-Centered Approaches to Fair and Responsible AI\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Artificial Intelligence (AI) plays an ever-expanding role in\nsociotechnical systems, it is important to articulate the relationships between\nhumans and AI. However, the scholarly communities studying human-AI\nrelationships -- including but not limited to social computing, machine\nlearning, science and technology studies, and other social sciences -- are\ndivided by the perspectives that define them. These perspectives vary both by\ntheir focus on humans or AI, and in the micro/macro lenses through which they\napproach subjects. These differences inhibit the integration of findings, and\nthus impede science and interdisciplinarity. In this position paper, we propose\nthe development of a framework AI-Mediated Exchange Theory (AI-MET) to bridge\nthese divides. As an extension to Social Exchange Theory (SET) in the social\nsciences, AI-MET views AI as influencing human-to-human relationships via a\ntaxonomy of mediation mechanisms. We list initial ideas of these mechanisms,\nand show how AI-MET can be used to help human-AI research communities speak to\none another.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 14:18:18 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Ma", "Xiao", ""], ["Brown", "Taylor W.", ""]]}, {"id": "2003.02097", "submitter": "Yara Rizk", "authors": "Yara Rizk, Vatche Isahagian, Merve Unuvar, Yasaman Khazaeni", "title": "A Snooze-less User-Aware Notification System for Proactive\n  Conversational Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of smart phones and electronic devices has placed a wealth of\ninformation at the fingertips of consumers as well as creators of digital\ncontent. This has led to millions of notifications being issued each second\nfrom alerts about posted YouTube videos to tweets, emails and personal\nmessages. Adding work related notifications and we can see how quickly the\nnumber of notifications increases. Not only does this cause reduced\nproductivity and concentration but has also been shown to cause alert fatigue.\nThis condition makes users desensitized to notifications, causing them to\nignore or miss important alerts. Depending on what domain users work in, the\ncost of missing a notification can vary from a mere inconvenience to life and\ndeath. Therefore, in this work, we propose an alert and notification framework\nthat intelligently issues, suppresses and aggregates notifications, based on\nevent severity, user preferences, or schedules, to minimize the need for users\nto ignore, or snooze their notifications and potentially forget about\naddressing important ones. Our framework can be deployed as a backend service,\nbut is better suited to be integrated into proactive conversational agents, a\nfield receiving a lot of attention with the digital transformation era, email\nservices, news services and others. However, the main challenge lies in\ndeveloping the right machine learning algorithms that can learn models from a\nwide set of users while customizing these models to individual users'\npreferences.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 14:31:21 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Rizk", "Yara", ""], ["Isahagian", "Vatche", ""], ["Unuvar", "Merve", ""], ["Khazaeni", "Yasaman", ""]]}, {"id": "2003.02260", "submitter": "Javad Fotouhi", "authors": "Javad Fotouhi, Arian Mehrfard, Tianyu Song, Alex Johnson, Greg Osgood,\n  Mathias Unberath, Mehran Armand, and Nassir Navab", "title": "Spatiotemporal-Aware Augmented Reality: Redefining HCI in Image-Guided\n  Therapy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suboptimal interaction with patient data and challenges in mastering 3D\nanatomy based on ill-posed 2D interventional images are essential concerns in\nimage-guided therapies. Augmented reality (AR) has been introduced in the\noperating rooms in the last decade; however, in image-guided interventions, it\nhas often only been considered as a visualization device improving traditional\nworkflows. As a consequence, the technology is gaining minimum maturity that it\nrequires to redefine new procedures, user interfaces, and interactions. The\nmain contribution of this paper is to reveal how exemplary workflows are\nredefined by taking full advantage of head-mounted displays when entirely\nco-registered with the imaging system at all times. The proposed AR landscape\nis enabled by co-localizing the users and the imaging devices via the operating\nroom environment and exploiting all involved frustums to move spatial\ninformation between different bodies. The awareness of the system from the\ngeometric and physical characteristics of X-ray imaging allows the redefinition\nof different human-machine interfaces. We demonstrate that this AR paradigm is\ngeneric, and can benefit a wide variety of procedures. Our system achieved an\nerror of $4.76\\pm2.91$ mm for placing K-wire in a fracture management\nprocedure, and yielded errors of $1.57\\pm1.16^\\circ$ and $1.46\\pm1.00^\\circ$ in\nthe abduction and anteversion angles, respectively, for total hip arthroplasty.\nWe hope that our holistic approach towards improving the interface of surgery\nnot only augments the surgeon's capabilities but also augments the surgical\nteam's experience in carrying out an effective intervention with reduced\ncomplications and provide novel approaches of documenting procedures for\ntraining purposes.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 18:59:55 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Fotouhi", "Javad", ""], ["Mehrfard", "Arian", ""], ["Song", "Tianyu", ""], ["Johnson", "Alex", ""], ["Osgood", "Greg", ""], ["Unberath", "Mathias", ""], ["Armand", "Mehran", ""], ["Navab", "Nassir", ""]]}, {"id": "2003.02307", "submitter": "Sorin Matei", "authors": "Sorin Adam Matei", "title": "What is affordance theory and how can it be used in communication\n  research?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affordance theory proposes that the use of an object is intrinsically\ndetermined by its physical shape. However, when translated to digital objects,\naffordance theory loses explanatory power, as the same physical affordances,\nfor example, screens, can have many socially constructed meanings and can be\nused in many ways. Furthermore, the affordance theory core idea that physical\naffordances have intrinsic, pre-cognitive meaning cannot be sustained for the\nhighly symbolic nature of digital affordances, which gain meaning through\nsocial learning and use. A possible way to solve this issue is to think about\non-screen affordances as symbols and affordance research as a semiotic and\nlinguistic enterprise.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 19:51:30 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Matei", "Sorin Adam", ""]]}, {"id": "2003.02312", "submitter": "Goda Klumbyte", "authors": "Goda Klumbyte, Claude Draude, Loren Britton", "title": "Re-Imagining HCI: New Materialist Philosophy and Figurations as Tool for\n  Design", "comments": "7 pages, 1 figure, paper was presented at the workshop 'Standing on\n  the Shoulders of Giants: Exploring the Intersection of Philosophy and HCI',\n  ACM CHI Conference on Human Factors in Computing Systems, 4-9 May 2019,\n  Glasgow, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we interrogate the practices of imagining in human-computer\ninteraction (HCI), particularly in scenario building (SBE) and persona\nconstruction. We discuss the philosophical premises of HCI imaginings in\nrationalism, cognitivism and phenomenology, and we propose (feminist) new\nmaterialist philosophy as an enriching perspective that helps generate a\nholistic, relational perspective of users, imaginaries and technologies. In the\nend we explore the method of figurations as a potential tool for HCI design.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 20:11:26 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Klumbyte", "Goda", ""], ["Draude", "Claude", ""], ["Britton", "Loren", ""]]}, {"id": "2003.02428", "submitter": "Steffen Holter", "authors": "Oscar Gomez, Steffen Holter, Jun Yuan, Enrico Bertini", "title": "ViCE: Visual Counterfactual Explanations for Machine Learning Models", "comments": "4 pages, 2 figures, ACM IUI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continued improvements in the predictive accuracy of machine learning\nmodels have allowed for their widespread practical application. Yet, many\ndecisions made with seemingly accurate models still require verification by\ndomain experts. In addition, end-users of a model also want to understand the\nreasons behind specific decisions. Thus, the need for interpretability is\nincreasingly paramount. In this paper we present an interactive visual\nanalytics tool, ViCE, that generates counterfactual explanations to\ncontextualize and evaluate model decisions. Each sample is assessed to identify\nthe minimal set of changes needed to flip the model's output. These\nexplanations aim to provide end-users with personalized actionable insights\nwith which to understand, and possibly contest or improve, automated decisions.\nThe results are effectively displayed in a visual interface where\ncounterfactual explanations are highlighted and interactive methods are\nprovided for users to explore the data and model. The functionality of the tool\nis demonstrated by its application to a home equity line of credit dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 04:43:02 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Gomez", "Oscar", ""], ["Holter", "Steffen", ""], ["Yuan", "Jun", ""], ["Bertini", "Enrico", ""]]}, {"id": "2003.02537", "submitter": "Irene Celino", "authors": "Irene Celino and Gloria Re Calegari", "title": "Submitting surveys via a conversational interface: an evaluation of user\n  acceptance and approach effectiveness", "comments": "44 pages, 8 figures", "journal-ref": "International Journal of Human Computer Studies, Vol. 139 (2020)", "doi": "10.1016/j.ijhcs.2020.102410", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversational interfaces are currently on the rise: more and more\napplications rely on a chat-like interaction pattern to increase their\nacceptability and to improve user experience. Also in the area of questionnaire\ndesign and administration, interaction design is increasingly looked at as an\nimportant ingredient of a digital solution. For those reasons, we designed and\ndeveloped a conversational survey tool to administer questionnaires with a\ncolloquial form through a chat-like Web interface.\n  In this paper, we present the evaluation results of our approach, taking into\naccount both the user point of view - by assessing user acceptance and\npreferences in terms of survey compilation experience - and the survey design\nperspective - by investigating the effectiveness of a conversational survey in\ncomparison to a traditional questionnaire. We show that users clearly\nappreciate the conversational form and prefer it over a traditional approach\nand that, from a data collection point of view, the conversational method shows\nthe same reliability and a higher response quality with respect to a\ntraditional questionnaire.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 11:31:24 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Celino", "Irene", ""], ["Calegari", "Gloria Re", ""]]}, {"id": "2003.02622", "submitter": "Toby Jia-Jun Li", "authors": "Toby Jia-Jun Li, Jingya Chen, Tom M. Mitchell, Brad A. Myers", "title": "Towards Effective Human-AI Collaboration in GUI-Based Interactive Task\n  Learning Agents", "comments": null, "journal-ref": "CHI 2020 Workshop on Artificial Intelligence for HCI: A Modern\n  Approach (AI4HCI)", "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that a key challenge in enabling usable and useful interactive task\nlearning for intelligent agents is to facilitate effective Human-AI\ncollaboration. We reflect on our past 5 years of efforts on designing,\ndeveloping and studying the SUGILITE system, discuss the issues on\nincorporating recent advances in AI with HCI principles in mixed-initiative\ninteractions and multi-modal interactions, and summarize the lessons we\nlearned. Lastly, we identify several challenges and opportunities, and describe\nour ongoing work\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 14:12:19 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Li", "Toby Jia-Jun", ""], ["Chen", "Jingya", ""], ["Mitchell", "Tom M.", ""], ["Myers", "Brad A.", ""]]}, {"id": "2003.02662", "submitter": "Diego Alberto Mercado-Ravell Dr.", "authors": "Brandon Yam-Viramontes and Diego Mercado-Ravell", "title": "Implementation of a Natural User Interface to Command a Drone", "comments": "2020 International Conference on Unmanned Aircraft Systems (ICUAS),\n  Athens, Greece, 2020", "journal-ref": null, "doi": "10.1109/ICUAS48674.2020.9213995", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the use of a Natural User Interface (NUI) through\nbody gestures using the open source library OpenPose, looking for a more\ndynamic and intuitive way to control a drone. For the implementation, we use\nthe Robotic Operative System (ROS) to control and manage the different\ncomponents of the project. Wrapped inside ROS, OpenPose (OP) processes the\nvideo obtained in real-time by a commercial drone, allowing to obtain the\nuser's pose. Finally, the keypoints from OpenPose are obtained and translated,\nusing geometric constraints, to specify high-level commands to the drone.\nReal-time experiments validate the full strategy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 14:39:33 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yam-Viramontes", "Brandon", ""], ["Mercado-Ravell", "Diego", ""]]}, {"id": "2003.02976", "submitter": "Dinislam Abdulgalimov", "authors": "Dinislam Abdulgalimov, Reuben Kirkham, James Nicholson, Vasilis\n  Vlachokyriakos, Pam Briggs, Patrick Olivier", "title": "Designing for Employee Voice", "comments": "10 pages, 4 figures, CHI 2020 Proceedings", "journal-ref": null, "doi": "10.1145/3313831.3376284", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Employee voice and workplace democracy have a positive impact on employee\nwellbeing and the performance of organizations. In this paper, we conducted\ninterviews with employees to identify facilitators and inhibitors for the voice\nwithin the workplace and a corresponding set of appropriate qualities:\nCivility, Validity, Safety and Egalitarianism. We then operationalised these\nqualities as a set of design goals - Assured Anonymity, Constructive\nModeration, Adequate Slowness and Controlled Access - in the design and\ndevelopment of a secure anonymous employee voice system. Our novel take on the\nEnterprise Social Network aims to foster good citizenship whilst also promoting\nfrank yet constructive discussion. We reflect on a two-week deployment of our\nsystem, the diverse range of candid discussions that emerged around important\nworkplace issues and the potential for change within the host organization. We\nconclude by reflecting on the ways in which our approach shaped the discourse\nand supported the creation of a trusted environment for employee voice.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 00:28:59 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Abdulgalimov", "Dinislam", ""], ["Kirkham", "Reuben", ""], ["Nicholson", "James", ""], ["Vlachokyriakos", "Vasilis", ""], ["Briggs", "Pam", ""], ["Olivier", "Patrick", ""]]}, {"id": "2003.02986", "submitter": "Xiao Ma", "authors": "Xiao Ma, Ariel Liu", "title": "Challenges in Supporting Exploratory Search through Voice Assistants", "comments": "For workshop \"Mapping Grand Challenges for the Conversational User\n  Interface Community\", CHI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice assistants have been successfully adopted for simple, routine tasks,\nsuch as asking for the weather or setting an alarm. However, as people get more\nfamiliar with voice assistants, they may increase their expectations for more\ncomplex tasks, such as exploratory search-- e.g., \"What should I do when I\nvisit Paris with kids? Oh, and ideally not too expensive.\" Compared to simple\nsearch tasks such as \"How tall is the Eiffel Tower?\", which can be answered\nwith a single-shot answer, the response to exploratory search is more nuanced,\nespecially through voice-based assistants. In this paper, we outline four\nchallenges in designing voice assistants that can better support exploratory\nsearch: addressing situationally induced impairments; working with mixed-modal\ninteractions; designing for diverse populations; and meeting users'\nexpectations and gaining their trust. Addressing these challenges is important\nfor developing more \"intelligent\" voice-based personal assistants.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 01:10:39 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Ma", "Xiao", ""], ["Liu", "Ariel", ""]]}, {"id": "2003.03025", "submitter": "Long-fei Chen", "authors": "Chen Long-fei, Yuichi Nakamura, Kazuaki Kondo", "title": "Modeling User Behaviors in Machine Operation Tasks for Adaptive Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adaptive guidance system that supports equipment operators requires a\ncomprehensive model, which involves a variety of user behaviors that considers\ndifferent skill and knowledge levels, as well as rapid-changing task\nsituations. In the present paper, we introduced a novel method for modeling\noperational tasks, aiming to integrate visual operation records provided by\nusers with diverse experience levels and personal characteristics. For this\npurpose, we investigated the relationships between user behavior patterns that\ncould be visually observed and their skill levels under machine operation\nconditions. We considered 144 samples of two sewing tasks performed by 12\noperators using a head-mounted RGB-D camera and a static gaze tracker.\nBehavioral features, such as the operator's gaze and head movements, hand\ninteractions, and hotspots, were observed with significant behavioral trends\nresulting from continuous user skill improvement. We used a two-step method to\nmodel the diversity of user behavior: prototype selection and experience\nintegration based on skill ranking. The experimental results showed that\nseveral features could serve as appropriate indices for user skill evaluation,\nas well as providing valuable clues for revealing personal behavioral\ncharacteristics. The integration of user records with different skills and\noperational habits allowed developing a rich, inclusive task model that could\nbe used flexibly to adapt to diverse user-specific needs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 04:05:08 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 12:30:36 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 05:40:10 GMT"}, {"version": "v4", "created": "Fri, 11 Sep 2020 06:31:23 GMT"}, {"version": "v5", "created": "Wed, 16 Sep 2020 06:57:52 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Long-fei", "Chen", ""], ["Nakamura", "Yuichi", ""], ["Kondo", "Kazuaki", ""]]}, {"id": "2003.03133", "submitter": "Kshitij Tiwari", "authors": "Kshitij Tiwari, Ville Kyrki, Allen Cheung, Naohide Yamamoto", "title": "DeFINE: Delayed Feedback based Immersive Navigation Environment for\n  Studying Goal-Directed Human Navigation", "comments": "43 pages, 10 figures, 5 tables, Submitted to Behavioral Research\n  Methods", "journal-ref": "Behav Res (2021)", "doi": "10.3758/s13428-021-01586-6", "report-no": null, "categories": "cs.HC cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of consumer-grade products for presenting an immersive\nvirtual environment (VE), there is a growing interest in utilizing VEs for\ntesting human navigation behavior. However, preparing a VE still requires a\nhigh level of technical expertise in computer graphics and virtual reality,\nposing a significant hurdle to embracing the emerging technology. To address\nthis issue, this paper presents Delayed Feedback based Immersive Navigation\nEnvironment (DeFINE), a framework that allows for easy creation and\nadministration of navigation tasks within customizable VEs via intuitive\ngraphical user interfaces and simple settings files. Importantly, DeFINE has a\nbuilt-in capability to provide performance feedback to participants during an\nexperiment, a feature that is critically missing in other similar frameworks.\nTo show the usability of DeFINE from both experimentalists' and participants'\nperspectives, a demonstration was made in which participants navigated to a\nhidden goal location with feedback that differentially weighted speed and\naccuracy of their responses. In addition, the participants evaluated DeFINE in\nterms of its ease of use, required workload, and proneness to induce\ncybersickness. The demonstration exemplified typical experimental manipulations\nDeFINE accommodates and what types of data it can collect for characterizing\nparticipants' task performance. With its out-of-the-box functionality and\npotential customizability due to open-source licensing, DeFINE makes VEs more\naccessible to many researchers.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 11:00:12 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 09:03:41 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Tiwari", "Kshitij", ""], ["Kyrki", "Ville", ""], ["Cheung", "Allen", ""], ["Yamamoto", "Naohide", ""]]}, {"id": "2003.03158", "submitter": "Malin Eiband", "authors": "Sarah Theres V\\\"olkel, Christina Schneegass, Malin Eiband, Daniel\n  Buschek", "title": "What is \"Intelligent\" in Intelligent User Interfaces? A Meta-Analysis of\n  25 Years of IUI", "comments": "20 pages, 5 figures, 1 table", "journal-ref": "Proceedings of the 25th International Conference on Intelligent\n  User Interfaces (IUI) 2020", "doi": "10.1145/3377325.3377500", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This reflection paper takes the 25th IUI conference milestone as an\nopportunity to analyse in detail the understanding of intelligence in the\ncommunity: Despite the focus on intelligent UIs, it has remained elusive what\nexactly renders an interactive system or user interface \"intelligent\", also in\nthe fields of HCI and AI at large. We follow a bottom-up approach to analyse\nthe emergent meaning of intelligence in the IUI community: In particular, we\napply text analysis to extract all occurrences of \"intelligent\" in all IUI\nproceedings. We manually review these with regard to three main questions: 1)\nWhat is deemed intelligent? 2) How (else) is it characterised? and 3) What\ncapabilities are attributed to an intelligent entity? We discuss the\ncommunity's emerging implicit perspective on characteristics of intelligence in\nintelligent user interfaces and conclude with ideas for stating one's own\nunderstanding of intelligence more explicitly.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 12:30:06 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["V\u00f6lkel", "Sarah Theres", ""], ["Schneegass", "Christina", ""], ["Eiband", "Malin", ""], ["Buschek", "Daniel", ""]]}, {"id": "2003.03193", "submitter": "Zhengwei Wang", "authors": "Zhengwei Wang, Qi She, Alan F. Smeaton, Tomas E. Ward, Graham Healy", "title": "A Neuro-AI Interface for Evaluating Generative Adversarial Networks", "comments": "Accepted by ICLR 2020 Workshop Bridging AI and Cognitive Science\n  (BAICS). arXiv admin note: substantial text overlap with arXiv:1905.04243", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are increasingly attracting attention\nin the computer vision, natural language processing, speech synthesis and\nsimilar domains. However, evaluating the performance of GANs is still an open\nand challenging problem. Existing evaluation metrics primarily measure the\ndissimilarity between real and generated images using automated statistical\nmethods. They often require large sample sizes for evaluation and do not\ndirectly reflect human perception of image quality. In this work, we introduce\nan evaluation metric called Neuroscore, for evaluating the performance of GANs,\nthat more directly reflects psychoperceptual image quality through the\nutilization of brain signals. Our results show that Neuroscore has superior\nperformance to the current evaluation metrics in that: (1) It is more\nconsistent with human judgment; (2) The evaluation process needs much smaller\nnumbers of samples; and (3) It is able to rank the quality of images on a per\nGAN basis. A convolutional neural network (CNN) based neuro-AI interface is\nproposed to predict Neuroscore from GAN-generated images directly without the\nneed for neural responses. Importantly, we show that including neural responses\nduring the training phase of the network can significantly improve the\nprediction capability of the proposed model. Codes and data can be referred at\nthis link: https://github.com/villawang/Neuro-AI-Interface.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 17:53:43 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 10:42:02 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Zhengwei", ""], ["She", "Qi", ""], ["Smeaton", "Alan F.", ""], ["Ward", "Tomas E.", ""], ["Healy", "Graham", ""]]}, {"id": "2003.03207", "submitter": "William Seymour", "authors": "William Seymour and Max Van Kleek", "title": "Does Siri Have a Soul? Exploring Voice Assistants Through Shinto Design\n  Fictions", "comments": "11 pages, 2 images. To appear in the Extended Abstracts of the 2020\n  CHI Conference on Human Factors in Computing Systems (CHI '20)", "journal-ref": null, "doi": "10.1145/3334480.3381809", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It can be difficult to critically reflect on technology that has become part\nof everyday rituals and routines. To combat this, speculative and fictional\napproaches have previously been used by HCI to decontextualise the familiar and\nimagine alternatives. In this work we turn to Japanese Shinto narratives as a\nway to defamiliarise voice assistants, inspired by the similarities between how\nassistants appear to 'inhabit' objects similarly to kami. Describing an\nalternate future where assistant presences live inside objects, this approach\nforegrounds some of the phenomenological quirks that can otherwise easily\nbecome lost. Divorced from the reality of daily life, this approach allows us\nto reevaluate some of the common interactions and design patterns that are\ncommon in the virtual assistants of the present.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 13:54:08 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Seymour", "William", ""], ["Van Kleek", "Max", ""]]}, {"id": "2003.03273", "submitter": "Daniel Buschek", "authors": "Florian Lehmann, Daniel Buschek", "title": "Heartbeats in the Wild: A Field Study Exploring ECG Biometrics in\n  Everyday Life", "comments": "14 pages, 10 figures, CHI'20", "journal-ref": null, "doi": "10.1145/3313831.3376536", "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on an in-depth study of electrocardiogram (ECG) biometrics\nin everyday life. We collected ECG data from 20 people over a week, using a\nnon-medical chest tracker. We evaluated user identification accuracy in several\nscenarios and observed equal error rates of 9.15% to 21.91%, heavily depending\non 1) the number of days used for training, and 2) the number of heartbeats\nused per identification decision. We conclude that ECG biometrics can work in\nthe wild but are less robust than expected based on the literature,\nhighlighting that previous lab studies obtained highly optimistic results with\nregard to real life deployments. We explain this with noise due to changing\nbody postures and states as well as interrupted measures. We conclude with\nimplications for future research and the design of ECG biometrics systems for\nreal world deployments, including critical reflections on privacy.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 15:18:52 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Lehmann", "Florian", ""], ["Buschek", "Daniel", ""]]}, {"id": "2003.03318", "submitter": "Marc Faddoul", "authors": "Marc Faddoul, Guillaume Chaslot and Hany Farid", "title": "A Longitudinal Analysis of YouTube's Promotion of Conspiracy Videos", "comments": "8 pages, 3 figures. This paper was first released on March 2nd, 2020\n  along with a coverage from the New York Times available at\n  https://www.nytimes.com/interactive/2020/03/02/technology/youtube-conspiracy-theory.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conspiracy theories have flourished on social media, raising concerns that\nsuch content is fueling the spread of disinformation, supporting extremist\nideologies, and in some cases, leading to violence. Under increased scrutiny\nand pressure from legislators and the public, YouTube announced efforts to\nchange their recommendation algorithms so that the most egregious conspiracy\nvideos are demoted and demonetized. To verify this claim, we have developed a\nclassifier for automatically determining if a video is conspiratorial (e.g.,\nthe moon landing was faked, the pyramids of Giza were built by aliens, end of\nthe world prophecies, etc.). We coupled this classifier with an emulation of\nYouTube's watch-next algorithm on more than a thousand popular informational\nchannels to obtain a year-long picture of the videos actively promoted by\nYouTube. We also obtained trends of the so-called filter-bubble effect for\nconspiracy theories.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 17:31:30 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Faddoul", "Marc", ""], ["Chaslot", "Guillaume", ""], ["Farid", "Hany", ""]]}, {"id": "2003.03438", "submitter": "Julian Frommel", "authors": "Julian Frommel, Valentin Sagl, Ansgar E. Depping, Colby Johanson,\n  Matthew K. Miller, Regan L. Mandryk", "title": "Recognizing Affiliation: Using Behavioural Traces to Predict the Quality\n  of Social Interactions in Online Games", "comments": "CHI '20", "journal-ref": null, "doi": "10.1145/3313831.3376446", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social interactions in multiplayer games can be supportive and\npositive or toxic and harmful; however, few methods can easily assess\ninterpersonal interaction quality in games. We use behavioural traces to\npredict affiliation between dyadic strangers, facilitated through their social\ninteractions in an online gaming setting. We collected audio, video, in-game,\nand self-report data from 23 dyads, extracted 75 features, trained Random\nForest and Support Vector Machine models, and evaluated their performance\npredicting binary (high/low) as well as continuous affiliation toward a\npartner. The models can predict both binary and continuous affiliation with up\nto 79.1% accuracy (F1) and 20.1% explained variance (R2) on unseen data, with\nfeatures based on verbal communication demonstrating the highest potential. Our\nfindings can inform the design of multiplayer games and game communities, and\nguide the development of systems for matchmaking and mitigating toxic behaviour\nin online games.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 20:56:05 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Frommel", "Julian", ""], ["Sagl", "Valentin", ""], ["Depping", "Ansgar E.", ""], ["Johanson", "Colby", ""], ["Miller", "Matthew K.", ""], ["Mandryk", "Regan L.", ""]]}, {"id": "2003.03516", "submitter": "Lingfeng Tao", "authors": "Lingfeng Tao, Michael Bowman, Xu Zhou, Jiucai Zhang, Xiaoli Zhang", "title": "Learn and Transfer Knowledge of Preferred Assistance Strategies in\n  Semi-autonomous Telemanipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling robots to provide effective assistance yet still accommodating the\noperator's commands for telemanipulation of an object is very challenging\nbecause robot's assistive action is not always intuitive for human operators\nand human behaviors and preferences are sometimes ambiguous for the robot to\ninterpret. Although various assistance approaches are being developed to\nimprove the control quality from different optimization perspectives, the\nproblem still remains in determining the appropriate approach that satisfies\nthe fine motion constraints for the telemanipulation task and preference of the\noperator. To address these problems, we developed a novel preference-aware\nassistance knowledge learning approach. An assistance preference model learns\nwhat assistance is preferred by a human, and a stagewise model updating method\nensures the learning stability while dealing with the ambiguity of human\npreference data. Such a preference-aware assistance knowledge enables a\nteleoperated robot hand to provide more active yet preferred assistance toward\nmanipulation success. We also developed knowledge transfer methods to transfer\nthe preference knowledge across different robot hand structures to avoid\nextensive robot-specific training. Experiments to telemanipulate a 3-finger\nhand and 2-finger hand, respectively, to use, move, and hand over a cup have\nbeen conducted. Results demonstrated that the methods enabled the robots to\neffectively learn the preference knowledge and allowed knowledge transfer\nbetween robots with less training effort.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 04:49:57 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 20:21:40 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Tao", "Lingfeng", ""], ["Bowman", "Michael", ""], ["Zhou", "Xu", ""], ["Zhang", "Jiucai", ""], ["Zhang", "Xiaoli", ""]]}, {"id": "2003.03541", "submitter": "Devansh Saxena", "authors": "Devansh Saxena, Karla Badillo-Urquiola, Pamela J. Wisniewski, and\n  Shion Guha", "title": "A Human-Centered Review of the Algorithms used within the U.S. Child\n  Welfare System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The U.S. Child Welfare System (CWS) is charged with improving outcomes for\nfoster youth; yet, they are overburdened and underfunded. To overcome this\nlimitation, several states have turned towards algorithmic decision-making\nsystems to reduce costs and determine better processes for improving CWS\noutcomes. Using a human-centered algorithmic design approach, we synthesize 50\npeer-reviewed publications on computational systems used in CWS to assess how\nthey were being developed, common characteristics of predictors used, as well\nas the target outcomes. We found that most of the literature has focused on\nrisk assessment models but does not consider theoretical approaches (e.g.,\nchild-foster parent matching) nor the perspectives of caseworkers (e.g., case\nnotes). Therefore, future algorithms should strive to be context-aware and\ntheoretically robust by incorporating salient factors identified by past\nresearch. We provide the HCI community with research avenues for developing\nhuman-centered algorithms that redirect attention towards more equitable\noutcomes for CWS.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 09:16:12 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Saxena", "Devansh", ""], ["Badillo-Urquiola", "Karla", ""], ["Wisniewski", "Pamela J.", ""], ["Guha", "Shion", ""]]}, {"id": "2003.03610", "submitter": "Radek O\\v{s}lej\\v{s}ek", "authors": "Radek O\\v{s}lej\\v{s}ek, V\\'it Rus\\v{n}\\'ak, Karol\\'ina Bursk\\'a,\n  Valdemar \\v{S}v\\'abensk\\'y, Jan Vykopal, Jakub \\v{C}egan", "title": "Conceptual Model of Visual Analytics for Hands-on Cybersecurity Training", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2020.2977336", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hands-on training is an effective way to practice theoretical cybersecurity\nconcepts and increase participants' skills. In this paper, we discuss the\napplication of visual analytics principles to the design, execution, and\nevaluation of training sessions. We propose a conceptual model employing visual\nanalytics that supports the sensemaking activities of users involved in various\nphases of the training life cycle. The model emerged from our long-term\nexperience in designing and organizing diverse hands-on cybersecurity training\nsessions. It provides a classification of visualizations and can be used as a\nframework for developing novel visualization tools supporting phases of the\ntraining life-cycle. We demonstrate the model application on examples covering\ntwo types of cybersecurity training programs.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 17:19:55 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["O\u0161lej\u0161ek", "Radek", ""], ["Rus\u0148\u00e1k", "V\u00edt", ""], ["Bursk\u00e1", "Karol\u00edna", ""], ["\u0160v\u00e1bensk\u00fd", "Valdemar", ""], ["Vykopal", "Jan", ""], ["\u010cegan", "Jakub", ""]]}, {"id": "2003.03645", "submitter": "Nabiha Asghar", "authors": "Nabiha Asghar, Ivan Kobyzev, Jesse Hoey, Pascal Poupart, and Muhammad\n  Bilal Sheikh", "title": "Generating Emotionally Aligned Responses in Dialogues using Affect\n  Control Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art neural dialogue systems excel at syntactic and semantic\nmodelling of language, but often have a hard time establishing emotional\nalignment with the human interactant during a conversation. In this work, we\nbring Affect Control Theory (ACT), a socio-mathematical model of emotions for\nhuman-human interactions, to the neural dialogue generation setting. ACT makes\npredictions about how humans respond to emotional stimuli in social situations.\nDue to this property, ACT and its derivative probabilistic models have been\nsuccessfully deployed in several applications of Human-Computer Interaction,\nincluding empathetic tutoring systems, assistive healthcare devices and\ntwo-person social dilemma games. We investigate how ACT can be used to develop\naffect-aware neural conversational agents, which produce emotionally aligned\nresponses to prompts and take into consideration the affective identities of\nthe interactants.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 19:31:08 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 06:46:25 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Asghar", "Nabiha", ""], ["Kobyzev", "Ivan", ""], ["Hoey", "Jesse", ""], ["Poupart", "Pascal", ""], ["Sheikh", "Muhammad Bilal", ""]]}, {"id": "2003.03703", "submitter": "Dongxu Li", "authors": "Dongxu Li, Xin Yu, Chenchen Xu, Lars Petersson, Hongdong Li", "title": "Transferring Cross-domain Knowledge for Video Sign Language Recognition", "comments": "CVPR2020 (oral) preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word-level sign language recognition (WSLR) is a fundamental task in sign\nlanguage interpretation. It requires models to recognize isolated sign words\nfrom videos. However, annotating WSLR data needs expert knowledge, thus\nlimiting WSLR dataset acquisition. On the contrary, there are abundant\nsubtitled sign news videos on the internet. Since these videos have no\nword-level annotation and exhibit a large domain gap from isolated signs, they\ncannot be directly used for training WSLR models. We observe that despite the\nexistence of a large domain gap, isolated and news signs share the same visual\nconcepts, such as hand gestures and body movements. Motivated by this\nobservation, we propose a novel method that learns domain-invariant visual\nconcepts and fertilizes WSLR models by transferring knowledge of subtitled news\nsign to them. To this end, we extract news signs using a base WSLR model, and\nthen design a classifier jointly trained on news and isolated signs to coarsely\nalign these two domain features. In order to learn domain-invariant features\nwithin each class and suppress domain-specific features, our method further\nresorts to an external memory to store the class centroids of the aligned news\nsigns. We then design a temporal attention based on the learnt descriptor to\nimprove recognition performance. Experimental results on standard WSLR datasets\nshow that our method outperforms previous state-of-the-art methods\nsignificantly. We also demonstrate the effectiveness of our method on\nautomatically localizing signs from sign news, achieving 28.1 for AP@0.5.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 03:05:21 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 14:53:06 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Li", "Dongxu", ""], ["Yu", "Xin", ""], ["Xu", "Chenchen", ""], ["Petersson", "Lars", ""], ["Li", "Hongdong", ""]]}, {"id": "2003.04204", "submitter": "Hongbo Zou", "authors": "Hongbo Zou, Hsuanwei Michelle Chen and Sharmistha Dey", "title": "Engaging Users through Social Media in Public Libraries", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The participatory library is an emerging concept which refers to the idea\nthat an integrated library system must allow users to take part in core\nfunctions of the library rather than engaging on the periphery. To embrace the\nparticipatory idea, libraries have employed many technologies, such as social\nmedia to help them build participatory services and engage users. To help\nlibrarians understand the impact of emerging technologies on a participatory\nservice building, this paper takes social media as an example to explore how to\nuse different engagement strategies that social media provides to engage more\nusers. This paper provides three major contributions to the library system. The\nlibraries can use the resultant engagement strategies to engage its users.\nAdditionally, the best-fit strategy can be inferred and designed based on the\npreferences of users. Lastly, the preferences of users can be understood based\non data analysis of social media. Three such contributions put together to\nfully address the proposed research question of how to use different engagement\nstrategies on social media to build participatory library services and better\nengage more users visiting the library?\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 00:08:33 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 20:34:21 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zou", "Hongbo", ""], ["Chen", "Hsuanwei Michelle", ""], ["Dey", "Sharmistha", ""]]}, {"id": "2003.04250", "submitter": "Brendan David-John", "authors": "Brendan John, Sophie J\\\"org, Sanjeev Koppal, Eakta Jain", "title": "The Security-Utility Trade-off for Iris Authentication and Eye Animation\n  for Social Virtual Avatars", "comments": "11 pages, 10 figures, to appear in IEEE TVCG Special Issue on IEEE VR\n  2020", "journal-ref": null, "doi": "10.1109/TVCG.2020.2973052", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gaze behavior of virtual avatars is critical to social presence and\nperceived eye contact during social interactions in Virtual Reality. Virtual\nReality headsets are being designed with integrated eye tracking to enable\ncompelling virtual social interactions. This paper shows that the near\ninfra-red cameras used in eye tracking capture eye images that contain iris\npatterns of the user. Because iris patterns are a gold standard biometric, the\ncurrent technology places the user's biometric identity at risk. Our first\ncontribution is an optical defocus based hardware solution to remove the iris\nbiometric from the stream of eye tracking images. We characterize the\nperformance of this solution with different internal parameters. Our second\ncontribution is a psychophysical experiment with a same-different task that\ninvestigates the sensitivity of users to a virtual avatar's eye movements when\nthis solution is applied. By deriving detection threshold values, our findings\nprovide a range of defocus parameters where the change in eye movements would\ngo unnoticed in a conversational setting. Our third contribution is a\nperceptual study to determine the impact of defocus parameters on the perceived\neye contact, attentiveness, naturalness, and truthfulness of the avatar. Thus,\nif a user wishes to protect their iris biometric, our approach provides a\nsolution that balances biometric protection while preventing their conversation\npartner from perceiving a difference in the user's virtual avatar. This work is\nthe first to develop secure eye tracking configurations for VR/AR/XR\napplications and motivates future work in the area.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 16:48:25 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["John", "Brendan", ""], ["J\u00f6rg", "Sophie", ""], ["Koppal", "Sanjeev", ""], ["Jain", "Eakta", ""]]}, {"id": "2003.04311", "submitter": "Kizito Nkurikiyeyezu", "authors": "Guillaume Lopez, Takuya Aoki, Kizito Nkurikiyeyezu, and Anna Yokokubo", "title": "Model for Thermal Comfort and Energy Saving Based on Individual\n  Sensation Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In office spaces, the ratio of energy consumption of air conditioning and\nlighting for maintaining the environment comfort is about 70%. On the other\nhand, many people claim being dissatisfied with the temperature of the air\nconditioning. Therefore, there is concern about work efficiency reduction\ncaused by the current air conditioning control. In this research, we propose an\nautomatic control system that improves both energy-saving and thermal comfort\nof all indoor users by quantifying individual differences in thermal comfort\nfrom vital information, on the basis of which the optimal settings of both air\nconditioning and wearable systems that can directly heat and cool individuals\nare determined. Various environments were simulated with different room sizes,\nnumbers of users in a room, and heating/cooling conditions. The simulation\nresults demonstrated the efficiency of the proposed system for both energy\nsaving and comfort maximization.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 00:07:26 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Lopez", "Guillaume", ""], ["Aoki", "Takuya", ""], ["Nkurikiyeyezu", "Kizito", ""], ["Yokokubo", "Anna", ""]]}, {"id": "2003.04661", "submitter": "Martin Johannes Kraemer", "authors": "Martin J. Kraemer, Ulrik Lyngs, Helena Webb, Ivan Flechais", "title": "Further Exploring Communal Technology Use in Smart Homes: Social\n  Expectations", "comments": "to appear in CHI '20 Extended Abstracts, April 25--30, 2020,\n  Honolulu, HI, USA", "journal-ref": null, "doi": "10.1145/3334480.3382972", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Device use in smart homes is becoming increasingly communal, requiring\ncohabitants to navigate a complex social and technological context. In this\npaper, we report findings from an exploratory survey grounded in our prior work\non communal technology use in the home [4]. The findings highlight the\nimportance of considering qualities of social relationships and technology in\nunderstanding expectations and intentions of communal technology use. We\npropose a design perspective of social expectations, and we suggest existing\ndesigns can be expanded using already available information such as location,\nand considering additional information, such as levels of trust and\nreliability.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 12:29:40 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Kraemer", "Martin J.", ""], ["Lyngs", "Ulrik", ""], ["Webb", "Helena", ""], ["Flechais", "Ivan", ""]]}, {"id": "2003.04799", "submitter": "Franziska Tachtler", "authors": "Franziska Tachtler, Toni Michel, Petr Slov\\'ak, Geraldine Fitzpatrick", "title": "Supporting the Supporters of Unaccompanied Migrant Youth: Designing for\n  Social-ecological Resilience", "comments": "10 pages (excluding references), 1 figure. To appear in the\n  Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems\n  (CHI'20), April 25--30, 2020, Honolulu, HI, USA", "journal-ref": "Proceedings of the 2020 CHI Conference on Human Factors in\n  Computing Systems (CHI'20)", "doi": "10.1145/3313831.3376458", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unaccompanied migrant youth, fleeing to a new country without their parents,\nare exposed to mental health risks. Resilience interventions mitigate such\nrisks, but access can be hindered by systemic and personal barriers. While much\nwork has recently addressed designing technology to promote mental health, none\nhas focused on the needs of these populations. This paper presents the results\nof interviews with 18 professional/ volunteer support workers and 5\nunaccompanied migrant youths, followed by three design workshops. The results\npoint to the diverse systems that can facilitate youths' resilience\ndevelopment. The relationship between the youth and volunteers acting as\nmentors is particularly important for increasing resilience but comes with\nchallenges. This suggests the relevance of a social-ecological model of\nresilience with a focus on designing technology to support the mentors in order\nto help them better support the youth. We conclude by mapping out the design\nspace for mentor support.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 15:24:07 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Tachtler", "Franziska", ""], ["Michel", "Toni", ""], ["Slov\u00e1k", "Petr", ""], ["Fitzpatrick", "Geraldine", ""]]}, {"id": "2003.04868", "submitter": "Philipp Markert", "authors": "Philipp Markert, Daniel V. Bailey, Maximilian Golla, Markus D\\\"urmuth,\n  Adam J. Aviv", "title": "This PIN Can Be Easily Guessed: Analyzing the Security of Smartphone\n  Unlock PINs", "comments": "15+3 pages, 9 figures, 8+5 tables", "journal-ref": "IEEE Symposium on Security and Privacy (SP), 2020", "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide the first comprehensive study of user-chosen 4- and\n6-digit PINs (n=1220) collected on smartphones with participants being\nexplicitly primed for device unlocking. We find that against a throttled\nattacker (with 10, 30, or 100 guesses, matching the smartphone unlock setting),\nusing 6-digit PINs instead of 4-digit PINs provides little to no increase in\nsecurity, and surprisingly may even decrease security. We also study the\neffects of blocklists, where a set of \"easy to guess\" PINs is disallowed during\nselection. Two such blocklists are in use today by iOS, for 4-digits (274 PINs)\nas well as 6-digits (2910 PINs). We extracted both blocklists compared them\nwith four other blocklists, including a small 4-digit (27 PINs), a large\n4-digit (2740 PINs), and two placebo blocklists for 4- and 6-digit PINs that\nalways excluded the first-choice PIN. We find that relatively small blocklists\nin use today by iOS offer little or no benefit against a throttled guessing\nattack. Security gains are only observed when the blocklists are much larger,\nwhich in turn comes at the cost of increased user frustration. Our analysis\nsuggests that a blocklist at about 10% of the PIN space may provide the best\nbalance between usability and security.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 17:30:16 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 11:01:41 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 12:02:18 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Markert", "Philipp", ""], ["Bailey", "Daniel V.", ""], ["Golla", "Maximilian", ""], ["D\u00fcrmuth", "Markus", ""], ["Aviv", "Adam J.", ""]]}, {"id": "2003.04997", "submitter": "Michael Byrne", "authors": "Philip Kortum, Michael D. Byrne, and Julie Whitmore", "title": "Voter Verification of BMD Ballots Is a Two-Part Question: Can They?\n  Mostly, They Can. Do They? Mostly, They Don't", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of whether or not voters actually verify ballots produced by\nballot marking devices (BMDs) is presently the subject of some controversy.\nRecent studies (e.g., Bernhard, et al. 2020) suggest the verification rate is\nlow. What is not clear from previous research is whether this is more a result\nof voters being unable to do so accurately or whether this is because voters\nsimply choose not to attempt verification in the first place. In order to\nunderstand this problem, we conducted an experiment in which 108 participants\nparticipated in a mock election where the BMD displayed the voters' true\nchoices, but then changed a subset of those choices on the printed ballot. The\ndesign of the printed ballot, the length of the ballot, the number of changes\nthat were made to the ballot, the location of those changes, and the\ninstructions provided to the voters were manipulated as part of the experiment.\nResults indicated that of those voters who chose to examine the printed ballot,\n76% detected anomalies, indicating that voters can reliably detect errors on\ntheir ballot if they will simply review it. This suggests that administrative\nremedies, rather than attempts to alter fundamental human perceptual\ncapabilities, could be employed to encourage voters to check their ballots,\nwhich could prove as an effective countermeasure.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 21:06:42 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Kortum", "Philip", ""], ["Byrne", "Michael D.", ""], ["Whitmore", "Julie", ""]]}, {"id": "2003.05026", "submitter": "Max Van Kleek", "authors": "Max Van Kleek", "title": "Super-reflective Data: Speculative Imaginings of a World Where Data\n  Works for People", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It's the year 2020, and every space and place on- and off-line has been\naugmented with digital things that observe, record, transmit, and compute, for\nthe purposes of recording endless data traces of what is happening in the\nworld. Individually, these things (and the invisible services the power them)\nhave reached considerable sophistication in their ability to analyse and\ndissect such observations, turning streams of audio and video into informative\ndata fragments. Yet somehow, individuals as end-users of platforms and services\nhave not seen the full potential of such data. In this speculative paper, we\npropose two hypothetical mini scenarios different from our current digital\nworld. In the former, instead of hoarding it, data controllers turn captured\ndata over to those who need it as quickly as possible, working together to\ncombine, validate, and refine it for maximum usefulness. This simultaneously\naddresses the data fragmentation and privacy problem, by handing over long-term\ndata governance to those that value it the most In the latter, we discuss\nethical dilemmas using the long-term use of such rich data and its tendency to\ncause people to relentlessly optimise.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 22:54:10 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Van Kleek", "Max", ""]]}, {"id": "2003.05108", "submitter": "Xiaoyu Zhang", "authors": "Xiaoyu Zhang, Senthil Chandrasegaran, Kwan-Liu Ma", "title": "ConceptScope: Organizing and Visualizing Knowledge in Documents based on\n  Domain Ontology", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": "10.1145/3411764.3445396", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current text visualization techniques typically provide overviews of document\ncontent and structure using intrinsic properties such as term frequencies,\nco-occurrences, and sentence structures. Such visualizations lack conceptual\noverviews incorporating domain-relevant knowledge, needed when examining\ndocuments such as research articles or technical reports. To address this\nshortcoming, we present ConceptScope, a technique that utilizes a domain\nontology to represent the conceptual relationships in a document in the form of\na Bubble Treemap visualization. Multiple coordinated views of document\nstructure and concept hierarchy with text overviews further aid document\nanalysis. ConceptScope facilitates exploration and comparison of single and\nmultiple documents respectively. We demonstrate ConceptScope by visualizing\nresearch articles and transcripts of technical presentations in computer\nscience. In a comparative study with DocuBurst, a popular document\nvisualization tool, ConceptScope was found to be more informative in exploring\nand comparing domain-specific documents, but less so when it came to documents\nthat spanned multiple disciplines.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 04:34:09 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 06:35:18 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Zhang", "Xiaoyu", ""], ["Chandrasegaran", "Senthil", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2003.05212", "submitter": "Hongzhuo Liang", "authors": "Shuang Li, Jiaxi Jiang, Philipp Ruppel, Hongzhuo Liang, Xiaojian Ma,\n  Norman Hendrich, Fuchun Sun, Jianwei Zhang", "title": "A Mobile Robot Hand-Arm Teleoperation System by Vision and IMU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multimodal mobile teleoperation system that\nconsists of a novel vision-based hand pose regression network (Transteleop) and\nan IMU-based arm tracking method. Transteleop observes the human hand through a\nlow-cost depth camera and generates not only joint angles but also depth images\nof paired robot hand poses through an image-to-image translation process. A\nkeypoint-based reconstruction loss explores the resemblance in appearance and\nanatomy between human and robotic hands and enriches the local features of\nreconstructed images. A wearable camera holder enables simultaneous hand-arm\ncontrol and facilitates the mobility of the whole teleoperation system. Network\nevaluation results on a test dataset and a variety of complex manipulation\ntasks that go beyond simple pick-and-place operations show the efficiency and\nstability of our multimodal teleoperation system.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 10:57:24 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Li", "Shuang", ""], ["Jiang", "Jiaxi", ""], ["Ruppel", "Philipp", ""], ["Liang", "Hongzhuo", ""], ["Ma", "Xiaojian", ""], ["Hendrich", "Norman", ""], ["Sun", "Fuchun", ""], ["Zhang", "Jianwei", ""]]}, {"id": "2003.05240", "submitter": "Christian Meske", "authors": "Christian Meske, Konstantin Wilms, Stefan Stieglitz", "title": "Enterprise Social Networks as Digital Infrastructures -- Understanding\n  the Utilitarian Value of Social Media at the Workplace", "comments": null, "journal-ref": "Information Systems Management, 36:4, pp. 350-367 (2019)", "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this study, we first show that while both the perceived usefulness and\nperceived enjoyment of enterprise social networks impact employees' intentions\nfor continuous participation, the utilitarian value significantly outpaces its\nhedonic value. Second, we prove that the network's utilitarian value is\nconstituted by its digital infrastructure characteristics: versatility,\nadaptability, interconnectedness and invisibility-in-use. The study is set\nwithin a software engineering company and bases on quantitative survey\nresearch, applying partial least squares structural equation modeling.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 11:56:27 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Meske", "Christian", ""], ["Wilms", "Konstantin", ""], ["Stieglitz", "Stefan", ""]]}, {"id": "2003.05249", "submitter": "Christian Meske", "authors": "Christian Meske, Ireti Amojo", "title": "Ethical Guidelines for the Construction of Digital Nudges", "comments": "Meske, C. and Amojo, I. (2020). Ethical Guidelines for the\n  Construction of Digital Nudges. 53rd Hawaii International Conference on\n  Systems Sciences (HICSS), pp. 3928-3937", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Under certain circumstances, humans tend to behave in irrational ways,\nleading to situations in which they make undesirable choices. The concept of\ndigital nudging addresses these limitations of bounded rationality by\nestablishing a libertarian paternalist alternative to nudge users in virtual\nenvironments towards their own preferential choices. Thereby, choice\narchitectures are designed to address biases and heuristics involved in\ncognitive thinking. As research on digital nudging has become increasingly\npopular in the Information Systems community, an increasing necessity for\nethical guidelines has emerged around this concept to safeguard its\nlegitimization in distinction to e.g. persuasion or manipulation. However,\nreflecting on ethical debates regarding digital nudging in academia, we find\nthat current conceptualizations are scare. This is where on the basis of\nexisting literature, we provide a conceptualization of ethical guidelines for\nthe design of digital nudges, and thereby aim to ensure the applicability of\nnudging mechanisms in virtual environments.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 12:08:26 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Meske", "Christian", ""], ["Amojo", "Ireti", ""]]}, {"id": "2003.05251", "submitter": "Sebastian Wallk\\\"otter", "authors": "Sebastian Wallkotter, Silvia Tulli, Ginevra Castellano, Ana Paiva,\n  Mohamed Chetouani", "title": "Explainable Agents Through Social Cues: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of how to make embodied agents explainable has experienced a surge\nof interest over the last three years, and, there are many terms that refer to\nthis concept, e.g., transparency or legibility. One reason for this high\nvariance in terminology is the unique array of social cues that embodied agents\ncan access in contrast to that accessed by non-embodied agents. Another reason\nis that different authors use these terms in different ways. Hence, we review\nthe existing literature on explainability and organize it by (1) providing an\noverview of existing definitions, (2) showing how explainability is implemented\nand how it exploits different social cues, and (3) showing how the impact of\nexplainability is measured. Additionally, we present a list of open questions\nand challenges that highlight areas that require further investigation by the\ncommunity. This provides the interested reader with an overview of the current\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 12:12:29 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 12:06:14 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 17:53:30 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Wallkotter", "Sebastian", ""], ["Tulli", "Silvia", ""], ["Castellano", "Ginevra", ""], ["Paiva", "Ana", ""], ["Chetouani", "Mohamed", ""]]}, {"id": "2003.05268", "submitter": "Chaehan So", "authors": "Chaehan So", "title": "Human-in-the-Loop Design Cycles -- A Process Framework that Integrates\n  Design Sprints, Agile Processes, and Machine Learning with Humans", "comments": "To be published in: Lecture Notes in Artificial Intelligence, 1st\n  International Conference on Artificial Intelligence in HCI, AI-HCI, Held as\n  Part of HCI International 2020, Kopenhagen, Denmark, July 19-24, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demands on more transparency of the backbox nature of machine learning models\nhave led to the recent rise of human-in-the-loop in machine learning, i.e.\nprocesses that integrate humans in the training and application of machine\nlearning models. The present work argues that this process requirement does not\nrepresent an obstacle but an opportunity to optimize the design process. Hence,\nthis work proposes a new process framework, Human-in-the-learning-loop (HILL)\nDesign Cycles - a design process that integrates the structural elements of\nagile and design thinking process, and controls the training of a machine\nlearning model by the human in the loop. The HILL Design Cycles process\nreplaces the qualitative user testing by a quantitative psychometric\nmeasurement instrument for design perception. The generated user feedback\nserves to train a machine learning model and to instruct the subsequent design\ncycle along four design dimensions (novelty, energy, simplicity, tool). Mapping\nthe four-dimensional user feedback into user stories and priorities, the design\nsprint thus transforms the user feedback directly into the implementation\nprocess. The human in the loop is a quality engineer who scrutinizes the\ncollected user feedback to prevents invalid data to enter machine learning\nmodel training.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 07:35:35 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["So", "Chaehan", ""]]}, {"id": "2003.05530", "submitter": "Alarith Uhde", "authors": "Holger Klapperich, Alarith Uhde, and Marc Hassenzahl", "title": "Understanding and Designing Automation with Peoples' Wellbeing in Mind", "comments": "7 pages, 4 figures", "journal-ref": "Extended Abstracts of the 2019 CHI Conference on Human Factors in\n  Computing Systems", "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, automation not only dominates industry but becomes more and more a\npart of our private, everyday lives. Following the notion of increased\nconvenience and more time for the \"important things in life\", automation\nrelieves us from many daily household chores - robots vacuum floors and\nautomated coffeemakers produce supposedly barista-quality coffee on the press\nof a button. In many cases these offers are embraced by people without further\nquestioning. Of course, automation frees us from many unloved activities, but\nwe may also lose something by delegating more and more everyday activities to\nautomation. In a series of four studies, we explored the experiential costs of\neveryday automation and strategies of how to design technology to reconcile\nexperience with the advantages of ever more powerful automation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 21:31:35 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Klapperich", "Holger", ""], ["Uhde", "Alarith", ""], ["Hassenzahl", "Marc", ""]]}, {"id": "2003.05533", "submitter": "Alarith Uhde", "authors": "Matthias Laschke and Alarith Uhde and Marc Hassenzahl", "title": "Positive Work Practices. Opportunities and Challenges in Designing\n  Meaningful Work-related Technology", "comments": "5 pages, to be published in Extended Abstracts of the 2020 CHI\n  Conference on Human Factors in Computing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Work is a rich source of meaning. However, beyond organizational changes,\nmost approaches in the research field of Meaningful Work neglected the power of\nwork-related technology to increase meaning. Using two cases as examples, this\npaper proposes a wellbeing-driven approach to the design of work-related\ntechnology. Despite the positive results of our cases, we argue that the use of\ntechnology as a means of increasing meaning in the workplace is still in its\ninfancy.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 21:38:53 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Laschke", "Matthias", ""], ["Uhde", "Alarith", ""], ["Hassenzahl", "Marc", ""]]}, {"id": "2003.05756", "submitter": "Marten Teitsma", "authors": "Marten Teitsma, Vasco Chibante Barosso, Pascal Boeschoten and Patrick\n  Hendriks (for the ALICE Collaboration)", "title": "Jiskefet, a bookkeeping application for ALICE", "comments": "6 pages, 2 figures, Proceedings of CHEP conference, 2019", "journal-ref": null, "doi": "10.1051/epjconf/202024504023", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new bookkeeping system called Jiskefet is being developed for A Large Ion\nCollider Experiment (ALICE) during Long Shutdown 2, to be in production until\nthe end of LHC Run 4 (2029). Jiskefet unifies two functionalities: a)\ngathering, storing and presenting metadata associated with the operations of\nthe ALICE experiment and b) tracking the asynchronous processing of the physics\ndata. It will replace the existing ALICE Electronic Logbook and AliMonitor,\nallowing for a technology refresh and the inclusion of new features based on\nthe experience collected during Run 1 and Run 2. The front end leverages web\ntechnologies much in use nowadays such as TypeScript and NodeJS and is adaptive\nto various clients such as tablets, mobile devices and other screens. The back\nend includes an OpenAPI specification based REST API and a relational database.\nThis paper will describe the organization of the work done by various student\nteams who work on Jiskefet in sequential and parallel semesters and how\ncontinuity is guaranteed by using guidelines on coding, documentation and\ndevelopment. It will also describe the current status of the development, the\ninitial experience in detector stand-alone commissioning setups and the future\nplans.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 12:51:32 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Teitsma", "Marten", "", "for the ALICE Collaboration"], ["Barosso", "Vasco Chibante", "", "for the ALICE Collaboration"], ["Boeschoten", "Pascal", "", "for the ALICE Collaboration"], ["Hendriks", "Patrick", "", "for the ALICE Collaboration"]]}, {"id": "2003.05782", "submitter": "Bedir Tekinerdogan", "authors": "Murat Acar and Bedir Tekinerdogan", "title": "Analyzing the Impact of Automated User Assistance Systems: A Systematic\n  Review", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: User assistance is generally defined as the guided assistance to a\nuser of a software system in order to help accomplish tasks and enhance user\nexperience. Automated user assistance systems are equipped with online help\nsystem that provides information to the user in an electronic format and which\ncan be opened directly in the application. Various different automated user\nassistance approaches have been proposed in the literature. However, there has\nbeen no attempt to systematically review and report the impact of automated\nuser assistance systems. Objective: The overall objective of this systematic\nreview is to identify the state of art in automated user assistance systems,\nand describe the reported evidence for automated user assistance. Method: A\nsystematic literature review is conducted by a multiphase study selection\nprocess using the published literature since 2002. Results: We reviewed 575\npapers that are discovered using a well-planned review protocol, and 31 of them\nwere assessed as primary studies related to our research questions.\nConclusions: Our study shows that user assistance systems can provide important\nbenefits for the user but still more research is required in this domain.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 13:14:56 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Acar", "Murat", ""], ["Tekinerdogan", "Bedir", ""]]}, {"id": "2003.05823", "submitter": "Jamison Heard", "authors": "Jamison Heard and Julian Fortune and Julie A. Adams", "title": "SAHRTA: A Supervisory-Based Adaptive Human-Robot Teaming Architecture", "comments": "Accepted to IEEE CogSIMA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervisory-based human-robot teams are deployed in various dynamic and\nextreme environments (e.g., space exploration). Achieving high task performance\nin such environments is critical, as a mistake may lead to significant monetary\nloss or human injury. Task performance may be augmented by adapting the\nsupervisory interface's interactions or autonomy levels based on the human\nsupervisor's workload level, as workload is related to task performance.\nTypical adaptive systems rely solely on the human's overall or cognitive\nworkload state to select what adaptation strategy to implement; however,\noverall workload encompasses many dimensions (i.e., cognitive, physical,\nvisual, auditory, and speech) called workload components. Selecting an\nappropriate adaptation strategy based on a complete human workload state\n(rather than a single workload dimension) may allow for more impactful\nadaptations that ensure high task performance. A Supervisory-Based Adaptive\nHuman-Robot Teaming Architecture (SAHRTA) that selects an appropriate level of\nautonomy or system interaction based on a complete real-time multi-dimensional\nworkload estimate and predicted future task performance is introduced. SAHRTA\nwas shown to improve overall task performance in a physically expanded version\nof the NASA Multi-Attribute Task Battery.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 14:39:46 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Heard", "Jamison", ""], ["Fortune", "Julian", ""], ["Adams", "Julie A.", ""]]}, {"id": "2003.05870", "submitter": "Francisco Javier Chiyah Garcia", "authors": "Francisco J. Chiyah Garcia, Jos\\'e Lopes, Helen Hastie", "title": "Natural Language Interaction to Facilitate Mental Models of Remote\n  Robots", "comments": "In Workshop on Mental Models of Robots at HRI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly complex and autonomous robots are being deployed in real-world\nenvironments with far-reaching consequences. High-stakes scenarios, such as\nemergency response or offshore energy platform and nuclear inspections, require\nrobot operators to have clear mental models of what the robots can and can't\ndo. However, operators are often not the original designers of the robots and\nthus, they do not necessarily have such clear mental models, especially if they\nare novice users. This lack of mental model clarity can slow adoption and can\nnegatively impact human-machine teaming. We propose that interaction with a\nconversational assistant, who acts as a mediator, can help the user with\nunderstanding the functionality of remote robots and increase transparency\nthrough natural language explanations, as well as facilitate the evaluation of\noperators' mental models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 16:03:27 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Garcia", "Francisco J. Chiyah", ""], ["Lopes", "Jos\u00e9", ""], ["Hastie", "Helen", ""]]}, {"id": "2003.05995", "submitter": "Francisco Javier Chiyah Garcia", "authors": "Francisco J. Chiyah Garcia, Jos\\'e Lopes, Xingkun Liu, Helen Hastie", "title": "CRWIZ: A Framework for Crowdsourcing Real-Time Wizard-of-Oz Dialogues", "comments": "10 pages, 5 figures. To Appear in LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large corpora of task-based and open-domain conversational dialogues are\nhugely valuable in the field of data-driven dialogue systems. Crowdsourcing\nplatforms, such as Amazon Mechanical Turk, have been an effective method for\ncollecting such large amounts of data. However, difficulties arise when\ntask-based dialogues require expert domain knowledge or rapid access to\ndomain-relevant information, such as databases for tourism. This will become\neven more prevalent as dialogue systems become increasingly ambitious,\nexpanding into tasks with high levels of complexity that require collaboration\nand forward planning, such as in our domain of emergency response. In this\npaper, we propose CRWIZ: a framework for collecting real-time Wizard of Oz\ndialogues through crowdsourcing for collaborative, complex tasks. This\nframework uses semi-guided dialogue to avoid interactions that breach\nprocedures and processes only known to experts, while enabling the capture of a\nwide variety of interactions. The framework is available at\nhttps://github.com/JChiyah/crwiz\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 19:47:29 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Garcia", "Francisco J. Chiyah", ""], ["Lopes", "Jos\u00e9", ""], ["Liu", "Xingkun", ""], ["Hastie", "Helen", ""]]}, {"id": "2003.06186", "submitter": "Daniel Buschek", "authors": "Sarah Theres V\\\"olkel, Ramona Sch\\\"odel, Daniel Buschek, Clemens\n  Stachl, Verena Winterhalter, Markus B\\\"uhner, Heinrich Hussmann", "title": "Developing a Personality Model for Speech-based Conversational Agents\n  Using the Psycholexical Approach", "comments": "14 pages, 2 figures, 3 tables, CHI'20", "journal-ref": null, "doi": "10.1145/3313831.3376210", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first systematic analysis of personality dimensions developed\nspecifically to describe the personality of speech-based conversational agents.\nFollowing the psycholexical approach from psychology, we first report on a new\nmulti-method approach to collect potentially descriptive adjectives from 1) a\nfree description task in an online survey (228 unique descriptors), 2) an\ninteraction task in the lab (176 unique descriptors), and 3) a text analysis of\n30,000 online reviews of conversational agents (Alexa, Google Assistant,\nCortana) (383 unique descriptors). We aggregate the results into a set of 349\nadjectives, which are then rated by 744 people in an online survey. A factor\nanalysis reveals that the commonly used Big Five model for human personality\ndoes not adequately describe agent personality. As an initial step to\ndeveloping a personality model, we propose alternative dimensions and discuss\nimplications for the design of agent personalities, personality-aware\npersonalisation, and future research.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 10:19:14 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["V\u00f6lkel", "Sarah Theres", ""], ["Sch\u00f6del", "Ramona", ""], ["Buschek", "Daniel", ""], ["Stachl", "Clemens", ""], ["Winterhalter", "Verena", ""], ["B\u00fchner", "Markus", ""], ["Hussmann", "Heinrich", ""]]}, {"id": "2003.06318", "submitter": "Andreas Komninos Dr", "authors": "Andreas Komninos, Emma Nicol and Mark Dunlop", "title": "Investigating Error Injection to Enhance the Effectiveness of Mobile\n  Text Entry Studies of Error Behaviour", "comments": "Work originally conducted in between Sept 2013 - Jan 2016, document\n  prepared Feb 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  During lab studies of text entry methods it is typical to observer very few\nerrors in participants' typing - users tend to type very carefully in labs.\nThis is a problem when investigating methods to support error awareness or\ncorrection as support mechanisms are not tested. We designed a novel evaluation\nmethod based around injection of errors into the users' typing stream and\nreport two user studies on the effectiveness of this technique. Injection\nallowed us to observe a larger number of instances and more diverse types of\nerror correction behaviour than would normally be possible in a single study,\nwithout having a significant impact on key input behaviour characteristics.\nQualitative feedback from both studies suggests that our injection algorithm\nwas successful in creating errors that appeared realistic to participants. The\nuse of error injection shows promise for the investigation of error correction\nbehaviour in text entry studies.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 14:28:35 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Komninos", "Andreas", ""], ["Nicol", "Emma", ""], ["Dunlop", "Mark", ""]]}, {"id": "2003.06323", "submitter": "Andreas Komninos Dr", "authors": "Andreas Komninos, Kyriakos Katsaris, Emma Nicol, Mark Dunlop, John\n  Garofalakis", "title": "Mobile Text Entry Behaviour in Lab and In-the-Wild studies: Is it\n  different?", "comments": "Work carried out Oct 2016-Jun 2017, manuscript produced in June 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text entry in smartphones remains a critical element of mobile HCI. It has\nbeen widely studied in lab settings, using primarily transcription tasks, and\nto a far lesser extent through in-the-wild (field) experiments. So far it\nremains unknown how well user behaviour during lab transcription tasks\napproximates real use. In this paper, we present a study that provides evidence\nthat lab text entry behaviour is clearly distinguishable from real world use.\nUsing machine learning techniques, we show that it is possible to accurately\nidentify the type of study in which text entry sessions took place. The\nimplications of our findings relate to the design of future studies in text\nentry, aiming to support input with virtual smartphone keyboards.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 14:39:51 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Komninos", "Andreas", ""], ["Katsaris", "Kyriakos", ""], ["Nicol", "Emma", ""], ["Dunlop", "Mark", ""], ["Garofalakis", "John", ""]]}, {"id": "2003.06461", "submitter": "Jessie Smith", "authors": "Jessie Smith, Nasim Sonboli, Casey Fiesler, Robin Burke", "title": "Exploring User Opinions of Fairness in Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Algorithmic fairness for artificial intelligence has become increasingly\nrelevant as these systems become more pervasive in society. One realm of AI,\nrecommender systems, presents unique challenges for fairness due to trade offs\nbetween optimizing accuracy for users and fairness to providers. But what is\nfair in the context of recommendation--particularly when there are multiple\nstakeholders? In an initial exploration of this problem, we ask users what\ntheir ideas of fair treatment in recommendation might be, and why. We analyze\nwhat might cause discrepancies or changes between user's opinions towards\nfairness to eventually help inform the design of fairer and more transparent\nrecommendation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 19:44:26 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 20:19:42 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Smith", "Jessie", ""], ["Sonboli", "Nasim", ""], ["Fiesler", "Casey", ""], ["Burke", "Robin", ""]]}, {"id": "2003.06495", "submitter": "Maegan Tucker", "authors": "Maegan Tucker, Myra Cheng, Ellen Novoseller, Richard Cheng, Yisong\n  Yue, Joel W. Burdick, and Aaron D. Ames", "title": "Human Preference-Based Learning for High-dimensional Optimization of\n  Exoskeleton Walking Gaits", "comments": "8 pages, 9 figures, 2 tables, to appear at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing lower-body exoskeleton walking gaits for user comfort requires\nunderstanding users' preferences over a high-dimensional gait parameter space.\nHowever, existing preference-based learning methods have only explored\nlow-dimensional domains due to computational limitations. To learn user\npreferences in high dimensions, this work presents LineCoSpar, a\nhuman-in-the-loop preference-based framework that enables optimization over\nmany parameters by iteratively exploring one-dimensional subspaces.\nAdditionally, this work identifies gait attributes that characterize broader\npreferences across users. In simulations and human trials, we empirically\nverify that LineCoSpar is a sample-efficient approach for high-dimensional\npreference optimization. Our analysis of the experimental data reveals a\ncorrespondence between human preferences and objective measures of dynamicity,\nwhile also highlighting differences in the utility functions underlying\nindividual users' gait preferences. This result has implications for\nexoskeleton gait synthesis, an active field with applications to clinical use\nand patient rehabilitation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 22:02:15 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 16:56:22 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Tucker", "Maegan", ""], ["Cheng", "Myra", ""], ["Novoseller", "Ellen", ""], ["Cheng", "Richard", ""], ["Yue", "Yisong", ""], ["Burdick", "Joel W.", ""], ["Ames", "Aaron D.", ""]]}, {"id": "2003.06659", "submitter": "Thomas Kerdreux", "authors": "Thomas Kerdreux and Louis Thiry and Erwan Kerdreux", "title": "Interactive Neural Style Transfer with Artists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present interactive painting processes in which a painter and various\nneural style transfer algorithms interact on a real canvas. Understanding what\nthese algorithms' outputs achieve is then paramount to describe the creative\nagency in our interactive experiments. We gather a set of paired\npainting-pictures images and present a new evaluation methodology based on the\npredictivity of neural style transfer algorithms. We point some algorithms'\ninstabilities and show that they can be used to enlarge the diversity and\npleasing oddity of the images synthesized by the numerous existing neural style\ntransfer algorithms. This diversity of images was perceived as a source of\ninspiration for human painters, portraying the machine as a computational\ncatalyst.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 15:27:44 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Kerdreux", "Thomas", ""], ["Thiry", "Louis", ""], ["Kerdreux", "Erwan", ""]]}, {"id": "2003.06692", "submitter": "Trisha Mittal", "authors": "Trisha Mittal, Pooja Guhan, Uttaran Bhattacharya, Rohan Chandra,\n  Aniket Bera and Dinesh Manocha", "title": "EmotiCon: Context-Aware Multimodal Emotion Recognition using Frege's\n  Principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present EmotiCon, a learning-based algorithm for context-aware perceived\nhuman emotion recognition from videos and images. Motivated by Frege's Context\nPrinciple from psychology, our approach combines three interpretations of\ncontext for emotion recognition. Our first interpretation is based on using\nmultiple modalities(e.g. faces and gaits) for emotion recognition. For the\nsecond interpretation, we gather semantic context from the input image and use\na self-attention-based CNN to encode this information. Finally, we use depth\nmaps to model the third interpretation related to socio-dynamic interactions\nand proximity among agents. We demonstrate the efficiency of our network\nthrough experiments on EMOTIC, a benchmark dataset. We report an Average\nPrecision (AP) score of 35.48 across 26 classes, which is an improvement of 7-8\nover prior methods. We also introduce a new dataset, GroupWalk, which is a\ncollection of videos captured in multiple real-world settings of people\nwalking. We report an AP of 65.83 across 4 categories on GroupWalk, which is\nalso an improvement over prior methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 19:55:21 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Mittal", "Trisha", ""], ["Guhan", "Pooja", ""], ["Bhattacharya", "Uttaran", ""], ["Chandra", "Rohan", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2003.06875", "submitter": "Dong Wei", "authors": "Dong Wei, Senjuti Basu Roy, Sihem Amer-Yahia", "title": "Recommending Deployment Strategies for Collaborative Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work contributes to aiding requesters in deploying collaborative tasks in\ncrowdsourcing. We initiate the study of recommending deployment strategies for\ncollaborative tasks to requesters that are consistent with deployment\nparameters they desire: a lower-bound on the quality of the crowd contribution,\nan upper-bound on the latency of task completion, and an upper-bound on the\ncost incurred by paying workers. A deployment strategy is a choice of value for\nthree dimensions: Structure (whether to solicit the workforce sequentially or\nsimultaneously), Organization (to organize it collaboratively or\nindependently), and Style (to rely solely on the crowd or to combine it with\nmachine algorithms). We propose StratRec, an optimization-driven middle layer\nthat recommends deployment strategies and alternative deployment parameters to\nrequesters by accounting for worker availability. Our solutions are grounded in\ndiscrete optimization and computational geometry techniques that produce\nresults with theoretical guarantees. We present extensive experiments on Amazon\nMechanical Turk and conduct synthetic experiments to validate the qualitative\nand scalability aspects of StratRec.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 17:36:55 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wei", "Dong", ""], ["Roy", "Senjuti Basu", ""], ["Amer-Yahia", "Sihem", ""]]}, {"id": "2003.06920", "submitter": "Boris Ruf", "authors": "Boris Ruf, Chaouki Boutharouite, Marcin Detyniecki", "title": "Getting Fairness Right: Towards a Toolbox for Practitioners", "comments": "Accepted at the Workshop on Fair and Responsible AI at CHI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential risk of AI systems unintentionally embedding and reproducing\nbias has attracted the attention of machine learning practitioners and society\nat large. As policy makers are willing to set the standards of algorithms and\nAI techniques, the issue on how to refine existing regulation, in order to\nenforce that decisions made by automated systems are fair and\nnon-discriminatory, is again critical. Meanwhile, researchers have demonstrated\nthat the various existing metrics for fairness are statistically mutually\nexclusive and the right choice mostly depends on the use case and the\ndefinition of fairness.\n  Recognizing that the solutions for implementing fair AI are not purely\nmathematical but require the commitments of the stakeholders to define the\ndesired nature of fairness, this paper proposes to draft a toolbox which helps\npractitioners to ensure fair AI practices. Based on the nature of the\napplication and the available training data, but also on legal requirements and\nethical, philosophical and cultural dimensions, the toolbox aims to identify\nthe most appropriate fairness objective. This approach attempts to structure\nthe complex landscape of fairness metrics and, therefore, makes the different\navailable options more accessible to non-technical people. In the proven\nabsence of a silver bullet solution for fair AI, this toolbox intends to\nproduce the fairest AI systems possible with respect to their local context.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 20:53:50 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ruf", "Boris", ""], ["Boutharouite", "Chaouki", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "2003.07370", "submitter": "Vivian Lai", "authors": "Vivian Lai, Samuel Carton, Chenhao Tan", "title": "Harnessing Explanations to Bridge AI and Humans", "comments": "4 pages, CHI 2020 Fair & Responsible AI Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are increasingly integrated into societally critical\napplications such as recidivism prediction and medical diagnosis, thanks to\ntheir superior predictive power. In these applications, however, full\nautomation is often not desired due to ethical and legal concerns. The research\ncommunity has thus ventured into developing interpretable methods that explain\nmachine predictions. While these explanations are meant to assist humans in\nunderstanding machine predictions and thereby allowing humans to make better\ndecisions, this hypothesis is not supported in many recent studies. To improve\nhuman decision-making with AI assistance, we propose future directions for\nclosing the gap between the efficacy of explanations and improvement in human\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 18:00:02 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Lai", "Vivian", ""], ["Carton", "Samuel", ""], ["Tan", "Chenhao", ""]]}, {"id": "2003.07434", "submitter": "Kayhan Ghafoor", "authors": "Halgurd S. Maghdid and Kayhan Zrar Ghafoor and Ali Safaa Sadiq and\n  Kevin Curran and Danda B. Rawat and Khaled Rabie", "title": "A Novel AI-enabled Framework to Diagnose Coronavirus COVID 19 using\n  Smartphone Embedded Sensors: Design Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronaviruses are a famous family of viruses that cause illness in both\nhumans and animals. The new type of coronavirus COVID-19 was firstly discovered\nin Wuhan, China. However, recently, the virus has widely spread in most of the\nworld and causing a pandemic according to the World Health Organization (WHO).\nFurther, nowadays, all the world countries are striving to control the\nCOVID-19. There are many mechanisms to detect coronavirus including clinical\nanalysis of chest CT scan images and blood test results. The confirmed COVID-19\npatient manifests as fever, tiredness, and dry cough. Particularly, several\ntechniques can be used to detect the initial results of the virus such as\nmedical detection Kits. However, such devices are incurring huge cost, taking\ntime to install them and use. Therefore, in this paper, a new framework is\nproposed to detect COVID-19 using built-in smartphone sensors. The proposal\nprovides a low-cost solution, since most of radiologists have already held\nsmartphones for different daily-purposes. Not only that but also ordinary\npeople can use the framework on their smartphones for the virus detection\npurposes. Nowadays Smartphones are powerful with existing computation-rich\nprocessors, memory space, and large number of sensors including cameras,\nmicrophone, temperature sensor, inertial sensors, proximity, colour-sensor,\nhumidity-sensor, and wireless chipsets/sensors. The designed Artificial\nIntelligence (AI) enabled framework reads the smartphone sensors signal\nmeasurements to predict the grade of severity of the pneumonia as well as\npredicting the result of the disease.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 20:38:13 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 04:27:50 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Maghdid", "Halgurd S.", ""], ["Ghafoor", "Kayhan Zrar", ""], ["Sadiq", "Ali Safaa", ""], ["Curran", "Kevin", ""], ["Rawat", "Danda B.", ""], ["Rabie", "Khaled", ""]]}, {"id": "2003.07492", "submitter": "Mohammad Arif Ul Alam", "authors": "Mohammad Arif Ul Alam, Nirmalya Roy, Sarah Holmes, Aryya Gangopadhyay,\n  Elizabeth Galik", "title": "AutoCogniSys: IoT Assisted Context-Aware Automatic Cognitive Health\n  Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cognitive impairment has become epidemic in older adult population. The\nrecent advent of tiny wearable and ambient devices, a.k.a Internet of Things\n(IoT) provides ample platforms for continuous functional and cognitive health\nassessment of older adults. In this paper, we design, implement and evaluate\nAutoCogniSys, a context-aware automated cognitive health assessment system,\ncombining the sensing powers of wearable physiological (Electrodermal Activity,\nPhotoplethysmography) and physical (Accelerometer, Object) sensors in\nconjunction with ambient sensors. We design appropriate signal processing and\nmachine learning techniques, and develop an automatic cognitive health\nassessment system in a natural older adults living environment. We validate our\napproaches using two datasets: (i) a naturalistic sensor data streams related\nto Activities of Daily Living and mental arousal of 22 older adults recruited\nin a retirement community center, individually living in their own apartments\nusing a customized inexpensive IoT system (IRB #HP-00064387) and (ii) a\npublicly available dataset for emotion detection. The performance of\nAutoCogniSys attests max. 93\\% of accuracy in assessing cognitive health of\nolder adults.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 01:44:59 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Alam", "Mohammad Arif Ul", ""], ["Roy", "Nirmalya", ""], ["Holmes", "Sarah", ""], ["Gangopadhyay", "Aryya", ""], ["Galik", "Elizabeth", ""]]}, {"id": "2003.07595", "submitter": "Dennis Assenmacher", "authors": "Lena Clever, Dennis Assenmacher, Kilian M\\\"uller, Moritz Vinzent\n  Seiler, Dennis M. Riehle, Mike Preuss, Christian Grimme", "title": "FakeYou! -- A Gamified Approach for Building and Evaluating Resilience\n  Against Fake News", "comments": "accepted for Disinformation in Open Online Media - 2nd\n  Multidisciplinary International Symposium, MISDOOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays fake news are heavily discussed in public and political debates.\nEven though the phenomenon of intended false information is rather old,\nmisinformation reaches a new level with the rise of the internet and\nparticipatory platforms. Due to Facebook and Co., purposeful false information\n- often called fake news - can be easily spread by everyone. Because of a high\ndata volatility and variety in content types (text, images,...) debunking of\nfake news is a complex challenge. This is especially true for automated\napproaches, which are prone to fail validating the veracity of the information.\nThis work focuses on an a gamified approach to strengthen the resilience of\nconsumers towards fake news. The game FakeYou motivates its players to\ncritically analyze headlines regarding their trustworthiness. Further, the game\nfollows a \"learning by doing strategy\": by generating own fake headlines, users\nshould experience the concepts of convincing fake headline formulations. We\nintroduce the game itself, as well as the underlying technical infrastructure.\nA first evaluation study shows, that users tend to use specific stylistic\ndevices to generate fake news. Further, the results indicate, that creating\ngood fakes and identifying correct headlines are challenging and hard to learn.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 09:24:22 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Clever", "Lena", ""], ["Assenmacher", "Dennis", ""], ["M\u00fcller", "Kilian", ""], ["Seiler", "Moritz Vinzent", ""], ["Riehle", "Dennis M.", ""], ["Preuss", "Mike", ""], ["Grimme", "Christian", ""]]}, {"id": "2003.07622", "submitter": "Stephan Wiefling", "authors": "Stephan Wiefling, Luigi Lo Iacono and Markus D\\\"urmuth", "title": "Is This Really You? An Empirical Study on Risk-Based Authentication\n  Applied in the Wild", "comments": "14 pages, 7 tables", "journal-ref": "34th IFIP TC-11 International Conference on Information Security\n  and Privacy Protection (IFIP SEC 2019). IFIP Advances in Information and\n  Communication Technology, vol. 562, pp. 134-148. Springer, Cham", "doi": "10.1007/978-3-030-22312-0_10", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk-based authentication (RBA) is an adaptive security measure to strengthen\npassword-based authentication. RBA monitors additional implicit features during\npassword entry such as device or geolocation information, and requests\nadditional authentication factors if a certain risk level is detected. RBA is\nrecommended by the NIST digital identity guidelines, is used by several large\nonline services, and offers protection against security risks such as password\ndatabase leaks, credential stuffing, insecure passwords and large-scale\nguessing attacks. Despite its relevance, the procedures used by\nRBA-instrumented online services are currently not disclosed. Consequently,\nthere is little scientific research about RBA, slowing down progress and deeper\nunderstanding, making it harder for end users to understand the security\nprovided by the services they use and trust, and hindering the widespread\nadoption of RBA.\n  In this paper, with a series of studies on eight popular online services, we\n(i) analyze which features and combinations/classifiers are used and are useful\nin practical instances, (ii) develop a framework and a methodology to measure\nRBA in the wild, and (iii) survey and discuss the differences in the user\ninterface for RBA. Following this, our work provides a first deeper\nunderstanding of practical RBA deployments and helps fostering further research\nin this direction.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 10:32:08 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Wiefling", "Stephan", ""], ["Iacono", "Luigi Lo", ""], ["D\u00fcrmuth", "Markus", ""]]}, {"id": "2003.08052", "submitter": "Yuan Shen", "authors": "Yuan Shen, Shanduojiao Jiang, Muhammad Rizky Wellyanto, and Ranjitha\n  Kumar", "title": "Can AI decrypt fashion jargon for you?", "comments": "5 pages, 6 figures, Accepted at workshop paper for AI4HCI at CHI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When people talk about fashion, they care about the underlying meaning of\nfashion concepts,e.g., style.For example, people ask questions like what\nfeatures make this dress smart.However, the product descriptions in today\nfashion websites are full of domain specific and low level words. It is not\nclear to people how exactly those low level descriptions can contribute to a\nstyle or any high level fashion concept. In this paper, we proposed a data\ndriven solution to address this concept understanding issues by leveraging a\nlarge number of existing product data on fashion sites. We first collected and\ncategorized 1546 fashion keywords into 5 different fashion categories. Then, we\ncollected a new fashion product dataset with 853,056 products in total.\nFinally, we trained a deep learning model that can explicitly predict and\nexplain high level fashion concepts in a product image with its low level and\ndomain specific fashion features.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 05:32:04 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Shen", "Yuan", ""], ["Jiang", "Shanduojiao", ""], ["Wellyanto", "Muhammad Rizky", ""], ["Kumar", "Ranjitha", ""]]}, {"id": "2003.08203", "submitter": "David Holtz", "authors": "David Holtz, Benjamin Carterette, Praveen Chandar, Zahra Nazari,\n  Henriette Cramer, Sinan Aral", "title": "The Engagement-Diversity Connection: Evidence from a Field Experiment on\n  Spotify", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It remains unknown whether personalized recommendations increase or decrease\nthe diversity of content people consume. We present results from a randomized\nfield experiment on Spotify testing the effect of personalized recommendations\non consumption diversity. In the experiment, both control and treatment users\nwere given podcast recommendations, with the sole aim of increasing podcast\nconsumption. Treatment users' recommendations were personalized based on their\nmusic listening history, whereas control users were recommended popular\npodcasts among users in their demographic group. We find that, on average, the\ntreatment increased podcast streams by 28.90%. However, the treatment also\ndecreased the average individual-level diversity of podcast streams by 11.51%,\nand increased the aggregate diversity of podcast streams by 5.96%, indicating\nthat personalized recommendations have the potential to create patterns of\nconsumption that are homogenous within and diverse across users, a pattern\nreflecting Balkanization. Our results provide evidence of an\n\"engagement-diversity trade-off\" when recommendations are optimized solely to\ndrive consumption: while personalized recommendations increase user engagement,\nthey also affect the diversity of consumed content. This shift in consumption\ndiversity can affect user retention and lifetime value, and impact the optimal\nstrategy for content producers. We also observe evidence that our treatment\naffected streams from sections of Spotify's app not directly affected by the\nexperiment, suggesting that exposure to personalized recommendations can affect\nthe content that users consume organically. We believe these findings highlight\nthe need for academics and practitioners to continue investing in\npersonalization methods that explicitly take into account the diversity of\ncontent recommended.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 16:49:59 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Holtz", "David", ""], ["Carterette", "Benjamin", ""], ["Chandar", "Praveen", ""], ["Nazari", "Zahra", ""], ["Cramer", "Henriette", ""], ["Aral", "Sinan", ""]]}, {"id": "2003.08433", "submitter": "George Amariucai", "authors": "Abhishek Jana, Md Kamruzzaman Sarker, Monireh Ebrahimi, Pascal\n  Hitzler, George T Amariucai", "title": "Neural Fuzzy Extractors: A Secure Way to Use Artificial Neural Networks\n  for Biometric User Authentication", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powered by new advances in sensor development and artificial intelligence,\nthe decreasing cost of computation, and the pervasiveness of handheld\ncomputation devices, biometric user authentication (and identification) is\nrapidly becoming ubiquitous. Modern approaches to biometric authentication,\nbased on sophisticated machine learning techniques, cannot avoid storing either\ntrained-classifier details or explicit user biometric data, thus exposing\nusers' credentials to falsification. In this paper, we introduce a secure way\nto handle user-specific information involved with the use of vector-space\nclassifiers or artificial neural networks for biometric authentication. Our\nproposed architecture, called a Neural Fuzzy Extractor (NFE), allows the\ncoupling of pre-existing classifiers with fuzzy extractors, through a\nartificial-neural-network-based buffer called an expander, with minimal or no\nperformance degradation. The NFE thus offers all the performance advantages of\nmodern deep-learning-based classifiers, and all the security of standard fuzzy\nextractors. We demonstrate the NFE retrofit to a classic artificial neural\nnetwork for a simple scenario of fingerprint-based user authentication.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 18:48:25 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Jana", "Abhishek", ""], ["Sarker", "Md Kamruzzaman", ""], ["Ebrahimi", "Monireh", ""], ["Hitzler", "Pascal", ""], ["Amariucai", "George T", ""]]}, {"id": "2003.08474", "submitter": "Karel Mundnich", "authors": "Karel Mundnich, Brandon M. Booth, Michelle L'Hommedieu, Tiantian Feng,\n  Benjamin Girault, Justin L'Hommedieu, Mackenzie Wildman, Sophia Skaaden,\n  Amrutha Nadarajan, Jennifer L. Villatte, Tiago H. Falk, Kristina Lerman,\n  Emilio Ferrara, and Shrikanth Narayanan", "title": "TILES-2018, a longitudinal physiologic and behavioral data set of\n  hospital workers", "comments": "57 pages, 9 figures, journal paper", "journal-ref": "Sci Data 7, 354 (2020)", "doi": "10.1038/s41597-020-00655-3", "report-no": null, "categories": "eess.SP cs.CY cs.HC stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a novel longitudinal multimodal corpus of physiological and\nbehavioral data collected from direct clinical providers in a hospital\nworkplace. We designed the study to investigate the use of off-the-shelf\nwearable and environmental sensors to understand individual-specific constructs\nsuch as job performance, interpersonal interaction, and well-being of hospital\nworkers over time in their natural day-to-day job settings. We collected\nbehavioral and physiological data from $n = 212$ participants through\nInternet-of-Things Bluetooth data hubs, wearable sensors (including a\nwristband, a biometrics-tracking garment, a smartphone, and an audio-feature\nrecorder), together with a battery of surveys to assess personality traits,\nbehavioral states, job performance, and well-being over time. Besides the\ndefault use of the data set, we envision several novel research opportunities\nand potential applications, including multi-modal and multi-task behavioral\nmodeling, authentication through biometrics, and privacy-aware and\nprivacy-preserving machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 21:07:16 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 19:09:17 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Mundnich", "Karel", ""], ["Booth", "Brandon M.", ""], ["L'Hommedieu", "Michelle", ""], ["Feng", "Tiantian", ""], ["Girault", "Benjamin", ""], ["L'Hommedieu", "Justin", ""], ["Wildman", "Mackenzie", ""], ["Skaaden", "Sophia", ""], ["Nadarajan", "Amrutha", ""], ["Villatte", "Jennifer L.", ""], ["Falk", "Tiago H.", ""], ["Lerman", "Kristina", ""], ["Ferrara", "Emilio", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2003.08499", "submitter": "Kaan Ak\\c{s}it", "authors": "Kaan Ak\\c{s}it, Jan Kautz, David Luebke", "title": "Gaze-Sensing LEDs for Head Mounted Displays", "comments": "14 pages, 7 figures. THIS WORK WAS CONDUCTED IN 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new gaze tracker for Head Mounted Displays (HMDs). We modify\ntwo off-the-shelf HMDs to be gaze-aware using Light Emitting Diodes (LEDs). Our\nkey contribution is to exploit the sensing capability of LEDs to create\nlow-power gaze tracker for virtual reality (VR) applications. This yields a\nsimple approach using minimal hardware to achieve good accuracy and low latency\nusing light-weight supervised Gaussian Process Regression (GPR) running on a\nmobile device. With our hardware, we show that Minkowski distance measure based\nGPR implementation outperforms the commonly used radial basis function-based\nsupport vector regression (SVR) without the need to precisely determine free\nparameters. We show that our gaze estimation method does not require complex\ndimension reduction techniques, feature extraction, or distortion corrections\ndue to off-axis optical paths. We demonstrate two complete HMD prototypes with\na sample eye-tracked application, and report on a series of subjective tests\nusing our prototypes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 23:03:06 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Ak\u015fit", "Kaan", ""], ["Kautz", "Jan", ""], ["Luebke", "David", ""]]}, {"id": "2003.08580", "submitter": "Nikita Samarin", "authors": "Nikita Samarin, Alisa Frik, Sean Brooks, Coye Cheshire, Serge Egelman", "title": "Surveying Vulnerable Populations: A Case Study of Civil Society\n  Organizations", "comments": "[v2] Appears in the Workshop on Inclusive Privacy and Security (WIPS)\n  co-located with Symposium on Usable Privacy and Security (SOUPS) 2020; [v1]\n  Appears in the Networked Privacy Workshop co-located with ACM Conference on\n  Human Factors in Computing Systems (CHI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to organizations in other sectors, civil society organizations\n(CSOs) are particularly vulnerable to security and privacy threats, as they\nlack adequate resources and expertise to defend themselves. At the same time,\ntheir security needs and practices have not gained much attention among\nresearchers, and existing solutions designed for the average users do not\nconsider the contexts in which CSO employees operate. As part of our\npreliminary work, we conducted an anonymous online survey with 102 CSO\nemployees to collect information about their perceived risks of different\nsecurity and privacy threats, and their self-reported mitigation strategies.\nThe design of our preliminary survey accounted for the unique requirements of\nour target population by establishing trust with respondents, using\nanonymity-preserving incentive strategies, and distributing the survey with the\nhelp of a trusted intermediary. However, by carefully examining our methods and\nthe feedback received from respondents, we uncovered several issues with our\nmethodology, including the length of the survey, the framing of the questions,\nand the design of the recruitment email. We hope that the discussion presented\nin this paper will inform and assist researchers and practitioners working on\nunderstanding and improving the security and privacy of CSOs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 05:30:21 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 21:01:40 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Samarin", "Nikita", ""], ["Frik", "Alisa", ""], ["Brooks", "Sean", ""], ["Cheshire", "Coye", ""], ["Egelman", "Serge", ""]]}, {"id": "2003.08753", "submitter": "Al Amin Hosain", "authors": "Al Amin Hosain, Panneer Selvam Santhalingam, Parth Pathak, Huzefa\n  Rangwala and Jana Kosecka", "title": "FineHand: Learning Hand Shapes for American Sign Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  American Sign Language recognition is a difficult gesture recognition\nproblem, characterized by fast, highly articulate gestures. These are comprised\nof arm movements with different hand shapes, facial expression and head\nmovements. Among these components, hand shape is the vital, often the most\ndiscriminative part of a gesture. In this work, we present an approach for\neffective learning of hand shape embeddings, which are discriminative for ASL\ngestures. For hand shape recognition our method uses a mix of manually labelled\nhand shapes and high confidence predictions to train deep convolutional neural\nnetwork (CNN). The sequential gesture component is captured by recursive neural\nnetwork (RNN) trained on the embeddings learned in the first stage. We will\ndemonstrate that higher quality hand shape models can significantly improve the\naccuracy of final video gesture classification in challenging conditions with\nvariety of speakers, different illumination and significant motion blurr. We\ncompare our model to alternative approaches exploiting different modalities and\nrepresentations of the data and show improved video gesture recognition\naccuracy on GMU-ASL51 benchmark dataset\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 23:32:08 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Hosain", "Al Amin", ""], ["Santhalingam", "Panneer Selvam", ""], ["Pathak", "Parth", ""], ["Rangwala", "Huzefa", ""], ["Kosecka", "Jana", ""]]}, {"id": "2003.08806", "submitter": "Zhengyang Wu", "authors": "Zhengyang Wu, Srivignesh Rajendran, Tarrence van As, Joelle\n  Zimmermann, Vijay Badrinarayanan, Andrew Rabinovich", "title": "MagicEyes: A Large Scale Eye Gaze Estimation Dataset for Mixed Reality", "comments": "arXiv admin note: substantial text overlap with arXiv:1908.09060", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of Virtual and Mixed Reality (XR) devices, eye tracking\nhas received significant attention in the computer vision community. Eye gaze\nestimation is a crucial component in XR -- enabling energy efficient rendering,\nmulti-focal displays, and effective interaction with content. In head-mounted\nXR devices, the eyes are imaged off-axis to avoid blocking the field of view.\nThis leads to increased challenges in inferring eye related quantities and\nsimultaneously provides an opportunity to develop accurate and robust learning\nbased approaches. To this end, we present MagicEyes, the first large scale eye\ndataset collected using real MR devices with comprehensive ground truth\nlabeling. MagicEyes includes $587$ subjects with $80,000$ images of\nhuman-labeled ground truth and over $800,000$ images with gaze target labels.\nWe evaluate several state-of-the-art methods on MagicEyes and also propose a\nnew multi-task EyeNet model designed for detecting the cornea, glints and pupil\nalong with eye segmentation in a single forward pass.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 08:23:57 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Wu", "Zhengyang", ""], ["Rajendran", "Srivignesh", ""], ["van As", "Tarrence", ""], ["Zimmermann", "Joelle", ""], ["Badrinarayanan", "Vijay", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "2003.08942", "submitter": "Christine Bauer", "authors": "Christine Bauer and Katharina Sophie Schmid and Christine Strauss", "title": "An Open Model for Researching the Role of Culture in Online\n  Self-Disclosure", "comments": "10 pages, 1 figure, 51st Hawaii International Conference on System\n  Sciences (HICSS 2018), Waikoloa, Big Island, HI, USA; nominated for best\n  paper award", "journal-ref": "Proceedings of the 51st Hawaii International Conference on System\n  Sciences (HICSS 2018), 3-6 January, Waikoloa, Big Island, HI, USA, pp\n  3637-3646", "doi": "10.24251/HICSS.2018.460", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of consumers' personal information (PI) is a significant source\nto learn about consumers. In online settings, many consumers disclose PI\nabundantly -- this is particularly true for information provided on social\nnetwork services. Still, people manage the privacy level they want to maintain\nby disclosing by disclosing PI accordingly. In addition, studies have shown\nthat consumers' online self-disclosure (OSD) differs across cultures.\nTherefore, intelligent systems should consider cultural issues when collecting,\nprocessing, storing or protecting data from consumers. However, existing\nstudies typically rely on a comparison of two cultures, providing valuable\ninsights but not drawing a comprehensive picture. We introduce an open research\nmodel for cultural OSD research, based on the privacy calculus theory. Our open\nresearch model incorporates six cultural dimensions, six predictors, and 24\nstructured propositions. It represents a comprehensive approach that provides a\nbasis to explain possible cultural OSD phenomena in a systematic way.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 16:57:34 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Bauer", "Christine", ""], ["Schmid", "Katharina Sophie", ""], ["Strauss", "Christine", ""]]}, {"id": "2003.08990", "submitter": "Kovila  P.L. Coopamootoo", "authors": "Kovila P.L. Coopamootoo", "title": "Dis-Empowerment Online: An Investigation of Privacy-Sharing Perceptions\n  & Method Preferences", "comments": "Kovila P.L. Coopamootoo, Dis-Empowerment Online - An Investigation of\n  Privacy-Sharing Perceptions & Method Preferences: Proceedings of AsiaUSEC'20,\n  Financial Cryptography and Data Security 2020 (FC). February 14, 2020 Kota\n  Kinabalu, Sabah, Malaysia Springer, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it is often claimed that users are empowered via online technologies,\nthere is also a general feeling of privacy dis-empowerment. We investigate the\nperception of privacy and sharing empowerment online, as well as the use of\nprivacy technologies, via a cross-national online study with N=907\nparticipants. We find that perception of privacy empowerment differs from that\nof sharing across dimensions of meaningfulness, competence and choice. We find\nsimilarities and differences in privacy method preference between the US, UK\nand Germany. We also find that non-technology methods of privacy protection are\namong the most preferred methods, while more advanced and standalone privacy\ntechnologies are least preferred.. By mapping the perception of privacy\ndis-empowerment into patterns of privacy behavior online, and clarifying the\nsimilarities and distinctions in privacy technology use, this paper provides an\nimportant foundation for future research and the design of privacy\ntechnologies. The findings may be used across disciplines to develop more\nuser-centric privacy technologies, that support and enable the user.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 19:17:55 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Coopamootoo", "Kovila P. L.", ""]]}, {"id": "2003.09061", "submitter": "Chen Wang", "authors": "Yilin Yang, Chen Wang, Yingying Chen and Yan Wang", "title": "EchoLock: Towards Low Effort Mobile User Identification", "comments": "This paper version is based on the USENIX Security '20 Summer\n  submission. Before this, there was a version for the Mobicom '19 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User identification plays a pivotal role in how we interact with our mobile\ndevices. Many existing authentication approaches require active input from the\nuser or specialized sensing hardware, and studies on mobile device usage show\nsignificant interest in less inconvenient procedures. In this paper, we propose\nEchoLock, a low effort identification scheme that validates the user by sensing\nhand geometry via commodity microphones and speakers. These acoustic signals\nproduce distinct structure-borne sound reflections when contacting the user's\nhand, which can be used to differentiate between different people based on how\nthey hold their mobile devices. We process these reflections to derive unique\nacoustic features in both the time and frequency domain, which can effectively\nrepresent physiological and behavioral traits, such as hand contours, finger\nsizes, holding strength, and gesture. Furthermore, learning-based algorithms\nare developed to robustly identify the user under various environments and\nconditions. We conduct extensive experiments with 20 participants using\ndifferent hardware setups in key use case scenarios and study various attack\nmodels to demonstrate the performance of our proposed system. Our results show\nthat EchoLock is capable of verifying users with over 90% accuracy, without\nrequiring any active input from the user.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 01:20:45 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 21:10:40 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Yang", "Yilin", ""], ["Wang", "Chen", ""], ["Chen", "Yingying", ""], ["Wang", "Yan", ""]]}, {"id": "2003.09083", "submitter": "Chen Wang", "authors": "Chen Wang, Cong Shi, Yingying Chen, Yan Wang and Nitesh Saxena", "title": "WearID: Wearable-Assisted Low-Effort Authentication to Voice Assistants\n  using Cross-Domain Speech Similarity", "comments": "This version is the CCS '19 submission. There are three prior\n  versions including Mobicom '18, CCS '18 and NDSS '19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the open nature of voice input, voice assistant (VA) systems (e.g.,\nGoogle Home and Amazon Alexa) are under a high risk of sensitive information\nleakage (e.g., personal schedules and shopping accounts). Though the existing\nVA systems may employ voice features to identify users, they are still\nvulnerable to various acoustic attacks (e.g., impersonation, replay and hidden\ncommand attacks). In this work, we focus on the security issues of the emerging\nVA systems and aim to protect the users' highly sensitive information from\nthese attacks. Towards this end, we propose a system, WearID, which uses an\noff-the-shelf wearable device (e.g., a smartwatch or bracelet) as a secure\ntoken to verify the user's voice commands to the VA system. In particular,\nWearID exploits the readily available motion sensors from most wearables to\ndescribe the command sound in vibration domain and check the received command\nsound across two domains (i.e., wearable's motion sensor vs. VA device's\nmicrophone) to ensure the sound is from the legitimate user.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 03:01:10 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 21:25:05 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Wang", "Chen", ""], ["Shi", "Cong", ""], ["Chen", "Yingying", ""], ["Wang", "Yan", ""], ["Saxena", "Nitesh", ""]]}, {"id": "2003.09096", "submitter": "Chen Wang", "authors": "Chen Wang, Zhenzhe Lin, Yucheng Xie, Xiaonan Guo, Yanzhi Ren and\n  Yingying Chen", "title": "WiEat: Fine-grained Device-free Eating Monitoring Leveraging Wi-Fi\n  Signals", "comments": "This version was submitted to Infocom '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eating is a fundamental activity in people's daily life. Studies have shown\nthat many health-related problems such as obesity, diabetes and anemia are\nclosely associated with people's unhealthy eating habits (e.g., skipping meals,\neating irregularly and overeating). Traditional eating monitoring solutions\nrelying on self-reports remain an onerous task, while the recent trend\nrequiring users to wear expensive dedicated hardware is still invasive. To\novercome these limitations, in this paper, we develop a device-free eating\nmonitoring system using WiFi-enabled devices (e.g., smartphone or laptop). Our\nsystem aims to automatically monitor users' eating activities through\nidentifying the fine-grained eating motions and detecting the chewing and\nswallowing. In particular, our system extracts the fine-grained Channel State\nInformation (CSI) from WiFi signals to distinguish eating from non-eating\nactivities and further recognizing users' detailed eating motions with\ndifferent utensils (e.g., using a folk, knife, spoon or bare hands). Moreover,\nthe system has the capability of identifying chewing and swallowing through\ndetecting users' minute facial muscle movements based on the derived CSI\nspectrogram. Such fine-grained eating monitoring results are beneficial to the\nunderstanding of the user's eating behaviors and can be used to estimate food\nintake types and amounts. Extensive experiments with 20 users over 1600-minute\neating show that the proposed system can recognize the user's eating motions\nwith up to 95% accuracy and estimate the chewing and swallowing amount with 10%\npercentage error.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 04:22:53 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 21:20:11 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Wang", "Chen", ""], ["Lin", "Zhenzhe", ""], ["Xie", "Yucheng", ""], ["Guo", "Xiaonan", ""], ["Ren", "Yanzhi", ""], ["Chen", "Yingying", ""]]}, {"id": "2003.09100", "submitter": "Simon Perrault", "authors": "Christopher L. Asplund, Takashi Obana, Parag Bhatnagar, Xun Quan Koh,\n  Simon T. Perrault", "title": "It's All in the Timing: Principles of Transient Distraction Illustrated\n  with Vibrotactile Tasks", "comments": "29 pages", "journal-ref": null, "doi": "10.1145/3386358", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vibration is an efficient way of conveying information from a device to its\nuser, and it is increasingly used for wrist or finger-worn devices such as\nsmart rings. Unexpected vibrations or sounds from the environment may disrupt\nthe perception of such information. Although disruptive effects have been\nsystematically explored in vision and audition, they have been less examined in\nthe haptic domain. Here we briefly review the relevant literature from HCI and\npsychology, distilling principles of when distraction is likely. We then\ninvestigate these principles through four experiments, examining how the timing\nand modality of relatively rare or unexpected stimuli (surprise distractors)\naffects the detection and recognition of vibrotactile target patterns. At short\ndistractor-target delays (< 350 ms), both auditory and vibrotactile surprise\ndistractors impaired performance. At a longer delay (1050 ms), performance was\nnot affected overall, even being improved with repeated exposure to the\nvibrotactile distractors. We discuss the importance of our findings in the\ncontext of HCI and cognitive psychology, and we provide design guidelines for\nmitigating the effects of distraction on haptic devices.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 04:39:42 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Asplund", "Christopher L.", ""], ["Obana", "Takashi", ""], ["Bhatnagar", "Parag", ""], ["Koh", "Xun Quan", ""], ["Perrault", "Simon T.", ""]]}, {"id": "2003.09169", "submitter": "Evgeny Stemasov", "authors": "Evgeny Stemasov, Tobias Wagner, Jan Gugenheimer, Enrico Rukzio", "title": "Mix&Match: Towards Omitting Modelling Through In-Situ Alteration and\n  Remixing of Model Repository Artifacts in Mixed Reality", "comments": "12 pages, 15 figures, 1 table, To appear in the Proceedings of the\n  ACM Conference on Human Factors in Computing Systems 2020 (CHI'20)", "journal-ref": "In Proceedings of the 2020 CHI Conference on Human Factors in\n  Computing Systems (CHI '20). Association for Computing Machinery, New York,\n  NY, USA, Paper 710, 1-12", "doi": "10.1145/3313831.3376839", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accessibility of tools to model artifacts is one of the core driving\nfactors for the adoption of Personal Fabrication. Subsequently, model\nrepositories like Thingiverse became important tools in (novice) makers'\nprocesses. They allow them to shorten or even omit the design process,\noffloading a majority of the effort to other parties. However, steps like\nmeasurement of surrounding constraints (e.g., clearance) which exist only\ninside the users' environment, can not be similarly outsourced. We propose\nMix&Match a mixed-reality-based system which allows users to browse model\nrepositories, preview the models in-situ, and adapt them to their environment\nin a simple and immediate fashion. Mix&Match aims to provide users with CSG\noperations which can be based on both virtual and real geometry. We present\ninteraction patterns and scenarios for Mix&Match, arguing for the combination\nof mixed reality and model repositories. This enables almost modelling-free\npersonal fabrication for both novices and expert makers.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 10:03:51 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Stemasov", "Evgeny", ""], ["Wagner", "Tobias", ""], ["Gugenheimer", "Jan", ""], ["Rukzio", "Enrico", ""]]}, {"id": "2003.09312", "submitter": "Nitish Nag", "authors": "Nitish Nag", "title": "Health State Estimation", "comments": "Ph.D. Dissertation @ University of California, Irvine", "journal-ref": "Proquest 2020, 307 pages", "doi": null, "report-no": "27743502", "categories": "cs.AI cs.CY cs.HC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Life's most valuable asset is health. Continuously understanding the state of\nour health and modeling how it evolves is essential if we wish to improve it.\nGiven the opportunity that people live with more data about their life today\nthan any other time in history, the challenge rests in interweaving this data\nwith the growing body of knowledge to compute and model the health state of an\nindividual continually. This dissertation presents an approach to build a\npersonal model and dynamically estimate the health state of an individual by\nfusing multi-modal data and domain knowledge. The system is stitched together\nfrom four essential abstraction elements: 1. the events in our life, 2. the\nlayers of our biological systems (from molecular to an organism), 3. the\nfunctional utilities that arise from biological underpinnings, and 4. how we\ninteract with these utilities in the reality of daily life. Connecting these\nfour elements via graph network blocks forms the backbone by which we\ninstantiate a digital twin of an individual. Edges and nodes in this graph\nstructure are then regularly updated with learning techniques as data is\ncontinuously digested. Experiments demonstrate the use of dense and\nheterogeneous real-world data from a variety of personal and environmental\nsensors to monitor individual cardiovascular health state. State estimation and\nindividual modeling is the fundamental basis to depart from disease-oriented\napproaches to a total health continuum paradigm. Precision in predicting health\nrequires understanding state trajectory. By encasing this estimation within a\nnavigational approach, a systematic guidance framework can plan actions to\ntransition a current state towards a desired one. This work concludes by\npresenting this framework of combining the health state and personal graph\nmodel to perpetually plan and assist us in living life towards our goals.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 21:06:32 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Nag", "Nitish", ""]]}, {"id": "2003.09333", "submitter": "Jeremy Frey", "authors": "J\\'er\\'emy Frey (IDC), Gilad Ostrin (BGU, IDC), May Grabli (IDC),\n  Jessica Cauchard (BGU, IDC)", "title": "Physiologically Driven Storytelling: Concept and Software Tool", "comments": "CHI '20 - SIGCHI Conference on Human Factors in Computing System, Apr\n  2020, Honolulu, United States", "journal-ref": null, "doi": "10.1145/3313831.3376643", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We put forth Physiologically Driven Storytelling, a new approach to\ninteractive storytelling where narratives adaptively unfold based on the\nreader's physiological state. We first describe a taxonomy framing how\nphysiological signals can be used to drive interactive systems both as input\nand output. We then propose applications to interactive storytelling and\ndescribe the implementation of a software tool to create Physiological\nInteractive Fiction (PIF). The results of an online study (N=140) provided\nguidelines towards augmenting the reading experience. PIF was then evaluated in\na lab study (N=14) to determine how physiological signals can be used to infer\na reader's state. Our results show that breathing, electrodermal activity, and\neye tracking can help differentiate positive from negative tones, and\nmonotonous from exciting events. This work demonstrates how PIF can support\nstorytelling in creating engaging content and experience tailored to the\nreader. Moreover, it opens the space to future physiologically driven systems\nwithin broader application areas.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 15:37:23 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Frey", "J\u00e9r\u00e9my", "", "IDC"], ["Ostrin", "Gilad", "", "BGU, IDC"], ["Grabli", "May", "", "IDC"], ["Cauchard", "Jessica", "", "BGU, IDC"]]}, {"id": "2003.09494", "submitter": "Mojtaba Noghabaei", "authors": "Mojtaba Noghabaei, and Kevin Han", "title": "Hazard recognition in an immersive virtual environment: Framework for\n  the simultaneous analysis of visual search and EEG patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanaged hazards in dangerous construction environments proved to be one of\nthe main sources of injuries and accidents. Hazard recognition is crucial to\nachieve effective safety management and reduce injuries and fatalities in\nhazardous job sites. Still, there has been lack of effort that can efficiently\nassist workers in improving their hazard recognition skills. This study\npresents virtual safety training in an Immersive Virtual Environment (IVE) to\nenhance worker's hazard recognition skills. A worker wearing a Virtual Reality\n(VR) device, that is equipped with an eye-tracker, virtually recognizes hazards\non simulated construction sites while a brainwave-sensing device records brain\nactivities. This platform can analyze the overall performance of the workers in\na visual hazard recognition task and identify hazards that need additional\nintervention for each worker. This study provides novel insights on how a\nworker's brain and eye act simultaneously during a visual hazard recognition\nprocess. The presented method can take current safety training programs into\nanother level by providing personalized feedback to the workers.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 20:12:38 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Noghabaei", "Mojtaba", ""], ["Han", "Kevin", ""]]}, {"id": "2003.09648", "submitter": "Samuel Gomes", "authors": "Samuel Gomes, Tom\\'as Alves, Jo\\~ao Dias, Carlos Martinho", "title": "Reward-Mediated Individual and Altruistic Behavior", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has taken particular interest in observing the dynamics\nbetween altruistic and individual behavior. This is a commonly approached\nproblem when reasoning about social dilemmas, which have a plethora of real\nworld counterparts in the fields of education, health and economics. Weighing\nhow incentives influence in-game behavior, our study examines individual and\naltruistic interactions, by analyzing the players' strategies and interaction\nmotives when facing different reward attribution strategies. Consequently, a\nmodel for interaction motives is also proposed, with the premise that the\nmotives for interactions can be defined as a continuous space, ranging from\nself-oriented (associated to self-improvement behaviors) to others-oriented\n(associated to extreme altruism behaviors) motives. To evaluate the promotion\nof individual and altruistic behavior, we leverage Message Across, an in-loco\ntwo-player videogame with adaptable reward attribution systems. We conducted\nseveral user tests (N = 66) to verify to what extent individual and altruistic\nreward attribution systems led players to vary their strategies and motives\norientation. Our results indicate that players' strategies and self-reported\norientation of interaction motives varied highly significantly upon the\ndeployment of individual and altruistic reward systems, which leads us to\nbelieve on the suitability of applying an incentive-based strategy to moderate\nthe emergence of individual and altruistic behavior in games.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 12:57:32 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 16:58:48 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Gomes", "Samuel", ""], ["Alves", "Tom\u00e1s", ""], ["Dias", "Jo\u00e3o", ""], ["Martinho", "Carlos", ""]]}, {"id": "2003.09670", "submitter": "Zitao Liu", "authors": "Hang Li, Wenbiao Ding, Zitao Liu", "title": "Identifying At-Risk K-12 Students in Multimodal Online Environments: A\n  Machine Learning Approach", "comments": "The 13th International Conference on Educational Data Mining (EDM\n  2020), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid emergence of K-12 online learning platforms, a new era of\neducation has been opened up. It is crucial to have a dropout warning framework\nto preemptively identify K-12 students who are at risk of dropping out of the\nonline courses. Prior researchers have focused on predicting dropout in Massive\nOpen Online Courses (MOOCs), which often deliver higher education, i.e.,\ngraduate level courses at top institutions. However, few studies have focused\non developing a machine learning approach for students in K-12 online courses.\nIn this paper, we develop a machine learning framework to conduct accurate\nat-risk student identification specialized in K-12 multimodal online\nenvironments. Our approach considers both online and offline factors around\nK-12 students and aims at solving the challenges of (1) multiple modalities,\ni.e., K-12 online environments involve interactions from different modalities\nsuch as video, voice, etc; (2) length variability, i.e., students with\ndifferent lengths of learning history; (3) time sensitivity, i.e., the dropout\nlikelihood is changing with time; and (4) data imbalance, i.e., only less than\n20\\% of K-12 students will choose to drop out the class. We conduct a wide\nrange of offline and online experiments to demonstrate the effectiveness of our\napproach. In our offline experiments, we show that our method improves the\ndropout prediction performance when compared to state-of-the-art baselines on a\nreal-world educational dataset. In our online experiments, we test our approach\non a third-party K-12 online tutoring platform for two months and the results\nshow that more than 70\\% of dropout students are detected by the system.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 14:34:36 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 14:04:45 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Li", "Hang", ""], ["Ding", "Wenbiao", ""], ["Liu", "Zitao", ""]]}, {"id": "2003.09699", "submitter": "Ronny G. Guendel", "authors": "Moeness G. Amin, Arun Ravisankar and Ronny G. Guendel", "title": "RF Sensing for Continuous Monitoring of Human Activities for Home\n  Consumer Applications", "comments": "12 pages", "journal-ref": "Big Data: Learning, Analytics, and Applications, International\n  Society for Optics and Photonics. SPIE, vol. 10989, (2019), pp. 33-44", "doi": "10.1117/12.2519984", "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar for indoor monitoring is an emerging area of research and development,\ncovering and supporting different health and wellbeing applications of smart\nhomes, assisted living, and medical diagnosis. We report on a successful RF\nsensing system for home monitoring applications. The system recognizes\nActivities of Daily Living(ADL) and detects unique motion characteristics,\nusing data processing and training algorithms. We also examine the challenges\nof continuously monitoring various human activities which can be categorized\ninto translation motions (active mode) and in-place motions (resting mode). We\nuse the range-map, offered by a range-Doppler radar, to obtain the transition\ntime between these two categories, characterized by changing and constant range\nvalues, respectively. This is achieved using the Radon transform that\nidentifies straight lines of different slopes in the range-map image. Over the\nin-place motion time intervals, where activities have insignificant or\nnegligible range swath, power threshold of the radar return micro-Doppler\nsignatures,which is employed to define the time-spans of individual activities\nwith insignificant or negligible range swath. Finding both the transition times\nand the time-spans of the different motions leads to improved classifications,\nas it avoids decisions rendered over time windows covering mixed activities.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 16:52:26 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Amin", "Moeness G.", ""], ["Ravisankar", "Arun", ""], ["Guendel", "Ronny G.", ""]]}, {"id": "2003.09996", "submitter": "Lionel Robert", "authors": "Suresh Kumaar Jayaraman, Dawn M. Tilbury, X. Jessie Yang, Anuj K.\n  Pradhan, Lionel P. Robert Jr", "title": "Analysis and Prediction of Pedestrian Crosswalk Behavior during\n  Automated Vehicle Interactions", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For safe navigation around pedestrians, automated vehicles (AVs) need to plan\ntheir motion by accurately predicting pedestrians trajectories over long time\nhorizons. Current approaches to AV motion planning around crosswalks predict\nonly for short time horizons (1-2 s) and are based on data from pedestrian\ninteractions with human-driven vehicles (HDVs). In this paper, we develop a\nhybrid systems model that uses pedestrians gap acceptance behavior and constant\nvelocity dynamics for long-term pedestrian trajectory prediction when\ninteracting with AVs. Results demonstrate the applicability of the model for\nlong-term (> 5 s) pedestrian trajectory prediction at crosswalks. Further we\ncompared measures of pedestrian crossing behaviors in the immersive virtual\nenvironment (when interacting with AVs) to that in the real world (results of\npublished studies of pedestrians interacting with HDVs), and found similarities\nbetween the two. These similarities demonstrate the applicability of the hybrid\nmodel of AV interactions developed from an immersive virtual environment (IVE)\nfor real-world scenarios for both AVs and HDVs.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 21:28:39 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Jayaraman", "Suresh Kumaar", ""], ["Tilbury", "Dawn M.", ""], ["Yang", "X. Jessie", ""], ["Pradhan", "Anuj K.", ""], ["Robert", "Lionel P.", "Jr"]]}, {"id": "2003.09998", "submitter": "Lionel Robert", "authors": "Suresh Kumaar Jayaraman, Lionel P. Robert Jr., Xi Jessie Yang, Anuj K.\n  Pradhan, Dawn M. Tilbury", "title": "Efficient Behavior-aware Control of Automated Vehicles at Crosswalks\n  using Minimal Information Pedestrian Prediction Model", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For automated vehicles (AVs) to reliably navigate through crosswalks, they\nneed to understand pedestrians crossing behaviors. Simple and reliable\npedestrian behavior models aid in real-time AV control by allowing the AVs to\npredict future pedestrian behaviors. In this paper, we present a Behavior aware\nModel Predictive Controller (B-MPC) for AVs that incorporates long-term\npredictions of pedestrian crossing behavior using a previously developed\npedestrian crossing model. The model incorporates pedestrians gap acceptance\nbehavior and utilizes minimal pedestrian information, namely their position and\nspeed, to predict pedestrians crossing behaviors. The BMPC controller is\nvalidated through simulations and compared to a rule-based controller. By\nincorporating predictions of pedestrian behavior, the B-MPC controller is able\nto efficiently plan for longer horizons and handle a wider range of pedestrian\ninteraction scenarios than the rule-based controller. Results demonstrate the\napplicability of the controller for safe and efficient navigation at crossing\nscenarios.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 21:34:38 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Jayaraman", "Suresh Kumaar", ""], ["Robert", "Lionel P.", "Jr."], ["Yang", "Xi Jessie", ""], ["Pradhan", "Anuj K.", ""], ["Tilbury", "Dawn M.", ""]]}, {"id": "2003.10074", "submitter": "Yuan Lu", "authors": "Yuan Lu, Qiang Tang, Guiling Wang", "title": "Dragoon: Private Decentralized HITs Made Practical", "comments": "small differences from a version accepted to appear in ICDCS 2020 (to\n  fix a minor bug)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid popularity of blockchain, decentralized human intelligence\ntasks (HITs) are proposed to crowdsource human knowledge without relying on\nvulnerable third-party platforms. However, the inherent limits of blockchain\ncause decentralized HITs to face a few \"new\" challenges. For example, the\nconfidentiality of solicited data turns out to be the sine qua non, though it\nwas an arguably dispensable property in the centralized setting. To ensure the\n\"new\" requirement of data privacy, existing decentralized HITs use generic\nzero-knowledge proof frameworks (e.g. SNARK), but scarcely perform well in\npractice, due to the inherently expensive cost of generality.\n  We present a practical decentralized protocol for HITs, which also achieves\nthe fairness between requesters and workers. At the core of our contributions,\nwe avoid the powerful yet highly-costly generic zk-proof tools and propose a\nspecial-purpose scheme to prove the quality of encrypted data. By various\nnon-trivial statement reformations, proving the quality of encrypted data is\nreduced to efficient verifiable decryption, thus making decentralized HITs\npractical. Along the way, we rigorously define the ideal functionality of\ndecentralized HITs and then prove the security due to the ideal-real paradigm.\n  We further instantiate our protocol to implement a system called Dragoon, an\ninstance of which is deployed atop Ethereum to facilitate an image annotation\ntask used by ImageNet. Our evaluations demonstrate its practicality: the\non-chain handling cost of Dragoon is even less than the handling fee of\nAmazon's Mechanical Turk for the same ImageNet HIT.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 04:20:26 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 17:15:06 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 18:41:37 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Lu", "Yuan", ""], ["Tang", "Qiang", ""], ["Wang", "Guiling", ""]]}, {"id": "2003.10303", "submitter": "David Conal Higgins", "authors": "David Higgins and Vince I. Madai", "title": "From Bit To Bedside: A Practical Framework For Artificial Intelligence\n  Product Development In Healthcare", "comments": "30 pages, 4 figures", "journal-ref": "Advanced Intelligent Systems, 2020, 2000052", "doi": "10.1002/aisy.202000052", "report-no": null, "categories": "cs.CY cs.AI cs.HC stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Artificial Intelligence (AI) in healthcare holds great potential to expand\naccess to high-quality medical care, whilst reducing overall systemic costs.\nDespite hitting the headlines regularly and many publications of\nproofs-of-concept, certified products are failing to breakthrough to the\nclinic. AI in healthcare is a multi-party process with deep knowledge required\nin multiple individual domains. The lack of understanding of the specific\nchallenges in the domain is, therefore, the major contributor to the failure to\ndeliver on the big promises. Thus, we present a decision perspective framework,\nfor the development of AI-driven biomedical products, from conception to market\nlaunch. Our framework highlights the risks, objectives and key results which\nare typically required to proceed through a three-phase process to the market\nlaunch of a validated medical AI product. We focus on issues related to\nClinical validation, Regulatory affairs, Data strategy and Algorithmic\ndevelopment. The development process we propose for AI in healthcare software\nstrongly diverges from modern consumer software development processes. We\nhighlight the key time points to guide founders, investors and key stakeholders\nthroughout their relevant part of the process. Our framework should be seen as\na template for innovation frameworks, which can be used to coordinate team\ncommunications and responsibilities towards a reasonable product development\nroadmap, thus unlocking the potential of AI in medicine.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 14:42:18 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Higgins", "David", ""], ["Madai", "Vince I.", ""]]}, {"id": "2003.10365", "submitter": "Chris Michael", "authors": "Chris J. Michael, Dina Acklin, Jaelle Scheuerman", "title": "On Interactive Machine Learning and the Potential of Cognitive Feedback", "comments": "14 pages, 2 figures, submitted and accepted to the 2nd Workshop on\n  Deep Models and Artificial Intelligence for Defense Applications: Potentials,\n  Theories, Practices, Tools and Risks sponsored by the Association for the\n  Advancement of Artificial Intelligence in cooperation with the Stanford\n  University Computer Science Department", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to increase productivity, capability, and data exploitation,\nnumerous defense applications are experiencing an integration of\nstate-of-the-art machine learning and AI into their architectures. Especially\nfor defense applications, having a human analyst in the loop is of high\ninterest due to quality control, accountability, and complex subject matter\nexpertise not readily automated or replicated by AI. However, many applications\nare suffering from a very slow transition. This may be in large part due to\nlack of trust, usability, and productivity, especially when adapting to\nunforeseen classes and changes in mission context. Interactive machine learning\nis a newly emerging field in which machine learning implementations are\ntrained, optimized, evaluated, and exploited through an intuitive\nhuman-computer interface. In this paper, we introduce interactive machine\nlearning and explain its advantages and limitations within the context of\ndefense applications. Furthermore, we address several of the shortcomings of\ninteractive machine learning by discussing how cognitive feedback may inform\nfeatures, data, and results in the state of the art. We define the three\ntechniques by which cognitive feedback may be employed: self reporting,\nimplicit cognitive feedback, and modeled cognitive feedback. The advantages and\ndisadvantages of each technique are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 16:28:14 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Michael", "Chris J.", ""], ["Acklin", "Dina", ""], ["Scheuerman", "Jaelle", ""]]}, {"id": "2003.10504", "submitter": "Alvi Md Ishmam", "authors": "Alvi Md Ishmam, Md Raihan Mia", "title": "Challenges of Bridging the Gap between Mass People and Welfare\n  Organizations in Bangladesh", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing for the development of marginalized communities is a big deal of\nchallenges for researchers. Different social organizations are working to\ndevelop the conditions of a specialized marginalized community namely Street\nChildren, one of the most underprivileged communities in Bangladesh. However,\nlack of proper engagement among different social welfare organizations, donors,\nand the mass community limits the goal of the development of street children.\nDeveloping a virtual organization hub can eliminate communication gap as well\nas the information gap by involving people of all communities. However, some\nhuman imposed stigmas may often limit the rate of success of potential virtual\ncomputing solutions intended for organizations working with the marginalized\ncommunities, which we also face in our case. After a partial successful\ndeployment, the design itself needs to be self comprehensive and trustworthy in\norder to overcome the stigmas that demand a reasonable amount of time.\nMoreover, after a wide scalable deployment, it is yet to be investigated\nwhether the design of our computational solution can attain the goal for the\nfacilitation of the organizations so that those organizations can become more\neffective for the development of street children than before.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 19:28:49 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 18:41:24 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Ishmam", "Alvi Md", ""], ["Mia", "Md Raihan", ""]]}, {"id": "2003.10681", "submitter": "Karina Roundtree", "authors": "Karina A. Roundtree and Jason R. Cody and Jennifer Leaf and H. Onan\n  Demirel and Julie A. Adams", "title": "Human collective visualization transparency", "comments": "32 pages, 4 figures,", "journal-ref": null, "doi": "10.1007/s11721-021-00194-6", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in collective robotic systems has increased rapidly due to the\npotential benefits that can be offered to operators, such as increased safety\nand support, who perform challenging tasks in high-risk environments.\nHuman-collective transparency research has focused on how the design of the\nalgorithms, visualizations, and control mechanisms influence human-collective\nbehavior. Traditional collective visualizations have shown all of the\nindividual entities composing a collective, which may become problematic as\ncollectives scale in size and heterogeneity, and tasks become more demanding.\nHuman operators can become overloaded with information, which will negatively\naffect their understanding of the collective's current state and overall\nbehaviors, which can cause poor teaming performance. An analysis of\nvisualization transparency and the derived visualization design guidance, based\non remote supervision of collectives, are the primary contributions of this\nmanuscript. The individual agent and abstract visualizations were analyzed for\nsequential best-of-n decision-making tasks involving four collectives, composed\nof 200 entities each. The abstract visualization provided better transparency\nby enabling operators with different individual differences and capabilities to\nperform relatively the same and promoted higher human-collective performance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 06:39:43 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Roundtree", "Karina A.", ""], ["Cody", "Jason R.", ""], ["Leaf", "Jennifer", ""], ["Demirel", "H. Onan", ""], ["Adams", "Julie A.", ""]]}, {"id": "2003.10886", "submitter": "Maryam Alimardani", "authors": "Maryam Alimardani (1), Annabella Hermans (1), Angelica M. Tinga (1 and\n  2) ((1) Department of Cognitive Science and Artificial Intelligence, Tilburg\n  University, The Netherlands, (2) Department of Human Factors in Vehicle\n  Automation, Institute for Road Safety Research, The Netherlands)", "title": "Assessment of Empathy in an Affective VR Environment using EEG Signals", "comments": "13 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancements in social robotics and virtual avatars, it becomes\nincreasingly important that these agents adapt their behavior to the mood,\nfeelings and personality of their users. One such aspect of the user is\nempathy. Whereas many studies measure empathy through offline measures that are\ncollected after empathic stimulation (e.g. post-hoc questionnaires), the\ncurrent study aimed to measure empathy online, using brain activity collected\nduring the experience. Participants watched an affective 360 video of a child\nexperiencing domestic violence in a virtual reality headset while their EEG\nsignals were recorded. Results showed a significant attenuation of alpha, theta\nand delta asymmetry in the frontal and central areas of the brain. Moreover, a\nsignificant relationship between participants' empathy scores and their frontal\nalpha asymmetry at baseline was found. These results demonstrate specific brain\nactivity alterations when participants are exposed to an affective virtual\nreality environment, with the level of empathy as a personality trait being\nvisible in brain activity during a baseline measurement. These findings suggest\nthe potential of EEG measurements for development of passive brain-computer\ninterfaces that assess the user's affective responses in real-time and\nconsequently adapt the behavior of socially intelligent agents for a\npersonalized interaction.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 14:35:27 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Alimardani", "Maryam", "", "1 and\n  2"], ["Hermans", "Annabella", "", "1 and\n  2"], ["Tinga", "Angelica M.", "", "1 and\n  2"]]}, {"id": "2003.10914", "submitter": "Hamed Jahromi", "authors": "Hamed Z. Jahromi, Ivan Bartolec, Edwin Gamboa, Andrew Hines, Raimund\n  Schatz", "title": "You Drive Me Crazy! Interactive QoE Assessment for Telepresence Robot\n  Control", "comments": null, "journal-ref": null, "doi": "10.1109/QoMEX48832.2020.9123117", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Telepresence robots (TPRs) are versatile, remotely controlled vehicles that\nenable physical presence and human-to-human interaction over a distance. Thanks\nto improving hardware and dropping price points, TPRs enjoy the growing\ninterest in various industries and application domains. Still, a satisfying\nexperience remains key for their acceptance and successful adoption, not only\nin terms of enabling remote communication with others, but also in terms of\nmanaging robot mobility by means of remote navigation. This paper focuses on\nthe latter aspect of remote operation which has been hitherto neglected. We\npresent the results of an extensive subjective study designed to systematically\nassess remote navigation Quality of Experience (QoE) in the context of using a\nTPR live over the Internet. Participants were 'beamed' into a remote office\nspace and asked to perform characteristic TPR remote operation tasks (driving,\nturning, parking). Visual and control dimensions of their experience were\nsystematically impaired by altering network characteristics (bandwidth, delay\nand packet loss rate) in a controlled fashion. Our results show that users can\ndifferentiate well between visual and navigation/control aspects of their\nexperience. Furthermore, QoE impairment sensitivity varies with the actual task\nat hand.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 15:10:57 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Jahromi", "Hamed Z.", ""], ["Bartolec", "Ivan", ""], ["Gamboa", "Edwin", ""], ["Hines", "Andrew", ""], ["Schatz", "Raimund", ""]]}, {"id": "2003.11079", "submitter": "Wei Ye", "authors": "Wei Ye, Dominik Mautz, Christian Boehm, Ambuj Singh, Claudia Plant", "title": "Incorporating User's Preference into Attributed Graph Clustering", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2020.2976063", "report-no": null, "categories": "cs.LG cs.HC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph clustering has been studied extensively on both plain graphs and\nattributed graphs. However, all these methods need to partition the whole graph\nto find cluster structures. Sometimes, based on domain knowledge, people may\nhave information about a specific target region in the graph and only want to\nfind a single cluster concentrated on this local region. Such a task is called\nlocal clustering. In contrast to global clustering, local clustering aims to\nfind only one cluster that is concentrating on the given seed vertex (and also\non the designated attributes for attributed graphs). Currently, very few\nmethods can deal with this kind of task. To this end, we propose two quality\nmeasures for a local cluster: Graph Unimodality (GU) and Attribute Unimodality\n(AU). The former measures the homogeneity of the graph structure while the\nlatter measures the homogeneity of the subspace that is composed of the\ndesignated attributes. We call their linear combination as Compactness.\nFurther, we propose LOCLU to optimize the Compactness score. The local cluster\ndetected by LOCLU concentrates on the region of interest, provides efficient\ninformation flow in the graph and exhibits a unimodal data distribution in the\nsubspace of the designated attributes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 19:07:22 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Ye", "Wei", ""], ["Mautz", "Dominik", ""], ["Boehm", "Christian", ""], ["Singh", "Ambuj", ""], ["Plant", "Claudia", ""]]}, {"id": "2003.11320", "submitter": "Christopher Starke", "authors": "Christopher Starke, Marco Luenich", "title": "Artificial Intelligence for EU Decision-Making. Effects on Citizens\n  Perceptions of Input, Throughput and Output Legitimacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lack of political legitimacy undermines the ability of the European Union\nto resolve major crises and threatens the stability of the system as a whole.\nBy integrating digital data into political processes, the EU seeks to base\ndecision-making increasingly on sound empirical evidence. In particular,\nartificial intelligence systems have the potential to increase political\nlegitimacy by identifying pressing societal issues, forecasting potential\npolicy outcomes, informing the policy process, and evaluating policy\neffectiveness. This paper investigates how citizens perceptions of EU input,\nthroughput, and output legitimacy are influenced by three distinct\ndecision-making arrangements. First, independent human decision-making, HDM,\nSecond, independent algorithmic decision-making, ADM, and, third, hybrid\ndecision-making by EU politicians and AI-based systems together. The results of\na pre-registered online experiment with 572 respondents suggest that existing\nEU decision-making arrangements are still perceived as the most democratic -\ninput legitimacy. However, regarding the decision-making process itself -\nthroughput legitimacy - and its policy outcomes - output legitimacy, no\ndifference was observed between the status quo and hybrid decision-making\ninvolving both ADM and democratically elected EU institutions. Where ADM\nsystems are the sole decision-maker, respondents tend to perceive these as\nillegitimate. The paper discusses the implications of these findings for EU\nlegitimacy and data-driven policy-making.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 10:56:28 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Starke", "Christopher", ""], ["Luenich", "Marco", ""]]}, {"id": "2003.11432", "submitter": "Trenton Schulz", "authors": "Trenton Schulz, Jo Herstad, Harald Holone", "title": "Privacy at Home: an Inquiry into Sensors and Robots for the Stay at Home\n  Elderly", "comments": "18 pages, 5 figures, International Conference on Human Aspects of IT\n  for the Aged Population, Part of 2018 HCI International", "journal-ref": "Human Aspects of IT for the Aged Population. Applications in\n  Health, Assistance, and Entertainment, LNCS 10927, pp. 377-394, 2018", "doi": "10.1007/978-3-319-92037-5_28", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The elderly in the future will use smart house technology, sensors, and\nrobots to stay at home longer. Privacy at home for these elderly is important.\nIn this exploratory paper, we examine different understandings of privacy and\nuse Palen and Dourish's framework to look at the negotiation of privacy along\nboundaries between a human at home, the robot, and its sensors. We select three\ndilemmas: turning sensors on and off, the robot seeing through walls, and\nmachine learning. We discuss these dilemmas and also discuss ways the robot can\nhelp make the elderly more aware of privacy issues and to build trust.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 14:55:57 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Schulz", "Trenton", ""], ["Herstad", "Jo", ""], ["Holone", "Harald", ""]]}, {"id": "2003.11443", "submitter": "Trenton Schulz", "authors": "Trenton Schulz, Patrick Holthaus, Farshid Amirabdollahian, Kheng Lee\n  Koay, Jim Torresen, and Jo Herstad", "title": "Differences of Human Perceptions of a Robot Moving using Linear or Slow\n  in, Slow out Velocity Profiles When Performing a Cleaning Task", "comments": "8 pages, 8 figures, 5 tables, 2019 28th IEEE International Conference\n  on Robot and Human Interactive Communication (RO-MAN)", "journal-ref": "2019 28th IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN), IEEE", "doi": "10.1109/RO-MAN46459.2019.8956355", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigated how a robot moving with different velocity profiles affects a\nperson's perception of it when working together on a task. The two profiles are\nthe common linear profile and a profile based on the animation principles of\nslow in, slow out. The investigation was accomplished by running an experiment\nin a home context where people and the robot cooperated on a clean-up task. We\nused the Godspeed series of questionnaires to gather people's perception of the\nrobot. Average scores for each series appear not to be different enough to\nreject the null hypotheses, but looking at the component items provides paths\nto future areas of research. We also discuss the scenario for the experiment\nand how it may be used for future research into using animation techniques for\nmoving robots and improving the legibility of a robot's locomotion.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 15:14:39 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Schulz", "Trenton", ""], ["Holthaus", "Patrick", ""], ["Amirabdollahian", "Farshid", ""], ["Koay", "Kheng Lee", ""], ["Torresen", "Jim", ""], ["Herstad", "Jo", ""]]}, {"id": "2003.11461", "submitter": "Shihao Xu", "authors": "Shihao Xu, Jing Fang, Xiping Hu, Edith Ngai, Yi Guo, Victor C.M.\n  Leung, Jun Cheng, Bin Hu", "title": "Emotion Recognition From Gait Analyses: Current Research and Future\n  Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human gait refers to a daily motion that represents not only mobility, but it\ncan also be used to identify the walker by either human observers or computers.\nRecent studies reveal that gait even conveys information about the walker's\nemotion. Individuals in different emotion states may show different gait\npatterns. The mapping between various emotions and gait patterns provides a new\nsource for automated emotion recognition. Compared to traditional emotion\ndetection biometrics, such as facial expression, speech and physiological\nparameters, gait is remotely observable, more difficult to imitate, and\nrequires less cooperation from the subject. These advantages make gait a\npromising source for emotion detection. This article reviews current research\non gait-based emotion detection, particularly on how gait parameters can be\naffected by different emotion states and how the emotion states can be\nrecognized through distinct gait patterns. We focus on the detailed methods and\ntechniques applied in the whole process of emotion recognition: data\ncollection, preprocessing, and classification. At last, we discuss possible\nfuture developments of efficient and effective gait-based emotion recognition\nusing the state of the art techniques on intelligent computation and big data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 08:22:33 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 11:28:02 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 01:39:01 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Xu", "Shihao", ""], ["Fang", "Jing", ""], ["Hu", "Xiping", ""], ["Ngai", "Edith", ""], ["Guo", "Yi", ""], ["Leung", "Victor C. M.", ""], ["Cheng", "Jun", ""], ["Hu", "Bin", ""]]}, {"id": "2003.11875", "submitter": "Trenton Schulz", "authors": "Trenton Schulz, Kristin Skeide Fuglerud", "title": "Creating Personas with Disabilities", "comments": "8 pages", "journal-ref": null, "doi": "10.1007/978-3-642-31534-3_22", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personas can help raise awareness among stakeholders about users' needs.\nWhile personas are made-up people, they are based on facts gathered from user\nresearch. Personas can also be used to raise awareness of universal design and\naccessibility needs of people with disabilities. We review the current state of\nthe art of the personas and review some research and industry projects that use\nthem. We outline techniques that can be used to create personas with\ndisabilities. This includes advice on how to get more information about\nassistive technology and how to better include people with disabilities in the\npersona creation process. We also describe our use of personas with\ndisabilities in several projects and discuss how it has helped to find\naccessibility issues.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 13:00:10 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Schulz", "Trenton", ""], ["Fuglerud", "Kristin Skeide", ""]]}, {"id": "2003.11959", "submitter": "Fanta Camara", "authors": "Fanta Camara, Nicola Bellotto, Serhan Cosar, Florian Weber, Dimitris\n  Nathanael, Matthias Althoff, Jingyuan Wu, Johannes Ruenz, Andr\\'e Dietrich,\n  Gustav Markkula, Anna Schieben, Fabio Tango, Natasha Merat and Charles W. Fox", "title": "Pedestrian Models for Autonomous Driving Part II: High-Level Models of\n  Human Behavior", "comments": "Accepted for publication in the IEEE Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2020.3006767", "report-no": null, "categories": "cs.RO cs.GT cs.HC cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles (AVs) must share space with pedestrians, both in\ncarriageway cases such as cars at pedestrian crossings and off-carriageway\ncases such as delivery vehicles navigating through crowds on pedestrianized\nhigh-streets. Unlike static obstacles, pedestrians are active agents with\ncomplex, interactive motions. Planning AV actions in the presence of\npedestrians thus requires modelling of their probable future behaviour as well\nas detecting and tracking them. This narrative review article is Part II of a\npair, together surveying the current technology stack involved in this process,\norganising recent research into a hierarchical taxonomy ranging from low-level\nimage detection to high-level psychological models, from the perspective of an\nAV designer. This self-contained Part II covers the higher levels of this\nstack, consisting of models of pedestrian behaviour, from prediction of\nindividual pedestrians' likely destinations and paths, to game-theoretic models\nof interactions between pedestrians and autonomous vehicles. This survey\nclearly shows that, although there are good models for optimal walking\nbehaviour, high-level psychological and social modelling of pedestrian\nbehaviour still remains an open research question that requires many conceptual\nissues to be clarified. Early work has been done on descriptive and qualitative\nmodels of behaviour, but much work is still needed to translate them into\nquantitative algorithms for practical AV control.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 14:55:18 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 14:48:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Camara", "Fanta", ""], ["Bellotto", "Nicola", ""], ["Cosar", "Serhan", ""], ["Weber", "Florian", ""], ["Nathanael", "Dimitris", ""], ["Althoff", "Matthias", ""], ["Wu", "Jingyuan", ""], ["Ruenz", "Johannes", ""], ["Dietrich", "Andr\u00e9", ""], ["Markkula", "Gustav", ""], ["Schieben", "Anna", ""], ["Tango", "Fabio", ""], ["Merat", "Natasha", ""], ["Fox", "Charles W.", ""]]}, {"id": "2003.12122", "submitter": "Kyungjun Lee", "authors": "Kyungjun Lee, Daisuke Sato, Saki Asakawa, Hernisa Kacorri, Chieko\n  Asakawa", "title": "Pedestrian Detection with Wearable Cameras for the Blind: A Two-way\n  Perspective", "comments": "The 2020 ACM CHI Conference on Human Factors in Computing Systems\n  (CHI 2020)", "journal-ref": null, "doi": "10.1145/3313831.3376398", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind people have limited access to information about their surroundings,\nwhich is important for ensuring one's safety, managing social interactions, and\nidentifying approaching pedestrians. With advances in computer vision, wearable\ncameras can provide equitable access to such information. However, the\nalways-on nature of these assistive technologies poses privacy concerns for\nparties that may get recorded. We explore this tension from both perspectives,\nthose of sighted passersby and blind users, taking into account camera\nvisibility, in-person versus remote experience, and extracted visual\ninformation. We conduct two studies: an online survey with MTurkers (N=206) and\nan in-person experience study between pairs of blind (N=10) and sighted (N=40)\nparticipants, where blind participants wear a working prototype for pedestrian\ndetection and pass by sighted participants. Our results suggest that both of\nthe perspectives of users and bystanders and the several factors mentioned\nabove need to be carefully considered to mitigate potential social tensions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 19:34:54 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 19:17:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Lee", "Kyungjun", ""], ["Sato", "Daisuke", ""], ["Asakawa", "Saki", ""], ["Kacorri", "Hernisa", ""], ["Asakawa", "Chieko", ""]]}, {"id": "2003.12282", "submitter": "Juri Buchm\\\"uller", "authors": "Juri F. Buchm\\\"uller, Udo Schlegel, Eren Cakmak, Evanthia Dimara,\n  Daniel A. Keim", "title": "SpatialRugs: Enhancing Spatial Awareness of Movement in Dense Pixel\n  Visualizations", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compact visual summaries of spatio-temporal movement data often strive to\nexpress accurate positions of movers. We present SpatialRugs, a technique to\nenhance the spatial awareness of movements in dense pixel visualizations.\nSpatialRugs apply 2D colormaps to visualize location mapped to a juxtaposed\ndisplay. We explore the effect of various colormaps discussing perceptual\nlimitations and introduce a custom color-smoothing method to mitigate distorted\npatterns of collective movement behavior.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 08:53:09 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Buchm\u00fcller", "Juri F.", ""], ["Schlegel", "Udo", ""], ["Cakmak", "Eren", ""], ["Dimara", "Evanthia", ""], ["Keim", "Daniel A.", ""]]}, {"id": "2003.12496", "submitter": "Henrik Detjen", "authors": "Henrik Detjen and Stefan Geisler and Stefan Schneegass", "title": "Maneuver-based Driving for Intervention in Autonomous Cars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way we communicate with autonomous cars will fundamentally change as soon\nas manual input is no longer required as back-up for the autonomous system.\nManeuver-based driving is a potential way to allow still the user to intervene\nwith the autonomous car to communicate requests such as stopping at the next\nparking lot. In this work, we highlight different research questions that still\nneed to be explored to gain insights into how such control can be realized in\nthe future.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 15:58:35 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Detjen", "Henrik", ""], ["Geisler", "Stefan", ""], ["Schneegass", "Stefan", ""]]}, {"id": "2003.12569", "submitter": "Yoichi Yamazaki", "authors": "Kazuaki Takeuchi, Yoichi Yamazaki, and Kentaro Yoshifuji", "title": "Avatar Work: Telework for Disabled People Unable to Go Outside by Using\n  Avatar Robots \"OriHime-D\" and Its Verification", "comments": "8 pages, 16 figures, accepted to the 2020 ACM/IEEE International\n  Conference on Human-Robot Interaction (HRI '20 Companion) at alt.HRI session,\n  2020", "journal-ref": "In Companion of the 2020 ACM/IEEE International Conference on\n  Human-Robot Interaction (HRI '20 Companion), March, 2020, 8pages", "doi": "10.1145/3371382.3380737", "report-no": null, "categories": "cs.RO cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a telework \"avatar work\" that enables people with\ndisabilities to engage in physical works such as customer service in order to\nrealize an inclusive society, where we can do anything if we have free mind,\neven though we are bedridden. In avatar work, disabled people can remotely\nengage in physical work by operating a proposed robot \"OriHime-D\" with a mouse\nor gaze input depending on their own disabilities. As a social implementation\ninitiative of avatar work, we have opened a two-week limited avatar robot cafe\nand have evaluated remote employment by people with disabilities using\nOriHime-D. As the results by 10 people with disabilities, we have confirmed\nthat the proposed avatar work leads to mental fulfillment for people with\ndisparities, and can be designed with adaptable workload. In addition, we have\nconfirmed that the work content of the experimental cafe is appropriate for\npeople with a variety of disabilities seeking social participation. This study\ncontributes to fulfillment all through life and lifetime working, and at the\nsame time leads to a solution to the employment shortage problem.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 12:44:47 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Takeuchi", "Kazuaki", ""], ["Yamazaki", "Yoichi", ""], ["Yoshifuji", "Kentaro", ""]]}, {"id": "2003.12604", "submitter": "Tom\\'as Alves", "authors": "Tom\\'as Alves, Samuel Gomes, Jo\\~ao Dias, Carlos Martinho", "title": "The Influence of Reward on the Social Valence of Interactions", "comments": "8 pages, 6 figures, 1 table, submitted to IEEE Conference on Games\n  (CoG) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the years, social norms have been promoted as an informal\nenforcement mechanism for achieving beneficial collective outcomes. Among the\nmost used methods to foster interactions, framing the context of a situation or\nsetting in-game rules have shown strong results as mediators on how an\nindividual interacts with their peers. Nevertheless, we found that there is a\nlack of research regarding the use of incentives such as scores to promote\nsocial interactions differing in valence. Weighing how incentives influence\nin-game behavior, we propose the use of rewards to promote interactions varying\nin valence, i.e. positive or negative, in a two-player scenario. To do so, we\ndefined social valence as a continuous scale with two poles represented by\nComplicate and Help. Then, we performed user tests where participants where\nasked to play a game with two reward-based systems to test on whether the\nscoring system influenced the social interaction valence. The results indicate\nthat the developed reward-based systems were able to foster interactions\ndiverging in social valence scores, providing insights on how factors such as\nincentives overlap individual's established social norms. These findings\nempower game developers and designers with a low-cost and effective policy tool\nthat is able to promote in-game behavior changes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 19:02:59 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 16:36:27 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Alves", "Tom\u00e1s", ""], ["Gomes", "Samuel", ""], ["Dias", "Jo\u00e3o", ""], ["Martinho", "Carlos", ""]]}, {"id": "2003.13044", "submitter": "Henrik Detjen", "authors": "Henrik Detjen, Stefan Geisler, Stefan Schneegass", "title": "Implicit Cooperation: Emotion Detection for Validation and Adaptation of\n  Automated Vehicles' Driving Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human emotion detection in automated vehicles helps to improve comfort and\nsafety. Research in the automotive domain focuses a lot on sensing drivers'\ndrowsiness and aggression. We present a new form of implicit driver-vehicle\ncooperation, where emotion detection is integrated into an automated vehicle's\ndecision-making process. Constant evaluation of the driver's reaction to\nvehicle behavior allows us to revise decisions and helps to increase the safety\nof future automated vehicles.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 14:53:11 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Detjen", "Henrik", ""], ["Geisler", "Stefan", ""], ["Schneegass", "Stefan", ""]]}, {"id": "2003.13235", "submitter": "Javad Ghofrani", "authors": "Bastian Deutschmann, Javad Ghofrani, Dirk Reichelt", "title": "Cognitive Production Systems: A Mapping Study", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Production plants today are becoming more and more complicated through more\nautomation and networking. It is becoming more difficult for humans to\nparticipate, due to higher speed and decreasing reaction time of these plants.\nTendencies to improve production systems with the help of cognitive systems can\nbe identified. The goal is to save resources and time. This mapping study gives\nan insight into the domain, categorizes different approaches and estimates\ntheir progress. Furthermore, it shows achieved optimizations and persisting\nproblems and barriers. These representations should make it easier in the\nfuture to address concrete problems in this research field. Human-Machine\nInteraction and Knowledge Gaining/Sharing represent the largest categories of\nthe domain. Most often, a gain in efficiency and maximized effectiveness can be\nachieved as optimization. The most common problem is the missing or only\ndifficult generalization of the presented concepts.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 06:30:10 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 11:58:18 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 06:10:52 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Deutschmann", "Bastian", ""], ["Ghofrani", "Javad", ""], ["Reichelt", "Dirk", ""]]}, {"id": "2003.13316", "submitter": "Bennett Kleinberg", "authors": "Bennett Kleinberg and Bruno Verschuere", "title": "How human judgment impairs automated deception detection performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Deception detection is a prevalent problem for security\npractitioners. With a need for more large-scale approaches, automated methods\nusing machine learning have gained traction. However, detection performance\nstill implies considerable error rates. Findings from other domains suggest\nthat hybrid human-machine integrations could offer a viable path in deception\ndetection tasks. Method: We collected a corpus of truthful and deceptive\nanswers about participants' autobiographical intentions (n=1640) and tested\nwhether a combination of supervised machine learning and human judgment could\nimprove deception detection accuracy. Human judges were presented with the\noutcome of the automated credibility judgment of truthful and deceptive\nstatements. They could either fully overrule it (hybrid-overrule condition) or\nadjust it within a given boundary (hybrid-adjust condition). Results: The data\nsuggest that in neither of the hybrid conditions did the human judgment add a\nmeaningful contribution. Machine learning in isolation identified truth-tellers\nand liars with an overall accuracy of 69%. Human involvement through\nhybrid-overrule decisions brought the accuracy back to the chance level. The\nhybrid-adjust condition did not deception detection performance. The\ndecision-making strategies of humans suggest that the truth bias - the tendency\nto assume the other is telling the truth - could explain the detrimental\neffect. Conclusion: The current study does not support the notion that humans\ncan meaningfully add to the deception detection performance of a machine\nlearning system.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 10:06:36 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Kleinberg", "Bennett", ""], ["Verschuere", "Bruno", ""]]}, {"id": "2003.13483", "submitter": "Nikhil Churamani", "authors": "Nikhil Churamani and Francisco Cruz and Sascha Griffiths and Pablo\n  Barros", "title": "iCub: Learning Emotion Expressions using Human Reward", "comments": "Published in the Workshop on Bio-inspired Social Robot Learning in\n  Home Scenarios, IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), Daejeon, Korea (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of the present study is to learn emotion expression\nrepresentations for artificial agents using reward shaping mechanisms. The\napproach takes inspiration from the TAMER framework for training a Multilayer\nPerceptron (MLP) to learn to express different emotions on the iCub robot in a\nhuman-robot interaction scenario. The robot uses a combination of a\nConvolutional Neural Network (CNN) and a Self-Organising Map (SOM) to recognise\nan emotion and then learns to express the same using the MLP. The objective is\nto teach a robot to respond adequately to the user's perception of emotions and\nlearn how to express different emotions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:54:25 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Churamani", "Nikhil", ""], ["Cruz", "Francisco", ""], ["Griffiths", "Sascha", ""], ["Barros", "Pablo", ""]]}, {"id": "2003.13545", "submitter": "Mashfiqui Rabbi", "authors": "Mashfiqui Rabbi, Meredith Philyaw-Kotov, Jinseok Li, Katherine Li,\n  Bess Rothman, Lexa Giragosian, Maya Reyes, Hannah Gadway, Rebecca Cunningham,\n  Erin Bonar, Inbal Nahum-Shani, Maureen Walton, Susan Murphy, Predrag Klasnja", "title": "Translating Behavioral Theory into Technological Interventions: Case\n  Study of an mHealth App to Increase Self-reporting of Substance-Use Related\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobile health (mHealth) applications are a powerful medium for providing\nbehavioral interventions, and systematic reviews suggest that theory-based\ninterventions are more effective. However, how exactly theoretical concepts\nshould be translated into features of technological interventions is often not\nclear. There is a gulf between the abstract nature of psychological theory and\nthe concreteness of the designs needed to build health technologies. In this\npaper, we use SARA, a mobile app we developed to support substance-use research\namong adolescents and young adults, as a case study of a process of translating\nbehavioral theory into mHealth intervention design. SARA was designed to\nincrease adherence to daily self-report in longitudinal epidemiological\nstudies. To achieve this goal, we implemented a number of constructs from the\noperant conditioning theory. We describe our design process and discuss how we\noperationalized theoretical constructs in the light of design constraints, user\nfeedback, and empirical data from four formative studies.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 15:12:32 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Rabbi", "Mashfiqui", ""], ["Philyaw-Kotov", "Meredith", ""], ["Li", "Jinseok", ""], ["Li", "Katherine", ""], ["Rothman", "Bess", ""], ["Giragosian", "Lexa", ""], ["Reyes", "Maya", ""], ["Gadway", "Hannah", ""], ["Cunningham", "Rebecca", ""], ["Bonar", "Erin", ""], ["Nahum-Shani", "Inbal", ""], ["Walton", "Maureen", ""], ["Murphy", "Susan", ""], ["Klasnja", "Predrag", ""]]}, {"id": "2003.13731", "submitter": "Michael Braun", "authors": "Michael Braun, Florian Weber, Florian Alt", "title": "Affective Automotive User Interfaces -- Reviewing the State of Emotion\n  Regulation in the Car", "comments": "Under Review at ACM CSUR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective technology offers exciting opportunities to improve road safety by\ncatering to human emotions. Modern car interiors enable the contactless\ndetection of user states, paving the way for a systematic promotion of safe\ndriver behavior through emotion regulation. We review the current literature\nregarding the impact of emotions on driver behavior and analyze the state of\nemotion regulation approaches in the car. We summarize challenges for affective\ninteraction in form of cultural aspects, technological hurdles and\nmethodological considerations, as well as opportunities to improve road safety\nby reinstating drivers into an emotionally balanced state. The purpose of this\nreview is to outline the community's combined knowledge for interested\nresearchers, to provide a focussed introduction for practitioners and to\nidentify future directions for affective interaction in the car.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 18:26:01 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Braun", "Michael", ""], ["Weber", "Florian", ""], ["Alt", "Florian", ""]]}, {"id": "2003.13762", "submitter": "Ashok Goel", "authors": "William Broniec, Sungeun An, Spencer Rugaber, Ashok K. Goel", "title": "Using VERA to explain the impact of social distancing on the spread of\n  COVID-19", "comments": "6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 continues to spread across the country and around the world. Current\nstrategies for managing the spread of COVID-19 include social distancing. We\npresent VERA, an interactive AI tool, that first enables users to specify\nconceptual models of the impact of social distancing on the spread of COVID-19.\nThen, VERA automatically spawns agent-based simulations from the conceptual\nmodels, and, given a data set, automatically fills in the values of the\nsimulation parameters from the data. Next, the user can view the simulation\nresults, and, if needed, revise the simulation parameters and run another\nexperimental trial, or build an alternative conceptual model. We describe the\nuse VERA to develop a SIR model for the spread of COVID-19 and its relationship\nwith healthcare capacity.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 19:22:07 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Broniec", "William", ""], ["An", "Sungeun", ""], ["Rugaber", "Spencer", ""], ["Goel", "Ashok K.", ""]]}, {"id": "2003.13830", "submitter": "Necati Cihan Camgoz", "authors": "Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, Richard Bowden", "title": "Sign Language Transformers: Joint End-to-end Sign Language Recognition\n  and Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work on Sign Language Translation has shown that having a mid-level\nsign gloss representation (effectively recognizing the individual signs)\nimproves the translation performance drastically. In fact, the current\nstate-of-the-art in translation requires gloss level tokenization in order to\nwork. We introduce a novel transformer based architecture that jointly learns\nContinuous Sign Language Recognition and Translation while being trainable in\nan end-to-end manner. This is achieved by using a Connectionist Temporal\nClassification (CTC) loss to bind the recognition and translation problems into\na single unified architecture. This joint approach does not require any\nground-truth timing information, simultaneously solving two co-dependant\nsequence-to-sequence learning problems and leads to significant performance\ngains.\n  We evaluate the recognition and translation performances of our approaches on\nthe challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset. We report\nstate-of-the-art sign language recognition and translation results achieved by\nour Sign Language Transformers. Our translation networks outperform both sign\nvideo to spoken language and gloss to spoken language translation models, in\nsome cases more than doubling the performance (9.58 vs. 21.80 BLEU-4 Score). We\nalso share new baseline translation results using transformer networks for\nseveral other text-to-text sign language translation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 21:35:09 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Camgoz", "Necati Cihan", ""], ["Koller", "Oscar", ""], ["Hadfield", "Simon", ""], ["Bowden", "Richard", ""]]}, {"id": "2003.13922", "submitter": "Tianhao Wang", "authors": "Aiping Xiong, Tianhao Wang, Ninghui Li, Somesh Jha", "title": "Towards Effective Differential Privacy Communication for Users' Data\n  Sharing Decision and Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy protects an individual's privacy by perturbing data on\nan aggregated level (DP) or individual level (LDP). We report four online\nhuman-subject experiments investigating the effects of using different\napproaches to communicate differential privacy techniques to laypersons in a\nhealth app data collection setting. Experiments 1 and 2 investigated\nparticipants' data disclosure decisions for low-sensitive and high-sensitive\npersonal information when given different DP or LDP descriptions. Experiments 3\nand 4 uncovered reasons behind participants' data sharing decisions, and\nexamined participants' subjective and objective comprehensions of these DP or\nLDP descriptions. When shown descriptions that explain the implications instead\nof the definition/processes of DP or LDP technique, participants demonstrated\nbetter comprehension and showed more willingness to share information with LDP\nthan with DP, indicating their understanding of LDP's stronger privacy\nguarantee compared with DP.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 02:36:39 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Xiong", "Aiping", ""], ["Wang", "Tianhao", ""], ["Li", "Ninghui", ""], ["Jha", "Somesh", ""]]}, {"id": "2003.13934", "submitter": "Simon Perrault", "authors": "Lancelot Dupont, Christophe Jouffrais, Simon T. Perrault", "title": "Vibrotactile Feedback for Vertical 2D Space Exploration", "comments": "4+1 pages", "journal-ref": null, "doi": "10.1145/3399715.3399834", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually impaired people encounter many challenges in their everyday life,\nespecially when it comes to navigating and representing space. The issue of\nshopping is addressed mostly on the level of navigation and product detection,\nbut conveying clues about the object position to the user is rarely\nimplemented. This work presents a prototype of vibrotactile wristband using\nspatiotemporal patterns to help visually impaired users reach an object in the\n2D plane in front of them. A pilot study on twelve blindfolded sighted subjects\nshowed that discretizing space in a seven by seven targets matrix and conveying\nclues with a discrete pattern on the vertical axis and a continuous pattern on\nthe horizontal axis is an intuitive and effective design.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 03:19:29 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Dupont", "Lancelot", ""], ["Jouffrais", "Christophe", ""], ["Perrault", "Simon T.", ""]]}, {"id": "2003.13987", "submitter": "Nora Castner", "authors": "Nora Castner, Thomas K\\\"ubler, Katharina Scheiter, Juilane Richter,\n  Th\\'er\\'ese Eder, Fabian H\\\"uttig, Constanze Keutel, Enkelejda Kasneci", "title": "Deep semantic gaze embedding and scanpath comparison for expertise\n  classification during OPT viewing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling eye movement indicative of expertise behavior is decisive in user\nevaluation. However, it is indisputable that task semantics affect gaze\nbehavior. We present a novel approach to gaze scanpath comparison that\nincorporates convolutional neural networks (CNN) to process scene information\nat the fixation level. Image patches linked to respective fixations are used as\ninput for a CNN and the resulting feature vectors provide the temporal and\nspatial gaze information necessary for scanpath similarity comparison.We\nevaluated our proposed approach on gaze data from expert and novice dentists\ninterpreting dental radiographs using a local alignment similarity score. Our\napproach was capable of distinguishing experts from novices with 93% accuracy\nwhile incorporating the image semantics. Moreover, our scanpath comparison\nusing image patch features has the potential to incorporate task semantics from\na variety of tasks\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 07:00:59 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Castner", "Nora", ""], ["K\u00fcbler", "Thomas", ""], ["Scheiter", "Katharina", ""], ["Richter", "Juilane", ""], ["Eder", "Th\u00e9r\u00e9se", ""], ["H\u00fcttig", "Fabian", ""], ["Keutel", "Constanze", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2003.14121", "submitter": "Pin-Chu Yang", "authors": "Pin-Chu Yang, Mohammed Al-Sada, Chang-Chieh Chiu, Kevin Kuo, Tito\n  Pradhono Tomo, Kanata Suzuki, Nelson Yalta, Kuo-Hao Shu and Tetsuya Ogata", "title": "HATSUKI : An anime character like robot figure platform with anime-style\n  expressions and imitation learning based action generation", "comments": "8 pages, 11 figures, submitted to Ro-MAN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Japanese character figurines are popular and have pivot position in Otaku\nculture. Although numerous robots have been developed, less have focused on\notaku-culture or on embodying the anime character figurine. Therefore, we take\nthe first steps to bridge this gap by developing Hatsuki, which is a humanoid\nrobot platform with anime based design. Hatsuki's novelty lies in aesthetic\ndesign, 2D facial expressions, and anime-style behaviors that allows it to\ndeliver rich interaction experiences resembling anime-characters. We explain\nour design implementation process of Hatsuki, followed by our evaluations. In\norder to explore user impressions and opinions towards Hatsuki, we conducted a\nquestionnaire in the world's largest anime-figurine event. The results indicate\nthat participants were generally very satisfied with Hatsuki's design, and\nproposed various use case scenarios and deployment contexts for Hatsuki. The\nsecond evaluation focused on imitation learning, as such method can provide\nbetter interaction ability in the real world and generate rich,\ncontext-adaptive behavior in different situations. We made Hatsuki learn 11\nactions, combining voice, facial expressions and motions, through neuron\nnetwork based policy model with our proposed interface. Results show our\napproach was successfully able to generate the actions through self-organized\ncontexts, which shows the potential for generalizing our approach in further\nactions under different contexts. Lastly, we present our future research\ndirection for Hatsuki, and provide our conclusion.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 11:53:50 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 12:59:24 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Yang", "Pin-Chu", ""], ["Al-Sada", "Mohammed", ""], ["Chiu", "Chang-Chieh", ""], ["Kuo", "Kevin", ""], ["Tomo", "Tito Pradhono", ""], ["Suzuki", "Kanata", ""], ["Yalta", "Nelson", ""], ["Shu", "Kuo-Hao", ""], ["Ogata", "Tetsuya", ""]]}, {"id": "2003.14200", "submitter": "Gaston Lenczner", "authors": "Gaston Lenczner, Bertrand Le Saux, Nicola Luminari, Adrien Chan Hon\n  Tong and Guy Le Besnerais", "title": "DISIR: Deep Image Segmentation with Interactive Refinement", "comments": "8 pages, 12 figures. Published in the ISPRS Annals of the\n  Photogrammetry, Remote Sensing and Spatial Information Sciences", "journal-ref": "XXIV ISPRS Congress, Commission II (Volume V-2-2020)", "doi": "10.5194/isprs-annals-V-2-2020-877-2020", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an interactive approach for multi-class segmentation of\naerial images. Precisely, it is based on a deep neural network which exploits\nboth RGB images and annotations. Starting from an initial output based on the\nimage only, our network then interactively refines this segmentation map using\na concatenation of the image and user annotations. Importantly, user\nannotations modify the inputs of the network - not its weights - enabling a\nfast and smooth process. Through experiments on two public aerial datasets, we\nshow that user annotations are extremely rewarding: each click corrects roughly\n5000 pixels. We analyze the impact of different aspects of our framework such\nas the representation of the annotations, the volume of training data or the\nnetwork architecture. Code is available at https://github.com/delair-ai/DISIR.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 13:37:42 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 14:04:45 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Lenczner", "Gaston", ""], ["Saux", "Bertrand Le", ""], ["Luminari", "Nicola", ""], ["Tong", "Adrien Chan Hon", ""], ["Besnerais", "Guy Le", ""]]}, {"id": "2003.14274", "submitter": "Yalong Yang", "authors": "Yalong Yang, Kim Marriott, Matthew Butler, Cagatay Goncu and Leona\n  Holloway", "title": "Tactile Presentation of Network Data: Text, Matrix or Diagram?", "comments": "To appear in the ACM CHI Conference on Human Factors in Computing\n  Systems (CHI 2020)", "journal-ref": null, "doi": "10.1145/3313831.3376367", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualisations are commonly used to understand social, biological and other\nkinds of networks. Currently, we do not know how to effectively present network\ndata to people who are blind or have low-vision (BLV). We ran a controlled\nstudy with 8 BLV participants comparing four tactile representations: organic\nnode-link diagram, grid node-link diagram, adjacency matrix and braille list.\nWe found that the node-link representations were preferred and more effective\nfor path following and cluster identification while the matrix and list were\nbetter for adjacency tasks. This is broadly in line with findings for the\ncorresponding visual representations.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 15:01:06 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Yang", "Yalong", ""], ["Marriott", "Kim", ""], ["Butler", "Matthew", ""], ["Goncu", "Cagatay", ""], ["Holloway", "Leona", ""]]}, {"id": "2003.14285", "submitter": "Liam Hiley BSc", "authors": "Liam Hiley and Alun Preece and Yulia Hicks and Supriyo Chakraborty and\n  Prudhvi Gurram and Richard Tomsett", "title": "Explaining Motion Relevance for Activity Recognition in Video Deep\n  Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A small subset of explainability techniques developed initially for image\nrecognition models has recently been applied for interpretability of 3D\nConvolutional Neural Network models in activity recognition tasks. Much like\nthe models themselves, the techniques require little or no modification to be\ncompatible with 3D inputs. However, these explanation techniques regard spatial\nand temporal information jointly. Therefore, using such explanation techniques,\na user cannot explicitly distinguish the role of motion in a 3D model's\ndecision. In fact, it has been shown that these models do not appropriately\nfactor motion information into their decision. We propose a selective relevance\nmethod for adapting the 2D explanation techniques to provide motion-specific\nexplanations, better aligning them with the human understanding of motion as\nconceptually separate from static spatial features. We demonstrate the utility\nof our method in conjunction with several widely-used 2D explanation methods,\nand show that it improves explanation selectivity for motion. Our results show\nthat the selective relevance method can not only provide insight on the role\nplayed by motion in the model's decision -- in effect, revealing and\nquantifying the model's spatial bias -- but the method also simplifies the\nresulting explanations for human consumption.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 15:19:04 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Hiley", "Liam", ""], ["Preece", "Alun", ""], ["Hicks", "Yulia", ""], ["Chakraborty", "Supriyo", ""], ["Gurram", "Prudhvi", ""], ["Tomsett", "Richard", ""]]}, {"id": "2003.14294", "submitter": "Megan Charity", "authors": "Megan Charity, Ahmed Khalifa, Julian Togelius", "title": "Baba is Y'all: Collaborative Mixed-Initiative Level Design", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a collaborative mixed-initiative system for building levels for\nthe puzzle game \"Baba is You\". Unlike previous mixed-initiative systems, Baba\nis Y'all is designed for collaborative asynchronous creation by multiple users\nover the internet. The system includes several AI-assisted features to help\ndesigners, including a level evolver and an automated player for playtesting.\nThe level archives catalogues levels according to which mechanics are\nimplemented and not implemented, allowing the system to ask users to design\nlevels with specific combinations of mechanics. We describe the operation of\nthe system and the results of small-scale informal user test, and discuss\nfuture development paths for this system as well as for collaborative\nmixed-initiative systems in general.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 15:29:53 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 16:15:20 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Charity", "Megan", ""], ["Khalifa", "Ahmed", ""], ["Togelius", "Julian", ""]]}, {"id": "2003.14310", "submitter": "Abhinandan Dalal", "authors": "Arindam Roy Chowdhury, Abhinandan Dalal and Shubhajit Sen", "title": "Accelerography: Feasibility of Gesture Typing using Accelerometer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to look into the feasibility of constructing alphabets\nusing gestures. The main idea is to construct gestures, that are easy to\nremember, not cumbersome to reproduce and easily identifiable. We construct\ngestures for the entire English alphabet and provide an algorithm to identify\nthe gestures, even when they are constructed continuously. We tackle the\nproblem statistically, taking into account the problem of randomness in the\nhand movement gestures of users, and achieve an average accuracy of 97.33% with\nthe entire English alphabet.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 20:12:46 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Chowdhury", "Arindam Roy", ""], ["Dalal", "Abhinandan", ""], ["Sen", "Shubhajit", ""]]}, {"id": "2003.14392", "submitter": "Eleftherios Triantafyllidis Mr.", "authors": "Eleftherios Triantafyllidis, Christopher McGreavy, Jiacheng Gu and\n  Zhibin Li", "title": "Multimodal Interfaces for Effective Teleoperation", "comments": "15 pages, 28 figures, 5 tables, 5 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in multi-modal interfaces aims to provide solutions to immersion and\nincrease overall human performance. A promising direction is combining\nauditory, visual and haptic interaction between the user and the simulated\nenvironment. However, no extensive comparisons exist to show how combining\naudiovisuohaptic interfaces affects human perception reflected on task\nperformance. Our paper explores this idea. We present a thorough,\nfull-factorial comparison of how all combinations of audio, visual and haptic\ninterfaces affect performance during manipulation. We evaluate how each\ninterface combination affects performance in a study (N=25) consisting of\nmanipulating tasks of varying difficulty. Performance is assessed using both\nsubjective, assessing cognitive workload and system usability, and objective\nmeasurements, incorporating time and spatial accuracy-based metrics. Results\nshow that regardless of task complexity, using stereoscopic-vision with the\nVRHMD increased performance across all measurements by 40% compared to\nmonocular-vision from the display monitor. Using haptic feedback improved\noutcomes by 10% and auditory feedback accounted for approximately 5%\nimprovement.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 17:36:55 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Triantafyllidis", "Eleftherios", ""], ["McGreavy", "Christopher", ""], ["Gu", "Jiacheng", ""], ["Li", "Zhibin", ""]]}]