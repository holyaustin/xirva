[{"id": "1810.00028", "submitter": "Aniket Bera", "authors": "Aniket Bera, Tanmay Randhavane, Emily Kubin, Husam Shaik, Kurt Gray,\n  Dinesh Manocha", "title": "Data-Driven Modeling of Group Entitativity in Virtual Environments", "comments": "Accepted at VRST 2018, November 28-December 1, 2018, Tokyo, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven algorithm to model and predict the socio-emotional\nimpact of groups on observers. Psychological research finds that highly\nentitative i.e. cohesive and uniform groups induce threat and unease in\nobservers. Our algorithm models realistic trajectory-level behaviors to\nclassify and map the motion-based entitativity of crowds. This mapping is based\non a statistical scheme that dynamically learns pedestrian behavior and\ncomputes the resultant entitativity induced emotion through group motion\ncharacteristics. We also present a novel interactive multi-agent simulation\nalgorithm to model entitative groups and conduct a VR user study to validate\nthe socio-emotional predictive power of our algorithm. We further show that\nmodel-generated high-entitativity groups do induce more negative emotions than\nlow-entitative groups.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 18:23:39 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Bera", "Aniket", ""], ["Randhavane", "Tanmay", ""], ["Kubin", "Emily", ""], ["Shaik", "Husam", ""], ["Gray", "Kurt", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1810.00161", "submitter": "Aoyu Wu", "authors": "Aoyu Wu and Bon Kyung Ku and Furui Cheng and Xinhuan Shu and Abishek\n  Puri and Yifang Wang and Huamin Qu", "title": "Pulse: Toward a Smart Campus by Communicating Real-time Wi-Fi Access\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enhance the mobility and convenience of the campus community, we designed\nand implemented the Pulse system, a visual interface for communicating the\ncrowd information to the lay public including campus members and visitors. This\nis a challenging task which requires analyzing and reconciling the demands and\ninterests for data as well as visual design among diverse target audiences.\nThrough an iterative design progress, we study and address the diverse\npreferences of the lay audiences, whereby design rationales are distilled. The\nfinal prototype combines a set of techniques such as chart junk and redundancy\nencoding. Initial feedback from a wide audience confirms the benefits and\nattractiveness of the system.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 06:49:11 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Wu", "Aoyu", ""], ["Ku", "Bon Kyung", ""], ["Cheng", "Furui", ""], ["Shu", "Xinhuan", ""], ["Puri", "Abishek", ""], ["Wang", "Yifang", ""], ["Qu", "Huamin", ""]]}, {"id": "1810.00267", "submitter": "Wieslaw Kopec", "authors": "Kinga Skorupska, Manuel Nu\\~nez, Wies{\\l}aw Kope\\'c, Rados{\\l}aw\n  Nielek", "title": "Older Adults and Crowdsourcing: Android TV App for Evaluating TEDx\n  Subtitle Quality", "comments": null, "journal-ref": null, "doi": "10.1145/3274428", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the insights from an exploratory qualitative pilot\nstudy testing the feasibility of a solution that would encourage older adults\nto participate in online crowdsourcing tasks in a non-computer scenario.\nTherefore, we developed an Android TV application using Amara API to retrieve\nsubtitles for TEDx talks which allows the participants to detect and categorize\nerrors to support the quality of the translation and transcription processes.\nIt relies on the older adults' innate skills as long-time native language users\nand the motivating factors of this socially and personally beneficial task. The\nstudy allowed us to verify the underlying concept of using Smart TVs as\ninterfaces for crowdsourcing, as well as possible barriers, including the\ninterface, configuration issues, topics and the process itself. We have also\nassessed the older adults' interaction and engagement with this TV-enabled\nonline crowdsourcing task and we are convinced that the design of our setup\naddresses some key barriers to crowdsourcing by older adults. It also validates\navenues for further research in this area focused on such considerations as\nautonomy and freedom of choice, familiarity, physical and cognitive comfort as\nwell as building confidence and the edutainment value.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 21:18:42 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Skorupska", "Kinga", ""], ["Nu\u00f1ez", "Manuel", ""], ["Kope\u0107", "Wies\u0142aw", ""], ["Nielek", "Rados\u0142aw", ""]]}, {"id": "1810.00358", "submitter": "Joni Salminen", "authors": "Joni Salminen and Bernard J. Jansen", "title": "Use Cases and Outlooks for Automatic Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The landscape of analytics is changing rapidly. Much of online user\nanalytics, however, is based on collection of various user analytics numbers.\nUnderstanding these numbers, and then relating them to higher numerical\nanalysis for the evaluation of key performance indicators (KPIs) can be quite\nchallenging, especially with large volumes of data. There is a plethora of\ntools and software packages that one can employ. However, these tools and\npackages require a quantitative competence and analytical sophistication that\naverage end users often do not possess. Additionally, they often do little to\nreduce the complexity of numerical data in a manner that allows ease of use in\ndecision making and communication. Dealing with numbers poses cognitive\nchallenges for individuals who often do cannot recall many numbers at a time.\nHere, we explore the concept of automatic analytics by demonstrating use case\nexamples and discussion on the current state and future of automated insights.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 11:25:49 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Salminen", "Joni", ""], ["Jansen", "Bernard J.", ""]]}, {"id": "1810.00462", "submitter": "Longsheng Jiang", "authors": "Longsheng Jiang, Yue Wang", "title": "A Human-Computer Interface Design for Quantitative Measure of Regret\n  Theory", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regret theory is a theory that describes human decision-making under risk.\nThe key of obtaining a quantitative model of regret theory is to measure the\npreference in humans' mind when they choose among a set of options. Unlike\nphysical quantities, measuring psychological preference is not procedure\ninvariant, i.e. the readings alter when the methods change. In this work, we\nalleviate this influence by choosing the procedure compatible with the way that\nan individual makes a choice. We believe the resulting model is closer to the\nnature of human decision-making. The preference elicitation process is\ndecomposed into a series of short surveys to reduce cognitive workload and\nincrease response accuracy. To make the questions natural and familiar to the\nsubjects, we follow the insight that humans generate, quantify and communicate\npreference in natural language. The fuzzy-set theory is hence utilized to model\nresponses from subjects. Based on these ideas, a graphical human-computer\ninterface (HCI) is designed to articulate the information as well as to\nefficiently collect human responses. The design also accounts for human\nheuristics and biases, e.g. range effect and anchoring effect, to enhance its\nreliability. The overall performance of the survey is satisfactory because the\nmeasured model shows prediction accuracy equivalent to the revisit-performance\nof the subjects.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 20:37:22 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Jiang", "Longsheng", ""], ["Wang", "Yue", ""]]}, {"id": "1810.00609", "submitter": "Anbumani Subramanian", "authors": "Adithya Subramanian, Anbumani Subramanian", "title": "One-Click Annotation with Guided Hierarchical Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The increase in data collection has made data annotation an interesting and\nvaluable task in the contemporary world. This paper presents a new methodology\nfor quickly annotating data using click-supervision and hierarchical object\ndetection. The proposed work is semi-automatic in nature where the task of\nannotations is split between the human and a neural network. We show that our\nimproved method of annotation reduces the time, cost and mental stress on a\nhuman annotator. The research also highlights how our method performs better\nthan the current approach in different circumstances such as variation in\nnumber of objects, object size and different datasets. Our approach also\nproposes a new method of using object detectors making it suitable for data\nannotation task. The experiment conducted on PASCAL VOC dataset revealed that\nannotation created from our approach achieves a mAP of 0.995 and a recall of\n0.903. The Our Approach has shown an overall improvement by 8.5%, 18.6% in mean\naverage precision and recall score for KITTI and 69.6%, 36% for CITYSCAPES\ndataset. The proposed framework is 3-4 times faster as compared to the standard\nannotation method.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 10:41:35 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Subramanian", "Adithya", ""], ["Subramanian", "Anbumani", ""]]}, {"id": "1810.00931", "submitter": "Alessandro Piscopo", "authors": "Alessandro Piscopo", "title": "Wikidata: A New Paradigm of Human-Bot Collaboration?", "comments": "Submitted to the workshop 'The Changing Contours of \"Participation\"\n  in Data-driven, Algorithmic Ecosystems: Challenges, Tactics, and an Agenda',\n  at CSCW 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Wikidata is a collaborative knowledge graph which has already drawn the\nattention of practitioners and researchers. It is the work of a community of\nvolunteers, supported by policies, guidelines and automatic programs (bots)\nwhich perform a broad range of tasks, doing the lion's share of the work on the\nplatform. In this paper, we highlight some of the most salient aspects of\nhuman-bot collaboration in Wikidata. We argue that the combination of automated\nand semi-automated work produces new challenges with respect to other online\ncollaboration platforms.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 19:39:19 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Piscopo", "Alessandro", ""]]}, {"id": "1810.01007", "submitter": "Stuart Reeves", "authors": "Stuart Reeves", "title": "How UX Practitioners Produce Findings in Usability Testing", "comments": "Pre-print accepted to ACM Transactions on Computer-Human Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usability testing has long been a core interest of HCI research and forms a\nkey element of industry practice. Yet our knowledge of it harbours striking\nabsences. There are few, if any detailed accounts of the contingent, material\nways in which usability testing is actually practiced. Further, it is rare that\nindustry practitioners' testing work is treated as indigenous and particular\n(instead subordinated as a `compromised' version). To service these problems,\nthis paper presents an ethnomethodological study of usability testing practices\nin a design consultancy. It unpacks how findings are produced in and as the\nwork of observers analysing the test as it unfolds between moderators taking\nparticipants through relevant tasks. The study nuances conventional views of\nusability findings as straightforwardly `there to be found' or `read off' by\ncompetent evaluators. It explores how evaluators / observers collaboratively\nwork to locate relevant troubles in the test's unfolding. However, in the\ncourse of doing this work, potential candidate troubles may also routinely be\ndissipated and effectively `ignored' in one way or another. The implications of\nthe study suggest refinements to current understandings of usability\nevaluations, and affirm the value to HCI in studying industry practitioners\nmore deeply.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 23:02:50 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 16:07:38 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Reeves", "Stuart", ""]]}, {"id": "1810.01070", "submitter": "Kazutaka Kurihara", "authors": "Kazutaka Kurihara, Nobuhiro Doi", "title": "GameControllerizer: Middleware to Program Inputs for Augmenting Digital\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes middleware, GameControllerizer, that allows users to\ncombine the processes of Internet of Things (IoT) devices, Web services, and\napplications of Artificial Intelligence (AI), and to convert them into game\ncontrol operations to augment existing digital games. The system facilitates\neasy trial-and-error development of new forms of entertainment and the\nconfiguration of gamification by enabling the use of diverse devices and\nsources of information as inputs to games. GameControllerizer consists of a\nvisual programming element that uses the Node-RED tool to allow users to\nprogram easily to convert diverse formats of information into inputs to games,\nand contains a game input emulation element whereby hardware- and\nsoftware-based emulation generates inputs for gaming devices. Evidence of the\nusefulness of the system was provided by a performance assessment and the\nproposal of a variety of use cases.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 05:10:50 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Kurihara", "Kazutaka", ""], ["Doi", "Nobuhiro", ""]]}, {"id": "1810.01835", "submitter": "Lex Fridman", "authors": "Lex Fridman", "title": "Human-Centered Autonomous Vehicle Systems: Principles of Effective\n  Shared Autonomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building effective, enjoyable, and safe autonomous vehicles is a lot harder\nthan has historically been considered. The reason is that, simply put, an\nautonomous vehicle must interact with human beings. This interaction is not a\nrobotics problem nor a machine learning problem nor a psychology problem nor an\neconomics problem nor a policy problem. It is all of these problems put into\none. It challenges our assumptions about the limitations of human beings at\ntheir worst and the capabilities of artificial intelligence systems at their\nbest. This work proposes a set of principles for designing and building\nautonomous vehicles in a human-centered way that does not run away from the\ncomplexity of human nature but instead embraces it. We describe our development\nof the Human-Centered Autonomous Vehicle (HCAV) as an illustrative case study\nof implementing these principles in practice.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 16:36:22 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Fridman", "Lex", ""]]}, {"id": "1810.01926", "submitter": "Mark Goadrich", "authors": "Mark Goadrich and James Droscha", "title": "Improving Solvability for Procedurally Generated Challenges in Physical\n  Solitaire Games Through Entangled Components", "comments": "10 pages, 19 figures. Accepted to IEEE Transactions on Games, Early\n  Access 5/24/2019", "journal-ref": null, "doi": "10.1109/TG.2019.2918223", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Challenges for physical solitaire puzzle games are typically designed in\nadvance by humans and limited in number. Alternatively, some games incorporate\nrules for stochastic setup, where the human solver randomly sets up the game\nboard before solving the challenge. These setup rules greatly increase the\nnumber of possible challenges, but can often generate unsolvable or\nuninteresting challenges. To better understand the compromises involved in\nminimizing undesirable challenges, we examine three games where component\ndesign choices can influence the stochastic nature of the resulting challenge\ngeneration algorithms. We evaluate the effect of these components and\nalgorithms on challenge solvability and challenge engagement. We find that\nalgorithms which control randomness through entangling components based on\nsub-elements of the puzzle mechanics can generate interesting challenges with a\nhigh probability of being solvable.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 19:35:44 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 20:21:11 GMT"}, {"version": "v3", "created": "Sun, 26 May 2019 03:09:43 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Goadrich", "Mark", ""], ["Droscha", "James", ""]]}, {"id": "1810.02017", "submitter": "Matthew Marge", "authors": "Matthew Marge, Claire Bonial, Stephanie Lukin, Cory Hayes, Ashley\n  Foots, Ron Artstein, Cassidy Henry, Kimberly Pollard, Carla Gordon, Felix\n  Gervits, Anton Leuski, Susan Hill, Clare Voss and David Traum", "title": "Balancing Efficiency and Coverage in Human-Robot Dialogue Collection", "comments": "Presented at AI-HRI AAAI-FSS, 2018 (arXiv:1809.06606)", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2018/01", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a multi-phased Wizard-of-Oz approach to collecting human-robot\ndialogue in a collaborative search and navigation task. The data is being used\nto train an initial automated robot dialogue system to support collaborative\nexploration tasks. In the first phase, a wizard freely typed robot utterances\nto human participants. For the second phase, this data was used to design a GUI\nthat includes buttons for the most common communications, and templates for\ncommunications with varying parameters. Comparison of the data gathered in\nthese phases show that the GUI enabled a faster pace of dialogue while still\nmaintaining high coverage of suitable responses, enabling more efficient\ntargeted data collection, and improvements in natural language understanding\nusing GUI-collected data. As a promising first step towards interactive\nlearning, this work shows that our approach enables the collection of useful\ntraining data for navigation-based HRI tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 01:27:02 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 20:32:43 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Marge", "Matthew", ""], ["Bonial", "Claire", ""], ["Lukin", "Stephanie", ""], ["Hayes", "Cory", ""], ["Foots", "Ashley", ""], ["Artstein", "Ron", ""], ["Henry", "Cassidy", ""], ["Pollard", "Kimberly", ""], ["Gordon", "Carla", ""], ["Gervits", "Felix", ""], ["Leuski", "Anton", ""], ["Hill", "Susan", ""], ["Voss", "Clare", ""], ["Traum", "David", ""]]}, {"id": "1810.02137", "submitter": "Chengzheng Sun", "authors": "Chengzheng Sun, David Sun, Agustina, and Weiwei Cai", "title": "Real Differences between OT and CRDT for Co-Editors", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OT (Operational Transformation) was invented for supporting real-time\nco-editors in the late 1980s and has evolved to become a core technique used in\ntoday's working co-editors and adopted in major industrial products. CRDT\n(Commutative Replicated Data Type) for co-editors was first proposed around\n2006, under the name of WOOT (WithOut Operational Transformation). Follow-up\nCRDT variations are commonly labeled as \"post-OT\" techniques capable of making\nconcurrent operations natively commutative, and have made broad claims of\nsuperiority over OT solutions, in terms of correctness, time and space\ncomplexity, simplicity, etc. Over one decade later, however, CRDT solutions are\nrarely found in working co-editors, while OT solutions remain the choice for\nbuilding the vast majority of co-editors. Contradictions between this reality\nand CRDT's purported advantages have been the source of much debate and\nconfusion in co-editing research and developer communities. What is CRDT really\nto co-editing? What are the real differences between OT and CRDT for\nco-editors? What are the key factors that may have affected the adoption of and\nchoice between OT and CRDT for co-editors in the real world? In this paper, we\nreport our discoveries, in relation to these questions and beyond, from a\ncomprehensive review and comparison study on representative OT and CRDT\nsolutions and working co-editors based on them. Moreover, this work reveals\nfacts and presents evidences that refute CRDT claimed advantages over OT. We\nhope the results reported in this paper will help clear up common myths,\nmisconceptions, and confusions surrounding alternative co-editing techniques,\nand accelerate progress in co-editing technology for real world applications.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 10:22:06 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Sun", "Chengzheng", ""], ["Sun", "David", ""], ["Agustina", "", ""], ["Cai", "Weiwei", ""]]}, {"id": "1810.02223", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Lina Yao, Chaoran Huang, Salil S. Kanhere, Dalin Zhang,\n  Yu Zhang", "title": "Brain2Object: Printing Your Mind from Brain Signals with Spatial\n  Correlation Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) signals are known to manifest differential\npatterns when individuals visually concentrate on different objects. In this\nwork, we present an end-to-end digital fabrication system, Brain2Object, to\nprint the 3D object that an individual is observing by decoding visually-evoked\nbrain signals. We propose a unified training framework that combines\nmulti-class Common Spatial Pattern and Convolutional Neural Networks to support\nthe backend computation. We learn the dynamical graph representations of brain\nsignals to accurately capture the structural information among EEG channels. A\nuser-friendly interface is developed as the system front end. Brain2Object\npresents a streamlined end-to-end workflow that can serve as a template for\ndeeper integration of BCI technologies to assist with our routine activities.\n  The proposed system is evaluated extensively using offline experiments and\nthrough an online demonstrator. The experimental results show that our approach\ncan achieve the recognition accuracy of 92.58% on a benchmark dataset and\n75.23% on a locally collected dataset. Moreover, our method consistently\noutperforms a wide range of baseline and state-of-the-art approaches. The\nproof-of-concept corroborates the practicality of our approach and illustrates\nthe ease with which such a system could be deployed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 14:00:51 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 02:02:45 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 03:31:20 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Zhang", "Xiang", ""], ["Yao", "Lina", ""], ["Huang", "Chaoran", ""], ["Kanhere", "Salil S.", ""], ["Zhang", "Dalin", ""], ["Zhang", "Yu", ""]]}, {"id": "1810.02251", "submitter": "Michael Green", "authors": "Michael Cerny Green, Gabriella A.B. Barros, Antonios Liapis, Julian\n  Togelius", "title": "DATA Agent", "comments": "8 pages, 4 images, 3 tables", "journal-ref": "Foundations of Digital Games (FDG) 2018", "doi": "10.1145/3235765.3235792", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces DATA Agent, a system which creates murder mystery\nadventures from open data. In the game, the player takes on the role of a\ndetective tasked with finding the culprit of a murder. All characters, places,\nand items in DATA Agent games are generated using open data as source content.\nThe paper discusses the general game design and user interface of DATA Agent,\nand provides details on the generative algorithms which transform linked data\ninto different game objects. Findings from a user study with 30 participants\nplaying through two games of DATA Agent show that the game is easy and fun to\nplay, and that the mysteries it generates are straightforward to solve.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:36:19 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Green", "Michael Cerny", ""], ["Barros", "Gabriella A. B.", ""], ["Liapis", "Antonios", ""], ["Togelius", "Julian", ""]]}, {"id": "1810.02364", "submitter": "Alexey Shvets", "authors": "Roman A. Solovyev, Maxim Vakhrushev, Alexander Radionov, Vladimir\n  Aliev and Alexey A. Shvets", "title": "Deep Learning Approaches for Understanding Simple Speech Commands", "comments": "12 page, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic classification of sound commands is becoming increasingly\nimportant, especially for mobile and embedded devices. Many of these devices\ncontain both cameras and microphones, and companies that develop them would\nlike to use the same technology for both of these classification tasks. One way\nof achieving this is to represent sound commands as images, and use\nconvolutional neural networks when classifying images as well as sounds. In\nthis paper we consider several approaches to the problem of sound\nclassification that we applied in TensorFlow Speech Recognition Challenge\norganized by Google Brain team on the Kaggle platform. Here we show different\nrepresentation of sounds (Wave frames, Spectrograms, Mel-Spectrograms, MFCCs)\nand apply several 1D and 2D convolutional neural networks in order to get the\nbest performance. Our experiments show that we found appropriate sound\nrepresentation and corresponding convolutional neural networks. As a result we\nachieved good classification accuracy that allowed us to finish the challenge\non 8-th place among 1315 teams.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 17:42:18 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Solovyev", "Roman A.", ""], ["Vakhrushev", "Maxim", ""], ["Radionov", "Alexander", ""], ["Aliev", "Vladimir", ""], ["Shvets", "Alexey A.", ""]]}, {"id": "1810.02391", "submitter": "Selim Kalayci", "authors": "Selim Kalayc{\\i}, \\c{C}a\\u{g}atay Demiralp, Zeynep H. G\\\"um\\\"u\\c{s}", "title": "Developing Design Guidelines for Precision Oncology Reports", "comments": "main text (4 pages) including 2 figures, plus 4 additional\n  supplementary documents merged in a single PDF file", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Precision oncology tests that profile tumors to identify clinically\nactionable targets have rapidly entered clinical practice. Effective visual\npresentation of the results of these tests is crucial in accurate clinical\ndecision-making. In current practice, these results are typically delivered to\noncologists as static prints, who then incorporate them into their clinical\ndecision-making process. However, due to a lack of guidelines for\nstandardization, different vendors use different report formats. There is very\nlittle known on the effectiveness of these report formats or the criteria\nnecessary to improve them. In this study, we have aimed to identify both the\ntasks and the needs of oncologists from precision oncology report design and\nthen to improve the designs based on these findings. To this end, we report\nresults from multiple interviews and a survey study (n=32) conducted with\npracticing oncologists. Based on these results, we compiled a set of design\ncriteria for precision oncology reports and developed a prototype report design\nusing these criteria, along with feedback from oncologists.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 18:49:11 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Kalayc\u0131", "Selim", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["G\u00fcm\u00fc\u015f", "Zeynep H.", ""]]}, {"id": "1810.02445", "submitter": "Florian Heimerl", "authors": "Florian Heimerl, Chih-Ching Chang, Alper Sarikaya, Michael Gleicher", "title": "Visual Designs for Binned Aggregation of Multi-Class Scatterplots", "comments": "to be submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point sets in 2D with multiple classes are a common type of data. A canonical\nvisualization design for them are scatterplots, which do not scale to large\ncollections of points. For these larger data sets, binned aggregation (or\nbinning) is often used to summarize the data, with many possible design\nalternatives for creating effective visual representations of these summaries.\nThere are a wide range of designs to show summaries of 2D multi-class point\ndata, each capable of supporting different analysis tasks. In this paper, we\nexplore the space of visual designs for such data, and provide design\nguidelines for different analysis scenarios. To support these guidelines, we\ncompile a set of abstract tasks and ground them in concrete examples using\nmultiple sample datasets. We then assess designs, and survey a range of design\ndecisions, considering their appropriateness to the tasks. In addition, we\nprovide a web-based implementation to experiment with design choices,\nsupporting the validation of designs based on task needs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 22:32:26 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 19:51:34 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Heimerl", "Florian", ""], ["Chang", "Chih-Ching", ""], ["Sarikaya", "Alper", ""], ["Gleicher", "Michael", ""]]}, {"id": "1810.02496", "submitter": "Georgios Portokalidis", "authors": "Dimitrios Damopoulos and Georgios Portokalidis", "title": "Hands-Free One-Time and Continuous Authentication Using Glass Wearable\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users with limited use of their hands, such as people suffering from\ndisabilities of the arm, shoulder, and hand (DASH), face challenges when\nauthenticating with computer terminals, specially with publicly accessible\nterminals such as ATMs. A new glass wearable device was recently introduced by\nGoogle and it was immediately welcomed by groups of users, such as the ones\ndescribed above, as Google Glass allows them to perform actions, like taking a\nphoto, using only verbal commands. This paper investigates whether glass\nwearable devices can be used to authenticate users, both to grant access\n(one-time) and to maintain access (continuous), in similar hands-free fashion.\nWe do so by designing and implementing Gauth, a system that enables users to\nauthenticate with a service simply by issuing a voice command, while facing the\ncomputer terminal they are going to use to access the service. To achieve this\ngoal, we create a physical communication channel from the terminal to the\ndevice using machine readable visual codes, like QR codes, and utilize the\ndevice's network adapter to communicate directly with a service. More\nimportantly, we continuously authenticate the user accessing the terminal,\nexploiting the fact that a user operating a terminal is most likely facing it\nmost of the time. We periodically issue authentication challenges, which are\ndisplayed as a QR code on the terminal, that cause the glass device to\nre-authenticate the user with an appropriate response. We evaluate our system\nto determine the technical limits of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 02:39:43 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Damopoulos", "Dimitrios", ""], ["Portokalidis", "Georgios", ""]]}, {"id": "1810.02842", "submitter": "Kuan-Jung Chiang", "authors": "Kuan-Jung Chiang, Chun-Shu Wei, Masaki Nakanishi, and Tzyy-Ping Jung", "title": "Cross-Subject Transfer Learning Improves the Practicality of Real-World\n  Applications of Brain-Computer Interfaces", "comments": "4 pages, 3 figures, 1 table. For NER'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steady-state visual evoked potential (SSVEP)-based brain-computer interfaces\n(BCIs) have shown its robustness in facilitating high-efficiency communication.\nState-of-the-art training-based SSVEP decoding methods such as extended\nCanonical Correlation Analysis (CCA) and Task-Related Component Analysis (TRCA)\nare the major players that elevate the efficiency of the SSVEP-based BCIs\nthrough a calibration process. However, due to notable human variability across\nindividuals and within individuals over time, calibration (training) data\ncollection is non-negligible and often laborious and time-consuming,\ndeteriorating the practicality of SSVEP BCIs in a real-world context. This\nstudy aims to develop a cross-subject transferring approach to reduce the need\nfor collecting training data from a test user with a newly proposed\nleast-squares transformation (LST) method. Study results show the capability of\nthe LST in reducing the number of training templates required for a 40-class\nSSVEP BCI. The LST method may lead to numerous real-world applications using\nnear-zero-training/plug-and-play high-speed SSVEP BCIs.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 18:33:54 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 22:32:08 GMT"}, {"version": "v3", "created": "Sat, 3 Nov 2018 05:16:51 GMT"}, {"version": "v4", "created": "Wed, 13 Mar 2019 21:14:29 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Chiang", "Kuan-Jung", ""], ["Wei", "Chun-Shu", ""], ["Nakanishi", "Masaki", ""], ["Jung", "Tzyy-Ping", ""]]}, {"id": "1810.03019", "submitter": "Alex Bigelow", "authors": "Alex Bigelow, Megan Monroe", "title": "Jacob's Ladder: The User Implications of Leveraging Graph Pivots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on a simple visual technique that boils extracting a\nsubgraph down to two operations---pivots and filters---that is agnostic to both\nthe data abstraction, and its visual complexity scales independent of the size\nof the graph. The system's design, as well as its qualitative evaluation with\nusers, clarifies exactly when and how the user's intent in a series of pivots\nis ambiguous---and, more usefully, when it is not. Reflections on our results\nshow how, in the event of an ambiguous case, this innately practical operation\ncould be further extended into \"smart pivots\" that anticipate the user's intent\nbeyond the current step. They also reveal ways that a series of graph pivots\ncan expose the semantics of the data from the user's perspective, and how this\ninformation could be leveraged to create adaptive data abstractions that do not\nrely as heavily on a system designer to create a comprehensive abstraction that\nanticipates all the user's tasks.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 16:35:45 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 15:29:09 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Bigelow", "Alex", ""], ["Monroe", "Megan", ""]]}, {"id": "1810.03031", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano", "title": "Text-based Sentiment Analysis and Music Emotion Recognition", "comments": "Ph.D. Thesis", "journal-ref": null, "doi": "10.6092/polito/porto/2709436", "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sentiment polarity of tweets, blog posts or product reviews has become highly\nattractive and is utilized in recommender systems, market predictions, business\nintelligence and more. Deep learning techniques are becoming top performers on\nanalyzing such texts. There are however several problems that need to be solved\nfor efficient use of deep neural networks on text mining and text polarity\nanalysis. First, deep neural networks need to be fed with data sets that are\nbig in size as well as properly labeled. Second, there are various\nuncertainties regarding the use of word embedding vectors: should they be\ngenerated from the same data set that is used to train the model or it is\nbetter to source them from big and popular collections? Third, to simplify\nmodel creation it is convenient to have generic neural network architectures\nthat are effective and can adapt to various texts, encapsulating much of design\ncomplexity. This thesis addresses the above problems to provide methodological\nand practical insights for utilizing neural networks on sentiment analysis of\ntexts and achieving state of the art results. Regarding the first problem, the\neffectiveness of various crowdsourcing alternatives is explored and two\nmedium-sized and emotion-labeled song data sets are created utilizing social\ntags. To address the second problem, a series of experiments with large text\ncollections of various contents and domains were conducted, trying word\nembeddings of various parameters. Regarding the third problem, a series of\nexperiments involving convolution and max-pooling neural layers were conducted.\nCombining convolutions of words, bigrams, and trigrams with regional\nmax-pooling layers in a couple of stacks produced the best results. The derived\narchitecture achieves competitive performance on sentiment polarity analysis of\nmovie, business and product reviews.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 17:42:19 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["\u00c7ano", "Erion", ""]]}, {"id": "1810.03145", "submitter": "Ruohan Wang", "authors": "Ruohan Wang and Pierluigi V. Amadori and Yiannis Demiris", "title": "Real-Time Workload Classification during Driving using HyperNetworks", "comments": "2018 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying human cognitive states from behavioral and physiological signals\nis a challenging problem with important applications in robotics. The problem\nis challenging due to the data variability among individual users, and sensor\nartefacts. In this work, we propose an end-to-end framework for real-time\ncognitive workload classification with mixture Hyper Long Short Term Memory\nNetworks, a novel variant of HyperNetworks. Evaluating the proposed approach on\nan eye-gaze pattern dataset collected from simulated driving scenarios of\ndifferent cognitive demands, we show that the proposed framework outperforms\nprevious baseline methods and achieves 83.9\\% precision and 87.8\\% recall\nduring test. We also demonstrate the merit of our proposed architecture by\nshowing improved performance over other LSTM-based methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 13:57:25 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Wang", "Ruohan", ""], ["Amadori", "Pierluigi V.", ""], ["Demiris", "Yiannis", ""]]}, {"id": "1810.03163", "submitter": "James Bagrow", "authors": "Daniel Berenberg and James P. Bagrow", "title": "Efficient Crowd Exploration of Large Networks: The Case of Causal\n  Attribution", "comments": "25 pages, 14 figures, in CSCW'18", "journal-ref": "PACM on Human-Computer Interaction, Vol. 2, No. CSCW, Article 24.\n  Publication date: November 2018", "doi": "10.1145/3274293", "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately and efficiently crowdsourcing complex, open-ended tasks can be\ndifficult, as crowd participants tend to favor short, repetitive \"microtasks\".\nWe study the crowdsourcing of large networks where the crowd provides the\nnetwork topology via microtasks. Crowds can explore many types of social and\ninformation networks, but we focus on the network of causal attributions, an\nimportant network that signifies cause-and-effect relationships. We conduct\nexperiments on Amazon Mechanical Turk (AMT) testing how workers propose and\nvalidate individual causal relationships and introduce a method for independent\ncrowd workers to explore large networks. The core of the method, Iterative\nPathway Refinement, is a theoretically-principled mechanism for efficient\nexploration via microtasks. We evaluate the method using synthetic networks and\napply it on AMT to extract a large-scale causal attribution network, then\ninvestigate the structure of this network as well as the activity patterns and\nefficiency of the workers who constructed this network. Worker interactions\nreveal important characteristics of causal perception and the network data they\ngenerate can improve our understanding of causality and causal inference.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 15:43:49 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Berenberg", "Daniel", ""], ["Bagrow", "James P.", ""]]}, {"id": "1810.03428", "submitter": "Federica Turi", "authors": "Federica Turi (ATHENA, UCA), Nathalie Gayraud (ATHENA, UCA), Maureen\n  Clerc (ATHENA, UCA)", "title": "Zero-calibration cVEP BCI using word prediction: a proof of concept", "comments": "BCI 2018 - 7th International BCI Meeting, May 2018, Pacific Grove,\n  California, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain Computer Interfaces (BCIs) based on visual evoked potentials (VEP)\nallow for spelling from a keyboard of flashing characters. Among VEP BCIs,\ncode-modulated visual evoked potentials (c-VEPs) are designed for high-speed\ncommunication . In c-VEPs, all characters flash simultaneously. In particular,\neach character flashes according to a predefined 63-bit binary sequence\n(m-sequence), circular-shifted by a different time lag. For a given character,\nthe m-sequence evokes a VEP in the electroencephalogram (EEG) of the subject,\nwhich can be used as a template. This template is obtained during a calibration\nphase at the beginning of each session. Then, the system outputs the desired\ncharacter after a predefined number of repetitions by estimating its time lag\nwith respect to the template. Our work avoids the calibration phase, by\nextracting from the VEP relative lags between successive characters, and\npredicting the full word using a dictionary.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 06:25:56 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Turi", "Federica", "", "ATHENA, UCA"], ["Gayraud", "Nathalie", "", "ATHENA, UCA"], ["Clerc", "Maureen", "", "ATHENA, UCA"]]}, {"id": "1810.03856", "submitter": "Rufin VanRullen", "authors": "Rufin VanRullen and Leila Reddy", "title": "Reconstructing Faces from fMRI Patterns using Deep Generative Neural\n  Networks", "comments": null, "journal-ref": "Commun Biol 2, 193 (2019)", "doi": "10.1038/s42003-019-0438-y", "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While objects from different categories can be reliably decoded from fMRI\nbrain response patterns, it has proved more difficult to distinguish visually\nsimilar inputs, such as different instances of the same category. Here, we\napply a recently developed deep learning system to the reconstruction of face\nimages from human fMRI patterns. We trained a variational auto-encoder (VAE)\nneural network using a GAN (Generative Adversarial Network) unsupervised\ntraining procedure over a large dataset of celebrity faces. The auto-encoder\nlatent space provides a meaningful, topologically organized 1024-dimensional\ndescription of each image. We then presented several thousand face images to\nhuman subjects, and learned a simple linear mapping between the multi-voxel\nfMRI activation patterns and the 1024 latent dimensions. Finally, we applied\nthis mapping to novel test images, turning the obtained fMRI patterns into VAE\nlatent codes, and ultimately the codes into face reconstructions. Qualitative\nand quantitative evaluation of the reconstructions revealed robust pairwise\ndecoding (>95% correct), and a strong improvement relative to a baseline model\n(PCA decomposition). Furthermore, this brain decoding model can readily be\nrecycled to probe human face perception along many dimensions of interest; for\nexample, the technique allowed for accurate gender classification, and even to\ndecode which face was imagined, rather than seen by the subject. We hypothesize\nthat the latent space of modern deep learning generative models could serve as\na valid approximation for human brain representations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 08:40:53 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 22:37:44 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["VanRullen", "Rufin", ""], ["Reddy", "Leila", ""]]}, {"id": "1810.03913", "submitter": "Shixia Liu", "authors": "Mengchen Liu, Shixia Liu, Hang Su, Kelei Cao, Jun Zhu", "title": "Analyzing the Noise Robustness of Deep Neural Networks", "comments": "IEEE VAST 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to maliciously generated\nadversarial examples. These examples are intentionally designed by making\nimperceptible perturbations and often mislead a DNN into making an incorrect\nprediction. This phenomenon means that there is significant risk in applying\nDNNs to safety-critical applications, such as driverless cars. To address this\nissue, we present a visual analytics approach to explain the primary cause of\nthe wrong predictions introduced by adversarial examples. The key is to analyze\nthe datapaths of the adversarial examples and compare them with those of the\nnormal examples. A datapath is a group of critical neurons and their\nconnections. To this end, we formulate the datapath extraction as a subset\nselection problem and approximately solve it based on back-propagation. A\nmulti-level visualization consisting of a segmented DAG (layer level), an Euler\ndiagram (feature map level), and a heat map (neuron level), has been designed\nto help experts investigate datapaths from the high-level layers to the\ndetailed neuron activations. Two case studies are conducted that demonstrate\nthe promise of our approach in support of explaining the working mechanism of\nadversarial examples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 11:13:44 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Liu", "Mengchen", ""], ["Liu", "Shixia", ""], ["Su", "Hang", ""], ["Cao", "Kelei", ""], ["Zhu", "Jun", ""]]}, {"id": "1810.03970", "submitter": "Alexander Prange", "authors": "Alexander Prange, Michael Barz, Daniel Sonntag", "title": "A categorisation and implementation of digital pen features for\n  behaviour characterisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a categorisation and implementation of digital ink\nfeatures for behaviour characterisation. Based on four feature sets taken from\nliterature, we provide a categorisation in different classes of syntactic and\nsemantic features. We implemented a publicly available framework to calculate\nthese features and show its deployment in the use case of analysing cognitive\nassessments performed using a digital pen.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 14:24:20 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Prange", "Alexander", ""], ["Barz", "Michael", ""], ["Sonntag", "Daniel", ""]]}, {"id": "1810.04058", "submitter": "Ian Xiao", "authors": "Ian Xiao", "title": "A Distributed Reinforcement Learning Solution With Knowledge Transfer\n  Capability for A Bike Rebalancing Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rebalancing is a critical service bottleneck for many transportation\nservices, such as Citi Bike. Citi Bike relies on manual orchestrations of\nrebalancing bikes between dispatchers and field agents. Motivated by such\nproblem and the lack of smart autonomous solutions in this area, this project\nexplored a new RL architecture called Distributed RL (DiRL) with Transfer\nLearning (TL) capability. The DiRL solution is adaptive to changing traffic\ndynamics when keeping bike stock under control at the minimum cost. DiRL\nachieved a 350% improvement in bike rebalancing autonomously and TL offered a\n62.4% performance boost in managing an entire bike network. Lastly, a field\ntrip to the dispatch office of Chariot, a ride-sharing service, provided\ninsights to overcome challenges of deploying an RL solution in the real world.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 14:57:55 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Xiao", "Ian", ""]]}, {"id": "1810.04144", "submitter": "Dinesh Kumar Amara", "authors": "Amara Dinesh Kumar, Koti Naga Renu Chebrolu, Vinayakumar R, Soman KP", "title": "A Brief Survey on Autonomous Vehicle Possible Attacks, Exploits and\n  Vulnerabilities", "comments": "5 Pages,1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Advanced driver assistance systems are advancing at a rapid pace and all\nmajor companies started investing in developing the autonomous vehicles. But\nthe security and reliability is still uncertain and debatable. Imagine that a\nvehicle is compromised by the attackers and then what they can do. An attacker\ncan control brake, accelerate and even steering which can lead to catastrophic\nconsequences. This paper gives a very short and brief overview of most of the\npossible attacks on autonomous vehicle software and hardware and their\npotential implications.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 09:31:14 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Kumar", "Amara Dinesh", ""], ["Chebrolu", "Koti Naga Renu", ""], ["R", "Vinayakumar", ""], ["KP", "Soman", ""]]}, {"id": "1810.04582", "submitter": "Theerawit Wilaiprasitporn", "authors": "Payongkit Lakhan, Nannapas Banluesombatkul, Vongsagon Changniam,\n  Ratwade Dhithijaiyratn, Pitshaporn Leelaarporn, Ekkarat Boonchieng, Supanida\n  Hompoonsup and Theerawit Wilaiprasitporn", "title": "Consumer Grade Brain Sensing for Emotion Recognition", "comments": null, "journal-ref": "IEEE Sensor Journal, 2019", "doi": "10.1109/JSEN.2019.2928781", "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For several decades, electroencephalography (EEG) has featured as one of the\nmost commonly used tools in emotional state recognition via monitoring of\ndistinctive brain activities. An array of datasets have been generated with the\nuse of diverse emotion-eliciting stimuli and the resulting brainwave responses\nconventionally captured with high-end EEG devices. However, the applicability\nof these devices is to some extent limited by practical constraints and may\nprove difficult to be deployed in highly mobile context omnipresent in everyday\nhappenings. In this study, we evaluate the potential of OpenBCI to bridge this\ngap by first comparing its performance to research grade EEG system, employing\nthe same algorithms that were applied on benchmark datasets. Moreover, for the\npurpose of emotion classification, we propose a novel method to facilitate the\nselection of audio-visual stimuli of high/low valence and arousal. Our setup\nentailed recruiting 200 healthy volunteers of varying years of age to identify\nthe top 60 affective video clips from a total of 120 candidates through\nstandardized self assessment, genre tags, and unsupervised machine learning.\nAdditional 43 participants were enrolled to watch the pre-selected clips during\nwhich emotional EEG brainwaves and peripheral physiological signals were\ncollected. These recordings were analyzed and extracted features fed into a\nclassification model to predict whether the elicited signals were associated\nwith a high or low level of valence and arousal. As it turned out, our\nprediction accuracies were decidedly comparable to those of previous studies\nthat utilized more costly EEG amplifiers for data acquisition.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 15:21:58 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 12:06:04 GMT"}, {"version": "v3", "created": "Tue, 30 Apr 2019 12:08:20 GMT"}, {"version": "v4", "created": "Fri, 9 Aug 2019 08:40:38 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Lakhan", "Payongkit", ""], ["Banluesombatkul", "Nannapas", ""], ["Changniam", "Vongsagon", ""], ["Dhithijaiyratn", "Ratwade", ""], ["Leelaarporn", "Pitshaporn", ""], ["Boonchieng", "Ekkarat", ""], ["Hompoonsup", "Supanida", ""], ["Wilaiprasitporn", "Theerawit", ""]]}, {"id": "1810.04668", "submitter": "Margit Antal", "authors": "Margit Antal and Elod Egyed-Zsigmond", "title": "Intrusion Detection Using Mouse Dynamics", "comments": "Submitted to IET Biometrics on 23 May 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to other behavioural biometrics, mouse dynamics is a less explored\narea. General purpose data sets containing unrestricted mouse usage data are\nusually not available. The Balabit data set was released in 2016 for a data\nscience competition, which against the few subjects, can be considered the\nfirst adequate publicly available one. This paper presents a performance\nevaluation study on this data set for impostor detection. The existence of very\nshort test sessions makes this data set challenging. Raw data were segmented\ninto mouse move, point and click and drag and drop types of mouse actions, then\nseveral features were extracted. In contrast to keystroke dynamics, mouse data\nis not sensitive, therefore it is possible to collect negative mouse dynamics\ndata and to use two-class classifiers for impostor detection. Both action- and\nset of actions-based evaluations were performed. Set of actions-based\nevaluation achieves 0.92 AUC on the test part of the data set. However, the\nsame type of evaluation conducted on the training part of the data set resulted\nin maximal AUC (1) using only 13 actions. Drag and drop mouse actions proved to\nbe the best actions for impostor detection.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 18:25:02 GMT"}], "update_date": "2018-10-14", "authors_parsed": [["Antal", "Margit", ""], ["Egyed-Zsigmond", "Elod", ""]]}, {"id": "1810.04673", "submitter": "Steven Jeuris", "authors": "Steven Jeuris, Paolo Tell, Steven Houben, Jakob E. Bardram", "title": "The Hidden Cost of Window Management", "comments": "Includes an ancillary video figure detailing a task switch during the\n  experiment: 'Example_task_switch.mp4'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most window management systems support multitasking by allowing users to\nopen, resize, position, and switch between application windows. Although\nmultitasking has become a way of life for most knowledge workers, our current\nunderstanding of how users use window management features to switch between\nmultiple tasks---which may comprise multiple application windows---is limited.\nIn this paper, we present a study providing an in-depth analysis of how task\nswitching is supported in Windows 7. As part of analysis, we developed an\ninterface-agnostic classification of common task switching operations supported\nby window managers which can be used to quantify the time spent on each\nconstituting action. Our study shows that task switching is a time intensive\nactivity and highlights the dominant actions that contribute to task switch\ntime. Furthermore, our classification highlights the specific operations that\nare optimized by more recent and experimental window managers and allows\nidentifying opportunities for design that could further reduce the overhead of\nswitching between tasks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 17:09:16 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Jeuris", "Steven", ""], ["Tell", "Paolo", ""], ["Houben", "Steven", ""], ["Bardram", "Jakob E.", ""]]}, {"id": "1810.04824", "submitter": "Fei Tan", "authors": "Fei Tan, Zhi Wei, Jun He, Xiang Wu, Bo Peng, Haoran Liu, and Zhenyu\n  Yan", "title": "A Blended Deep Learning Approach for Predicting User Intended Actions", "comments": "10 pages, International Conference on Data Mining 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User intended actions are widely seen in many areas. Forecasting these\nactions and taking proactive measures to optimize business outcome is a crucial\nstep towards sustaining the steady business growth. In this work, we focus on\npre- dicting attrition, which is one of typical user intended actions.\nConventional attrition predictive modeling strategies suffer a few inherent\ndrawbacks. To overcome these limitations, we propose a novel end-to-end\nlearning scheme to keep track of the evolution of attrition patterns for the\npredictive modeling. It integrates user activity logs, dynamic and static user\nprofiles based on multi-path learning. It exploits historical user records by\nestablishing a decaying multi-snapshot technique. And finally it employs the\nprecedent user intentions via guiding them to the subsequent learning\nprocedure. As a result, it addresses all disadvantages of conventional methods.\nWe evaluate our methodology on two public data repositories and one private\nuser usage dataset provided by Adobe Creative Cloud. The extensive experiments\ndemonstrate that it can offer the appealing performance in comparison with\nseveral existing approaches as rated by different popular metrics. Furthermore,\nwe introduce an advanced interpretation and visualization strategy to\neffectively characterize the periodicity of user activity logs. It can help to\npinpoint important factors that are critical to user attrition and retention\nand thus suggests actionable improvement targets for business practice. Our\nwork will provide useful insights into the prediction and elucidation of other\nuser intended actions as well.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 02:48:20 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Tan", "Fei", ""], ["Wei", "Zhi", ""], ["He", "Jun", ""], ["Wu", "Xiang", ""], ["Peng", "Bo", ""], ["Liu", "Haoran", ""], ["Yan", "Zhenyu", ""]]}, {"id": "1810.04877", "submitter": "Sao Mai Nguyen", "authors": "Nicolas Duminy, Sao Mai Nguyen (Lab-STICC, IMT Atlantique), Dominique\n  Duhaut", "title": "Learning a Set of Interrelated Tasks by Using Sequences of Motor\n  Policies for a Strategic Intrinsically Motivated Learner", "comments": null, "journal-ref": "2018 Second IEEE International Conference on Robotic Computing\n  (IRC), Jan 2018, Laguna Hills, France. IEEE, pp.288 - 291, 2018,", "doi": "10.1109/IRC.2018.00061", "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an active learning architecture for robots, capable of organizing\nits learning process to achieve a field of complex tasks by learning sequences\nof motor policies, called Intrinsically Motivated Procedure Babbling (IM-PB).\nThe learner can generalize over its experience to continuously learn new tasks.\nIt chooses actively what and how to learn based by empirical measures of its\nown progress. In this paper, we are considering the learning of a set of\ninterrelated tasks outcomes hierarchically organized. We introduce a framework\ncalled 'procedures', which are sequences of policies defined by the combination\nof previously learned skills. Our algorithmic architecture uses the procedures\nto autonomously discover how to combine simple skills to achieve complex goals.\nIt actively chooses between 2 strategies of goal-directed exploration :\nexploration of the policy space or the procedural space. We show on a simulated\nenvironment that our new architecture is capable of tackling the learning of\ncomplex motor policies, to adapt the complexity of its policies to the task at\nhand. We also show that our 'procedures' framework helps the learner to tackle\ndifficult hierarchical tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 07:54:16 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 20:16:56 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Duminy", "Nicolas", "", "Lab-STICC, IMT Atlantique"], ["Nguyen", "Sao Mai", "", "Lab-STICC, IMT Atlantique"], ["Duhaut", "Dominique", ""]]}, {"id": "1810.04879", "submitter": "Sao Mai Nguyen", "authors": "Maxime Devanne (LIFL), Sao Mai Nguyen (Lab-STICC, IMT Atlantique)", "title": "Generating Shared Latent Variables for Robots to Imitate Human Movements\n  and Understand their Physical Limitations", "comments": null, "journal-ref": "Computer Vision -- ECCV 2018 Workshops", "doi": "10.1007/978-3-030-11012-3_15", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assistive robotics and particularly robot coaches may be very helpful for\nrehabilitation healthcare. In this context, we propose a method based on\nGaussian Process Latent Variable Model (GP-LVM) to transfer knowledge between a\nphysiotherapist, a robot coach and a patient. Our model is able to map visual\nhuman body features to robot data in order to facilitate the robot learning and\nimitation. In addition , we propose to extend the model to adapt robots'\nunderstanding to patient's physical limitations during the assessment of\nrehabilitation exercises. Experimental evaluation demonstrates promising\nresults for both robot imitation and model adaptation according to the\npatients' limitations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 08:01:08 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 22:09:34 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Devanne", "Maxime", "", "LIFL"], ["Nguyen", "Sao Mai", "", "Lab-STICC, IMT Atlantique"]]}, {"id": "1810.04943", "submitter": "Daniel Sonntag", "authors": "Daniel Sonntag", "title": "Interactive Cognitive Assessment Tools: A Case Study on Digital Pens for\n  the Clinical Assessment of Dementia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Interactive cognitive assessment tools may be valuable for doctors and\ntherapists to reduce costs and improve quality in healthcare systems. Use cases\nand scenarios include the assessment of dementia. In this paper, we present our\napproach to the semi-automatic assessment of dementia. We describe a case study\nwith digital pens for the patients including background, problem description\nand possible solutions. We conclude with lessons learned when implementing\ndigital tests, and a generalisation for use outside the cognitive impairments\nfield.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 10:32:54 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Sonntag", "Daniel", ""]]}, {"id": "1810.05100", "submitter": "Nalin Asanka Gamagedara Arachchilage", "authors": "Chamila Wijayarathna and Nalin Asanka Gamagedara Arachchilage", "title": "A methodology to Evaluate the Usability of Security APIs", "comments": "6", "journal-ref": "IEEE International Conference on Information and Automation for\n  Sustainability (ICIAfS), 2019", "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing number of cyber-attacks demotivate people to use Information and\nCommunication Technology (ICT) for industrial as well as day to day work. A\nmain reason for the increasing number of cyber-attacks is mistakes that\nprogrammers make while developing software applications that are caused by\nusability issues exist in security Application Programming Interfaces (APIs).\nThese mistakes make software vulnerable to cyber-attacks. In this paper, we\nattempt to take a step closer to solve this problem by proposing a methodology\nto evaluate the usability and identify usability issues exist in security APIs.\nBy conducting a review of previous research, we identified 5 usability\nevaluation methodologies that have been proposed to evaluate the usability of\ngeneral APIs and characteristics of those methodologies that would affect when\nusing these methodologies to evaluate security APIs. Based on the findings, we\npropose a methodology to evaluate the usability of security APIs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 16:04:51 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Wijayarathna", "Chamila", ""], ["Arachchilage", "Nalin Asanka Gamagedara", ""]]}, {"id": "1810.05157", "submitter": "Andreea Bobu", "authors": "Andreea Bobu, Andrea Bajcsy, Jaime F. Fisac, Anca D. Dragan", "title": "Learning under Misspecified Objective Spaces", "comments": "Conference on Robot Learning (CoRL) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning robot objective functions from human input has become increasingly\nimportant, but state-of-the-art techniques assume that the human's desired\nobjective lies within the robot's hypothesis space. When this is not true, even\nmethods that keep track of uncertainty over the objective fail because they\nreason about which hypothesis might be correct, and not whether any of the\nhypotheses are correct. We focus specifically on learning from physical human\ncorrections during the robot's task execution, where not having a rich enough\nhypothesis space leads to the robot updating its objective in ways that the\nperson did not actually intend. We observe that such corrections appear\nirrelevant to the robot, because they are not the best way of achieving any of\nthe candidate objectives. Instead of naively trusting and learning from every\nhuman interaction, we propose robots learn conservatively by reasoning in real\ntime about how relevant the human's correction is for the robot's hypothesis\nspace. We test our inference method in an experiment with human interaction\ndata, and demonstrate that this alleviates unintended learning in an in-person\nuser study with a 7DoF robot manipulator.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 17:58:27 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 00:47:32 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 07:09:31 GMT"}, {"version": "v4", "created": "Fri, 26 Oct 2018 05:21:19 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Bobu", "Andreea", ""], ["Bajcsy", "Andrea", ""], ["Fisac", "Jaime F.", ""], ["Dragan", "Anca D.", ""]]}, {"id": "1810.05246", "submitter": "Chris Donahue", "authors": "Chris Donahue, Ian Simon, Sander Dieleman", "title": "Piano Genie", "comments": "Published as a conference paper at ACM IUI 2019", "journal-ref": null, "doi": "10.1145/3301275.3302288", "report-no": null, "categories": "cs.LG cs.HC cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Piano Genie, an intelligent controller which allows non-musicians\nto improvise on the piano. With Piano Genie, a user performs on a simple\ninterface with eight buttons, and their performance is decoded into the space\nof plausible piano music in real time. To learn a suitable mapping procedure\nfor this problem, we train recurrent neural network autoencoders with discrete\nbottlenecks: an encoder learns an appropriate sequence of buttons corresponding\nto a piano piece, and a decoder learns to map this sequence back to the\noriginal piece. During performance, we substitute a user's input for the\nencoder output, and play the decoder's prediction each time the user presses a\nbutton. To improve the intuitiveness of Piano Genie's performance behavior, we\nimpose musically meaningful constraints over the encoder's outputs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 21:00:44 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 08:53:31 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Donahue", "Chris", ""], ["Simon", "Ian", ""], ["Dieleman", "Sander", ""]]}, {"id": "1810.05507", "submitter": "Zixing Zhang", "authors": "Zixing Zhang, Jing Han, Eduardo Coutinho, Bj\\\"orn Schuller", "title": "Dynamic Difficulty Awareness Training for Continuous Emotion Prediction", "comments": "accepted by IEEE T-MM", "journal-ref": null, "doi": "10.1109/TMM.2018.2871949", "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-continuous emotion prediction has become an increasingly compelling task\nin machine learning. Considerable efforts have been made to advance the\nperformance of these systems. Nonetheless, the main focus has been the\ndevelopment of more sophisticated models and the incorporation of different\nexpressive modalities (e. g., speech, face, and physiology). In this paper,\nmotivated by the benefit of difficulty awareness in a human learning procedure,\nwe propose a novel machine learning framework, namely, Dynamic Difficulty\nAwareness Training (DDAT), which sheds fresh light on the research -- directly\nexploiting the difficulties in learning to boost the machine learning process.\nThe DDAT framework consists of two stages: information retrieval and\ninformation exploitation. In the first stage, we make use of the reconstruction\nerror of input features or the annotation uncertainty to estimate the\ndifficulty of learning specific information. The obtained difficulty level is\nthen used in tandem with original features to update the model input in a\nsecond learning stage with the expectation that the model can learn to focus on\nhigh difficulty regions of the learning process. We perform extensive\nexperiments on a benchmark database (RECOLA) to evaluate the effectiveness of\nthe proposed framework. The experimental results show that our approach\noutperforms related baselines as well as other well-established time-continuous\nemotion prediction systems, which suggests that dynamically integrating the\ndifficulty information for neural networks can help enhance the learning\nprocess.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 07:30:52 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Zhang", "Zixing", ""], ["Han", "Jing", ""], ["Coutinho", "Eduardo", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1810.05798", "submitter": "Marzyeh Ghassemi", "authors": "Marzyeh Ghassemi, Mahima Pushkarna, James Wexler, Jesse Johnson, Paul\n  Varghese", "title": "ClinicalVis: Supporting Clinical Task-Focused Design Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making decisions about what clinical tasks to prepare for is multi-factored,\nand especially challenging in intensive care environments where resources must\nbe balanced with patient needs. Electronic health records (EHRs) are a rich\ndata source, but are task-agnostic and can be difficult to use as\nsummarizations of patient needs for a specific task, such as \"could this\npatient need a ventilator tomorrow?\" In this paper, we introduce ClinicalVis,\nan open-source EHR visualization-based prototype system for task-focused design\nevaluation of interactions between healthcare providers (HCPs) and EHRs. We\nsituate ClinicalVis in a task-focused proof-of-concept design study targeting\nthese interactions with real patient data. We conduct an empirical study of 14\nHCPs, and discuss our findings on usability, accuracy, preference, and\nconfidence in treatment decisions. We also present design implications that our\nfindings suggest for future EHR interfaces, the presentation of clinical data\nfor task-based planning, and evaluating task-focused HCP/EHR interactions in\npractice.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 04:58:46 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Ghassemi", "Marzyeh", ""], ["Pushkarna", "Mahima", ""], ["Wexler", "James", ""], ["Johnson", "Jesse", ""], ["Varghese", "Paul", ""]]}, {"id": "1810.05993", "submitter": "Boris Ivanovic", "authors": "Boris Ivanovic and Marco Pavone", "title": "The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With\n  Dynamic Spatiotemporal Graphs", "comments": "IEEE/CVF International Conference on Computer Vision (ICCV) 2019 --\n  10 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing safe human-robot interaction systems is a necessary step towards\nthe widespread integration of autonomous agents in society. A key component of\nsuch systems is the ability to reason about the many potential futures (e.g.\ntrajectories) of other agents in the scene. Towards this end, we present the\nTrajectron, a graph-structured model that predicts many potential future\ntrajectories of multiple agents simultaneously in both highly dynamic and\nmultimodal scenarios (i.e. where the number of agents in the scene is\ntime-varying and there are many possible highly-distinct futures for each\nagent). It combines tools from recurrent sequence modeling and variational deep\ngenerative modeling to produce a distribution of future trajectories for each\nagent in a scene. We demonstrate the performance of our model on several\ndatasets, obtaining state-of-the-art results on standard trajectory prediction\nmetrics as well as introducing a new metric for comparing models that output\ndistributions.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 08:11:03 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 05:56:45 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 23:12:48 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ivanovic", "Boris", ""], ["Pavone", "Marco", ""]]}, {"id": "1810.06519", "submitter": "Brett Israelsen", "authors": "Brett W Israelsen, Nisar R Ahmed, Eric Frew, Dale Lawrence, Brian\n  Argrow", "title": "Factorized Machine Self-Confidence for Decision-Making Agents", "comments": "title change, leaving as stand-alone tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic assurances from advanced autonomous systems assist human users in\nunderstanding, trusting, and using such systems appropriately. Designing these\nsystems with the capacity of assessing their own capabilities is one approach\nto creating an algorithmic assurance. The idea of `machine self-confidence' is\nintroduced for autonomous systems. Using a factorization based framework for\nself-confidence assessment, one component of self-confidence, called\n`solver-quality', is discussed in the context of Markov decision processes for\nautonomous systems. Markov decision processes underlie much of the theory of\nreinforcement learning, and are commonly used for planning and decision making\nunder uncertainty in robotics and autonomous systems. A `solver quality' metric\nis formally defined in the context of decision making algorithms based on\nMarkov decision processes. A method for assessing solver quality is then\nderived, drawing inspiration from empirical hardness models. Finally, numerical\nexperiments for an unmanned autonomous vehicle navigation problem under\ndifferent solver, parameter, and environment conditions indicate that the\nself-confidence metric exhibits the desired properties. Discussion of results,\nand avenues for future investigation are included.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 17:06:38 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 18:31:04 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Israelsen", "Brett W", ""], ["Ahmed", "Nisar R", ""], ["Frew", "Eric", ""], ["Lawrence", "Dale", ""], ["Argrow", "Brian", ""]]}, {"id": "1810.06748", "submitter": "German I. Parisi", "authors": "Di Fu, Pablo Barros, German I. Parisi, Haiyan Wu, Sven Magg, Xun Liu,\n  Stefan Wermter", "title": "Assessing the Contribution of Semantic Congruency to Multisensory\n  Integration and Conflict Resolution", "comments": "Workshop on Crossmodal Learning for Intelligent Robotics at IROS'18,\n  Madrid, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The efficient integration of multisensory observations is a key property of\nthe brain that yields the robust interaction with the environment. However,\nartificial multisensory perception remains an open issue especially in\nsituations of sensory uncertainty and conflicts. In this work, we extend\nprevious studies on audio-visual (AV) conflict resolution in complex\nenvironments. In particular, we focus on quantitatively assessing the\ncontribution of semantic congruency during an AV spatial localization task. In\naddition to conflicts in the spatial domain (i.e. spatially misaligned\nstimuli), we consider gender-specific conflicts with male and female avatars.\nOur results suggest that while semantically related stimuli affect the\nmagnitude of the visual bias (perceptually shifting the location of the sound\ntowards a semantically congruent visual cue), humans still strongly rely on\nenvironmental statistics to solve AV conflicts. Together with previously\nreported results, this work contributes to a better understanding of how\nmultisensory integration and conflict resolution can be modelled in artificial\nagents and robots operating in real-world environments.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 23:22:10 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Fu", "Di", ""], ["Barros", "Pablo", ""], ["Parisi", "German I.", ""], ["Wu", "Haiyan", ""], ["Magg", "Sven", ""], ["Liu", "Xun", ""], ["Wermter", "Stefan", ""]]}, {"id": "1810.06828", "submitter": "Leigh Clark", "authors": "Leigh Clark, Phillip Doyle, Diego Garaialde, Emer Gilmartin, Stephan\n  Schl\\\"ogl, Jens Edlund, Matthew Aylett, Jo\\~ao Cabral, Cosmin Munteanu,\n  Benjamin Cowan", "title": "The State of Speech in HCI: Trends, Themes and Challenges", "comments": "Version of record & supplementary data pending", "journal-ref": null, "doi": "10.1093/iwc/iwz016", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech interfaces are growing in popularity. Through a review of 68 research\npapers this work maps the trends, themes, findings and methods of empirical\nresearch on speech interfaces in HCI. We find that most studies are\nusability/theory-focused or explore wider system experiences, evaluating Wizard\nof Oz, prototypes, or developed systems by using self-report questionnaires to\nmeasure concepts like usability and user attitudes. A thematic analysis of the\nresearch found that speech HCI work focuses on nine key topics: system speech\nproduction, modality comparison, user speech production, assistive technology\n\\& accessibility, design insight, experiences with interactive voice response\n(IVR) systems, using speech technology for development, people's experiences\nwith intelligent personal assistants (IPAs) and how user memory affects speech\ninterface interaction. From these insights we identify gaps and challenges in\nspeech research, notably the need to develop theories of speech interface\ninteraction, grow critical mass in this domain, increase design work, and\nexpand research from single to multiple user interaction contexts so as to\nreflect current use contexts. We also highlight the need to improve measure\nreliability, validity and consistency, in the wild deployment and reduce\nbarriers to building fully functional speech interfaces for research.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 06:08:50 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Clark", "Leigh", ""], ["Doyle", "Phillip", ""], ["Garaialde", "Diego", ""], ["Gilmartin", "Emer", ""], ["Schl\u00f6gl", "Stephan", ""], ["Edlund", "Jens", ""], ["Aylett", "Matthew", ""], ["Cabral", "Jo\u00e3o", ""], ["Munteanu", "Cosmin", ""], ["Cowan", "Benjamin", ""]]}, {"id": "1810.06979", "submitter": "Yuan Gao", "authors": "Yuan Gao (1), Fangkai Yang (2), Martin Frisk (1), Daniel Hernandez\n  (3), Christopher Peters (2) and Ginevra Castellano (1) ((1) Department of\n  Information Technology, Uppsala University, Uppsala, Sweden, (2) Department\n  of Computational Science and Technology, Royal Institute of Technology, KTH,\n  Stockholm, Sweden, (3) Department of Computer Science, University of York,\n  York, United Kingdom)", "title": "Learning Socially Appropriate Robot Approaching Behavior Toward Groups\n  using Deep Reinforcement Learning", "comments": "accepted for The 28th IEEE International Conference on Robot & Human\n  Interactive Communication (Ro-Man)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has recently been widely applied in robotics to\nstudy tasks such as locomotion and grasping, but its application to social\nhuman-robot interaction (HRI) remains a challenge. In this paper, we present a\ndeep learning scheme that acquires a prior model of robot approaching behavior\nin simulation and applies it to real-world interaction with a physical robot\napproaching groups of humans. The scheme, which we refer to as Staged Social\nBehavior Learning (SSBL), considers different stages of learning in social\nscenarios. We learn robot approaching behaviors towards small groups in\nsimulation and evaluate the performance of the model using objective and\nsubjective measures in a perceptual study and a HRI user study with human\nparticipants. Results show that our model generates more socially appropriate\nbehavior compared to a state-of-the-art model.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 13:27:10 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 07:43:20 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 13:53:51 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Gao", "Yuan", ""], ["Yang", "Fangkai", ""], ["Frisk", "Martin", ""], ["Hernandez", "Daniel", ""], ["Peters", "Christopher", ""], ["Castellano", "Ginevra", ""]]}, {"id": "1810.07273", "submitter": "R.Stuart Geiger", "authors": "R. Stuart Geiger, Aaron Halfaker", "title": "Operationalizing Conflict and Cooperation between Automated Software\n  Agents in Wikipedia: A Replication and Expansion of 'Even Good Bots Fight'", "comments": "33 pages. In ACM CSCW 2018", "journal-ref": "Proc ACM on Human Computer Interaction. 1(2), Article 49. CSCW\n  2018", "doi": "10.1145/3134684", "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper replicates, extends, and refutes conclusions made in a study\npublished in PLoS ONE (\"Even Good Bots Fight\"), which claimed to identify\nsubstantial levels of conflict between automated software agents (or bots) in\nWikipedia using purely quantitative methods. By applying an integrative\nmixed-methods approach drawing on trace ethnography, we place these alleged\ncases of bot-bot conflict into context and arrive at a better understanding of\nthese interactions. We found that overwhelmingly, the interactions previously\ncharacterized as problematic instances of conflict are typically better\ncharacterized as routine, productive, even collaborative work. These results\nchallenge past work and show the importance of qualitative/quantitative\ncollaboration. In our paper, we present quantitative metrics and qualitative\nheuristics for operationalizing bot-bot conflict. We give thick descriptions of\nkinds of events that present as bot-bot reverts, helping distinguish conflict\nfrom non-conflict. We computationally classify these kinds of events through\npatterns in edit summaries. By interpreting found/trace data in the\nsocio-technical contexts in which people give that data meaning, we gain more\nfrom quantitative measurements, drawing deeper understandings about the\ngovernance of algorithmic systems in Wikipedia. We have also released our data\ncollection, processing, and analysis pipeline, to facilitate computational\nreproducibility of our findings and to help other researchers interested in\nconducting similar mixed-method scholarship in other platforms and contexts.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 20:59:19 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Geiger", "R. Stuart", ""], ["Halfaker", "Aaron", ""]]}, {"id": "1810.07294", "submitter": "Erfan Pakdamanian", "authors": "Erfan Pakdamanian, Lu Feng, Inki Kim", "title": "The Effect of Whole-Body Haptic Feedback on Driver's Perception in\n  Negotiating a Curve", "comments": "Human Factors and Ergonomics Society 2018 Annual Meeting", "journal-ref": "Pakdamanian, Erfan, Lu Feng, and Inki Kim. In Proceedings of the\n  Human Factors and Ergonomics Society Annual Meeting, vol. 62, no. 1, pp.\n  19-23. Sage CA: Los Angeles, CA: SAGE Publications, 2018", "doi": "10.1177/1541931218621005", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It remains uncertain regarding the safety of driving in autonomous vehicles\nthat, after a long, passive control and inattention to the driving situation,\nhow the drivers will be effectively informed to take-over the control in\nemergency. In particular, the active role of vehicle force feedback on the\ndriver's risk perception on curves has not been fully explored. To investigate\nit, the current paper examined the driver's cognitive and visual responses to\nthe whole-body haptic feedback during curve negotiations. The effects of force\nfeedback on drivers' responses on curves were investigated in a high-fidelity\ndriving simulator while measuring EEG and visual gaze over ten participants.\nThe preliminary analyses of the first two participants revealed that pupil\ndiameter and fixation time on the curves were significantly longer when the\ndriver received whole-body feedback, compared to none. The findings suggest\nthat whole-body feedback can be used as an effective \"advance notification\" of\nhazards.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 22:05:37 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Pakdamanian", "Erfan", ""], ["Feng", "Lu", ""], ["Kim", "Inki", ""]]}, {"id": "1810.07460", "submitter": "Jan Romportl", "authors": "Eva Zackova and Jan Romportl", "title": "What might matter in autonomous cars adoption: first person versus third\n  person scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discussion between the automotive industry, governments, ethicists,\npolicy makers and general public about autonomous cars' moral agency is\nwidening, and therefore we see the need to bring more insight into what\nmeta-factors might actually influence the outcomes of such discussions, surveys\nand plebiscites. In our study, we focus on the psychological (personality\ntraits), practical (active driving experience), gender and rhetoric/framing\nfactors that might impact and even determine respondents' a priori preferences\nof autonomous cars' operation. We conducted an online survey (N=430) to collect\ndata that show that the third person scenario is less biased than the first\nperson scenario when presenting ethical dilemma related to autonomous cars.\nAccording to our analysis, gender bias should be explored in more extensive\nfuture studies as well. We recommend any participatory technology assessment\ndiscourse to use the third person scenario and to direct attention to the way\nany autonomous car related debate is introduced, especially in terms of\nlinguistic and communication aspects and gender.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 10:24:22 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Zackova", "Eva", ""], ["Romportl", "Jan", ""]]}, {"id": "1810.07593", "submitter": "Abdelrahman Yaseen", "authors": "Abdelrahman Yaseen and Katrin Lohan", "title": "Playing Pairs with Pepper", "comments": "Presented at AI-HRI AAAI-FSS, 2018 (arXiv:1809.06606)", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2018/07", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots become increasingly prevalent in almost all areas of society, the\nfactors affecting humans trust in those robots becomes increasingly important.\nThis paper is intended to investigate the factor of robot attributes, looking\nspecifically at the relationship between anthropomorphism and human development\nof trust. To achieve this, an interaction game, Matching the Pairs, was\ndesigned and implemented on two robots of varying levels of anthropomorphism,\nPepper and Husky. Participants completed both pre- and post-test questionnaires\nthat were compared and analyzed predominantly with the use of quantitative\nmethods, such as paired sample t-tests. Post-test analyses suggested a positive\nrelationship between trust and anthropomorphism with $80\\%$ of participants\nconfirming that the robots' adoption of facial features assisted in\nestablishing trust. The results also indicated a positive relationship between\ninteraction and trust with $90\\%$ of participants confirming this for both\nrobots post-test\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 14:57:43 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Yaseen", "Abdelrahman", ""], ["Lohan", "Katrin", ""]]}, {"id": "1810.07665", "submitter": "Shujun Li", "authors": "Ximing Liu, Yingjiu Li, Robert H. Deng, Bing Chang and Shujun Li", "title": "When Human cognitive modeling meets PINs: User-independent\n  inter-keystroke timing attacks", "comments": "16 pages, 9 figures", "journal-ref": "Computers & Security, vol. 80, pp. 90-107, 2018", "doi": "10.1016/j.cose.2018.09.003", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the first user-independent inter-keystroke timing attacks\non PINs. Our attack method is based on an inter-keystroke timing dictionary\nbuilt from a human cognitive model whose parameters can be determined by a\nsmall amount of training data on any users (not necessarily the target\nvictims). Our attacks can thus be potentially launched on a large scale in\nreal-world settings. We investigate inter-keystroke timing attacks in different\nonline attack settings and evaluate their performance on PINs at different\nstrength levels. Our experimental results show that the proposed attack\nperforms significantly better than random guessing attacks. We further\ndemonstrate that our attacks pose a serious threat to real-world applications\nand propose various ways to mitigate the threat.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 16:54:39 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Liu", "Ximing", ""], ["Li", "Yingjiu", ""], ["Deng", "Robert H.", ""], ["Chang", "Bing", ""], ["Li", "Shujun", ""]]}, {"id": "1810.07791", "submitter": "Dominika Woszczyk", "authors": "Dominika Woszczyk, Gerasimos Spanakis", "title": "MaaSim: A Liveability Simulation for Improving the Quality of Life in\n  Cities", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Urbanism is no longer planned on paper thanks to powerful models and 3D\nsimulation platforms. However, current work is not open to the public and lacks\nan optimisation agent that could help in decision making. This paper describes\nthe creation of an open-source simulation based on an existing Dutch\nliveability score with a built-in AI module. Features are selected using\nfeature engineering and Random Forests. Then, a modified scoring function is\nbuilt based on the former liveability classes. The score is predicted using\nRandom Forest for regression and achieved a recall of 0.83 with 10-fold\ncross-validation. Afterwards, Exploratory Factor Analysis is applied to select\nthe actions present in the model. The resulting indicators are divided into 5\ngroups, and 12 actions are generated. The performance of four optimisation\nalgorithms is compared, namely NSGA-II, PAES, SPEA2 and eps-MOEA, on three\nestablished criteria of quality: cardinality, the spread of the solutions,\nspacing, and the resulting score and number of turns. Although all four\nalgorithms show different strengths, eps-MOEA is selected to be the most\nsuitable for this problem. Ultimately, the simulation incorporates the model\nand the selected AI module in a GUI written in the Kivy framework for Python.\nTests performed on users show positive responses and encourage further\ninitiatives towards joining technology and public applications.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 15:19:41 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Woszczyk", "Dominika", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "1810.07841", "submitter": "Xiexing Feng", "authors": "Xiexing Feng, Libo Cao, Yunxian Zhang, Hongbo Gao and Lifan Tan", "title": "The Effects of Using Taxi-Hailing Application on Driving Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver distraction has become a major threat to the road safety, and the\nglobally booming taxi-hailing application introduces new source of distraction\nto drivers. Although various in-vehicle information systems (IVIS) have been\nstudied extensively, no documentation exists objectively measuring the extent\nto which interacting with taxi-hailing application during driving impacts\ndrivers' behavior. To fill this gap, a simulator-based study was conducted to\nsynthetically compare the effects that different output modalities (visual,\naudio, combined visual-audio) and input modalities (baseline, manual, speech)\nimposed on the driving performance. The results show that the visual output\nintroduced more negative effects on driving performance compared to audio\noutput. In the combined output, visual component dominated the effects imposed\non the longitudinal control and hazard detection; audio component only\nexacerbated the negative effects of visual component on the lateral control.\nSpeech input modality was overall less detrimental to driving performance than\nmanual input modality, especially reflected in the drivers' quicker reaction to\nhazard events. The visual-manual interaction modality most severely impaired\nthe hazard detecting ability, while also led to strong compensative behaviors.\nThe audio-speech and visual-speech modality associated with more smooth lateral\ncontrol and faster response to hazard events respectively compared to other\nmodality. These results could be applied to improve the design of not only the\ntaxi-hailing application, but also other input-output balanced IVIS.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 00:03:55 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 14:50:51 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Feng", "Xiexing", ""], ["Cao", "Libo", ""], ["Zhang", "Yunxian", ""], ["Gao", "Hongbo", ""], ["Tan", "Lifan", ""]]}, {"id": "1810.08666", "submitter": "Lindah Kotut", "authors": "Lindah Kotut, Michael Horning, Derek Haqq, Shuo Niu, Timothy Stelter,\n  D. Scott McCrickard", "title": "Tensions on Trails: Understanding Differences between Group and\n  Community Needs in Outdoor Settings", "comments": "Paper submitted to CSCW 2018 Rural Computing Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares the needs of groups and communities in outdoor settings,\nseeking to identify subtle but important differences in the ways that their\nneeds can be supported. We first examine the questions of who uses technology\nin outdoor settings, what their technological uses and needs are, and what\nconflicts exist between different trail users regarding technology use and\nexperience. We then consider selected categories of people to understand their\ndistinct needs when acting as groups and as communities. We conclude that it is\nimportant to explore the tensions between groups and communities to identify\ndesign opportunities.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 20:34:56 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Kotut", "Lindah", ""], ["Horning", "Michael", ""], ["Haqq", "Derek", ""], ["Niu", "Shuo", ""], ["Stelter", "Timothy", ""], ["McCrickard", "D. Scott", ""]]}, {"id": "1810.08691", "submitter": "Dawei Liang", "authors": "Dawei Liang, Edison Thomaz", "title": "Audio-Based Activities of Daily Living (ADL) Recognition with\n  Large-Scale Acoustic Embeddings from Online Videos", "comments": "18 pages,7 figures; new version: results updates", "journal-ref": "ACM IMWUT 3(1) 2019 Article 17", "doi": "10.1145/3314404", "report-no": null, "categories": "cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, activity sensing and recognition has been shown to play a key\nenabling role in a wide range of applications, from sustainability and\nhuman-computer interaction to health care. While many recognition tasks have\ntraditionally employed inertial sensors, acoustic-based methods offer the\nbenefit of capturing rich contextual information, which can be useful when\ndiscriminating complex activities. Given the emergence of deep learning\ntechniques and leveraging new, large-scaled multi-media datasets, this paper\nrevisits the opportunity of training audio-based classifiers without the\nonerous and time-consuming task of annotating audio data. We propose a\nframework for audio-based activity recognition that makes use of millions of\nembedding features from public online video sound clips. Based on the\ncombination of oversampling and deep learning approaches, our framework does\nnot require further feature processing or outliers filtering as in prior work.\nWe evaluated our approach in the context of Activities of Daily Living (ADL) by\nrecognizing 15 everyday activities with 14 participants in their own homes,\nachieving 64.2% and 83.6% averaged within-subject accuracy in terms of top-1\nand top-3 classification respectively. Individual class performance was also\nexamined in the paper to further study the co-occurrence characteristics of the\nactivities and the robustness of the framework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 21:19:16 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 09:08:24 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Liang", "Dawei", ""], ["Thomaz", "Edison", ""]]}, {"id": "1810.08707", "submitter": "Leonardo Fanzeres", "authors": "Leonardo A. Fanzeres (1), Adriana S. Vivacqua (1), Luiz W. P.\n  Biscainho (2) ((1) PPGI, DCC/IM, Universidade Federal do Rio de Janeiro, (2)\n  DEL/Poli & PEE/COPPE, Universidade Federal do Rio de Janeiro)", "title": "Mobile Sound Recognition for the Deaf and Hard of Hearing", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human perception of surrounding events is strongly dependent on audio cues.\nThus, acoustic insulation can seriously impact situational awareness. We\npresent an exploratory study in the domain of assistive computing, eliciting\nrequirements and presenting solutions to problems found in the development of\nan environmental sound recognition system, which aims to assist deaf and hard\nof hearing people in the perception of sounds. To take advantage of smartphones\ncomputational ubiquity, we propose a system that executes all processing on the\ndevice itself, from audio features extraction to recognition and visual\npresentation of results. Our application also presents the confidence level of\nthe classification to the user. A test of the system conducted with deaf users\nprovided important and inspiring feedback from participants.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 22:47:52 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Fanzeres", "Leonardo A.", ""], ["Vivacqua", "Adriana S.", ""], ["Biscainho", "Luiz W. P.", ""]]}, {"id": "1810.08812", "submitter": "Yunlong Wang", "authors": "Yunlong Wang, Ahmed Fadhil, Jan-Philipp Lange, Harald Reiterer", "title": "Integrating Taxonomies into Theory-Based Digital Health Interventions\n  for Behavior Change: A Holistic Framework", "comments": null, "journal-ref": null, "doi": "10.2196/resprot.8055", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital health interventions have been emerging in the last decade. Due to\ntheir interdisciplinary nature, digital health interventions are guided and\ninfluenced by theories (e.g., behavioral theories, behavior change\ntechnologies, persuasive technology) from different research communities.\nHowever, digital health interventions are always coded using various taxonomies\nand reported in insufficient perspectives. The inconsistency and\nincomprehensiveness will bring difficulty for conducting systematic reviews and\nsharing contributions among communities. Based on existing related work,\ntherefore, we propose a holistic framework that embeds behavioral theories,\nbehavior change technique (BCT) taxonomy, and persuasive system design (PSD)\nprinciples. Including four development steps, two toolboxes, and one workflow,\nour framework aims to guide digital health intervention developers to design,\nevaluate, and report their work in a formative and comprehensive way.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 14:30:59 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Wang", "Yunlong", ""], ["Fadhil", "Ahmed", ""], ["Lange", "Jan-Philipp", ""], ["Reiterer", "Harald", ""]]}, {"id": "1810.09159", "submitter": "Florian Brachten", "authors": "Florian Brachten, Milad Mirbabaie, Stefan Stieglitz, Olivia Berger,\n  Sarah Bludau, Kristina Schrickel", "title": "Threat or Opportunity? - Examining Social Bots in Social Media Crisis\n  Communication", "comments": "Accepted for publication in the Proceedings of the Australasian\n  Conference on Information Systems, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Crisis situations are characterised by their sudden occurrence and an unclear\ninformation situation. In that context, social media platforms have become a\nhighly utilised resource for collective information gathering to fill these\ngaps. However, there are indications that not only humans, but also social bots\nare active on these platforms during crisis situations. Although identifying\nthe impact of social bots during extreme events seems to be a highly relevant\ntopic, research remains sparse. To fill this research gap, we started a bigger\nproject in analysing the influence of social bots during crisis situations. As\na part of this project, we initially conducted a case study on the Manchester\nBombing 2017 and analysed the social bot activity. Our results indicate that\nmainly benign bots are active during crisis situations. While the quantity of\nthe bot accounts is rather low, their tweet activity indicates a high\ninfluence.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 09:58:48 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Brachten", "Florian", ""], ["Mirbabaie", "Milad", ""], ["Stieglitz", "Stefan", ""], ["Berger", "Olivia", ""], ["Bludau", "Sarah", ""], ["Schrickel", "Kristina", ""]]}, {"id": "1810.09487", "submitter": "Philipp Tschandl MD PhD", "authors": "Philipp Tschandl, Giuseppe Argenziano, Majid Razmara, Jordan Yap", "title": "Diagnostic Accuracy of Content Based Dermatoscopic Image Retrieval with\n  Deep Classification Features", "comments": null, "journal-ref": "Tschandl P, Argenziano G, Razmara M, Yap J. Diagnostic Accuracy of\n  Content Based Dermatoscopic Image Retrieval with Deep Classification\n  Features. Br J Dermatol 2018 Sep 12. doi: 10.1111/bjd.17189", "doi": "10.1111/bjd.17189", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background: Automated classification of medical images through neural\nnetworks can reach high accuracy rates but lack interpretability.\n  Objectives: To compare the diagnostic accuracy obtained by using content\nbased image retrieval (CBIR) to retrieve visually similar dermatoscopic images\nwith corresponding disease labels against predictions made by a neural network.\n  Methods: A neural network was trained to predict disease classes on\ndermatoscopic images from three retrospectively collected image datasets\ncontaining 888, 2750 and 16691 images respectively. Diagnosis predictions were\nmade based on the most commonly occurring diagnosis in visually similar images,\nor based on the top-1 class prediction of the softmax output from the network.\nOutcome measures were area under the ROC curve for predicting a malignant\nlesion (AUC), multiclass-accuracy and mean average precision (mAP), measured on\nunseen test images of the corresponding dataset.\n  Results: In all three datasets the skin cancer predictions from CBIR\n(evaluating the 16 most similar images) showed AUC values similar to softmax\npredictions (0.842, 0.806 and 0.852 versus 0.830, 0.810 and 0.847 respectively;\np-value>0.99 for all). Similarly, the multiclass-accuracy of CBIR was\ncomparable to softmax predictions. Networks trained for detecting only 3\nclasses performed better on a dataset with 8 classes when using CBIR as\ncompared to softmax predictions (mAP 0.184 vs. 0.368 and 0.198 vs. 0.403\nrespectively).\n  Conclusions: Presenting visually similar images based on features from a\nneural network shows comparable accuracy to the softmax probability-based\ndiagnoses of convolutional neural networks. CBIR may be more helpful than a\nsoftmax classifier in improving diagnostic accuracy of clinicians in a routine\nclinical setting.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 18:20:01 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Tschandl", "Philipp", ""], ["Argenziano", "Giuseppe", ""], ["Razmara", "Majid", ""], ["Yap", "Jordan", ""]]}, {"id": "1810.09590", "submitter": "R.Stuart Geiger", "authors": "R. Stuart Geiger", "title": "The Lives of Bots", "comments": "Originally published in 2011", "journal-ref": "Book chapter in __Wikipedia: A Critical Point of View__ (Institute\n  of Network Cultures, Amsterdam), 2011", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated software agents --- or bots --- have long been an important part of\nhow Wikipedia's volunteer community of editors write, edit, update, monitor,\nand moderate content. In this paper, I discuss the complex social and technical\nenvironment in which Wikipedia's bots operate. This paper focuses on the\nestablishment and role of English Wikipedia's bot policies and the Bot\nApprovals Group, a volunteer committee that reviews applications for new bots\nand helps resolve conflicts between Wikipedians about automation. In\nparticular, I examine an early bot controversy over the first bot in Wikipedia\nto automatically enforce a social norm about how Wikipedian editors ought to\ninteract in discussion spaces. As I show, bots enforce many rules in Wikipedia,\nbut humans produce these bots and negotiate rules around their operation.\nBecause of the openness of Wikipedia's processes around automation, we can\nvividly observe the often-invisible human work involved in such algorithmic\nsystems --- in stark contrast to most other user-generated content platforms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 23:04:05 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Geiger", "R. Stuart", ""]]}, {"id": "1810.09634", "submitter": "Chien-Ju Ho", "authors": "Chien-Ju Ho and Ming Yin", "title": "Working in Pairs: Understanding the Effects of Worker Interactions in\n  Crowdwork", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has gained popularity as a tool to harness human brain power to\nhelp solve problems that are difficult for computers. Previous work in\ncrowdsourcing often assumes that workers complete crowdwork independently. In\nthis paper, we relax the independent property of crowdwork and explore how\nintroducing direct, synchronous, and free-style interactions between workers\nwould affect crowdwork. In particular, motivated by the concept of peer\ninstruction in educational settings, we study the effects of peer communication\nin crowdsourcing environments. In the crowdsourcing setting with peer\ncommunication, pairs of workers are asked to complete the same task together by\nfirst generating their initial answers to the task independently and then\nfreely discussing the tasks with each other and updating their answers after\nthe discussion. We experimentally examine the effects of peer communication in\ncrowdwork on various common types of tasks on crowdsourcing platforms,\nincluding image labeling, optical character recognition (OCR), audio\ntranscription, and nutrition analysis. Our experiment results show that the\nwork quality is significantly improved in tasks with peer communication\ncompared to tasks where workers complete the work independently. However,\nparticipating in tasks with peer communication has limited effects on\ninfluencing worker's independent performance in tasks of the same type in the\nfuture.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 02:50:41 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Ho", "Chien-Ju", ""], ["Yin", "Ming", ""]]}, {"id": "1810.09952", "submitter": "Ziran Wang", "authors": "Ziran Wang and BaekGyu Kim and Hiromitsu Kobayashi and Guoyuan Wu and\n  Matthew J. Barth", "title": "Agent-Based Modeling and Simulation of Connected and Automated Vehicles\n  Using Game Engine: A Cooperative On-Ramp Merging Study", "comments": "14 pages, 6 figures. 2019 Transportation Research Board Annual\n  Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agent-based modeling and simulation (ABMS) has been a popular approach to\nmodeling autonomous and interacting agents in a multi-agent system.\nSpecifically, ABMS can be applied to connected and automated vehicles (CAVs),\nsince CAVs can be driven autonomously with the help of on-board sensors, and\ncooperate with each other through vehicle-to-everything (V2X) communications.\nIn this work, we apply ABMS to CAVs using the game engine Unity3D, taking\nadvantage of its visualization capability and other capabilities. Agent-based\nmodels of CAVs are built in the Unity3D environment, where vehicles are enabled\nwith connectivity and autonomy by C#-based scripting API. We also build a\nsimulation network in Unity3D based on the city of Mountain View, California. A\ncase study of cooperative on-ramp merging has been carried out with the\nproposed distributed consensus-based protocol, and then compared with the\nhuman-in-the-loop simulation where the on-ramp vehicle is driven by four\ndifferent human drivers on a driving simulator. The benefits of introducing the\nproposed protocol are evaluated in terms of travel time, energy consumption,\nand pollutant emissions. It is shown from the results that the proposed\ncooperative on-ramp merging protocol can reduce average travel time by 7%,\nreduce energy consumption and pollutant emissions by 8% and 58%, respectively,\nand guarantee the driving safety when compared to the human-in-the-loop\nscenario.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 16:31:26 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Wang", "Ziran", ""], ["Kim", "BaekGyu", ""], ["Kobayashi", "Hiromitsu", ""], ["Wu", "Guoyuan", ""], ["Barth", "Matthew J.", ""]]}, {"id": "1810.10206", "submitter": "Jean-Daniel Taupiac", "authors": "Jean-Daniel Taupiac (ICAR), Nancy Rodriguez (ICAR), Olivier Strauss\n  (ICAR)", "title": "Immercity: a curation content application in Virtual and Augmented\n  reality", "comments": null, "journal-ref": "10th International Conference, VAMR 2018, Held as Part of HCI\n  International 2018, II (2), Springer, 2018, Virtual, Augmented and Mixed\n  Reality: Applications in Health, Cultural Heritage, and Industry,\n  978-3-319-91583-8. https://link.springer.com/book/10.1007/978-3-319-91584-5", "doi": "10.1007/978-3-319-91584-5_18", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working with emergent and appealing technologies as Virtual Reality,\nMixed Reality and Augmented Reality, the issue of definitions appear very\noften. Indeed, our experience with various publics allows us to notice that\ntechnology definitions pose ambiguity and representation problems for informed\nas well as novice users. In this paper we present Immercity, a content curation\nsystem designed in the context of a collaboration between the University of\nMontpellier and CapGemi-ni, to deliver a technology watch. It is also used as a\ntestbed for our experiences with Virtual, Mixed and Augmented reality to\nexplore new interaction techniques and devices, artificial intelligence\nintegration, visual affordances, performance , etc. But another, very\ninteresting goal appeared: use Immercity to communicate about Virtual, Mixed\nand Augmented Reality by using them as a support.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 06:23:46 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Taupiac", "Jean-Daniel", "", "ICAR"], ["Rodriguez", "Nancy", "", "ICAR"], ["Strauss", "Olivier", "", "ICAR"]]}, {"id": "1810.10353", "submitter": "Mengying Lei", "authors": "Yang Li, Mengying Lei, Xianrui Zhang, Weigang Cui, Yuzhu Guo, Ting-Wen\n  Huang, and Hua-Liang Wei", "title": "Boosted Convolutional Neural Networks for Motor Imagery EEG Decoding\n  with Multiwavelet-based Time-Frequency Conditional Granger Causality Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding EEG signals of different mental states is a challenging task for\nbrain-computer interfaces (BCIs) due to nonstationarity of perceptual decision\nprocesses. This paper presents a novel boosted convolutional neural networks\n(ConvNets) decoding scheme for motor imagery (MI) EEG signals assisted by the\nmultiwavelet-based time-frequency (TF) causality analysis. Specifically,\nmultiwavelet basis functions are first combined with Geweke spectral measure to\nobtain high-resolution TF-conditional Granger causality (CGC) representations,\nwhere a regularized orthogonal forward regression (ROFR) algorithm is adopted\nto detect a parsimonious model with good generalization performance. The\ncausality images for network input preserving time, frequency and location\ninformation of connectivity are then designed based on the TF-CGC distributions\nof alpha band multichannel EEG signals. Further constructed boosted ConvNets by\nusing spatio-temporal convolutions as well as advances in deep learning\nincluding cropping and boosting methods, to extract discriminative causality\nfeatures and classify MI tasks. Our proposed approach outperforms the\ncompetition winner algorithm with 12.15% increase in average accuracy and\n74.02% decrease in associated inter subject standard deviation for the same\nbinary classification on BCI competition-IV dataset-IIa. Experiment results\nindicate that the boosted ConvNets with causality images works well in decoding\nMI-EEG signals and provides a promising framework for developing MI-BCI\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 07:39:12 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Li", "Yang", ""], ["Lei", "Mengying", ""], ["Zhang", "Xianrui", ""], ["Cui", "Weigang", ""], ["Guo", "Yuzhu", ""], ["Huang", "Ting-Wen", ""], ["Wei", "Hua-Liang", ""]]}, {"id": "1810.10565", "submitter": "Yulun Du", "authors": "Yulun Du, Chirag Raman, Alan W Black, Louis-Philippe Morency, Maxine\n  Eskenazi", "title": "Multimodal Polynomial Fusion for Detecting Driver Distraction", "comments": "INTERSPEECH 2018", "journal-ref": null, "doi": "10.21437/Interspeech.2018-2011", "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distracted driving is deadly, claiming 3,477 lives in the U.S. in 2015 alone.\nAlthough there has been a considerable amount of research on modeling the\ndistracted behavior of drivers under various conditions, accurate automatic\ndetection using multiple modalities and especially the contribution of using\nthe speech modality to improve accuracy has received little attention. This\npaper introduces a new multimodal dataset for distracted driving behavior and\ndiscusses automatic distraction detection using features from three modalities:\nfacial expression, speech and car signals. Detailed multimodal feature analysis\nshows that adding more modalities monotonically increases the predictive\naccuracy of the model. Finally, a simple and effective multimodal fusion\ntechnique using a polynomial fusion layer shows superior distraction detection\nresults compared to the baseline SVM and neural network models.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 18:16:42 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Du", "Yulun", ""], ["Raman", "Chirag", ""], ["Black", "Alan W", ""], ["Morency", "Louis-Philippe", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "1810.10581", "submitter": "Abhik Singla", "authors": "Abhik Singla, Partha Pratim Roy and Debi Prosad Dogra", "title": "Visual Rendering of Shapes on 2D Display Devices Guided by Hand Gestures", "comments": "Submitted to Elsevier Displays Journal, 32 pages, 18 figures, 7\n  tables", "journal-ref": null, "doi": "10.1016/j.displa.2019.03.001", "report-no": null, "categories": "cs.HC cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing of touchless user interface is gaining popularity in various\ncontexts. Using such interfaces, users can interact with electronic devices\neven when the hands are dirty or non-conductive. Also, user with partial\nphysical disability can interact with electronic devices using such systems.\nResearch in this direction has got major boost because of the emergence of\nlow-cost sensors such as Leap Motion, Kinect or RealSense devices. In this\npaper, we propose a Leap Motion controller-based methodology to facilitate\nrendering of 2D and 3D shapes on display devices. The proposed method tracks\nfinger movements while users perform natural gestures within the field of view\nof the sensor. In the next phase, trajectories are analyzed to extract extended\nNpen++ features in 3D. These features represent finger movements during the\ngestures and they are fed to unidirectional left-to-right Hidden Markov Model\n(HMM) for training. A one-to-one mapping between gestures and shapes is\nproposed. Finally, shapes corresponding to these gestures are rendered over the\ndisplay using MuPad interface. We have created a dataset of 5400 samples\nrecorded by 10 volunteers. Our dataset contains 18 geometric and 18\nnon-geometric shapes such as \"circle\", \"rectangle\", \"flower\", \"cone\", \"sphere\"\netc. The proposed methodology achieves an accuracy of 92.87% when evaluated\nusing 5-fold cross validation method. Our experiments revel that the extended\n3D features perform better than existing 3D features in the context of shape\nrepresentation and classification. The method can be used for developing useful\nHCI applications for smart display devices.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 16:59:52 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Singla", "Abhik", ""], ["Roy", "Partha Pratim", ""], ["Dogra", "Debi Prosad", ""]]}, {"id": "1810.10733", "submitter": "Quan Ze Chen", "authors": "Quanze Chen, Jonathan Bragg, Lydia B. Chilton, Daniel S. Weld", "title": "Cicero: Multi-Turn, Contextual Argumentation for Accurate Crowdsourcing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches for ensuring high quality crowdwork have failed to\nachieve high-accuracy on difficult problems. Aggregating redundant answers\noften fails on the hardest problems when the majority is confused.\nArgumentation has been shown to be effective in mitigating these drawbacks.\nHowever, existing argumentation systems only support limited interactions and\nshow workers general justifications, not context-specific arguments targeted to\ntheir reasoning.\n  This paper presents Cicero, a new workflow that improves crowd accuracy on\ndifficult tasks by engaging workers in multi-turn, contextual discussions\nthrough real-time, synchronous argumentation. Our experiments show that\ncompared to previous argumentation systems which only improve the average\nindividual worker accuracy by 6.8 percentage points on the Relation Extraction\ndomain, our workflow achieves 16.7 percentage point improvement. Furthermore,\nprevious argumentation approaches don't apply to tasks with many possible\nanswers; in contrast, Cicero works well in these cases, raising accuracy from\n66.7% to 98.8% on the Codenames domain.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 06:19:12 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Chen", "Quanze", ""], ["Bragg", "Jonathan", ""], ["Chilton", "Lydia B.", ""], ["Weld", "Daniel S.", ""]]}, {"id": "1810.10743", "submitter": "Jun Yang Dr.", "authors": "Min Chen, Jun Zhou, Guangming Tao, Jun Yang, Long Hu", "title": "Wearable Affective Robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of the artificial intelligence (AI), the AI applications\nhave influenced and changed people's daily life greatly. Here, a wearable\naffective robot that integrates the affective robot, social robot, brain\nwearable, and wearable 2.0 is proposed for the first time. The proposed\nwearable affective robot is intended for a wide population, and we believe that\nit can improve the human health on the spirit level, meeting the fashion\nrequirements at the same time. In this paper, the architecture and design of an\ninnovative wearable affective robot, which is dubbed as Fitbot, are introduced\nin terms of hardware and algorithm's perspectives. In addition, the important\nfunctional component of the robot-brain wearable device is introduced from the\naspect of the hardware design, EEG data acquisition and analysis, user behavior\nperception, and algorithm deployment, etc. Then, the EEG based cognition of\nuser's behavior is realized. Through the continuous acquisition of the\nin-depth, in-breadth data, the Fitbot we present can gradually enrich user's\nlife modeling and enable the wearable robot to recognize user's intention and\nfurther understand the behavioral motivation behind the user's emotion. The\nlearning algorithm for the life modeling embedded in Fitbot can achieve better\nuser's experience of affective social interaction. Finally, the application\nservice scenarios and some challenging issues of a wearable affective robot are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 07:27:47 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Chen", "Min", ""], ["Zhou", "Jun", ""], ["Tao", "Guangming", ""], ["Yang", "Jun", ""], ["Hu", "Long", ""]]}, {"id": "1810.10771", "submitter": "Irene Celino", "authors": "Irene Celino, Gloria Re Calegari", "title": "An Incremental Truth Inference Approach to Aggregate Crowdsourcing\n  Contributions in Games with a Purpose", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce our approach for incremental truth inference over the\ncontributions provided by players of Games with a Purpose: we motivate the need\nfor such a method with the specificity of GWAP vs. traditional crowdsourcing;\nwe explain and formalize the proposed process and we explain its positive\nconsequences; finally, we illustrate the results of an experimental comparison\nwith state-of-the-art approaches, performed on data collected through two\ndifferent GWAPs, thus showing the properties of our proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 08:43:29 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Celino", "Irene", ""], ["Calegari", "Gloria Re", ""]]}, {"id": "1810.11009", "submitter": "Ahmed Fadhil", "authors": "Ahmed Fadhil", "title": "The Good, The Bad & The Ugly Features: A Meta-analysis on User Review\n  About Food Journaling Apps", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Users review about an app is a crucial component for open mobile application\nmarket, such as the AppStore and the Google play. Analyzing these reviews can\nreveal user's sentiment towards a feature in the app. There exist several\nanalytical tools to summarize user reviews and extract meaningful sense out of\nthem. However, these tools are still limited in terms of expressiveness and\naccurately classifying the reviews into more than a positive and a negative\nreview. There is a need to get more insights from user app reviews and direct\nit to future app development. In this paper, we present our result of analyzing\nuser reviews of 20 food journaling and health tracking apps. We gathered and\nanalyzed reviews per app and classified them into three distinct categories\nusing the sentiment treebank with recursive neural tensor network. We then\nanalyzed the vocabulary frequency per category using the Gensim implementation\nof Word2Vec model. The analysis result clustered the reviews into good, bad and\nugly feature reviews. Different usage patterns were detected from users review.\nWe identified major reasons why users express a certain sentiment towards an\napp and learned how users' satisfaction or complaints was related to a specific\nfeature. This research could be a guideline for app developers to follow when\ndeveloping an app to refrain from adopting techniques that might demotivate\n(hinder) the application use or adopt those perceived positively by the users.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 11:05:17 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Fadhil", "Ahmed", ""]]}, {"id": "1810.11063", "submitter": "Filipo Sharevski", "authors": "Adam Trowbridge, Jessica Westbrook, Filipo Sharevski", "title": "Sorry: Ambient Tactical Deception Via Malware-Based Social Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we argue, drawing from the perspectives of cybersecurity and\nsocial psychology, that Internet-based manipulation of an individual or group\nreality using ambient tactical deception is possible using only software and\nchanging words in a web browser. We call this attack Ambient Tactical Deception\n(ATD). Ambient, in artificial intelligence, describes software that is\n\"unobtrusive,\" and completely integrated into a user's life. Tactical deception\nis an information warfare term for the use of deception on an opposing force.\nWe suggest that an ATD attack could change the sentiment of text in a web\nbrowser. This could alter the victim's perception of reality by providing\ndisinformation. Within the limit of online communication, even a pause in\nreplying to a text can affect how people perceive each other. The outcomes of\nan ATD attack could include alienation, upsetting a victim, and influencing\ntheir feelings about an election, a spouse, or a corporation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 18:42:01 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Trowbridge", "Adam", ""], ["Westbrook", "Jessica", ""], ["Sharevski", "Filipo", ""]]}, {"id": "1810.11094", "submitter": "Dominique Vaufreydaz", "authors": "Thomas Guntz (PERVASIVE), James Crowley (PERVASIVE), Dominique\n  Vaufreydaz (PERVASIVE), Raffaella Balzarini (PERVASIVE), Philippe Dessus\n  (LSE, PERVASIVE)", "title": "The Role of Emotion in Problem Solving: First Results from Observing\n  Chess", "comments": null, "journal-ref": "ICMI 2018 - Workshop at 20th ACM International Conference on\n  Multimodal Interaction, Oct 2018, Boulder, Colorado, United States. pp.1-13", "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present results from recent experiments that suggest that\nchess players associate emotions to game situations and reactively use these\nassociations to guide search for planning and problem solving. We describe the\ndesign of an instrument for capturing and interpreting multimodal signals of\nhumans engaged in solving challenging problems. We review results from a pilot\nexperiment with human experts engaged in solving challenging problems in Chess\nthat revealed an unexpected observation of rapid changes in emotion as players\nattempt to solve challenging problems. We propose a cognitive model that\ndescribes the process by which subjects select chess chunks for use in\ninterpretation of the game situation and describe initial results from a second\nexperiment designed to test this model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 06:35:31 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Guntz", "Thomas", "", "PERVASIVE"], ["Crowley", "James", "", "PERVASIVE"], ["Vaufreydaz", "Dominique", "", "PERVASIVE"], ["Balzarini", "Raffaella", "", "PERVASIVE"], ["Dessus", "Philippe", "", "LSE, PERVASIVE"]]}, {"id": "1810.11143", "submitter": "Yen-Chia Hsu", "authors": "Yen-Chia Hsu, Jennifer Cross, Paul Dille, Michael Tasota, Beatrice\n  Dias, Randy Sargent, Ting-Hao 'Kenneth' Huang, Illah Nourbakhsh", "title": "Smell Pittsburgh: Community-Empowered Mobile Smell Reporting System", "comments": "Accepted by ACM IUI 2019 conference, with error corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban air pollution has been linked to various human health considerations,\nincluding cardiopulmonary diseases. Communities who suffer from poor air\nquality often rely on experts to identify pollution sources due to the lack of\naccessible tools. Taking this into account, we developed Smell Pittsburgh, a\nsystem that enables community members to report odors and track where these\nodors are frequently concentrated. All smell report data are publicly\naccessible online. These reports are also sent to the local health department\nand visualized on a map along with air quality data from monitoring stations.\nThis visualization provides a comprehensive overview of the local pollution\nlandscape. Additionally, with these reports and air quality data, we developed\na model to predict upcoming smell events and send push notifications to inform\ncommunities. Our evaluation of this system demonstrates that engaging residents\nin documenting their experiences with pollution odors can help identify local\nair pollution patterns, and can empower communities to advocate for better air\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 23:51:05 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 16:43:31 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2019 18:55:57 GMT"}, {"version": "v4", "created": "Tue, 8 Jan 2019 19:30:17 GMT"}, {"version": "v5", "created": "Wed, 1 Jul 2020 17:27:20 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Hsu", "Yen-Chia", ""], ["Cross", "Jennifer", ""], ["Dille", "Paul", ""], ["Tasota", "Michael", ""], ["Dias", "Beatrice", ""], ["Sargent", "Randy", ""], ["Huang", "Ting-Hao 'Kenneth'", ""], ["Nourbakhsh", "Illah", ""]]}, {"id": "1810.11367", "submitter": "Eytan Adar", "authors": "Xin Rong, Joshua Luckson, Eytan Adar", "title": "LAMVI-2: A Visual Tool for Comparing and Tuning Word Embedding Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuning machine learning models, particularly deep learning architectures, is\na complex process. Automated hyperparameter tuning algorithms often depend on\nspecific optimization metrics. However, in many situations, a developer trades\none metric against another: accuracy versus overfitting, precision versus\nrecall, smaller models and accuracy, etc. With deep learning, not only are the\nmodel's representations opaque, the model's behavior when parameters \"knobs\"\nare changed may also be unpredictable. Thus, picking the \"best\" model often\nrequires time-consuming model comparison. In this work, we introduce LAMVI-2, a\nvisual analytics system to support a developer in comparing hyperparameter\nsettings and outcomes. By focusing on word-embedding models (\"deep learning for\ntext\") we integrate views to compare both high-level statistics as well as\ninternal model behaviors (e.g., comparing word 'distances'). We demonstrate how\ndevelopers can work with LAMVI-2 to more quickly and accurately narrow down an\nappropriate and effective application-specific model.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 20:05:42 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Rong", "Xin", ""], ["Luckson", "Joshua", ""], ["Adar", "Eytan", ""]]}, {"id": "1810.11545", "submitter": "Nicholas Waytowich", "authors": "Vinicius G. Goecks, Gregory M. Gremillion, Vernon J. Lawhern, John\n  Valasek, Nicholas R. Waytowich", "title": "Efficiently Combining Human Demonstrations and Interventions for Safe\n  Training of Autonomous Systems in Real-Time", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how to utilize different forms of human interaction\nto safely train autonomous systems in real-time by learning from both human\ndemonstrations and interventions. We implement two components of the\nCycle-of-Learning for Autonomous Systems, which is our framework for combining\nmultiple modalities of human interaction. The current effort employs human\ndemonstrations to teach a desired behavior via imitation learning, then\nleverages intervention data to correct for undesired behaviors produced by the\nimitation learner to teach novel tasks to an autonomous agent safely, after\nonly minutes of training. We demonstrate this method in an autonomous perching\ntask using a quadrotor with continuous roll, pitch, yaw, and throttle commands\nand imagery captured from a downward-facing camera in a high-fidelity simulated\nenvironment. Our method improves task completion performance for the same\namount of human interaction when compared to learning from demonstrations\nalone, while also requiring on average 32% less data to achieve that\nperformance. This provides evidence that combining multiple modes of human\ninteraction can increase both the training speed and overall performance of\npolicies for autonomous systems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 22:23:27 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 21:31:07 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Goecks", "Vinicius G.", ""], ["Gremillion", "Gregory M.", ""], ["Lawhern", "Vernon J.", ""], ["Valasek", "John", ""], ["Waytowich", "Nicholas R.", ""]]}, {"id": "1810.11593", "submitter": "Matthew Peveler", "authors": "Mathew Peveler and Jeffery Kephart and Hui Su", "title": "Reagent: Converting Ordinary Webpages into Interactive Software Agents", "comments": "Demo available at https://bit.ly/2OGKvez", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Reagent, a technology that readily converts ordinary webpages\ncontaining structured data into software agents with which one can interact\nnaturally, via a combination of speech and pointing. Previous efforts to make\nwebpage content manipulable by third-party software components in browsers or\ndesktop applications have generally relied upon specialized instrumentation\nincluded in the webpages -- a practice that neither scales well nor applies to\npre-existing webpages. In contrast, Reagent automatically captures semantic\ndetails and semantically-meaningful mouse events from arbitrary webpages that\ncontain no pre-existing special instrumentation. Reagent combines these events\nwith text transcriptions of user speech to derive and execute parameterized\ncommands representing human intent. Thus, users may request various\nvisualization or analytic operations to be performed on data displayed on a\npage by speaking to it and/or pointing to elements within it. When unable to\ninfer translations between event labels and human terminology, Reagent\nproactively asks users for definitions and adds them to its dictionary. We\ndemonstrate Reagent in the context of a collection of pre-existing webpages\nthat contain football team and player statistics.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 04:07:12 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Peveler", "Mathew", ""], ["Kephart", "Jeffery", ""], ["Su", "Hui", ""]]}, {"id": "1810.11748", "submitter": "Sosuke Kobayashi", "authors": "Riku Arakawa and Sosuke Kobayashi and Yuya Unno and Yuta Tsuboi and\n  Shin-ichi Maeda", "title": "DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable\n  Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration has been one of the greatest challenges in reinforcement learning\n(RL), which is a large obstacle in the application of RL to robotics. Even with\nstate-of-the-art RL algorithms, building a well-learned agent often requires\ntoo many trials, mainly due to the difficulty of matching its actions with\nrewards in the distant future. A remedy for this is to train an agent with\nreal-time feedback from a human observer who immediately gives rewards for some\nactions. This study tackles a series of challenges for introducing such a\nhuman-in-the-loop RL scheme. The first contribution of this work is our\nexperiments with a precisely modeled human observer: binary, delay,\nstochasticity, unsustainability, and natural reaction. We also propose an RL\nmethod called DQN-TAMER, which efficiently uses both human feedback and distant\nrewards. We find that DQN-TAMER agents outperform their baselines in Maze and\nTaxi simulated environments. Furthermore, we demonstrate a real-world\nhuman-in-the-loop RL application where a camera automatically recognizes a\nuser's facial expressions as feedback to the agent while the agent explores a\nmaze.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 02:18:40 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Arakawa", "Riku", ""], ["Kobayashi", "Sosuke", ""], ["Unno", "Yuya", ""], ["Tsuboi", "Yuta", ""], ["Maeda", "Shin-ichi", ""]]}, {"id": "1810.12379", "submitter": "S Uskudarli", "authors": "T. B. Dinesh and S. Uskudarli", "title": "Renarration for All", "comments": null, "journal-ref": "IIITB Data Science Communications, vol 1, (2016)", "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accessibility of content for all has been a key goal of the Web since its\nconception. However, true accessibility -- access to relevant content in the\nglobal context -- has been elusive for reasons that extend beyond physical\naccessibility issues. Among them are the spoken languages, literacy levels,\nexpertise, and culture. These issues are highly significant, since information\nmay not reach those who are the most in need of it. For example, the minimum\nwage laws that are published in legalese on government sites and the\nlow-literate and immigrant populations. While some organizations and volunteers\nwork on bridging such gaps by creating and disseminating alternative versions\nof such content, Web scale solutions much be developed to take advantage of its\ndistributed dissemination capabilities. This work examines content\naccessibility from the perspective of inclusiveness. For this purpose, a human\nin the loop approach for renarrating Web content is proposed, where a\nrenarrator creates an alternative narrative of some Web content with the intent\nof extending its reach. A renarration relates some Web content with an\nalternative version by means of transformations like simplification,\nelaboration, translation, or production of audio and video material. This work\npresents a model and a basic architecture for supporting renarrations along\nwith various scenarios. We also discuss the potentials of the W3C specification\nfor Web Annotation Data Model towards a more inclusive and decentralized social\nweb.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 19:42:41 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Dinesh", "T. B.", ""], ["Uskudarli", "S.", ""]]}, {"id": "1810.12514", "submitter": "Mehran Maghoumi", "authors": "Mehran Maghoumi and Joseph J. LaViola Jr", "title": "DeepGRU: Deep Gesture Recognition Utility", "comments": "Published in ISVC 2019. Code is available at\n  https://github.com/Maghoumi/DeepGRU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DeepGRU, a novel end-to-end deep network model informed by recent\ndevelopments in deep learning for gesture and action recognition, that is\nstreamlined and device-agnostic. DeepGRU, which uses only raw skeleton, pose or\nvector data is quickly understood, implemented, and trained, and yet achieves\nstate-of-the-art results on challenging datasets. At the heart of our method\nlies a set of stacked gated recurrent units (GRU), two fully-connected layers\nand a novel global attention model. We evaluate our method on seven publicly\navailable datasets, containing various number of samples and spanning over a\nbroad range of interactions (full-body, multi-actor, hand gestures, etc.). In\nall but one case we outperform the state-of-the-art pose-based methods. For\ninstance, we achieve a recognition accuracy of 84.9% and 92.3% on cross-subject\nand cross-view tests of the NTU RGB+D dataset respectively, and also 100%\nrecognition accuracy on the UT-Kinect dataset. While DeepGRU works well on\nlarge datasets with many training samples, we show that even in the absence of\na large number of training data, and with as little as four samples per class,\nDeepGRU can beat traditional methods specifically designed for small training\nsets. Lastly, we demonstrate that even without powerful hardware, and using\nonly the CPU, our method can still be trained in under 10 minutes on\nsmall-scale datasets, making it an enticing choice for rapid application\nprototyping and development.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 03:43:22 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 07:40:54 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2019 07:25:33 GMT"}, {"version": "v4", "created": "Thu, 10 Oct 2019 05:41:24 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Maghoumi", "Mehran", ""], ["LaViola", "Joseph J.", "Jr"]]}, {"id": "1810.12627", "submitter": "Hans-J\\\"urgen Profitlich", "authors": "Daniel Sonntag, Hans-J\\\"urgen Profitlich", "title": "An architecture of open-source tools to combine textual information\n  extraction, faceted search and information visualisation", "comments": "Preprint submitted to Artificial Intelligence in Medicine", "journal-ref": null, "doi": "10.1016/j.artmed.2018.08.003", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents our steps to integrate complex and partly unstructured\nmedical data into a clinical research database with subsequent decision\nsupport. Our main application is an integrated faceted search tool, accompanied\nby the visualisation of results of automatic information extraction from\ntextual documents. We describe the details of our technical architecture\n(open-source tools), to be replicated at other universities, research\ninstitutes, or hospitals. Our exemplary use cases are nephrology and\nmammography. The software was first developed in the nephrology domain and then\nadapted to the mammography use case. We report on these case studies,\nillustrating how the application can be used by a clinician and which questions\ncan be answered. We show that our architecture and the employed software\nmodules are suitable for both areas of application with a limited amount of\nadaptations. For example, in nephrology we try to answer questions about the\ntemporal characteristics of event sequences to gain significant insight from\nthe data for cohort selection. We present a versatile time-line tool that\nenables the user to explore relations between a multitude of diagnosis and\nlaboratory values.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 10:20:42 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Sonntag", "Daniel", ""], ["Profitlich", "Hans-J\u00fcrgen", ""]]}, {"id": "1810.12644", "submitter": "Joachim Meyer", "authors": "Nir Douer and Joachim Meyer", "title": "The Responsibility Quantification (ResQu) Model of Human Interaction\n  with Automation", "comments": null, "journal-ref": "IEEE Transactions on Automation Science and Engineering, Vol. 17\n  (2), pp. 1044-1060, April 2020", "doi": "10.1109/TASE.2020.2965466", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent systems and advanced automation are involved in information\ncollection and evaluation, in decision-making and in the implementation of\nchosen actions. In such systems, human responsibility becomes equivocal.\nUnderstanding human casual responsibility is particularly important when\nintelligent autonomous systems can harm people, as with autonomous vehicles or,\nmost notably, with autonomous weapon systems (AWS). Using Information Theory,\nwe develop a responsibility quantification (ResQu) model of human involvement\nin intelligent automated systems and demonstrate its applications on decisions\nregarding AWS. The analysis reveals that human comparative responsibility to\noutcomes is often low, even when major functions are allocated to the human.\nThus, broadly stated policies of keeping humans in the loop and having\nmeaningful human control are misleading and cannot truly direct decisions on\nhow to involve humans in intelligent systems and advanced automation. The\ncurrent model is an initial step in the complex goal to create a comprehensive\nresponsibility model, that will enable quantification of human causal\nresponsibility. It assumes stationarity, full knowledge regarding the\ncharacteristic of the human and automation and ignores temporal aspects.\nDespite these limitations, it can aid in the analysis of systems designs\nalternatives and policy decisions regarding human responsibility in intelligent\nsystems and advanced automation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 10:45:57 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 08:11:48 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 07:18:40 GMT"}, {"version": "v4", "created": "Wed, 29 Apr 2020 15:19:14 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Douer", "Nir", ""], ["Meyer", "Joachim", ""]]}, {"id": "1810.12718", "submitter": "Lukas Vermeer", "authors": "Bahattin Tolga \\\"Oztan, Zo\\'e van Havre, Caio Gomes, Lukas Vermeer", "title": "Mediation Analysis in Online Experiments at Booking.com: Disentangling\n  Direct and Indirect Effects", "comments": "Presented at the 2018 Conference on Digital Experimentation\n  (CODE@MIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online experimentation is at the core of Booking.com's customer-centric\nproduct development. While randomised controlled trials are a powerful tool for\nestimating the overall effects of product changes on business metrics, they\noften fall short in explaining the mechanism of change. This becomes\nproblematic when decision-making depends on being able to distinguish between\nthe direct effect of a treatment on some outcome variable and its indirect\neffect via a mediator variable. In this paper, we demonstrate the need for\nmediation analyses in online experimentation, and use simulated data to show\nhow these methods help identify and estimate direct causal effect. Failing to\ntake into account all confounders can lead to biased estimates, so we include\nsensitivity analyses to help gauge the robustness of estimates to missing\ncausal factors.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:37:37 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["\u00d6ztan", "Bahattin Tolga", ""], ["van Havre", "Zo\u00e9", ""], ["Gomes", "Caio", ""], ["Vermeer", "Lukas", ""]]}, {"id": "1810.13028", "submitter": "Nikhil Garg", "authors": "Nikhil Garg and Ramesh Johari", "title": "Designing Informative Rating Systems: Evidence from an Online Labor\n  Market", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Platforms critically rely on rating systems to learn the quality of market\nparticipants. In practice, however, these ratings are often highly inflated,\nand therefore not very informative. In this paper, we first investigate whether\nthe platform can obtain less inflated, more informative ratings by altering the\nmeaning and relative importance of the levels in the rating system. Second, we\nseek a principled approach for the platform to make these choices in the design\nof the rating system. First, we analyze the results of a randomized controlled\ntrial on an online labor market in which an additional question was added to\nthe feedback form. Between treatment conditions, we vary the question phrasing\nand answer choices; in particular, the treatment conditions include several\npositive-skewed verbal rating scales with descriptive phrases or adjectives\nproviding specific interpretation for each rating level. The online labor\nmarket test reveals that current inflationary norms can in fact be countered by\nre-anchoring the meaning of the levels of the rating system. In particular, the\npositive-skewed verbal rating scales yield rating distributions that\nsignificantly reduce rating inflation and are much more informative about\nseller quality. Second, we develop a model-based framework to compare and\nselect among rating system designs, and apply this framework to the data\nobtained from the online labor market test. Our simulations demonstrate that\nour model-based framework for scale design and optimization can identify the\nmost informative rating system and substantially improve the quality of\ninformation obtained over baseline designs. Overall, our study illustrates that\nrating systems that are informative in practice can be designed, and\ndemonstrates how to design them in a principled manner.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 23:15:39 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 16:55:40 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Garg", "Nikhil", ""], ["Johari", "Ramesh", ""]]}, {"id": "1810.13206", "submitter": "Himangshu Sarma", "authors": "Himangshu Sarma, Navanath Saharia", "title": "A speech-based driver assisting module for Intelligent Transport System", "comments": "3rd International Conference on Contemporary Computing and\n  Informatics (IC3I), India, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aim of this research is to transform images of roadside traffic panels to\nspeech to assist the vehicle driver, which is a new approach in the\nstate-of-the-art of the advanced driver assistance systems. The designed system\ncomprises of three modules, where the first module is used to capture and\ndetect the text area in traffic panels, second module is responsible for\nconverting the image of the detected text area to editable text and the last\nmodule is responsible for transforming the text to speech. Additionally, during\nexperiments, we developed a corpus of 250 images of traffic panels for two\nIndian languages.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 10:47:10 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Sarma", "Himangshu", ""], ["Saharia", "Navanath", ""]]}, {"id": "1810.13251", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Koji Yatani, Mark D. Gross, Tom Yeh", "title": "Tabby: Explorable Design for 3D Printing Textures", "comments": "Pacific Graphics 2018. arXiv admin note: substantial text overlap\n  with arXiv:1703.05700", "journal-ref": null, "doi": "10.2312/pg.20181273", "report-no": null, "categories": "cs.HC cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Tabby, an interactive and explorable design tool for 3D\nprinting textures. Tabby allows texture design with direct manipulation in the\nfollowing workflow: 1) select a target surface, 2) sketch and manipulate a\ntexture with 2D drawings, and then 3) generate 3D printing textures onto an\narbitrary curved surface. To enable efficient texture creation, Tabby leverages\nan auto-completion approach which automates the tedious, repetitive process of\napplying texture, while allowing flexible customization. Our user evaluation\nstudy with seven participants confirms that Tabby can effectively support the\ndesign exploration of different patterns for both novice and experienced users.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 17:40:06 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Suzuki", "Ryo", ""], ["Yatani", "Koji", ""], ["Gross", "Mark D.", ""], ["Yeh", "Tom", ""]]}]