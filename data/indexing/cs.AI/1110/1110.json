[{"id": "1110.0020", "submitter": "A. C. Cem Say", "authors": "\\\"O. Y{\\i}lmaz, A. C. C. Say", "title": "Causes of Ineradicable Spurious Predictions in Qualitative Simulation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  551-575, 2006", "doi": "10.1613/jair.2065", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently proved that a sound and complete qualitative simulator does\nnot exist, that is, as long as the input-output vocabulary of the\nstate-of-the-art QSIM algorithm is used, there will always be input models\nwhich cause any simulator with a coverage guarantee to make spurious\npredictions in its output. In this paper, we examine whether a meaningfully\nexpressive restriction of this vocabulary is possible so that one can build a\nsimulator with both the soundness and completeness properties. We prove several\nnegative results: All sound qualitative simulators, employing subsets of the\nQSIM representation which retain the operating region transition feature, and\nsupport at least the addition and constancy constraints, are shown to be\ninherently incomplete. Even when the simulations are restricted to run in a\nsingle operating region, a constraint vocabulary containing just the addition,\nconstancy, derivative, and multiplication relations makes the construction of\nsound and complete qualitative simulators impossible.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 20:44:00 GMT"}], "update_date": "2011-11-21", "authors_parsed": [["Y\u0131lmaz", "\u00d6.", ""], ["Say", "A. C. C.", ""]]}, {"id": "1110.0023", "submitter": "L. Liu", "authors": "L. Liu, M. Truszczynski", "title": "Properties and Applications of Programs with Monotone and Convex\n  Constraints", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  299-334, 2006", "doi": "10.1613/jair.2009", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study properties of programs with monotone and convex constraints. We\nextend to these formalisms concepts and results from normal logic programming.\nThey include the notions of strong and uniform equivalence with their\ncharacterizations, tight programs and Fages Lemma, program completion and loop\nformulas. Our results provide an abstract account of properties of some recent\nextensions of logic programming with aggregates, especially the formalism of\nlparse programs. They imply a method to compute stable models of lparse\nprograms by means of off-the-shelf solvers of pseudo-boolean constraints, which\nis often much faster than the smodels system.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 20:51:03 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Liu", "L.", ""], ["Truszczynski", "M.", ""]]}, {"id": "1110.0024", "submitter": "S. F. Smith", "authors": "S. F. Smith, M. J. Streeter", "title": "How the Landscape of Random Job Shop Scheduling Instances Depends on the\n  Ratio of Jobs to Machines", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 26, pages\n  247-287, 2006", "doi": "10.1613/jair.2013", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the search landscape of random instances of the job shop\nscheduling problem (JSP). Specifically, we investigate how the expected values\nof (1) backbone size, (2) distance between near-optimal schedules, and (3)\nmakespan of random schedules vary as a function of the job to machine ratio\n(N/M). For the limiting cases N/M approaches 0 and N/M approaches infinity we\nprovide analytical results, while for intermediate values of N/M we perform\nexperiments. We prove that as N/M approaches 0, backbone size approaches 100%,\nwhile as N/M approaches infinity the backbone vanishes. In the process we show\nthat as N/M approaches 0 (resp. N/M approaches infinity), simple priority rules\nalmost surely generate an optimal schedule, providing theoretical evidence of\nan \"easy-hard-easy\" pattern of typical-case instance difficulty in job shop\nscheduling. We also draw connections between our theoretical results and the\n\"big valley\" picture of JSP landscapes.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 20:51:28 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Smith", "S. F.", ""], ["Streeter", "M. J.", ""]]}, {"id": "1110.0026", "submitter": "B. Faltings", "authors": "B. Faltings, P. Pu, P. Viappiani", "title": "Preference-based Search using Example-Critiquing with Suggestions", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  465-503, 2006", "doi": "10.1613/jair.2075", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider interactive tools that help users search for their most preferred\nitem in a large collection of options. In particular, we examine\nexample-critiquing, a technique for enabling users to incrementally construct\npreference models by critiquing example options that are presented to them. We\npresent novel techniques for improving the example-critiquing technology by\nadding suggestions to its displayed options. Such suggestions are calculated\nbased on an analysis of users current preference model and their potential\nhidden preferences. We evaluate the performance of our model-based suggestion\ntechniques with both synthetic and real users. Results show that such\nsuggestions are highly attractive to users and can stimulate them to express\nmore preferences to improve the chance of identifying their most preferred item\nby up to 78%.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 20:55:50 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Faltings", "B.", ""], ["Pu", "P.", ""], ["Viappiani", "P.", ""]]}, {"id": "1110.0027", "submitter": "Daniel Bryce", "authors": "J. Pineau, G. Gordon, S. Thrun", "title": "Anytime Point-Based Approximations for Large POMDPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  335-380, 2006", "doi": "10.1613/jair.2078", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Partially Observable Markov Decision Process has long been recognized as\na rich framework for real-world planning and control problems, especially in\nrobotics. However exact solutions in this framework are typically\ncomputationally intractable for all but the smallest problems. A well-known\ntechnique for speeding up POMDP solving involves performing value backups at\nspecific belief points, rather than over the entire belief simplex. The\nefficiency of this approach, however, depends greatly on the selection of\npoints. This paper presents a set of novel techniques for selecting informative\nbelief points which work well in practice. The point selection procedure is\ncombined with point-based value backups to form an effective anytime POMDP\nalgorithm called Point-Based Value Iteration (PBVI). The first aim of this\npaper is to introduce this algorithm and present a theoretical analysis\njustifying the choice of belief selection technique. The second aim of this\npaper is to provide a thorough empirical comparison between PBVI and other\nstate-of-the-art POMDP methods, in particular the Perseus algorithm, in an\neffort to highlight their similarities and differences. Evaluation is performed\nusing both standard POMDP domains and realistic robotic tasks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 20:56:49 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2011 15:08:13 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Pineau", "J.", ""], ["Gordon", "G.", ""], ["Thrun", "S.", ""]]}, {"id": "1110.0028", "submitter": "C. Guestrin", "authors": "C. Guestrin, M. Hauskrecht, B. Kveton", "title": "Solving Factored MDPs with Hybrid State and Action Variables", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  153-201, 2006", "doi": "10.1613/jair.2085", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient representations and solutions for large decision problems with\ncontinuous and discrete variables are among the most important challenges faced\nby the designers of automated decision support systems. In this paper, we\ndescribe a novel hybrid factored Markov decision process (MDP) model that\nallows for a compact representation of these problems, and a new hybrid\napproximate linear programming (HALP) framework that permits their efficient\nsolutions. The central idea of HALP is to approximate the optimal value\nfunction by a linear combination of basis functions and optimize its weights by\nlinear programming. We analyze both theoretical and computational aspects of\nthis approach, and demonstrate its scale-up potential on several hybrid\noptimization problems.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 20:57:35 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Guestrin", "C.", ""], ["Hauskrecht", "M.", ""], ["Kveton", "B.", ""]]}, {"id": "1110.0029", "submitter": "Xavier Carreras", "authors": "M. Surdeanu, L. Marquez, X. Carreras, P. R. Comas", "title": "Combination Strategies for Semantic Role Labeling", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 29, pages\n  105-151, 2007", "doi": "10.1613/jair.2088", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and analyzes a battery of inference models for the\nproblem of semantic role labeling: one based on constraint satisfaction, and\nseveral strategies that model the inference as a meta-learning problem using\ndiscriminative classifiers. These classifiers are developed with a rich set of\nnovel features that encode proposition and sentence-level information. To our\nknowledge, this is the first work that: (a) performs a thorough analysis of\nlearning-based inference models for semantic role labeling, and (b) compares\nseveral inference strategies in this context. We evaluate the proposed\ninference strategies in the framework of the CoNLL-2005 shared task using only\nautomatically-generated syntactic information. The extensive experimental\nevaluation and analysis indicates that all the proposed inference strategies\nare successful -they all outperform the current best results reported in the\nCoNLL-2005 evaluation exercise- but each of the proposed approaches has its\nadvantages and disadvantages. Several important traits of a state-of-the-art\nSRL combination strategy emerge from this analysis: (i) individual models\nshould be combined at the granularity of candidate arguments rather than at the\ngranularity of complete solutions; (ii) the best combination strategy uses an\ninference model based in learning; and (iii) the learning-based inference\nbenefits from max-margin classifiers and global feedback.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 20:58:00 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2011 17:05:51 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Surdeanu", "M.", ""], ["Marquez", "L.", ""], ["Carreras", "X.", ""], ["Comas", "P. R.", ""]]}, {"id": "1110.0107", "submitter": "Roland Memisevic", "authors": "Roland Memisevic", "title": "Learning to relate images: Mapping units, complex cells and simultaneous\n  eigenspaces", "comments": "Revised argument in sections 4 and 3.3. Added illustration of\n  subspaces (Figure 13). Added inference Equation (Eq. 17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI nlin.AO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental operation in many vision tasks, including motion understanding,\nstereopsis, visual odometry, or invariant recognition, is establishing\ncorrespondences between images or between images and data from other\nmodalities. We present an analysis of the role that multiplicative interactions\nplay in learning such correspondences, and we show how learning and inferring\nrelationships between images can be viewed as detecting rotations in the\neigenspaces shared among a set of orthogonal matrices. We review a variety of\nrecent multiplicative sparse coding methods in light of this observation. We\nalso review how the squaring operation performed by energy models and by models\nof complex cells can be thought of as a way to implement multiplicative\ninteractions. This suggests that the main utility of including complex cells in\ncomputational models of vision may be that they can encode relations not\ninvariances.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2011 15:14:16 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2012 21:55:29 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Memisevic", "Roland", ""]]}, {"id": "1110.0214", "submitter": "Ridwan Al Iqbal", "authors": "Ridwan Al Iqbal", "title": "Eclectic Extraction of Propositional Rules from Neural Networks", "comments": "ICCIT 2011, Dhaka, Bangladesh", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Network is among the most popular algorithm for supervised\nlearning. However, Neural Networks have a well-known drawback of being a \"Black\nBox\" learner that is not comprehensible to the Users. This lack of transparency\nmakes it unsuitable for many high risk tasks such as medical diagnosis that\nrequires a rational justification for making a decision. Rule Extraction\nmethods attempt to curb this limitation by extracting comprehensible rules from\na trained Network. Many such extraction algorithms have been developed over the\nyears with their respective strengths and weaknesses. They have been broadly\ncategorized into three types based on their approach to use internal model of\nthe Network. Eclectic Methods are hybrid algorithms that combine the other\napproaches to attain more performance. In this paper, we present an Eclectic\nmethod called HERETIC. Our algorithm uses Inductive Decision Tree learning\ncombined with information of the neural network structure for extracting\nlogical rules. Experiments and theoretical analysis show HERETIC to be better\nin terms of speed and performance.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2011 18:59:42 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Iqbal", "Ridwan Al", ""]]}, {"id": "1110.0248", "submitter": "Yongzhi Cao", "authors": "Yongzhi Cao, Huaiqing Wang, Sherry X. Sun, and Guoqing Chen", "title": "A Behavioral Distance for Fuzzy-Transition Systems", "comments": "12 double column pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In contrast to the existing approaches to bisimulation for fuzzy systems, we\nintroduce a behavioral distance to measure the behavioral similarity of states\nin a nondeterministic fuzzy-transition system. This behavioral distance is\ndefined as the greatest fixed point of a suitable monotonic function and\nprovides a quantitative analogue of bisimilarity. The behavioral distance has\nthe important property that two states are at zero distance if and only if they\nare bisimilar. Moreover, for any given threshold, we find that states with\nbehavioral distances bounded by the threshold are equivalent. In addition, we\nshow that two system combinators---parallel composition and product---are\nnon-expansive with respect to our behavioral distance, which makes\ncompositional verification possible.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2011 00:32:22 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Cao", "Yongzhi", ""], ["Wang", "Huaiqing", ""], ["Sun", "Sherry X.", ""], ["Chen", "Guoqing", ""]]}, {"id": "1110.0532", "submitter": "Caleb Phillips", "authors": "Caleb Phillips, Lee Becker, and Elizabeth Bradley", "title": "Strange Beta: An Assistance System for Indoor Rock Climbing Route\n  Setting Using Chaotic Variations and Machine Learning", "comments": "University of Colorado Computer Science Department Technical Report", "journal-ref": "Chaos 22, 013130 (2012)", "doi": "10.1063/1.3693047", "report-no": "CU-CS-1087-11", "categories": "cs.AI cs.HC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies machine learning and the mathematics of chaos to the task\nof designing indoor rock-climbing routes. Chaotic variation has been used to\ngreat advantage on music and dance, but the challenges here are quite\ndifferent, beginning with the representation. We present a formalized system\nfor transcribing rock climbing problems, then describe a variation generator\nthat is designed to support human route-setters in designing new and\ninteresting climbing problems. This variation generator, termed Strange Beta,\ncombines chaos and machine learning, using the former to introduce novelty and\nthe latter to smooth transitions in a manner that is consistent with the style\nof the climbs This entails parsing the domain-specific natural language that\nrock climbers use to describe routes and movement and then learning the\npatterns in the results. We validated this approach with a pilot study in a\nsmall university rock climbing gym, followed by a large blinded study in a\ncommercial climbing gym, in cooperation with experienced climbers and expert\nroute setters. The results show that {\\sc Strange Beta} can help a human setter\nproduce routes that are at least as good as, and in some cases better than,\nthose produced in the traditional manner.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2011 22:23:46 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Phillips", "Caleb", ""], ["Becker", "Lee", ""], ["Bradley", "Elizabeth", ""]]}, {"id": "1110.0593", "submitter": "Duncan Blythe", "authors": "Duncan A. J. Blythe", "title": "Two Projection Pursuit Algorithms for Machine Learning under\n  Non-Stationarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis derives, tests and applies two linear projection algorithms for\nmachine learning under non-stationarity. The first finds a direction in a\nlinear space upon which a data set is maximally non-stationary. The second aims\nto robustify two-way classification against non-stationarity. The algorithm is\ntested on a key application scenario, namely Brain Computer Interfacing.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 07:34:13 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Blythe", "Duncan A. J.", ""]]}, {"id": "1110.0623", "submitter": "Arne Meier", "authors": "Arne Meier, Johannes Schmidt, Michael Thomas, Heribert Vollmer", "title": "On the Parameterized Complexity of Default Logic and Autoepistemic Logic", "comments": "12 pages + 2 pages appendix, 1 figure, Version without Appendix\n  submitted to LATA 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the application of Courcelle's Theorem and the logspace\nversion of Elberfeld etal. in the context of the implication problem for\npropositional sets of formulae, the extension existence problem for default\nlogic, as well as the expansion existence problem for autoepistemic logic and\nobtain fixed-parameter time and space efficient algorithms for these problems.\nOn the other hand, we exhibit, for each of the above problems, families of\ninstances of a very simple structure that, for a wide range of different\nparameterizations, do not have efficient fixed-parameter algorithms (even in\nthe sense of the large class XPnu), unless P=NP.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 09:51:25 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2011 19:47:26 GMT"}], "update_date": "2011-10-07", "authors_parsed": [["Meier", "Arne", ""], ["Schmidt", "Johannes", ""], ["Thomas", "Michael", ""], ["Vollmer", "Heribert", ""]]}, {"id": "1110.0624", "submitter": "Agostino Dovier", "authors": "Agostino Dovier and Andrea Formisano and Enrico Pontelli", "title": "Autonomous Agents Coordination: Action Languages meet CLP(FD) and Linda", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a knowledge representation formalism, in the form of a\nhigh-level Action Description Language for multi-agent systems, where\nautonomous agents reason and act in a shared environment. Agents are\nautonomously pursuing individual goals, but are capable of interacting through\na shared knowledge repository. In their interactions through shared portions of\nthe world, the agents deal with problems of synchronization and concurrency;\nthe action language allows the description of strategies to ensure a consistent\nglobal execution of the agents' autonomously derived plans. A distributed\nplanning problem is formalized by providing the declarative specifications of\nthe portion of the problem pertaining a single agent. Each of these\nspecifications is executable by a stand-alone CLP-based planner. The\ncoordination among agents exploits a Linda infrastructure. The proposal is\nvalidated in a prototype implementation developed in SICStus Prolog.\n  To appear in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 09:55:41 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Dovier", "Agostino", ""], ["Formisano", "Andrea", ""], ["Pontelli", "Enrico", ""]]}, {"id": "1110.0631", "submitter": "Fabrizio Riguzzi PhD", "authors": "Fabrizio Riguzzi and Terrance Swift", "title": "Well-Definedness and Efficient Inference for Probabilistic Logic\n  Programming under the Distribution Semantics", "comments": "31 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution semantics is one of the most prominent approaches for the\ncombination of logic programming and probability theory. Many languages follow\nthis semantics, such as Independent Choice Logic, PRISM, pD, Logic Programs\nwith Annotated Disjunctions (LPADs) and ProbLog. When a program contains\nfunctions symbols, the distribution semantics is well-defined only if the set\nof explanations for a query is finite and so is each explanation.\nWell-definedness is usually either explicitly imposed or is achieved by\nseverely limiting the class of allowed programs. In this paper we identify a\nlarger class of programs for which the semantics is well-defined together with\nan efficient procedure for computing the probability of queries. Since LPADs\noffer the most general syntax, we present our results for them, but our results\nare applicable to all languages under the distribution semantics. We present\nthe algorithm \"Probabilistic Inference with Tabling and Answer subsumption\"\n(PITA) that computes the probability of queries by transforming a probabilistic\nprogram into a normal program and then applying SLG resolution with answer\nsubsumption. PITA has been implemented in XSB and tested on six domains: two\nwith function symbols and four without. The execution times are compared with\nthose of ProbLog, cplint and CVE, PITA was almost always able to solve larger\nproblems in a shorter time, on domains with and without function symbols.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 10:29:32 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Riguzzi", "Fabrizio", ""], ["Swift", "Terrance", ""]]}, {"id": "1110.0879", "submitter": "Subhransu Maji", "authors": "Subhransu Maji", "title": "Linearized Additive Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the additive model learning literature and adapt a penalized\nspline formulation due to Eilers and Marx, to train additive classifiers\nefficiently. We also propose two new embeddings based two classes of orthogonal\nbasis with orthogonal derivatives, which can also be used to efficiently learn\nadditive classifiers. This paper follows the popular theme in the current\nliterature where kernel SVMs are learned much more efficiently using a\napproximate embedding and linear machine. In this paper we show that spline\nbasis are especially well suited for learning additive models because of their\nsparsity structure and the ease of computing the embedding which enables one to\ntrain these models in an online manner, without incurring the memory overhead\nof precomputing the storing the embeddings. We show interesting connections\nbetween B-Spline basis and histogram intersection kernel and show that for a\nparticular choice of regularization and degree of the B-Splines, our proposed\nlearning algorithm closely approximates the histogram intersection kernel SVM.\nThis enables one to learn additive models with almost no memory overhead\ncompared to fast a linear solver, such as LIBLINEAR, while being only 5-6X\nslower on average. On two large scale image classification datasets, MNIST and\nDaimler Chrysler pedestrians, the proposed additive classifiers are as accurate\nas the kernel SVM, while being two orders of magnitude faster to train.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 02:11:38 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Maji", "Subhransu", ""]]}, {"id": "1110.0999", "submitter": "Alberto Pettorossi", "authors": "Fabio Fioravanti, Alberto Pettorossi, Maurizio Proietti, Valerio Senni", "title": "Generalization Strategies for the Verification of Infinite State Systems", "comments": "24 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for the automated verification of temporal properties of\ninfinite state systems. Our verification method is based on the specialization\nof constraint logic programs (CLP) and works in two phases: (1) in the first\nphase, a CLP specification of an infinite state system is specialized with\nrespect to the initial state of the system and the temporal property to be\nverified, and (2) in the second phase, the specialized program is evaluated by\nusing a bottom-up strategy. The effectiveness of the method strongly depends on\nthe generalization strategy which is applied during the program specialization\nphase. We consider several generalization strategies obtained by combining\ntechniques already known in the field of program analysis and program\ntransformation, and we also introduce some new strategies. Then, through many\nverification experiments, we evaluate the effectiveness of the generalization\nstrategies we have considered. Finally, we compare the implementation of our\nspecialization-based verification method to other constraint-based model\nchecking tools. The experimental results show that our method is competitive\nwith the methods used by those other tools. To appear in Theory and Practice of\nLogic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 14:27:07 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Fioravanti", "Fabio", ""], ["Pettorossi", "Alberto", ""], ["Proietti", "Maurizio", ""], ["Senni", "Valerio", ""]]}, {"id": "1110.1016", "submitter": "S. Edelkamp", "authors": "S. Edelkamp, R. Englert, J. Hoffmann, F. Liporace, S. Thiebaux, S.\n  Trueg", "title": "Engineering Benchmarks for Planning: the Domains Used in the\n  Deterministic Part of IPC-4", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 26, pages\n  453-541, 2006", "doi": "10.1613/jair.1982", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a field of research about general reasoning mechanisms, it is essential to\nhave appropriate benchmarks. Ideally, the benchmarks should reflect possible\napplications of the developed technology. In AI Planning, researchers more and\nmore tend to draw their testing examples from the benchmark collections used in\nthe International Planning Competition (IPC). In the organization of (the\ndeterministic part of) the fourth IPC, IPC-4, the authors therefore invested\nsignificant effort to create a useful set of benchmarks. They come from five\ndifferent (potential) real-world applications of planning: airport ground\ntraffic control, oil derivative transportation in pipeline networks,\nmodel-checking safety properties, power supply restoration, and UMTS call\nsetup. Adapting and preparing such an application for use as a benchmark in the\nIPC involves, at the time, inevitable (often drastic) simplifications, as well\nas careful choice between, and engineering of, domain encodings. For the first\ntime in the IPC, we used compilations to formulate complex domain features in\nsimple languages such as STRIPS, rather than just dropping the more interesting\nproblem constraints in the simpler language subsets. The article explains and\ndiscusses the five application domains and their adaptation to form the PDDL\ntest suites used in IPC-4. We summarize known theoretical results on structural\nproperties of the domains, regarding their computational complexity and\nprovable properties of their topology under the h+ function (an idealized\nversion of the relaxed plan heuristic). We present new (empirical) results\nilluminating properties such as the quality of the most wide-spread heuristic\nfunctions (planning graph, serial planning graph, and relaxed plan), the growth\nof propositional representations over instance size, and the number of actions\navailable to achieve each fact; we discuss these data in conjunction with the\nbest results achieved by the different kinds of planners participating in\nIPC-4.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2011 19:02:41 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Edelkamp", "S.", ""], ["Englert", "R.", ""], ["Hoffmann", "J.", ""], ["Liporace", "F.", ""], ["Thiebaux", "S.", ""], ["Trueg", "S.", ""]]}, {"id": "1110.1228", "submitter": "Ehtibar Dzhafarov", "authors": "Ehtibar N. Dzhafarov and Janne V. Kujala", "title": "Order-distance and other metric-like functions on jointly distributed\n  random variables", "comments": "14 pages, to appear in Proc. Amer. Math. Soc", "journal-ref": "Proc. Amer. Math. Soc, 141, 3291-3301, 2013", "doi": "10.1090/S0002-9939-2013-11575-3", "report-no": null, "categories": "math.PR cs.AI math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a class of real-valued nonnegative binary functions on a set of\njointly distributed random variables, which satisfy the triangle inequality and\nvanish at identical arguments (pseudo-quasi-metrics). These functions are\nuseful in dealing with the problem of selective probabilistic causality\nencountered in behavioral sciences and in quantum physics. The problem reduces\nto that of ascertaining the existence of a joint distribution for a set of\nvariables with known distributions of certain subsets of this set. Any\nviolation of the triangle inequality or its consequences by one of our\nfunctions when applied to such a set rules out the existence of this joint\ndistribution. We focus on an especially versatile and widely applicable\npseudo-quasi-metric called an order-distance and its special case called a\nclassification distance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 11:28:24 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2011 11:01:18 GMT"}, {"version": "v3", "created": "Sat, 1 Sep 2012 00:49:43 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Dzhafarov", "Ehtibar N.", ""], ["Kujala", "Janne V.", ""]]}, {"id": "1110.1259", "submitter": "Federico Ricci-Tersenghi", "authors": "E. Dominguez, A. Lage-Castellanos, R. Mulet, F. Ricci-Tersenghi, T.\n  Rizzo", "title": "Characterizing and Improving Generalized Belief Propagation Algorithms\n  on the 2D Edwards-Anderson Model", "comments": "19 pages, 13 figures", "journal-ref": "J. Stat. Mech. P12007 (2011)", "doi": "10.1088/1742-5468/2011/12/P12007", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of different message passing algorithms in the two\ndimensional Edwards Anderson model. We show that the standard Belief\nPropagation (BP) algorithm converges only at high temperature to a paramagnetic\nsolution. Then, we test a Generalized Belief Propagation (GBP) algorithm,\nderived from a Cluster Variational Method (CVM) at the plaquette level. We\ncompare its performance with BP and with other algorithms derived under the\nsame approximation: Double Loop (DL) and a two-ways message passing algorithm\n(HAK). The plaquette-CVM approximation improves BP in at least three ways: the\nquality of the paramagnetic solution at high temperatures, a better estimate\n(lower) for the critical temperature, and the fact that the GBP message passing\nalgorithm converges also to non paramagnetic solutions. The lack of convergence\nof the standard GBP message passing algorithm at low temperatures seems to be\nrelated to the implementation details and not to the appearance of long range\norder. In fact, we prove that a gauge invariance of the constrained CVM free\nenergy can be exploited to derive a new message passing algorithm which\nconverges at even lower temperatures. In all its region of convergence this new\nalgorithm is faster than HAK and DL by some orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 13:37:04 GMT"}], "update_date": "2011-12-26", "authors_parsed": [["Dominguez", "E.", ""], ["Lage-Castellanos", "A.", ""], ["Mulet", "R.", ""], ["Ricci-Tersenghi", "F.", ""], ["Rizzo", "T.", ""]]}, {"id": "1110.1301", "submitter": "Stamatia Bibi", "authors": "Michael Deynet", "title": "Predicting User Actions in Software Processes", "comments": "4th Workshop on Intelligent Techniques in Software Engineering, 5\n  September 2011, at the European Conference on Machine Learning and Principles\n  and Practices of Knowledge Discovery in Databases (ECML-PKDD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an approach for user (e.g. SW architect) assisting in\nsoftware processes. The approach observes the user's action and tries to\npredict his next step. For this we use approaches in the area of machine\nlearning (sequence learning) and adopt these for the use in software processes.\n  Keywords: Software engineering, Software process description languages,\nSoftware processes, Machine learning, Sequence prediction\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 15:42:02 GMT"}], "update_date": "2011-10-07", "authors_parsed": [["Deynet", "Michael", ""]]}, {"id": "1110.1303", "submitter": "Stamatia Bibi", "authors": "Makrina Viola Kosti, Sofia Lazaridou, Nikoleta Bourazani, Lefteris\n  Angelis", "title": "Discovering patterns of correlation and similarities in software project\n  data with the Circos visualization tool", "comments": "4th Workshop on Intelligent Techniques in Software Engineering, 5\n  September 2011 at the European Conference on Machine Learning and Principles\n  and Practices of Knowledge Discovery in Databases (ECML-PKDD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software cost estimation based on multivariate data from completed projects\nrequires the building of efficient models. These models essentially describe\nrelations in the data, either on the basis of correlations between variables or\nof similarities between the projects. The continuous growth of the amount of\ndata gathered and the need to perform preliminary analysis in order to discover\npatterns able to drive the building of reasonable models, leads the researchers\ntowards intelligent and time-saving tools which can effectively describe data\nand their relationships. The goal of this paper is to suggest an innovative\nvisualization tool, widely used in bioinformatics, which represents relations\nin data in an aesthetic and intelligent way. In order to illustrate the\ncapabilities of the tool, we use a well known dataset from software engineering\nprojects.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 15:48:11 GMT"}], "update_date": "2011-10-07", "authors_parsed": [["Kosti", "Makrina Viola", ""], ["Lazaridou", "Sofia", ""], ["Bourazani", "Nikoleta", ""], ["Angelis", "Lefteris", ""]]}, {"id": "1110.1328", "submitter": "Venu Satuluri", "authors": "Venu Satuluri and Srinivasan Parthasarathy", "title": "Bayesian Locality Sensitive Hashing for Fast Similarity Search", "comments": "13 pages, 5 Tables, 21 figures. Added acknowledgments in v3. A\n  slightly shorter version of this paper without the appendix has been\n  published in the PVLDB journal, 5(5):430-441, 2012.\n  http://vldb.org/pvldb/vol5/p430_venusatuluri_vldb2012.pdf", "journal-ref": "PVLDB 5(5):430-441, 2012", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of objects and an associated similarity measure, the\nall-pairs similarity search problem asks us to find all pairs of objects with\nsimilarity greater than a certain user-specified threshold. Locality-sensitive\nhashing (LSH) based methods have become a very popular approach for this\nproblem. However, most such methods only use LSH for the first phase of\nsimilarity search - i.e. efficient indexing for candidate generation. In this\npaper, we present BayesLSH, a principled Bayesian algorithm for the subsequent\nphase of similarity search - performing candidate pruning and similarity\nestimation using LSH. A simpler variant, BayesLSH-Lite, which calculates\nsimilarities exactly, is also presented. BayesLSH is able to quickly prune away\na large majority of the false positive candidate pairs, leading to significant\nspeedups over baseline approaches. For BayesLSH, we also provide probabilistic\nguarantees on the quality of the output, both in terms of accuracy and recall.\nFinally, the quality of BayesLSH's output can be easily tuned and does not\nrequire any manual setting of the number of hashes to use for similarity\nestimation, unlike standard approaches. For two state-of-the-art candidate\ngeneration algorithms, AllPairs and LSH, BayesLSH enables significant speedups,\ntypically in the range 2x-20x for a wide variety of datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 17:13:48 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2011 17:46:46 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2012 19:34:39 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Satuluri", "Venu", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1110.1391", "submitter": "K. Choi", "authors": "K. Choi, H. Isahara, J. Oh", "title": "A Comparison of Different Machine Transliteration Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  119-151, 2006", "doi": "10.1613/jair.1999", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine transliteration is a method for automatically converting words in one\nlanguage into phonetically equivalent ones in another language. Machine\ntransliteration plays an important role in natural language applications such\nas information retrieval and machine translation, especially for handling\nproper nouns and technical terms. Four machine transliteration models --\ngrapheme-based transliteration model, phoneme-based transliteration model,\nhybrid transliteration model, and correspondence-based transliteration model --\nhave been proposed by several researchers. To date, however, there has been\nlittle research on a framework in which multiple transliteration models can\noperate simultaneously. Furthermore, there has been no comparison of the four\nmodels within the same framework and using the same data. We addressed these\nproblems by 1) modeling the four models within the same framework, 2) comparing\nthem under the same conditions, and 3) developing a way to improve machine\ntransliteration through this comparison. Our comparison showed that the hybrid\nand correspondence-based models were the most effective and that the four\nmodels can be used in a complementary manner to improve machine transliteration\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 20:48:58 GMT"}], "update_date": "2011-10-10", "authors_parsed": [["Choi", "K.", ""], ["Isahara", "H.", ""], ["Oh", "J.", ""]]}, {"id": "1110.1394", "submitter": "M. Lapata", "authors": "M. Lapata, A. Lascarides", "title": "Learning Sentence-internal Temporal Relations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  85-117, 2006", "doi": "10.1613/jair.2015", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a data intensive approach for inferring\nsentence-internal temporal relations. Temporal inference is relevant for\npractical NLP applications which either extract or synthesize temporal\ninformation (e.g., summarisation, question answering). Our method bypasses the\nneed for manual coding by exploiting the presence of markers like after\", which\novertly signal a temporal relation. We first show that models trained on main\nand subordinate clauses connected with a temporal marker achieve good\nperformance on a pseudo-disambiguation task simulating temporal inference\n(during testing the temporal marker is treated as unseen and the models must\nselect the right marker from a set of possible candidates). Secondly, we assess\nwhether the proposed approach holds promise for the semi-automatic creation of\ntemporal annotations. Specifically, we use a model trained on noisy and\napproximate data (i.e., main and subordinate clauses) to predict\nintra-sentential relations present in TimeBank, a corpus annotated rich\ntemporal information. Our experiments compare and contrast several\nprobabilistic models differing in their feature space, linguistic assumptions\nand data requirements. We evaluate performance against gold standard corpora\nand also against human subjects.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 20:55:54 GMT"}], "update_date": "2011-10-10", "authors_parsed": [["Lapata", "M.", ""], ["Lascarides", "A.", ""]]}, {"id": "1110.1416", "submitter": "Yuming Xu", "authors": "Xu Yuming", "title": "The matrices of argumentation frameworks", "comments": "20pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We introduce matrix and its block to the Dung's theory of argumentation\nframeworks. It is showed that each argumentation framework has a matrix\nrepresentation, and the common extension-based semantics of argumentation\nframework can be characterized by blocks of matrix and their relations. In\ncontrast with traditional method of directed graph, the matrix way has the\nadvantage of computability. Therefore, it has an extensive perspective to bring\nthe theory of matrix into the research of argumentation frameworks and related\nareas.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2011 00:28:58 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2011 01:35:56 GMT"}], "update_date": "2011-10-20", "authors_parsed": [["Yuming", "Xu", ""]]}, {"id": "1110.1892", "submitter": "Roberto Rossi", "authors": "Roberto Rossi and Brahim Hnich and S. Armagan Tarim and Steven\n  Prestwich", "title": "Confidence-based Reasoning in Stochastic Constraint Programming", "comments": "53 pages, working draft", "journal-ref": "Artificial Intelligence, Elsevier, 228(1):129-152, 2015", "doi": "10.1016/j.artint.2015.07.004", "report-no": null, "categories": "math.OC cs.AI math.CO math.PR stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a novel approach, based on sampling, for finding\nassignments that are likely to be solutions to stochastic constraint\nsatisfaction problems and constraint optimisation problems. Our approach\nreduces the size of the original problem being analysed; by solving this\nreduced problem, with a given confidence probability, we obtain assignments\nthat satisfy the chance constraints in the original model within prescribed\nerror tolerance thresholds. To achieve this, we blend concepts from stochastic\nconstraint programming and statistics. We discuss both exact and approximate\nvariants of our method. The framework we introduce can be immediately employed\nin concert with existing approaches for solving stochastic constraint programs.\nA thorough computational study on a number of stochastic combinatorial\noptimisation problems demonstrates the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2011 22:47:47 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 23:05:36 GMT"}, {"version": "v3", "created": "Thu, 24 Jul 2014 15:58:15 GMT"}, {"version": "v4", "created": "Sat, 8 Nov 2014 16:22:45 GMT"}, {"version": "v5", "created": "Sun, 19 Apr 2015 12:20:27 GMT"}, {"version": "v6", "created": "Sun, 5 Jul 2015 13:58:53 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Rossi", "Roberto", ""], ["Hnich", "Brahim", ""], ["Tarim", "S. Armagan", ""], ["Prestwich", "Steven", ""]]}, {"id": "1110.1992", "submitter": "Stamatia Bibi", "authors": "Eleni Constantinou, George Kakarontzas, and Ioannis Stamelos", "title": "Open Source Software: How Can Design Metrics Facilitate Architecture\n  Recovery?", "comments": "4th Workshop on Intelligent Techniques in Software Engineering, 5\n  September 2011 at the European Conference on Machine Learning and Principles\n  and Practices of Knowledge Discovery in Databases (ECML-PKDD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern software development methodologies include reuse of open source code.\nReuse can be facilitated by architectural knowledge of the software, not\nnecessarily provided in the documentation of open source software. The effort\nrequired to comprehend the system's source code and discover its architecture\ncan be considered a major drawback in reuse. In a recent study we examined the\ncorrelations between design metrics and classes' architecture layer. In this\npaper, we apply our methodology in more open source projects to verify the\napplicability of our method. Keywords: system understanding; program\ncomprehension; object-oriented; reuse; architecture layer; design metrics;\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 10:33:09 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Constantinou", "Eleni", ""], ["Kakarontzas", "George", ""], ["Stamelos", "Ioannis", ""]]}, {"id": "1110.2162", "submitter": "Ruben Sipos", "authors": "Ruben Sipos, Pannaga Shivaswamy, Thorsten Joachims", "title": "Large-Margin Learning of Submodular Summarization Methods", "comments": "update: improved formatting (figure placement) and algorithm\n  pseudocode clarity (Fig. 3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a supervised learning approach to training\nsubmodular scoring functions for extractive multi-document summarization. By\ntaking a structured predicition approach, we provide a large-margin method that\ndirectly optimizes a convex relaxation of the desired performance measure. The\nlearning method applies to all submodular summarization methods, and we\ndemonstrate its effectiveness for both pairwise as well as coverage-based\nscoring functions on multiple datasets. Compared to state-of-the-art functions\nthat were tuned manually, our method significantly improves performance and\nenables high-fidelity models with numbers of parameters well beyond what could\nreasonbly be tuned by hand.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 19:54:57 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2011 17:51:20 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Sipos", "Ruben", ""], ["Shivaswamy", "Pannaga", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1110.2200", "submitter": "M. Fox", "authors": "M. Fox, D. Long", "title": "Modelling Mixed Discrete-Continuous Domains for Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  235-297, 2006", "doi": "10.1613/jair.2044", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present pddl+, a planning domain description language for\nmodelling mixed discrete-continuous planning domains. We describe the syntax\nand modelling style of pddl+, showing that the language makes convenient the\nmodelling of complex time-dependent effects. We provide a formal semantics for\npddl+ by mapping planning instances into constructs of hybrid automata. Using\nthe syntax of HAs as our semantic model we construct a semantic mapping to\nlabelled transition systems to complete the formal interpretation of pddl+\nplanning instances. An advantage of building a mapping from pddl+ to HA theory\nis that it forms a bridge between the Planning and Real Time Systems research\ncommunities. One consequence is that we can expect to make use of some of the\ntheoretical properties of HAs. For example, for a restricted class of HAs the\nReachability problem (which is equivalent to Plan Existence) is decidable.\npddl+ provides an alternative to the continuous durative action model of\npddl2.1, adding a more flexible and robust model of time-dependent behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 21:16:30 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Fox", "M.", ""], ["Long", "D.", ""]]}, {"id": "1110.2203", "submitter": "R. H. C. Yap", "authors": "R. H. C. Yap, Y. Zhang", "title": "Set Intersection and Consistency in Constraint Networks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  441-464, 2006", "doi": "10.1613/jair.2058", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that there is a close relation between consistency in\na constraint network and set intersection. A proof schema is provided as a\ngeneric way to obtain consistency properties from properties on set\nintersection. This approach not only simplifies the understanding of and\nunifies many existing consistency results, but also directs the study of\nconsistency to that of set intersection properties in many situations, as\ndemonstrated by the results on the convexity and tightness of constraints in\nthis paper. Specifically, we identify a new class of tree convex constraints\nwhere local consistency ensures global consistency. This generalizes row convex\nconstraints. Various consistency results are also obtained on constraint\nnetworks where only some, in contrast to all in the existing work,constraints\nare tight.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 21:27:27 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Yap", "R. H. C.", ""], ["Zhang", "Y.", ""]]}, {"id": "1110.2204", "submitter": "J. Culberson", "authors": "J. Culberson, Y. Gao", "title": "Consistency and Random Constraint Satisfaction Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 28, pages\n  517-557, 2007", "doi": "10.1613/jair.2155", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the possibility of designing non-trivial random CSP\nmodels by exploiting the intrinsic connection between structures and\ntypical-case hardness. We show that constraint consistency, a notion that has\nbeen developed to improve the efficiency of CSP algorithms, is in fact the key\nto the design of random CSP models that have interesting phase transition\nbehavior and guaranteed exponential resolution complexity without putting much\nrestriction on the parameter of constraint tightness or the domain size of the\nproblem. We propose a very flexible framework for constructing problem\ninstances withinteresting behavior and develop a variety of concrete methods to\nconstruct specific random CSP models that enforce different levels of\nconstraint consistency. A series of experimental studies with interesting\nobservations are carried out to illustrate the effectiveness of introducing\nstructural elements in random instances, to verify the robustness of our\nproposal, and to investigate features of some specific models based on our\nframework that are highly related to the behavior of backtracking search\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 21:35:30 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Culberson", "J.", ""], ["Gao", "Y.", ""]]}, {"id": "1110.2205", "submitter": "E. Pontelli", "authors": "E. Pontelli, T. C. Son, P. H. Tu", "title": "Answer Sets for Logic Programs with Arbitrary Abstract Constraint Atoms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 29, pages\n  353-389, 2007", "doi": "10.1613/jair.2171", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present two alternative approaches to defining answer sets\nfor logic programs with arbitrary types of abstract constraint atoms (c-atoms).\nThese approaches generalize the fixpoint-based and the level mapping based\nanswer set semantics of normal logic programs to the case of logic programs\nwith arbitrary types of c-atoms. The results are four different answer set\ndefinitions which are equivalent when applied to normal logic programs. The\nstandard fixpoint-based semantics of logic programs is generalized in two\ndirections, called answer set by reduct and answer set by complement. These\ndefinitions, which differ from each other in the treatment of\nnegation-as-failure (naf) atoms, make use of an immediate consequence operator\nto perform answer set checking, whose definition relies on the notion of\nconditional satisfaction of c-atoms w.r.t. a pair of interpretations. The other\ntwo definitions, called strongly and weakly well-supported models, are\ngeneralizations of the notion of well-supported models of normal logic programs\nto the case of programs with c-atoms. As for the case of fixpoint-based\nsemantics, the difference between these two definitions is rooted in the\ntreatment of naf atoms. We prove that answer sets by reduct (resp. by\ncomplement) are equivalent to weakly (resp. strongly) well-supported models of\na program, thus generalizing the theorem on the correspondence between stable\nmodels and well-supported models of a normal logic program to the class of\nprograms with c-atoms. We show that the newly defined semantics coincide with\npreviously introduced semantics for logic programs with monotone c-atoms, and\nthey extend the original answer set semantics of normal logic programs. We also\nstudy some properties of answer sets of programs with c-atoms, and relate our\ndefinitions to several semantics for logic programs with aggregates presented\nin the literature.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 21:36:17 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Pontelli", "E.", ""], ["Son", "T. C.", ""], ["Tu", "P. H.", ""]]}, {"id": "1110.2209", "submitter": "A. S. Fukunaga", "authors": "A. S. Fukunaga, R. E. Korf", "title": "Bin Completion Algorithms for Multicontainer Packing, Knapsack, and\n  Covering Problems", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 28, pages\n  393-429, 2007", "doi": "10.1613/jair.2106", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many combinatorial optimization problems such as the bin packing and multiple\nknapsack problems involve assigning a set of discrete objects to multiple\ncontainers. These problems can be used to model task and resource allocation\nproblems in multi-agent systems and distributed systms, and can also be found\nas subproblems of scheduling problems. We propose bin completion, a\nbranch-and-bound strategy for one-dimensional, multicontainer packing problems.\nBin completion combines a bin-oriented search space with a powerful dominance\ncriterion that enables us to prune much of the space. The performance of the\nbasic bin completion framework can be enhanced by using a number of extensions,\nincluding nogood-based pruning techniques that allow further exploitation of\nthe dominance criterion. Bin completion is applied to four problems: multiple\nknapsack, bin covering, min-cost covering, and bin packing. We show that our\nbin completion algorithms yield new, state-of-the-art results for the multiple\nknapsack, bin covering, and min-cost covering problems, outperforming previous\nalgorithms by several orders of magnitude with respect to runtime on some\nclasses of hard, random problem instances. For the bin packing problem, we\ndemonstrate significant improvements compared to most previous results, but\nshow that bin completion is not competitive with current state-of-the-art\ncutting-stock based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 21:55:37 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Fukunaga", "A. S.", ""], ["Korf", "R. E.", ""]]}, {"id": "1110.2211", "submitter": "L. P. Kaelbling", "authors": "L. P. Kaelbling, H. M. Pasula, L. S. Zettlemoyer", "title": "Learning Symbolic Models of Stochastic Domains", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 29, pages\n  309-352, 2007", "doi": "10.1613/jair.2113", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we work towards the goal of developing agents that can learn\nto act in complex worlds. We develop a probabilistic, relational planning rule\nrepresentation that compactly models noisy, nondeterministic action effects,\nand show how such rules can be effectively learned. Through experiments in\nsimple planning domains and a 3D simulated blocks world with realistic physics,\nwe demonstrate that this learning algorithm allows agents to effectively model\nworld dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 21:58:58 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Kaelbling", "L. P.", ""], ["Pasula", "H. M.", ""], ["Zettlemoyer", "L. S.", ""]]}, {"id": "1110.2212", "submitter": "F. Rossi", "authors": "F. Rossi, K. B. Venable, N. Yorke-Smith", "title": "Uncertainty in Soft Temporal Constraint Problems:A General Framework and\n  Controllability Algorithms for the Fuzzy Case", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  617-674, 2006", "doi": "10.1613/jair.2135", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-life temporal scenarios, uncertainty and preferences are often\nessential and coexisting aspects. We present a formalism where quantitative\ntemporal constraints with both preferences and uncertainty can be defined. We\nshow how three classical notions of controllability (that is, strong, weak, and\ndynamic), which have been developed for uncertain temporal problems, can be\ngeneralized to handle preferences as well. After defining this general\nframework, we focus on problems where preferences follow the fuzzy approach,\nand with properties that assure tractability. For such problems, we propose\nalgorithms to check the presence of the controllability properties. In\nparticular, we show that in such a setting dealing simultaneously with\npreferences and uncertainty does not increase the complexity of controllability\ntesting. We also develop a dynamic execution algorithm, of polynomial\ncomplexity, that produces temporal plans under uncertainty that are optimal\nwith respect to fuzzy preferences.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 22:02:16 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Rossi", "F.", ""], ["Venable", "K. B.", ""], ["Yorke-Smith", "N.", ""]]}, {"id": "1110.2213", "submitter": "C. Bettini", "authors": "C. Bettini, S. Mascetti, X. S. Wang", "title": "Supporting Temporal Reasoning by Mapping Calendar Expressions to Minimal\n  Periodic Sets", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 28, pages\n  299-348, 2007", "doi": "10.1613/jair.2136", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years several research efforts have focused on the concept of\ntime granularity and its applications. A first stream of research investigated\nthe mathematical models behind the notion of granularity and the algorithms to\nmanage temporal data based on those models. A second stream of research\ninvestigated symbolic formalisms providing a set of algebraic operators to\ndefine granularities in a compact and compositional way. However, only very\nlimited manipulation algorithms have been proposed to operate directly on the\nalgebraic representation making it unsuitable to use the symbolic formalisms in\napplications that need manipulation of granularities.\n  This paper aims at filling the gap between the results from these two streams\nof research, by providing an efficient conversion from the algebraic\nrepresentation to the equivalent low-level representation based on the\nmathematical models. In addition, the conversion returns a minimal\nrepresentation in terms of period length. Our results have a major practical\nimpact: users can more easily define arbitrary granularities in terms of\nalgebraic operators, and then access granularity reasoning and other services\noperating efficiently on the equivalent, minimal low-level representation. As\nan example, we illustrate the application to temporal constraint reasoning with\nmultiple granularities.\n  From a technical point of view, we propose an hybrid algorithm that\ninterleaves the conversion of calendar subexpressions into periodical sets with\nthe minimization of the period length. The algorithm returns set-based\ngranularity representations having minimal period length, which is the most\nrelevant parameter for the performance of the considered reasoning services.\nExtensive experimental work supports the techniques used in the algorithm, and\nshows the efficiency and effectiveness of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 22:03:55 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Bettini", "C.", ""], ["Mascetti", "S.", ""], ["Wang", "X. S.", ""]]}, {"id": "1110.2216", "submitter": "P. F. Felzenszwalb", "authors": "P. F. Felzenszwalb, D. McAllester", "title": "The Generalized A* Architecture", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 29, pages\n  153-190, 2007", "doi": "10.1613/jair.2187", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing a lightest derivation of a global\nstructure using a set of weighted rules. A large variety of inference problems\nin AI can be formulated in this framework. We generalize A* search and\nheuristics derived from abstractions to a broad class of lightest derivation\nproblems. We also describe a new algorithm that searches for lightest\nderivations using a hierarchy of abstractions. Our generalization of A* gives a\nnew algorithm for searching AND/OR graphs in a bottom-up fashion. We discuss\nhow the algorithms described here provide a general architecture for addressing\nthe pipeline problem --- the problem of passing information back and forth\nbetween various stages of processing in a perceptual system. We consider\nexamples in computer vision and natural language processing. We apply the\nhierarchical search algorithm to the problem of estimating the boundaries of\nconvex objects in grayscale images and compare it to other search methods. A\nsecond set of experiments demonstrate the use of a new compositional model for\nfinding salient curves in images.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 22:14:15 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Felzenszwalb", "P. F.", ""], ["McAllester", "D.", ""]]}, {"id": "1110.2341", "submitter": "Mehdi Kashefikia", "authors": "Mehdi Kashefikia, Nasser Nematbakhsh, Reza Askari Moghadam", "title": "Multiple ant-bee colony optimization for load balancing in\n  packet-switched networks", "comments": "This paper has been withdrawn by the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the important issues in computer networks is \"Load Balancing\" which\nleads to efficient use of the network resources. To achieve a balanced network\nit is necessary to find different routes between the source and destination. In\nthe current paper we propose a new approach to find different routes using\nswarm intelligence techniques and multi colony algorithms. In the proposed\nalgorithm that is an improved version of MACO algorithm, we use different\ncolonies of ants and bees and appoint these colony members as intelligent\nagents to monitor the network and update the routing information. The survey\nincludes comparison and critiques of MACO. The simulation results show a\ntangible improvement in the aforementioned approach.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2011 12:15:35 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2011 06:22:12 GMT"}], "update_date": "2011-10-27", "authors_parsed": [["Kashefikia", "Mehdi", ""], ["Nematbakhsh", "Nasser", ""], ["Moghadam", "Reza Askari", ""]]}, {"id": "1110.2407", "submitter": "Ricardo  Rodriguez Oscar", "authors": "Xavier Caicedo and Ricardo Oscar Rodriguez", "title": "Bi-modal G\\\"odel logic over [0,1]-valued Kripke frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the G\\\"odel bi-modal logic determined by fuzzy Kripke models\nwhere both the propositions and the accessibility relation are infinitely\nvalued over the standard G\\\"odel algebra [0,1] and prove strong completeness of\nFischer Servi intuitionistic modal logic IK plus the prelinearity axiom with\nrespect to this semantics. We axiomatize also the bi-modal analogues of $T,$\n$S4,$ and $S5$ obtained by restricting to models over frames satisfying the\n[0,1]-valued versions of the structural properties which characterize these\nlogics. As application of the completeness theorems we obtain a representation\ntheorem for bi-modal G\\\"odel algebras.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2011 15:51:16 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Caicedo", "Xavier", ""], ["Rodriguez", "Ricardo Oscar", ""]]}, {"id": "1110.2726", "submitter": "D. Gabelaia", "authors": "D. Gabelaia, R. Kontchakov, A. Kurucz, F. Wolter, M. Zakharyaschev", "title": "Combining Spatial and Temporal Logics: Expressiveness vs. Complexity", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 23, pages\n  167-243, 2005", "doi": "10.1613/jair.1537", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we construct and investigate a hierarchy of spatio-temporal\nformalisms that result from various combinations of propositional spatial and\ntemporal logics such as the propositional temporal logic PTL, the spatial\nlogics RCC-8, BRCC-8, S4u and their fragments. The obtained results give a\nclear picture of the trade-off between expressiveness and computational\nrealisability within the hierarchy. We demonstrate how different combining\nprinciples as well as spatial and temporal primitives can produce NP-, PSPACE-,\nEXPSPACE-, 2EXPSPACE-complete, and even undecidable spatio-temporal logics out\nof components that are at most NP- or PSPACE-complete.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:17:41 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Gabelaia", "D.", ""], ["Kontchakov", "R.", ""], ["Kurucz", "A.", ""], ["Wolter", "F.", ""], ["Zakharyaschev", "M.", ""]]}, {"id": "1110.2728", "submitter": "A. Gerevini", "authors": "A. Gerevini, A. Saetti, I. Serina", "title": "An Approach to Temporal Planning and Scheduling in Domains with\n  Predictable Exogenous Events", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  187-231, 2006", "doi": "10.1613/jair.1742", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The treatment of exogenous events in planning is practically important in\nmany real-world domains where the preconditions of certain plan actions are\naffected by such events. In this paper we focus on planning in temporal domains\nwith exogenous events that happen at known times, imposing the constraint that\ncertain actions in the plan must be executed during some predefined time\nwindows. When actions have durations, handling such temporal constraints adds\nan extra difficulty to planning. We propose an approach to planning in these\ndomains which integrates constraint-based temporal reasoning into a graph-based\nplanning framework using local search. Our techniques are implemented in a\nplanner that took part in the 4th International Planning Competition (IPC-4). A\nstatistical analysis of the results of IPC-4 demonstrates the effectiveness of\nour approach in terms of both CPU-time and plan quality. Additional experiments\nshow the good performance of the temporal reasoning techniques integrated into\nour planner.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:18:04 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Gerevini", "A.", ""], ["Saetti", "A.", ""], ["Serina", "I.", ""]]}, {"id": "1110.2729", "submitter": "F. Bacchus", "authors": "F. Bacchus", "title": "The Power of Modeling - a Response to PDDL2.1", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  125-132, 2003", "doi": "10.1613/jair.1993", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this commentary I argue that although PDDL is a very useful standard for\nthe planning competition, its design does not properly consider the issue of\ndomain modeling. Hence, I would not advocate its use in specifying planning\ndomains outside of the context of the planning competition. Rather, the field\nneeds to explore different approaches and grapple more directly with the\nproblem of effectively modeling and utilizing all of the diverse pieces of\nknowledge we typically have about planning domains.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:18:48 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Bacchus", "F.", ""]]}, {"id": "1110.2730", "submitter": "M. S. Boddy", "authors": "M. S. Boddy", "title": "Imperfect Match: PDDL 2.1 and Real Applications", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  133-137, 2003", "doi": "10.1613/jair.1994", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PDDL was originally conceived and constructed as a lingua franca for the\nInternational Planning Competition. PDDL2.1 embodies a set of extensions\nintended to support the expression of something closer to real planning\nproblems. This objective has only been partially achieved, due in large part to\na deliberate focus on not moving too far from classical planning models and\nsolution methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:19:10 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Boddy", "M. S.", ""]]}, {"id": "1110.2731", "submitter": "H. A. Geffner", "authors": "H. A. Geffner", "title": "PDDL 2.1: Representation vs. Computation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  139-144, 2003", "doi": "10.1613/jair.1995", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I comment on the PDDL 2.1 language and its use in the planning competition,\nfocusing on the choices made for accommodating time and concurrency. I also\ndiscuss some methodological issues that have to do with the move toward more\nexpressive planning languages and the balance needed in planning research\nbetween semantics and computation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:19:24 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Geffner", "H. A.", ""]]}, {"id": "1110.2732", "submitter": "J. C. Beck", "authors": "J. C. Beck, N. Wilson", "title": "Proactive Algorithms for Job Shop Scheduling with Probabilistic\n  Durations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 28, pages\n  183-232, 2007", "doi": "10.1613/jair.2080", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most classical scheduling formulations assume a fixed and known duration for\neach activity. In this paper, we weaken this assumption, requiring instead that\neach duration can be represented by an independent random variable with a known\nmean and variance. The best solutions are ones which have a high probability of\nachieving a good makespan. We first create a theoretical framework, formally\nshowing how Monte Carlo simulation can be combined with deterministic\nscheduling algorithms to solve this problem. We propose an associated\ndeterministic scheduling problem whose solution is proved, under certain\nconditions, to be a lower bound for the probabilistic problem. We then propose\nand investigate a number of techniques for solving such problems based on\ncombinations of Monte Carlo simulation, solutions to the associated\ndeterministic problem, and either constraint programming or tabu search. Our\nempirical results demonstrate that a combination of the use of the associated\ndeterministic problem and Monte Carlo simulation results in algorithms that\nscale best both in terms of problem size and uncertainty. Further experiments\npoint to the correlation between the quality of the deterministic solution and\nthe quality of the probabilistic solution as a major factor responsible for\nthis success.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:19:43 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Beck", "J. C.", ""], ["Wilson", "N.", ""]]}, {"id": "1110.2733", "submitter": "L. Blumrosen", "authors": "L. Blumrosen, N. Nisan, I. Segal", "title": "Auctions with Severely Bounded Communication", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 28, pages\n  233-266, 2007", "doi": "10.1613/jair.2081", "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study auctions with severe bounds on the communication allowed: each\nbidder may only transmit t bits of information to the auctioneer. We consider\nboth welfare- and profit-maximizing auctions under this communication\nrestriction. For both measures, we determine the optimal auction and show that\nthe loss incurred relative to unconstrained auctions is mild. We prove\nnon-surprising properties of these kinds of auctions, e.g., that in optimal\nmechanisms bidders simply report the interval in which their valuation lies in,\nas well as some surprising properties, e.g., that asymmetric auctions are\nbetter than symmetric ones and that multi-round auctions reduce the\ncommunication complexity only by a linear factor.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:20:01 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Blumrosen", "L.", ""], ["Nisan", "N.", ""], ["Segal", "I.", ""]]}, {"id": "1110.2734", "submitter": "A. Darwiche", "authors": "A. Darwiche, J. Huang", "title": "The Language of Search", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 29, pages\n  191-219, 2007", "doi": "10.1613/jair.2097", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with a class of algorithms that perform exhaustive\nsearch on propositional knowledge bases. We show that each of these algorithms\ndefines and generates a propositional language. Specifically, we show that the\ntrace of a search can be interpreted as a combinational circuit, and a search\nalgorithm then defines a propositional language consisting of circuits that are\ngenerated across all possible executions of the algorithm. In particular, we\nshow that several versions of exhaustive DPLL search correspond to such\nwell-known languages as FBDD, OBDD, and a precisely-defined subset of d-DNNF.\nBy thus mapping search algorithms to propositional languages, we provide a\nuniform and practical framework in which successful search techniques can be\nharnessed for compilation of knowledge into various languages of interest, and\na new methodology whereby the power and limitations of search algorithms can be\nunderstood by looking up the tractability and succinctness of the corresponding\npropositional languages.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:20:23 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Darwiche", "A.", ""], ["Huang", "J.", ""]]}, {"id": "1110.2735", "submitter": "L. Barbulescu", "authors": "L. Barbulescu, A. E. Howe, M. Roberts, L. D. Whitley", "title": "Understanding Algorithm Performance on an Oversubscribed Scheduling\n  Application", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  577-615, 2006", "doi": "10.1613/jair.2038", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best performing algorithms for a particular oversubscribed scheduling\napplication, Air Force Satellite Control Network (AFSCN) scheduling, appear to\nhave little in common. Yet, through careful experimentation and modeling of\nperformance in real problem instances, we can relate characteristics of the\nbest algorithms to characteristics of the application. In particular, we find\nthat plateaus dominate the search spaces (thus favoring algorithms that make\nlarger changes to solutions) and that some randomization in exploration is\ncritical to good performance (due to the lack of gradient information on the\nplateaus). Based on our explanations of algorithm performance, we develop a new\nalgorithm that combines characteristics of the best performers; the new\nalgorithms performance is better than the previous best. We show how hypothesis\ndriven experimentation and search modeling can both explain algorithm\nperformance and motivate the design of a new algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:21:07 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Barbulescu", "L.", ""], ["Howe", "A. E.", ""], ["Roberts", "M.", ""], ["Whitley", "L. D.", ""]]}, {"id": "1110.2736", "submitter": "A. I. Coles", "authors": "A. I. Coles, A. J. Smith", "title": "Marvin: A Heuristic Search Planner with Online Macro-Action Learning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 28, pages\n  119-156, 2007", "doi": "10.1613/jair.2077", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Marvin, a planner that competed in the Fourth\nInternational Planning Competition (IPC 4). Marvin uses\naction-sequence-memoisation techniques to generate macro-actions, which are\nthen used during search for a solution plan. We provide an overview of its\narchitecture and search behaviour, detailing the algorithms used. We also\nempirically demonstrate the effectiveness of its features in various planning\ndomains; in particular, the effects on performance due to the use of\nmacro-actions, the novel features of its search behaviour, and the native\nsupport of ADL and Derived Predicates.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:21:41 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Coles", "A. I.", ""], ["Smith", "A. J.", ""]]}, {"id": "1110.2737", "submitter": "E. A. Hansen", "authors": "E. A. Hansen, R. Zhou", "title": "Anytime Heuristic Search", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 28, pages\n  267-297, 2007", "doi": "10.1613/jair.2096", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe how to convert the heuristic search algorithm A* into an anytime\nalgorithm that finds a sequence of improved solutions and eventually converges\nto an optimal solution. The approach we adopt uses weighted heuristic search to\nfind an approximate solution quickly, and then continues the weighted search to\nfind improved solutions as well as to improve a bound on the suboptimality of\nthe current solution. When the time available to solve a search problem is\nlimited or uncertain, this creates an anytime heuristic search algorithm that\nallows a flexible tradeoff between search time and solution quality. We analyze\nthe properties of the resulting Anytime A* algorithm, and consider its\nperformance in three domains; sliding-tile puzzles, STRIPS planning, and\nmultiple sequence alignment. To illustrate the generality of this approach, we\nalso describe how to transform the memory-efficient search algorithm Recursive\nBest-First Search (RBFS) into an anytime algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:21:59 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Hansen", "E. A.", ""], ["Zhou", "R.", ""]]}, {"id": "1110.2738", "submitter": "Y. Chen", "authors": "Y. Chen, F. Lin", "title": "Discovering Classes of Strongly Equivalent Logic Programs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 28, pages\n  431-451, 2007", "doi": "10.1613/jair.2131", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we apply computer-aided theorem discovery technique to discover\ntheorems about strongly equivalent logic programs under the answer set\nsemantics. Our discovered theorems capture new classes of strongly equivalent\nlogic programs that can lead to new program simplification rules that preserve\nstrong equivalence. Specifically, with the help of computers, we discovered\nexact conditions that capture the strong equivalence between a rule and the\nempty set, between two rules, between two rules and one of the two rules,\nbetween two rules and another rule, and between three rules and two of the\nthree rules.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:22:15 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Chen", "Y.", ""], ["Lin", "F.", ""]]}, {"id": "1110.2739", "submitter": "N. Creignou", "authors": "N. Creignou, H. Daude, U. Egly", "title": "Phase Transition for Random Quantified XOR-Formulas", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 29, pages\n  1-18, 2007", "doi": "10.1613/jair.2120", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The QXORSAT problem is the quantified version of the satisfiability problem\nXORSAT in which the connective exclusive-or is used instead of the usual or. We\nstudy the phase transition associated with random QXORSAT instances. We give a\ndescription of this phase transition in the case of one alternation of\nquantifiers, thus performing an advanced practical and theoretical study on the\nphase transition of a quantified roblem.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:22:52 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Creignou", "N.", ""], ["Daude", "H.", ""], ["Egly", "U.", ""]]}, {"id": "1110.2740", "submitter": "B. Bidyuk", "authors": "B. Bidyuk, R. Dechter", "title": "Cutset Sampling for Bayesian Networks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 28, pages\n  1-48, 2007", "doi": "10.1613/jair.2149", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a new sampling methodology for Bayesian networks that\nsamples only a subset of variables and applies exact inference to the rest.\nCutset sampling is a network structure-exploiting application of the\nRao-Blackwellisation principle to sampling in Bayesian networks. It improves\nconvergence by exploiting memory-based inference algorithms. It can also be\nviewed as an anytime approximation of the exact cutset-conditioning algorithm\ndeveloped by Pearl. Cutset sampling can be implemented efficiently when the\nsampled variables constitute a loop-cutset of the Bayesian network and, more\ngenerally, when the induced width of the networks graph conditioned on the\nobserved sampled variables is bounded by a constant w. We demonstrate\nempirically the benefit of this scheme on a range of benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:23:15 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Bidyuk", "B.", ""], ["Dechter", "R.", ""]]}, {"id": "1110.2741", "submitter": "C. Pralet", "authors": "C. Pralet, T. Schiex, G. Verfaillie", "title": "An Algebraic Graphical Model for Decision with Uncertainties,\n  Feasibilities, and Utilities", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 29, pages\n  421-489, 2007", "doi": "10.1613/jair.2151", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous formalisms and dedicated algorithms have been designed in the last\ndecades to model and solve decision making problems. Some formalisms, such as\nconstraint networks, can express \"simple\" decision problems, while others are\ndesigned to take into account uncertainties, unfeasible decisions, and\nutilities. Even in a single formalism, several variants are often proposed to\nmodel different types of uncertainty (probability, possibility...) or utility\n(additive or not). In this article, we introduce an algebraic graphical model\nthat encompasses a large number of such formalisms: (1) we first adapt previous\nstructures from Friedman, Chu and Halpern for representing uncertainty,\nutility, and expected utility in order to deal with generic forms of sequential\ndecision making; (2) on these structures, we then introduce composite graphical\nmodels that express information via variables linked by \"local\" functions,\nthanks to conditional independence; (3) on these graphical models, we finally\ndefine a simple class of queries which can represent various scenarios in terms\nof observabilities and controllabilities. A natural decision-tree semantics for\nsuch queries is completed by an equivalent operational semantics, which induces\ngeneric algorithms. The proposed framework, called the\nPlausibility-Feasibility-Utility (PFU) framework, not only provides a better\nunderstanding of the links between existing formalisms, but it also covers yet\nunpublished frameworks (such as possibilistic influence diagrams) and unifies\nformalisms such as quantified boolean formulas and influence diagrams. Our\nbacktrack and variable elimination generic algorithms are a first step towards\nunified algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:23:33 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Pralet", "C.", ""], ["Schiex", "T.", ""], ["Verfaillie", "G.", ""]]}, {"id": "1110.2742", "submitter": "T. Di Noia", "authors": "T. Di Noia, E. Di Sciascio, F. M. Donini", "title": "Semantic Matchmaking as Non-Monotonic Reasoning: A Description Logic\n  Approach", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 29, pages\n  269-307, 2007", "doi": "10.1613/jair.2153", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matchmaking arises when supply and demand meet in an electronic marketplace,\nor when agents search for a web service to perform some task, or even when\nrecruiting agencies match curricula and job profiles. In such open\nenvironments, the objective of a matchmaking process is to discover best\navailable offers to a given request. We address the problem of matchmaking from\na knowledge representation perspective, with a formalization based on\nDescription Logics. We devise Concept Abduction and Concept Contraction as\nnon-monotonic inferences in Description Logics suitable for modeling\nmatchmaking in a logical framework, and prove some related complexity results.\nWe also present reasonable algorithms for semantic matchmaking based on the\ndevised inferences, and prove that they obey to some commonsense properties.\nFinally, we report on the implementation of the proposed matchmaking framework,\nwhich has been used both as a mediator in e-marketplaces and for semantic web\nservices discovery.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:23:49 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Di Noia", "T.", ""], ["Di Sciascio", "E.", ""], ["Donini", "F. M.", ""]]}, {"id": "1110.2743", "submitter": "J. C. Beck", "authors": "J. C. Beck", "title": "Solution-Guided Multi-Point Constructive Search for Job Shop Scheduling", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 29, pages\n  49-77, 2007", "doi": "10.1613/jair.2169", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solution-Guided Multi-Point Constructive Search (SGMPCS) is a novel\nconstructive search technique that performs a series of resource-limited tree\nsearches where each search begins either from an empty solution (as in\nrandomized restart) or from a solution that has been encountered during the\nsearch. A small number of these \"elite solutions is maintained during the\nsearch. We introduce the technique and perform three sets of experiments on the\njob shop scheduling problem. First, a systematic, fully crossed study of SGMPCS\nis carried out to evaluate the performance impact of various parameter\nsettings. Second, we inquire into the diversity of the elite solution set,\nshowing, contrary to expectations, that a less diverse set leads to stronger\nperformance. Finally, we compare the best parameter setting of SGMPCS from the\nfirst two experiments to chronological backtracking, limited discrepancy\nsearch, randomized restart, and a sophisticated tabu search algorithm on a set\nof well-known benchmark problems. Results demonstrate that SGMPCS is\nsignificantly better than the other constructive techniques tested, though lags\nbehind the tabu search.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:24:37 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Beck", "J. C.", ""]]}, {"id": "1110.2765", "submitter": "S. S. Fatima", "authors": "S. S. Fatima, N. R. Jennings, M. J. Wooldridge", "title": "Multi-Issue Negotiation with Deadlines", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  381-417, 2006", "doi": "10.1613/jair.2056", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies bilateral multi-issue negotiation between self-interested\nautonomous agents. Now, there are a number of different procedures that can be\nused for this process; the three main ones being the package deal procedure in\nwhich all the issues are bundled and discussed together, the simultaneous\nprocedure in which the issues are discussed simultaneously but independently of\neach other, and the sequential procedure in which the issues are discussed one\nafter another. Since each of them yields a different outcome, a key problem is\nto decide which one to use in which circumstances. Specifically, we consider\nthis question for a model in which the agents have time constraints (in the\nform of both deadlines and discount factors) and information uncertainty (in\nthat the agents do not know the opponents utility function). For this model, we\nconsider issues that are both independent and those that are interdependent and\ndetermine equilibria for each case for each procedure. In so doing, we show\nthat the package deal is in fact the optimal procedure for each party. We then\ngo on to show that, although the package deal may be computationally more\ncomplex than the other two procedures, it generates Pareto optimal outcomes\n(unlike the other two), it has similar earliest and latest possible times of\nagreement to the simultaneous procedure (which is better than the sequential\nprocedure), and that it (like the other two procedures) generates a unique\noutcome only under certain conditions (which we define).\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 19:22:19 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Fatima", "S. S.", ""], ["Jennings", "N. R.", ""], ["Wooldridge", "M. J.", ""]]}, {"id": "1110.2767", "submitter": "D. A. Dolgov", "authors": "D. A. Dolgov, E. H. Durfee", "title": "Resource Allocation Among Agents with MDP-Induced Preferences", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  505-549, 2006", "doi": "10.1613/jair.2102", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allocating scarce resources among agents to maximize global utility is, in\ngeneral, computationally challenging. We focus on problems where resources\nenable agents to execute actions in stochastic environments, modeled as Markov\ndecision processes (MDPs), such that the value of a resource bundle is defined\nas the expected value of the optimal MDP policy realizable given these\nresources. We present an algorithm that simultaneously solves the\nresource-allocation and the policy-optimization problems. This allows us to\navoid explicitly representing utilities over exponentially many resource\nbundles, leading to drastic (often exponential) reductions in computational\ncomplexity. We then use this algorithm in the context of self-interested agents\nto design a combinatorial auction for allocating resources. We empirically\ndemonstrate the effectiveness of our approach by showing that it can, in\nminutes, optimally solve problems for which a straightforward combinatorial\nresource-allocation technique would require the agents to enumerate up to 2^100\nresource bundles and the auctioneer to solve an NP-complete problem with an\ninput of that size.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 19:23:10 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Dolgov", "D. A.", ""], ["Durfee", "E. H.", ""]]}, {"id": "1110.3002", "submitter": "Carlos Gershenson", "authors": "Carlos Gershenson", "title": "Are Minds Computable?", "comments": "7 pages, comments welcome", "journal-ref": null, "doi": null, "report-no": "C3 Report 2011.08", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This essay explores the limits of Turing machines concerning the modeling of\nminds and suggests alternatives to go beyond those limits.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 17:26:03 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Gershenson", "Carlos", ""]]}, {"id": "1110.3239", "submitter": "Giorgio Corani", "authors": "Giorgio Corani and Cassio P. De Campos", "title": "Improving parameter learning of Bayesian nets from incomplete data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the estimation of parameters of a Bayesian network from\nincomplete data. The task is usually tackled by running the\nExpectation-Maximization (EM) algorithm several times in order to obtain a high\nlog-likelihood estimate. We argue that choosing the maximum log-likelihood\nestimate (as well as the maximum penalized log-likelihood and the maximum a\nposteriori estimate) has severe drawbacks, being affected both by overfitting\nand model uncertainty. Two ideas are discussed to overcome these issues: a\nmaximum entropy approach and a Bayesian model averaging approach. Both ideas\ncan be easily applied on top of EM, while the entropy idea can be also\nimplemented in a more sophisticated way, through a dedicated non-linear solver.\nA vast set of experiments shows that these ideas produce significantly better\nestimates and inferences than the traditional and widely used maximum\n(penalized) log-likelihood and maximum a posteriori estimates. In particular,\nif EM is adopted as optimization engine, the model averaging approach is the\nbest performing one; its performance is matched by the entropy approach when\nimplemented using the non-linear solver. The results suggest that the\napplicability of these ideas is immediate (they are easy to implement and to\nintegrate in currently available inference engines) and that they constitute a\nbetter way to learn Bayesian network parameters.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 12:17:51 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Corani", "Giorgio", ""], ["De Campos", "Cassio P.", ""]]}, {"id": "1110.3385", "submitter": "Tshilidzi Marwala", "authors": "Pretesh Patel and Tshilidzi Marwala", "title": "Fuzzy Inference Systems Optimization", "comments": "Paper Submitted to INTECH", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares various optimization methods for fuzzy inference system\noptimization. The optimization methods compared are genetic algorithm, particle\nswarm optimization and simulated annealing. When these techniques were\nimplemented it was observed that the performance of each technique within the\nfuzzy inference system classification was context dependent.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2011 05:39:34 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["Patel", "Pretesh", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "1110.3672", "submitter": "Daniele Theseider Dupr\\'e", "authors": "Laura Giordano, Alberto Martelli, Daniele Theseider Dupr\\'e", "title": "Reasoning about Actions with Temporal Answer Sets", "comments": "To appear in Theory and Practice of Logic Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we combine Answer Set Programming (ASP) with Dynamic Linear\nTime Temporal Logic (DLTL) to define a temporal logic programming language for\nreasoning about complex actions and infinite computations. DLTL extends\npropositional temporal logic of linear time with regular programs of\npropositional dynamic logic, which are used for indexing temporal modalities.\nThe action language allows general DLTL formulas to be included in domain\ndescriptions to constrain the space of possible extensions. We introduce a\nnotion of Temporal Answer Set for domain descriptions, based on the usual\nnotion of Answer Set. Also, we provide a translation of domain descriptions\ninto standard ASP and we use Bounded Model Checking techniques for the\nverification of DLTL constraints.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 14:12:07 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["Giordano", "Laura", ""], ["Martelli", "Alberto", ""], ["Dupr\u00e9", "Daniele Theseider", ""]]}, {"id": "1110.3888", "submitter": "Yuming Xu", "authors": "Xu Yuming", "title": "Handling controversial arguments by matrix", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce matrix and its block to the Dung's theory of argumentation\nframework. It is showed that each argumentation framework has a matrix\nrepresentation, and the indirect attack relation and indirect defence relation\ncan be characterized by computing the matrix. This provide a powerful\nmathematics way to determine the \"controversial arguments\" in an argumentation\nframework. Also, we introduce several kinds of blocks based on the matrix, and\nvarious prudent semantics of argumentation frameworks can all be determined by\ncomputing and comparing the matrices and their blocks which we have defined. In\ncontrast with traditional method of directed graph, the matrix method has an\nexcellent advantage: computability(even can be realized on computer easily).\nSo, there is an intensive perspective to import the theory of matrix to the\nresearch of argumentation frameworks and its related areas.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 07:01:28 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2011 05:58:23 GMT"}], "update_date": "2011-10-21", "authors_parsed": [["Yuming", "Xu", ""]]}, {"id": "1110.3907", "submitter": "Peng Sun", "authors": "Peng Sun, Mark D. Reid, Jie Zhou", "title": "AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem", "comments": "8-pages camera ready version for ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an improvement to model learning when using multi-class\nLogitBoost for classification. Motivated by the statistical view, LogitBoost\ncan be seen as additive tree regression. Two important factors in this setting\nare: 1) coupled classifier output due to a sum-to-zero constraint, and 2) the\ndense Hessian matrices that arise when computing tree node split gain and node\nvalue fittings. In general, this setting is too complicated for a tractable\nmodel learning algorithm. However, too aggressive simplification of the setting\nmay lead to degraded performance. For example, the original LogitBoost is\noutperformed by ABC-LogitBoost due to the latter's more careful treatment of\nthe above two factors.\n  In this paper we propose techniques to address the two main difficulties of\nthe LogitBoost setting: 1) we adopt a vector tree (i.e. each node value is\nvector) that enforces a sum-to-zero constraint, and 2) we use an adaptive block\ncoordinate descent that exploits the dense Hessian when computing tree split\ngain and node values. Higher classification accuracy and faster convergence\nrates are observed for a range of public data sets when compared to both the\noriginal and the ABC-LogitBoost implementations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 08:26:59 GMT"}, {"version": "v2", "created": "Thu, 17 May 2012 19:43:06 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2012 07:14:17 GMT"}], "update_date": "2012-07-05", "authors_parsed": [["Sun", "Peng", ""], ["Reid", "Mark D.", ""], ["Zhou", "Jie", ""]]}, {"id": "1110.3961", "submitter": "Neeraj Kumar Sharma", "authors": "Vibha Gaur, Neeraj Kumar Sharma", "title": "A Dynamic Framework of Reputation Systems for an Agent Mediated e-market", "comments": "19 Pages; International Journal of Computer Science Issues\n  (IJCSI),Vol 8, Issue 4, July 2011, ISSN(online): 1694-0814", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.IT cs.SI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of an agent mediated e-market system lies in the underlying\nreputation management system to improve the quality of services in an\ninformation asymmetric e-market. Reputation provides an operatable metric for\nestablishing trustworthiness between mutually unknown online entities.\nReputation systems encourage honest behaviour and discourage malicious\nbehaviour of participating agents in the e-market. A dynamic reputation model\nwould provide virtually instantaneous knowledge about the changing e-market\nenvironment and would utilise Internets' capacity for continuous interactivity\nfor reputation computation. This paper proposes a dynamic reputation framework\nusing reinforcement learning and fuzzy set theory that ensures judicious use of\ninformation sharing for inter-agent cooperation. This framework is sensitive to\nthe changing parameters of e-market like the value of transaction and the\nvarying experience of agents with the purpose of improving inbuilt defense\nmechanism of the reputation system against various attacks so that e-market\nreaches an equilibrium state and dishonest agents are weeded out of the market.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 12:50:26 GMT"}], "update_date": "2011-10-19", "authors_parsed": [["Gaur", "Vibha", ""], ["Sharma", "Neeraj Kumar", ""]]}, {"id": "1110.4076", "submitter": "V. Bulitko", "authors": "V. Bulitko, G. Lee", "title": "Learning in Real-Time Search: A Unifying Framework", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  119-157, 2006", "doi": "10.1613/jair.1789", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time search methods are suited for tasks in which the agent is\ninteracting with an initially unknown environment in real time. In such\nsimultaneous planning and learning problems, the agent has to select its\nactions in a limited amount of time, while sensing only a local part of the\nenvironment centered at the agents current location. Real-time heuristic search\nagents select actions using a limited lookahead search and evaluating the\nfrontier states with a heuristic function. Over repeated experiences, they\nrefine heuristic values of states to avoid infinite loops and to converge to\nbetter solutions. The wide spread of such settings in autonomous software and\nhardware agents has led to an explosion of real-time search algorithms over the\nlast two decades. Not only is a potential user confronted with a hodgepodge of\nalgorithms, but he also faces the choice of control parameters they use. In\nthis paper we address both problems. The first contribution is an introduction\nof a simple three-parameter framework (named LRTS) which extracts the core\nideas behind many existing algorithms. We then prove that LRTA*, epsilon-LRTA*,\nSLA*, and gamma-Trap algorithms are special cases of our framework. Thus, they\nare unified and extended with additional features. Second, we prove\ncompleteness and convergence of any algorithm covered by the LRTS framework.\nThird, we prove several upper-bounds relating the control parameters and\nsolution quality. Finally, we analyze the influence of the three control\nparameters empirically in the realistic scalable domains of real-time\nnavigation on initially unknown maps from a commercial role-playing game as\nwell as routing in ad hoc sensor networks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 17:00:02 GMT"}], "update_date": "2011-10-19", "authors_parsed": [["Bulitko", "V.", ""], ["Lee", "G.", ""]]}, {"id": "1110.4099", "submitter": "Nelson Alfonso G\\'omez Cruz M.Sc. (c)", "authors": "Carlos Eduardo Maldonado and Nelson Alfonso G\\'omez-Cruz", "title": "The Complexification of Engineering", "comments": "9 pages, 1 figure, 1 table, preprint; Complexity. In the print (2011)", "journal-ref": "2012, Complexity 17(4), pp. 8-15", "doi": "10.1002/cplx.20395", "report-no": null, "categories": "nlin.AO cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper deals with the arrow of complexification of engineering. We claim\nthat the complexification of engineering consists in (a) that shift throughout\nwhich engineering becomes a science; thus it ceases to be a (mere) praxis or\nprofession; (b) becoming a science, engineering can be considered as one of the\nsciences of complexity. In reality, the complexification of engineering is the\nprocess by which engineering can be studied, achieved and understood in terms\nof knowledge, and not of goods and services any longer. Complex engineered\nsystems and bio-inspired engineering are so far the two expressions of a\ncomplex engineering.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 19:53:09 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2011 14:00:59 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Maldonado", "Carlos Eduardo", ""], ["G\u00f3mez-Cruz", "Nelson Alfonso", ""]]}, {"id": "1110.4657", "submitter": "Boris Mitavskiy", "authors": "Boris Mitavskiy, Jonathan Rowe and Chris Cannings", "title": "A Version of Geiringer-like Theorem for Decision Making in the\n  Environments with Randomness and Incomplete Information", "comments": "53 pages in size. This work has been recently submitted to the IJICC\n  (International Journal on Intelligent Computing and Cybernetics)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: In recent years Monte-Carlo sampling methods, such as Monte Carlo\ntree search, have achieved tremendous success in model free reinforcement\nlearning. A combination of the so called upper confidence bounds policy to\npreserve the \"exploration vs. exploitation\" balance to select actions for\nsample evaluations together with massive computing power to store and to update\ndynamically a rather large pre-evaluated game tree lead to the development of\nsoftware that has beaten the top human player in the game of Go on a 9 by 9\nboard. Much effort in the current research is devoted to widening the range of\napplicability of the Monte-Carlo sampling methodology to partially observable\nMarkov decision processes with non-immediate payoffs. The main challenge\nintroduced by randomness and incomplete information is to deal with the action\nevaluation at the chance nodes due to drastic differences in the possible\npayoffs the same action could lead to. The aim of this article is to establish\na version of a theorem that originated from population genetics and has been\nlater adopted in evolutionary computation theory that will lead to novel\nMonte-Carlo sampling algorithms that provably increase the AI potential. Due to\nspace limitations the actual algorithms themselves will be presented in the\nsequel papers, however, the current paper provides a solid mathematical\nfoundation for the development of such algorithms and explains why they are so\npromising.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 22:53:03 GMT"}], "update_date": "2011-10-24", "authors_parsed": [["Mitavskiy", "Boris", ""], ["Rowe", "Jonathan", ""], ["Cannings", "Chris", ""]]}, {"id": "1110.4719", "submitter": "Thierry  Petit", "authors": "Thierry Petit, Nicolas Beldiceanu and Xavier Lorca", "title": "A Generalized Arc-Consistency Algorithm for a Class of Counting\n  Constraints: Revised Edition that Incorporates One Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the SEQ BIN meta-constraint with a polytime algorithm\nachieving general- ized arc-consistency according to some properties. SEQ BIN\ncan be used for encoding counting con- straints such as CHANGE, SMOOTH or\nINCREAS- ING NVALUE. For some of these constraints and some of their variants\nGAC can be enforced with a time and space complexity linear in the sum of\ndomain sizes, which improves or equals the best known results of the\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2011 07:49:48 GMT"}], "update_date": "2011-10-24", "authors_parsed": [["Petit", "Thierry", ""], ["Beldiceanu", "Nicolas", ""], ["Lorca", "Xavier", ""]]}, {"id": "1110.5102", "submitter": "Congcong Li", "authors": "Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen", "title": "Towards Holistic Scene Understanding: Feedback Enabled Cascaded\n  Classification Models", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding includes many related sub-tasks, such as scene\ncategorization, depth estimation, object detection, etc. Each of these\nsub-tasks is often notoriously hard, and state-of-the-art classifiers already\nexist for many of them. These classifiers operate on the same raw image and\nprovide correlated outputs. It is desirable to have an algorithm that can\ncapture such correlation without requiring any changes to the inner workings of\nany classifier.\n  We propose Feedback Enabled Cascaded Classification Models (FE-CCM), that\njointly optimizes all the sub-tasks, while requiring only a `black-box'\ninterface to the original classifier for each sub-task. We use a two-layer\ncascade of classifiers, which are repeated instantiations of the original ones,\nwith the output of the first layer fed into the second layer as input. Our\ntraining method involves a feedback step that allows later classifiers to\nprovide earlier classifiers information about which error modes to focus on. We\nshow that our method significantly improves performance in all the sub-tasks in\nthe domain of scene understanding, where we consider depth estimation, scene\ncategorization, event categorization, object detection, geometric labeling and\nsaliency detection. Our method also improves performance in two robotic\napplications: an object-grasping robot and an object-finding robot.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2011 00:31:00 GMT"}], "update_date": "2011-10-25", "authors_parsed": [["Li", "Congcong", ""], ["Kowdle", "Adarsh", ""], ["Saxena", "Ashutosh", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1110.5172", "submitter": "Valmi Dufour-Lussier", "authors": "Valmi Dufour-Lussier (INRIA Lorraine - LORIA), Florence Le Ber (INRIA\n  Lorraine - LORIA, LHyGeS), Jean Lieber (INRIA Lorraine - LORIA)", "title": "Quels formalismes temporels pour repr\\'esenter des connaissances\n  extraites de textes de recettes de cuisine ?", "comments": "Repr\\'esentation et raisonnement sur le temps et l'espace (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Taaable projet goal is to create a case-based reasoning system for\nretrieval and adaptation of cooking recipes. Within this framework, we are\ndiscussing the temporal aspects of recipes and the means of representing those\nin order to adapt their text.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2011 09:33:31 GMT"}], "update_date": "2011-10-25", "authors_parsed": [["Dufour-Lussier", "Valmi", "", "INRIA Lorraine - LORIA"], ["Ber", "Florence Le", "", "INRIA\n  Lorraine - LORIA, LHyGeS"], ["Lieber", "Jean", "", "INRIA Lorraine - LORIA"]]}, {"id": "1110.5667", "submitter": "Noah Goodman", "authors": "Irvin Hwang, Andreas Stuhlm\\\"uller, Noah D. Goodman", "title": "Inducing Probabilistic Programs by Bayesian Program Merging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report outlines an approach to learning generative models from data. We\nexpress models as probabilistic programs, which allows us to capture abstract\npatterns within the examples. By choosing our language for programs to be an\nextension of the algebraic data type of the examples, we can begin with a\nprogram that generates all and only the examples. We then introduce greater\nabstraction, and hence generalization, incrementally to the extent that it\nimproves the posterior probability of the examples given the program. Motivated\nby previous approaches to model merging and program induction, we search for\nsuch explanatory abstractions using program transformations. We consider two\ntypes of transformation: Abstraction merges common subexpressions within a\nprogram into new functions (a form of anti-unification). Deargumentation\nsimplifies functions by reducing the number of arguments. We demonstrate that\nthis approach finds key patterns in the domain of nested lists, including\nparameterized sub-functions and stochastic recursion.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2011 21:06:39 GMT"}], "update_date": "2011-10-27", "authors_parsed": [["Hwang", "Irvin", ""], ["Stuhlm\u00fcller", "Andreas", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1110.6200", "submitter": "Jacob Eisenstein", "authors": "Jacob Eisenstein, Duen Horng \"Polo\" Chau, Aniket Kittur, Eric P. Xing", "title": "TopicViz: Semantic Navigation of Document Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When people explore and manage information, they think in terms of topics and\nthemes. However, the software that supports information exploration sees text\nat only the surface level. In this paper we show how topic modeling -- a\ntechnique for identifying latent themes across large collections of documents\n-- can support semantic exploration. We present TopicViz, an interactive\nenvironment for information exploration. TopicViz combines traditional search\nand citation-graph functionality with a range of novel interactive\nvisualizations, centered around a force-directed layout that links documents to\nthe latent themes discovered by the topic model. We describe several use\nscenarios in which TopicViz supports rapid sensemaking on large document\ncollections.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2011 21:37:24 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2011 03:47:30 GMT"}], "update_date": "2011-11-07", "authors_parsed": [["Eisenstein", "Jacob", ""], ["Chau", "Duen Horng \"Polo\"", ""], ["Kittur", "Aniket", ""], ["Xing", "Eric P.", ""]]}, {"id": "1110.6290", "submitter": "Lars Kotthoff", "authors": "Ian P. Gent and Chris Jefferson and Lars Kotthoff and Ian Miguel", "title": "Modelling Constraint Solver Architecture Design as a Constraint Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing component-based constraint solvers is a complex problem. Some\ncomponents are required, some are optional and there are interdependencies\nbetween the components. Because of this, previous approaches to solver design\nand modification have been ad-hoc and limited. We present a system that\ntransforms a description of the components and the characteristics of the\ntarget constraint solver into a constraint problem. Solving this problem yields\nthe description of a valid solver. Our approach represents a significant step\ntowards the automated design and synthesis of constraint solvers that are\nspecialised for individual constraint problem classes or instances.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 10:41:43 GMT"}], "update_date": "2011-10-31", "authors_parsed": [["Gent", "Ian P.", ""], ["Jefferson", "Chris", ""], ["Kotthoff", "Lars", ""], ["Miguel", "Ian", ""]]}, {"id": "1110.6384", "submitter": "Serge Gaspers", "authors": "Serge Gaspers and Stefan Szeider", "title": "Backdoors to Acyclic SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoor sets, a notion introduced by Williams et al. in 2003, are certain\nsets of key variables of a CNF formula F that make it easy to solve the\nformula; by assigning truth values to the variables in a backdoor set, the\nformula gets reduced to one or several polynomial-time solvable formulas. More\nspecifically, a weak backdoor set of F is a set X of variables such that there\nexits a truth assignment t to X that reduces F to a satisfiable formula F[t]\nthat belongs to a polynomial-time decidable base class C. A strong backdoor set\nis a set X of variables such that for all assignments t to X, the reduced\nformula F[t] belongs to C.\n  We study the problem of finding backdoor sets of size at most k with respect\nto the base class of CNF formulas with acyclic incidence graphs, taking k as\nthe parameter. We show that\n  1. the detection of weak backdoor sets is W[2]-hard in general but\nfixed-parameter tractable for r-CNF formulas, for any fixed r>=3, and\n  2. the detection of strong backdoor sets is fixed-parameter approximable.\n  Result 1 is the the first positive one for a base class that does not have a\ncharacterization with obstructions of bounded size. Result 2 is the first\npositive one for a base class for which strong backdoor sets are more powerful\nthan deletion backdoor sets.\n  Not only SAT, but also #SAT can be solved in polynomial time for CNF formulas\nwith acyclic incidence graphs. Hence Result 2 establishes a new structural\nparameter that makes #SAT fixed-parameter tractable and that is incomparable\nwith known parameters such as treewidth and clique-width.\n  We obtain the algorithms by a combination of an algorithmic version of the\nErd\\\"os-P\\'osa Theorem, Courcelle's model checking for monadic second order\nlogic, and new combinatorial results on how disjoint cycles can interact with\nthe backdoor set.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 16:10:32 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2011 15:09:42 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2012 17:15:41 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["Gaspers", "Serge", ""], ["Szeider", "Stefan", ""]]}, {"id": "1110.6387", "submitter": "Serge Gaspers", "authors": "Serge Gaspers and Stefan Szeider", "title": "Backdoors to Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A backdoor set is a set of variables of a propositional formula such that\nfixing the truth values of the variables in the backdoor set moves the formula\ninto some polynomial-time decidable class. If we know a small backdoor set we\ncan reduce the question of whether the given formula is satisfiable to the same\nquestion for one or several easy formulas that belong to the tractable class\nunder consideration. In this survey we review parameterized complexity results\nfor problems that arise in the context of backdoor sets, such as the problem of\nfinding a backdoor set of size at most k, parameterized by k. We also discuss\nrecent results on backdoor sets for problems that are beyond NP.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 16:23:45 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2011 14:54:50 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Gaspers", "Serge", ""], ["Szeider", "Stefan", ""]]}, {"id": "1110.6437", "submitter": "Stuart Armstrong", "authors": "Stuart Armstrong", "title": "Anthropic decision theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.AI hep-th physics.pop-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper sets out to resolve how agents ought to act in the Sleeping Beauty\nproblem and various related anthropic (self-locating belief) problems, not\nthrough the calculation of anthropic probabilities, but through finding the\ncorrect decision to make. It creates an anthropic decision theory (ADT) that\ndecides these problems from a small set of principles. By doing so, it\ndemonstrates that the attitude of agents with regards to each other (selfish or\naltruistic) changes the decisions they reach, and that it is very important to\ntake this into account. To illustrate ADT, it is then applied to two major\nanthropic problems and paradoxes, the Presumptuous Philosopher and Doomsday\nproblems, thus resolving some issues about the probability of human extinction.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 13:34:36 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2011 13:34:58 GMT"}, {"version": "v3", "created": "Wed, 20 Sep 2017 15:52:49 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Armstrong", "Stuart", ""]]}, {"id": "1110.6483", "submitter": "Nicolaie Popescu-Bodorin", "authors": "N. Popescu-Bodorin, V. E. Balas, I. M. Motoc", "title": "Iris Codes Classification Using Discriminant and Witness Directions", "comments": "6 pages, 5 figures, Proc. 5th IEEE Int. Symp. on Computational\n  Intelligence and Intelligent Informatics (Floriana, Malta, September 15-17),\n  ISBN: 978-1-4577-1861-8 (electronic), 978-1-4577-1860-1 (print)", "journal-ref": "Proc. 5th IEEE Int. Symp. on Computational Intelligence and\n  Intelligent Informatics, pp. 143-148, 2011", "doi": "10.1109/ISCIII.2011.6069760", "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main topic discussed in this paper is how to use intelligence for\nbiometric decision defuzzification. A neural training model is proposed and\ntested here as a possible solution for dealing with natural fuzzification that\nappears between the intra- and inter-class distribution of scores computed\nduring iris recognition tests. It is shown here that the use of proposed neural\nnetwork support leads to an improvement in the artificial perception of the\nseparation between the intra- and inter-class score distributions by moving\nthem away from each other.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 23:25:59 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2011 21:32:22 GMT"}], "update_date": "2011-11-10", "authors_parsed": [["Popescu-Bodorin", "N.", ""], ["Balas", "V. E.", ""], ["Motoc", "I. M.", ""]]}, {"id": "1110.6589", "submitter": "Amit Mishra", "authors": "Amit K. Mishra and Chris Baker", "title": "A cognitive diversity framework for radar target classification", "comments": null, "journal-ref": "The IET COGnitive systems with Interactive Sensors 2010", "doi": null, "report-no": "The IET COGnitive systems with Interactive Sensors 2010", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of targets by radar has proved to be notoriously difficult\nwith the best systems still yet to attain sufficiently high levels of\nperformance and reliability. In the current contribution we explore a new\ndesign of radar based target recognition, where angular diversity is used in a\ncognitive manner to attain better performance. Performance is bench- marked\nagainst conventional classification schemes. The proposed scheme can easily be\nextended to cognitive target recognition based on multiple diversity\nstrategies.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2011 09:26:34 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Mishra", "Amit K.", ""], ["Baker", "Chris", ""]]}]