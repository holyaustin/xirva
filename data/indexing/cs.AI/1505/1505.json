[{"id": "1505.00002", "submitter": "Anthony Di Franco", "authors": "Anthony Di Franco", "title": "FIFTH system for general-purpose connectionist computation", "comments": "Submitted, COSYNE 2015 (extended abstract)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, work on formalizing connectionist computation in a way that is at\nleast Turing-complete has focused on recurrent architectures and developed\nequivalences to Turing machines or similar super-Turing models, which are of\nmore theoretical than practical significance. We instead develop connectionist\ncomputation within the framework of information propagation networks extended\nwith unbounded recursion, which is related to constraint logic programming and\nis more declarative than the semantics typically used in practical programming,\nbut is still formally known to be Turing-complete. This approach yields\ncontributions to the theory and practice of both connectionist computation and\nprogramming languages. Connectionist computations are carried out in a way that\nlets them communicate with, and be understood and interrogated directly in\nterms of the high-level semantics of a general-purpose programming language.\nMeanwhile, difficult (unbounded-dimension, NP-hard) search problems in\nprogramming that have previously been left to the programmer to solve in a\nheuristic, domain-specific way are solved uniformly a priori in a way that\napproximately achieves information-theoretic limits on performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 22:20:04 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Di Franco", "Anthony", ""]]}, {"id": "1505.00138", "submitter": "Dimitri Kartsaklis", "authors": "Dimitri Kartsaklis", "title": "Compositional Distributional Semantics with Compact Closed Categories\n  and Frobenius Algebras", "comments": "Ph.D. Dissertation, University of Oxford", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI math.CT math.QA quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis contributes to ongoing research related to the categorical\ncompositional model for natural language of Coecke, Sadrzadeh and Clark in\nthree ways: Firstly, I propose a concrete instantiation of the abstract\nframework based on Frobenius algebras (joint work with Sadrzadeh). The theory\nimproves shortcomings of previous proposals, extends the coverage of the\nlanguage, and is supported by experimental work that improves existing results.\nThe proposed framework describes a new class of compositional models that find\nintuitive interpretations for a number of linguistic phenomena. Secondly, I\npropose and evaluate in practice a new compositional methodology which\nexplicitly deals with the different levels of lexical ambiguity (joint work\nwith Pulman). A concrete algorithm is presented, based on the separation of\nvector disambiguation from composition in an explicit prior step. Extensive\nexperimental work shows that the proposed methodology indeed results in more\naccurate composite representations for the framework of Coecke et al. in\nparticular and every other class of compositional models in general. As a last\ncontribution, I formalize the explicit treatment of lexical ambiguity in the\ncontext of the categorical framework by resorting to categorical quantum\nmechanics (joint work with Coecke). In the proposed extension, the concept of a\ndistributional vector is replaced with that of a density matrix, which\ncompactly represents a probability distribution over the potential different\nmeanings of the specific word. Composition takes the form of quantum\nmeasurements, leading to interesting analogies between quantum physics and\nlinguistics.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 10:00:33 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Kartsaklis", "Dimitri", ""]]}, {"id": "1505.00162", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern", "title": "A Modification of the Halpern-Pearl Definition of Causality", "comments": "This is an extended version of a paper that will appear in IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original Halpern-Pearl definition of causality [Halpern and Pearl, 2001]\nwas updated in the journal version of the paper [Halpern and Pearl, 2005] to\ndeal with some problems pointed out by Hopkins and Pearl [2003]. Here the\ndefinition is modified yet again, in a way that (a) leads to a simpler\ndefinition, (b) handles the problems pointed out by Hopkins and Pearl, and many\nothers, (c) gives reasonable answers (that agree with those of the original and\nupdated definition) in the standard problematic examples of causality, and (d)\nhas lower complexity than either the original or updated definitions.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 11:44:51 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Halpern", "Joseph Y.", ""]]}, {"id": "1505.00274", "submitter": "Miao Liu", "authors": "Miao Liu, Christopher Amato, Xuejun Liao, Lawrence Carin, Jonathan P.\n  How", "title": "Stick-Breaking Policy Learning in Dec-POMDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation maximization (EM) has recently been shown to be an efficient\nalgorithm for learning finite-state controllers (FSCs) in large decentralized\nPOMDPs (Dec-POMDPs). However, current methods use fixed-size FSCs and often\nconverge to maxima that are far from optimal. This paper considers a\nvariable-size FSC to represent the local policy of each agent. These\nvariable-size FSCs are constructed using a stick-breaking prior, leading to a\nnew framework called \\emph{decentralized stick-breaking policy representation}\n(Dec-SBPR). This approach learns the controller parameters with a variational\nBayesian algorithm without having to assume that the Dec-POMDP model is\navailable. The performance of Dec-SBPR is demonstrated on several benchmark\nproblems, showing that the algorithm scales to large problems while\noutperforming other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 20:29:27 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 20:48:32 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Liu", "Miao", ""], ["Amato", "Christopher", ""], ["Liao", "Xuejun", ""], ["Carin", "Lawrence", ""], ["How", "Jonathan P.", ""]]}, {"id": "1505.00277", "submitter": "Dana Movshovitz-Attias", "authors": "Dana Movshovitz-Attias, William W. Cohen", "title": "Grounded Discovery of Coordinate Term Relationships between Software\n  Entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for the detection of coordinate-term relationships\nbetween entities from the software domain, that refer to Java classes. Usually,\nrelations are found by examining corpus statistics associated with text\nentities. In some technical domains, however, we have access to additional\ninformation about the real-world objects named by the entities, suggesting that\ncoupling information about the \"grounded\" entities with corpus statistics might\nlead to improved methods for relation discovery. To this end, we develop a\nsimilarity measure for Java classes using distributional information about how\nthey are used in software, which we combine with corpus statistics on the\ndistribution of contexts in which the classes appear in text. Using our\napproach, cross-validation accuracy on this dataset can be improved\ndramatically, from around 60% to 88%. Human labeling results show that our\nclassifier has an F1 score of 86% over the top 1000 predicted pairs.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 20:40:00 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Movshovitz-Attias", "Dana", ""], ["Cohen", "William W.", ""]]}, {"id": "1505.00278", "submitter": "Michal \\v{C}ertick\\'y", "authors": "Bj\\\"orn Persson Mattsson, Tom\\'a\\v{s} Vajda, Michal \\v{C}ertick\\'y", "title": "Automatic Observer Script for StarCraft: Brood War Bot Games (technical\n  report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short report describes an automated BWAPI-based script developed for\nlive streams of a StarCraft Brood War bot tournament, SSCAIT. The script\ncontrols the in-game camera in order to follow the relevant events and improve\nthe viewer experience. We enumerate its novel features and provide a few\nimplementation notes.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 20:41:19 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Mattsson", "Bj\u00f6rn Persson", ""], ["Vajda", "Tom\u00e1\u0161", ""], ["\u010certick\u00fd", "Michal", ""]]}, {"id": "1505.00284", "submitter": "Benjamin Rosman", "authors": "Benjamin Rosman, Majd Hawasly, Subramanian Ramamoorthy", "title": "Bayesian Policy Reuse", "comments": "32 pages, submitted to the Machine Learning Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-lived autonomous agent should be able to respond online to novel\ninstances of tasks from a familiar domain. Acting online requires 'fast'\nresponses, in terms of rapid convergence, especially when the task instance has\na short duration, such as in applications involving interactions with humans.\nThese requirements can be problematic for many established methods for learning\nto act. In domains where the agent knows that the task instance is drawn from a\nfamily of related tasks, albeit without access to the label of any given\ninstance, it can choose to act through a process of policy reuse from a\nlibrary, rather than policy learning from scratch. In policy reuse, the agent\nhas prior knowledge of the class of tasks in the form of a library of policies\nthat were learnt from sample task instances during an offline training phase.\nWe formalise the problem of policy reuse, and present an algorithm for\nefficiently responding to a novel task instance by reusing a policy from the\nlibrary of existing policies, where the choice is based on observed 'signals'\nwhich correlate to policy performance. We achieve this by posing the problem as\na Bayesian choice problem with a corresponding notion of an optimal response,\nbut the computation of that response is in many cases intractable. Therefore,\nto reduce the computation cost of the posterior, we follow a Bayesian\noptimisation approach and define a set of policy selection functions, which\nbalance exploration in the policy library against exploitation of previously\ntried policies, together with a model of expected performance of the policy\nlibrary on their corresponding task instances. We validate our method in\nseveral simulated domains of interactive, short-duration episodic tasks,\nshowing rapid convergence in unknown task variations.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 21:13:00 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 15:44:51 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Rosman", "Benjamin", ""], ["Hawasly", "Majd", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1505.00399", "submitter": "Christopher Lin", "authors": "Christopher H. Lin and Andrey Kolobov and Ece Kamar and Eric Horvitz", "title": "Metareasoning for Planning Under Uncertainty", "comments": "Extended version of IJCAI 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional model for online planning under uncertainty assumes that an\nagent can stop and plan without incurring costs for the time spent planning.\nHowever, planning time is not free in most real-world settings. For example, an\nautonomous drone is subject to nature's forces, like gravity, even while it\nthinks, and must either pay a price for counteracting these forces to stay in\nplace, or grapple with the state change caused by acquiescing to them. Policy\noptimization in these settings requires metareasoning---a process that trades\noff the cost of planning and the potential policy improvement that can be\nachieved. We formalize and analyze the metareasoning problem for Markov\nDecision Processes (MDPs). Our work subsumes previously studied special cases\nof metareasoning and shows that in the general case, metareasoning is at most\npolynomially harder than solving MDPs with any given algorithm that disregards\nthe cost of thinking. For reasons we discuss, optimal general metareasoning\nturns out to be impractical, motivating approximations. We present approximate\nmetareasoning procedures which rely on special properties of the BRTDP planning\nalgorithm and explore the effectiveness of our methods on a variety of\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 07:09:08 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Lin", "Christopher H.", ""], ["Kolobov", "Andrey", ""], ["Kamar", "Ece", ""], ["Horvitz", "Eric", ""]]}, {"id": "1505.00401", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "Visualization of Tradeoff in Evaluation: from Precision-Recall & PN to\n  LIFT, ROC & BIRD", "comments": "23 pages, 12 equations, 2 figures, 2 tables, 1 sidebar", "journal-ref": null, "doi": null, "report-no": "KIT-14-002", "categories": "cs.LG cs.AI cs.IR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation often aims to reduce the correctness or error characteristics of a\nsystem down to a single number, but that always involves trade-offs. Another\nway of dealing with this is to quote two numbers, such as Recall and Precision,\nor Sensitivity and Specificity. But it can also be useful to see more than\nthis, and a graphical approach can explore sensitivity to cost, prevalence,\nbias, noise, parameters and hyper-parameters.\n  Moreover, most techniques are implicitly based on two balanced classes, and\nour ability to visualize graphically is intrinsically two dimensional, but we\noften want to visualize in a multiclass context. We review the dichotomous\napproaches relating to Precision, Recall, and ROC as well as the related LIFT\nchart, exploring how they handle unbalanced and multiclass data, and deriving\nnew probabilistic and information theoretic variants of LIFT that help deal\nwith the issues associated with the handling of multiple and unbalanced\nclasses.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 07:27:27 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 04:02:22 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1505.00423", "submitter": "Josif Grabocka", "authors": "Josif Grabocka and Nicolas Schilling and Lars Schmidt-Thieme", "title": "Optimal Time-Series Motifs", "comments": "Submitted to KDD2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motifs are the most repetitive/frequent patterns of a time-series. The\ndiscovery of motifs is crucial for practitioners in order to understand and\ninterpret the phenomena occurring in sequential data. Currently, motifs are\nsearched among series sub-sequences, aiming at selecting the most frequently\noccurring ones. Search-based methods, which try out series sub-sequence as\nmotif candidates, are currently believed to be the best methods in finding the\nmost frequent patterns.\n  However, this paper proposes an entirely new perspective in finding motifs.\nWe demonstrate that searching is non-optimal since the domain of motifs is\nrestricted, and instead we propose a principled optimization approach able to\nfind optimal motifs. We treat the occurrence frequency as a function and\ntime-series motifs as its parameters, therefore we \\textit{learn} the optimal\nmotifs that maximize the frequency function. In contrast to searching, our\nmethod is able to discover the most repetitive patterns (hence optimal), even\nin cases where they do not explicitly occur as sub-sequences. Experiments on\nseveral real-life time-series datasets show that the motifs found by our method\nare highly more frequent than the ones found through searching, for exactly the\nsame distance threshold.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 12:11:43 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Grabocka", "Josif", ""], ["Schilling", "Nicolas", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1505.00566", "submitter": "Palash Dey", "authors": "Palash Dey, Y. Narahari", "title": "Estimating the Margin of Victory of an Election using Sampling", "comments": "To appear in IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The margin of victory of an election is a useful measure to capture the\nrobustness of an election outcome. It also plays a crucial role in determining\nthe sample size of various algorithms in post election audit, polling etc. In\nthis work, we present efficient sampling based algorithms for estimating the\nmargin of victory of elections.\n  More formally, we introduce the \\textsc{$(c, \\epsilon, \\delta)$--Margin of\nVictory} problem, where given an election $\\mathcal{E}$ on $n$ voters, the goal\nis to estimate the margin of victory $M(\\mathcal{E})$ of $\\mathcal{E}$ within\nan additive factor of $c MoV(\\mathcal{E})+\\epsilon n$. We study the\n\\textsc{$(c, \\epsilon, \\delta)$--Margin of Victory} problem for many commonly\nused voting rules including scoring rules, approval, Bucklin, maximin, and\nCopeland$^{\\alpha}.$ We observe that even for the voting rules for which\ncomputing the margin of victory is NP-Hard, there may exist efficient sampling\nbased algorithms, as observed in the cases of maximin and Copeland$^{\\alpha}$\nvoting rules.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 09:25:42 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Dey", "Palash", ""], ["Narahari", "Y.", ""]]}, {"id": "1505.00755", "submitter": "Olegs Verhodubs", "authors": "Olegs Verhodubs", "title": "Towards the Ontology Web Search Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The project of the Ontology Web Search Engine is presented in this paper. The\nmain purpose of this paper is to develop such a project that can be easily\nimplemented. Ontology Web Search Engine is software to look for and index\nontologies in the Web. OWL (Web Ontology Languages) ontologies are meant, and\nthey are necessary for the functioning of the SWES (Semantic Web Expert\nSystem). SWES is an expert system that will use found ontologies from the Web,\ngenerating rules from them, and will supplement its knowledge base with these\ngenerated rules. It is expected that the SWES will serve as a universal expert\nsystem for the average user.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 19:04:19 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Verhodubs", "Olegs", ""]]}, {"id": "1505.00828", "submitter": "Carlo Comin MSc", "authors": "Carlo Comin, Romeo Rizzi", "title": "Dynamic Consistency of Conditional Simple Temporal Networks via Mean\n  Payoff Games: a Singly-Exponential Time DC-Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Simple Temporal Network (CSTN) is a constraint-based\ngraph-formalism for conditional temporal planning. It offers a more flexible\nformalism than the equivalent CSTP model of Tsamardinos, Vidal and Pollack,\nfrom which it was derived mainly as a sound formalization. Three notions of\nconsistency arise for CSTNs and CSTPs: weak, strong, and dynamic. Dynamic\nconsistency is the most interesting notion, but it is also the most challenging\nand it was conjectured to be hard to assess. Tsamardinos, Vidal and Pollack\ngave a doubly-exponential time algorithm for deciding whether a CSTN is\ndynamically-consistent and to produce, in the positive case, a dynamic\nexecution strategy of exponential size. In the present work we offer a proof\nthat deciding whether a CSTN is dynamically-consistent is coNP-hard and provide\nthe first singly-exponential time algorithm for this problem, also producing a\ndynamic execution strategy whenever the input CSTN is dynamically-consistent.\nThe algorithm is based on a novel connection with Mean Payoff Games, a family\nof two-player combinatorial games on graphs well known for having applications\nin model-checking and formal verification. The presentation of such connection\nis mediated by the Hyper Temporal Network model, a tractable generalization of\nSimple Temporal Networks whose consistency checking is equivalent to\ndetermining Mean Payoff Games. In order to analyze the algorithm we introduce a\nrefined notion of dynamic-consistency, named \\epsilon-dynamic-consistency, and\npresent a sharp lower bounding analysis on the critical value of the reaction\ntime \\hat{\\varepsilon} where the CSTN transits from being, to not being,\ndynamically-consistent. The proof technique introduced in this analysis of\n\\hat{\\varepsilon} is applicable more in general when dealing with linear\ndifference constraints which include strict inequalities.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 22:14:28 GMT"}, {"version": "v2", "created": "Fri, 8 May 2015 09:48:07 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2015 08:16:45 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2015 15:42:23 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Comin", "Carlo", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1505.00863", "submitter": "Shuangyong Song", "authors": "Shuangyong Song, Yao Meng, Zhongguang Zheng, Jun Sun", "title": "A Feature-based Classification Technique for Answering Multi-choice\n  World History Questions", "comments": "5 pages, no figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11.\nIn this paper, we describe our system for solving real-world university\nentrance exam questions, which are related to world history. Wikipedia is used\nas the main external resource for our system. Since problems with choosing\nright/wrong sentence from multiple sentence choices account for about\ntwo-thirds of the total, we individually design a classification based model\nfor solving this type of questions. For other types of questions, we also\ndesign some simple methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 02:06:23 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Song", "Shuangyong", ""], ["Meng", "Yao", ""], ["Zheng", "Zhongguang", ""], ["Sun", "Jun", ""]]}, {"id": "1505.01071", "submitter": "Darine Ameyed", "authors": "Darine Ameyed, Moeiz Miraoui, Chakib Tadj", "title": "A Spatiotemporal Context Definition for Service Adaptation Prediction in\n  a Pervasive Computing Environment", "comments": "Context-aware; Pervasive Computing; Context Definition; 2015.\n  International Journal of Advanced Studies in Computer Science and Engineering\n  (IJASCSE) http://www.ijascse.org/publications ;2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.ET cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pervasive systems refers to context-aware systems that can sense their\ncontext, and adapt their behavior accordingly to provide adaptable services.\nProactive adaptation of such systems allows changing the service and the\ncontext based on prediction. However, the definition of the context is still\nvague and not suitable to prediction. In this paper we discuss and classify\nprevious definitions of context. Then, we present a new definition which allows\npervasive systems to understand and predict their contexts. We analyze the\nessential lines that fall within the context definition, and propose some\nscenarios to make it clear our approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 16:34:23 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Ameyed", "Darine", ""], ["Miraoui", "Moeiz", ""], ["Tadj", "Chakib", ""]]}, {"id": "1505.01121", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Marcus Rohrbach and Mario Fritz", "title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about\n  Images", "comments": "ICCV'15 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a question answering task on real-world images that is set up as a\nVisual Turing Test. By combining latest advances in image representation and\nnatural language processing, we propose Neural-Image-QA, an end-to-end\nformulation to this problem for which all parts are trained jointly. In\ncontrast to previous efforts, we are facing a multi-modal problem where the\nlanguage output (answer) is conditioned on visual and natural language input\n(image and question). Our approach Neural-Image-QA doubles the performance of\nthe previous best approach on this problem. We provide additional insights into\nthe problem by analyzing how much information is contained only in the language\npart for which we provide a new human baseline. To study human consensus, which\nis related to the ambiguities inherent in this challenging task, we propose two\nnovel metrics and collect additional answers which extends the original DAQUAR\ndataset to DAQUAR-Consensus.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 18:39:29 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 08:10:01 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2015 12:13:20 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Rohrbach", "Marcus", ""], ["Fritz", "Mario", ""]]}, {"id": "1505.01221", "submitter": "Marius Lindauer", "authors": "Frank Hutter and Marius Lindauer and Adrian Balint and Sam Bayless and\n  Holger Hoos and Kevin Leyton-Brown", "title": "The Configurable SAT Solver Challenge (CSSC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that different solution strategies work well for different\ntypes of instances of hard combinatorial problems. As a consequence, most\nsolvers for the propositional satisfiability problem (SAT) expose parameters\nthat allow them to be customized to a particular family of instances. In the\ninternational SAT competition series, these parameters are ignored: solvers are\nrun using a single default parameter setting (supplied by the authors) for all\nbenchmark instances in a given track. While this competition format rewards\nsolvers with robust default settings, it does not reflect the situation faced\nby a practitioner who only cares about performance on one particular\napplication and can invest some time into tuning solver parameters for this\napplication. The new Configurable SAT Solver Competition (CSSC) compares\nsolvers in this latter setting, scoring each solver by the performance it\nachieved after a fully automated configuration step. This article describes the\nCSSC in more detail, and reports the results obtained in its two instantiations\nso far, CSSC 2013 and 2014.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 23:39:24 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 08:48:53 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Hutter", "Frank", ""], ["Lindauer", "Marius", ""], ["Balint", "Adrian", ""], ["Bayless", "Sam", ""], ["Hoos", "Holger", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "1505.01419", "submitter": "Ziqi Liu", "authors": "Ziqi Liu, Yu-Xiang Wang, Alexander J. Smola", "title": "Fast Differentially Private Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially private collaborative filtering is a challenging task, both in\nterms of accuracy and speed. We present a simple algorithm that is provably\ndifferentially private, while offering good performance, using a novel\nconnection of differential privacy to Bayesian posterior sampling via\nStochastic Gradient Langevin Dynamics. Due to its simplicity the algorithm\nlends itself to efficient implementation. By careful systems design and by\nexploiting the power law behavior of the data to maximize CPU cache bandwidth\nwe are able to generate 1024 dimensional models at a rate of 8.5 million\nrecommendations per second on a single PC.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 16:18:06 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 06:32:40 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Liu", "Ziqi", ""], ["Wang", "Yu-Xiang", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1505.01539", "submitter": "Luis Ortiz", "authors": "Luis E. Ortiz", "title": "Graphical Potential Games", "comments": "15 pages, To appear at The 26th International Conference on Game\n  Theory, part of the Stony Brook Game Theory Summer Festival 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Potential games, originally introduced in the early 1990's by Lloyd Shapley,\nthe 2012 Nobel Laureate in Economics, and his colleague Dov Monderer, are a\nvery important class of models in game theory. They have special properties\nsuch as the existence of Nash equilibria in pure strategies. This note\nintroduces graphical versions of potential games. Special cases of graphical\npotential games have already found applicability in many areas of science and\nengineering beyond economics, including artificial intelligence, computer\nvision, and machine learning. They have been effectively applied to the study\nand solution of important real-world problems such as routing and congestion in\nnetworks, distributed resource allocation (e.g., public goods), and\nrelaxation-labeling for image segmentation. Implicit use of graphical potential\ngames goes back at least 40 years. Several classes of games considered standard\nin the literature, including coordination games, local interaction games,\nlattice games, congestion games, and party-affiliation games, are instances of\ngraphical potential games. This note provides several characterizations of\ngraphical potential games by leveraging well-known results from the literature\non probabilistic graphical models. A major contribution of the work presented\nhere that particularly distinguishes it from previous work is establishing that\nthe convergence of certain type of game-playing rules implies that the\nagents/players must be embedded in some graphical potential game.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 23:27:55 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Ortiz", "Luis E.", ""]]}, {"id": "1505.01603", "submitter": "Aske Plaat", "authors": "Aske Plaat, Jonathan Schaeffer, Wim Pijls, Arie de Bruin", "title": "Best-First and Depth-First Minimax Search in Practice", "comments": "Computer Science in the Netherlands 1995. arXiv admin note: text\n  overlap with arXiv:1404.1515", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most practitioners use a variant of the Alpha-Beta algorithm, a simple\ndepth-first pro- cedure, for searching minimax trees. SSS*, with its best-first\nsearch strategy, reportedly offers the potential for more efficient search.\nHowever, the complex formulation of the al- gorithm and its alleged excessive\nmemory requirements preclude its use in practice. For two decades, the search\nefficiency of \"smart\" best-first SSS* has cast doubt on the effectiveness of\n\"dumb\" depth-first Alpha-Beta. This paper presents a simple framework for\ncalling Alpha-Beta that allows us to create a variety of algorithms, including\nSSS* and DUAL*. In effect, we formulate a best-first algorithm using\ndepth-first search. Expressed in this framework SSS* is just a special case of\nAlpha-Beta, solving all of the perceived drawbacks of the algorithm. In\npractice, Alpha-Beta variants typically evaluate less nodes than SSS*. A new\ninstance of this framework, MTD(f), out-performs SSS* and NegaScout, the\nAlpha-Beta variant of choice by practitioners.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 06:54:26 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Plaat", "Aske", ""], ["Schaeffer", "Jonathan", ""], ["Pijls", "Wim", ""], ["de Bruin", "Arie", ""]]}, {"id": "1505.01620", "submitter": "Dieter Hutter", "authors": "Serge Autexier and Dieter Hutter", "title": "Structure Formation in Large Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structuring theories is one of the main approaches to reduce the\ncombinatorial explosion associated with reasoning and exploring large theories.\nIn the past we developed the notion of development graphs as a means to\nrepresent and maintain structured theories. In this paper we present a\nmethodology and a resulting implementation to reveal the hidden structure of\nflat theories by transforming them into detailed development graphs. We review\nour approach using plain TSTP-representations of MIZAR articles obtaining more\nstructured and also more concise theories.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 08:16:19 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Autexier", "Serge", ""], ["Hutter", "Dieter", ""]]}, {"id": "1505.01629", "submitter": "Christoph Benzmueller", "authors": "Max Wisniewski and Alexander Steen and Christoph Benzm\\\"uller", "title": "LeoPARD --- A Generic Platform for the Implementation of Higher-Order\n  Reasoners", "comments": "6 pages, to appear in the proceedings of CICM'2015 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.MA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LeoPARD supports the implementation of knowledge representation and reasoning\ntools for higher-order logic(s). It combines a sophisticated data structure\nlayer (polymorphically typed {\\lambda}-calculus with nameless spine notation,\nexplicit substitutions, and perfect term sharing) with an ambitious multi-agent\nblackboard architecture (supporting prover parallelism at the term, clause, and\nsearch level). Further features of LeoPARD include a parser for all TPTP\ndialects, a command line interpreter, and generic means for the integration of\nexternal reasoners.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 08:54:19 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Wisniewski", "Max", ""], ["Steen", "Alexander", ""], ["Benzm\u00fcller", "Christoph", ""]]}, {"id": "1505.01757", "submitter": "Kazem Taghva", "authors": "Kazem Taghva", "title": "Contextual Analysis for Middle Eastern Languages with Hidden Markov\n  Models", "comments": null, "journal-ref": "International Journal on Natural Language Computing, vol. 4, No.\n  4, August 2015, pp. 1-11", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Displaying a document in Middle Eastern languages requires contextual\nanalysis due to different presentational forms for each character of the\nalphabet. The words of the document will be formed by the joining of the\ncorrect positional glyphs representing corresponding presentational forms of\nthe characters. A set of rules defines the joining of the glyphs. As usual,\nthese rules vary from language to language and are subject to interpretation by\nthe software developers.\n  In this paper, we propose a machine learning approach for contextual analysis\nbased on the first order Hidden Markov Model. We will design and build a model\nfor the Farsi language to exhibit this technology. The Farsi model achieves 94\n\\% accuracy with the training based on a short list of 89 Farsi vocabularies\nconsisting of 2780 Farsi characters.\n  The experiment can be easily extended to many languages including Arabic,\nUrdu, and Sindhi. Furthermore, the advantage of this approach is that the same\nsoftware can be used to perform contextual analysis without coding complex\nrules for each specific language. Of particular interest is that the languages\nwith fewer speakers can have greater representation on the web, since they are\ntypically ignored by software developers due to lack of financial incentives.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 16:03:02 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Taghva", "Kazem", ""]]}, {"id": "1505.01809", "submitter": "Jacob Devlin", "authors": "Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong\n  He, Geoffrey Zweig, Margaret Mitchell", "title": "Language Models for Image Captioning: The Quirks and What Works", "comments": "See http://research.microsoft.com/en-us/projects/image_captioning for\n  project information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two recent approaches have achieved state-of-the-art results in image\ncaptioning. The first uses a pipelined process where a set of candidate words\nis generated by a convolutional neural network (CNN) trained on images, and\nthen a maximum entropy (ME) language model is used to arrange these words into\na coherent sentence. The second uses the penultimate activation layer of the\nCNN as input to a recurrent neural network (RNN) that then generates the\ncaption sequence. In this paper, we compare the merits of these different\nlanguage modeling approaches for the first time by using the same\nstate-of-the-art CNN as input. We examine issues in the different approaches,\nincluding linguistic irregularities, caption repetition, and data set overlap.\nBy combining key aspects of the ME and RNN methods, we achieve a new record\nperformance over previously published results on the benchmark COCO dataset.\nHowever, the gains we see in BLEU do not translate to human judgments.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 18:36:14 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 22:10:49 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 22:03:40 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Devlin", "Jacob", ""], ["Cheng", "Hao", ""], ["Fang", "Hao", ""], ["Gupta", "Saurabh", ""], ["Deng", "Li", ""], ["He", "Xiaodong", ""], ["Zweig", "Geoffrey", ""], ["Mitchell", "Margaret", ""]]}, {"id": "1505.01825", "submitter": "Joseph Ramsey", "authors": "Joseph D. Ramsey", "title": "Effects of Nonparanormal Transform on PC and GES Search Accuracies", "comments": "10 pages, 18 tables, tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liu, et al., 2009 developed a transformation of a class of non-Gaussian\nunivariate distributions into Gaussian distributions. Liu and collaborators\n(2012) subsequently applied the transform to search for graphical causal models\nfor a number of empirical data sets. To our knowledge, there has been no\npublished investigation by simulation of the conditions under which the\ntransform aids, or harms, standard graphical model search procedures. We\nconsider here how the transform affects the performance of two search\nalgorithms in particular, PC (Spirtes et al., 2000; Meek 1995) and GES (Meek\n1997; Chickering 2002). We find that the transform is harmless but ineffective\nfor most cases but quite effective in very special cases for GES, namely, for\nmoderate non-Gaussianity and moderate non-linearity. For strong-linearity,\nanother algorithm, PC-GES (a combination of PC with GES), is equally effective.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 19:39:22 GMT"}, {"version": "v2", "created": "Fri, 8 May 2015 20:20:44 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Ramsey", "Joseph D.", ""]]}, {"id": "1505.02000", "submitter": "Matthew Lai", "authors": "Matthew Lai", "title": "Deep Learning for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides an overview of the current state of the art deep\nlearning architectures and optimisation techniques, and uses the ADNI\nhippocampus MRI dataset as an example to compare the effectiveness and\nefficiency of different convolutional architectures on the task of patch-based\n3-dimensional hippocampal segmentation, which is important in the diagnosis of\nAlzheimer's Disease. We found that a slightly unconventional \"stacked 2D\"\napproach provides much better classification performance than simple 2D patches\nwithout requiring significantly more computational power. We also examined the\npopular \"tri-planar\" approach used in some recently published studies, and\nfound that it provides much better results than the 2D approaches, but also\nwith a moderate increase in computational power requirement. Finally, we\nevaluated a full 3D convolutional architecture, and found that it provides\nmarginally better results than the tri-planar approach, but at the cost of a\nvery significant increase in computational power requirement.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 11:35:53 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Lai", "Matthew", ""]]}, {"id": "1505.02070", "submitter": "Mirko Stojadinovi\\'c", "authors": "Mirko Stojadinovi\\'c, Mladen Nikoli\\'c, Filip Mari\\'c", "title": "Short Portfolio Training for CSP Solving", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different approaches for solving Constraint Satisfaction Problems (CSPs)\nand related Constraint Optimization Problems (COPs) exist. However, there is no\nsingle solver (nor approach) that performs well on all classes of problems and\nmany portfolio approaches for selecting a suitable solver based on simple\nsyntactic features of the input CSP instance have been developed. In this paper\nwe first present a simple portfolio method for CSP based on k-nearest neighbors\nmethod. Then, we propose a new way of using portfolio systems --- training them\nshortly in the exploitation time, specifically for the set of instances to be\nsolved and using them on that set. Thorough evaluation has been performed and\nhas shown that the approach yields good results. We evaluated several machine\nlearning techniques for our portfolio. Due to its simplicity and efficiency,\nthe selected k-nearest neighbors method is especially suited for our short\ntraining approach and it also yields the best results among the tested methods.\nWe also confirm that our approach yields good results on SAT domain.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 15:42:13 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Stojadinovi\u0107", "Mirko", ""], ["Nikoli\u0107", "Mladen", ""], ["Mari\u0107", "Filip", ""]]}, {"id": "1505.02074", "submitter": "Mengye Ren", "authors": "Mengye Ren, Ryan Kiros, Richard Zemel", "title": "Exploring Models and Data for Image Question Answering", "comments": "12 pages. Conference paper at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to address the problem of image-based question-answering (QA)\nwith new models and datasets. In our work, we propose to use neural networks\nand visual semantic embeddings, without intermediate stages such as object\ndetection and image segmentation, to predict answers to simple questions about\nimages. Our model performs 1.8 times better than the only published results on\nan existing image QA dataset. We also present a question generation algorithm\nthat converts image descriptions, which are widely available, into QA form. We\nused this algorithm to produce an order-of-magnitude larger dataset, with more\nevenly distributed answers. A suite of baseline results on this new dataset are\nalso presented.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 15:59:44 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 19:55:07 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2015 06:44:44 GMT"}, {"version": "v4", "created": "Sun, 29 Nov 2015 22:45:12 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Ren", "Mengye", ""], ["Kiros", "Ryan", ""], ["Zemel", "Richard", ""]]}, {"id": "1505.02206", "submitter": "Dinesh Jayaraman", "authors": "Dinesh Jayaraman and Kristen Grauman", "title": "Learning image representations tied to ego-motion", "comments": "Supplementary material appended at end. In ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how images of objects and scenes behave in response to specific\nego-motions is a crucial aspect of proper visual development, yet existing\nvisual learning methods are conspicuously disconnected from the physical source\nof their images. We propose to exploit proprioceptive motor signals to provide\nunsupervised regularization in convolutional neural networks to learn visual\nrepresentations from egocentric video. Specifically, we enforce that our\nlearned features exhibit equivariance i.e. they respond predictably to\ntransformations associated with distinct ego-motions. With three datasets, we\nshow that our unsupervised feature learning approach significantly outperforms\nprevious approaches on visual recognition and next-best-view prediction tasks.\nIn the most challenging test, we show that features learned from video captured\non an autonomous driving platform improve large-scale scene recognition in\nstatic images from a disjoint domain.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 23:15:00 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 19:30:18 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Jayaraman", "Dinesh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1505.02405", "submitter": "Vasco Manquinho", "authors": "Miguel Neves and Ruben Martins and Mikol\\'a\\v{s} Janota and In\\^es\n  Lynce and Vasco Manquinho", "title": "Exploiting Resolution-based Representations for MaxSAT Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent MaxSAT algorithms rely on a succession of calls to a SAT solver\nin order to find an optimal solution. In particular, several algorithms take\nadvantage of the ability of SAT solvers to identify unsatisfiable subformulas.\nUsually, these MaxSAT algorithms perform better when small unsatisfiable\nsubformulas are found early. However, this is not the case in many problem\ninstances, since the whole formula is given to the SAT solver in each call. In\nthis paper, we propose to partition the MaxSAT formula using a resolution-based\ngraph representation. Partitions are then iteratively joined by using a\nproximity measure extracted from the graph representation of the formula. The\nalgorithm ends when only one partition remains and the optimal solution is\nfound. Experimental results show that this new approach further enhances a\nstate of the art MaxSAT solver to optimally solve a larger set of industrial\nproblem instances.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 16:38:15 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Neves", "Miguel", ""], ["Martins", "Ruben", ""], ["Janota", "Mikol\u00e1\u0161", ""], ["Lynce", "In\u00eas", ""], ["Manquinho", "Vasco", ""]]}, {"id": "1505.02408", "submitter": "Vasco Manquinho", "authors": "Miguel Neves, In\\^es Lynce and Vasco Manquinho", "title": "DistMS: A Non-Portfolio Distributed Solver for Maximum Satisfiability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most successful parallel SAT and MaxSAT solvers follow a portfolio\napproach, where each thread applies a different algorithm (or the same\nalgorithm configured differently) to solve a given problem instance. The main\ngoal of building a portfolio is to diversify the search process being carried\nout by each thread. As soon as one thread finishes, the instance can be deemed\nsolved. In this paper we present a new open source distributed solver for\nMaxSAT solving that addresses two issues commonly found in multicore parallel\nsolvers, namely memory contention and scalability. Preliminary results show\nthat our non-portfolio distributed MaxSAT solver outperforms its sequential\nversion and is able to solve more instances as the number of processes\nincreases.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 16:55:42 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Neves", "Miguel", ""], ["Lynce", "In\u00eas", ""], ["Manquinho", "Vasco", ""]]}, {"id": "1505.02419", "submitter": "Mo Yu", "authors": "Matthew R. Gormley and Mo Yu and Mark Dredze", "title": "Improved Relation Extraction with Feature-Rich Compositional Embedding\n  Models", "comments": "12 pages for EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional embedding models build a representation (or embedding) for a\nlinguistic structure based on its component word embeddings. We propose a\nFeature-rich Compositional Embedding Model (FCM) for relation extraction that\nis expressive, generalizes to new domains, and is easy-to-implement. The key\nidea is to combine both (unlexicalized) hand-crafted features with learned word\nembeddings. The model is able to directly tackle the difficulties met by\ntraditional compositional embeddings models, such as handling arbitrary types\nof sentence annotations and utilizing global information for composition. We\ntest the proposed model on two relation extraction tasks, and demonstrate that\nour model outperforms both previous compositional models and traditional\nfeature rich models on the ACE 2005 relation extraction task, and the SemEval\n2010 relation classification task. The combination of our model and a\nlog-linear classifier with hand-crafted features gives state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 18:47:06 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 01:01:34 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2015 02:01:34 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Gormley", "Matthew R.", ""], ["Yu", "Mo", ""], ["Dredze", "Mark", ""]]}, {"id": "1505.02433", "submitter": "Miao Fan", "authors": "Miao Fan, Qiang Zhou, Andrew Abel, Thomas Fang Zheng and Ralph\n  Grishman", "title": "Probabilistic Belief Embedding for Knowledge Base Completion", "comments": "arXiv admin note: text overlap with arXiv:1503.08155", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes a novel embedding model which measures the probability\nof each belief $\\langle h,r,t,m\\rangle$ in a large-scale knowledge repository\nvia simultaneously learning distributed representations for entities ($h$ and\n$t$), relations ($r$), and the words in relation mentions ($m$). It facilitates\nknowledge completion by means of simple vector operations to discover new\nbeliefs. Given an imperfect belief, we can not only infer the missing entities,\npredict the unknown relations, but also tell the plausibility of the belief,\njust leveraging the learnt embeddings of remaining evidences. To demonstrate\nthe scalability and the effectiveness of our model, we conduct experiments on\nseveral large-scale repositories which contain millions of beliefs from\nWordNet, Freebase and NELL, and compare it with other cutting-edge approaches\nvia competing the performances assessed by the tasks of entity inference,\nrelation prediction and triplet classification with respective metrics.\nExtensive experimental results show that the proposed model outperforms the\nstate-of-the-arts with significant improvements.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 20:22:47 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 02:19:39 GMT"}, {"version": "v3", "created": "Tue, 19 May 2015 14:56:16 GMT"}, {"version": "v4", "created": "Fri, 22 May 2015 16:58:33 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Fan", "Miao", ""], ["Zhou", "Qiang", ""], ["Abel", "Andrew", ""], ["Zheng", "Thomas Fang", ""], ["Grishman", "Ralph", ""]]}, {"id": "1505.02449", "submitter": "Daniel Raggi", "authors": "Daniel Raggi, Alan Bundy, Gudmund Grov, Alison Pease", "title": "Automating change of representation for proofs in discrete mathematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation determines how we can reason about a specific problem.\nSometimes one representation helps us find a proof more easily than others.\nMost current automated reasoning tools focus on reasoning within one\nrepresentation. There is, therefore, a need for the development of better tools\nto mechanise and automate formal and logically sound changes of representation.\n  In this paper we look at examples of representational transformations in\ndiscrete mathematics, and show how we have used Isabelle's Transfer tool to\nautomate the use of these transformations in proofs. We give a brief overview\nof a general theory of transformations that we consider appropriate for\nthinking about the matter, and we explain how it relates to the Transfer\npackage. We show our progress towards developing a general tactic that\nincorporates the automatic search for representation within the proving\nprocess.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 22:14:55 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Raggi", "Daniel", ""], ["Bundy", "Alan", ""], ["Grov", "Gudmund", ""], ["Pease", "Alison", ""]]}, {"id": "1505.02487", "submitter": "Caroline Even", "authors": "Caroline Even, Andreas Schutt, and Pascal Van Hentenryck", "title": "A Constraint Programming Approach for Non-Preemptive Evacuation\n  Scheduling", "comments": "Submitted to the 21st International Conference on Principles and\n  Practice of Constraint Programming (CP 2015). 15 pages + 1 reference page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale controlled evacuations require emergency services to select\nevacuation routes, decide departure times, and mobilize resources to issue\norders, all under strict time constraints. Existing algorithms almost always\nallow for preemptive evacuation schedules, which are less desirable in\npractice. This paper proposes, for the first time, a constraint-based\nscheduling model that optimizes the evacuation flow rate (number of vehicles\nsent at regular time intervals) and evacuation phasing of widely populated\nareas, while ensuring a nonpreemptive evacuation for each residential zone. Two\noptimization objectives are considered: (1) to maximize the number of evacuees\nreaching safety and (2) to minimize the overall duration of the evacuation.\nPreliminary results on a set of real-world instances show that the approach can\nproduce, within a few seconds, a non-preemptive evacuation schedule which is\neither optimal or at most 6% away of the optimal preemptive solution.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 05:38:24 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Even", "Caroline", ""], ["Schutt", "Andreas", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1505.02552", "submitter": "Guillaume Perez", "authors": "Guillaume Perez and Jean-Charles R\\'egin", "title": "Relations between MDDs and Tuples and Dynamic Modifications of MDDs\n  based constraints", "comments": "15 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relations between Multi-valued Decision Diagrams (MDD) and\ntuples (i.e. elements of the Cartesian Product of variables). First, we improve\nthe existing methods for transforming a set of tuples, Global Cut Seeds,\nsequences of tuples into MDDs. Then, we present some in-place algorithms for\nadding and deleting tuples from an MDD. Next, we consider an MDD constraint\nwhich is modified during the search by deleting some tuples. We give an\nalgorithm which adapts MDD-4R to these dynamic and persistent modifications.\nSome experiments show that MDD constraints are competitive with Table\nconstraints.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 10:32:59 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Perez", "Guillaume", ""], ["R\u00e9gin", "Jean-Charles", ""]]}, {"id": "1505.02648", "submitter": "Waqar  Ahmed", "authors": "Waqar Ahmed and Osman Hasan", "title": "Towards Formal Fault Tree Analysis using Theorem Proving", "comments": "16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault Tree Analysis (FTA) is a dependability analysis technique that has been\nwidely used to predict reliability, availability and safety of many complex\nengineering systems. Traditionally, these FTA-based analyses are done using\npaper-and-pencil proof methods or computer simulations, which cannot ascertain\nabsolute correctness due to their inherent limitations. As a complementary\napproach, we propose to use the higher-order-logic theorem prover HOL4 to\nconduct the FTA-based analysis of safety-critical systems where accuracy of\nfailure analysis is a dire need. In particular, the paper presents a\nhigher-order-logic formalization of generic Fault Tree gates, i.e., AND, OR,\nNAND, NOR, XOR and NOT and the formal verification of their failure probability\nexpressions. Moreover, we have formally verified the generic probabilistic\ninclusion-exclusion principle, which is one of the foremost requirements for\nconducting the FTA-based failure analysis of any given system. For illustration\npurposes, we conduct the FTA-based failure analysis of a solar array that is\nused as the main source of power for the Dong Fang Hong-3 (DFH-3) satellite.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 05:14:08 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Ahmed", "Waqar", ""], ["Hasan", "Osman", ""]]}, {"id": "1505.02729", "submitter": "Nakul Verma", "authors": "Nakul Verma and Kristin Branson", "title": "Sample complexity of learning Mahalanobis distance metrics", "comments": "26 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning seeks a transformation of the feature space that enhances\nprediction quality for the given task at hand. In this work we provide\nPAC-style sample complexity rates for supervised metric learning. We give\nmatching lower- and upper-bounds showing that the sample complexity scales with\nthe representation dimension when no assumptions are made about the underlying\ndata distribution. However, by leveraging the structure of the data\ndistribution, we show that one can achieve rates that are fine-tuned to a\nspecific notion of intrinsic complexity for a given dataset. Our analysis\nreveals that augmenting the metric learning optimization criterion with a\nsimple norm-based regularization can help adapt to a dataset's intrinsic\ncomplexity, yielding better generalization. Experiments on benchmark datasets\nvalidate our analysis and show that regularizing the metric can help discern\nthe signal even when the data contains high amounts of noise.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 18:55:42 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Verma", "Nakul", ""], ["Branson", "Kristin", ""]]}, {"id": "1505.02830", "submitter": "Yun-Ching Liu", "authors": "Yun-Ching Liu and Yoshimasa Tsuruoka", "title": "Adapting Improved Upper Confidence Bounds for Monte-Carlo Tree Search", "comments": "To appear in the 14th International Conference on Advances in\n  Computer Games (ACG 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The UCT algorithm, which combines the UCB algorithm and Monte-Carlo Tree\nSearch (MCTS), is currently the most widely used variant of MCTS. Recently, a\nnumber of investigations into applying other bandit algorithms to MCTS have\nproduced interesting results. In this research, we will investigate the\npossibility of combining the improved UCB algorithm, proposed by Auer et al.\n(2010), with MCTS. However, various characteristics and properties of the\nimproved UCB algorithm may not be ideal for a direct application to MCTS.\nTherefore, some modifications were made to the improved UCB algorithm, making\nit more suitable for the task of game tree search. The Mi-UCT algorithm is the\napplication of the modified UCB algorithm applied to trees. The performance of\nMi-UCT is demonstrated on the games of $9\\times 9$ Go and $9\\times 9$ NoGo, and\nhas shown to outperform the plain UCT algorithm when only a small number of\nplayouts are given, and rougly on the same level when more playouts are\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 22:59:31 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Liu", "Yun-Ching", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "1505.03101", "submitter": "Albert Mero\\~no-Pe\\~nuela", "authors": "Albert Mero\\~no-Pe\\~nuela, Christophe Gu\\'eret and Stefan Schlobach", "title": "Release Early, Release Often: Predicting Change in Versioned Knowledge\n  Organization Systems on the Web", "comments": "16 pages, 6 figures, ISWC 2015 conference pre-print The paper has\n  been withdrawn due to significant overlap with a subsequent paper submitted\n  to a conference for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Semantic Web is built on top of Knowledge Organization Systems (KOS)\n(vocabularies, ontologies, concept schemes) that provide a structured,\ninteroperable and distributed access to Linked Data on the Web. The maintenance\nof these KOS over time has produced a number of KOS version chains: subsequent\nunique version identifiers to unique states of a KOS. However, the release of\nnew KOS versions pose challenges to both KOS publishers and users. For\npublishers, updating a KOS is a knowledge intensive task that requires a lot of\nmanual effort, often implying deep deliberation on the set of changes to\nintroduce. For users that link their datasets to these KOS, a new version\ncompromises the validity of their links, often creating ramifications. In this\npaper we describe a method to automatically detect which parts of a Web KOS are\nlikely to change in a next version, using supervised learning on past versions\nin the KOS version chain. We use a set of ontology change features to model and\npredict change in arbitrary Web KOS. We apply our method on 139 varied datasets\nsystematically retrieved from the Semantic Web, obtaining robust results at\ncorrectly predicting change. To illustrate the accuracy, genericity and domain\nindependence of the method, we study the relationship between its effectiveness\nand several characterizations of the evaluated datasets, finding that\npredictors like the number of versions in a chain and their release frequency\nhave a fundamental impact in predictability of change in Web KOS. Consequently,\nwe argue for adopting a release early, release often philosophy in Web KOS\ndevelopment cycles.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 18:03:21 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 20:11:34 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Mero\u00f1o-Pe\u00f1uela", "Albert", ""], ["Gu\u00e9ret", "Christophe", ""], ["Schlobach", "Stefan", ""]]}, {"id": "1505.03463", "submitter": "Joanna Drummond", "authors": "Andrew Perrault, Joanna Drummond, Fahiem Bacchus", "title": "Exploring Strategy-Proofness, Uniqueness, and Pareto Optimality for the\n  Stable Matching Problem with Couples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Stable Matching Problem with Couples (SMP-C) is a ubiquitous real-world\nextension of the stable matching problem (SMP) involving complementarities.\nAlthough SMP can be solved in polynomial time, SMP-C is NP-Complete. Hence, it\nis not clear which, if any, of the theoretical results surrounding the\ncanonical SMP problem apply in this setting. In this paper, we use a\nrecently-developed SAT encoding to solve SMP-C exactly. This allows us to\nenumerate all stable matchings for any given instance of SMP-C. With this tool,\nwe empirically evaluate some of the properties that have been hypothesized to\nhold for SMP-C.\n  We take particular interest in investigating if, as the size of the market\ngrows, the percentage of instances with unique stable matchings also grows.\nWhile we did not find this trend among the random problem instances we sampled,\nwe did find that the percentage of instances with an resident optimal matching\nseems to more closely follow the trends predicted by previous conjectures. We\nalso define and investigate resident Pareto optimal stable matchings, finding\nthat, even though this is important desideratum for the deferred acceptance\nstyle algorithms previously designed to solve SMP-C, they do not always find\none.\n  We also investigate strategy-proofness for SMP-C, showing that even if only\none stable matching exists, residents still have incentive to misreport their\npreferences. However, if a problem has a resident optimal stable matching, we\nshow that residents cannot manipulate via truncation.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 17:25:56 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Perrault", "Andrew", ""], ["Drummond", "Joanna", ""], ["Bacchus", "Fahiem", ""]]}, {"id": "1505.03540", "submitter": "Mohammad Havaei", "authors": "Mohammad Havaei, Axel Davy, David Warde-Farley, Antoine Biard, Aaron\n  Courville, Yoshua Bengio, Chris Pal, Pierre-Marc Jodoin, Hugo Larochelle", "title": "Brain Tumor Segmentation with Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2016.05.004", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a fully automatic brain tumor segmentation method\nbased on Deep Neural Networks (DNNs). The proposed networks are tailored to\nglioblastomas (both low and high grade) pictured in MR images. By their very\nnature, these tumors can appear anywhere in the brain and have almost any kind\nof shape, size, and contrast. These reasons motivate our exploration of a\nmachine learning solution that exploits a flexible, high capacity DNN while\nbeing extremely efficient. Here, we give a description of different model\nchoices that we've found to be necessary for obtaining competitive performance.\nWe explore in particular different architectures based on Convolutional Neural\nNetworks (CNN), i.e. DNNs specifically adapted to image data.\n  We present a novel CNN architecture which differs from those traditionally\nused in computer vision. Our CNN exploits both local features as well as more\nglobal contextual features simultaneously. Also, different from most\ntraditional uses of CNNs, our networks use a final layer that is a\nconvolutional implementation of a fully connected layer which allows a 40 fold\nspeed up. We also describe a 2-phase training procedure that allows us to\ntackle difficulties related to the imbalance of tumor labels. Finally, we\nexplore a cascade architecture in which the output of a basic CNN is treated as\nan additional source of information for a subsequent CNN. Results reported on\nthe 2013 BRATS test dataset reveal that our architecture improves over the\ncurrently published state-of-the-art while being over 30 times faster.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 20:06:21 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 17:37:02 GMT"}, {"version": "v3", "created": "Fri, 20 May 2016 06:30:23 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Havaei", "Mohammad", ""], ["Davy", "Axel", ""], ["Warde-Farley", "David", ""], ["Biard", "Antoine", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""], ["Pal", "Chris", ""], ["Jodoin", "Pierre-Marc", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1505.03662", "submitter": "Gabriel Martins Dias", "authors": "Gabriel Martins Dias, Boris Bellalta and Simon Oechsner", "title": "Predicting Occupancy Trends in Barcelona's Bicycle Service Stations\n  Using Open Data", "comments": "7 pages, 7 figures, 1 table, accepted to SAI Intelligent Systems\n  Conference 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2008, the CEO of the company that manages and maintains the public bicycle\nservice in Barcelona recognized that one may not expect to always find a place\nto leave the rented bike nearby their destination, similarly to the case when,\ndriving a car, people may not find a parking lot. In this work, we make\npredictions about the statuses of the stations of the public bicycle service in\nBarcelona. We show that it is feasible to correctly predict nearly half of the\ntimes when the stations are either completely full of bikes or completely\nempty, up to 2 days before they actually happen. That is, users might avoid\nstations at times when they could not return a bicycle that they have rented\nbefore, or when they would not find a bike to rent. To achieve that, we apply\nthe Random Forest algorithm to classify the status of the stations and improve\nthe lifetime of the models using publicly available data, such as information\nabout the weather forecast. Finally, we expect that the results of the\npredictions can be used to improve the quality of the service and make it more\nreliable for the users.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 09:37:18 GMT"}, {"version": "v2", "created": "Fri, 15 May 2015 20:47:18 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2015 09:24:05 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Dias", "Gabriel Martins", ""], ["Bellalta", "Boris", ""], ["Oechsner", "Simon", ""]]}, {"id": "1505.03953", "submitter": "Susmit Jha", "authors": "Susmit Jha, Sanjit A. Seshia", "title": "A Theory of Formal Synthesis via Inductive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal synthesis is the process of generating a program satisfying a\nhigh-level formal specification. In recent times, effective formal synthesis\nmethods have been proposed based on the use of inductive learning. We refer to\nthis class of methods that learn programs from examples as formal inductive\nsynthesis. In this paper, we present a theoretical framework for formal\ninductive synthesis. We discuss how formal inductive synthesis differs from\ntraditional machine learning. We then describe oracle-guided inductive\nsynthesis (OGIS), a framework that captures a family of synthesizers that\noperate by iteratively querying an oracle. An instance of OGIS that has had\nmuch practical impact is counterexample-guided inductive synthesis (CEGIS). We\npresent a theoretical characterization of CEGIS for learning any program that\ncomputes a recursive language. In particular, we analyze the relative power of\nCEGIS variants where the types of counterexamples generated by the oracle\nvaries. We also consider the impact of bounded versus unbounded memory\navailable to the learning algorithm. In the special case where the universe of\ncandidate programs is finite, we relate the speed of convergence to the notion\nof teaching dimension studied in machine learning theory. Altogether, the\nresults of the paper take a first step towards a theoretical foundation for the\nemerging field of formal inductive synthesis.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 03:47:18 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 14:13:04 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2015 01:21:36 GMT"}, {"version": "v4", "created": "Sat, 21 May 2016 07:41:54 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Jha", "Susmit", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1505.03996", "submitter": "Natalia Criado", "authors": "Natalia Criado, Jose M. Such", "title": "Norm Monitoring under Partial Action Observability", "comments": "Accepted at the IEEE Transaction on Cybernetics", "journal-ref": null, "doi": "10.1109/TCYB.2015.2513430", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of using norms for controlling multi-agent systems, a vitally\nimportant question that has not yet been addressed in the literature is the\ndevelopment of mechanisms for monitoring norm compliance under partial action\nobservability. This paper proposes the reconstruction of unobserved actions to\ntackle this problem. In particular, we formalise the problem of reconstructing\nunobserved actions, and propose an information model and algorithms for\nmonitoring norms under partial action observability using two different\nprocesses for reconstructing unobserved actions. Our evaluation shows that\nreconstructing unobserved actions increases significantly the number of norm\nviolations and fulfilments detected.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 09:16:07 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 08:30:04 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Criado", "Natalia", ""], ["Such", "Jose M.", ""]]}, {"id": "1505.04097", "submitter": "Charmgil Hong", "authors": "Charmgil Hong, Milos Hauskrecht", "title": "MCODE: Multivariate Conditional Outlier Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection aims to identify unusual data instances that deviate from\nexpected patterns. The outlier detection is particularly challenging when\noutliers are context dependent and when they are defined by unusual\ncombinations of multiple outcome variable values. In this paper, we develop and\nstudy a new conditional outlier detection approach for multivariate outcome\nspaces that works by (1) transforming the conditional detection to the outlier\ndetection problem in a new (unconditional) space and (2) defining outlier\nscores by analyzing the data in the new space. Our approach relies on the\nclassifier chain decomposition of the multi-dimensional classification problem\nthat lets us transform the output space into a probability vector, one\nprobability for each dimension of the output space. Outlier scores applied to\nthese transformed vectors are then used to detect the outliers. Experiments on\nmultiple multi-dimensional classification problems with the different outlier\ninjection rates show that our methodology is robust and able to successfully\nidentify outliers when outliers are either sparse (manifested in one or very\nfew dimensions) or dense (affecting multiple dimensions).\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 15:37:51 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Hong", "Charmgil", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1505.04107", "submitter": "Kaladzavi Guidedi", "authors": "Guidedi Kaladzavi, Papa Fary Diallo, Kolyang, Moussa Lo", "title": "OntoSOC: Sociocultural Knowledge Ontology", "comments": "8 pages, 5 figures, 2 tables", "journal-ref": "IJWesT Vol. 6, No. 2 (2015)", "doi": "10.5121/ijwest.2015.6201", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents a sociocultural knowledge ontology (OntoSOC) modeling\napproach. OntoSOC modeling approach is based on Engestrom Human Activity Theory\n(HAT). That Theory allowed us to identify fundamental concepts and\nrelationships between them. The top-down precess has been used to define\ndifferents sub-concepts. The modeled vocabulary permits us to organise data, to\nfacilitate information retrieval by introducing a semantic layer in social web\nplatform architecture, we project to implement. This platform can be considered\nas a collective memory and Participative and Distributed Information System\n(PDIS) which will allow Cameroonian communities to share an co-construct\nknowledge on permanent organized activities.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 16:17:54 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Kaladzavi", "Guidedi", ""], ["Diallo", "Papa Fary", ""], ["Kolyang", "", ""], ["Lo", "Moussa", ""]]}, {"id": "1505.04112", "submitter": "Jennifer Warrender BSc MSc", "authors": "Jennifer D. Warrender and Phillip Lord", "title": "How, What and Why to test an ontology", "comments": "4 pages, accepted at Bio-Ontologies 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology development relates to software development in that they both\ninvolve the production of formal computational knowledge. It is possible,\ntherefore, that some of the techniques used in software engineering could also\nbe used for ontologies; for example, in software engineering testing is a\nwell-established process, and part of many different methodologies.\n  The application of testing to ontologies, therefore, seems attractive. The\nKaryotype Ontology is developed using the novel Tawny-OWL library. This\nprovides a fully programmatic environment for ontology development, which\nincludes a complete test harness.\n  In this paper, we describe how we have used this harness to build an\nextensive series of tests as well as used a commodity continuous integration\nsystem to link testing deeply into our development process; this environment,\nis applicable to any OWL ontology whether written using Tawny-OWL or not.\nMoreover, we present a novel analysis of our tests, introducing a new\nclassification of what our different tests are. For each class of test, we\ndescribe why we use these tests, also by comparison to software tests. We\nbelieve that this systematic comparison between ontology and software\ndevelopment will help us move to a more agile form of ontology development.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 16:30:17 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Warrender", "Jennifer D.", ""], ["Lord", "Phillip", ""]]}, {"id": "1505.04123", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Javier Pe\\~na", "title": "Margins, Kernels and Non-linear Smoothed Perceptrons", "comments": "17 pages, published in the proceedings of the International\n  Conference on Machine Learning, 2014", "journal-ref": "Ramdas, Aaditya, and Javier Pena. \"Margins, kernels and non-linear\n  smoothed perceptrons.\" Proceedings of the 31st International Conference on\n  Machine Learning (ICML-14). 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of finding a non-linear classification function that\nlies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of\nview (finding a perfect separator when one exists) and the dual point of view\n(giving a certificate of non-existence), with special focus on generalizations\nof two classical schemes - the Perceptron (primal) and Von-Neumann (dual)\nalgorithms.\n  We cast our problem as one of maximizing the regularized normalized\nhard-margin ($\\rho$) in an RKHS and %use the Representer Theorem to rephrase it\nin terms of a Mahalanobis dot-product/semi-norm associated with the kernel's\n(normalized and signed) Gram matrix. We derive an accelerated smoothed\nalgorithm with a convergence rate of $\\tfrac{\\sqrt {\\log n}}{\\rho}$ given $n$\nseparable points, which is strikingly similar to the classical kernelized\nPerceptron algorithm whose rate is $\\tfrac1{\\rho^2}$. When no such classifier\nexists, we prove a version of Gordan's separation theorem for RKHSs, and give a\nreinterpretation of negative margins. This allows us to give guarantees for a\nprimal-dual algorithm that halts in $\\min\\{\\tfrac{\\sqrt n}{|\\rho|},\n\\tfrac{\\sqrt n}{\\epsilon}\\}$ iterations with a perfect separator in the RKHS if\nthe primal is feasible or a dual $\\epsilon$-certificate of near-infeasibility.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 16:54:58 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Pe\u00f1a", "Javier", ""]]}, {"id": "1505.04150", "submitter": "Zhipeng Wang", "authors": "Zhipeng Wang and Mingbo Cai", "title": "Reinforcement Learning applied to Single Neuron", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper extends the reinforcement learning ideas into the multi-agents\nsystem, which is far more complicated than the previously studied single-agent\nsystem. We studied two different multi-agents systems. One is the\nfully-connected neural network consists of multiple single neurons. Another one\nis the simplified mechanical arm system which is controlled by multiple\nneurons. We suppose that each neuron is like an agent and it can do Gibbs\nsampling of the posterior probability of stimulus features. The policy is\noptimized in a way that the cumulative global rewards are maximized. The\nalgorithm for the second system is based on the same idea but we incorporate\nthe physics model into the constraints. The simulation results show that for\nthe first system our algorithm converges well. For the second system it does\nnot converge well in a reasonable simulation time length. In summary, we took\nthe initial endeavor to study the reinforcement learning for multi-agents\nsystem. The computational complexity is always an issue and significant amount\nof works have to be done in order to better understand the problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 18:36:20 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Wang", "Zhipeng", ""], ["Cai", "Mingbo", ""]]}, {"id": "1505.04214", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Aarti Singh", "title": "Algorithmic Connections Between Active Learning and Stochastic Convex\n  Optimization", "comments": "15 pages, published in proceedings of Algorithmic Learning Theory,\n  Springer Berlin Heidelberg, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interesting theoretical associations have been established by recent papers\nbetween the fields of active learning and stochastic convex optimization due to\nthe common role of feedback in sequential querying mechanisms. In this paper,\nwe continue this thread in two parts by exploiting these relations for the\nfirst time to yield novel algorithms in both fields, further motivating the\nstudy of their intersection. First, inspired by a recent optimization algorithm\nthat was adaptive to unknown uniform convexity parameters, we present a new\nactive learning algorithm for one-dimensional thresholds that can yield minimax\nrates by adapting to unknown noise parameters. Next, we show that one can\nperform $d$-dimensional stochastic minimization of smooth uniformly convex\nfunctions when only granted oracle access to noisy gradient signs along any\ncoordinate instead of real-valued gradients, by using a simple randomized\ncoordinate descent procedure where each line search can be solved by\n$1$-dimensional active learning, provably achieving the same error convergence\nrate as having the entire real-valued gradient. Combining these two parts\nyields an algorithm that solves stochastic convex optimization of uniformly\nconvex and smooth functions using only noisy gradient signs by repeatedly\nperforming active learning, achieves optimal rates and is adaptive to all\nunknown convexity and smoothness parameters.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 22:38:28 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Singh", "Aarti", ""]]}, {"id": "1505.04215", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Barnabas Poczos, Aarti Singh, Larry Wasserman", "title": "An Analysis of Active Learning With Uniform Feature Noise", "comments": "24 pages, 2 figures, published in the proceedings of the 17th\n  International Conference on Artificial Intelligence and Statistics (AISTATS),\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In active learning, the user sequentially chooses values for feature $X$ and\nan oracle returns the corresponding label $Y$. In this paper, we consider the\neffect of feature noise in active learning, which could arise either because\n$X$ itself is being measured, or it is corrupted in transmission to the oracle,\nor the oracle returns the label of a noisy version of the query point. In\nstatistics, feature noise is known as \"errors in variables\" and has been\nstudied extensively in non-active settings. However, the effect of feature\nnoise in active learning has not been studied before. We consider the\nwell-known Berkson errors-in-variables model with additive uniform noise of\nwidth $\\sigma$.\n  Our simple but revealing setting is that of one-dimensional binary\nclassification setting where the goal is to learn a threshold (point where the\nprobability of a $+$ label crosses half). We deal with regression functions\nthat are antisymmetric in a region of size $\\sigma$ around the threshold and\nalso satisfy Tsybakov's margin condition around the threshold. We prove minimax\nlower and upper bounds which demonstrate that when $\\sigma$ is smaller than the\nminimiax active/passive noiseless error derived in \\cite{CN07}, then noise has\nno effect on the rates and one achieves the same noiseless rates. For larger\n$\\sigma$, the \\textit{unflattening} of the regression function on convolution\nwith uniform noise, along with its local antisymmetry around the threshold,\ntogether yield a behaviour where noise \\textit{appears} to be beneficial. Our\nkey result is that active learning can buy significant improvement over a\npassive strategy even in the presence of feature noise.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 22:54:01 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1505.04265", "submitter": "Viktoras Veitas Mr.", "authors": "Viktoras Veitas and David Weinbaum (Weaver)", "title": "Cognitive Development of the Web", "comments": "Working paper, 22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "ECCO working paper 2015-02", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sociotechnological system is a system constituted of human individuals\nand their artifacts: technological artifacts, institutions, conceptual and\nrepresentational systems, worldviews, knowledge systems, culture and the whole\nbiosphere as a volutionary niche. In our view the sociotechnological system as\na super-organism is shaped and determined both by the characteristics of the\nagents involved and the characteristics emergent in their interactions at\nmultiple scales. Our approach to sociotechnological dynamics will maintain a\nbalance between perspectives: the individual and the collective. Accordingly,\nwe analyze dynamics of the Web as a sociotechnological system made of people,\ncomputers and digital artifacts (Web pages, databases, search engines, etc.).\nMaking sense of the sociotechnological system while being part of it, is also a\nconstant interplay between pragmatic and value based approaches. The first is\nfocusing on the actualities of the system while the second highlights the\nobserver's projections. In our attempt to model sociotechnological dynamics and\nenvision its future, we take special care to make explicit our values as part\nof the analysis. In sociotechnological systems with a high degree of\nreflexivity (coupling between the perception of the system and the system's\nbehavior), highlighting values is of critical importance. In this essay, we\nchoose to see the future evolution of the web as facilitating a basic value,\nthat is, continuous open-ended intelligence expansion. By that we mean that we\nsee intelligence expansion as the determinant of the 'greater good' and 'well\nbeing' of both of individuals and collectives at all scales. Our working\ndefinition of intelligence here is the progressive process of sense-making of\nself, other, environment and universe. Intelligence expansion, therefore, means\nan increasing ability of sense-making.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 11:55:56 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Veitas", "Viktoras", "", "Weaver"], ["Weinbaum", "David", "", "Weaver"]]}, {"id": "1505.04406", "submitter": "Stephen Bach", "authors": "Stephen H. Bach, Matthias Broecheler, Bert Huang, Lise Getoor", "title": "Hinge-Loss Markov Random Fields and Probabilistic Soft Logic", "comments": null, "journal-ref": "Journal of Machine Learning Research (JMLR), 18(109):1-67, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in developing high-impact machine learning\ntechnologies is balancing the need to model rich, structured domains with the\nability to scale to big data. Many important problem areas are both richly\nstructured and large scale, from social and biological networks, to knowledge\ngraphs and the Web, to images, video, and natural language. In this paper, we\nintroduce two new formalisms for modeling structured data, and show that they\ncan both capture rich structure and scale to big data. The first, hinge-loss\nMarkov random fields (HL-MRFs), is a new kind of probabilistic graphical model\nthat generalizes different approaches to convex inference. We unite three\napproaches from the randomized algorithms, probabilistic graphical models, and\nfuzzy logic communities, showing that all three lead to the same inference\nobjective. We then define HL-MRFs by generalizing this unified objective. The\nsecond new formalism, probabilistic soft logic (PSL), is a probabilistic\nprogramming language that makes HL-MRFs easy to define using a syntax based on\nfirst-order logic. We introduce an algorithm for inferring most-probable\nvariable assignments (MAP inference) that is much more scalable than\ngeneral-purpose convex optimization methods, because it uses message passing to\ntake advantage of sparse dependency structures. We then show how to learn the\nparameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous\ndiscrete models, but much more scalable. Together, these algorithms enable\nHL-MRFs and PSL to model rich, structured data at scales not previously\npossible.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 15:19:09 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 04:32:24 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 00:10:03 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Bach", "Stephen H.", ""], ["Broecheler", "Matthias", ""], ["Huang", "Bert", ""], ["Getoor", "Lise", ""]]}, {"id": "1505.04497", "submitter": "Jan Leike", "authors": "Mayank Daswani and Jan Leike", "title": "A Definition of Happiness for Reinforcement Learning Agents", "comments": "AGI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is happiness for reinforcement learning agents? We seek a formal\ndefinition satisfying a list of desiderata. Our proposed definition of\nhappiness is the temporal difference error, i.e. the difference between the\nvalue of the obtained reward and observation and the agent's expectation of\nthis value. This definition satisfies most of our desiderata and is compatible\nwith empirical research on humans. We state several implications and discuss\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 03:14:39 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Daswani", "Mayank", ""], ["Leike", "Jan", ""]]}, {"id": "1505.04518", "submitter": "Christopher Marriott", "authors": "Chris Marriott and Jobran Chebib", "title": "Emergence-focused design in complex system simulation", "comments": "European Conference on Artificial Life 2015 - York, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergence is a phenomenon taken for granted in science but also still not\nwell understood. We have developed a model of artificial genetic evolution\nintended to allow for emergence on genetic, population and social levels. We\npresent the details of the current state of our environment, agent, and\nreproductive models. In developing our models we have relied on a principle of\nusing non-linear systems to model as many systems as possible including\nmutation and recombination, gene-environment interaction, agent metabolism,\nagent survival, resource gathering and sexual reproduction. In this paper we\nreview the genetic dynamics that have emerged in our system including\ngenotype-phenotype divergence, genetic drift, pseudogenes, and gene\nduplication. We conclude that emergence-focused design in complex system\nsimulation is necessary to reproduce the multilevel emergence seen in the\nnatural world.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 05:42:38 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Marriott", "Chris", ""], ["Chebib", "Jobran", ""]]}, {"id": "1505.04542", "submitter": "Daisuke Ishii", "authors": "Daisuke Ishii, Kazuki Yoshizoe, Toyotaro Suzumura", "title": "Scalable Parallel Numerical Constraint Solver Using Global Load\n  Balancing", "comments": "To be presented at X10'15 Workshop", "journal-ref": null, "doi": "10.1145/2771774.2771776", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable parallel solver for numerical constraint satisfaction\nproblems (NCSPs). Our parallelization scheme consists of homogeneous worker\nsolvers, each of which runs on an available core and communicates with others\nvia the global load balancing (GLB) method. The parallel solver is implemented\nwith X10 that provides an implementation of GLB as a library. In experiments,\nseveral NCSPs from the literature were solved and attained up to 516-fold\nspeedup using 600 cores of the TSUBAME2.5 supercomputer.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 08:04:45 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Ishii", "Daisuke", ""], ["Yoshizoe", "Kazuki", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1505.04578", "submitter": "Emanuel Diamant", "authors": "Emanuel Diamant", "title": "Advances in Artificial Intelligence: Deep Intentions, Shallow\n  Achievements", "comments": "The paper was submitted to the ICAI'15 conference (Las Vegas, Nevada,\n  USA, July 27-30, 2015) and was accepted as a poster presentation. arXiv admin\n  note: substantial text overlap with arXiv:1502.04791", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, AI has made a remarkable progress due to recently\nrevived Deep Learning technology. Deep Learning enables to process large\namounts of data using simplified neuron networks that simulate the way in which\nthe brain works. At the same time, there is another point of view that posits\nthat brain is processing information, not data. This duality hampered AI\nprogress for years. To provide a remedy for this situation, I propose a new\ndefinition of information that considers it as a coupling between two separate\nentities - physical information (that implies data processing) and semantic\ninformation (that provides physical information interpretation). In such a\ncase, intelligence arises as a result of information processing. The paper\npoints on the consequences of this turn for the AI design philosophy.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 10:05:24 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Diamant", "Emanuel", ""]]}, {"id": "1505.04636", "submitter": "Alex Smola J", "authors": "Mu Li, Dave G. Andersen, Alexander J. Smola", "title": "Graph Partitioning via Parallel Submodular Approximation to Accelerate\n  Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed computing excels at processing large scale data, but the\ncommunication cost for synchronizing the shared parameters may slow down the\noverall performance. Fortunately, the interactions between parameter and data\nin many problems are sparse, which admits efficient partition in order to\nreduce the communication overhead.\n  In this paper, we formulate data placement as a graph partitioning problem.\nWe propose a distributed partitioning algorithm. We give both theoretical\nguarantees and a highly efficient implementation. We also provide a highly\nefficient implementation of the algorithm and demonstrate its promising results\non both text datasets and social networks. We show that the proposed algorithm\nleads to 1.6x speedup of a state-of-the-start distributed machine learning\nsystem by eliminating 90\\% of the network communication.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:43:46 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Li", "Mu", ""], ["Andersen", "Dave G.", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1505.04677", "submitter": "Vilem Vychodil", "authors": "Vilem Vychodil", "title": "On sets of graded attribute implications with witnessed non-redundancy", "comments": null, "journal-ref": "Information Sciences 329 (2016), 434-446", "doi": "10.1016/j.ins.2015.09.044", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study properties of particular non-redundant sets of if-then rules\ndescribing dependencies between graded attributes. We introduce notions of\nsaturation and witnessed non-redundancy of sets of graded attribute\nimplications are show that bases of graded attribute implications given by\nsystems of pseudo-intents correspond to non-redundant sets of graded attribute\nimplications with saturated consequents where the non-redundancy is witnessed\nby antecedents of the contained graded attribute implications. We introduce an\nalgorithm which transforms any complete set of graded attribute implications\nparameterized by globalization into a base given by pseudo-intents.\nExperimental evaluation is provided to compare the method of obtaining bases\nfor general parameterizations by hedges with earlier graph-based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 15:15:53 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Vychodil", "Vilem", ""]]}, {"id": "1505.04746", "submitter": "Nasser Ghadiri", "authors": "Somaye Davari and Nasser Ghadiri", "title": "Spatial database implementation of fuzzy region connection calculus for\n  analysing the relationship of diseases", "comments": "ICEE2015", "journal-ref": null, "doi": "10.1109/IranianCEE.2015.7146310", "report-no": null, "categories": "cs.DB cs.AI cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing huge amounts of spatial data plays an important role in many\nemerging analysis and decision-making domains such as healthcare, urban\nplanning, agriculture and so on. For extracting meaningful knowledge from\ngeographical data, the relationships between spatial data objects need to be\nanalyzed. An important class of such relationships are topological relations\nlike the connectedness or overlap between regions. While real-world\ngeographical regions such as lakes or forests do not have exact boundaries and\nare fuzzy, most of the existing analysis methods neglect this inherent feature\nof topological relations. In this paper, we propose a method for handling the\ntopological relations in spatial databases based on fuzzy region connection\ncalculus (RCC). The proposed method is implemented in PostGIS spatial database\nand evaluated in analyzing the relationship of diseases as an important\napplication domain. We also used our fuzzy RCC implementation for fuzzification\nof the skyline operator in spatial databases. The results of the evaluation\nshow that our method provides a more realistic view of spatial relationships\nand gives more flexibility to the data analyst to extract meaningful and\naccurate results in comparison with the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 18:13:29 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 17:45:03 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Davari", "Somaye", ""], ["Ghadiri", "Nasser", ""]]}, {"id": "1505.04771", "submitter": "Eric Malmi", "authors": "Eric Malmi, Pyry Takala, Hannu Toivonen, Tapani Raiko, Aristides\n  Gionis", "title": "DopeLearning: A Computational Approach to Rap Lyrics Generation", "comments": "This is a pre-print of an article appearing at KDD'16", "journal-ref": null, "doi": "10.1145/2939672.2939679", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing rap lyrics requires both creativity to construct a meaningful,\ninteresting story and lyrical skills to produce complex rhyme patterns, which\nform the cornerstone of good flow. We present a rap lyrics generation method\nthat captures both of these aspects. First, we develop a prediction model to\nidentify the next line of existing lyrics from a set of candidate next lines.\nThis model is based on two machine-learning techniques: the RankSVM algorithm\nand a deep neural network model with a novel structure. Results show that the\nprediction model can identify the true next line among 299 randomly selected\nlines with an accuracy of 17%, i.e., over 50 times more likely than by random.\nSecond, we employ the prediction model to combine lines from existing songs,\nproducing lyrics with rhyme and a meaning. An evaluation of the produced lyrics\nshows that in terms of quantitative rhyme density, the method outperforms the\nbest human rappers by 21%. The rap lyrics generator has been deployed as an\nonline tool called DeepBeat, and the performance of the tool has been assessed\nby analyzing its usage logs. This analysis shows that machine-learned rankings\ncorrelate with user preferences.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 19:35:21 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 20:51:02 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Malmi", "Eric", ""], ["Takala", "Pyry", ""], ["Toivonen", "Hannu", ""], ["Raiko", "Tapani", ""], ["Gionis", "Aristides", ""]]}, {"id": "1505.04813", "submitter": "Hao Wu", "authors": "Hao Wu", "title": "What is Learning? A primary discussion about information and\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Nowadays, represented by Deep Learning techniques, the field of machine\nlearning is experiencing unprecedented prosperity and its influence is\ndemonstrated in academia, industry and civil society. \"Intelligent\" has become\na label which could not be neglected for most applications; celebrities and\nscientists also warned that the development of full artificial intelligence may\nspell the end of the human race. It seems that the answer to building a\ncomputer system that could automatically improve with experience is right on\nthe next corner. While for AI and machine learning researchers, it is a\nconsensus that we are not anywhere near the core technique which could bring\nthe Terminator, Number 5 or R2D2 into real life, and there is not even a formal\ndefinition about what is intelligence, or one of its basic properties:\nLearning. Therefore, even though researchers know these concerns are not\nnecessary currently, there is no generalized explanation about why these\nconcerns are not necessary, and what properties people should take into account\nthat would make these concerns to be necessary. In this paper, starts from\nanalysing the relation between information and its representation, a necessary\ncondition for a model to be a learning model is proposed. This condition and\nrelated future works could be used to verify whether a system is able to learn\nor not, and enrich our understanding of learning: one important property of\nIntelligence.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 01:17:47 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Wu", "Hao", ""]]}, {"id": "1505.04935", "submitter": "Alina S\\^irbu", "authors": "Alina S\\^irbu and Ozalp Babaoglu", "title": "Towards Data-Driven Autonomics in Data Centers", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continued reliance on human operators for managing data centers is a major\nimpediment for them from ever reaching extreme dimensions. Large computer\nsystems in general, and data centers in particular, will ultimately be managed\nusing predictive computational and executable models obtained through\ndata-science tools, and at that point, the intervention of humans will be\nlimited to setting high-level goals and policies rather than performing\nlow-level operations. Data-driven autonomics, where management and control are\nbased on holistic predictive models that are built and updated using generated\ndata, opens one possible path towards limiting the role of operators in data\ncenters. In this paper, we present a data-science study of a public Google\ndataset collected in a 12K-node cluster with the goal of building and\nevaluating a predictive model for node failures. We use BigQuery, the big data\nSQL platform from the Google Cloud suite, to process massive amounts of data\nand generate a rich feature set characterizing machine state over time. We\ndescribe how an ensemble classifier can be built out of many Random Forest\nclassifiers each trained on these features, to predict if machines will fail in\na future 24-hour window. Our evaluation reveals that if we limit false positive\nrates to 5%, we can achieve true positive rates between 27% and 88% with\nprecision varying between 50% and 72%. We discuss the practicality of including\nour predictive model as the central component of a data-driven autonomic\nmanager and operating it on-line with live data streams (rather than off-line\non data logs). All of the scripts used for BigQuery and classification analyses\nare publicly available from the authors' website.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 09:58:05 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 13:45:52 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["S\u00eerbu", "Alina", ""], ["Babaoglu", "Ozalp", ""]]}, {"id": "1505.04972", "submitter": "Arthur Ryman", "authors": "Arthur Ryman", "title": "Recursion in RDF Data Shape Languages", "comments": "31 pages, 2 figures, invited expert contribution to the W3C RDF Data\n  Shapes Working Group", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An RDF data shape is a description of the expected contents of an RDF\ndocument (aka graph) or dataset. A major part of this description is the set of\nconstraints that the document or dataset is required to satisfy. W3C recently\n(2014) chartered the RDF Data Shapes Working Group to define SHACL, a standard\nRDF data shape language. We refer to the ability to name and reference shape\nlanguage elements as recursion. This article provides a precise definition of\nthe meaning of recursion as used in Resource Shape 2.0. The definition of\nrecursion presented in this article is largely independent of language-specific\ndetails. We speculate that it also applies to ShEx and to all three of the\ncurrent proposals for SHACL. In particular, recursion is not permitted in the\nSHACL-SPARQL proposal, but we conjecture that recursion could be added by using\nthe definition proposed here as a top-level control structure.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 12:45:59 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 22:27:03 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Ryman", "Arthur", ""]]}, {"id": "1505.04981", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Sandro Sozzo and Tomas Veloz", "title": "A New Fundamental Evidence of Non-Classical Structure in the Combination\n  of Natural Concepts", "comments": "14 pages. arXiv admin note: substantial text overlap with\n  arXiv:1503.04260", "journal-ref": "Philosophical Trasactions of the Royal Society A. 374, 20150095,\n  (2015)", "doi": "10.1098/rsta.2015.0095", "report-no": null, "categories": "cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recently performed cognitive experiments on conjunctions and negations of\ntwo concepts with the aim of investigating the combination problem of concepts.\nOur experiments confirmed the deviations (conceptual vagueness, underextension,\noverextension, etc.) from the rules of classical (fuzzy) logic and probability\ntheory observed by several scholars in concept theory, while our data were\nsuccessfully modeled in a quantum-theoretic framework developed by ourselves.\nIn this paper, we isolate a new, very stable and systematic pattern of\nviolation of classicality that occurs in concept combinations. In addition, the\nstrength and regularity of this non-classical effect leads us to believe that\nit occurs at a more fundamental level than the deviations observed up to now.\nIt is our opinion that we have identified a deep non-classical mechanism\ndetermining not only how concepts are combined but, rather, how they are\nformed. We show that this effect can be faithfully modeled in a two-sector Fock\nspace structure, and that it can be exactly explained by assuming that human\nthought is the supersposition of two processes, a 'logical reasoning', guided\nby 'logic', and a 'conceptual reasoning' guided by 'emergence', and that the\nlatter generally prevails over the former. All these findings provide a new\nfundamental support to our quantum-theoretic approach to human cognition.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 13:02:21 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 21:47:59 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Aerts", "Diederik", ""], ["Sozzo", "Sandro", ""], ["Veloz", "Tomas", ""]]}, {"id": "1505.05004", "submitter": "Maxime Gasse", "authors": "Maxime Gasse (DM2L), Alex Aussem (DM2L), Haytham Elghazel (DM2L)", "title": "An Experimental Comparison of Hybrid Algorithms for Bayesian Network\n  Structure Learning", "comments": "arXiv admin note: text overlap with arXiv:1101.5184 by other authors.\n  Lecture notes in computer science, springer, 2012, Machine Learning and\n  Knowledge Discovery in Databases, 7523, pp.58-73", "journal-ref": null, "doi": "10.1007/978-3-642-33460-3_9", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hybrid algorithm for Bayesian network structure learning,\ncalled Hybrid HPC (H2PC). It first reconstructs the skeleton of a Bayesian\nnetwork and then performs a Bayesian-scoring greedy hill-climbing search to\norient the edges. It is based on a subroutine called HPC, that combines ideas\nfrom incremental and divide-and-conquer constraint-based methods to learn the\nparents and children of a target variable. We conduct an experimental\ncomparison of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the\nmost powerful state-of-the-art algorithm for Bayesian network structure\nlearning, on several benchmarks with various data sizes. Our extensive\nexperiments show that H2PC outperforms MMHC both in terms of goodness of fit to\nnew data and in terms of the quality of the network structure itself, which is\ncloser to the true dependence structure of the data. The source code (in R) of\nH2PC as well as all data sets used for the empirical tests are publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 14:15:10 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 10:19:46 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Gasse", "Maxime", "", "DM2L"], ["Aussem", "Alex", "", "DM2L"], ["Elghazel", "Haytham", "", "DM2L"]]}, {"id": "1505.05022", "submitter": "Daniela Inclezan", "authors": "Daniela Inclezan and Michael Gelfond", "title": "Modular Action Language ALM", "comments": "65 pages, 7 figures. To appear in Theory and Practice of Logic\n  Programming (TPLP)", "journal-ref": "Theory and Practice of Logic Programming 16 (2016) 189-235", "doi": "10.1017/S1471068415000095", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a new modular action language, ALM, and illustrates the\nmethodology of its use. It is based on the approach of Gelfond and Lifschitz\n(1993; 1998) in which a high-level action language is used as a front end for a\nlogic programming system description. The resulting logic programming\nrepresentation is used to perform various computational tasks. The methodology\nbased on existing action languages works well for small and even medium size\nsystems, but is not meant to deal with larger systems that require structuring\nof knowledge. ALM is meant to remedy this problem. Structuring of knowledge in\nALM is supported by the concepts of module (a formal description of a specific\npiece of knowledge packaged as a unit), module hierarchy, and library, and by\nthe division of a system description of ALM into two parts: theory and\nstructure. A theory consists of one or more modules with a common theme,\npossibly organized into a module hierarchy based on a dependency relation. It\ncontains declarations of sorts, attributes, and properties of the domain\ntogether with axioms describing them. Structures are used to describe the\ndomain's objects. These features, together with the means for defining classes\nof a domain as special cases of previously defined ones, facilitate the\nstepwise development, testing, and readability of a knowledge base, as well as\nthe creation of knowledge representation libraries. To appear in Theory and\nPractice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 14:44:25 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2015 12:11:09 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Inclezan", "Daniela", ""], ["Gelfond", "Michael", ""]]}, {"id": "1505.05063", "submitter": "Conrado Miranda", "authors": "Conrado Silva Miranda, Fernando Jos\\'e Von Zuben", "title": "Necessary and Sufficient Conditions for Surrogate Functions of Pareto\n  Frontiers and Their Synthesis Using Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the necessary and sufficient conditions that surrogate\nfunctions must satisfy to properly define frontiers of non-dominated solutions\nin multi-objective optimization problems. These new conditions work directly on\nthe objective space, thus being agnostic about how the solutions are evaluated.\nTherefore, real objectives or user-designed objectives' surrogates are allowed,\nopening the possibility of linking independent objective surrogates. To\nillustrate the practical consequences of adopting the proposed conditions, we\nuse Gaussian processes as surrogates endowed with monotonicity soft constraints\nand with an adjustable degree of flexibility, and compare them to regular\nGaussian processes and to a frontier surrogate method in the literature that is\nthe closest to the method proposed in this paper. Results show that the\nnecessary and sufficient conditions proposed here are finely managed by the\nconstrained Gaussian process, guiding to high-quality surrogates capable of\nsuitably synthesizing an approximation to the Pareto frontier in challenging\ninstances of multi-objective optimization, while an existing approach that does\nnot take the theory proposed in consideration defines surrogates which greatly\nviolate the conditions to describe a valid frontier.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 16:09:23 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 22:45:29 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2015 06:01:11 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Miranda", "Conrado Silva", ""], ["Von Zuben", "Fernando Jos\u00e9", ""]]}, {"id": "1505.05190", "submitter": "Hiroharu Kato", "authors": "Hiroharu Kato and Tatsuya Harada", "title": "Image Reconstruction from Bag-of-Visual-Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to reconstruct an original image from\nBag-of-Visual-Words (BoVW). Image reconstruction from features can be a means\nof identifying the characteristics of features. Additionally, it enables us to\ngenerate novel images via features. Although BoVW is the de facto standard\nfeature for image recognition and retrieval, successful image reconstruction\nfrom BoVW has not been reported yet. What complicates this task is that BoVW\nlacks the spatial information for including visual words. As described in this\npaper, to estimate an original arrangement, we propose an evaluation function\nthat incorporates the naturalness of local adjacency and the global position,\nwith a method to obtain related parameters using an external image database. To\nevaluate the performance of our method, we reconstruct images of objects of 101\nkinds. Additionally, we apply our method to analyze object classifiers and to\ngenerate novel images via BoVW.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 21:12:15 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Kato", "Hiroharu", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1505.05312", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "A New Oscillating-Error Technique for Classifiers", "comments": null, "journal-ref": "Cogent Engineering, 4:1, 2017", "doi": "10.1080/23311916.2017.1293480", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new method for reducing the error in a classifier. It\nuses an error correction update that includes the very simple rule of either\nadding or subtracting the error adjustment, based on whether the variable value\nis currently larger or smaller than the desired value. While a traditional\nneuron would sum the inputs together and then apply a function to the total,\nthis new method can change the function decision for each input value. This\ngives added flexibility to the convergence procedure, where through a series of\ntranspositions, variables that are far away can continue towards the desired\nvalue, whereas variables that are originally much closer can oscillate from one\nside to the other. Tests show that the method can successfully classify some\nbenchmark datasets. It can also work in a batch mode, with reduced training\ntimes and can be used as part of a neural network architecture. Some\ncomparisons with an earlier wave shape paper are also made.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 10:43:21 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 18:50:41 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2015 15:54:16 GMT"}, {"version": "v4", "created": "Thu, 7 Apr 2016 16:51:57 GMT"}, {"version": "v5", "created": "Mon, 31 Oct 2016 14:42:06 GMT"}, {"version": "v6", "created": "Sat, 4 Feb 2017 19:34:18 GMT"}, {"version": "v7", "created": "Tue, 10 Oct 2017 07:47:39 GMT"}, {"version": "v8", "created": "Tue, 21 Nov 2017 09:04:59 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1505.05364", "submitter": "AlexanderArtikis", "authors": "Alexander Artikis and Marek Sergot and Georgios Paliouras", "title": "Reactive Reasoning with the Event Calculus", "comments": "International Workshop on Reactive Concepts in Knowledge\n  Representation (ReactKnow 2014), co-located with the 21st European Conference\n  on Artificial Intelligence (ECAI 2014). Proceedings of the International\n  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),\n  pages 9-15, technical report, ISSN 1430-3701, Leipzig University, 2014.\n  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562. 2014,1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems for symbolic event recognition accept as input a stream of\ntime-stamped events from sensors and other computational devices, and seek to\nidentify high-level composite events, collections of events that satisfy some\npattern. RTEC is an Event Calculus dialect with novel implementation and\n'windowing' techniques that allow for efficient event recognition, scalable to\nlarge data streams. RTEC can deal with applications where event data arrive\nwith a (variable) delay from, and are revised by, the underlying sources. RTEC\ncan update already recognised events and recognise new events when data arrive\nwith a delay or following data revision. Our evaluation shows that RTEC can\nsupport real-time event recognition and is capable of meeting the performance\nrequirements identified in a recent survey of event processing use cases.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 13:26:36 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Artikis", "Alexander", ""], ["Sergot", "Marek", ""], ["Paliouras", "Georgios", ""]]}, {"id": "1505.05365", "submitter": "Harald Beck", "authors": "Harald Beck and Minh Dao-Tran and Thomas Eiter and Michael Fink", "title": "Towards Ideal Semantics for Analyzing Stream Reasoning", "comments": "International Workshop on Reactive Concepts in Knowledge\n  Representation (ReactKnow 2014), co-located with the 21st European Conference\n  on Artificial Intelligence (ECAI 2014). Proceedings of the International\n  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),\n  pages 17-22, technical report, ISSN 1430-3701, Leipzig University, 2014.\n  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562 2014,1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of smart applications has drawn interest to logical reasoning over\ndata streams. Recently, different query languages and stream\nprocessing/reasoning engines were proposed in different communities. However,\ndue to a lack of theoretical foundations, the expressivity and semantics of\nthese diverse approaches are given only informally. Towards clear\nspecifications and means for analytic study, a formal framework is needed to\ndefine their semantics in precise terms. To this end, we present a first step\ntowards an ideal semantics that allows for exact descriptions and comparisons\nof stream reasoning systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 13:27:23 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Beck", "Harald", ""], ["Dao-Tran", "Minh", ""], ["Eiter", "Thomas", ""], ["Fink", "Michael", ""]]}, {"id": "1505.05366", "submitter": "Joerg Puehrer", "authors": "Gerhard Brewka and Stefan Ellmauthaler and J\\\"org P\\\"uhrer", "title": "Multi-Context Systems for Reactive Reasoning in Dynamic Environments", "comments": "International Workshop on Reactive Concepts in Knowledge\n  Representation (ReactKnow 2014), co-located with the 21st European Conference\n  on Artificial Intelligence (ECAI 2014). Proceedings of the International\n  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),\n  pages 23-29, technical report, ISSN 1430-3701, Leipzig University, 2014.\n  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show in this paper how managed multi-context systems (mMCSs) can be turned\ninto a reactive formalism suitable for continuous reasoning in dynamic\nenvironments. We extend mMCSs with (abstract) sensors and define the notion of\na run of the extended systems. We then show how typical problems arising in\nonline reasoning can be addressed: handling potentially inconsistent sensor\ninput, modeling intelligent forms of forgetting, selective integration of\nknowledge, and controlling the reasoning effort spent by contexts, like setting\ncontexts to an idle mode. We also investigate the complexity of some important\nrelated decision problems and discuss different design choices which are given\nto the knowledge engineer.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 13:28:11 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Brewka", "Gerhard", ""], ["Ellmauthaler", "Stefan", ""], ["P\u00fchrer", "J\u00f6rg", ""]]}, {"id": "1505.05367", "submitter": "Stefan Ellmauthaler", "authors": "Stefan Ellmauthaler and J\\\"org P\\\"uhrer", "title": "Asynchronous Multi-Context Systems", "comments": "International Workshop on Reactive Concepts in Knowledge\n  Representation (ReactKnow 2014), co-located with the 21st European Conference\n  on Artificial Intelligence (ECAI 2014). Proceedings of the International\n  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),\n  pages 31-37, technical report, ISSN 1430-3701, Leipzig University, 2014.\n  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present asynchronous multi-context systems (aMCSs), which\nprovide a framework for loosely coupling different knowledge representation\nformalisms that allows for online reasoning in a dynamic environment. Systems\nof this kind may interact with the outside world via input and output streams\nand may therefore react to a continuous flow of external information. In\ncontrast to recent proposals, contexts in an aMCS communicate with each other\nin an asynchronous way which fits the needs of many application domains and is\nbeneficial for scalability. The federal semantics of aMCSs renders our\nframework an integration approach rather than a knowledge representation\nformalism itself. We illustrate the introduced concepts by means of an example\nscenario dealing with rescue services. In addition, we compare aMCSs to\nreactive multi-context systems and describe how to simulate the latter with our\nnovel approach.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 13:29:45 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Ellmauthaler", "Stefan", ""], ["P\u00fchrer", "J\u00f6rg", ""]]}, {"id": "1505.05368", "submitter": "Matthias Knorr", "authors": "Ricardo Gon\\c{c}alves and Matthias Knorr and Jo\\~ao Leite", "title": "On Minimal Change in Evolving Multi-Context Systems (Preliminary Report)", "comments": "International Workshop on Reactive Concepts in Knowledge\n  Representation (ReactKnow 2014), co-located with the 21st European Conference\n  on Artificial Intelligence (ECAI 2014). Proceedings of the International\n  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),\n  pages 47-53, technical report, ISSN 1430-3701, Leipzig University, 2014.\n  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managed Multi-Context Systems (mMCSs) provide a general framework for\nintegrating knowledge represented in heterogeneous KR formalisms. However,\nmMCSs are essentially static as they were not designed to run in a dynamic\nscenario. Some recent approaches, among them evolving Multi-Context Systems\n(eMCSs), extend mMCSs by allowing not only the ability to integrate knowledge\nrepresented in heterogeneous KR formalisms, but at the same time to both react\nto, and reason in the presence of commonly temporary dynamic observations, and\nevolve by incorporating new knowledge. The notion of minimal change is a\ncentral notion in dynamic scenarios, specially in those that admit several\npossible alternative evolutions. Since eMCSs combine heterogeneous KR\nformalisms, each of which may require different notions of minimal change, the\nstudy of minimal change in eMCSs is an interesting and highly non-trivial\nproblem. In this paper, we study the notion of minimal change in eMCSs, and\ndiscuss some alternative minimal change criteria.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 13:30:19 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Gon\u00e7alves", "Ricardo", ""], ["Knorr", "Matthias", ""], ["Leite", "Jo\u00e3o", ""]]}, {"id": "1505.05373", "submitter": "J\\\"org P\\\"uhrer", "authors": "J\\\"org P\\\"uhrer", "title": "Towards a Simulation-Based Programming Paradigm for AI applications", "comments": "International Workshop on Reactive Concepts in Knowledge\n  Representation (ReactKnow 2014), co-located with the 21st European Conference\n  on Artificial Intelligence (ECAI 2014). Proceedings of the International\n  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),\n  pages 55-61, technical report, ISSN 1430-3701, Leipzig University, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present initial ideas for a programming paradigm based on simulation that\nis targeted towards applications of artificial intelligence (AI). The approach\naims at integrating techniques from different areas of AI and is based on the\nidea that simulated entities may freely exchange data and behavioural patterns.\nWe define basic notions of a simulation-based programming paradigm and show how\nit can be used for implementing AI applications.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 13:34:34 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["P\u00fchrer", "J\u00f6rg", ""]]}, {"id": "1505.05375", "submitter": "Matthias Thimm", "authors": "Matthias Thimm", "title": "Towards Large-scale Inconsistency Measurement", "comments": "International Workshop on Reactive Concepts in Knowledge\n  Representation (ReactKnow 2014), co-located with the 21st European Conference\n  on Artificial Intelligence (ECAI 2014). Proceedings of the International\n  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),\n  pages 63-70, technical report, ISSN 1430-3701, Leipzig University, 2014.\n  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of inconsistency measurement on large knowledge\nbases by considering stream-based inconsistency measurement, i.e., we\ninvestigate inconsistency measures that cannot consider a knowledge base as a\nwhole but process it within a stream. For that, we present, first, a novel\ninconsistency measure that is apt to be applied to the streaming case and,\nsecond, stream-based approximations for the new and some existing inconsistency\nmeasures. We conduct an extensive empirical analysis on the behavior of these\ninconsistency measures on large knowledge bases, in terms of runtime, accuracy,\nand scalability. We conclude that for two of these measures, the approximation\nof the new inconsistency measure and an approximation of the contension\ninconsistency measure, large-scale inconsistency measurement is feasible.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 13:35:09 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Thimm", "Matthias", ""]]}, {"id": "1505.05451", "submitter": "Nasser Ghadiri", "authors": "Javad Salimi Sartakhti, Homayun Afrabandpey, Nasser Ghadiri", "title": "Fuzzy Least Squares Twin Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least Squares Twin Support Vector Machine (LST-SVM) has been shown to be an\nefficient and fast algorithm for binary classification. It combines the\noperating principles of Least Squares SVM (LS-SVM) and Twin SVM (T-SVM); it\nconstructs two non-parallel hyperplanes (as in T-SVM) by solving two systems of\nlinear equations (as in LS-SVM). Despite its efficiency, LST-SVM is still\nunable to cope with two features of real-world problems. First, in many\nreal-world applications, labels of samples are not deterministic; they come\nnaturally with their associated membership degrees. Second, samples in\nreal-world applications may not be equally important and their importance\ndegrees affect the classification. In this paper, we propose Fuzzy LST-SVM\n(FLST-SVM) to deal with these two characteristics of real-world data. Two\nmodels are introduced for FLST-SVM: the first model builds up crisp hyperplanes\nusing training samples and their corresponding membership degrees. The second\nmodel, on the other hand, constructs fuzzy hyperplanes using training samples\nand their membership degrees. Numerical evaluation of the proposed method with\nsynthetic and real datasets demonstrate significant improvement in the\nclassification accuracy of FLST-SVM when compared to well-known existing\nversions of SVM.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 16:57:02 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 08:59:40 GMT"}, {"version": "v3", "created": "Wed, 21 Nov 2018 21:05:16 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Sartakhti", "Javad Salimi", ""], ["Afrabandpey", "Homayun", ""], ["Ghadiri", "Nasser", ""]]}, {"id": "1505.05502", "submitter": "Ricardo Gon\\c{c}alves", "authors": "Ricardo Gon\\c{c}alves and Matthias Knorr and Jo\\~ao Leite", "title": "Towards Efficient Evolving Multi-Context Systems (Preliminary Report)", "comments": "International Workshop on Reactive Concepts in Knowledge\n  Representation (ReactKnow 2014), co-located with the 21st European Conference\n  on Artificial Intelligence (ECAI 2014). Proceedings of the International\n  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),\n  pages 39-45, technical report, ISSN 1430-3701, Leipzig University, 2014.\n  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562 . arXiv admin note:\n  substantial text overlap with arXiv:1505.05368", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managed Multi-Context Systems (mMCSs) provide a general framework for\nintegrating knowledge represented in heterogeneous KR formalisms. Recently,\nevolving Multi-Context Systems (eMCSs) have been introduced as an extension of\nmMCSs that add the ability to both react to, and reason in the presence of\ncommonly temporary dynamic observations, and evolve by incorporating new\nknowledge. However, the general complexity of such an expressive formalism may\nsimply be too high in cases where huge amounts of information have to be\nprocessed within a limited short amount of time, or even instantaneously. In\nthis paper, we investigate under which conditions eMCSs may scale in such\nsituations and we show that such polynomial eMCSs can be applied in a practical\nuse case.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 13:33:52 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Gon\u00e7alves", "Ricardo", ""], ["Knorr", "Matthias", ""], ["Leite", "Jo\u00e3o", ""]]}, {"id": "1505.05613", "submitter": "Chris De Vries", "authors": "Christopher M. de Vries, Lance De Vine, Shlomo Geva, Richi Nayak", "title": "Parallel Streaming Signature EM-tree: A Clustering Algorithm for Web\n  Scale Applications", "comments": "11 pages, WWW 2015", "journal-ref": null, "doi": "10.1145/2736277.2741111", "report-no": null, "categories": "cs.IR cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of the web presents an unsolved problem of automatically\nanalyzing billions of pages of natural language. We introduce a scalable\nalgorithm that clusters hundreds of millions of web pages into hundreds of\nthousands of clusters. It does this on a single mid-range machine using\nefficient algorithms and compressed document representations. It is applied to\ntwo web-scale crawls covering tens of terabytes. ClueWeb09 and ClueWeb12\ncontain 500 and 733 million web pages and were clustered into 500,000 to\n700,000 clusters. To the best of our knowledge, such fine grained clustering\nhas not been previously demonstrated. Previous approaches clustered a sample\nthat limits the maximum number of discoverable clusters. The proposed EM-tree\nalgorithm uses the entire collection in clustering and produces several orders\nof magnitude more clusters than the existing algorithms. Fine grained\nclustering is necessary for meaningful clustering in massive collections where\nthe number of distinct topics grows linearly with collection size. These\nfine-grained clusters show an improved cluster quality when assessed with two\nnovel evaluations using ad hoc search relevance judgments and spam\nclassifications for external validation. These evaluations solve the problem of\nassessing the quality of clusters where categorical labeling is unavailable and\nunfeasible.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 06:22:04 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["de Vries", "Christopher M.", ""], ["De Vine", "Lance", ""], ["Geva", "Shlomo", ""], ["Nayak", "Richi", ""]]}, {"id": "1505.05723", "submitter": "Indre Zliobaite", "authors": "Indre Zliobaite", "title": "On the relation between accuracy and fairness in binary classification", "comments": "Accepted for presentation to the 2nd workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (http://www.fatml.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our study revisits the problem of accuracy-fairness tradeoff in binary\nclassification. We argue that comparison of non-discriminatory classifiers\nneeds to account for different rates of positive predictions, otherwise\nconclusions about performance may be misleading, because accuracy and\ndiscrimination of naive baselines on the same dataset vary with different rates\nof positive predictions. We provide methodological recommendations for sound\ncomparison of non-discriminatory classifiers, and present a brief theoretical\nand empirical analysis of tradeoffs between accuracy and non-discrimination.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 13:20:06 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Zliobaite", "Indre", ""]]}, {"id": "1505.05770", "submitter": "Danilo Jimenez Rezende", "authors": "Danilo Jimenez Rezende and Shakir Mohamed", "title": "Variational Inference with Normalizing Flows", "comments": "Proceedings of the 32nd International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of approximate posterior distribution is one of the core problems\nin variational inference. Most applications of variational inference employ\nsimple families of posterior approximations in order to allow for efficient\ninference, focusing on mean-field or other simple structured approximations.\nThis restriction has a significant impact on the quality of inferences made\nusing variational methods. We introduce a new approach for specifying flexible,\narbitrarily complex and scalable approximate posterior distributions. Our\napproximations are distributions constructed through a normalizing flow,\nwhereby a simple initial density is transformed into a more complex one by\napplying a sequence of invertible transformations until a desired level of\ncomplexity is attained. We use this view of normalizing flows to develop\ncategories of finite and infinitesimal flows and provide a unified view of\napproaches for constructing rich posterior approximations. We demonstrate that\nthe theoretical advantages of having posteriors that better match the true\nposterior, combined with the scalability of amortized variational approaches,\nprovides a clear improvement in performance and applicability of variational\ninference.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 15:36:37 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 09:13:28 GMT"}, {"version": "v3", "created": "Tue, 26 May 2015 15:46:33 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2015 18:36:32 GMT"}, {"version": "v5", "created": "Mon, 13 Jun 2016 08:46:44 GMT"}, {"version": "v6", "created": "Tue, 14 Jun 2016 09:01:36 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1505.05947", "submitter": "Alexander Lavin", "authors": "Alexander Lavin", "title": "A Pareto Front-Based Multiobjective Path Planning Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path planning is one of the most vital elements of mobile robotics. With a\npriori knowledge of the environment, global path planning provides a\ncollision-free route through the workspace. The global path plan can be\ncalculated with a variety of informed search algorithms, most notably the A*\nsearch method, guaranteed to deliver a complete and optimal solution that\nminimizes the path cost. Path planning optimization typically looks to minimize\nthe distance traversed from start to goal, yet many mobile robot applications\ncall for additional path planning objectives, presenting a multiobjective\noptimization (MOO) problem. Past studies have applied genetic algorithms to MOO\npath planning problems, but these may have the disadvantages of computational\ncomplexity and suboptimal solutions. Alternatively, the algorithm in this paper\napproaches MOO path planning with the use of Pareto fronts, or finding\nnon-dominated solutions. The algorithm presented incorporates Pareto optimality\ninto every step of A* search, thus it is named A*-PO. Results of simulations\nshow A*-PO outperformed several variations of the standard A* algorithm for MOO\npath planning. A planetary exploration rover case study was added to\ndemonstrate the viability of A*-PO in a real-world application.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 04:35:12 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Lavin", "Alexander", ""]]}, {"id": "1505.06072", "submitter": "Pedro Felzenszwalb", "authors": "Pedro F. Felzenszwalb, Benar F. Svaiter", "title": "Diffusion Methods for Classification with Pairwise Relationships", "comments": "To appear in the Quarterly of Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define two algorithms for propagating information in classification\nproblems with pairwise relationships. The algorithms are based on contraction\nmaps and are related to non-linear diffusion and random walks on graphs. The\napproach is also related to message passing algorithms, including belief\npropagation and mean field methods. The algorithms we describe are guaranteed\nto converge on graphs with arbitrary topology. Moreover they always converge to\na unique fixed point, independent of initialization. We prove that the fixed\npoints of the algorithms under consideration define lower-bounds on the energy\nfunction and the max-marginals of a Markov random field. The theoretical\nresults also illustrate a relationship between message passing algorithms and\nvalue iteration for an infinite horizon Markov decision process. We illustrate\nthe practical application of the algorithms under study with numerical\nexperiments in image restoration, stereo depth estimation and binary\nclassification on a grid.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 13:36:58 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 14:37:37 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2015 14:10:08 GMT"}, {"version": "v4", "created": "Tue, 14 May 2019 18:10:26 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Felzenszwalb", "Pedro F.", ""], ["Svaiter", "Benar F.", ""]]}, {"id": "1505.06294", "submitter": "Dimitri Kartsaklis", "authors": "Dimitri Kartsaklis and Mehrnoosh Sadrzadeh", "title": "A Frobenius Model of Information Structure in Categorical Compositional\n  Distributional Semantics", "comments": "Accepted for presentation in the 14th Meeting on Mathematics of\n  Language (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI math.CT math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The categorical compositional distributional model of Coecke, Sadrzadeh and\nClark provides a linguistically motivated procedure for computing the meaning\nof a sentence as a function of the distributional meaning of the words therein.\nThe theoretical framework allows for reasoning about compositional aspects of\nlanguage and offers structural ways of studying the underlying relationships.\nWhile the model so far has been applied on the level of syntactic structures, a\nsentence can bring extra information conveyed in utterances via intonational\nmeans. In the current paper we extend the framework in order to accommodate\nthis additional information, using Frobenius algebraic structures canonically\ninduced over the basis of finite-dimensional vector spaces. We detail the\ntheory, provide truth-theoretic and distributional semantics for meanings of\nintonationally-marked utterances, and present justifications and extensive\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 09:08:30 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Kartsaklis", "Dimitri", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "1505.06366", "submitter": "Viktoras Veitas Mr.", "authors": "David Weinbaum (Weaver) and Viktoras Veitas", "title": "Open Ended Intelligence: The individuation of Intelligent Agents", "comments": "Preprint; 35 pages, 2 figures; Keywords: intelligence, cognition,\n  individuation, assemblage, self-organization, sense-making, coordination,\n  enaction; en-US proofreading", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial General Intelligence is a field of research aiming to distill the\nprinciples of intelligence that operate independently of a specific problem\ndomain or a predefined context and utilize these principles in order to\nsynthesize systems capable of performing any intellectual task a human being is\ncapable of and eventually go beyond that. While \"narrow\" artificial\nintelligence which focuses on solving specific problems such as speech\nrecognition, text comprehension, visual pattern recognition, robotic motion,\netc. has shown quite a few impressive breakthroughs lately, understanding\ngeneral intelligence remains elusive. In the paper we offer a novel theoretical\napproach to understanding general intelligence. We start with a brief\nintroduction of the current conceptual approach. Our critique exposes a number\nof serious limitations that are traced back to the ontological roots of the\nconcept of intelligence. We then propose a paradigm shift from intelligence\nperceived as a competence of individual agents defined in relation to an a\npriori given problem domain or a goal, to intelligence perceived as a formative\nprocess of self-organization by which intelligent agents are individuated. We\ncall this process open-ended intelligence. Open-ended intelligence is developed\nas an abstraction of the process of cognitive development so its application\ncan be extended to general agents and systems. We introduce and discuss three\nfacets of the idea: the philosophical concept of individuation, sense-making\nand the individuation of general cognitive agents. We further show how\nopen-ended intelligence can be framed in terms of a distributed,\nself-organizing network of interacting elements and how such process is\nscalable. The framework highlights an important relation between coordination\nand intelligence and a new understanding of values. We conclude with a number\nof questions for future research.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 19:32:54 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 14:57:23 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Weinbaum", "David", "", "Weaver"], ["Veitas", "Viktoras", ""]]}, {"id": "1505.06537", "submitter": "Manish Joshi", "authors": "Manish R. Joshi and Varsha M. Pathak", "title": "A survey of SMS based Information Systems", "comments": "17 pages 3 Figures 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short Message Service (SMS) based Information Systems (SMSbIS) provide an\nexcellent alternative to a traditional approach of obtaining specific\ninformation by direct (through phone) or indirect (IVRS, Web, Email) probing.\nInformation and communication technology and far reaching mobile penetration\nhas opened this new research trend Number of key players in Search industry\nincluding Microsoft and Google are attracted by the expected increase in volume\nof use of such applications. The wide range of applications and their public\nacceptance has motivated researchers to work in this research domain. Several\napplications such as SMS based information access using database management\nservices, SMS based information retrieval through internet (search engine), SMS\nbased information extraction, question answering, image retrieval etc. have\nbeen emerged. With the aim to understand the functionality involved in these\nsystems, an extensive review of a few of these SMSbISs has been planned and\nexecuted by us. These systems are classified into four categories based on the\nobjectives and domains of the applications. As a result of this study a well\nstructured functional model is presented here. The model is evaluated in\ndifferent dimensions, which is presented in this paper. In addition to this a\nchronological progress with respect to research and development in this\nupcoming field is compiled in this paper. Such an extensive review presented in\nthis paper would definitely help the researchers and developers to understand\nthe technical aspects of this field. The functional framework presented here\nwould be useful to the system designers to design and develop an SMS based\nInformation System of any specific domain.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 10:08:19 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Joshi", "Manish R.", ""], ["Pathak", "Varsha M.", ""]]}, {"id": "1505.06573", "submitter": "Andrzej Grzybowski", "authors": "Andrzej Z. Grzybowski", "title": "New results on inconsistency indices and their relationship with the\n  quality of priority vector estimation", "comments": "26 pages, 2 figures, 19 tables", "journal-ref": "Expert Systems With Applications 43 (2016) 197- 212", "doi": "10.1016/j.eswa.2015.08.049", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article is devoted to the problem of inconsistency in the pairwise\ncomparisons based prioritization methodology. The issue of \"inconsistency\" in\nthis context has gained much attention in recent years. The literature provides\nus with a number of different \"inconsistency\" indices suggested for measuring\nthe inconsistency of the pairwise comparison matrix (PCM). The latter is\nunderstood as a deviation of the PCM from the \"consistent case\" - a notion that\nis formally well-defined in this theory. However the usage of the indices is\njustified only by some heuristics. It is still unclear what they really\n\"measure\". What is even more important and still not known is the relationship\nbetween their values and the \"consistency\" of the decision maker's judgments on\none hand, and the prioritization results upon the other. We provide examples\nshowing that it is necessary to distinguish between these three following\ntasks: the \"measuring\" of the \"PCM inconsistency\" and the PCM-based \"measuring\"\nof the consistency of decision maker's judgments and, finally, the \"measuring\"\nof the usefulness of the PCM as a source of information for estimation of the\npriority vector (PV). Next we focus on the third task, which seems to be the\nmost important one in Multi-Criteria Decision Making. With the help of Monte\nCarlo experiments, we study the performance of various inconsistency indices as\nindicators of the final PV estimation quality. The presented results allow a\ndeeper understanding of the information contained in these indices and help in\nchoosing a proper one in a given situation. They also enable us to develop a\nnew inconsistency characteristic and, based on it, to propose the PCM\nacceptance approach that is supported by the classical statistical methodology.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 09:20:45 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 09:42:40 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2015 15:24:51 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Grzybowski", "Andrzej Z.", ""]]}, {"id": "1505.06651", "submitter": "Yanjing Wang", "authors": "Yanjing Wang", "title": "A Logic of Knowing How", "comments": "14 pages, a 12-page version accepted by LORI V", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a single-agent modal logic framework for reasoning\nabout goal-direct \"knowing how\" based on ideas from linguistics, philosophy,\nmodal logic and automated planning. We first define a modal language to express\n\"I know how to guarantee phi given psi\" with a semantics not based on standard\nepistemic models but labelled transition systems that represent the agent's\nknowledge of his own abilities. A sound and complete proof system is given to\ncapture the valid reasoning patterns about \"knowing how\" where the most\nimportant axiom suggests its compositional nature.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 14:43:55 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 08:36:47 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2015 09:35:29 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Wang", "Yanjing", ""]]}, {"id": "1505.06850", "submitter": "Joseph Corneli", "authors": "Joseph Corneli and Anna Jordanous", "title": "Implementing feedback in creative systems: A workshop approach", "comments": "8 pp., submitted to IJCAI 2015 Workshop 42, \"AI and Feedback\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  One particular challenge in AI is the computational modelling and simulation\nof creativity. Feedback and learning from experience are key aspects of the\ncreative process. Here we investigate how we could implement feedback in\ncreative systems using a social model. From the field of creative writing we\nborrow the concept of a Writers Workshop as a model for learning through\nfeedback. The Writers Workshop encourages examination, discussion and debates\nof a piece of creative work using a prescribed format of activities. We propose\na computational model of the Writers Workshop as a roadmap for incorporation of\nfeedback in artificial creativity systems. We argue that the Writers Workshop\nsetting describes the anatomy of the creative process. We support our claim\nwith a case study that describes how to implement the Writers Workshop model in\na computational creativity system. We present this work using patterns other\npeople can follow to implement similar designs in their own systems. We\nconclude by discussing the broader relevance of this model to other aspects of\nAI.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 08:38:57 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Corneli", "Joseph", ""], ["Jordanous", "Anna", ""]]}, {"id": "1505.07096", "submitter": "Pietro Michelucci", "authors": "Pietro Michelucci, Lea Shanley, Janis Dickinson, Haym Hirsh", "title": "A U.S. Research Roadmap for Human Computation", "comments": "32 pages, 25 figures, Workshop report from the CRA-sponsored Human\n  Computation Roadmap Summit: P. Michelucci, L. Shanley, J. Dickinson, and H.\n  Hirsh, A U.S. Research Roadmap for Human Computation, Computing Community\n  Consortium Technical Report, 2015", "journal-ref": null, "doi": "10.13140/RG.2.1.4517.2648", "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web has made it possible to harness human cognition en masse to achieve\nnew capabilities. Some of these successes are well known; for example Wikipedia\nhas become the go-to place for basic information on all things; Duolingo\nengages millions of people in real-life translation of text, while\nsimultaneously teaching them to speak foreign languages; and fold.it has\nenabled public-driven scientific discoveries by recasting complex biomedical\nchallenges into popular online puzzle games. These and other early successes\nhint at the tremendous potential for future crowd-powered capabilities for the\nbenefit of health, education, science, and society. In the process, a new field\ncalled Human Computation has emerged to better understand, replicate, and\nimprove upon these successes through scientific research. Human Computation\nrefers to the science that underlies online crowd-powered systems and was the\ntopic of a recent visioning activity in which a representative cross-section of\nresearchers, industry practitioners, visionaries, funding agency\nrepresentatives, and policy makers came together to understand what makes\ncrowd-powered systems successful. Teams of experts considered past, present,\nand future human computation systems to explore which kinds of crowd-powered\nsystems have the greatest potential for societal impact and which kinds of\nresearch will best enable the efficient development of new crowd-powered\nsystems to achieve this impact. This report summarize the products and findings\nof those activities as well as the unconventional process and activities\nemployed by the workshop, which were informed by human computation research.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 19:49:24 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Michelucci", "Pietro", ""], ["Shanley", "Lea", ""], ["Dickinson", "Janis", ""], ["Hirsh", "Haym", ""]]}, {"id": "1505.07263", "submitter": "Alessandro Provetti", "authors": "Luca Padovani and Alessandro Provetti", "title": "Qsmodels: ASP Planning in Interactive Gaming Environment", "comments": "Proceedings of Logics in Artificial Intelligence, 9th European\n  Conference, {JELIA} 2004, pp. 689-692. Lisbon, Portugal, September 27-30,\n  2004", "journal-ref": null, "doi": "10.1007/978-3-540-30227-8_58", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Qsmodels is a novel application of Answer Set Programming to interactive\ngaming environment. We describe a software architecture by which the behavior\nof a bot acting inside the Quake 3 Arena can be controlled by a planner. The\nplanner is written as an Answer Set Program and is interpreted by the Smodels\nsolver.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 10:58:03 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Padovani", "Luca", ""], ["Provetti", "Alessandro", ""]]}, {"id": "1505.07434", "submitter": "Qing Chuan Ye", "authors": "Qing Chuan Ye, Yingqian Zhang, Rommert Dekker", "title": "Fair task allocation in transportation", "comments": null, "journal-ref": "Ye QC, et al. Fair task allocation in transportation. Omega\n  (2016), http://dx.doi.org/10.1016/j.omega.2016.05.005", "doi": "10.1016/j.omega.2016.05.005", "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task allocation problems have traditionally focused on cost optimization.\nHowever, more and more attention is being given to cases in which cost should\nnot always be the sole or major consideration. In this paper we study a fair\ntask allocation problem in transportation where an optimal allocation not only\nhas low cost but more importantly, it distributes tasks as even as possible\namong heterogeneous participants who have different capacities and costs to\nexecute tasks. To tackle this fair minimum cost allocation problem we analyze\nand solve it in two parts using two novel polynomial-time algorithms. We show\nthat despite the new fairness criterion, the proposed algorithms can solve the\nfair minimum cost allocation problem optimally in polynomial time. In addition,\nwe conduct an extensive set of experiments to investigate the trade-off between\ncost minimization and fairness. Our experimental results demonstrate the\nbenefit of factoring fairness into task allocation. Among the majority of test\ninstances, fairness comes with a very small price in terms of cost.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 19:03:07 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 15:48:39 GMT"}, {"version": "v3", "created": "Wed, 7 Dec 2016 15:07:41 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Ye", "Qing Chuan", ""], ["Zhang", "Yingqian", ""], ["Dekker", "Rommert", ""]]}, {"id": "1505.07548", "submitter": "Andrew Smith", "authors": "Jian Lou and Andrew M. Smith and Yevgeniy Vorobeychik", "title": "Multidefender Security Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stackelberg security game models and associated computational tools have seen\ndeployment in a number of high-consequence security settings, such as LAX\ncanine patrols and Federal Air Marshal Service. These models focus on isolated\nsystems with only one defender, despite being part of a more complex system\nwith multiple players. Furthermore, many real systems such as transportation\nnetworks and the power grid exhibit interdependencies between targets and,\nconsequently, between decision makers jointly charged with protecting them. To\nunderstand such multidefender strategic interactions present in security, we\ninvestigate game theoretic models of security games with multiple defenders.\nUnlike most prior analysis, we focus on the situations in which each defender\nmust protect multiple targets, so that even a single defender's best response\ndecision is, in general, highly non-trivial. We start with an analytical\ninvestigation of multidefender security games with independent targets,\noffering an equilibrium and price-of-anarchy analysis of three models with\nincreasing generality. In all models, we find that defenders have the incentive\nto over-protect targets, at times significantly. Additionally, in the simpler\nmodels, we find that the price of anarchy is unbounded, linearly increasing\nboth in the number of defenders and the number of targets per defender.\nConsidering interdependencies among targets, we develop a novel mixed-integer\nlinear programming formulation to compute a defender's best response, and make\nuse of this formulation in approximating Nash equilibria of the game. We apply\nthis approach towards computational strategic analysis of several models of\nnetworks representing interdependencies, including real-world power networks.\nOur analysis shows how network structure and the probability of failure spread\ndetermine the propensity of defenders to over- or under-invest in security.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 04:54:53 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Lou", "Jian", ""], ["Smith", "Andrew M.", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1505.07751", "submitter": "John Sudano Ph D", "authors": "John J. Sudano", "title": "Pignistic Probability Transforms for Mixes of Low- and High-Probability\n  Events", "comments": "7 pages, International Society of Information Fusion Conference\n  Proceedings Fusion 2001 at Montreal, Quebec, Canada", "journal-ref": "Fourth International Conference on Information Fusion, August\n  2001, Montreal. Pages TPUB3 23-27", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some real world information fusion situations, time critical decisions\nmust be made with an incomplete information set. Belief function theories\n(e.g., Dempster-Shafer theory of evidence, Transferable Belief Model) have been\nshown to provide a reasonable methodology for processing or fusing the\nquantitative clues or information measurements that form the incomplete\ninformation set. For decision making, the pignistic (from the Latin pignus, a\nbet) probability transform has been shown to be a good method of using Beliefs\nor basic belief assignments (BBAs) to make decisions. For many systems, one\nneed only address the most-probable elements in the set. For some critical\nsystems, one must evaluate the risk of wrong decisions and establish safe\nprobability thresholds for decision making. This adds a greater complexity to\ndecision making, since one must address all elements in the set that are above\nthe risk decision threshold. The problem is greatly simplified if most of the\nprobabilities fall below this threshold. Finding a probability transform that\nproperly represents mixes of low- and high-probability events is essential.\nThis article introduces four new pignistic probability transforms with an\nimplementation that uses the latest values of Beliefs, Plausibilities, or BBAs\nto improve the pignistic probability estimates. Some of them assign smaller\nvalues of probabilities for smaller values of Beliefs or BBAs than the Smets\npignistic transform. They also assign higher probability values for larger\nvalues of Beliefs or BBAs than the Smets pignistic transform. These probability\ntransforms will assign a value of probability that converges faster to the\nvalues below the risk threshold. A probability information content (PIC)\nvariable is also introduced that assigns an information content value to any\nset of probability. Four operators are defined to help simplify the\nderivations.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 12:05:27 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Sudano", "John J.", ""]]}, {"id": "1505.07872", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Towards combinatorial clustering: preliminary research survey", "comments": "102 pages, 66 figures, 67 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes clustering problems from the combinatorial viewpoint. A\nbrief systemic survey is presented including the following: (i) basic\nclustering problems (e.g., classification, clustering, sorting, clustering with\nan order over cluster), (ii) basic approaches to assessment of objects and\nobject proximities (i.e., scales, comparison, aggregation issues), (iii) basic\napproaches to evaluation of local quality characteristics for clusters and\ntotal quality characteristics for clustering solutions, (iv) clustering as\nmulticriteria optimization problem, (v) generalized modular clustering\nframework, (vi) basic clustering models/methods (e.g., hierarchical clustering,\nk-means clustering, minimum spanning tree based clustering, clustering as\nassignment, detection of clisue/quasi-clique based clustering, correlation\nclustering, network communities based clustering), Special attention is\ntargeted to formulation of clustering as multicriteria optimization models.\nCombinatorial optimization models are used as auxiliary problems (e.g.,\nassignment, partitioning, knapsack problem, multiple choice problem,\nmorphological clique problem, searching for consensus/median for structures).\nNumerical examples illustrate problem formulations, solving methods, and\napplications. The material can be used as follows: (a) a research survey, (b) a\nfundamental for designing the structure/architecture of composite modular\nclustering software, (c) a bibliography reference collection, and (d) a\ntutorial.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 21:57:06 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1505.08153", "submitter": "Mohammad Sabokrou", "authors": "Mohsen Fayyaz, Mohammad Hajizadeh_Saffar, Mohammad Sabokrou and\n  Mahmood Fathy", "title": "Feature Representation for Online Signature Verification", "comments": "10 pages, 10 figures, Submitted to IEEE Transactions on Information\n  Forensics and Security", "journal-ref": null, "doi": "10.1109/AISP.2015.7123528", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometrics systems have been used in a wide range of applications and have\nimproved people authentication. Signature verification is one of the most\ncommon biometric methods with techniques that employ various specifications of\na signature. Recently, deep learning has achieved great success in many fields,\nsuch as image, sounds and text processing. In this paper, deep learning method\nhas been used for feature extraction and feature selection.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 19:09:02 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Fayyaz", "Mohsen", ""], ["Hajizadeh_Saffar", "Mohammad", ""], ["Sabokrou", "Mohammad", ""], ["Fathy", "Mahmood", ""]]}]