[{"id": "0902.0043", "submitter": "Christoph Benzmueller", "authors": "Christoph Benzmueller, Chad E. Brown, Michael Kohlhase", "title": "Cut-Simulation and Impredicativity", "comments": "21 pages", "journal-ref": "Logical Methods in Computer Science, Volume 5, Issue 1 (March 3,\n  2009) lmcs:1144", "doi": "10.2168/LMCS-5(1:6)2009", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate cut-elimination and cut-simulation in impredicative\n(higher-order) logics. We illustrate that adding simple axioms such as Leibniz\nequations to a calculus for an impredicative logic -- in our case a sequent\ncalculus for classical type theory -- is like adding cut. The phenomenon\nequally applies to prominent axioms like Boolean- and functional\nextensionality, induction, choice, and description. This calls for the\ndevelopment of calculi where these principles are built-in instead of being\ntreated axiomatically.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2009 03:24:08 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2009 23:37:34 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Benzmueller", "Christoph", ""], ["Brown", "Chad E.", ""], ["Kohlhase", "Michael", ""]]}, {"id": "0902.0514", "submitter": "Ross Duncan", "authors": "Lucas Dixon and Ross Duncan", "title": "Graphical Reasoning in Compact Closed Categories for Quantum Computation", "comments": "21 pages, 9 figures. This is the journal version of the paper\n  published at AISC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compact closed categories provide a foundational formalism for a variety of\nimportant domains, including quantum computation. These categories have a\nnatural visualisation as a form of graphs. We present a formalism for\nequational reasoning about such graphs and develop this into a generic proof\nsystem with a fixed logical kernel for equational reasoning about compact\nclosed categories. Automating this reasoning process is motivated by the slow\nand error prone nature of manual graph manipulation. A salient feature of our\nsystem is that it provides a formal and declarative account of derived results\nthat can include `ellipses'-style notation. We illustrate the framework by\ninstantiating it for a graphical language of quantum computation and show how\nthis can be used to perform symbolic computation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2009 14:21:01 GMT"}], "update_date": "2009-02-04", "authors_parsed": [["Dixon", "Lucas", ""], ["Duncan", "Ross", ""]]}, {"id": "0902.0744", "submitter": "Joe Futrelle", "authors": "James D. Myers, Joe Futrelle, Jeff Gaynor, Joel Plutchak, Peter\n  Bajcsy, Jason Kastner, Kailash Kotwani, Jong Sung Lee, Luigi Marini, Rob\n  Kooper, Robert E. McGrath, Terry McLaren, Alejandro Rodriguez, Yong Liu\n  (National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign)", "title": "Embedding Data within Knowledge Spaces", "comments": "10 pages with 1 figure. Corrected incorrect transliteration in\n  abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The promise of e-Science will only be realized when data is discoverable,\naccessible, and comprehensible within distributed teams, across disciplines,\nand over the long-term--without reliance on out-of-band (non-digital) means. We\nhave developed the open-source Tupelo semantic content management framework and\nare employing it to manage a wide range of e-Science entities (including data,\ndocuments, workflows, people, and projects) and a broad range of metadata\n(including provenance, social networks, geospatial relationships, temporal\nrelations, and domain descriptions). Tupelo couples the use of global\nidentifiers and resource description framework (RDF) statements with an\naggregatable content repository model to provide a unified space for securely\nmanaging distributed heterogeneous content and relationships.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2009 16:52:41 GMT"}], "update_date": "2009-02-05", "authors_parsed": [["Myers", "James D.", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["Futrelle", "Joe", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["Gaynor", "Jeff", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["Plutchak", "Joel", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["Bajcsy", "Peter", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["Kastner", "Jason", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["Kotwani", "Kailash", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["Lee", "Jong Sung", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["Marini", "Luigi", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["Kooper", "Rob", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["McGrath", "Robert E.", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["McLaren", "Terry", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["Rodriguez", "Alejandro", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"], ["Liu", "Yong", "", "National Center for Supercomputing Applications, University of Illinois at\n  Urbana-Champaign"]]}, {"id": "0902.0798", "submitter": "Ernesto Diaz-Aviles", "authors": "Ernesto Diaz-Aviles", "title": "Alleviating Media Bias Through Intelligent Agent Blogging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Consumers of mass media must have a comprehensive, balanced and plural\nselection of news to get an unbiased perspective; but achieving this goal can\nbe very challenging, laborious and time consuming. News stories development\nover time, its (in)consistency, and different level of coverage across the\nmedia outlets are challenges that a conscientious reader has to overcome in\norder to alleviate bias.\n  In this paper we present an intelligent agent framework currently\nfacilitating analysis of the main sources of on-line news in El Salvador. We\nshow how prior tools of text analysis and Web 2.0 technologies can be combined\nwith minimal manual intervention to help individuals on their rational decision\nprocess, while holding media outlets accountable for their work.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2009 21:25:59 GMT"}], "update_date": "2009-02-06", "authors_parsed": [["Diaz-Aviles", "Ernesto", ""]]}, {"id": "0902.0899", "submitter": "Camilla Schwind", "authors": "R\\'egis Alenda (LSIS), Nicola Olivetti (LSIS), Camilla Schwind (LIF)", "title": "Comparative concept similarity over Minspaces: Axiomatisation and\n  Tableaux Calculus", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the logic of comparative concept similarity $\\CSL$ introduced by\nSheremet, Tishkovsky, Wolter and Zakharyaschev to capture a form of qualitative\nsimilarity comparison. In this logic we can formulate assertions of the form \"\nobjects A are more similar to B than to C\". The semantics of this logic is\ndefined by structures equipped by distance functions evaluating the similarity\ndegree of objects. We consider here the particular case of the semantics\ninduced by \\emph{minspaces}, the latter being distance spaces where the minimum\nof a set of distances always exists. It turns out that the semantics over\narbitrary minspaces can be equivalently specified in terms of preferential\nstructures, typical of conditional logics. We first give a direct\naxiomatisation of this logic over Minspaces. We next define a decision\nprocedure in the form of a tableaux calculus. Both the calculus and the\naxiomatisation take advantage of the reformulation of the semantics in terms of\npreferential structures.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2009 12:55:42 GMT"}], "update_date": "2009-02-06", "authors_parsed": [["Alenda", "R\u00e9gis", "", "LSIS"], ["Olivetti", "Nicola", "", "LSIS"], ["Schwind", "Camilla", "", "LIF"]]}, {"id": "0902.1080", "submitter": "Baptiste Jeudy", "authors": "Baptiste Jeudy (LAHC), Christine Largeron (LAHC), Fran\\c{c}ois\n  Jacquenet (LAHC)", "title": "A Model for Managing Collections of Patterns", "comments": null, "journal-ref": "ACM Symposium on Applied Computing, Seoul : Cor\\'ee, R\\'epublique\n  de (2007)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining algorithms are now able to efficiently deal with huge amount of\ndata. Various kinds of patterns may be discovered and may have some great\nimpact on the general development of knowledge. In many domains, end users may\nwant to have their data mined by data mining tools in order to extract patterns\nthat could impact their business. Nevertheless, those users are often\noverwhelmed by the large quantity of patterns extracted in such a situation.\nMoreover, some privacy issues, or some commercial one may lead the users not to\nbe able to mine the data by themselves. Thus, the users may not have the\npossibility to perform many experiments integrating various constraints in\norder to focus on specific patterns they would like to extract. Post processing\nof patterns may be an answer to that drawback. Thus, in this paper we present a\nframework that could allow end users to manage collections of patterns. We\npropose to use an efficient data structure on which some algebraic operators\nmay be used in order to retrieve or access patterns in pattern bases.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2009 12:50:12 GMT"}], "update_date": "2009-02-09", "authors_parsed": [["Jeudy", "Baptiste", "", "LAHC"], ["Largeron", "Christine", "", "LAHC"], ["Jacquenet", "Fran\u00e7ois", "", "LAHC"]]}, {"id": "0902.1227", "submitter": "Raajay Viswanathan", "authors": "Avinash Achar, Srivatsan Laxman, Raajay Viswanathan and P. S. Sastry", "title": "Discovering general partial orders in event streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent episode discovery is a popular framework for pattern discovery in\nevent streams. An episode is a partially ordered set of nodes with each node\nassociated with an event type. Efficient (and separate) algorithms exist for\nepisode discovery when the associated partial order is total (serial episode)\nand trivial (parallel episode). In this paper, we propose efficient algorithms\nfor discovering frequent episodes with general partial orders. These algorithms\ncan be easily specialized to discover serial or parallel episodes. Also, the\nalgorithms are flexible enough to be specialized for mining in the space of\ncertain interesting subclasses of partial orders. We point out that there is an\ninherent combinatorial explosion in frequent partial order mining and most\nimportantly, frequency alone is not a sufficient measure of interestingness. We\npropose a new interestingness measure for general partial order episodes and a\ndiscovery method based on this measure, for filtering out uninteresting partial\norders. Simulations demonstrate the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2009 07:50:02 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2009 06:18:30 GMT"}], "update_date": "2009-12-11", "authors_parsed": [["Achar", "Avinash", ""], ["Laxman", "Srivatsan", ""], ["Viswanathan", "Raajay", ""], ["Sastry", "P. S.", ""]]}, {"id": "0902.1629", "submitter": "Anna Kucerova", "authors": "O. Hrstka and A. Kucerova", "title": "Improvements of real coded genetic algorithms based on differential\n  operators preventing premature convergence", "comments": "23 pages, 2 figures, 4 tables", "journal-ref": "Advances in Engineering Software, 35 (3-4), 237-246, 2004", "doi": "10.1016/S0965-9978(03)00113-3", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents several types of evolutionary algorithms (EAs) used for\nglobal optimization on real domains. The interest has been focused on\nmultimodal problems, where the difficulties of a premature convergence usually\noccurs. First the standard genetic algorithm (SGA) using binary encoding of\nreal values and its unsatisfactory behavior with multimodal problems is briefly\nreviewed together with some improvements of fighting premature convergence. Two\ntypes of real encoded methods based on differential operators are examined in\ndetail: the differential evolution (DE), a very modern and effective method\nfirstly published by R. Storn and K. Price, and the simplified real-coded\ndifferential genetic algorithm SADE proposed by the authors. In addition, an\nimprovement of the SADE method, called CERAF technology, enabling the\npopulation of solutions to escape from local extremes, is examined. All methods\nare tested on an identical set of objective functions and a systematic\ncomparison based on a reliable methodology is presented. It is confirmed that\nreal coded methods generally exhibit better behavior on real domains than the\nbinary algorithms, even when extended by several improvements. Furthermore, the\npositive influence of the differential operators due to their possibility of\nself-adaptation is demonstrated. From the reliability point of view, it seems\nthat the real encoded differential algorithm, improved by the technology\ndescribed in this paper, is a universal and reliable method capable of solving\nall proposed test problems.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2009 11:02:50 GMT"}], "update_date": "2009-02-11", "authors_parsed": [["Hrstka", "O.", ""], ["Kucerova", "A.", ""]]}, {"id": "0902.1647", "submitter": "Anna Kucerova", "authors": "O. Hrstka, A. Kucerova, M. Leps and J. Zeman", "title": "A competitive comparison of different types of evolutionary algorithms", "comments": "25 pages, 8 figures, 5 tables", "journal-ref": "Computers & Structures, 81 (18-19), 1979-1990, 2003", "doi": "10.1016/S0045-7949(03)00217-7", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents comparison of several stochastic optimization algorithms\ndeveloped by authors in their previous works for the solution of some problems\narising in Civil Engineering. The introduced optimization methods are: the\ninteger augmented simulated annealing (IASA), the real-coded augmented\nsimulated annealing (RASA), the differential evolution (DE) in its original\nfashion developed by R. Storn and K. Price and simplified real-coded\ndifferential genetic algorithm (SADE). Each of these methods was developed for\nsome specific optimization problem; namely the Chebychev trial polynomial\nproblem, the so called type 0 function and two engineering problems - the\nreinforced concrete beam layout and the periodic unit cell problem\nrespectively. Detailed and extensive numerical tests were performed to examine\nthe stability and efficiency of proposed algorithms. The results of our\nexperiments suggest that the performance and robustness of RASA, IASA and SADE\nmethods are comparable, while the DE algorithm performs slightly worse. This\nfact together with a small number of internal parameters promotes the SADE\nmethod as the most robust for practical use.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2009 13:41:15 GMT"}], "update_date": "2009-02-11", "authors_parsed": [["Hrstka", "O.", ""], ["Kucerova", "A.", ""], ["Leps", "M.", ""], ["Zeman", "J.", ""]]}, {"id": "0902.1690", "submitter": "Anna Kucerova", "authors": "A. Kucerova, M. Leps and J. Zeman", "title": "Back analysis of microplane model parameters using soft computing\n  methods", "comments": "21 pages, 27 figures, 7 tables", "journal-ref": "CAMES: Computer Assisted Mechanics and Engineering Sciences, 14\n  (2), 219-242, 2007", "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new procedure based on layered feed-forward neural networks for the\nmicroplane material model parameters identification is proposed in the present\npaper. Novelties are usage of the Latin Hypercube Sampling method for the\ngeneration of training sets, a systematic employment of stochastic sensitivity\nanalysis and a genetic algorithm-based training of a neural network by an\nevolutionary algorithm. Advantages and disadvantages of this approach together\nwith possible extensions are thoroughly discussed and analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2009 16:43:06 GMT"}], "update_date": "2009-02-11", "authors_parsed": [["Kucerova", "A.", ""], ["Leps", "M.", ""], ["Zeman", "J.", ""]]}, {"id": "0902.1911", "submitter": "Junsheng  Zhang", "authors": "Hai Zhuge and Junsheng Zhang", "title": "Topological Centrality and Its Applications", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": "KGRC-2009-02", "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent development of network structure analysis shows that it plays an\nimportant role in characterizing complex system of many branches of sciences.\nDifferent from previous network centrality measures, this paper proposes the\nnotion of topological centrality (TC) reflecting the topological positions of\nnodes and edges in general networks, and proposes an approach to calculating\nthe topological centrality. The proposed topological centrality is then used to\ndiscover communities and build the backbone network. Experiments and\napplications on research network show the significance of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2009 15:14:11 GMT"}], "update_date": "2009-02-12", "authors_parsed": [["Zhuge", "Hai", ""], ["Zhang", "Junsheng", ""]]}, {"id": "0902.2206", "submitter": "KIlian Weinberger", "authors": "Kilian Weinberger, Anirban Dasgupta, Josh Attenberg, John Langford,\n  Alex Smola", "title": "Feature Hashing for Large Scale Multitask Learning", "comments": "Fixed broken theorem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical evidence suggests that hashing is an effective strategy for\ndimensionality reduction and practical nonparametric estimation. In this paper\nwe provide exponential tail bounds for feature hashing and show that the\ninteraction between random subspaces is negligible with high probability. We\ndemonstrate the feasibility of this approach with experimental results for a\nnew use case -- multitask learning with hundreds of thousands of tasks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2009 20:06:36 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2009 16:18:39 GMT"}, {"version": "v3", "created": "Wed, 20 May 2009 19:05:20 GMT"}, {"version": "v4", "created": "Thu, 21 May 2009 21:18:40 GMT"}, {"version": "v5", "created": "Sat, 27 Feb 2010 15:32:35 GMT"}], "update_date": "2010-02-27", "authors_parsed": [["Weinberger", "Kilian", ""], ["Dasgupta", "Anirban", ""], ["Attenberg", "Josh", ""], ["Langford", "John", ""], ["Smola", "Alex", ""]]}, {"id": "0902.2362", "submitter": "Christophe Lecoutre", "authors": "Olivier Roussel, Christophe Lecoutre", "title": "XML Representation of Constraint Networks: Format XCSP 2.1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new extended format to represent constraint networks using XML.\nThis format allows us to represent constraints defined either in extension or\nin intension. It also allows us to reference global constraints. Any instance\nof the problems CSP (Constraint Satisfaction Problem), QCSP (Quantified CSP)\nand WCSP (Weighted CSP) can be represented using this format.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2009 18:24:27 GMT"}], "update_date": "2009-02-16", "authors_parsed": [["Roussel", "Olivier", ""], ["Lecoutre", "Christophe", ""]]}, {"id": "0902.2871", "submitter": "Kaninda Musumbu", "authors": "Kaninda Musumbu (LaBRI)", "title": "The Semantics of Kalah Game", "comments": null, "journal-ref": "ACM International conference Proceeding series, ISBN 0-9544145-6-X\n  (2005) 191 - 196", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work consisted in developing a plateau game. There are the\ntraditional ones (monopoly, cluedo, ect.) but those which interest us leave\nless place at the chance (luck) than to the strategy such that the chess game.\nKallah is an old African game, its rules are simple but the strategies to be\nused are very complex to implement. Of course, they are based on a strongly\nmathematical basis as in the film \"Rain-Man\" where one can see that gambling\ncan be payed with strategies based on mathematical theories. The Artificial\nIntelligence gives the possibility \"of thinking\" to a machine and, therefore,\nallows it to make decisions. In our work, we use it to give the means to the\ncomputer choosing its best movement.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2009 09:08:35 GMT"}], "update_date": "2009-02-18", "authors_parsed": [["Musumbu", "Kaninda", "", "LaBRI"]]}, {"id": "0902.2969", "submitter": "Giorgi Japaridze", "authors": "Giorgi Japaridze", "title": "Ptarithmetic", "comments": "Substantially better versions are on their way. Hence the present\n  article probably will not be published", "journal-ref": "The Baltic International Yearbook on Cognition, Logic and\n  Communication 8 (2013), Article 5, pp. 1-186", "doi": "10.4148/1944-3676.1074", "report-no": null, "categories": "cs.LO cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present article introduces ptarithmetic (short for \"polynomial time\narithmetic\") -- a formal number theory similar to the well known Peano\narithmetic, but based on the recently born computability logic (see\nhttp://www.cis.upenn.edu/~giorgi/cl.html) instead of classical logic. The\nformulas of ptarithmetic represent interactive computational problems rather\nthan just true/false statements, and their \"truth\" is understood as existence\nof a polynomial time solution. The system of ptarithmetic elaborated in this\narticle is shown to be sound and complete. Sound in the sense that every\ntheorem T of the system represents an interactive number-theoretic\ncomputational problem with a polynomial time solution and, furthermore, such a\nsolution can be effectively extracted from a proof of T. And complete in the\nsense that every interactive number-theoretic problem with a polynomial time\nsolution is represented by some theorem T of the system.\n  The paper is self-contained, and can be read without any previous familiarity\nwith computability logic.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2009 19:14:09 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2009 12:48:43 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2010 10:17:31 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Japaridze", "Giorgi", ""]]}, {"id": "0902.2975", "submitter": "Claus-Peter Wirth", "authors": "Claus-Peter Wirth, Ruediger Lunde", "title": "Writing Positive/Negative-Conditional Equations Conveniently", "comments": "ii + 21 pages", "journal-ref": null, "doi": null, "report-no": "SEKI Working-Paper SWP-94-04", "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convenient notation for positive/negative-conditional equations.\nThe idea is to merge rules specifying the same function by using case-, if-,\nmatch-, and let-expressions. Based on the presented macro-rule-construct,\npositive/negative-conditional equational specifications can be written on a\nhigher level. A rewrite system translates the macro-rule-constructs into\npositive/negative-conditional equations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2009 19:13:32 GMT"}], "update_date": "2009-02-18", "authors_parsed": [["Wirth", "Claus-Peter", ""], ["Lunde", "Ruediger", ""]]}, {"id": "0902.2995", "submitter": "Claus-Peter Wirth", "authors": "Ruediger Lunde, Claus-Peter Wirth", "title": "ASF+ --- eine ASF-aehnliche Spezifikationssprache", "comments": "iv + 58 pages", "journal-ref": null, "doi": null, "report-no": "SEKI Working-Paper SWP-94-05", "categories": "cs.AI cs.SC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Maintaining the main aspects of the algebraic specification language ASF as\npresented in [Bergstra&al.89] we have extend ASF with the following concepts:\nWhile once exported names in ASF must stay visible up to the top the module\nhierarchy, ASF+ permits a more sophisticated hiding of signature names. The\nerroneous merging of distinct structures that occurs when importing different\nactualizations of the same parameterized module in ASF is avoided in ASF+ by a\nmore adequate form of parameter binding. The new ``Namensraum''-concept of ASF+\npermits the specifier on the one hand directly to identify the origin of hidden\nnames and on the other to decide whether an imported module is only to be\naccessed or whether an important property of it is to be modified. In the first\ncase he can access one single globally provided version; in the second he has\nto import a copy of the module. Finally ASF+ permits semantic conditions on\nparameters and the specification of tasks for a theorem prover.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2009 20:52:11 GMT"}], "update_date": "2009-02-18", "authors_parsed": [["Lunde", "Ruediger", ""], ["Wirth", "Claus-Peter", ""]]}, {"id": "0902.3176", "submitter": "John Langford", "authors": "Alina Beygelzimer, John Langford, and Pradeep Ravikumar", "title": "Error-Correcting Tournaments", "comments": "Minor wording improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a family of pairwise tournaments reducing $k$-class classification\nto binary classification. These reductions are provably robust against a\nconstant fraction of binary errors. The results improve on the PECOC\nconstruction \\cite{SECOC} with an exponential improvement in computation, from\n$O(k)$ to $O(\\log_2 k)$, and the removal of a square root in the regret\ndependence, matching the best possible computation and regret up to a constant.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2009 16:01:24 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2009 01:14:27 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2009 22:06:32 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2010 15:03:58 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Langford", "John", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "0902.3196", "submitter": "Christoph Schommer", "authors": "Claudine Brucks, Michael Hilker, Christoph Schommer, Cynthia Wagner,\n  Ralph Weires", "title": "Symbolic Computing with Incremental Mindmaps to Manage and Mine Data\n  Streams - Some Applications", "comments": "4 pages; 4 figures", "journal-ref": "Proceedings of the 4th International Workshop on Neural-Symbolic\n  Learning and Reasoning (NeSy '08); July 2008., Patras, Greece", "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our understanding, a mind-map is an adaptive engine that basically works\nincrementally on the fundament of existing transactional streams. Generally,\nmind-maps consist of symbolic cells that are connected with each other and that\nbecome either stronger or weaker depending on the transactional stream. Based\non the underlying biologic principle, these symbolic cells and their\nconnections as well may adaptively survive or die, forming different cell\nagglomerates of arbitrary size. In this work, we intend to prove mind-maps'\neligibility following diverse application scenarios, for example being an\nunderlying management system to represent normal and abnormal traffic behaviour\nin computer networks, supporting the detection of the user behaviour within\nsearch engines, or being a hidden communication layer for natural language\ninteraction.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2009 17:19:50 GMT"}], "update_date": "2009-02-19", "authors_parsed": [["Brucks", "Claudine", ""], ["Hilker", "Michael", ""], ["Schommer", "Christoph", ""], ["Wagner", "Cynthia", ""], ["Weires", "Ralph", ""]]}, {"id": "0902.3294", "submitter": "Claus-Peter Wirth", "authors": "Claus-Peter Wirth", "title": "Progress in Computer-Assisted Inductive Theorem Proving by\n  Human-Orientedness and Descente Infinie?", "comments": "ii + 35 pages", "journal-ref": "Logic Journal of the IGPL, 2012, Volume 20, Pp. 1046-1063", "doi": "10.1093/jigpal/jzr048", "report-no": "SEKI Working-Paper SR-2006-01", "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short position paper we briefly review the development history of\nautomated inductive theorem proving and computer-assisted mathematical\ninduction. We think that the current low expectations on progress in this field\nresult from a faulty narrow-scope historical projection. Our main motivation is\nto explain--on an abstract but hopefully sufficiently descriptive level--why we\nbelieve that future progress in the field is to result from human-orientedness\nand descente infinie.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2009 22:37:31 GMT"}, {"version": "v2", "created": "Tue, 11 May 2010 18:16:50 GMT"}, {"version": "v3", "created": "Wed, 1 Sep 2010 14:06:16 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Wirth", "Claus-Peter", ""]]}, {"id": "0902.3430", "submitter": "Afshin Rostamizadeh", "authors": "Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh", "title": "Domain Adaptation: Learning Bounds and Algorithms", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the general problem of domain adaptation which arises in\na variety of applications where the distribution of the labeled sample\navailable somewhat differs from that of the test data. Building on previous\nwork by Ben-David et al. (2007), we introduce a novel distance between\ndistributions, discrepancy distance, that is tailored to adaptation problems\nwith arbitrary loss functions. We give Rademacher complexity bounds for\nestimating the discrepancy distance from finite samples for different loss\nfunctions. Using this distance, we derive novel generalization bounds for\ndomain adaptation for a wide family of loss functions. We also present a series\nof novel adaptation bounds for large classes of regularization-based\nalgorithms, including support vector machines and kernel ridge regression based\non the empirical discrepancy. This motivates our analysis of the problem of\nminimizing the empirical discrepancy for various loss functions for which we\nalso give novel algorithms. We report the results of preliminary experiments\nthat demonstrate the benefits of our discrepancy minimization algorithms for\ndomain adaptation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2009 18:42:16 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2009 16:56:37 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Mansour", "Yishay", ""], ["Mohri", "Mehryar", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "0902.3513", "submitter": "Mark Burgin", "authors": "Mark Burgin and Gordana Dodig-Crnkovic", "title": "A Systematic Approach to Artificial Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents and agent systems are becoming more and more important in the\ndevelopment of a variety of fields such as ubiquitous computing, ambient\nintelligence, autonomous computing, intelligent systems and intelligent\nrobotics. The need for improvement of our basic knowledge on agents is very\nessential. We take a systematic approach and present extended classification of\nartificial agents which can be useful for understanding of what artificial\nagents are and what they can be in the future. The aim of this classification\nis to give us insights in what kind of agents can be created and what type of\nproblems demand a specific kind of agents for their solution.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2009 04:58:40 GMT"}], "update_date": "2009-02-23", "authors_parsed": [["Burgin", "Mark", ""], ["Dodig-Crnkovic", "Gordana", ""]]}, {"id": "0902.3614", "submitter": "Claus-Peter Wirth", "authors": "Claus-Peter Wirth", "title": "Syntactic Confluence Criteria for Positive/Negative-Conditional Term\n  Rewriting Systems", "comments": "ii + 187 pages", "journal-ref": "J. Symbolic Computation, 2009, 44:60--98", "doi": null, "report-no": "SEKI Report SR-95-09", "categories": "cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We study the combination of the following already known ideas for showing\nconfluence of unconditional or conditional term rewriting systems into\npractically more useful confluence criteria for conditional systems: Our\nsyntactical separation into constructor and non-constructor symbols, Huet's\nintroduction and Toyama's generalization of parallel closedness for\nnon-noetherian unconditional systems, the use of shallow confluence for proving\nconfluence of noetherian and non-noetherian conditional systems, the idea that\ncertain kinds of limited confluence can be assumed for checking the\nfulfilledness or infeasibility of the conditions of conditional critical pairs,\nand the idea that (when termination is given) only prime superpositions have to\nbe considered and certain normalization restrictions can be applied for the\nsubstitutions fulfilling the conditions of conditional critical pairs. Besides\ncombining and improving already known methods, we present the following new\nideas and results: We strengthen the criterion for overlay joinable noetherian\nsystems, and, by using the expressiveness of our syntactical separation into\nconstructor and non-constructor symbols, we are able to present criteria for\nlevel confluence that are not criteria for shallow confluence actually and also\nable to weaken the severe requirement of normality (stiffened with\nleft-linearity) in the criteria for shallow confluence of noetherian and\nnon-noetherian conditional systems to the easily satisfied requirement of\nquasi-normality. Finally, the whole paper may also give a practically useful\noverview of the syntactical means for showing confluence of conditional term\nrewriting systems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2009 16:26:52 GMT"}], "update_date": "2009-02-23", "authors_parsed": [["Wirth", "Claus-Peter", ""]]}, {"id": "0902.3623", "submitter": "Claus-Peter Wirth", "authors": "Claus-Peter Wirth", "title": "A Self-Contained and Easily Accessible Discussion of the Method of\n  Descente Infinie and Fermat's Only Explicitly Known Proof by Descente Infinie", "comments": "ii + 36 pages, French abstract (R\\'esum\\'e) included in paper", "journal-ref": null, "doi": null, "report-no": "SEKI Working-Paper SWP-2006-02, Second edition", "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the only proof of Pierre Fermat by descente infinie that is known\nto exist today. As the text of its Latin original requires active mathematical\ninterpretation, it is more a proof sketch than a proper mathematical proof. We\ndiscuss descente infinie from the mathematical, logical, historical,\nlinguistic, and refined logic-historical points of view. We provide the\nrequired preliminaries from number theory and develop a self-contained proof in\na modern form, which nevertheless is intended to follow Fermat's ideas closely.\nWe then annotate an English translation of Fermat's original proof with terms\nfrom the modern proof. Including all important facts, we present a concise and\nself-contained discussion of Fermat's proof sketch, which is easily accessible\nto laymen in number theory as well as to laymen in the history of mathematics,\nand which provides new clarification of the Method of Descente Infinie to the\nexperts in these fields. Last but not least, this paper fills a gap regarding\nthe easy accessibility of the subject.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2009 17:16:52 GMT"}, {"version": "v2", "created": "Sun, 28 Nov 2010 22:23:01 GMT"}, {"version": "v3", "created": "Wed, 8 Dec 2010 15:39:54 GMT"}, {"version": "v4", "created": "Tue, 14 Dec 2010 08:32:44 GMT"}], "update_date": "2010-12-15", "authors_parsed": [["Wirth", "Claus-Peter", ""]]}, {"id": "0902.3635", "submitter": "Claus-Peter Wirth", "authors": "Claus-Peter Wirth", "title": "lim+, delta+, and Non-Permutability of beta-Steps", "comments": "ii + 36 pages", "journal-ref": "Journal of Symbolic Computation, 2012, Volume 47, Pp. 1109-1135", "doi": "10.1016/j.jsc.2011.12.035", "report-no": "SEKI Report SR-2005-01", "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a human-oriented formal example proof of the (lim+) theorem, i.e. that\nthe sum of limits is the limit of the sum, which is of value for reference on\nits own, we exhibit a non-permutability of beta-steps and delta+-steps\n(according to Smullyan's classification), which is not visible with\nnon-liberalized delta-rules and not serious with further liberalized\ndelta-rules, such as the delta++-rule. Besides a careful presentation of the\nsearch for a proof of (lim+) with several pedagogical intentions, the main\nsubject is to explain why the order of beta-steps plays such a practically\nimportant role in some calculi.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2009 18:07:28 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Wirth", "Claus-Peter", ""]]}, {"id": "0902.3648", "submitter": "Claus-Peter Wirth", "authors": "Volker Mattick, Claus-Peter Wirth", "title": "An Algebraic Dexter-Based Hypertext Reference Model", "comments": "ii + 48 pages", "journal-ref": null, "doi": null, "report-no": "Research Report 719/1999 (green/grey series), Fachbereich\n  Informatik, University of Dortmund", "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first formal algebraic specification of a hypertext reference\nmodel. It is based on the well-known Dexter Hypertext Reference Model and\nincludes modifications with respect to the development of hypertext since the\nWWW came up. Our hypertext model was developed as a product model with the aim\nto automatically support the design process and is extended to a model of\nhypertext-systems in order to be able to describe the state transitions in this\nprocess. While the specification should be easy to read for non-experts in\nalgebraic specification, it guarantees a unique understanding and enables a\nclose connection to logic-based development and verification.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2009 19:57:30 GMT"}], "update_date": "2009-02-23", "authors_parsed": [["Mattick", "Volker", ""], ["Wirth", "Claus-Peter", ""]]}, {"id": "0902.3730", "submitter": "Claus-Peter Wirth", "authors": "Claus-Peter Wirth", "title": "Full First-Order Sequent and Tableau Calculi With Preservation of\n  Solutions and the Liberalized delta-Rule but Without Skolemization", "comments": "ii + 40 pages", "journal-ref": "Caferra, R. and Salzer, G., eds., Automated Deduction in Classical\n  and Non-Classical Logics (FTP'98), LNAI 1761, pp. 283-298, Springer, 2000", "doi": null, "report-no": "Research Report 698/1998 (green/grey series), Fachbereich\n  Informatik, University of Dortmund", "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a combination of raising, explicit variable dependency\nrepresentation, the liberalized delta-rule, and preservation of solutions for\nfirst-order deductive theorem proving. Our main motivation is to provide the\nfoundation for our work on inductive theorem proving, where the preservation of\nsolutions is indispensable.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2009 09:45:20 GMT"}], "update_date": "2009-02-24", "authors_parsed": [["Wirth", "Claus-Peter", ""]]}, {"id": "0902.3749", "submitter": "Claus-Peter Wirth", "authors": "Claus-Peter Wirth", "title": "Hilbert's epsilon as an Operator of Indefinite Committed Choice", "comments": "ii + 73 pages. arXiv admin note: substantial text overlap with\n  arXiv:1104.2444", "journal-ref": "Journal of Applied Logic 6 (2008), pp. 287-317", "doi": "10.1016/j.jal.2007.07.009", "report-no": "SEKI Report SR-2006-02", "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paul Bernays and David Hilbert carefully avoided overspecification of\nHilbert's epsilon-operator and axiomatized only what was relevant for their\nproof-theoretic investigations. Semantically, this left the epsilon-operator\nunderspecified. In the meanwhile, there have been several suggestions for\nsemantics of the epsilon as a choice operator. After reviewing the literature\non semantics of Hilbert's epsilon operator, we propose a new semantics with the\nfollowing features: We avoid overspecification (such as right-uniqueness), but\nadmit indefinite choice, committed choice, and classical logics. Moreover, our\nsemantics for the epsilon supports proof search optimally and is natural in the\nsense that it does not only mirror some cases of referential interpretation of\nindefinite articles in natural language, but may also contribute to philosophy\nof language. Finally, we ask the question whether our epsilon within our\nfree-variable framework can serve as a paradigm useful in the specification and\ncomputation of semantics of discourses in natural language.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2009 17:04:16 GMT"}, {"version": "v2", "created": "Sun, 23 May 2010 16:23:47 GMT"}, {"version": "v3", "created": "Mon, 16 Jan 2012 21:10:14 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Wirth", "Claus-Peter", ""]]}, {"id": "0902.4521", "submitter": "Heng Huang", "authors": "Dijun Luo, Heng Huang, Chris Ding", "title": "Are Tensor Decomposition Solutions Unique? On the global convergence of\n  HOSVD and ParaFac algorithms", "comments": "Submitted to CVPR2009 in Nov. 20, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For tensor decompositions such as HOSVD and ParaFac, the objective functions\nare nonconvex. This implies, theoretically, there exists a large number of\nlocal optimas: starting from different starting point, the iteratively improved\nsolution will converge to different local solutions. This non-uniqueness\npresent a stability and reliability problem for image compression and\nretrieval. In this paper, we present the results of a comprehensive\ninvestigation of this problem. We found that although all tensor decomposition\nalgorithms fail to reach a unique global solution on random data and severely\nscrambled data; surprisingly however, on all real life several data sets (even\nwith substantial scramble and occlusions), HOSVD always produce the unique\nglobal solution in the parameter region suitable to practical applications,\nwhile ParaFac produce non-unique solutions. We provide an eigenvalue based rule\nfor the assessing the solution uniqueness.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2009 07:20:49 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Luo", "Dijun", ""], ["Huang", "Heng", ""], ["Ding", "Chris", ""]]}, {"id": "0902.4682", "submitter": "Claus-Peter Wirth", "authors": "Claus-Peter Wirth, Joerg Siekmann, Christoph Benzmueller, Serge\n  Autexier", "title": "Lectures on Jacques Herbrand as a Logician", "comments": "ii + 82 pages", "journal-ref": null, "doi": null, "report-no": "SEKI Report SR-2009-01", "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give some lectures on the work on formal logic of Jacques Herbrand, and\nsketch his life and his influence on automated theorem proving. The intended\naudience ranges from students interested in logic over historians to logicians.\nBesides the well-known correction of Herbrand's False Lemma by Goedel and\nDreben, we also present the hardly known unpublished correction of Heijenoort\nand its consequences on Herbrand's Modus Ponens Elimination. Besides Herbrand's\nFundamental Theorem and its relation to the Loewenheim-Skolem-Theorem, we\ncarefully investigate Herbrand's notion of intuitionism in connection with his\nnotion of falsehood in an infinite domain. We sketch Herbrand's two proofs of\nthe consistency of arithmetic and his notion of a recursive function, and last\nbut not least, present the correct original text of his unification algorithm\nwith a new translation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2009 19:59:17 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2009 18:29:54 GMT"}, {"version": "v3", "created": "Tue, 12 May 2009 19:09:43 GMT"}, {"version": "v4", "created": "Thu, 30 Jun 2011 18:43:12 GMT"}, {"version": "v5", "created": "Tue, 27 May 2014 08:58:06 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Wirth", "Claus-Peter", ""], ["Siekmann", "Joerg", ""], ["Benzmueller", "Christoph", ""], ["Autexier", "Serge", ""]]}]