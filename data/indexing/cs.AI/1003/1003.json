[{"id": "1003.0034", "submitter": "Jennifer Vaughan", "authors": "Yiling Chen and Jennifer Wortman Vaughan", "title": "A New Understanding of Prediction Markets Via No-Regret Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the striking mathematical connections that exist between market\nscoring rules, cost function based prediction markets, and no-regret learning.\nWe show that any cost function based prediction market can be interpreted as an\nalgorithm for the commonly studied problem of learning from expert advice by\nequating trades made in the market with losses observed by the learning\nalgorithm. If the loss of the market organizer is bounded, this bound can be\nused to derive an O(sqrt(T)) regret bound for the corresponding learning\nalgorithm. We then show that the class of markets with convex cost functions\nexactly corresponds to the class of Follow the Regularized Leader learning\nalgorithms, with the choice of a cost function in the market corresponding to\nthe choice of a regularizer in the learning problem. Finally, we show an\nequivalence between market scoring rules and prediction markets with convex\ncost functions. This implies that market scoring rules can also be interpreted\nnaturally as Follow the Regularized Leader algorithms, and may be of\nindependent interest. These connections provide new insight into how it is that\ncommonly studied markets, such as the Logarithmic Market Scoring Rule, can\naggregate opinions into accurate estimates of the likelihood of future events.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2010 23:27:22 GMT"}], "update_date": "2010-03-02", "authors_parsed": [["Chen", "Yiling", ""], ["Vaughan", "Jennifer Wortman", ""]]}, {"id": "1003.0120", "submitter": "John Langford", "authors": "Alex Strehl, John Langford, Sham Kakade, Lihong Li", "title": "Learning from Logged Implicit Exploration Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a sound and consistent foundation for the use of \\emph{nonrandom}\nexploration data in \"contextual bandit\" or \"partially labeled\" settings where\nonly the value of a chosen action is learned.\n  The primary challenge in a variety of settings is that the exploration\npolicy, in which \"offline\" data is logged, is not explicitly known. Prior\nsolutions here require either control of the actions during the learning\nprocess, recorded random exploration, or actions chosen obliviously in a\nrepeated manner. The techniques reported here lift these restrictions, allowing\nthe learning of a policy for choosing actions given features from historical\ndata where no randomization occurred or was logged.\n  We empirically verify our solution on two reasonably sized sets of real-world\ndata obtained from Yahoo!.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2010 17:53:46 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2010 16:06:16 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Strehl", "Alex", ""], ["Langford", "John", ""], ["Kakade", "Sham", ""], ["Li", "Lihong", ""]]}, {"id": "1003.0146", "submitter": "Lihong Li", "authors": "Lihong Li, Wei Chu, John Langford, Robert E. Schapire", "title": "A Contextual-Bandit Approach to Personalized News Article Recommendation", "comments": "10 pages, 5 figures", "journal-ref": "Presented at the Nineteenth International Conference on World Wide\n  Web (WWW 2010), Raleigh, NC, USA, 2010", "doi": "10.1145/1772690.1772758", "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized web services strive to adapt their services (advertisements,\nnews articles, etc) to individual users by making use of both content and user\ninformation. Despite a few recent advances, this problem remains challenging\nfor at least two reasons. First, web service is featured with dynamically\nchanging pools of content, rendering traditional collaborative filtering\nmethods inapplicable. Second, the scale of most web services of practical\ninterest calls for solutions that are both fast in learning and computation.\n  In this work, we model personalized recommendation of news articles as a\ncontextual bandit problem, a principled approach in which a learning algorithm\nsequentially selects articles to serve users based on contextual information\nabout the users and articles, while simultaneously adapting its\narticle-selection strategy based on user-click feedback to maximize total user\nclicks.\n  The contributions of this work are three-fold. First, we propose a new,\ngeneral contextual bandit algorithm that is computationally efficient and well\nmotivated from learning theory. Second, we argue that any bandit algorithm can\nbe reliably evaluated offline using previously recorded random traffic.\nFinally, using this offline evaluation method, we successfully applied our new\nalgorithm to a Yahoo! Front Page Today Module dataset containing over 33\nmillion events. Results showed a 12.5% click lift compared to a standard\ncontext-free bandit algorithm, and the advantage becomes even greater when data\ngets more scarce.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2010 02:18:59 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2012 23:49:42 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Li", "Lihong", ""], ["Chu", "Wei", ""], ["Langford", "John", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1003.0319", "submitter": "Uwe Aickelin", "authors": "Feng Gu, Julie Greensmith, Uwe Aickelin", "title": "Further Exploration of the Dendritic Cell Algorithm: Antigen Multiplier\n  and Time Windows", "comments": "12 pages, 3 figures, 3 tables, 7th International Conference on\n  Artificial Immune Systems (ICARIS 2008), Phuket, Thailand", "journal-ref": "Proceedings of the 7th International Conference on Artificial\n  Immune Systems (ICARIS 2008), Phuket, Thailand, 2008, 142-153", "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an immune-inspired algorithm, the Dendritic Cell Algorithm (DCA), produces\npromising performances in the field of anomaly detection. This paper presents\nthe application of the DCA to a standard data set, the KDD 99 data set. The\nresults of different implementation versions of the DXA, including the antigen\nmultiplier and moving time windows are reported. The real-valued Negative\nSelection Algorithm (NSA) using constant-sized detectors and the C4.5 decision\ntree algorithm are used, to conduct a baseline comparison. The results suggest\nthat the DCA is applicable to KDD 99 data set, and the antigen multiplier and\nmoving time windows have the same effect on the DCA for this particular data\nset. The real-valued NSA with constant-sized detectors is not applicable to the\ndata set, and the C4.5 decision tree algorithm provides a benchmark of the\nclassification performance for this data set.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 11:43:55 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Gu", "Feng", ""], ["Greensmith", "Julie", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1003.0339", "submitter": "Uwe Aickelin", "authors": "Jamie Twycross, Uwe Aickelin", "title": "libtissue - implementing innate immunity", "comments": "8 pages, 4 tables, 5 figures, Workshop on Artificial Immune Systems\n  and Immune System Modelling (AISB06), Bristol, UK", "journal-ref": "499-506, Proceedings of the IEEE Congress on Evolutionary\n  Computation (CEC2006), Vancouver, Canada, 2006", "doi": "10.1109/CEC.2006.1688351", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous paper the authors argued the case for incorporating ideas from\ninnate immunity into articficial immune systems (AISs) and presented an outline\nfor a conceptual framework for such systems. A number of key general properties\nobserved in the biological innate and adaptive immune systems were hughlighted,\nand how such properties might be instantiated in artificial systems was\ndiscussed in detail. The next logical step is to take these ideas and build a\nsoftware system with which AISs with these properties can be implemented and\nexperimentally evaluated. This paper reports on the results of that step - the\nlibtissue system.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 13:09:26 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2010 14:59:58 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Twycross", "Jamie", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1003.0358", "submitter": "Dan Ciresan", "authors": "Dan Claudiu Ciresan, Ueli Meier, Luca Maria Gambardella, Juergen\n  Schmidhuber", "title": "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition", "comments": "14 pages, 2 figures, 4 listings", "journal-ref": "Neural Computation, Volume 22, Number 12, December 2010", "doi": "10.1162/NECO_a_00052", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good old on-line back-propagation for plain multi-layer perceptrons yields a\nvery low 0.35% error rate on the famous MNIST handwritten digits benchmark. All\nwe need to achieve this best result so far are many hidden layers, many neurons\nper layer, numerous deformed training images, and graphics cards to greatly\nspeed up learning.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 14:32:11 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Ciresan", "Dan Claudiu", ""], ["Meier", "Ueli", ""], ["Gambardella", "Luca Maria", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1003.0404", "submitter": "Uwe Aickelin", "authors": "Feng Gu, Julie Greensmith, Uwe Aickelin", "title": "Exploration Of The Dendritic Cell Algorithm Using The Duration Calculus", "comments": "13 pages, 2 figures, 8th International Conference on Artificial\n  Immune Systems (ICARIS 2009), Lecture Notes in Computer Science 5666, York,\n  UK", "journal-ref": "Proceedings of 8th International Conference on Artificial Immune\n  Systems (ICARIS 2009), Lecture Notes in Computer Science 5666, York, UK", "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the newest members in Artificial Immune Systems (AIS), the\nDendritic Cell Algorithm (DCA) has been applied to a range of problems. These\napplications mainly belong to the field of anomaly detection. However,\nreal-time detection, a new challenge to anomaly detection, requires improvement\non the real-time capability of the DCA. To assess such capability, formal\nmethods in the research of rea-time systems can be employed. The findings of\nthe assessment can provide guideline for the future development of the\nalgorithm. Therefore, in this paper we use an interval logic based method,\nnamed the Duration Calculus (DC), to specify a simplified single-cell model of\nthe DCA. Based on the DC specifications with further induction, we find that\neach individual cell in the DCA can perform its function as a detector in\nreal-time. Since the DCA can be seen as many such cells operating in parallel,\nit is potentially capable of performing real-time detection. However, the\nanalysis process of the standard DCA constricts its real-time capability. As a\nresult, we conclude that the analysis process of the standard DCA should be\nreplaced by a real-time analysis component, which can perform periodic analysis\nfor the purpose of real-time detection.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 16:57:54 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Gu", "Feng", ""], ["Greensmith", "Julie", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1003.0590", "submitter": "Eduard Babkin", "authors": "Sami Al-Maqtari (LITIS), Habib Abdulrab (LITIS), Eduard Babkin (LITIS)", "title": "A new model for solution of complex distributed constrained problems", "comments": null, "journal-ref": "Computer Systems and Applications, ACS/IEEE International\n  Conference on 0 (2009) 660-667", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe an original computational model for solving\ndifferent types of Distributed Constraint Satisfaction Problems (DCSP). The\nproposed model is called Controller-Agents for Constraints Solving (CACS). This\nmodel is intended to be used which is an emerged field from the integration\nbetween two paradigms of different nature: Multi-Agent Systems (MAS) and the\nConstraint Satisfaction Problem paradigm (CSP) where all constraints are\ntreated in central manner as a black-box. This model allows grouping\nconstraints to form a subset that will be treated together as a local problem\ninside the controller. Using this model allows also handling non-binary\nconstraints easily and directly so that no translating of constraints into\nbinary ones is needed. This paper presents the implementation outlines of a\nprototype of DCSP solver, its usage methodology and overview of the CACS\napplication for timetabling problems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 13:40:43 GMT"}], "update_date": "2010-03-03", "authors_parsed": [["Al-Maqtari", "Sami", "", "LITIS"], ["Abdulrab", "Habib", "", "LITIS"], ["Babkin", "Eduard", "", "LITIS"]]}, {"id": "1003.0617", "submitter": "Louise Dennis Dr", "authors": "Louise A. Dennis, Michael Fisher, Nicholas Lincoln, Alexei Lisitsa,\n  Sandor M. Veres", "title": "Agent Based Approaches to Engineering Autonomous Space Software", "comments": "3 pages, 1 Figure, Formal Methods in Aerospace", "journal-ref": "EPTCS 20, 2010, pp. 63-67", "doi": "10.4204/EPTCS.20.6", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to the engineering of space software such as satellite\ncontrol systems are based around the development of feedback controllers using\npackages such as MatLab's Simulink toolbox. These provide powerful tools for\nengineering real time systems that adapt to changes in the environment but are\nlimited when the controller itself needs to be adapted.\n  We are investigating ways in which ideas from temporal logics and agent\nprogramming can be integrated with the use of such control systems to provide a\nmore powerful layer of autonomous decision making. This paper will discuss our\ninitial approaches to the engineering of such systems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 15:38:48 GMT"}], "update_date": "2010-04-01", "authors_parsed": [["Dennis", "Louise A.", ""], ["Fisher", "Michael", ""], ["Lincoln", "Nicholas", ""], ["Lisitsa", "Alexei", ""], ["Veres", "Sandor M.", ""]]}, {"id": "1003.0659", "submitter": "Evan Ettinger", "authors": "Evan Ettinger and Yoav Freund", "title": "Particle Filtering on the Audio Localization Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel particle filtering algorithm for tracking a moving sound\nsource using a microphone array. If there are N microphones in the array, we\ntrack all $N \\choose 2$ delays with a single particle filter over time. Since\nit is known that tracking in high dimensions is rife with difficulties, we\ninstead integrate into our particle filter a model of the low dimensional\nmanifold that these delays lie on. Our manifold model is based off of work on\nmodeling low dimensional manifolds via random projection trees [1]. In\naddition, we also introduce a new weighting scheme to our particle filtering\nalgorithm based on recent advancements in online learning. We show that our\nnovel TDOA tracking algorithm that integrates a manifold model can greatly\noutperform standard particle filters on this audio tracking task.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 19:50:38 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2010 21:40:35 GMT"}], "update_date": "2010-03-03", "authors_parsed": [["Ettinger", "Evan", ""], ["Freund", "Yoav", ""]]}, {"id": "1003.0746", "submitter": "Raphael Chenouard", "authors": "Raphael Chenouard (LINA), Fr\\'ed\\'eric Jouault (INRIA - EMN)", "title": "Automatically Discovering Hidden Transformation Chaining Constraints", "comments": null, "journal-ref": "ACM/IEEE 12th International Conference on Model Driven Engineering\n  Languages and Systems, Denver : United States (2009)", "doi": "10.1007/978-3-642-04425-0_8", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model transformations operate on models conforming to precisely defined\nmetamodels. Consequently, it often seems relatively easy to chain them: the\noutput of a transformation may be given as input to a second one if metamodels\nmatch. However, this simple rule has some obvious limitations. For instance, a\ntransformation may only use a subset of a metamodel. Therefore, chaining\ntransformations appropriately requires more information. We present here an\napproach that automatically discovers more detailed information about actual\nchaining constraints by statically analyzing transformations. The objective is\nto provide developers who decide to chain transformations with more data on\nwhich to base their choices. This approach has been successfully applied to the\ncase of a library of endogenous transformations. They all have the same source\nand target metamodel but have some hidden chaining constraints. In such a case,\nthe simple metamodel matching rule given above does not provide any useful\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 08:04:45 GMT"}], "update_date": "2010-03-04", "authors_parsed": [["Chenouard", "Raphael", "", "LINA"], ["Jouault", "Fr\u00e9d\u00e9ric", "", "INRIA - EMN"]]}, {"id": "1003.0789", "submitter": "Uwe Aickelin", "authors": "Julie Greensmith, Uwe Aickelin, Gianni Tedesco", "title": "Information Fusion for Anomaly Detection with the Dendritic Cell\n  Algorithm", "comments": "21 pages, 17 figures, Information Fusion", "journal-ref": "Information Fusion, 11 (1), 21-34, 2010", "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dendritic cells are antigen presenting cells that provide a vital link\nbetween the innate and adaptive immune system, providing the initial detection\nof pathogenic invaders. Research into this family of cells has revealed that\nthey perform information fusion which directs immune responses. We have derived\na Dendritic Cell Algorithm based on the functionality of these cells, by\nmodelling the biological signals and differentiation pathways to build a\ncontrol mechanism for an artificial immune system. We present algorithmic\ndetails in addition to experimental results, when the algorithm was applied to\nanomaly detection for the detection of port scans. The results show the\nDendritic Cell Algorithm is sucessful at detecting port scans.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 12:04:01 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Greensmith", "Julie", ""], ["Aickelin", "Uwe", ""], ["Tedesco", "Gianni", ""]]}, {"id": "1003.1256", "submitter": "Uwe Aickelin", "authors": "Gianni Tedesco, Jamie Twycross, Uwe Aickelin", "title": "Integrating Innate and Adaptive Immunity for Intrusion Detection", "comments": "10 pages, 3 figures, 1 table", "journal-ref": "Proceedings of the 5th International Conference on Artificial\n  Immune Systems (ICARIS2006), Lecture Notes in Computer Science 4163, 193-202,\n  2006", "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network Intrusion Detection Systems (NDIS) monitor a network with the aim of\ndiscerning malicious from benign activity on that network. While a wide range\nof approaches have met varying levels of success, most IDS's rely on having\naccess to a database of known attack signatures which are written by security\nexperts. Nowadays, in order to solve problems with false positive alters,\ncorrelation algorithms are used to add additional structure to sequences of IDS\nalerts. However, such techniques are of no help in discovering novel attacks or\nvariations of known attacks, something the human immune system (HIS) is capable\nof doing in its own specialised domain. This paper presents a novel immune\nalgorithm for application to an intrusion detection problem. The goal is to\ndiscover packets containing novel variations of attacks covered by an existing\nsignature base.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2010 12:51:22 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Tedesco", "Gianni", ""], ["Twycross", "Jamie", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1003.1343", "submitter": "Gregory Benford", "authors": "David H. Wolpert and Gregory Benford", "title": "What does Newcomb's paradox teach us?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Newcomb's paradox you choose to receive either the contents of a\nparticular closed box, or the contents of both that closed box and another one.\nBefore you choose, a prediction algorithm deduces your choice, and fills the\ntwo boxes based on that deduction. Newcomb's paradox is that game theory\nappears to provide two conflicting recommendations for what choice you should\nmake in this scenario. We analyze Newcomb's paradox using a recent extension of\ngame theory in which the players set conditional probability distributions in a\nBayes net. We show that the two game theory recommendations in Newcomb's\nscenario have different presumptions for what Bayes net relates your choice and\nthe algorithm's prediction. We resolve the paradox by proving that these two\nBayes nets are incompatible. We also show that the accuracy of the algorithm's\nprediction, the focus of much previous work, is irrelevant. In addition we show\nthat Newcomb's scenario only provides a contradiction between game theory's\nexpected utility and dominance principles if one is sloppy in specifying the\nunderlying Bayes net. We also show that Newcomb's paradox is time-reversal\ninvariant; both the paradox and its resolution are unchanged if the algorithm\nmakes its `prediction' after you make your choice rather than before.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2010 00:52:29 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Wolpert", "David H.", ""], ["Benford", "Gregory", ""]]}, {"id": "1003.1493", "submitter": "Rdv Ijcsis", "authors": "Mariana Maceiras Cabrera, Ernesto Ocampo Edye", "title": "Integration of Rule Based Expert Systems and Case Based Reasoning in an\n  Acute Bacterial Meningitis Clinical Decision Support System", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This article presents the results of the research carried out on the\ndevelopment of a medical diagnostic system applied to the Acute Bacterial\nMeningitis, using the Case Based Reasoning methodology. The research was\nfocused on the implementation of the adaptation stage, from the integration of\nCase Based Reasoning and Rule Based Expert Systems. In this adaptation stage we\nuse a higher level RBC that stores and allows reutilizing change experiences,\ncombined with a classic rule-based inference engine. In order to take into\naccount the most evident clinical situation, a pre-diagnosis stage is\nimplemented using a rule engine that, given an evident situation, emits the\ncorresponding diagnosis and avoids the complete process.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2010 17:09:49 GMT"}], "update_date": "2010-04-28", "authors_parsed": [["Cabrera", "Mariana Maceiras", ""], ["Edye", "Ernesto Ocampo", ""]]}, {"id": "1003.1504", "submitter": "Rdv Ijcsis", "authors": "Saba Bashir, Farhan Hassan Khan, M.Younus Javed, Aihab Khan, Malik\n  Sikandar Hayat Khiyal", "title": "Indexer Based Dynamic Web Services Discovery", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recent advancement in web services plays an important role in business to\nbusiness and business to consumer interaction. Discovery mechanism is not only\nused to find a suitable service but also provides collaboration between service\nproviders and consumers by using standard protocols. A static web service\ndiscovery mechanism is not only time consuming but requires continuous human\ninteraction. This paper proposed an efficient dynamic web services discovery\nmechanism that can locate relevant and updated web services from service\nregistries and repositories with timestamp based on indexing value and\ncategorization for faster and efficient discovery of service. The proposed\nprototype focuses on quality of service issues and introduces concept of local\ncache, categorization of services, indexing mechanism, CSP (Constraint\nSatisfaction Problem) solver, aging and usage of translator. Performance of\nproposed framework is evaluated by implementing the algorithm and correctness\nof our method is shown. The results of proposed framework shows greater\nperformance and accuracy in dynamic discovery mechanism of web services\nresolving the existing issues of flexibility, scalability, based on quality of\nservice, and discovers updated and most relevant services with ease of usage.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2010 18:04:28 GMT"}], "update_date": "2010-04-28", "authors_parsed": [["Bashir", "Saba", ""], ["Khan", "Farhan Hassan", ""], ["Javed", "M. Younus", ""], ["Khan", "Aihab", ""], ["Khiyal", "Malik Sikandar Hayat", ""]]}, {"id": "1003.1588", "submitter": "Umberto Straccia", "authors": "Fernando Bobillo and Felix Bou and Umberto Straccia", "title": "On the Failure of the Finite Model Property in some Fuzzy Description\n  Logics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy Description Logics (DLs) are a family of logics which allow the\nrepresentation of (and the reasoning with) structured knowledge affected by\nvagueness. Although most of the not very expressive crisp DLs, such as ALC,\nenjoy the Finite Model Property (FMP), this is not the case once we move into\nthe fuzzy case. In this paper we show that if we allow arbitrary knowledge\nbases, then the fuzzy DLs ALC under Lukasiewicz and Product fuzzy logics do not\nverify the FMP even if we restrict to witnessed models; in other words, finite\nsatisfiability and witnessed satisfiability are different for arbitrary\nknowledge bases. The aim of this paper is to point out the failure of FMP\nbecause it affects several algorithms published in the literature for reasoning\nunder fuzzy ALC.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2010 10:18:12 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Bobillo", "Fernando", ""], ["Bou", "Felix", ""], ["Straccia", "Umberto", ""]]}, {"id": "1003.1598", "submitter": "Uwe Aickelin", "authors": "Jamie Twycross, Uwe Aickelin", "title": "Information Fusion in the Immune System", "comments": "10 pages, 6 tables, 6 figures, Information Fusion", "journal-ref": "Information Fusion, 11 (1), 35-44, 2010", "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biologically-inspired methods such as evolutionary algorithms and neural\nnetworks are proving useful in the field of information fusion. Artificial\nImmune Systems (AISs) are a biologically-inspired approach which take\ninspiration from the biological immune system. Interestingly, recent research\nhas show how AISs which use multi-level information sources as input data can\nbe used to build effective algorithms for real time computer intrusion\ndetection. This research is based on biological information fusion mechanisms\nused by the human immune system and as such might be of interest to the\ninformation fusion community. The aim of this paper is to present a summary of\nsome of the biological information fusion mechanisms seen in the human immune\nsystem, and of how these mechanisms have been implemented as AISs\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2010 11:18:01 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Twycross", "Jamie", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1003.1658", "submitter": "Zolt\\'an K\\'asa", "authors": "Agnes Achs", "title": "A multivalued knowledge-base model", "comments": null, "journal-ref": "Acta Univ. Sapientiae, Informatica, 2,1(2010) 51-79", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic aim of our study is to give a possible model for handling uncertain\ninformation. This model is worked out in the framework of DATALOG. At first the\nconcept of fuzzy Datalog will be summarized, then its extensions for\nintuitionistic- and interval-valued fuzzy logic is given and the concept of\nbipolar fuzzy Datalog is introduced. Based on these ideas the concept of\nmultivalued knowledge-base will be defined as a quadruple of any background\nknowledge; a deduction mechanism; a connecting algorithm, and a function set of\nthe program, which help us to determine the uncertainty levels of the results.\nAt last a possible evaluation strategy is given.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2010 16:00:28 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2010 13:08:57 GMT"}, {"version": "v3", "created": "Sun, 21 Mar 2010 19:27:10 GMT"}], "update_date": "2010-04-08", "authors_parsed": [["Achs", "Agnes", ""]]}, {"id": "1003.1954", "submitter": "D\\'avid P\\'al", "authors": "D\\'avid P\\'al, Barnab\\'as P\\'oczos, Csaba Szepesv\\'ari", "title": "Estimation of R\\'enyi Entropy and Mutual Information Based on\n  Generalized Nearest-Neighbor Graphs", "comments": "to appear at NIPS 2010 (Neural Information Processing Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple and computationally efficient nonparametric estimators of\nR\\'enyi entropy and mutual information based on an i.i.d. sample drawn from an\nunknown, absolutely continuous distribution over $\\R^d$. The estimators are\ncalculated as the sum of $p$-th powers of the Euclidean lengths of the edges of\nthe `generalized nearest-neighbor' graph of the sample and the empirical copula\nof the sample respectively. For the first time, we prove the almost sure\nconsistency of these estimators and upper bounds on their rates of convergence,\nthe latter of which under the assumption that the density underlying the sample\nis Lipschitz continuous. Experiments demonstrate their usefulness in\nindependent subspace analysis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 19:01:51 GMT"}, {"version": "v2", "created": "Mon, 25 Oct 2010 23:18:35 GMT"}], "update_date": "2010-10-27", "authors_parsed": [["P\u00e1l", "D\u00e1vid", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1003.2429", "submitter": "Jure Leskovec", "authors": "Jure Leskovec, Daniel Huttenlocher, Jon Kleinberg", "title": "Predicting Positive and Negative Links in Online Social Networks", "comments": null, "journal-ref": "WWW 2010: ACM WWW International conference on World Wide Web, 2010", "doi": null, "report-no": null, "categories": "physics.soc-ph cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online social networks in which relationships can be either positive\n(indicating relations such as friendship) or negative (indicating relations\nsuch as opposition or antagonism). Such a mix of positive and negative links\narise in a variety of online settings; we study datasets from Epinions,\nSlashdot and Wikipedia. We find that the signs of links in the underlying\nsocial networks can be predicted with high accuracy, using models that\ngeneralize across this diverse range of sites. These models provide insight\ninto some of the fundamental principles that drive the formation of signed\nlinks in networks, shedding light on theories of balance and status from social\npsychology; they also suggest social computing applications by which the\nattitude of one user toward another can be estimated from evidence provided by\ntheir relationships with other members of the surrounding social network.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2010 21:27:11 GMT"}], "update_date": "2010-03-15", "authors_parsed": [["Leskovec", "Jure", ""], ["Huttenlocher", "Daniel", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1003.2586", "submitter": "Francesca A. Lisi", "authors": "Francesca A. Lisi", "title": "Inductive Logic Programming in Databases: from Datalog to DL+log", "comments": "30 pages, 3 figures, 2 tables.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address an issue that has been brought to the attention of\nthe database community with the advent of the Semantic Web, i.e. the issue of\nhow ontologies (and semantics conveyed by them) can help solving typical\ndatabase problems, through a better understanding of KR aspects related to\ndatabases. In particular, we investigate this issue from the ILP perspective by\nconsidering two database problems, (i) the definition of views and (ii) the\ndefinition of constraints, for a database whose schema is represented also by\nmeans of an ontology. Both can be reformulated as ILP problems and can benefit\nfrom the expressive and deductive power of the KR framework DL+log. We\nillustrate the application scenarios by means of examples. Keywords: Inductive\nLogic Programming, Relational Databases, Ontologies, Description Logics, Hybrid\nKnowledge Representation and Reasoning Systems. Note: To appear in Theory and\nPractice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2010 17:40:43 GMT"}], "update_date": "2010-03-15", "authors_parsed": [["Lisi", "Francesca A.", ""]]}, {"id": "1003.2641", "submitter": "Frederic Dambreville", "authors": "Fr\\'ed\\'eric Dambreville", "title": "Release ZERO.0.1 of package RefereeToolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RefereeToolbox is a java package implementing combination operators for\nfusing evidences. It is downloadable from:\nhttp://refereefunction.fredericdambreville.com/releases RefereeToolbox is based\non an interpretation of the fusion rules by means of Referee Functions. This\napproach implies a dissociation between the definition of the combination and\nits actual implementation, which is common to all referee-based combinations.\nAs a result, RefereeToolbox is designed with the aim to be generic and\nevolutive.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2010 21:25:10 GMT"}], "update_date": "2010-03-16", "authors_parsed": [["Dambreville", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1003.2700", "submitter": "Agnieszka Lawrynowicz", "authors": "Joanna Jozefowska, Agnieszka Lawrynowicz, Tomasz Lukaszewski", "title": "The role of semantics in mining frequent patterns from knowledge bases\n  in description logics with rules", "comments": "40 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": "RA-01/09", "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for mining frequent patterns in a language that\ncombines both Semantic Web ontologies and rules. In particular we consider the\nsetting of using a language that combines description logics with DL-safe\nrules. This setting is important for the practical application of data mining\nto the Semantic Web. We focus on the relation of the semantics of the\nrepresentation formalism to the task of frequent pattern discovery, and for the\ncore of our method, we propose an algorithm that exploits the semantics of the\ncombined knowledge base. We have developed a proof-of-concept data mining\nimplementation of this. Using this we have empirically shown that using the\ncombined knowledge base to perform semantic tests can make data mining faster\nby pruning useless candidate patterns before their evaluation. We have also\nshown that the quality of the set of patterns produced may be improved: the\npatterns are more compact, and there are fewer patterns. We conclude that\nexploiting the semantics of a chosen representation formalism is key to the\ndesign and application of (onto-)relational frequent pattern discovery methods.\nNote: To appear in Theory and Practice of Logic Programming (TPLP)\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2010 12:47:46 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2010 18:20:40 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Jozefowska", "Joanna", ""], ["Lawrynowicz", "Agnieszka", ""], ["Lukaszewski", "Tomasz", ""]]}, {"id": "1003.3082", "submitter": "Achmad Benny Mutiara", "authors": "L.Y. Banowosari, I.W.S. Wicaksana, and A.B. Mutiara", "title": "Agreement Maintenance Based on Schema and Ontology Change in P2P\n  Environment", "comments": "6 pages, the *11th Annual International Seminar on Global Meltdown or\n  Recession: India vis-\\`a-vis the rest of the world, January 4-5, 2010 at New\n  Delhi.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concern about developing a semantic agreement maintenance\nmethod based on semantic distance by calculating the change of local schema or\nontology. This approach is important in dynamic and autonomous environment, in\nwhich the current approach assumed that agreement or mapping in static\nenvironment. The contribution of this research is to develop a framework based\non semantic agreement maintenance approach for P2P environment. This framework\nbased on two level hybrid P2P model architecture, which consist of two peer\ntype: (1) super peer that use to register and manage the other peers, and (2)\nsimple peer, as a simple peer, it exports and shares its contents with others.\nThis research develop a model to maintain the semantic agreement in P2P\nenvironment, so the current approach which does not have the mechanism to know\nthe change, since it assumed that ontology and local schema are in the static\ncondition, and it is different in dynamic condition. The main issues are how to\ncalculate the change of local schema or common ontology and the calculation\nresult is used to determine which algorithm in maintaining the agreement. The\nexperiment on the job matching domain in Indonesia have been done to show how\nfar the performance of the approach. From the experiment, the main result are\n(i) the more change so the F-measure value tend to be decreased, (ii) there is\nno significant different in F-measure value for various modification type (add,\ndelete, rename), and (iii) the correct choice of algorithm would improve the\nF-measure value.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2010 05:28:48 GMT"}], "update_date": "2010-03-17", "authors_parsed": [["Banowosari", "L. Y.", ""], ["Wicaksana", "I. W. S.", ""], ["Mutiara", "A. B.", ""]]}, {"id": "1003.3766", "submitter": "Uwe Aickelin", "authors": "Peer-Olaf Siebers, Uwe Aickelin, Helen Celia, Chris Clegg", "title": "Modelling and simulating retail management practices: a first approach", "comments": "33 pages, INFORMS Simulation Society Workshop,", "journal-ref": "International Journal of Simulation and Process Modelling 5 (3),\n  215-232 , 2009", "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent systems offer a new and exciting way of understanding the world\nof work. We apply agent-based modeling and simulation to investigate a set of\nproblems in a retail context. Specifically, we are working to understand the\nrelationship between people management practices on the shop-floor and retail\nperformance. Despite the fact we are working within a relatively novel and\ncomplex domain, it is clear that using an agent-based approach offers great\npotential for improving organizational capabilities in the future. Our\nmulti-disciplinary research team has worked closely with one of the UK's top\nten retailers to collect data and build an understanding of shop-floor\noperations and the key actors in a department (customers, staff, and managers).\nBased on this case study we have built and tested our first version of a retail\nbranch agent-based simulation model where we have focused on how we can\nsimulate the effects of people management practices on customer satisfaction\nand sales. In our experiments we have looked at employee development and\ncashier empowerment as two examples of shop floor management practices. In this\npaper we describe the underlying conceptual ideas and the features of our\nsimulation model. We present a selection of experiments we have conducted in\norder to validate our simulation model and to show its potential for answering\n\"what-if\" questions in a retail context. We also introduce a novel performance\nmeasure which we have created to quantify customers' satisfaction with service,\nbased on their individual shopping experiences.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2010 11:01:53 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""], ["Celia", "Helen", ""], ["Clegg", "Chris", ""]]}, {"id": "1003.3767", "submitter": "Uwe Aickelin", "authors": "Peer-Olaf Siebers, Uwe Aickelin, Helen Celia, Chris Clegg", "title": "Multi-Agent Simulation and Management Practices", "comments": "19 pages, 3 figures, Encyclopedia of Decision Making and Decision\n  Support Technologies", "journal-ref": "Encyclopedia of Decision Making and Decision Support Technologies,\n  645-652, 2008", "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent agents offer a new and exciting way of understanding the world of\nwork. Agent-Based Simulation (ABS), one way of using intelligent agents,\ncarries great potential for progressing our understanding of management\npractices and how they link to retail performance. We have developed simulation\nmodels based on research by a multi-disciplinary team of economists, work\npsychologists and computer scientists. We will discuss our experiences of\nimplementing these concepts working with a well-known retail department store.\nThere is no doubt that management practices are linked to the performance of an\norganisation (Reynolds et al., 2005; Wall & Wood, 2005). Best practices have\nbeen developed, but when it comes down to the actual application of these\nguidelines considerable ambiguity remains regarding their effectiveness within\nparticular contexts (Siebers et al., forthcoming a). Most Operational Research\n(OR) methods can only be used as analysis tools once management practices have\nbeen implemented. Often they are not very useful for giving answers to\nspeculative 'what-if' questions, particularly when one is interested in the\ndevelopment of the system over time rather than just the state of the system at\na certain point in time. Simulation can be used to analyse the operation of\ndynamic and stochastic systems. ABS is particularly useful when complex\ninteractions between system entities exist, such as autonomous decision making\nor negotiation. In an ABS model the researcher explicitly describes the\ndecision process of simulated actors at the micro level. Structures emerge at\nthe macro level as a result of the actions of the agents and their interactions\nwith other agents and the environment. 3 We will show how ABS experiments can\ndeal with testing and optimising management practices such as training,\nempowerment or teamwork. Hence, questions such as \"will staff setting their own\nbreak times improve performance?\" can be investigated.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2010 11:02:04 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""], ["Celia", "Helen", ""], ["Clegg", "Chris", ""]]}, {"id": "1003.3775", "submitter": "Uwe Aickelin", "authors": "Adrian Adewunmi, Uwe Aickelin", "title": "Optimisation of a Crossdocking Distribution Centre Simulation Model", "comments": "6 pages, 7 tables, 2008 International Simulation Multi-Conference\n  (SCS), San Diego, USA", "journal-ref": "Proceedings of 2008 International Simulation Multi-Conference\n  (SCS), San Diego, USA, 434-439", "doi": null, "report-no": null, "categories": "cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on continuing research into the modelling of an order\npicking process within a Crossdocking distribution centre using Simulation\nOptimisation. The aim of this project is to optimise a discrete event\nsimulation model and to understand factors that affect finding its optimal\nperformance. Our initial investigation revealed that the precision of the\nselected simulation output performance measure and the number of replications\nrequired for the evaluation of the optimisation objective function through\nsimulation influences the ability of the optimisation technique. We\nexperimented with Common Random Numbers, in order to improve the precision of\nour simulation output performance measure, and intended to use the number of\nreplications utilised for this purpose as the initial number of replications\nfor the optimisation of our Crossdocking distribution centre simulation model.\nOur results demonstrate that we can improve the precision of our selected\nsimulation output performance measure value using Common Random Numbers at\nvarious levels of replications. Furthermore, after optimising our Crossdocking\ndistribution centre simulation model, we are able to achieve optimal\nperformance using fewer simulations runs for the simulation model which uses\nCommon Random Numbers as compared to the simulation model which does not use\nCommon Random Numbers.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2010 11:46:30 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Adewunmi", "Adrian", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1003.3821", "submitter": "Dan Guralnik", "authors": "Dan Guralnik", "title": "A Formal Approach to Modeling the Memory of a Living Organism", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a living organism as an observer of the evolution of its\nenvironment recording sensory information about the state space X of the\nenvironment in real time. Sensory information is sampled and then processed on\ntwo levels. On the biological level, the organism serves as an evaluation\nmechanism of the subjective relevance of the incoming data to the observer: the\nobserver assigns excitation values to events in X it could recognize using its\nsensory equipment. On the algorithmic level, sensory input is used for updating\na database, the memory of the observer whose purpose is to serve as a\ngeometric/combinatorial model of X, whose nodes are weighted by the excitation\nvalues produced by the evaluation mechanism. These values serve as a guidance\nsystem for deciding how the database should transform as observation data\nmounts. We define a searching problem for the proposed model and discuss the\nmodel's flexibility and its computational efficiency, as well as the\npossibility of implementing it as a dynamic network of neuron-like units. We\nshow how various easily observable properties of the human memory and thought\nprocess can be explained within the framework of this model. These include:\nreasoning (with efficiency bounds), errors, temporary and permanent loss of\ninformation. We are also able to define general learning problems in terms of\nthe new model, such as the language acquisition problem.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2010 15:56:37 GMT"}], "update_date": "2010-03-22", "authors_parsed": [["Guralnik", "Dan", ""]]}, {"id": "1003.3967", "submitter": "Andreas Krause", "authors": "Daniel Golovin and Andreas Krause", "title": "Adaptive Submodularity: Theory and Applications in Active Learning and\n  Stochastic Optimization", "comments": "60 pages, 6 figures. Version 5 addresses a flaw in the proof of\n  Theorem 13 identified by Nan and Saligrama (2017). The revision includes a\n  weaker version of Theorem 13, guaranteeing squared logarithmic approximation\n  under an additional strong adaptive submodularity condition. This condition\n  is met by all applications considered in the paper, as discussed in the\n  revised Sections 7, 8 and 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving stochastic optimization problems under partial observability, where\none needs to adaptively make decisions with uncertain outcomes, is a\nfundamental but notoriously difficult challenge. In this paper, we introduce\nthe concept of adaptive submodularity, generalizing submodular set functions to\nadaptive policies. We prove that if a problem satisfies this property, a simple\nadaptive greedy algorithm is guaranteed to be competitive with the optimal\npolicy. In addition to providing performance guarantees for both stochastic\nmaximization and coverage, adaptive submodularity can be exploited to\ndrastically speed up the greedy algorithm by using lazy evaluations. We\nillustrate the usefulness of the concept by giving several examples of adaptive\nsubmodular objectives arising in diverse applications including sensor\nplacement, viral marketing and active learning. Proving adaptive submodularity\nfor these problems allows us to recover existing results in these applications\nas special cases, improve approximation guarantees and handle natural\ngeneralizations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2010 04:06:22 GMT"}, {"version": "v2", "created": "Mon, 24 May 2010 02:25:04 GMT"}, {"version": "v3", "created": "Mon, 30 Aug 2010 23:00:54 GMT"}, {"version": "v4", "created": "Wed, 17 Oct 2012 03:04:19 GMT"}, {"version": "v5", "created": "Wed, 6 Dec 2017 08:21:07 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Golovin", "Daniel", ""], ["Krause", "Andreas", ""]]}, {"id": "1003.4140", "submitter": "Uwe Aickelin", "authors": "Feng Gu, Julie Greensmith, Uwe Aickelin", "title": "Integrating Real-Time Analysis With The Dendritic Cell Algorithm Through\n  Segmentation", "comments": "8 pages, 7 tables, 3 figures, Genetic and Evolutionary Computation\n  Conference (GECCO 2009), Montreal, Canada", "journal-ref": "Proceedings of Genetic and Evolutionary Computation Conference\n  (GECCO 2009), Montreal, Canada, 1203-1210", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an immune inspired algorithm, the Dendritic Cell Algorithm (DCA) has been\napplied to a range of problems, particularly in the area of intrusion\ndetection. Ideally, the intrusion detection should be performed in real-time,\nto continuously detect misuses as soon as they occur. Consequently, the\nanalysis process performed by an intrusion detection system must operate in\nreal-time or near-to real-time. The analysis process of the DCA is currently\nperformed offline, therefore to improve the algorithm's performance we suggest\nthe development of a real-time analysis component. The initial step of the\ndevelopment is to apply segmentation to the DCA. This involves segmenting the\ncurrent output of the DCA into slices and performing the analysis in various\nways. Two segmentation approaches are introduced and tested in this paper,\nnamely antigen based segmentation (ABS) and time based segmentation (TBS). The\nresults of the corresponding experiments suggest that applying segmentation\nproduces different and significantly better results in some cases, when\ncompared to the standard DCA without segmentation. Therefore, we conclude that\nthe segmentation is applicable to the DCA for the purpose of real-time\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 12:06:32 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Gu", "Feng", ""], ["Greensmith", "Julie", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1003.4141", "submitter": "Uwe Aickelin", "authors": "Mazlina Abdul Majid, Uwe Aickelin, Peer-Olaf Siebers", "title": "Investigating Output Accuracy for a Discrete Event Simulation Model and\n  an Agent Based Simulation Model", "comments": "5 pages, 4 figures, INFORMS Simulation Society Research Workshop", "journal-ref": "Proceedings of the INFORMS Simulation Society Research Workshop,\n  June 25-27, 2009, Warwick, UK, 101-105", "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate output accuracy for a Discrete Event Simulation\n(DES) model and Agent Based Simulation (ABS) model. The purpose of this\ninvestigation is to find out which of these simulation techniques is the best\none for modelling human reactive behaviour in the retail sector. In order to\nstudy the output accuracy in both models, we have carried out a validation\nexperiment in which we compared the results from our simulation models to the\nperformance of a real system. Our experiment was carried out using a large UK\ndepartment store as a case study. We had to determine an efficient\nimplementation of management policy in the store's fitting room using DES and\nABS. Overall, we have found that both simulation models were a good\nrepresentation of the real system when modelling human reactive behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 12:16:47 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Majid", "Mazlina Abdul", ""], ["Aickelin", "Uwe", ""], ["Siebers", "Peer-Olaf", ""]]}, {"id": "1003.4142", "submitter": "Uwe Aickelin", "authors": "Jungwon Kim, Julie Greensmith, Jamie Twycross, Uwe Aickelin", "title": "Malicious Code Execution Detection and Response Immune System inspired\n  by the Danger Theory", "comments": "4 pages, 1 table, Adaptive and Resilient Computing Security Workshop\n  (ARCS-05), Santa Fe, USA", "journal-ref": "Proceedings of Adaptive and Resilient Computing Security Workshop\n  (ARCS-05), Santa Fe, USA, 2005,", "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of system calls is one method employed by anomaly detection\nsystems to recognise malicious code execution. Similarities can be drawn\nbetween this process and the behaviour of certain cells belonging to the human\nimmune system, and can be applied to construct an artificial immune system. A\nrecently developed hypothesis in immunology, the Danger Theory, states that our\nimmune system responds to the presence of intruders through sensing molecules\nbelonging to those invaders, plus signals generated by the host indicating\ndanger and damage. We propose the incorporation of this concept into a\nresponsive intrusion detection system, where behavioural information of the\nsystem and running processes is combined with information regarding individual\nsystem calls.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 12:24:02 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Kim", "Jungwon", ""], ["Greensmith", "Julie", ""], ["Twycross", "Jamie", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1003.4145", "submitter": "Uwe Aickelin", "authors": "Amanda Whitbrook, Uwe Aickelin, Jonathan Garibaldi", "title": "Mimicking the Behaviour of Idiotypic AIS Robot Controllers Using\n  Probabilistic Systems", "comments": "7 pages, 2 figures, 6 tables, 13th World Multi-Conference on\n  Systemics, Cybernetics and Informatics: WMSCI 2009, Orlando, Florida, USA", "journal-ref": "Proceedings of the 13th World Multi-Conference on Systemics,\n  Cybernetics and Informatics: WMSCI 2009, Orlando, Florida, USA, 272-278", "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has shown that robot navigation systems that employ an\narchitecture based upon the idiotypic network theory of the immune system have\nan advantage over control techniques that rely on reinforcement learning only.\nThis is thought to be a result of intelligent behaviour selection on the part\nof the idiotypic robot. In this paper an attempt is made to imitate idiotypic\ndynamics by creating controllers that use reinforcement with a number of\ndifferent probabilistic schemes to select robot behaviour. The aims are to show\nthat the idiotypic system is not merely performing some kind of periodic random\nbehaviour selection, and to try to gain further insight into the processes that\ngovern the idiotypic mechanism. Trials are carried out using simulated Pioneer\nrobots that undertake navigation exercises. Results show that a scheme that\nboosts the probability of selecting highly-ranked alternative behaviours to 50%\nduring stall conditions comes closest to achieving the properties of the\nidiotypic system, but remains unable to match it in terms of all round\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 12:32:30 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Whitbrook", "Amanda", ""], ["Aickelin", "Uwe", ""], ["Garibaldi", "Jonathan", ""]]}, {"id": "1003.4196", "submitter": "Uwe Aickelin", "authors": "Peer-Olaf Siebers, Galina Sherman, Uwe Aickelin", "title": "Development of a Cargo Screening Process Simulator: A First Approach", "comments": "10 pages, 7 figures, 6th International Mediterranean Modeling\n  Multiconference (EMSS 2009), Tenerife, Spain", "journal-ref": "Proceedings of the 6th International Mediterranean Modeling\n  Multiconference (EMSS 2009), Tenerife, Spain, 200-209", "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of current cargo screening processes at sea and air ports is\nlargely unknown as few benchmarks exists against which they could be measured.\nSome manufacturers provide benchmarks for individual sensors but we found no\nbenchmarks that take a holistic view of the overall screening procedures and no\nbenchmarks that take operator variability into account. Just adding up\nresources and manpower used is not an effective way for assessing systems where\nhuman decision-making and operator compliance to rules play a vital role. Our\naim is to develop a decision support tool (cargo-screening system simulator)\nthat will map the right technology and manpower to the right commodity-threat\ncombination in order to maximise detection rates. In this paper we present our\nideas for developing such a system and highlight the research challenges we\nhave identified. Then we introduce our first case study and report on the\nprogress we have made so far.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 15:32:48 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Siebers", "Peer-Olaf", ""], ["Sherman", "Galina", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1003.4781", "submitter": "Xu Miao", "authors": "Xu Miao, Rajesh P.N. Rao", "title": "Large Margin Boltzmann Machines and Large Margin Sigmoid Belief Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "UW-CSE-09-04-01", "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current statistical models for structured prediction make simplifying\nassumptions about the underlying output graph structure, such as assuming a\nlow-order Markov chain, because exact inference becomes intractable as the\ntree-width of the underlying graph increases. Approximate inference algorithms,\non the other hand, force one to trade off representational power with\ncomputational efficiency. In this paper, we propose two new types of\nprobabilistic graphical models, large margin Boltzmann machines (LMBMs) and\nlarge margin sigmoid belief networks (LMSBNs), for structured prediction.\nLMSBNs in particular allow a very fast inference algorithm for arbitrary graph\nstructures that runs in polynomial time with a high probability. This\nprobability is data-distribution dependent and is maximized in learning. The\nnew approach overcomes the representation-efficiency trade-off in previous\nmodels and allows fast structured prediction with complicated graph structures.\nWe present results from applying a fully connected model to multi-label scene\nclassification and demonstrate that the proposed approach can yield significant\nperformance gains over current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 02:21:11 GMT"}], "update_date": "2010-03-26", "authors_parsed": [["Miao", "Xu", ""], ["Rao", "Rajesh P. N.", ""]]}, {"id": "1003.5173", "submitter": "Charles Robert", "authors": "Charles A. B. Robert (LORIA)", "title": "LEXSYS: Architecture and Implication for Intelligent Agent systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LEXSYS, (Legume Expert System) was a project conceived at IITA (International\nInstitute of Tropical Agriculture) Ibadan Nigeria. It was initiated by the\nCOMBS (Collaborative Group on Maize-Based Systems Research in the 1990. It was\nmeant for a general framework for characterizing on-farm testing for technology\ndesign for sustainable cereal-based cropping system. LEXSYS is not a true\nexpert system as the name would imply, but simply a user-friendly information\nsystem. This work is an attempt to give a formal representation of the existing\nsystem and then present areas where intelligent agent can be applied.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2010 16:01:52 GMT"}], "update_date": "2010-03-29", "authors_parsed": [["Robert", "Charles A. B.", "", "LORIA"]]}, {"id": "1003.5305", "submitter": "David Tolpin", "authors": "David Tolpin and Solomon Eyal Shimony", "title": "Rational Value of Information Estimation for Measurement Selection", "comments": "7 pages, 2 figures, presented at URPDM2010; plots fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing value of information (VOI) is a crucial task in various aspects of\ndecision-making under uncertainty, such as in meta-reasoning for search; in\nselecting measurements to make, prior to choosing a course of action; and in\nmanaging the exploration vs. exploitation tradeoff. Since such applications\ntypically require numerous VOI computations during a single run, it is\nessential that VOI be computed efficiently. We examine the issue of anytime\nestimation of VOI, as frequently it suffices to get a crude estimate of the\nVOI, thus saving considerable computational resources. As a case study, we\nexamine VOI estimation in the measurement selection problem. Empirical\nevaluation of the proposed scheme in this domain shows that computational\nresources can indeed be significantly reduced, at little cost in expected\nrewards achieved in the overall decision problem.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2010 14:56:16 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2010 08:52:06 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Tolpin", "David", ""], ["Shimony", "Solomon Eyal", ""]]}, {"id": "1003.5899", "submitter": "Agnieszka Patyk", "authors": "Agnieszka Patyk", "title": "Geometric Algebra Model of Distributed Representations", "comments": "30 pages, 19 figures", "journal-ref": null, "doi": "10.1007/978-1-84996-108-0_19", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formalism based on GA is an alternative to distributed representation models\ndeveloped so far --- Smolensky's tensor product, Holographic Reduced\nRepresentations (HRR) and Binary Spatter Code (BSC). Convolutions are replaced\nby geometric products, interpretable in terms of geometry which seems to be the\nmost natural language for visualization of higher concepts. This paper recalls\nthe main ideas behind the GA model and investigates recognition test results\nusing both inner product and a clipped version of matrix representation. The\ninfluence of accidental blade equality on recognition is also studied. Finally,\nthe efficiency of the GA model is compared to that of previously developed\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 19:03:43 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Patyk", "Agnieszka", ""]]}, {"id": "1003.5956", "submitter": "Lihong Li", "authors": "Lihong Li and Wei Chu and John Langford and Xuanhui Wang", "title": "Unbiased Offline Evaluation of Contextual-bandit-based News Article\n  Recommendation Algorithms", "comments": "10 pages, 7 figures, revised from the published version at the WSDM\n  2011 conference", "journal-ref": null, "doi": "10.1145/1935826.1935878", "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandit algorithms have become popular for online recommendation\nsystems such as Digg, Yahoo! Buzz, and news recommendation in general.\n\\emph{Offline} evaluation of the effectiveness of new algorithms in these\napplications is critical for protecting online user experiences but very\nchallenging due to their \"partial-label\" nature. Common practice is to create a\nsimulator which simulates the online environment for the problem at hand and\nthen run an algorithm against this simulator. However, creating simulator\nitself is often difficult and modeling bias is usually unavoidably introduced.\nIn this paper, we introduce a \\emph{replay} methodology for contextual bandit\nalgorithm evaluation. Different from simulator-based approaches, our method is\ncompletely data-driven and very easy to adapt to different applications. More\nimportantly, our method can provide provably unbiased evaluations. Our\nempirical results on a large-scale news article recommendation dataset\ncollected from Yahoo! Front Page conform well with our theoretical results.\nFurthermore, comparisons between our offline replay and online bucket\nevaluation of several contextual bandit algorithms show accuracy and\neffectiveness of our offline evaluation method.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2010 01:20:07 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2012 23:33:07 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Li", "Lihong", ""], ["Chu", "Wei", ""], ["Langford", "John", ""], ["Wang", "Xuanhui", ""]]}]