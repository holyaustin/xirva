[{"id": "1510.00087", "submitter": "Adrian Weller", "authors": "Adrian Weller and Justin Domke", "title": "Clamping Improves TRW and Mean Field Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the effect of clamping variables for approximate inference in\nundirected graphical models with pairwise relationships and discrete variables.\nFor any number of variable labels, we demonstrate that clamping and summing\napproximate sub-partition functions can lead only to a decrease in the\npartition function estimate for TRW, and an increase for the naive mean field\nmethod, in each case guaranteeing an improvement in the approximation and\nbound. We next focus on binary variables, add the Bethe approximation to\nconsideration and examine ways to choose good variables to clamp, introducing\nnew methods. We show the importance of identifying highly frustrated cycles,\nand of checking the singleton entropy of a variable. We explore the value of\nour methods by empirical analysis and draw lessons to guide practitioners.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 01:49:04 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Weller", "Adrian", ""], ["Domke", "Justin", ""]]}, {"id": "1510.00331", "submitter": "Tadahiro Taniguchi", "authors": "Tadahiro Taniguchi, Toshiaki Takano and Ryo Yoshino", "title": "Multimodal Hierarchical Dirichlet Process-based Active Perception", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an active perception method for recognizing object\ncategories based on the multimodal hierarchical Dirichlet process (MHDP). The\nMHDP enables a robot to form object categories using multimodal information,\ne.g., visual, auditory, and haptic information, which can be observed by\nperforming actions on an object. However, performing many actions on a target\nobject requires a long time. In a real-time scenario, i.e., when the time is\nlimited, the robot has to determine the set of actions that is most effective\nfor recognizing a target object. We propose an MHDP-based active perception\nmethod that uses the information gain (IG) maximization criterion and lazy\ngreedy algorithm. We show that the IG maximization criterion is optimal in the\nsense that the criterion is equivalent to a minimization of the expected\nKullback--Leibler divergence between a final recognition state and the\nrecognition state after the next set of actions. However, a straightforward\ncalculation of IG is practically impossible. Therefore, we derive an efficient\nMonte Carlo approximation method for IG by making use of a property of the\nMHDP. We also show that the IG has submodular and non-decreasing properties as\na set function because of the structure of the graphical model of the MHDP.\nTherefore, the IG maximization problem is reduced to a submodular maximization\nproblem. This means that greedy and lazy greedy algorithms are effective and\nhave a theoretical justification for their performance. We conducted an\nexperiment using an upper-torso humanoid robot and a second one using synthetic\ndata. The experimental results show that the method enables the robot to select\na set of actions that allow it to recognize target objects quickly and\naccurately. The results support our theoretical outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 17:36:42 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 15:45:44 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2016 17:13:57 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Taniguchi", "Tadahiro", ""], ["Takano", "Toshiaki", ""], ["Yoshino", "Ryo", ""]]}, {"id": "1510.00523", "submitter": "Takahisa Toda", "authors": "Takahisa Toda and Takehide Soh", "title": "Implementing Efficient All Solutions SAT Solvers", "comments": null, "journal-ref": "ACM Journal of Experimental Algorithmics, Vol. 21, No. 1, Article\n  1.12, 2016", "doi": "10.1145/2975585", "report-no": null, "categories": "cs.DS cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All solutions SAT (AllSAT for short) is a variant of propositional\nsatisfiability problem. Despite its significance, AllSAT has been relatively\nunexplored compared to other variants. We thus survey and discuss major\ntechniques of AllSAT solvers. We faithfully implement them and conduct\ncomprehensive experiments using a large number of instances and various types\nof solvers including one of the few public softwares. The experiments reveal\nsolver's characteristics. Our implemented solvers are made publicly available\nso that other researchers can easily develop their solver by modifying our\ncodes and compare it with existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 08:32:59 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Toda", "Takahisa", ""], ["Soh", "Takehide", ""]]}, {"id": "1510.00552", "submitter": "Daniele Ramazzotti", "authors": "Francesco Bonchi, Sara Hajian, Bud Mishra, Daniele Ramazzotti", "title": "Exposing the Probabilistic Causal Structure of Discrimination", "comments": null, "journal-ref": null, "doi": "10.1007/s41060-016-0040-z", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrimination discovery from data is an important task aiming at identifying\npatterns of illegal and unethical discriminatory activities against\nprotected-by-law groups, e.g., ethnic minorities. While any legally-valid proof\nof discrimination requires evidence of causality, the state-of-the-art methods\nare essentially correlation-based, albeit, as it is well known, correlation\ndoes not imply causation.\n  In this paper we take a principled causal approach to the data mining problem\nof discrimination detection in databases. Following Suppes' probabilistic\ncausation theory, we define a method to extract, from a dataset of historical\ndecision records, the causal structures existing among the attributes in the\ndata. The result is a type of constrained Bayesian network, which we dub\nSuppes-Bayes Causal Network (SBCN). Next, we develop a toolkit of methods based\non random walks on top of the SBCN, addressing different anti-discrimination\nlegal concepts, such as direct and indirect discrimination, group and\nindividual discrimination, genuine requirement, and favoritism. Our experiments\non real-world datasets confirm the inferential power of our approach in all\nthese different tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 10:31:29 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 08:38:16 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 21:10:10 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Bonchi", "Francesco", ""], ["Hajian", "Sara", ""], ["Mishra", "Bud", ""], ["Ramazzotti", "Daniele", ""]]}, {"id": "1510.00604", "submitter": "Laura Steinert", "authors": "Laura Steinert, Jens Hoefinghoff, Josef Pauli", "title": "Online Vision- and Action-Based Object Classification Using Both\n  Symbolic and Subsymbolic Knowledge Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If a robot is supposed to roam an environment and interact with objects, it\nis often necessary to know all possible objects in advance, so that a database\nwith models of all objects can be generated for visual identification. However,\nthis constraint cannot always be fulfilled. Due to that reason, a model based\nobject recognition cannot be used to guide the robot's interactions. Therefore,\nthis paper proposes a system that analyzes features of encountered objects and\nthen uses these features to compare unknown objects to already known ones. From\nthe resulting similarity appropriate actions can be derived. Moreover, the\nsystem enables the robot to learn object categories by grouping similar objects\nor by splitting existing categories. To represent the knowledge a hybrid form\nis used, consisting of both symbolic and subsymbolic representations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 14:08:36 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Steinert", "Laura", ""], ["Hoefinghoff", "Jens", ""], ["Pauli", "Josef", ""]]}, {"id": "1510.00819", "submitter": "Jai Manral", "authors": "Jai Manral", "title": "Intelligent Search Optimization using Artificial Fuzzy Logics", "comments": "Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information on the web is prodigious; searching relevant information is\ndifficult making web users to rely on search engines for finding relevant\ninformation on the web. Search engines index and categorize web pages according\nto their contents using crawlers and rank them accordingly. For given user\nquery they retrieve millions of webpages and display them to users according to\nweb-page rank. Every search engine has their own algorithms based on certain\nparameters for ranking web-pages. Search Engine Optimization (SEO) is that\ntechnique by which webmasters try to improve ranking of their websites by\noptimizing it according to search engines ranking parameters. It is the aim of\nthis research to identify the most popular SEO techniques used by search\nengines for ranking web-pages and to establish their importance for indexing\nand categorizing web data. The research tries to establish that using more SEO\nparameters in ranking algorithms helps in retrieving better search results thus\nincreasing user satisfaction.\n  In the accomplished research, a web based Meta search engine is proposed to\naggregates search results from different search engines and rank web-pages\nbased on new page ranking algorithm which will assign heuristic page rank to\nweb-pages based on SEO parameters such as title tag, Meta description, sitemap\netc. The research also provides insight into techniques which webmasters can\nuse for better ranking their websites in Google and Bing.\n  Initial results has shown that using certain SEO parameters in present\nranking algorithm helps in retrieving more useful results for user queries.\nThese results generated from Meta search engine outperformed existing search\nengines in terms of better retrieved search results and high precision.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 13:28:50 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Manral", "Jai", ""]]}, {"id": "1510.00878", "submitter": "Claudio Alexandre", "authors": "Claudio Alexandre and Jo\\~ao Balsa", "title": "Client Profiling for an Anti-Money Laundering System", "comments": "7 pages, 15 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a data mining approach for profiling bank clients in order to\nsupport the process of detection of anti-money laundering operations. We first\npresent the overall system architecture, and then focus on the relevant\ncomponent for this paper. We detail the experiments performed on real world\ndata from a financial institution, which allowed us to group clients in\nclusters and then generate a set of classification rules. We discuss the\nrelevance of the founded client profiles and of the generated classification\nrules. According to the defined overall agent-based architecture, these rules\nwill be incorporated in the knowledge base of the intelligent agents\nresponsible for the signaling of suspicious transactions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 22:31:58 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 16:57:57 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Alexandre", "Claudio", ""], ["Balsa", "Jo\u00e3o", ""]]}, {"id": "1510.01064", "submitter": "Jelena Bradic", "authors": "Alexander Hanbo Li and Jelena Bradic", "title": "Boosting in the presence of outliers: adaptive classification with\n  non-convex loss functions", "comments": null, "journal-ref": "Journal of the American Statistical Association: theory and\n  methods, 2017", "doi": "10.1080/01621459.2016.1273116", "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the role and efficiency of the non-convex loss functions\nfor binary classification problems. In particular, we investigate how to design\na simple and effective boosting algorithm that is robust to the outliers in the\ndata. The analysis of the role of a particular non-convex loss for prediction\naccuracy varies depending on the diminishing tail properties of the gradient of\nthe loss -- the ability of the loss to efficiently adapt to the outlying data,\nthe local convex properties of the loss and the proportion of the contaminated\ndata. In order to use these properties efficiently, we propose a new family of\nnon-convex losses named $\\gamma$-robust losses. Moreover, we present a new\nboosting framework, {\\it Arch Boost}, designed for augmenting the existing work\nsuch that its corresponding classification algorithm is significantly more\nadaptable to the unknown data contamination. Along with the Arch Boosting\nframework, the non-convex losses lead to the new class of boosting algorithms,\nnamed adaptive, robust, boosting (ARB). Furthermore, we present theoretical\nexamples that demonstrate the robustness properties of the proposed algorithms.\nIn particular, we develop a new breakdown point analysis and a new influence\nfunction analysis that demonstrate gains in robustness. Moreover, we present\nnew theoretical results, based only on local curvatures, which may be used to\nestablish statistical and optimization properties of the proposed Arch boosting\nalgorithms with highly non-convex loss functions. Extensive numerical\ncalculations are used to illustrate these theoretical properties and reveal\nadvantages over the existing boosting methods when data exhibits a number of\noutliers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 08:50:56 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Li", "Alexander Hanbo", ""], ["Bradic", "Jelena", ""]]}, {"id": "1510.01291", "submitter": "Samuel Kounaves", "authors": "Dongping Fang, Elizabeth Oberlin, Wei Ding, Samuel P. Kounaves", "title": "A Common-Factor Approach for Multivariate Data Cleaning with an\n  Application to Mars Phoenix Mission Data", "comments": "12 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data quality is fundamentally important to ensure the reliability of data for\nstakeholders to make decisions. In real world applications, such as scientific\nexploration of extreme environments, it is unrealistic to require raw data\ncollected to be perfect. As data miners, when it is infeasible to physically\nknow the why and the how in order to clean up the data, we propose to seek the\nintrinsic structure of the signal to identify the common factors of\nmultivariate data. Using our new data driven learning method, the common-factor\ndata cleaning approach, we address an interdisciplinary challenge on\nmultivariate data cleaning when complex external impacts appear to interfere\nwith multiple data measurements. Existing data analyses typically process one\nsignal measurement at a time without considering the associations among all\nsignals. We analyze all signal measurements simultaneously to find the hidden\ncommon factors that drive all measurements to vary together, but not as a\nresult of the true data measurements. We use common factors to reduce the\nvariations in the data without changing the base mean level of the data to\navoid altering the physical meaning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 19:21:22 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 16:47:30 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Fang", "Dongping", ""], ["Oberlin", "Elizabeth", ""], ["Ding", "Wei", ""], ["Kounaves", "Samuel P.", ""]]}, {"id": "1510.01344", "submitter": "Mohammad Havaei", "authors": "Mohammad Havaei, Hugo Larochelle, Philippe Poulin, Pierre-Marc Jodoin", "title": "Within-Brain Classification for Brain Tumor Segmentation", "comments": null, "journal-ref": null, "doi": "10.1007/s11548-015-1311-1", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: In this paper, we investigate a framework for interactive brain\ntumor segmentation which, at its core, treats the problem of interactive brain\ntumor segmentation as a machine learning problem.\n  Methods: This method has an advantage over typical machine learning methods\nfor this task where generalization is made across brains. The problem with\nthese methods is that they need to deal with intensity bias correction and\nother MRI-specific noise. In this paper, we avoid these issues by approaching\nthe problem as one of within brain generalization. Specifically, we propose a\nsemi-automatic method that segments a brain tumor by training and generalizing\nwithin that brain only, based on some minimum user interaction.\n  Conclusion: We investigate how adding spatial feature coordinates (i.e. $i$,\n$j$, $k$) to the intensity features can significantly improve the performance\nof different classification methods such as SVM, kNN and random forests. This\nwould only be possible within an interactive framework. We also investigate the\nuse of a more appropriate kernel and the adaptation of hyper-parameters\nspecifically for each brain.\n  Results: As a result of these experiments, we obtain an interactive method\nwhose results reported on the MICCAI-BRATS 2013 dataset are the second most\naccurate compared to published methods, while using significantly less memory\nand processing power than most state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 20:32:04 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Havaei", "Mohammad", ""], ["Larochelle", "Hugo", ""], ["Poulin", "Philippe", ""], ["Jodoin", "Pierre-Marc", ""]]}, {"id": "1510.01463", "submitter": "Yunwen Lei", "authors": "Yunwen Lei, Lixin Ding and Yingzhou Bi", "title": "Local Rademacher Complexity Bounds based on Covering Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a general result on controlling local Rademacher\ncomplexities, which captures in an elegant form to relate the complexities with\nconstraint on the expected norm to the corresponding ones with constraint on\nthe empirical norm. This result is convenient to apply in real applications and\ncould yield refined local Rademacher complexity bounds for function classes\nsatisfying general entropy conditions. We demonstrate the power of our\ncomplexity bounds by applying them to derive effective generalization error\nbounds.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 07:44:08 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Lei", "Yunwen", ""], ["Ding", "Lixin", ""], ["Bi", "Yingzhou", ""]]}, {"id": "1510.01599", "submitter": "Marco Maratea", "authors": "Remi Brochenin and Yuliya Lierler and Marco Maratea", "title": "Disjunctive Answer Set Solvers via Templates", "comments": "To appear in Theory and Practice of Logic Programming (TPLP)", "journal-ref": "Theory and Practice of Logic Programming 16 (2016) 465-497", "doi": "10.1017/S1471068415000411", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer set programming is a declarative programming paradigm oriented towards\ndifficult combinatorial search problems. A fundamental task in answer set\nprogramming is to compute stable models, i.e., solutions of logic programs.\nAnswer set solvers are the programs that perform this task. The problem of\ndeciding whether a disjunctive program has a stable model is\n$\\Sigma^P_2$-complete. The high complexity of reasoning within disjunctive\nlogic programming is responsible for few solvers capable of dealing with such\nprograms, namely DLV, GnT, Cmodels, CLASP and WASP. In this paper we show that\ntransition systems introduced by Nieuwenhuis, Oliveras, and Tinelli to model\nand analyze satisfiability solvers can be adapted for disjunctive answer set\nsolvers. Transition systems give a unifying perspective and bring clarity in\nthe description and comparison of solvers. They can be effectively used for\nanalyzing, comparing and proving correctness of search algorithms as well as\ninspiring new ideas in the design of disjunctive answer set solvers. In this\nlight, we introduce a general template, which accounts for major techniques\nimplemented in disjunctive solvers. We then illustrate how this general\ntemplate captures solvers DLV, GnT and Cmodels. We also show how this framework\nprovides a convenient tool for designing new solving algorithms by means of\ncombinations of techniques employed in different solvers.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 14:42:38 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Brochenin", "Remi", ""], ["Lierler", "Yuliya", ""], ["Maratea", "Marco", ""]]}, {"id": "1510.01659", "submitter": "Fahad Muhammad", "authors": "Muhammad Fahad", "title": "DKP-AOM: results for OAEI 2015", "comments": "8 pages, 3 figures, 3 tables, initial results of OM workshop,\n  Ontology Matching Workshop 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the results obtained by our DKP-AOM system within\nthe OAEI 2015 campaign. DKP-AOM is an ontology merging tool designed to merge\nheterogeneous ontologies. In OAEI, we have participated with its ontology\nmapping component which serves as a basic module capable of matching large\nscale ontologies before their merging. This is our first successful\nparticipation in the Conference, OA4QA and Anatomy track of OAEI. DKP-AOM is\nparticipating with two versions (DKP-AOM and DKP-AOM_lite), DKP-AOM performs\ncoherence analysis. In OA4QA track, DKPAOM out-performed in the evaluation and\ngenerated accurate alignments allowed to answer all the queries of the\nevaluation. We can also see its competitive results for the conference track in\nthe evaluation initiative among other reputed systems. In the anatomy track, it\nhas produced alignments within an allocated time and appeared in the list of\nsystems which produce coherent results. Finally, we discuss some future work\ntowards the development of DKP-AOM.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 16:48:24 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Fahad", "Muhammad", ""]]}, {"id": "1510.01784", "submitter": "Ruining He", "authors": "Ruining He, Julian McAuley", "title": "VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback", "comments": "AAAI'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern recommender systems model people and items by discovering or `teasing\napart' the underlying dimensions that encode the properties of items and users'\npreferences toward them. Critically, such dimensions are uncovered based on\nuser feedback, often in implicit form (such as purchase histories, browsing\nlogs, etc.); in addition, some recommender systems make use of side\ninformation, such as product attributes, temporal information, or review text.\nHowever one important feature that is typically ignored by existing\npersonalized recommendation and ranking methods is the visual appearance of the\nitems being considered. In this paper we propose a scalable factorization model\nto incorporate visual signals into predictors of people's opinions, which we\napply to a selection of large, real-world datasets. We make use of visual\nfeatures extracted from product images using (pre-trained) deep networks, on\ntop of which we learn an additional layer that uncovers the visual dimensions\nthat best explain the variation in people's feedback. This not only leads to\nsignificantly more accurate personalized ranking methods, but also helps to\nalleviate cold start issues, and qualitatively to analyze the visual dimensions\nthat influence people's opinions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 23:46:15 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["He", "Ruining", ""], ["McAuley", "Julian", ""]]}, {"id": "1510.01970", "submitter": "Khadija Tijani", "authors": "Khadija Tijani (CSTB, G-SCOP\\_GCSP, LIG Laboratoire d'Informatique de\n  Grenoble), Dung Ngo, Stephane Ploix (G-SCOP\\_GCSP), Benjamin Haas (CSTB),\n  Julie Dugdale (LIG Laboratoire d'Informatique de Grenoble)", "title": "Towards a general framework for an observation and knowledge based model\n  of occupant behaviour in office buildings", "comments": "IBPC 2015 Turin , Jun 2015, Turin, Italy. 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new general approach based on Bayesian networks to\nmodel the human behaviour. This approach represents human behaviour\nwithprobabilistic cause-effect relations based not only on previous works, but\nalso with conditional probabilities coming either from expert knowledge or\ndeduced from observations. The approach has been used in the co-simulation of\nbuilding physics and human behaviour in order to assess the CO 2 concentration\nin an office.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 14:49:58 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Tijani", "Khadija", "", "CSTB, G-SCOP\\_GCSP, LIG Laboratoire d'Informatique de\n  Grenoble"], ["Ngo", "Dung", "", "G-SCOP\\_GCSP"], ["Ploix", "Stephane", "", "G-SCOP\\_GCSP"], ["Haas", "Benjamin", "", "CSTB"], ["Dugdale", "Julie", "", "LIG Laboratoire d'Informatique de Grenoble"]]}, {"id": "1510.02045", "submitter": "Miroslav Dud\\'ik", "authors": "Nikhil Devanur, Miroslav Dud\\'ik, Zhiyi Huang, David M. Pennock", "title": "Budget Constraints in Prediction Markets", "comments": null, "journal-ref": "In Proceedings of the 31st Conference on Uncertainty in Artificial\n  Intelligence, pages 238-247, 2015", "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a detailed characterization of optimal trades under budget\nconstraints in a prediction market with a cost-function-based automated market\nmaker. We study how the budget constraints of individual traders affect their\nability to impact the market price. As a concrete application of our\ncharacterization, we give sufficient conditions for a property we call budget\nadditivity: two traders with budgets B and B' and the same beliefs would have a\ncombined impact equal to a single trader with budget B+B'. That way, even if a\nsingle trader cannot move the market much, a crowd of like-minded traders can\nhave the same desired effect. When the set of payoff vectors associated with\noutcomes, with coordinates corresponding to securities, is affinely\nindependent, we obtain that a generalization of the heavily-used logarithmic\nmarket scoring rule is budget additive, but the quadratic market scoring rule\nis not. Our results may be used both descriptively, to understand if a\nparticular market maker is affected by budget constraints or not, and\nprescriptively, as a recipe to construct markets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 18:02:01 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Devanur", "Nikhil", ""], ["Dud\u00edk", "Miroslav", ""], ["Huang", "Zhiyi", ""], ["Pennock", "David M.", ""]]}, {"id": "1510.02173", "submitter": "John-Alexander Assael", "authors": "John-Alexander M. Assael, Niklas Wahlstr\\\"om, Thomas B. Sch\\\"on, Marc\n  Peter Deisenroth", "title": "Data-Efficient Learning of Feedback Policies from Image Pixels using\n  Deep Dynamical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-efficient reinforcement learning (RL) in continuous state-action spaces\nusing very high-dimensional observations remains a key challenge in developing\nfully autonomous systems. We consider a particularly important instance of this\nchallenge, the pixels-to-torques problem, where an RL agent learns a\nclosed-loop control policy (\"torques\") from pixel information only. We\nintroduce a data-efficient, model-based reinforcement learning algorithm that\nlearns such a closed-loop policy directly from pixel information. The key\ningredient is a deep dynamical model for learning a low-dimensional feature\nembedding of images jointly with a predictive model in this low-dimensional\nfeature space. Joint learning is crucial for long-term predictions, which lie\nat the core of the adaptive nonlinear model predictive control strategy that we\nuse for closed-loop control. Compared to state-of-the-art RL methods for\ncontinuous states and actions, our approach learns quickly, scales to\nhigh-dimensional state spaces, is lightweight and an important step toward\nfully autonomous end-to-end learning from pixels to torques.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 00:20:42 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 15:21:01 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Assael", "John-Alexander M.", ""], ["Wahlstr\u00f6m", "Niklas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1510.02828", "submitter": "Mauricio Toro", "authors": "Mauricio Toro and Camilo Rueda and Carlos Ag\\'on and G\\'erard Assayag", "title": "Gelisp: A Library to Represent Musical CSPs and Search Strategies", "comments": "7 pages, 2 figures, not published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present Gelisp, a new library to represent musical\nConstraint Satisfaction Problems and search strategies intuitively. Gelisp has\ntwo interfaces, a command-line one for Common Lisp and a graphical one for\nOpenMusic. Using Gelisp, we solved a problem of automatic music generation\nproposed by composer Michael Jarrell and we found solutions for the\nAll-interval series.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 21:32:13 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Toro", "Mauricio", ""], ["Rueda", "Camilo", ""], ["Ag\u00f3n", "Carlos", ""], ["Assayag", "G\u00e9rard", ""]]}, {"id": "1510.02867", "submitter": "Tshilidzi Marwala", "authors": "Tshilidzi Marwala and Evan Hurwitz", "title": "Artificial Intelligence and Asymmetric Information Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When human agents come together to make decisions, it is often the case that\none human agent has more information than the other. This phenomenon is called\ninformation asymmetry and this distorts the market. Often if one human agent\nintends to manipulate a decision in its favor the human agent can signal wrong\nor right information. Alternatively, one human agent can screen for information\nto reduce the impact of asymmetric information on decisions. With the advent of\nartificial intelligence, signaling and screening have been made easier. This\npaper studies the impact of artificial intelligence on the theory of asymmetric\ninformation. It is surmised that artificial intelligent agents reduce the\ndegree of information asymmetry and thus the market where these agents are\ndeployed become more efficient. It is also postulated that the more artificial\nintelligent agents there are deployed in the market the less is the volume of\ntrades in the market. This is because for many trades to happen the asymmetry\nof information on goods and services to be traded should exist, creating a\nsense of arbitrage.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 03:07:10 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 04:06:04 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 15:38:31 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Marwala", "Tshilidzi", ""], ["Hurwitz", "Evan", ""]]}, {"id": "1510.02879", "submitter": "Aravind Srinivas", "authors": "Janarthanan Rajendran, Aravind Srinivas, Mitesh M. Khapra, P Prasanna,\n  Balaraman Ravindran", "title": "Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive\n  Transfer from multiple sources in the same domain", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge from prior source tasks in solving a new target task\ncan be useful in several learning applications. The application of transfer\nposes two serious challenges which have not been adequately addressed. First,\nthe agent should be able to avoid negative transfer, which happens when the\ntransfer hampers or slows down the learning instead of helping it. Second, the\nagent should be able to selectively transfer, which is the ability to select\nand transfer from different and multiple source tasks for different parts of\nthe state space of the target task. We propose A2T (Attend, Adapt and\nTransfer), an attentive deep architecture which adapts and transfers from these\nsource tasks. Our model is generic enough to effect transfer of either policies\nor value functions. Empirical evaluations on different learning algorithms show\nthat A2T is an effective architecture for transfer by being able to avoid\nnegative transfer while transferring selectively from multiple source tasks in\nthe same domain.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 05:32:24 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2015 07:07:51 GMT"}, {"version": "v3", "created": "Thu, 22 Sep 2016 20:55:43 GMT"}, {"version": "v4", "created": "Tue, 27 Dec 2016 22:59:55 GMT"}, {"version": "v5", "created": "Tue, 18 Apr 2017 01:05:04 GMT"}, {"version": "v6", "created": "Mon, 21 Sep 2020 22:16:47 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Rajendran", "Janarthanan", ""], ["Srinivas", "Aravind", ""], ["Khapra", "Mitesh M.", ""], ["Prasanna", "P", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1510.02951", "submitter": "Igor Razgon", "authors": "Igor Razgon", "title": "On oblivious branching programs with bounded repetition that cannot\n  efficiently compute CNFs of bounded treewidth", "comments": "Follow-up work of arxiv:1308.3829", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study complexity of an extension of ordered binary decision\ndiagrams (OBDDs) called $c$-OBDDs on CNFs of bounded (primal graph) treewidth.\nIn particular, we show that for each $k$ there is a class of CNFs of treewidth\n$k \\geq 3$ for which the equivalent $c$-OBDDs are of size\n$\\Omega(n^{k/(8c-4)})$. Moreover, this lower bound holds if $c$-OBDD is\nnon-deterministic and semantic. Our second result uses the above lower bound to\nseparate the above model from sentential decision diagrams (SDDs). In order to\nobtain the lower bound, we use a structural graph parameter called matching\nwidth. Our third result shows that matching width and pathwidth are linearly\nrelated.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 15:22:32 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Razgon", "Igor", ""]]}, {"id": "1510.03042", "submitter": "Thuc Le Ph.D", "authors": "Thuc Duy Le, Tao Hoang, Jiuyong Li, Lin Liu, Shu Hu", "title": "ParallelPC: an R package for efficient constraint based causal\n  exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering causal relationships from data is the ultimate goal of many\nresearch areas. Constraint based causal exploration algorithms, such as PC,\nFCI, RFCI, PC-simple, IDA and Joint-IDA have achieved significant progress and\nhave many applications. A common problem with these methods is the high\ncomputational complexity, which hinders their applications in real world high\ndimensional datasets, e.g gene expression datasets. In this paper, we present\nan R package, ParallelPC, that includes the parallelised versions of these\ncausal exploration algorithms. The parallelised algorithms help speed up the\nprocedure of experimenting big datasets and reduce the memory used when running\nthe algorithms. The package is not only suitable for super-computers or\nclusters, but also convenient for researchers using personal computers with\nmulti core CPUs. Our experiment results on real world datasets show that using\nthe parallelised algorithms it is now practical to explore causal relationships\nin high dimensional datasets with thousands of variables in a single multicore\ncomputer. ParallelPC is available in CRAN repository at\nhttps://cran.rproject.org/web/packages/ParallelPC/index.html.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 11:55:39 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Le", "Thuc Duy", ""], ["Hoang", "Tao", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""], ["Hu", "Shu", ""]]}, {"id": "1510.03164", "submitter": "Purushottam Kar", "authors": "Shuai Li and Purushottam Kar", "title": "Context-Aware Bandits", "comments": "The paper has been withdrawn as the work has been superseded", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient Context-Aware clustering of Bandits (CAB) algorithm,\nwhich can capture collaborative effects. CAB can be easily deployed in a\nreal-world recommendation system, where multi-armed bandits have been shown to\nperform well in particular with respect to the cold-start problem. CAB utilizes\na context-aware clustering augmented by exploration-exploitation strategies.\nCAB dynamically clusters the users based on the content universe under\nconsideration. We give a theoretical analysis in the standard stochastic\nmulti-armed bandits setting. We show the efficiency of our approach on\nproduction and real-world datasets, demonstrate the scalability, and, more\nimportantly, the significant increased prediction performance against several\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 07:04:16 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 05:47:32 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 16:18:43 GMT"}, {"version": "v4", "created": "Fri, 10 Jun 2016 20:51:08 GMT"}, {"version": "v5", "created": "Sun, 26 Feb 2017 15:53:30 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Li", "Shuai", ""], ["Kar", "Purushottam", ""]]}, {"id": "1510.03179", "submitter": "Adrian Groza", "authors": "Adrian Groza", "title": "Data structuring for the ontological modelling of wind energy systems", "comments": "th Int. Conf. on Modelling and Development of Intelligent Systems\n  (MDIS2015), Sibiu, Romania, 28 Oct. - 1 Nov. 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small wind projects encounter difficulties to be efficiently deployed, partly\nbecause wrong way data and information are managed. Ontologies can overcome the\ndrawbacks of partially available, noisy, inconsistent, and heterogeneous data\nsources, by providing a semantic middleware between low level data and more\ngeneral knowledge. In this paper, we engineer an ontology for the wind energy\ndomain using description logic as technical instrumentation. We aim to\nintegrate corpus of heterogeneous knowledge, both digital and human, in order\nto help the interested user to speed-up the initialization of a small-scale\nwind project. We exemplify one use case scenario of our ontology, that consists\nof automatically checking whether a planned wind project is compliant or not\nwith the active regulations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 08:23:28 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Groza", "Adrian", ""]]}, {"id": "1510.03317", "submitter": "Anastasia Paparrizou Ms", "authors": "Christian Bessiere, Luc De Raedt, Tias Guns, Lars Kotthoff, Mirco\n  Nanni, Siegfried Nijssen, Barry O'Sullivan, Anastasia Paparrizou, Dino\n  Pedreschi, Helmut Simonis", "title": "The Inductive Constraint Programming Loop", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint programming is used for a variety of real-world optimisation\nproblems, such as planning, scheduling and resource allocation problems. At the\nsame time, one continuously gathers vast amounts of data about these problems.\nCurrent constraint programming software does not exploit such data to update\nschedules, resources and plans. We propose a new framework, that we call the\nInductive Constraint Programming loop. In this approach data is gathered and\nanalyzed systematically, in order to dynamically revise and adapt constraints\nand optimization criteria. Inductive Constraint Programming aims at bridging\nthe gap between the areas of data mining and machine learning on the one hand,\nand constraint programming on the other hand.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 14:51:03 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Bessiere", "Christian", ""], ["De Raedt", "Luc", ""], ["Guns", "Tias", ""], ["Kotthoff", "Lars", ""], ["Nanni", "Mirco", ""], ["Nijssen", "Siegfried", ""], ["O'Sullivan", "Barry", ""], ["Paparrizou", "Anastasia", ""], ["Pedreschi", "Dino", ""], ["Simonis", "Helmut", ""]]}, {"id": "1510.03336", "submitter": "Subutai Ahmad", "authors": "Alexander Lavin, Subutai Ahmad", "title": "Evaluating Real-time Anomaly Detection Algorithms - the Numenta Anomaly\n  Benchmark", "comments": "14th International Conference on Machine Learning and Applications\n  (IEEE ICMLA), 2015. Fixed typo in equation and formatting", "journal-ref": null, "doi": "10.1109/ICMLA.2015.141", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much of the world's data is streaming, time-series data, where anomalies give\nsignificant information in critical situations; examples abound in domains such\nas finance, IT, security, medical, and energy. Yet detecting anomalies in\nstreaming data is a difficult task, requiring detectors to process data in\nreal-time, not batches, and learn while simultaneously making predictions.\nThere are no benchmarks to adequately test and score the efficacy of real-time\nanomaly detectors. Here we propose the Numenta Anomaly Benchmark (NAB), which\nattempts to provide a controlled and repeatable environment of open-source\ntools to test and measure anomaly detection algorithms on streaming data. The\nperfect detector would detect all anomalies as soon as possible, trigger no\nfalse alarms, work with real-world time-series data across a variety of\ndomains, and automatically adapt to changing statistics. Rewarding these\ncharacteristics is formalized in NAB, using a scoring algorithm designed for\nstreaming data. NAB evaluates detectors on a benchmark dataset with labeled,\nreal-world time-series data. We present these components, and give results and\nanalyses for several open source, commercially-used algorithms. The goal for\nNAB is to provide a standard, open source framework with which the research\ncommunity can compare and evaluate different algorithms for detecting anomalies\nin streaming data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 15:30:34 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 23:09:58 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2015 20:52:44 GMT"}, {"version": "v4", "created": "Tue, 17 Nov 2015 17:17:06 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lavin", "Alexander", ""], ["Ahmad", "Subutai", ""]]}, {"id": "1510.03370", "submitter": "Scott Garrabrant", "authors": "Scott Garrabrant, Siddharth Bhaskar, Abram Demski, Joanna Garrabrant,\n  George Koleszarik, Evan Lloyd", "title": "Asymptotic Logical Uncertainty and The Benford Test", "comments": null, "journal-ref": null, "doi": null, "report-no": "2015--11", "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm A which assigns probabilities to logical sentences. For\nany simple infinite sequence of sentences whose truth-values appear\nindistinguishable from a biased coin that outputs \"true\" with probability p, we\nhave that the sequence of probabilities that A assigns to these sentences\nconverges to p.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 17:14:44 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Garrabrant", "Scott", ""], ["Bhaskar", "Siddharth", ""], ["Demski", "Abram", ""], ["Garrabrant", "Joanna", ""], ["Koleszarik", "George", ""], ["Lloyd", "Evan", ""]]}, {"id": "1510.03517", "submitter": "Xiang Wang", "authors": "Xiang Wang, Ronald D. Haynes, Qihong Feng", "title": "A Multilevel Coordinate Search Algorithm for Well Placement, Control and\n  Joint Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining optimal well placements and controls are two important tasks in\noil field development. These problems are computationally expensive, nonconvex,\nand contain multiple optima. The practical solution of these problems require\nefficient and robust algorithms. In this paper, the multilevel coordinate\nsearch (MCS) algorithm is applied for well placement and control optimization\nproblems. MCS is a derivative-free algorithm that combines global and local\nsearch. Both synthetic and real oil fields are considered. The performance of\nMCS is compared to generalized pattern search (GPS), particle swarm\noptimization (PSO), and covariance matrix adaptive evolution strategy (CMA-ES)\nalgorithms. Results show that the MCS algorithm is strongly competitive, and\noutperforms for the joint optimization problem and with a limited computational\nbudget. The effect of parameter settings for MCS are compared for the test\nexamples. For the joint optimization problem we compare the performance of the\nsimultaneous and sequential procedures and show the utility of the latter.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 03:09:43 GMT"}, {"version": "v2", "created": "Sat, 2 Apr 2016 09:03:42 GMT"}, {"version": "v3", "created": "Thu, 1 Sep 2016 03:43:47 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Wang", "Xiang", ""], ["Haynes", "Ronald D.", ""], ["Feng", "Qihong", ""]]}, {"id": "1510.03592", "submitter": "Stefano Rosati", "authors": "Mattia Carpin, Stefano Rosati, Mohammad Emtiyaz Khan, and Bixio\n  Rimoldi", "title": "UAVs using Bayesian Optimization to Locate WiFi Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of localizing non-collaborative WiFi devices in a\nlarge region. Our main motive is to localize humans by localizing their WiFi\ndevices, e.g. during search-and-rescue operations after a natural disaster. We\nuse an active sensing approach that relies on Unmanned Aerial Vehicles (UAVs)\nto collect signal-strength measurements at informative locations. The problem\nis challenging since the measurement is received at arbitrary times and they\nare received only when the UAV is in close proximity to the device. For these\nreasons, it is extremely important to make prudent decision with very few\nmeasurements. We use the Bayesian optimization approach based on Gaussian\nprocess (GP) regression. This approach works well for our application since GPs\ngive reliable predictions with very few measurements while Bayesian\noptimization makes a judicious trade-off between exploration and exploitation.\nIn field experiments conducted over a region of 1000 $\\times$ 1000 $m^2$, we\nshow that our approach reduces the search area to less than 100 meters around\nthe WiFi device within 5 minutes only. Overall, our approach localizes the\ndevice in less than 15 minutes with an error of less than 20 meters.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 09:30:11 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 12:00:00 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Carpin", "Mattia", ""], ["Rosati", "Stefano", ""], ["Khan", "Mohammad Emtiyaz", ""], ["Rimoldi", "Bixio", ""]]}, {"id": "1510.03931", "submitter": "Wei Zhang", "authors": "Wei Zhang, Yang Yu, Bowen Zhou", "title": "Structured Memory for Neural Turing Machines", "comments": "4 pages, accepted to Reasoning, Attention, Memory (RAM) NIPS 2015\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Neural Turing Machines (NTM) contain memory component that simulates \"working\nmemory\" in the brain to store and retrieve information to ease simple\nalgorithms learning. So far, only linearly organized memory is proposed, and\nduring experiments, we observed that the model does not always converge, and\noverfits easily when handling certain tasks. We think memory component is key\nto some faulty behaviors of NTM, and better organization of memory component\ncould help fight those problems. In this paper, we propose several different\nstructures of memory for NTM, and we proved in experiments that two of our\nproposed structured-memory NTMs could lead to better convergence, in term of\nspeed and prediction accuracy on copy task and associative recall task as in\n(Graves et al. 2014).\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 00:08:17 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 21:52:04 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2015 03:12:49 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Zhang", "Wei", ""], ["Yu", "Yang", ""], ["Zhou", "Bowen", ""]]}, {"id": "1510.04163", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Chong Wang, Eric Xing", "title": "Embarrassingly Parallel Variational Inference in Nonconjugate Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a parallel variational inference (VI) procedure for use in\ndata-distributed settings, where each machine only has access to a subset of\ndata and runs VI independently, without communicating with other machines. This\ntype of \"embarrassingly parallel\" procedure has recently been developed for\nMCMC inference algorithms; however, in many cases it is not possible to\ndirectly extend this procedure to VI methods without requiring certain\nrestrictive exponential family conditions on the form of the model.\nFurthermore, most existing (nonparallel) VI methods are restricted to use on\nconditionally conjugate models, which limits their applicability. To combat\nthese issues, we make use of the recently proposed nonparametric VI to\nfacilitate an embarrassingly parallel VI procedure that can be applied to a\nwider scope of models, including to nonconjugate models. We derive our\nembarrassingly parallel VI algorithm, analyze our method theoretically, and\ndemonstrate our method empirically on a few nonconjugate models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 15:48:19 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Neiswanger", "Willie", ""], ["Wang", "Chong", ""], ["Xing", "Eric", ""]]}, {"id": "1510.04183", "submitter": "Dmytro Terletskyi", "authors": "D.O. Terletskyi, O.I. Provotar", "title": "Mathematical Foundations for Designing and Development of Intelligent\n  Systems of Information Analysis", "comments": null, "journal-ref": "Problems in Programming, 2014, Vol. 16, No.2-3, pp. 233-241", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is an attempt to combine different ways of working with sets of\nobjects and their classes for designing and development of artificial\nintelligent systems (AIS) of analysis information, using object-oriented\nprogramming (OOP). This paper contains analysis of basic concepts of OOP and\ntheir relation with set theory and artificial intelligence (AI). Process of\nsets and multisets creation from different sides, in particular mathematical\nset theory, OOP and AI is considered. Definition of object and its properties,\nhomogeneous and inhomogeneous classes of objects, set of objects, multiset of\nobjects and constructive methods of their creation and classification are\nproposed. In addition, necessity of some extension of existing OOP tools for\nthe purpose of practical implementation AIS of analysis information, using\nproposed approach, is shown.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 16:09:43 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 17:41:06 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Terletskyi", "D. O.", ""], ["Provotar", "O. I.", ""]]}, {"id": "1510.04188", "submitter": "Dmytro Terletskyi", "authors": "Dmytro Terletskyi", "title": "Universal and Determined Constructors of Multisets of Objects", "comments": "arXiv admin note: text overlap with arXiv:1510.04183", "journal-ref": "Information Theories and Applications, Vol. 21, Number 4, 2014,\n  pp. 339-361", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contains analysis of creation of sets and multisets as an approach\nfor modeling of some aspects of human thinking. The creation of sets is\nconsidered within constructive object-oriented version of set theory (COOST),\nfrom different sides, in particular classical set theory, object-oriented\nprogramming (OOP) and development of intelligent information systems (IIS). The\nmain feature of COOST in contrast to other versions of set theory is an\nopportunity to describe essences of objects more precisely, using their\nproperties and methods, which can be applied to them. That is why this version\nof set theory is object-oriented and close to OOP. Within COOST, the author\nproposes universal constructor of multisets of objects that gives us a\npossibility to create arbitrary multisets of objects. In addition, a few\ndetermined constructors of multisets of objects, which allow creating\nmultisets, using strictly defined schemas, also are proposed in the paper. Such\nconstructors are very useful in cases of very big cardinalities of multisets,\nbecause they give us an opportunity to calculate a multiplicity of each object\nand cardinality of multiset before its creation. The proposed constructors of\nmultisets of objects allow us to model in a sense corresponding processes of\nhuman thought, that in turn give us an opportunity to develop IIS, using these\ntools.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 16:27:26 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Terletskyi", "Dmytro", ""]]}, {"id": "1510.04194", "submitter": "Dmytro Terletskyi", "authors": "Dmytro Terletskyi, Alexandr Provotar", "title": "Object-Oriented Dynamic Networks", "comments": "arXiv admin note: text overlap with arXiv:1510.04183", "journal-ref": "International Book Series Information Science and Computing, Book\n  30 Computational Models for Business and Engineering Domains, ITHEA, 2014,\n  pp. 123-136", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contains description of such knowledge representation model as\nObject-Oriented Dynamic Network (OODN), which gives us an opportunity to\nrepresent knowledge, which can be modified in time, to build new relations\nbetween objects and classes of objects and to represent results of their\nmodifications. The model is based on representation of objects via their\nproperties and methods. It gives us a possibility to classify the objects and,\nin a sense, to build hierarchy of their types. Furthermore, it enables to\nrepresent relation of modification between concepts, to build new classes of\nobjects based on existing classes and to create sets and multisets of concepts.\nOODN can be represented as a connected and directed graph, where nodes are\nconcepts and edges are relations between them. Using such model of knowledge\nrepresentation, we can consider modifications of knowledge and movement through\nthe graph of network as a process of logical reasoning or finding the right\nsolutions or creativity, etc. The proposed approach gives us an opportunity to\nmodel some aspects of human knowledge system and main mechanisms of human\nthought, in particular getting a new experience and knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 16:39:30 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Terletskyi", "Dmytro", ""], ["Provotar", "Alexandr", ""]]}, {"id": "1510.04206", "submitter": "Dmytro Terletskyi", "authors": "Dmytro Terletskyi", "title": "Exploiters-Based Knowledge Extraction in Object-Oriented Knowledge\n  Representation", "comments": null, "journal-ref": "Proceedings of 24th International Workshop, Concurrency,\n  Specification & Programming 2015, Rzeszow, Poland, September 28-30, 2015,\n  Vol. 2, pp. 211-221", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contains the consideration of knowledge extraction mechanisms of\nsuch object-oriented knowledge representation models as frames, object-oriented\nprogramming and object-oriented dynamic networks. In addition, conception of\nuniversal exploiters within object-oriented dynamic networks is also discussed.\nThe main result of the paper is introduction of new exploiters-based knowledge\nextraction approach, which provides generation of a finite set of new classes\nof objects, based on the basic set of classes. The methods for calculation of\nquantity of new classes, which can be obtained using proposed approach, and of\nquantity of types, which each of them describes, are proposed. Proof that basic\nset of classes, extended according to proposed approach, together with union\nexploiter create upper semilattice is given. The approach always allows\ngenerating of finitely defined set of new classes of objects for any\nobject-oriented dynamic network. A quantity of these classes can be precisely\ncalculated before the generation. It allows saving of only basic set of classes\nin the knowledge base.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 17:21:37 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Terletskyi", "Dmytro", ""]]}, {"id": "1510.04212", "submitter": "Dmytro Terletskyi", "authors": "Dmytro Terletskyi", "title": "Inheritance in Object-Oriented Knowledge Representation", "comments": "in Information and Software Technologies, Communications in Computer\n  and Information Science, Springer, 2015", "journal-ref": "Information and Software Technologies, Volume 538 of the series\n  Communications in Computer and Information Science, pp. 293-305, 2015", "doi": "10.1007/978-3-319-24770-0_26", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contains the consideration of inheritance mechanism in such\nknowledge representation models as object-oriented programming, frames and\nobject-oriented dynamic networks. In addition, inheritance within\nrepresentation of vague and imprecise knowledge are also discussed. New types\nof inheritance, general classification of all known inheritance types and\napproach, which allows avoiding in many cases problems with exceptions,\nredundancy and ambiguity within object-oriented dynamic networks and their\nfuzzy extension, are introduced in the paper. The proposed approach bases on\nconception of homogeneous and inhomogeneous or heterogeneous class of objects,\nwhich allow building of inheritance hierarchy more flexibly and efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 17:34:11 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Terletskyi", "Dmytro", ""]]}, {"id": "1510.04373", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and David Balduzzi and W. Bastiaan Kleijn and Mengjie\n  Zhang", "title": "Scatter Component Analysis: A Unified Framework for Domain Adaptation\n  and Domain Generalization", "comments": "to appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses classification tasks on a particular target domain in\nwhich labeled training data are only available from source domains different\nfrom (but related to) the target. Two closely related frameworks, domain\nadaptation and domain generalization, are concerned with such tasks, where the\nonly difference between those frameworks is the availability of the unlabeled\ntarget data: domain adaptation can leverage unlabeled target information, while\ndomain generalization cannot. We propose Scatter Component Analyis (SCA), a\nfast representation learning algorithm that can be applied to both domain\nadaptation and domain generalization. SCA is based on a simple geometrical\nmeasure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA\nfinds a representation that trades between maximizing the separability of\nclasses, minimizing the mismatch between domains, and maximizing the\nseparability of data; each of which is quantified through scatter. The\noptimization problem of SCA can be reduced to a generalized eigenvalue problem,\nwhich results in a fast and exact solution. Comprehensive experiments on\nbenchmark cross-domain object recognition datasets verify that SCA performs\nmuch faster than several state-of-the-art algorithms and also provides\nstate-of-the-art classification accuracy in both domain adaptation and domain\ngeneralization. We also show that scatter can be used to establish a\ntheoretical generalization bound in the case of domain adaptation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 01:41:12 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 21:35:08 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Balduzzi", "David", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1510.04420", "submitter": "Paramjot Kaur Sarao", "authors": "Paramjot Kaur Sarao, Puneet Mittal, Rupinder Kaur", "title": "Narrative Science Systems: A Review", "comments": null, "journal-ref": "International Journal of Research in Computer Science, 5(1), 2015,\n  pp 9-14", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic narration of events and entities is the need of the hour,\nespecially when live reporting is critical and volume of information to be\nnarrated is huge. This paper discusses the challenges in this context, along\nwith the algorithms used to build such systems. From a systematic study, we can\ninfer that most of the work done in this area is related to statistical data.\nIt was also found that subjective evaluation or contribution of experts is also\nlimited for narration context.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 07:06:39 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Sarao", "Paramjot Kaur", ""], ["Mittal", "Puneet", ""], ["Kaur", "Rupinder", ""]]}, {"id": "1510.04609", "submitter": "Bharat Singh", "authors": "Bharat Singh, Soham De, Yangmuzi Zhang, Thomas Goldstein, and Gavin\n  Taylor", "title": "Layer-Specific Adaptive Learning Rates for Deep Networks", "comments": "ICMLA 2015, deep learning, adaptive learning rates for training,\n  layer specific learning rate", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing complexity of deep learning architectures is resulting in\ntraining time requiring weeks or even months. This slow training is due in part\nto vanishing gradients, in which the gradients used by back-propagation are\nextremely large for weights connecting deep layers (layers near the output\nlayer), and extremely small for shallow layers (near the input layer); this\nresults in slow learning in the shallow layers. Additionally, it has also been\nshown that in highly non-convex problems, such as deep neural networks, there\nis a proliferation of high-error low curvature saddle points, which slows down\nlearning dramatically. In this paper, we attempt to overcome the two above\nproblems by proposing an optimization method for training deep neural networks\nwhich uses learning rates which are both specific to each layer in the network\nand adaptive to the curvature of the function, increasing the learning rate at\nlow curvature points. This enables us to speed up learning in the shallow\nlayers of the network and quickly escape high-error low curvature saddle\npoints. We test our method on standard image classification datasets such as\nMNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy\nas well as reduces the required training time over standard algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 16:31:46 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Singh", "Bharat", ""], ["De", "Soham", ""], ["Zhang", "Yangmuzi", ""], ["Goldstein", "Thomas", ""], ["Taylor", "Gavin", ""]]}, {"id": "1510.04817", "submitter": "Javier Alvez", "authors": "Javier \\'Alvez and Paqui Lucio and German Rigau", "title": "Improving the Competency of First-Order Ontologies", "comments": "8 pages, 2 tables", "journal-ref": "Proceedings of the 8th International Conference on Knowledge\n  Capture (K-CAP 2015). Palisades, NY. 2015", "doi": "10.1145/2815833.2815841", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new framework to evaluate and improve first-order (FO)\nontologies using automated theorem provers (ATPs) on the basis of competency\nquestions (CQs). Our framework includes both the adaptation of a methodology\nfor evaluating ontologies to the framework of first-order logic and a new set\nof non-trivial CQs designed to evaluate FO versions of SUMO, which\nsignificantly extends the very small set of CQs proposed in the literature.\nMost of these new CQs have been automatically generated from a small set of\npatterns and the mapping of WordNet to SUMO. Applying our framework, we\ndemonstrate that Adimen-SUMO v2.2 outperforms TPTP-SUMO. In addition, using the\nfeedback provided by ATPs we have set an improved version of Adimen-SUMO\n(v2.4). This new version outperforms the previous ones in terms of competency.\nFor instance, \"Humans can reason\" is automatically inferred from Adimen-SUMO\nv2.4, while it is neither deducible from TPTP-SUMO nor Adimen-SUMO v2.2.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 09:01:35 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["\u00c1lvez", "Javier", ""], ["Lucio", "Paqui", ""], ["Rigau", "German", ""]]}, {"id": "1510.04826", "submitter": "Javier \\'Alvez", "authors": "Javier \\'Alvez and Paqui Lucio and German Rigau", "title": "Evaluating the Competency of a First-Order Ontology", "comments": "4 pages, 4 figures", "journal-ref": "Proceedings of the 8th International Conference on Knowledge\n  Capture (K-CAP 2015). Palisades, NY. 2015", "doi": "10.1145/2815833.2816946", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on the results of evaluating the competency of a first-order\nontology for its use with automated theorem provers (ATPs). The evaluation\nfollows the adaptation of the methodology based on competency questions (CQs)\n[Gr\\\"uninger&Fox,1995] to the framework of first-order logic, which is\npresented in [\\'Alvez&Lucio&Rigau,2015], and is applied to Adimen-SUMO\n[\\'Alvez&Lucio&Rigau,2015]. The set of CQs used for this evaluation has been\nautomatically generated from a small set of semantic patterns and the mapping\nof WordNet to SUMO. Analysing the results, we can conclude that it is feasible\nto use ATPs for working with Adimen-SUMO v2.4, enabling the resolution of goals\nby means of performing non-trivial inferences.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 09:49:33 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["\u00c1lvez", "Javier", ""], ["Lucio", "Paqui", ""], ["Rigau", "German", ""]]}, {"id": "1510.04914", "submitter": "Charlie Vanaret", "authors": "Charlie Vanaret and Jean-Baptiste Gotteland and Nicolas Durand and\n  Jean-Marc Alliot", "title": "Hybridization of Interval CP and Evolutionary Algorithms for Optimizing\n  Difficult Problems", "comments": "21st International Conference on Principles and Practice of\n  Constraint Programming (CP 2015), 2015", "journal-ref": null, "doi": "10.1007/978-3-319-23219-5_32", "report-no": null, "categories": "cs.AI cs.DC cs.MS math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The only rigorous approaches for achieving a numerical proof of optimality in\nglobal optimization are interval-based methods that interleave branching of the\nsearch-space and pruning of the subdomains that cannot contain an optimal\nsolution. State-of-the-art solvers generally integrate local optimization\nalgorithms to compute a good upper bound of the global minimum over each\nsubspace. In this document, we propose a cooperative framework in which\ninterval methods cooperate with evolutionary algorithms. The latter are\nstochastic algorithms in which a population of candidate solutions iteratively\nevolves in the search-space to reach satisfactory solutions.\n  Within our cooperative solver Charibde, the evolutionary algorithm and the\ninterval-based algorithm run in parallel and exchange bounds, solutions and\nsearch-space in an advanced manner via message passing. A comparison of\nCharibde with state-of-the-art interval-based solvers (GlobSol, IBBA, Ibex) and\nNLP solvers (Couenne, BARON) on a benchmark of difficult COCONUT problems shows\nthat Charibde is highly competitive against non-rigorous solvers and converges\nfaster than rigorous solvers by an order of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 15:18:42 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Vanaret", "Charlie", ""], ["Gotteland", "Jean-Baptiste", ""], ["Durand", "Nicolas", ""], ["Alliot", "Jean-Marc", ""]]}, {"id": "1510.04931", "submitter": "Jan Leike", "authors": "Jan Leike and Marcus Hutter", "title": "Bad Universal Priors and Notions of Optimality", "comments": "COLT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A big open question of algorithmic information theory is the choice of the\nuniversal Turing machine (UTM). For Kolmogorov complexity and Solomonoff\ninduction we have invariance theorems: the choice of the UTM changes bounds\nonly by a constant. For the universally intelligent agent AIXI (Hutter, 2005)\nno invariance theorem is known. Our results are entirely negative: we discuss\ncases in which unlucky or adversarial choices of the UTM cause AIXI to\nmisbehave drastically. We show that Legg-Hutter intelligence and thus balanced\nPareto optimality is entirely subjective, and that every policy is Pareto\noptimal in the class of all computable environments. This undermines all\nexisting optimality properties for AIXI. While it may still serve as a gold\nstandard for AI, our results imply that AIXI is a relative theory, dependent on\nthe choice of the UTM.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 16:22:23 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Leike", "Jan", ""], ["Hutter", "Marcus", ""]]}, {"id": "1510.04935", "submitter": "Maximilian Nickel", "authors": "Maximilian Nickel, Lorenzo Rosasco, Tomaso Poggio", "title": "Holographic Embeddings of Knowledge Graphs", "comments": "To appear in AAAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning embeddings of entities and relations is an efficient and versatile\nmethod to perform machine learning on relational data such as knowledge graphs.\nIn this work, we propose holographic embeddings (HolE) to learn compositional\nvector space representations of entire knowledge graphs. The proposed method is\nrelated to holographic models of associative memory in that it employs circular\ncorrelation to create compositional representations. By using correlation as\nthe compositional operator HolE can capture rich interactions but\nsimultaneously remains efficient to compute, easy to train, and scalable to\nvery large datasets. In extensive experiments we show that holographic\nembeddings are able to outperform state-of-the-art methods for link prediction\nin knowledge graphs and relational learning benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 16:29:07 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 18:05:52 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Nickel", "Maximilian", ""], ["Rosasco", "Lorenzo", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1510.04972", "submitter": "Weiyi Sun", "authors": "Weiyi Sun, Anna Rumshisky, Ozlem Uzuner", "title": "Normalization of Relative and Incomplete Temporal Expressions in\n  Clinical Narratives", "comments": "Draft version", "journal-ref": "Journal of the American Medical Informatics Association (2015)", "doi": "10.1093/jamia/ocu004", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the RI-TIMEXes in temporally annotated corpora and propose two\nhypotheses regarding the normalization of RI-TIMEXes in the clinical narrative\ndomain: the anchor point hypothesis and the anchor relation hypothesis. We\nannotate the RI-TIMEXes in three corpora to study the characteristics of\nRI-TMEXes in different domains. This informed the design of our RI-TIMEX\nnormalization system for the clinical domain, which consists of an anchor point\nclassifier, an anchor relation classifier and a rule-based RI-TIMEX text span\nparser. We experiment with different feature sets and perform error analysis\nfor each system component. The annotation confirmed the hypotheses that we can\nsimplify the RI-TIMEXes normalization task using two multi-label classifiers.\nOur system achieves anchor point classification, anchor relation classification\nand rule-based parsing accuracy of 74.68%, 87.71% and 57.2% (82.09% under\nrelaxed matching criteria) respectively on the held-out test set of the 2012\ni2b2 temporal relation challenge. Experiments with feature sets reveals some\ninteresting findings such as the verbal tense feature does not inform the\nanchor relation classification in clinical narratives as much as the tokens\nnear the RI-TIMEX. Error analysis shows that underrepresented anchor point and\nanchor relation classes are difficult to detect. We formulate the RI-TIMEX\nnormalization problem as a pair of multi-label classification problems.\nConsidering only the RI-TIMEX extraction and normalization, the system achieves\nstatistically significant improvement over the RI-TIMEX results of the best\nsystems in the 2012 i2b2 challenge.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 18:35:13 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Sun", "Weiyi", ""], ["Rumshisky", "Anna", ""], ["Uzuner", "Ozlem", ""]]}, {"id": "1510.05189", "submitter": "Fulton Wang", "authors": "Fulton Wang, Cynthia Rudin", "title": "Causal Falling Rule Lists", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (workshop version of\n  previous submission)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A causal falling rule list (CFRL) is a sequence of if-then rules that\nspecifies heterogeneous treatment effects, where (i) the order of rules\ndetermines the treatment effect subgroup a subject belongs to, and (ii) the\ntreatment effect decreases monotonically down the list. A given CFRL\nparameterizes a hierarchical bayesian regression model in which the treatment\neffects are incorporated as parameters, and assumed constant within\nmodel-specific subgroups. We formulate the search for the CFRL best supported\nby the data as a Bayesian model selection problem, where we perform a search\nover the space of CFRL models, and approximate the evidence for a given CFRL\nmodel using standard variational techniques. We apply CFRL to a census wage\ndataset to identify subgroups of differing wage inequalities between men and\nwomen.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 00:57:00 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 00:28:30 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Wang", "Fulton", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1510.05373", "submitter": "Matthias Thimm", "authors": "Matthias Thimm, Serena Villata", "title": "System Descriptions of the First International Competition on\n  Computational Models of Argumentation (ICCMA'15)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the system description of the 18 solvers submitted to\nthe First International Competition on Computational Models of Argumentation\n(ICCMA'15) and therefore gives an overview on state-of-the-art of computational\napproaches to abstract argumentation problems. Further information on the\nresults of the competition and the performance of the individual solvers can be\nfound on at http://argumentationcompetition.org/2015/.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 07:48:32 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Thimm", "Matthias", ""], ["Villata", "Serena", ""]]}, {"id": "1510.05572", "submitter": "Jan Leike", "authors": "Jan Leike and Marcus Hutter", "title": "On the Computability of AIXI", "comments": "UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How could we solve the machine learning and the artificial intelligence\nproblem if we had infinite computation? Solomonoff induction and the\nreinforcement learning agent AIXI are proposed answers to this question. Both\nare known to be incomputable. In this paper, we quantify this using the\narithmetical hierarchy, and prove upper and corresponding lower bounds for\nincomputability. We show that AIXI is not limit computable, thus it cannot be\napproximated using finite computation. Our main result is a limit-computable\n{\\epsilon}-optimal version of AIXI with infinite horizon that maximizes\nexpected rewards.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 16:31:37 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Leike", "Jan", ""], ["Hutter", "Marcus", ""]]}, {"id": "1510.05613", "submitter": "Venkatraman Narayanan", "authors": "Venkatraman Narayanan and Maxim Likhachev", "title": "PERCH: Perception via Search for Multi-Object Recognition and\n  Localization", "comments": "8 pages, International Conference on Robotics and Automation (ICRA),\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many robotic domains such as flexible automated manufacturing or personal\nassistance, a fundamental perception task is that of identifying and localizing\nobjects whose 3D models are known. Canonical approaches to this problem include\ndiscriminative methods that find correspondences between feature descriptors\ncomputed over the model and observed data. While these methods have been\nemployed successfully, they can be unreliable when the feature descriptors fail\nto capture variations in observed data; a classic cause being occlusion. As a\nstep towards deliberative reasoning, we present PERCH: PErception via SeaRCH,\nan algorithm that seeks to find the best explanation of the observed sensor\ndata by hypothesizing possible scenes in a generative fashion. Our\ncontributions are: i) formulating the multi-object recognition and localization\ntask as an optimization problem over the space of hypothesized scenes, ii)\nexploiting structure in the optimization to cast it as a combinatorial search\nproblem on what we call the Monotone Scene Generation Tree, and iii) leveraging\nparallelization and recent advances in multi-heuristic search in making\ncombinatorial search tractable. We prove that our system can guaranteedly\nproduce the best explanation of the scene under the chosen cost function, and\nvalidate our claims on real world RGB-D test data. Our experimental results\nshow that we can identify and localize objects under heavy occlusion--cases\nwhere state-of-the-art methods struggle.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 18:39:05 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 20:49:52 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Narayanan", "Venkatraman", ""], ["Likhachev", "Maxim", ""]]}, {"id": "1510.05911", "submitter": "Tim Weninger PhD", "authors": "Baoxu Shi, Tim Weninger", "title": "Discriminative Predicate Path Mining for Fact Checking in Knowledge\n  Graphs", "comments": "17 pages, 4 Figures. To Appear in Knowledge Based Systems", "journal-ref": null, "doi": "10.1016/j.knosys.2016.04.015", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional fact checking by experts and analysts cannot keep pace with the\nvolume of newly created information. It is important and necessary, therefore,\nto enhance our ability to computationally determine whether some statement of\nfact is true or false. We view this problem as a link-prediction task in a\nknowledge graph, and present a discriminative path-based method for fact\nchecking in knowledge graphs that incorporates connectivity, type information,\nand predicate interactions. Given a statement S of the form (subject,\npredicate, object), for example, (Chicago, capitalOf, Illinois), our approach\nmines discriminative paths that alternatively define the generalized statement\n(U.S. city, predicate, U.S. state) and uses the mined rules to evaluate the\nveracity of statement S. We evaluate our approach by examining thousands of\nclaims related to history, geography, biology, and politics using a public,\nmillion node knowledge graph extracted from Wikipedia and PubMedDB. Not only\ndoes our approach significantly outperform related models, we also find that\nthe discriminative predicate path model is easily interpretable and provides\nsensible reasons for the final determination.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 14:31:31 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 14:43:30 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Shi", "Baoxu", ""], ["Weninger", "Tim", ""]]}, {"id": "1510.05963", "submitter": "Pramod Anantharam", "authors": "Amit Sheth, Pramod Anantharam, Cory Henson", "title": "Semantic, Cognitive, and Perceptual Computing: Advances toward Computing\n  for Human Experience", "comments": "13 pages, 4 Figures, IEEE Computer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web continues to evolve and serve as the infrastructure for\ncarrying massive amounts of multimodal and multisensory observations. These\nobservations capture various situations pertinent to people's needs and\ninterests along with all their idiosyncrasies. To support human-centered\ncomputing that empower people in making better and timely decisions, we look\ntowards computation that is inspired by human perception and cognition. Toward\nthis goal, we discuss computing paradigms of semantic computing, cognitive\ncomputing, and an emerging aspect of computing, which we call perceptual\ncomputing. In our view, these offer a continuum to make the most out of vast,\ngrowing, and diverse data pertinent to human needs and interests. We propose\ndetails of perceptual computing characterized by interpretation and exploration\noperations comparable to the interleaving of bottom and top brain processing.\n  This article consists of two parts. First we describe semantic computing,\ncognitive computing, and perceptual computing to lay out distinctions while\nacknowledging their complementary capabilities. We then provide a conceptual\noverview of the newest of these three paradigms--perceptual computing. For\nfurther insights, we focus on an application scenario of asthma management\nconverting massive, heterogeneous and multimodal (big) data into actionable\ninformation or smart data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 16:57:49 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Sheth", "Amit", ""], ["Anantharam", "Pramod", ""], ["Henson", "Cory", ""]]}, {"id": "1510.06143", "submitter": "arXiv Admin", "authors": "Aaron Q. Li, Amr Ahmed, Mu Li, Vanja Josifovski", "title": "High Performance Latent Variable Models", "comments": "arXiv admin note: This paper has been withdrawn due to an\n  irreconcilable author dispute", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models have accumulated a considerable amount of interest\nfrom the industry and academia for their versatility in a wide range of\napplications. A large amount of effort has been made to develop systems that is\nable to extend the systems to a large scale, in the hope to make use of them on\nindustry scale data. In this paper, we describe a system that operates at a\nscale orders of magnitude higher than previous works, and an order of magnitude\nfaster than state-of-the-art system at the same scale, at the same time showing\nmore robustness and more accurate results.\n  Our system uses a number of advances in distributed inference: high\nperformance in synchronization of sufficient statistics with relaxed\nconsistency model; fast sampling, using the Metropolis-Hastings-Walker method\nto overcome dense generative models; statistical modeling, moving beyond Latent\nDirichlet Allocation (LDA) to Pitman-Yor distributions (PDP) and Hierarchical\nDirichlet Process (HDP) models; sophisticated parameter projection schemes, to\nresolve the conflicts within the constraint between parameters arising from the\nrelaxed consistency model.\n  This work significantly extends the domain of applicability of what is\ncommonly known as the Parameter Server. We obtain results with up to hundreds\nbillion oftokens, thousands of topics, and a vocabulary of a few million\ntoken-types, using up to 60,000 processor cores operating on a production\ncluster of a large Internet company. This demonstrates the feasibility to scale\nto problems orders of magnitude larger than any previously published work.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 06:23:55 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 22:39:06 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2015 03:37:21 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2015 05:16:06 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Li", "Aaron Q.", ""], ["Ahmed", "Amr", ""], ["Li", "Mu", ""], ["Josifovski", "Vanja", ""]]}, {"id": "1510.06153", "submitter": "Aaron Li", "authors": "Aaron Q Li, Yuntian Deng, Kublai Jing, Joseph W Robinson", "title": "Creating Scalable and Interactive Web Applications Using High\n  Performance Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project we outline a modularized, scalable system for comparing\nAmazon products in an interactive and informative way using efficient latent\nvariable models and dynamic visualization. We demonstrate how our system can\nbuild on the structure and rich review information of Amazon products in order\nto provide a fast, multifaceted, and intuitive comparison. By providing a\ncondensed per-topic comparison visualization to the user, we are able to\ndisplay aggregate information from the entire set of reviews while providing an\ninterface that is at least as compact as the \"most helpful reviews\" currently\ndisplayed by Amazon, yet far more informative.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 07:29:23 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Li", "Aaron Q", ""], ["Deng", "Yuntian", ""], ["Jing", "Kublai", ""], ["Robinson", "Joseph W", ""]]}, {"id": "1510.06335", "submitter": "Matteo Venanzi", "authors": "Matteo Venanzi, John Guiver, Pushmeet Kohli, Nick Jennings", "title": "Time-Sensitive Bayesian Information Aggregation for Crowdsourcing\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing systems commonly face the problem of aggregating multiple\njudgments provided by potentially unreliable workers. In addition, several\naspects of the design of efficient crowdsourcing processes, such as defining\nworker's bonuses, fair prices and time limits of the tasks, involve knowledge\nof the likely duration of the task at hand. Bringing this together, in this\nwork we introduce a new time--sensitive Bayesian aggregation method that\nsimultaneously estimates a task's duration and obtains reliable aggregations of\ncrowdsourced judgments. Our method, called BCCTime, builds on the key insight\nthat the time taken by a worker to perform a task is an important indicator of\nthe likely quality of the produced judgment. To capture this, BCCTime uses\nlatent variables to represent the uncertainty about the workers' completion\ntime, the tasks' duration and the workers' accuracy. To relate the quality of a\njudgment to the time a worker spends on a task, our model assumes that each\ntask is completed within a latent time window within which all workers with a\npropensity to genuinely attempt the labelling task (i.e., no spammers) are\nexpected to submit their judgments. In contrast, workers with a lower\npropensity to valid labeling, such as spammers, bots or lazy labelers, are\nassumed to perform tasks considerably faster or slower than the time required\nby normal workers. Specifically, we use efficient message-passing Bayesian\ninference to learn approximate posterior probabilities of (i) the confusion\nmatrix of each worker, (ii) the propensity to valid labeling of each worker,\n(iii) the unbiased duration of each task and (iv) the true label of each task.\nUsing two real-world public datasets for entity linking tasks, we show that\nBCCTime produces up to 11% more accurate classifications and up to 100% more\ninformative estimates of a task's duration compared to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 16:42:55 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 21:09:58 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Venanzi", "Matteo", ""], ["Guiver", "John", ""], ["Kohli", "Pushmeet", ""], ["Jennings", "Nick", ""]]}, {"id": "1510.07217", "submitter": "Sixue Liu", "authors": "Sixue Liu", "title": "An Efficient Implementation for WalkSAT", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic local search (SLS) algorithms have exhibited great effectiveness\nin finding models of random instances of the Boolean satisfiability problem\n(SAT). As one of the most widely known and used SLS algorithm, WalkSAT plays a\nkey role in the evolutions of SLS for SAT, and also hold state-of-the-art\nperformance on random instances. This work proposes a novel implementation for\nWalkSAT which decreases the redundant calculations leading to a dramatically\nspeeding up, thus dominates the latest version of WalkSAT including its\nadvanced variants.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 08:11:32 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 03:54:23 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 09:47:18 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Liu", "Sixue", ""]]}, {"id": "1510.07313", "submitter": "Dorsa Sadigh", "authors": "Dorsa Sadigh, Ashish Kapoor", "title": "Safe Control under Uncertainty", "comments": "10 pages, 6 figures, Submitted to HSCC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controller synthesis for hybrid systems that satisfy temporal specifications\nexpressing various system properties is a challenging problem that has drawn\nthe attention of many researchers. However, making the assumption that such\ntemporal properties are deterministic is far from the reality. For example,\nmany of the properties the controller has to satisfy are learned through\nmachine learning techniques based on sensor input data. In this paper, we\npropose a new logic, Probabilistic Signal Temporal Logic (PrSTL), as an\nexpressive language to define the stochastic properties, and enforce\nprobabilistic guarantees on them. We further show how to synthesize safe\ncontrollers using this logic for cyber-physical systems under the assumption\nthat the stochastic properties are based on a set of Gaussian random variables.\nOne of the key distinguishing features of PrSTL is that the encoded logic is\nadaptive and changes as the system encounters additional data and updates its\nbeliefs about the latent random variables that define the safety properties. We\ndemonstrate our approach by synthesizing safe controllers under the PrSTL\nspecifications for multiple case studies including control of quadrotors and\nautonomous vehicles in dynamic environments.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 22:02:48 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Sadigh", "Dorsa", ""], ["Kapoor", "Ashish", ""]]}, {"id": "1510.07389", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Christoph Dann, Christopher G. Lucas, Eric P.\n  Xing", "title": "The Human Kernel", "comments": "11 pages, 5 figures. To appear in Neural Information Processing\n  Systems (NIPS) 2015. Version 2: Figure 2 (i)-(n) now displays the second set\n  of progressive function learning experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian nonparametric models, such as Gaussian processes, provide a\ncompelling framework for automatic statistical modelling: these models have a\nhigh degree of flexibility, and automatically calibrated complexity. However,\nautomating human expertise remains elusive; for example, Gaussian processes\nwith standard kernels struggle on function extrapolation problems that are\ntrivial for human learners. In this paper, we create function extrapolation\nproblems and acquire human responses, and then design a kernel learning\nframework to reverse engineer the inductive biases of human learners across a\nset of behavioral experiments. We use the learned kernels to gain psychological\ninsights and to extrapolate in human-like ways that go beyond traditional\nstationary and polynomial kernels. Finally, we investigate Occam's razor in\nhuman and Gaussian process based function learning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 07:39:47 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 18:21:11 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2015 18:07:35 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Dann", "Christoph", ""], ["Lucas", "Christopher G.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1510.07526", "submitter": "Yang Yu", "authors": "Yang Yu, Wei Zhang, Chung-Wei Hang, Bing Xiang and Bowen Zhou", "title": "Empirical Study on Deep Learning Models for Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore deep learning models with memory component or\nattention mechanism for question answering task. We combine and compare three\nmodels, Neural Machine Translation, Neural Turing Machine, and Memory Networks\nfor a simulated QA data set. This paper is the first one that uses Neural\nMachine Translation and Neural Turing Machines for solving QA tasks. Our\nresults suggest that the combination of attention and memory have potential to\nsolve certain QA problem.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 16:03:27 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 16:56:48 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 15:36:56 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Yu", "Yang", ""], ["Zhang", "Wei", ""], ["Hang", "Chung-Wei", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1510.07787", "submitter": "Kazuki Yoshizoe", "authors": "Kazuki Yoshizoe and Aika Terada and Koji Tsuda", "title": "Redesigning pattern mining algorithms for supercomputers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Upcoming many core processors are expected to employ a distributed memory\narchitecture similar to currently available supercomputers, but parallel\npattern mining algorithms amenable to the architecture are not comprehensively\nstudied. We present a novel closed pattern mining algorithm with a\nwell-engineered communication protocol, and generalize it to find statistically\nsignificant patterns from personal genome data. For distributing communication\nevenly, it employs global load balancing with multiple stacks distributed on a\nset of cores organized as a hypercube with random edges. Our algorithm achieved\nup to 1175-fold speedup by using 1200 cores for solving a problem with 11,914\nitems and 697 transactions, while the naive approach of separating the search\nspace failed completely.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 06:59:14 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Yoshizoe", "Kazuki", ""], ["Terada", "Aika", ""], ["Tsuda", "Koji", ""]]}, {"id": "1510.07889", "submitter": "Peizhi Shi", "authors": "Peizhi Shi and Ke Chen", "title": "Learning Constructive Primitives for Online Level Generation and\n  Real-time Content Adaptation in Super Mario Bros", "comments": "v1 is invalid because a wrong license was chosen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural content generation (PCG) is of great interest to game design and\ndevelopment as it generates game content automatically. Motivated by the recent\nlearning-based PCG framework and other existing PCG works, we propose an\nalternative approach to online content generation and adaptation in Super Mario\nBros (SMB). Unlike most of existing works in SMB, our approach exploits the\nsynergy between rule-based and learning-based methods to produce constructive\nprimitives, quality yet controllable game segments in SMB. As a result, a\ncomplete quality game level can be generated online by integrating relevant\nconstructive primitives via controllable parameters regarding geometrical\nfeatures and procedure-level properties. Also the adaptive content can be\ngenerated in real time by dynamically selecting proper constructive primitives\nvia an adaptation criterion, e.g., dynamic difficulty adjustment (DDA). Our\napproach is of several favorable properties in terms of content quality\nassurance, generation efficiency and controllability. Extensive simulation\nresults demonstrate that the proposed approach can generate controllable yet\nquality game levels online and adaptable content for DDA in real time.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 12:42:54 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 20:09:42 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2015 11:47:32 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Shi", "Peizhi", ""], ["Chen", "Ke", ""]]}, {"id": "1510.08266", "submitter": "Michael Codish", "authors": "Michael Codish and Michael Frank and Avraham Itzhakov and Alice Miller", "title": "Computing the Ramsey Number R(4,3,3) using Abstraction and Symmetry\n  breaking", "comments": "arXiv admin note: text overlap with arXiv:1409.5189", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number $R(4,3,3)$ is often presented as the unknown Ramsey number with\nthe best chances of being found \"soon\". Yet, its precise value has remained\nunknown for almost 50 years. This paper presents a methodology based on\n\\emph{abstraction} and \\emph{symmetry breaking} that applies to solve hard\ngraph edge-coloring problems. The utility of this methodology is demonstrated\nby using it to compute the value $R(4,3,3)=30$. Along the way it is required to\nfirst compute the previously unknown set ${\\cal R}(3,3,3;13)$ consisting of\n78{,}892 Ramsey colorings.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 11:46:46 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 12:06:17 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Codish", "Michael", ""], ["Frank", "Michael", ""], ["Itzhakov", "Avraham", ""], ["Miller", "Alice", ""]]}, {"id": "1510.08525", "submitter": "Christopher Alvin", "authors": "Chris Alvin, Sumit Gulwani, Rupak Majumdar, Supratik Mukhopadhyay", "title": "Automatic Synthesis of Geometry Problems for an Intelligent Tutoring\n  System", "comments": "A formal version of the accepted AAAI '14 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an intelligent tutoring system, GeoTutor, for Euclidean\nGeometry that is automatically able to synthesize proof problems and their\nrespective solutions given a geometric figure together with a set of properties\ntrue of it. GeoTutor can provide personalized practice problems that address\nstudent deficiencies in the subject matter.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 00:10:03 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Alvin", "Chris", ""], ["Gulwani", "Sumit", ""], ["Majumdar", "Rupak", ""], ["Mukhopadhyay", "Supratik", ""]]}, {"id": "1510.08565", "submitter": "Kaisheng Yao", "authors": "Kaisheng Yao and Geoffrey Zweig and Baolin Peng", "title": "Attention with Intention for a Neural Network Conversation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a conversation or a dialogue process, attention and intention play\nintrinsic roles. This paper proposes a neural network based approach that\nmodels the attention and intention processes. It essentially consists of three\nrecurrent networks. The encoder network is a word-level model representing\nsource side sentences. The intention network is a recurrent network that models\nthe dynamics of the intention process. The decoder network is a recurrent\nnetwork produces responses to the input from the source side. It is a language\nmodel that is dependent on the intention and has an attention mechanism to\nattend to particular source side words, when predicting a symbol in the\nresponse. The model is trained end-to-end without labeling data. Experiments\nshow that this model generates natural responses to user inputs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 05:31:28 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 02:17:15 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2015 07:26:01 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Yao", "Kaisheng", ""], ["Zweig", "Geoffrey", ""], ["Peng", "Baolin", ""]]}, {"id": "1510.08568", "submitter": "Wanru Gao", "authors": "Wanru Gao, Samadhi Nallaperuma and Frank Neumann", "title": "Feature-Based Diversity Optimization for Problem Instance Classification", "comments": "20 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the behaviour of heuristic search methods is a challenge. This\neven holds for simple local search methods such as 2-OPT for the Traveling\nSalesperson problem. In this paper, we present a general framework that is able\nto construct a diverse set of instances that are hard or easy for a given\nsearch heuristic. Such a diverse set is obtained by using an evolutionary\nalgorithm for constructing hard or easy instances that are diverse with respect\nto different features of the underlying problem. Examining the constructed\ninstance sets, we show that many combinations of two or three features give a\ngood classification of the TSP instances in terms of whether they are hard to\nbe solved by 2-OPT.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 05:40:54 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 05:12:48 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 08:40:06 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Gao", "Wanru", ""], ["Nallaperuma", "Samadhi", ""], ["Neumann", "Frank", ""]]}, {"id": "1510.08578", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried", "title": "My Reflections on the First Man vs. Machine No-Limit Texas Hold 'em\n  Competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first ever human vs. computer no-limit Texas hold 'em competition took\nplace from April 24-May 8, 2015 at River's Casino in Pittsburgh, PA. In this\narticle I present my thoughts on the competition design, agent architecture,\nand lessons learned.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 06:53:15 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2016 20:41:52 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Ganzfried", "Sam", ""]]}, {"id": "1510.08906", "submitter": "Christoph Dann", "authors": "Christoph Dann, Emma Brunskill", "title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning", "comments": "28 pages, appeared in Neural Information Processing Systems (NIPS)\n  2015, updated version with fixed typos and modified Lemma 1 and Lemma C.5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been significant progress in understanding reinforcement\nlearning in discounted infinite-horizon Markov decision processes (MDPs) by\nderiving tight sample complexity bounds. However, in many real-world\napplications, an interactive learning agent operates for a fixed or bounded\nperiod of time, for example tutoring students for exams or handling customer\nservice requests. Such scenarios can often be better treated as episodic\nfixed-horizon MDPs, for which only looser bounds on the sample complexity\nexist. A natural notion of sample complexity in this setting is the number of\nepisodes required to guarantee a certain performance with high probability (PAC\nguarantee). In this paper, we derive an upper PAC bound $\\tilde\nO(\\frac{|\\mathcal S|^2 |\\mathcal A| H^2}{\\epsilon^2} \\ln\\frac 1 \\delta)$ and a\nlower PAC bound $\\tilde \\Omega(\\frac{|\\mathcal S| |\\mathcal A| H^2}{\\epsilon^2}\n\\ln \\frac 1 {\\delta + c})$ that match up to log-terms and an additional linear\ndependency on the number of states $|\\mathcal S|$. The lower bound is the first\nof its kind for this setting. Our upper bound leverages Bernstein's inequality\nto improve on previous bounds for episodic finite-horizon MDPs which have a\ntime-horizon dependency of at least $H^3$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 21:14:42 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 16:36:03 GMT"}, {"version": "v3", "created": "Wed, 11 May 2016 15:27:28 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Dann", "Christoph", ""], ["Brunskill", "Emma", ""]]}, {"id": "1510.08971", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Robust Subspace Clustering via Tighter Rank Approximation", "comments": "ACM CIKM 2015", "journal-ref": null, "doi": "10.1145/2806416.2806506", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix rank minimization problem is in general NP-hard. The nuclear norm is\nused to substitute the rank function in many recent studies. Nevertheless, the\nnuclear norm approximation adds all singular values together and the\napproximation error may depend heavily on the magnitudes of singular values.\nThis might restrict its capability in dealing with many practical problems. In\nthis paper, an arctangent function is used as a tighter approximation to the\nrank function. We use it on the challenging subspace clustering problem. For\nthis nonconvex minimization problem, we develop an effective optimization\nprocedure based on a type of augmented Lagrange multipliers (ALM) method.\nExtensive experiments on face clustering and motion segmentation show that the\nproposed method is effective for rank approximation.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 05:34:49 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1510.08983", "submitter": "Yu Zhang", "authors": "Yu Zhang and Guoguo Chen and Dong Yu and Kaisheng Yao and Sanjeev\n  Khudanpur and James Glass", "title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the deep long short-term memory (DLSTM) recurrent\nneural networks by introducing gated direct connections between memory cells in\nadjacent layers. These direct links, called highway connections, enable\nunimpeded information flow across different layers and thus alleviate the\ngradient vanishing problem when building deeper LSTMs. We further introduce the\nlatency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole\nhistory while keeping the latency under control. Efficient algorithms are\nproposed to train these novel networks using both frame and sequence\ndiscriminative criteria. Experiments on the AMI distant speech recognition\n(DSR) task indicate that we can train deeper LSTMs and achieve better\nimprovement from sequence training with highway LSTMs (HLSTMs). Our novel model\nobtains $43.9/47.7\\%$ WER on AMI (SDM) dev and eval sets, outperforming all\nprevious works. It beats the strong DNN and DLSTM baselines with $15.7\\%$ and\n$5.3\\%$ relative improvement respectively.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 06:40:14 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 09:48:01 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhang", "Yu", ""], ["Chen", "Guoguo", ""], ["Yu", "Dong", ""], ["Yao", "Kaisheng", ""], ["Khudanpur", "Sanjeev", ""], ["Glass", "James", ""]]}, {"id": "1510.09033", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "Turing's Red Flag", "comments": "To appear in the Communications of the ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sometime in the future we will have to deal with the impact of AI's being\nmistaken for humans. For this reason, I propose that any autonomous system\nshould be designed so that it is unlikely to be mistaken for anything besides\nan autonomous sysem, and should identify itself at the start of any interaction\nwith another agent.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 10:04:11 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "1510.09130", "submitter": "Mingjun Zhong", "authors": "Mingjun Zhong, Nigel Goddard, Charles Sutton", "title": "Latent Bayesian melding for integrating individual and population models", "comments": "11 pages, Advances in Neural Information Processing Systems (NIPS),\n  2015. (Spotlight Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical problems, a more coarse-grained model may be suitable for\npopulation-level behaviour, whereas a more detailed model is appropriate for\naccurate modelling of individual behaviour. This raises the question of how to\nintegrate both types of models. Methods such as posterior regularization follow\nthe idea of generalized moment matching, in that they allow matching\nexpectations between two models, but sometimes both models are most\nconveniently expressed as latent variable models. We propose latent Bayesian\nmelding, which is motivated by averaging the distributions over populations\nstatistics of both the individual-level and the population-level models under a\nlogarithmic opinion pool framework. In a case study on electricity\ndisaggregation, which is a type of single-channel blind source separation\nproblem, we show that latent Bayesian melding leads to significantly more\naccurate predictions than an approach based solely on generalized moment\nmatching.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 15:39:06 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Zhong", "Mingjun", ""], ["Goddard", "Nigel", ""], ["Sutton", "Charles", ""]]}]