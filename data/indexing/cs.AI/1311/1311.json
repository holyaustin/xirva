[{"id": "1311.0351", "submitter": "Bin Yang", "authors": "Bin Yang, Hong Zhao and William Zhu", "title": "Rough matroids based on coverings", "comments": "15pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of covering-based rough sets has made a substantial\ncontribution to the classical rough sets. However, many vital problems in rough\nsets, including attribution reduction, are NP-hard and therefore the algorithms\nfor solving them are usually greedy. Matroid, as a generalization of linear\nindependence in vector spaces, it has a variety of applications in many fields\nsuch as algorithm design and combinatorial optimization. An excellent\nintroduction to the topic of rough matroids is due to Zhu and Wang. On the\nbasis of their work, we study the rough matroids based on coverings in this\npaper. First, we investigate some properties of the definable sets with respect\nto a covering. Specifically, it is interesting that the set of all definable\nsets with respect to a covering, equipped with the binary relation of inclusion\n$\\subseteq$, constructs a lattice. Second, we propose the rough matroids based\non coverings, which are a generalization of the rough matroids based on\nrelations. Finally, some properties of rough matroids based on coverings are\nexplored. Moreover, an equivalent formulation of rough matroids based on\ncoverings is presented. These interesting and important results exhibit many\npotential connections between rough sets and matroids.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 06:58:46 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 01:29:01 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Yang", "Bin", ""], ["Zhao", "Hong", ""], ["Zhu", "William", ""]]}, {"id": "1311.0413", "submitter": "Gordana Dodig Crnkovic", "authors": "Gordana Dodig-Crnkovic", "title": "Information, Computation, Cognition. Agency-based Hierarchies of Levels", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Nature can be seen as informational structure with computational dynamics\n(info-computationalism), where an (info-computational) agent is needed for the\npotential information of the world to actualize. Starting from the definition\nof information as the difference in one physical system that makes a difference\nin another physical system, which combines Bateson and Hewitt definitions, the\nargument is advanced for natural computation as a computational model of the\ndynamics of the physical world where information processing is constantly going\non, on a variety of levels of organization. This setting helps elucidating the\nrelationships between computation, information, agency and cognition, within\nthe common conceptual framework, which has special relevance for biology and\nrobotics.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 21:33:11 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Dodig-Crnkovic", "Gordana", ""]]}, {"id": "1311.0541", "submitter": "Michael Otte", "authors": "Joshua Bialkowski and Michael Otte and Emilio Frazzoli", "title": "Free-configuration Biased Sampling for Motion Planning: Errata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document contains improved and updated proofs of convergence for the\nsampling method presented in our paper \"Free-configuration Biased Sampling for\nMotion Planning\".\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 23:03:04 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Bialkowski", "Joshua", ""], ["Otte", "Michael", ""], ["Frazzoli", "Emilio", ""]]}, {"id": "1311.0716", "submitter": "Michael Laufer Ph.D.", "authors": "Michael Swan Laufer", "title": "Artificial Intelligence in Humans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I put forward that in many instances, thinking mechanisms are\nequivalent to artificial intelligence modules programmed into the human mind.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 14:19:49 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Laufer", "Michael Swan", ""]]}, {"id": "1311.0944", "submitter": "Bin Yang", "authors": "Bin Yang and William Zhu", "title": "Connectivity for matroids based on rough sets", "comments": "16 pages, 8figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mathematics and computer science, connectivity is one of the basic\nconcepts of matroid theory: it asks for the minimum number of elements which\nneed to be removed to disconnect the remaining nodes from each other. It is\nclosely related to the theory of network flow problems. The connectivity of a\nmatroid is an important measure of its robustness as a network. Therefore, it\nis very necessary to investigate the conditions under which a matroid is\nconnected. In this paper, the connectivity for matroids is studied through\nrelation-based rough sets. First, a symmetric and transitive relation is\nintroduced from a general matroid and its properties are explored from the\nviewpoint of matroids. Moreover, through the relation introduced by a general\nmatroid, an undirected graph is generalized. Specifically, the connection of\nthe graph can be investigated by the relation-based rough sets. Second, we\nstudy the connectivity for matroids by means of relation-based rough sets and\nsome conditions under which a general matroid is connected are presented.\nFinally, it is easy to prove that the connectivity for a general matroid with\nsome special properties and its induced undirected graph is equivalent. These\nresults show an important application of relation-based rough sets to matroids.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 01:39:32 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Yang", "Bin", ""], ["Zhu", "William", ""]]}, {"id": "1311.1213", "submitter": "Lav Varshney", "authors": "Lav R. Varshney, Florian Pinel, Kush R. Varshney, Debarun\n  Bhattacharjya, Angela Schoergendorfer, and Yi-Min Chee", "title": "A Big Data Approach to Computational Creativity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational creativity is an emerging branch of artificial intelligence\nthat places computers in the center of the creative process. Broadly,\ncreativity involves a generative step to produce many ideas and a selective\nstep to determine the ones that are the best. Many previous attempts at\ncomputational creativity, however, have not been able to achieve a valid\nselective step. This work shows how bringing data sources from the creative\ndomain and from hedonic psychophysics together with big data analytics\ntechniques can overcome this shortcoming to yield a system that can produce\nnovel and high-quality creative artifacts. Our data-driven approach is\ndemonstrated through a computational creativity system for culinary recipes and\nmenus we developed and deployed, which can operate either autonomously or\nsemi-autonomously with human interaction. We also comment on the volume,\nvelocity, variety, and veracity of data in computational creativity.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 21:00:14 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Varshney", "Lav R.", ""], ["Pinel", "Florian", ""], ["Varshney", "Kush R.", ""], ["Bhattacharjya", "Debarun", ""], ["Schoergendorfer", "Angela", ""], ["Chee", "Yi-Min", ""]]}, {"id": "1311.1436", "submitter": "Suleiman Yerima", "authors": "Suleiman Y. Yerima, Gerard P. Parr, Sally I. McClean and Philip J.\n  Morrow", "title": "Adaptive Measurement-Based Policy-Driven QoS Management with\n  Fuzzy-Rule-based Resource Allocation", "comments": "26 pages, 17 figures", "journal-ref": "Future Internet EISSN 1999-5903", "doi": "10.3390/fi4030646", "report-no": null, "categories": "cs.NI cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Fixed and wireless networks are increasingly converging towards common\nconnectivity with IP-based core networks. Providing effective end-to-end\nresource and QoS management in such complex heterogeneous converged network\nscenarios requires unified, adaptive and scalable solutions to integrate and\nco-ordinate diverse QoS mechanisms of different access technologies with\nIP-based QoS. Policy-Based Network Management (PBNM) is one approach that could\nbe employed to address this challenge. Hence, a policy-based framework for\nend-to-end QoS management in converged networks, CNQF (Converged Networks QoS\nManagement Framework) has been proposed within our project. In this paper, the\nCNQF architecture, a Java implementation of its prototype and experimental\nvalidation of key elements are discussed. We then present a fuzzy-based CNQF\nresource management approach and study the performance of our implementation\nwith real traffic flows on an experimental testbed. The results demonstrate the\nefficacy of our resource-adaptive approach for practical PBNM systems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 16:23:39 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Yerima", "Suleiman Y.", ""], ["Parr", "Gerard P.", ""], ["McClean", "Sally I.", ""], ["Morrow", "Philip J.", ""]]}, {"id": "1311.1632", "submitter": "Frank Loebe", "authors": "Heinrich Herre", "title": "Persistence, Change, and the Integration of Objects and Processes in the\n  Framework of the General Formal Ontology", "comments": "13 pages; minor revisions (compared to version 1), mainly wording and\n  typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss various problems, associated to temporal phenomena.\nThese problems include persistence and change, the integration of objects and\nprocesses, and truth-makers for temporal propositions. We propose an approach\nwhich interprets persistence as a phenomenon emanating from the activity of the\nmind, and which, additionally, postulates that persistence, finally, rests on\npersonal identity. The General Formal Ontology (GFO) is a top level ontology\nbeing developed at the University of Leipzig. Top level ontologies can be\nroughly divided into 3D-ontologies, and 4D-ontologies. GFO is the only top\nlevel ontology, used in applications, which is a 4D-ontology admitting\nadditionally 3D objects. Objects and processes are integrated in a natural way.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 10:45:54 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2013 00:14:14 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Herre", "Heinrich", ""]]}, {"id": "1311.1704", "submitter": "Prem Gopalan", "authors": "Prem Gopalan, Jake M. Hofman, David M. Blei", "title": "Scalable Recommendation with Poisson Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian Poisson matrix factorization model for forming\nrecommendations from sparse user behavior data. These data are large user/item\nmatrices where each user has provided feedback on only a small subset of items,\neither explicitly (e.g., through star ratings) or implicitly (e.g., through\nviews or purchases). In contrast to traditional matrix factorization\napproaches, Poisson factorization implicitly models each user's limited\nattention to consume items. Moreover, because of the mathematical form of the\nPoisson likelihood, the model needs only to explicitly consider the observed\nentries in the matrix, leading to both scalable computation and good predictive\nperformance. We develop a variational inference algorithm for approximate\nposterior inference that scales up to massive data sets. This is an efficient\nalgorithm that iterates over the observed entries and adjusts an approximate\nposterior over the user/item representations. We apply our method to large\nreal-world user data containing users rating movies, users listening to songs,\nand users reading scientific papers. In all these settings, Bayesian Poisson\nfactorization outperforms state-of-the-art matrix factorization methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 14:58:40 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 17:23:05 GMT"}, {"version": "v3", "created": "Tue, 20 May 2014 19:19:30 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Gopalan", "Prem", ""], ["Hofman", "Jake M.", ""], ["Blei", "David M.", ""]]}, {"id": "1311.1761", "submitter": "Sergey Levine", "authors": "Sergey Levine", "title": "Exploring Deep and Recurrent Architectures for Optimal Control", "comments": "Appears in the Neural Information Processing Systems (NIPS 2013)\n  Workshop on Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sophisticated multilayer neural networks have achieved state of the art\nresults on multiple supervised tasks. However, successful applications of such\nmultilayer networks to control have so far been limited largely to the\nperception portion of the control pipeline. In this paper, we explore the\napplication of deep and recurrent neural networks to a continuous,\nhigh-dimensional locomotion task, where the network is used to represent a\ncontrol policy that maps the state of the system (represented by joint angles)\ndirectly to the torques at each joint. By using a recent reinforcement learning\nalgorithm called guided policy search, we can successfully train neural network\ncontrollers with thousands of parameters, allowing us to compare a variety of\narchitectures. We discuss the differences between the locomotion control task\nand previous supervised perception tasks, present experimental results\ncomparing various architectures, and discuss future directions in the\napplication of techniques from deep learning to the problem of optimal control.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 17:39:31 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["Levine", "Sergey", ""]]}, {"id": "1311.1764", "submitter": "Amel Grissa", "authors": "Amel Grissa Touzi, Hela Ben Massoud and Alaya Ayadi", "title": "Automatic ontology generation for data mining using fca and clustering", "comments": "10pages, 8 figures KEOD 2013, accepted but not enregistrement De:\n  KEOD Secretariat [keod.secretariat@insticc.org] Envoy\\'e: mardi 21 mai 2013\n  10:47 We are happy to inform you that the regular paper you have submitted to\n  KEOD, with number 34, entitled \"Automatic Ontology Generation for Data Mining\n  Using FCA and Clustering\", has been accepted as a Short Paper. arXiv admin\n  note: text overlap with arXiv:1310.7829 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increased need for formalized representations of the domain\nof Data Mining, the success of using Formal Concept Analysis (FCA) and Ontology\nin several Computer Science fields, we present in this paper a new approach for\nautomatic generation of Fuzzy Ontology of Data Mining (FODM), through the\nfusion of conceptual clustering, fuzzy logic, and FCA. In our approach, we\npropose to generate ontology taking in consideration another degree of\ngranularity into the process of generation. Indeed, we suggest to define an\nontology between classes resulting from a preliminary classification on the\ndata. We prove that this approach optimize the definition of the ontology,\noffered a better interpretation of the data and optimized both the space memory\nand the execution time for exploiting this data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 17:48:24 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["Touzi", "Amel Grissa", ""], ["Massoud", "Hela Ben", ""], ["Ayadi", "Alaya", ""]]}, {"id": "1311.1935", "submitter": "Serge Smidtas", "authors": "Serge Smidtas, Magalie Peyrot", "title": "Unsupervised learning human's activities by overexpressed recognized\n  non-speech sounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity and environment produces sounds such as, at home, the noise\nproduced by water, cough, or television. These sounds can be used to determine\nthe activity in the environment. The objective is to monitor a person's\nactivity or determine his environment using a single low cost microphone by\nsound analysis. The purpose is to adapt programs to the activity or environment\nor detect abnormal situations. Some patterns of over expressed repeatedly in\nthe sequences of recognized sounds inter and intra environment allow to\ncharacterize activities such as the entrance of a person in the house, or a tv\nprogram watched. We first manually annotated 1500 sounds of daily life activity\nof old persons living at home recognized sounds. Then we inferred an ontology\nand enriched the database of annotation with a crowed sourced manual annotation\nof 7500 sounds to help with the annotation of the most frequent sounds. Using\nlearning sound algorithms, we defined 50 types of the most frequent sounds. We\nused this set of recognizable sounds as a base to tag sounds and put tags on\nthem. By using over expressed number of motifs of sequences of the tags, we\nwere able to categorize using only a single low-cost microphone, complex\nactivities of daily life of a persona at home as watching TV, entrance in the\napartment of a person, or phone conversation including detecting unknown\nactivities as repeated tasks performed by users.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 11:18:42 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Smidtas", "Serge", ""], ["Peyrot", "Magalie", ""]]}, {"id": "1311.2106", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer and Jeff Bilmes", "title": "Submodular Optimization with Submodular Cover and Submodular Knapsack\n  Constraints", "comments": "23 pages. A short version of this appeared in Advances of NIPS-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two new optimization problems -- minimizing a submodular\nfunction subject to a submodular lower bound constraint (submodular cover) and\nmaximizing a submodular function subject to a submodular upper bound constraint\n(submodular knapsack). We are motivated by a number of real-world applications\nin machine learning including sensor placement and data subset selection, which\nrequire maximizing a certain submodular function (like coverage or diversity)\nwhile simultaneously minimizing another (like cooperative cost). These problems\nare often posed as minimizing the difference between submodular functions [14,\n35] which is in the worst case inapproximable. We show, however, that by\nphrasing these problems as constrained optimization, which is more natural for\nmany applications, we achieve a number of bounded approximation guarantees. We\nalso show that both these problems are closely related and an approximation\nalgorithm solving one can be used to obtain an approximation guarantee for the\nother. We provide hardness results for both problems thus showing that our\napproximation factors are tight up to log-factors. Finally, we empirically\ndemonstrate the performance and good scalability properties of our algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 23:28:02 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Iyer", "Rishabh", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1311.2531", "submitter": "Tom Froese", "authors": "Tom Froese, Nathaniel Virgo and Takashi Ikegami", "title": "Motility at the origin of life: Its characterization and a model", "comments": "29 pages, 5 figures, Artificial Life", "journal-ref": null, "doi": "10.1162/ARTL_a_00096", "report-no": null, "categories": "cs.AI cs.NE nlin.AO nlin.PS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to recent advances in synthetic biology and artificial life, the origin\nof life is currently a hot topic of research. We review the literature and\nargue that the two traditionally competing \"replicator-first\" and\n\"metabolism-first\" approaches are merging into one integrated theory of\nindividuation and evolution. We contribute to the maturation of this more\ninclusive approach by highlighting some problematic assumptions that still lead\nto an impoverished conception of the phenomenon of life. In particular, we\nargue that the new consensus has so far failed to consider the relevance of\nintermediate timescales. We propose that an adequate theory of life must\naccount for the fact that all living beings are situated in at least four\ndistinct timescales, which are typically associated with metabolism, motility,\ndevelopment, and evolution. On this view, self-movement, adaptive behavior and\nmorphological changes could have already been present at the origin of life. In\norder to illustrate this possibility we analyze a minimal model of life-like\nphenomena, namely of precarious, individuated, dissipative structures that can\nbe found in simple reaction-diffusion systems. Based on our analysis we suggest\nthat processes in intermediate timescales could have already been operative in\nprebiotic systems. They may have facilitated and constrained changes occurring\nin the faster- and slower-paced timescales of chemical self-individuation and\nevolution by natural selection, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 18:58:46 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Froese", "Tom", ""], ["Virgo", "Nathaniel", ""], ["Ikegami", "Takashi", ""]]}, {"id": "1311.2702", "submitter": "Tobias Kuhn", "authors": "Tobias Kuhn, Alexandre Bergel", "title": "Verifiable Source Code Documentation in Controlled Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing documentation about software internals is rarely considered a\nrewarding activity. It is highly time-consuming and the resulting documentation\nis fragile when the software is continuously evolving in a multi-developer\nsetting. Unfortunately, traditional programming environments poorly support the\nwriting and maintenance of documentation. Consequences are severe as the lack\nof documentation on software structure negatively impacts the overall quality\nof the software product. We show that using a controlled natural language with\na reasoner and a query engine is a viable technique for verifying the\nconsistency and accuracy of documentation and source code. Using ACE, a\nstate-of-the-art controlled natural language, we present positive results on\nthe comprehensibility and the general feasibility of creating and verifying\ndocumentation. As a case study, we used automatic documentation verification to\nidentify and fix severe flaws in the architecture of a non-trivial piece of\nsoftware. Moreover, a user experiment shows that our language is faster and\neasier to learn and understand than other formal languages for software\ndocumentation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 07:44:10 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Kuhn", "Tobias", ""], ["Bergel", "Alexandre", ""]]}, {"id": "1311.2886", "submitter": "Mustafa  Ayhan", "authors": "Mustafa Batuhan Ayhan", "title": "A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a\n  Gear Motor Company", "comments": "Published in \"International Journal of Managing Value and Supply\n  Chains (IJMVSC) Vol.4, No. 3, September 2013\"", "journal-ref": "International Journal of Managing Value and Supply Chains (IJMVSC)\n  Vol.4, No. 3, September 2013", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suuplier selection is one of the most important functions of a purchasing\ndepartment. Since by deciding the best supplier, companies can save material\ncosts and increase competitive advantage.However this decision becomes\ncompilcated in case of multiple suppliers, multiple conflicting criteria, and\nimprecise parameters. In addition the uncertainty and vagueness of the experts'\nopinion is the prominent characteristic of the problem. therefore an\nextensively used multi criteria decision making tool Fuzzy AHP can be utilized\nas an approach for supplier selection problem. This paper reveals the\napplication of Fuzzy AHP in a gear motor company determining the best supplier\nwith respect to selected criteria. the contribution of this study is not only\nthe application of the Fuzzy AHP methodology for supplier selection problem,\nbut also releasing a comprehensive literature review of multi criteria decision\nmaking problems. In addition by stating the steps of Fuzzy AHP clearly and\nnumerically, this study can be a guide of the methodology to be implemented to\nother multiple criteria decision making problems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 08:50:30 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Ayhan", "Mustafa Batuhan", ""]]}, {"id": "1311.2912", "submitter": "Michael Laufer Ph.D.", "authors": "Michael S. Laufer", "title": "A Misanthropic Reinterpretation of the Chinese Room Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chinese room problem asks if computers can think; I ask here if most\nhumans can.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2013 20:51:24 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Laufer", "Michael S.", ""]]}, {"id": "1311.2914", "submitter": "R\\'emi Lemoy", "authors": "R\\'emi Lemoy, Mikko Alava and Erik Aurell", "title": "A novel local search based on variable-focusing for random K-SAT", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new local search algorithm for satisfiability problems. Usual\napproaches focus uniformly on unsatisfied clauses. The new method works by\npicking uniformly random variables in unsatisfied clauses. A Variable-based\nFocused Metropolis Search (V-FMS) is then applied to random 3-SAT. We show that\nit is quite comparable in performance to the clause-based FMS. Consequences for\nalgorithmic design are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 15:56:27 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2013 13:09:14 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Lemoy", "R\u00e9mi", ""], ["Alava", "Mikko", ""], ["Aurell", "Erik", ""]]}, {"id": "1311.3198", "submitter": "Marie-Laure Mugnier", "authors": "M\\'elanie K\\\"onig and Michel Lecl\\`ere and Marie-Laure Mugnier and\n  Micha\\\"el Thomazo", "title": "Sound, Complete and Minimal UCQ-Rewriting for Existential Rules", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": "Report-no: LIRMM-RR-13034", "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of Ontology-Based Data Access, with ontologies\nrepresented in the framework of existential rules, also known as Datalog+/-. A\nwell-known approach involves rewriting the query using ontological knowledge.\nWe focus here on the basic rewriting technique which consists of rewriting the\ninitial query into a union of conjunctive queries. First, we study a generic\nbreadth-first rewriting algorithm, which takes as input any rewriting operator,\nand define properties of rewriting operators that ensure the correctness of the\nalgorithm. Then, we focus on piece-unifiers, which provide a rewriting operator\nwith the desired properties. Finally, we propose an implementation of this\nframework and report some experiments.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 16:36:35 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["K\u00f6nig", "M\u00e9lanie", ""], ["Lecl\u00e8re", "Michel", ""], ["Mugnier", "Marie-Laure", ""], ["Thomazo", "Micha\u00ebl", ""]]}, {"id": "1311.3353", "submitter": "Roberto Amadini", "authors": "Roberto Amadini, Maurizio Gabbrielli, Jacopo Mauro", "title": "SUNNY: a Lazy Portfolio Approach for Constraint Solving", "comments": null, "journal-ref": "Theory and Practice of Logic Programming 14 (2014) 509-524", "doi": "10.1017/S1471068414000179", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  *** To appear in Theory and Practice of Logic Programming (TPLP) ***\n  Within the context of constraint solving, a portfolio approach allows one to\nexploit the synergy between different solvers in order to create a globally\nbetter solver. In this paper we present SUNNY: a simple and flexible algorithm\nthat takes advantage of a portfolio of constraint solvers in order to compute\n--- without learning an explicit model --- a schedule of them for solving a\ngiven Constraint Satisfaction Problem (CSP). Motivated by the performance\nreached by SUNNY vs. different simulations of other state of the art\napproaches, we developed sunny-csp, an effective portfolio solver that exploits\nthe underlying SUNNY algorithm in order to solve a given CSP. Empirical tests\nconducted on exhaustive benchmarks of MiniZinc models show that the actual\nperformance of SUNNY conforms to the predictions. This is encouraging both for\nimproving the power of CSP portfolio solvers and for trying to export them to\nfields such as Answer Set Programming and Constraint Logic Programming.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 00:37:22 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 16:58:32 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Amadini", "Roberto", ""], ["Gabbrielli", "Maurizio", ""], ["Mauro", "Jacopo", ""]]}, {"id": "1311.3355", "submitter": "Yongqun He", "authors": "Yongqun He, Zoushuang Xiang", "title": "HINO: a BFO-aligned ontology representing human molecular interactions\n  and pathways", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB q-bio.MN", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Many database resources, such as Reactome, collect manually annotated\nreactions, interactions, and pathways from peer-reviewed publications. The\ninteractors (e.g., a protein), interactions, and pathways in these data\nresources are often represented as instances in using BioPAX, a standard\npathway data exchange format. However, these interactions are better\nrepresented as classes (or universals) since they always occur given\nappropriate conditions. This study aims to represent various human interaction\npathways and networks as classes via a formal ontology aligned with the Basic\nFormal Ontology (BFO). Towards this goal, the Human Interaction Network\nOntology (HINO) was generated by extending the BFO-aligned Interaction Network\nOntology (INO). All human pathways and associated processes and interactors\nlisted in Reactome and represented in BioPAX were first converted to ontology\nclasses by aligning them under INO. Related terms and associated relations and\nhierarchies from external ontologies (e.g., CHEBI and GO) were also retrieved\nand imported into HINO. HINO ontology terms were resolved in the linked\nontology data server Ontobee. The RDF triples stored in the RDF triple store\nare queryable through a SPARQL program. Such an ontology system supports\nadvanced pathway data integration and applications.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 01:07:11 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["He", "Yongqun", ""], ["Xiang", "Zoushuang", ""]]}, {"id": "1311.3368", "submitter": "Sameer Singh", "authors": "Sameer Singh and Sebastian Riedel and Andrew McCallum", "title": "Anytime Belief Propagation Using Sparse Domains", "comments": "NIPS 2013 Workshop on Resource-Efficient Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Belief Propagation has been widely used for marginal inference, however it is\nslow on problems with large-domain variables and high-order factors. Previous\nwork provides useful approximations to facilitate inference on such models, but\nlacks important anytime properties such as: 1) providing accurate and\nconsistent marginals when stopped early, 2) improving the approximation when\nrun longer, and 3) converging to the fixed point of BP. To this end, we propose\na message passing algorithm that works on sparse (partially instantiated)\ndomains, and converges to consistent marginals using dynamic message\nscheduling. The algorithm grows the sparse domains incrementally, selecting the\nnext value to add using prioritization schemes based on the gradients of the\nmarginal inference objective. Our experiments demonstrate local anytime\nconsistency and fast convergence, providing significant speedups over BP to\nobtain low-error marginals: up to 25 times on grid models, and up to 6 times on\na real-world natural language processing task.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 02:39:45 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Singh", "Sameer", ""], ["Riedel", "Sebastian", ""], ["McCallum", "Andrew", ""]]}, {"id": "1311.3735", "submitter": "Nicola Di Mauro", "authors": "Nicola Di Mauro and Floriana Esposito", "title": "Ensemble Relational Learning based on Selective Propositionalization", "comments": "10 pages. arXiv admin note: text overlap with arXiv:1006.5188", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with structured data needs the use of expressive representation\nformalisms that, however, puts the problem to deal with the computational\ncomplexity of the machine learning process. Furthermore, real world domains\nrequire tools able to manage their typical uncertainty. Many statistical\nrelational learning approaches try to deal with these problems by combining the\nconstruction of relevant relational features with a probabilistic tool. When\nthe combination is static (static propositionalization), the constructed\nfeatures are considered as boolean features and used offline as input to a\nstatistical learner; while, when the combination is dynamic (dynamic\npropositionalization), the feature construction and probabilistic tool are\ncombined into a single process. In this paper we propose a selective\npropositionalization method that search the optimal set of relational features\nto be used by a probabilistic learner in order to minimize a loss function. The\nnew propositionalization approach has been combined with the random subspace\nensemble method. Experiments on real-world datasets shows the validity of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 06:14:15 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Di Mauro", "Nicola", ""], ["Esposito", "Floriana", ""]]}, {"id": "1311.3800", "submitter": "Mohammad Mehdi Keikha", "authors": "Mohammad Mehdi Keikha and Mohammad Ali Nematbakhsh and Behrouz Tork\n  Ladani", "title": "Structural Weights in Ontology Matching", "comments": null, "journal-ref": "2013-Vol 4- International Journal of Web & Semantic Technology\n  (IJWesT)", "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology matching finds correspondences between similar entities of different\nontologies. Two ontologies may be similar in some aspects such as structure,\nsemantic etc. Most ontology matching systems integrate multiple matchers to\nextract all the similarities that two ontologies may have. Thus, we face a\nmajor problem to aggregate different similarities. Some matching systems use\nexperimental weights for aggregation of similarities among different matchers\nwhile others use machine learning approaches and optimization algorithms to\nfind optimal weights to assign to different matchers. However, both approaches\nhave their own deficiencies. In this paper, we will point out the problems and\nshortcomings of current similarity aggregation strategies. Then, we propose a\nnew strategy, which enables us to utilize the structural information of\nontologies to get weights of matchers, for the similarity aggregation task. For\nachieving this goal, we create a new Ontology Matching system which it uses\nthree available matchers, namely GMO, ISub and VDoc. We have tested our\nsimilarity aggregation strategy on the OAEI 2012 data set. Experimental results\nshow significant improvements in accuracies of several cases, especially in\nmatching the classes of ontologies. We will compare the performance of our\nsimilarity aggregation strategy with other well-known strategies\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 10:34:31 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Keikha", "Mohammad Mehdi", ""], ["Nematbakhsh", "Mohammad Ali", ""], ["Ladani", "Behrouz Tork", ""]]}, {"id": "1311.3829", "submitter": "Sofia Benbelkacem", "authors": "Sofia Benbelkacem, Baghdad Atmani, Mohamed Benamina", "title": "Planning based on classification by induction graph", "comments": "International Conference on Data Mining & Knowledge Management\n  Process CDKP-2013", "journal-ref": null, "doi": "10.5121/csit.2013.3823", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Artificial Intelligence, planning refers to an area of research that\nproposes to develop systems that can automatically generate a result set, in\nthe form of an integrated decision-making system through a formal procedure,\nknown as plan. Instead of resorting to the scheduling algorithms to generate\nplans, it is proposed to operate the automatic learning by decision tree to\noptimize time. In this paper, we propose to build a classification model by\ninduction graph from a learning sample containing plans that have an associated\nset of descriptors whose values change depending on each plan. This model will\nthen operate for classifying new cases by assigning the appropriate plan.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 12:43:56 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Benbelkacem", "Sofia", ""], ["Atmani", "Baghdad", ""], ["Benamina", "Mohamed", ""]]}, {"id": "1311.3959", "submitter": "M M Hassan Mahmud", "authors": "M. M. Hassan Mahmud, Majd Hawasly, Benjamin Rosman, Subramanian\n  Ramamoorthy", "title": "Clustering Markov Decision Processes For Continual Transfer", "comments": "56 pages, Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms to effectively represent a set of Markov decision\nprocesses (MDPs), whose optimal policies have already been learned, by a\nsmaller source subset for lifelong, policy-reuse-based transfer learning in\nreinforcement learning. This is necessary when the number of previous tasks is\nlarge and the cost of measuring similarity counteracts the benefit of transfer.\nThe source subset forms an `$\\epsilon$-net' over the original set of MDPs, in\nthe sense that for each previous MDP $M_p$, there is a source $M^s$ whose\noptimal policy has $<\\epsilon$ regret in $M_p$. Our contributions are as\nfollows. We present EXP-3-Transfer, a principled policy-reuse algorithm that\noptimally reuses a given source policy set when learning for a new MDP. We\npresent a framework to cluster the previous MDPs to extract a source subset.\nThe framework consists of (i) a distance $d_V$ over MDPs to measure\npolicy-based similarity between MDPs; (ii) a cost function $g(\\cdot)$ that uses\n$d_V$ to measure how good a particular clustering is for generating useful\nsource tasks for EXP-3-Transfer and (iii) a provably convergent algorithm,\nMHAV, for finding the optimal clustering. We validate our algorithms through\nexperiments in a surveillance domain.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 19:40:58 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2016 23:26:39 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2016 22:20:37 GMT"}, {"version": "v4", "created": "Sun, 1 May 2016 12:27:39 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Mahmud", "M. M. Hassan", ""], ["Hawasly", "Majd", ""], ["Rosman", "Benjamin", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1311.3982", "submitter": "Juston Moore", "authors": "Aaron Schein, Juston Moore, Hanna Wallach", "title": "Inferring Multilateral Relations from Dynamic Pairwise Interactions", "comments": "NIPS 2013 Workshop on Frontiers of Network Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlations between anomalous activity patterns can yield pertinent\ninformation about complex social processes: a significant deviation from normal\nbehavior, exhibited simultaneously by multiple pairs of actors, provides\nevidence for some underlying relationship involving those pairs---i.e., a\nmultilateral relation. We introduce a new nonparametric Bayesian latent\nvariable model that explicitly captures correlations between anomalous\ninteraction counts and uses these shared deviations from normal activity\npatterns to identify and characterize multilateral relations. We showcase our\nmodel's capabilities using the newly curated Global Database of Events,\nLocation, and Tone, a dataset that has seen considerable interest in the social\nsciences and the popular press, but which has is largely unexplored by the\nmachine learning community. We provide a detailed analysis of the latent\nstructure inferred by our model and show that the multilateral relations\ncorrespond to major international events and long-term international\nrelationships. These findings lead us to recommend our model for any\ndata-driven analysis of interaction networks where dynamic interactions over\nthe edges provide evidence for latent social structure.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 21:22:37 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Schein", "Aaron", ""], ["Moore", "Juston", ""], ["Wallach", "Hanna", ""]]}, {"id": "1311.4056", "submitter": "Xinyang Deng", "authors": "Hongming Mo, Xiaoyan Su, Yong Hu, Yong Deng", "title": "A generalized evidence distance", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dempster-Shafer theory of evidence (D-S theory) is widely used in uncertain\ninformation process. The basic probability assignment(BPA) is a key element in\nD-S theory. How to measure the distance between two BPAs is an open issue. In\nthis paper, a new method to measure the distance of two BPAs is proposed. The\nproposed method is a generalized of existing evidence distance. Numerical\nexamples are illustrated that the proposed method can overcome the shortcomings\nof existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 13:09:14 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Mo", "Hongming", ""], ["Su", "Xiaoyan", ""], ["Hu", "Yong", ""], ["Deng", "Yong", ""]]}, {"id": "1311.4064", "submitter": "Nate Derbinsky", "authors": "Nate Derbinsky, Jos\\'e Bento, Jonathan S. Yedidia", "title": "Methods for Integrating Knowledge with the Three-Weight Optimization\n  Algorithm for Hybrid Cognitive Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider optimization as an approach for quickly and\nflexibly developing hybrid cognitive capabilities that are efficient, scalable,\nand can exploit knowledge to improve solution speed and quality. In this\ncontext, we focus on the Three-Weight Algorithm, which aims to solve general\noptimization problems. We propose novel methods by which to integrate knowledge\nwith this algorithm to improve expressiveness, efficiency, and scaling, and\ndemonstrate these techniques on two example problems (Sudoku and circle\npacking).\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 14:03:31 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Derbinsky", "Nate", ""], ["Bento", "Jos\u00e9", ""], ["Yedidia", "Jonathan S.", ""]]}, {"id": "1311.4086", "submitter": "Abdelhak Mansoul", "authors": "Abdelhak Mansoul, Baghdad Atmani, Sofia Benbelkacem", "title": "A hybrid decision support system : application on healthcare", "comments": "13 pages, 4 figures, SEAS 2013 Dubai conference, paper id : 16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many systems based on knowledge, especially expert systems for medical\ndecision support have been developed. Only systems are based on production\nrules, and cannot learn and evolve only by updating them. In addition, taking\ninto account several criteria induces an exorbitant number of rules to be\ninjected into the system. It becomes difficult to translate medical knowledge\nor a support decision as a simple rule. Moreover, reasoning based on generic\ncases became classic and can even reduce the range of possible solutions. To\nremedy that, we propose an approach based on using a multi-criteria decision\nguided by a case-based reasoning (CBR) approach.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 18:13:42 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Mansoul", "Abdelhak", ""], ["Atmani", "Baghdad", ""], ["Benbelkacem", "Sofia", ""]]}, {"id": "1311.4166", "submitter": "Xinyang Deng", "authors": "Shiyu Chen, Yong Hu, Sankaran Mahadevan, Yong Deng", "title": "A Visibility Graph Averaging Aggregation Operator", "comments": "33 pages, 9 figures", "journal-ref": null, "doi": "10.1016/j.physa.2014.02.015", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of aggregation is considerable importance in many disciplines. In\nthis paper, a new type of operator called visibility graph averaging (VGA)\naggregation operator is proposed. This proposed operator is based on the\nvisibility graph which can convert a time series into a graph. The weights are\nobtained according to the importance of the data in the visibility graph.\nFinally, the VGA operator is used in the analysis of the TAIEX database to\nillustrate that it is practical and compared with the classic aggregation\noperators, it shows its advantage that it not only implements the aggregation\nof the data purely, but also conserves the time information, and meanwhile, the\ndetermination of the weights is more reasonable.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 15:01:31 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Chen", "Shiyu", ""], ["Hu", "Yong", ""], ["Mahadevan", "Sankaran", ""], ["Deng", "Yong", ""]]}, {"id": "1311.4180", "submitter": "Volker Tresp", "authors": "Volker Tresp, Sonja Zillner, Maria J. Costa, Yi Huang, Alexander\n  Cavallaro, Peter A. Fasching, Andre Reis, Martin Sedlmayr, Thomas Ganslandt,\n  Klemens Budde, Carl Hinrichs, Danilo Schmidt, Philipp Daumke, Daniel Sonntag,\n  Thomas Wittenberg, Patricia G. Oppelt, Denis Krompass", "title": "Towards a New Science of a Clinical Data Intelligence", "comments": "NIPS 2013 Workshop: Machine Learning for Clinical Data Analysis and\n  Healthcare, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we define Clinical Data Intelligence as the analysis of data\ngenerated in the clinical routine with the goal of improving patient care. We\ndefine a science of a Clinical Data Intelligence as a data analysis that\npermits the derivation of scientific, i.e., generalizable and reliable results.\nWe argue that a science of a Clinical Data Intelligence is sensible in the\ncontext of a Big Data analysis, i.e., with data from many patients and with\ncomplete patient information. We discuss that Clinical Data Intelligence\nrequires the joint efforts of knowledge engineering, information extraction\n(from textual and other unstructured data), and statistics and statistical\nmachine learning. We describe some of our main results as conjectures and\nrelate them to a recently funded research project involving two major German\nuniversity hospitals.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 16:24:22 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 17:51:10 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2013 09:58:22 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Tresp", "Volker", ""], ["Zillner", "Sonja", ""], ["Costa", "Maria J.", ""], ["Huang", "Yi", ""], ["Cavallaro", "Alexander", ""], ["Fasching", "Peter A.", ""], ["Reis", "Andre", ""], ["Sedlmayr", "Martin", ""], ["Ganslandt", "Thomas", ""], ["Budde", "Klemens", ""], ["Hinrichs", "Carl", ""], ["Schmidt", "Danilo", ""], ["Daumke", "Philipp", ""], ["Sonntag", "Daniel", ""], ["Wittenberg", "Thomas", ""], ["Oppelt", "Patricia G.", ""], ["Krompass", "Denis", ""]]}, {"id": "1311.4319", "submitter": "Lars Kotthoff", "authors": "Lars Kotthoff", "title": "Ranking Algorithms by Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common way of doing algorithm selection is to train a machine learning\nmodel and predict the best algorithm from a portfolio to solve a particular\nproblem. While this method has been highly successful, choosing only a single\nalgorithm has inherent limitations -- if the choice was bad, no remedial action\ncan be taken and parallelism cannot be exploited, to name but a few problems.\nIn this paper, we investigate how to predict the ranking of the portfolio\nalgorithms on a particular problem. This information can be used to choose the\nsingle best algorithm, but also to allocate resources to the algorithms\naccording to their rank. We evaluate a range of approaches to predict the\nranking of a set of algorithms on a problem. We furthermore introduce a\nframework for categorizing ranking predictions that allows to judge the\nexpressiveness of the predictive output. Our experimental evaluation\ndemonstrates on a range of data sets from the literature that it is beneficial\nto consider the relationship between algorithms when predicting rankings. We\nfurthermore show that relatively naive approaches deliver rankings of good\nquality already.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 10:22:53 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Kotthoff", "Lars", ""]]}, {"id": "1311.4527", "submitter": "Jose Bento", "authors": "Jose Bento, Nate Derbinsky, Javier Alonso-Mora, Jonathan Yedidia", "title": "A message-passing algorithm for multi-agent trajectory planning", "comments": "In Advances in Neural Information Processing Systems (NIPS), 2013.\n  Demo video available at http://www.youtube.com/watch?v=yuGCkVT8Bew", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.MA cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel approach for computing collision-free \\emph{global}\ntrajectories for $p$ agents with specified initial and final configurations,\nbased on an improved version of the alternating direction method of multipliers\n(ADMM). Compared with existing methods, our approach is naturally\nparallelizable and allows for incorporating different cost functionals with\nonly minor adjustments. We apply our method to classical challenging instances\nand observe that its computational requirements scale well with $p$ for several\ncost functionals. We also show that a specialization of our algorithm can be\nused for {\\em local} motion planning by solving the problem of joint\noptimization in velocity space.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 20:38:57 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Bento", "Jose", ""], ["Derbinsky", "Nate", ""], ["Alonso-Mora", "Javier", ""], ["Yedidia", "Jonathan", ""]]}, {"id": "1311.4564", "submitter": "Sofia Benbelkacem", "authors": "Baghdad Atmani, Sofia Benbelkacem and Mohamed Benamina", "title": "Planning by case-based reasoning based on fuzzy logic", "comments": "International Conference of Artificial Intelligence and Fuzzy Logic\n  AIFL-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The treatment of complex systems often requires the manipulation of vague,\nimprecise and uncertain information. Indeed, the human being is competent in\nhandling of such systems in a natural way. Instead of thinking in mathematical\nterms, humans describes the behavior of the system by language proposals. In\norder to represent this type of information, Zadeh proposed to model the\nmechanism of human thought by approximate reasoning based on linguistic\nvariables. He introduced the theory of fuzzy sets in 1965, which provides an\ninterface between language and digital worlds. In this paper, we propose a\nBoolean modeling of the fuzzy reasoning that we baptized Fuzzy-BML and uses the\ncharacteristics of induction graph classification. Fuzzy-BML is the process by\nwhich the retrieval phase of a CBR is modelled not in the conventional form of\nmathematical equations, but in the form of a database with membership functions\nof fuzzy rules.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 21:29:32 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Atmani", "Baghdad", ""], ["Benbelkacem", "Sofia", ""], ["Benamina", "Mohamed", ""]]}, {"id": "1311.4639", "submitter": "Katsumi Inoue", "authors": "Katsumi Inoue and Chiaki Sakama (Editors)", "title": "Post-Proceedings of the First International Workshop on Learning and\n  Nonmonotonic Reasoning", "comments": "67 pages, 5 papers, 1 abstract, 1 cover", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Representation and Reasoning and Machine Learning are two important\nfields in AI. Nonmonotonic logic programming (NMLP) and Answer Set Programming\n(ASP) provide formal languages for representing and reasoning with commonsense\nknowledge and realize declarative problem solving in AI. On the other side,\nInductive Logic Programming (ILP) realizes Machine Learning in logic\nprogramming, which provides a formal background to inductive learning and the\ntechniques have been applied to the fields of relational learning and data\nmining. Generally speaking, NMLP and ASP realize nonmonotonic reasoning while\nlack the ability of learning. By contrast, ILP realizes inductive learning\nwhile most techniques have been developed under the classical monotonic logic.\nWith this background, some researchers attempt to combine techniques in the\ncontext of nonmonotonic ILP. Such combination will introduce a learning\nmechanism to programs and would exploit new applications on the NMLP side,\nwhile on the ILP side it will extend the representation language and enable us\nto use existing solvers. Cross-fertilization between learning and nonmonotonic\nreasoning can also occur in such as the use of answer set solvers for ILP,\nspeed-up learning while running answer set solvers, learning action theories,\nlearning transition rules in dynamical systems, abductive learning, learning\nbiological networks with inhibition, and applications involving default and\nnegation. This workshop is the first attempt to provide an open forum for the\nidentification of problems and discussion of possible collaborations among\nresearchers with complementary expertise. The workshop was held on September\n15th of 2013 in Corunna, Spain. This post-proceedings contains five technical\npapers (out of six accepted papers) and the abstract of the invited talk by Luc\nDe Raedt.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 07:39:58 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Inoue", "Katsumi", "", "Editors"], ["Sakama", "Chiaki", "", "Editors"]]}, {"id": "1311.4987", "submitter": "Yang Yu", "authors": "Chao Qian and Yang Yu and Zhi-Hua Zhou", "title": "Analyzing Evolutionary Optimization in Noisy Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many optimization tasks have to be handled in noisy environments, where we\ncannot obtain the exact evaluation of a solution but only a noisy one. For\nnoisy optimization tasks, evolutionary algorithms (EAs), a kind of stochastic\nmetaheuristic search algorithm, have been widely and successfully applied.\nPrevious work mainly focuses on empirical studying and designing EAs for noisy\noptimization, while, the theoretical counterpart has been little investigated.\nIn this paper, we investigate a largely ignored question, i.e., whether an\noptimization problem will always become harder for EAs in a noisy environment.\nWe prove that the answer is negative, with respect to the measurement of the\nexpected running time. The result implies that, for optimization tasks that\nhave already been quite hard to solve, the noise may not have a negative\neffect, and the easier a task the more negatively affected by the noise. On a\nrepresentative problem where the noise has a strong negative effect, we examine\ntwo commonly employed mechanisms in EAs dealing with noise, the re-evaluation\nand the threshold selection strategies. The analysis discloses that the two\nstrategies, however, both are not effective, i.e., they do not make the EA more\nnoise tolerant. We then find that a small modification of the threshold\nselection allows it to be proven as an effective strategy for dealing with the\nnoise in the problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 09:28:52 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Qian", "Chao", ""], ["Yu", "Yang", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1311.5355", "submitter": "Michael  Gr. Voskoglou Prof. Dr.", "authors": "Michael Gr. Voskoglou, Igor Ya. Subbotin", "title": "Dealing with the Fuzziness of Human Reasoning", "comments": "16 pages, 3 figures, 1 table. arXiv admin note: substantial text\n  overlap with arXiv:1212.2614", "journal-ref": "International Journal of Applications of Fuzzy Sets and Artifcial\n  Intelligence (ISSN 2241-1240), Vol.3, 91-106, 2013", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning, the most important human brain operation, is charactrized by a\ndegree fuzziness. In the present paper we construct a fuzzy model for the\nreasoning process giving through the calculation of the possibilities of all\npossible individuals' profiles a quantitative/qualitative view of their\nbehaviour during the above process and we use the centroid defuzzification\ntechnique for measuring the reasoning skills. We also present a number of\nclassroom experiments illustrating our results in practice.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 10:35:57 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Voskoglou", "Michael Gr.", ""], ["Subbotin", "Igor Ya.", ""]]}, {"id": "1311.5998", "submitter": "Xinyang Deng", "authors": "Yunpeng Li, Jie Liu, Yong Deng", "title": "A brief network analysis of Artificial Intelligence publication", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an illustration to the history of Artificial\nIntelligence(AI) with a statistical analysis of publish since 1940. We\ncollected and mined through the IEEE publish data base to analysis the\ngeological and chronological variance of the activeness of research in AI. The\nconnections between different institutes are showed. The result shows that the\nleading community of AI research are mainly in the USA, China, the Europe and\nJapan. The key institutes, authors and the research hotspots are revealed. It\nis found that the research institutes in the fields like Data Mining, Computer\nVision, Pattern Recognition and some other fields of Machine Learning are quite\nconsistent, implying a strong interaction between the community of each field.\nIt is also showed that the research of Electronic Engineering and Industrial or\nCommercial applications are very active in California. Japan is also publishing\na lot of papers in robotics. Due to the limitation of data source, the result\nmight be overly influenced by the number of published articles, which is to our\nbest improved by applying network keynode analysis on the research community\ninstead of merely count the number of publish.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 13:54:36 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Li", "Yunpeng", ""], ["Liu", "Jie", ""], ["Deng", "Yong", ""]]}, {"id": "1311.6054", "submitter": "Issam Qaffou", "authors": "Issam Qaffou, Mohamed Sadgal, Abdelaziz Elfazziki", "title": "Q-learning optimization in a multi-agents system for image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To know which operators to apply and in which order, as well as attributing\ngood values to their parameters is a challenge for users of computer vision.\nThis paper proposes a solution to this problem as a multi-agent system modeled\naccording to the Vowel approach and using the Q-learning algorithm to optimize\nits choice. An implementation is given to test and validate this method.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 21:25:13 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Qaffou", "Issam", ""], ["Sadgal", "Mohamed", ""], ["Elfazziki", "Abdelaziz", ""]]}, {"id": "1311.6079", "submitter": "Amirreza Shaban", "authors": "Amirreza Shaban, Hamid R. Rabiee and Mahyar Najibi", "title": "Local Similarities, Global Coding: An Algorithm for Feature Coding and\n  its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data coding as a building block of several image processing algorithms has\nbeen received great attention recently. Indeed, the importance of the locality\nassumption in coding approaches is studied in numerous works and several\nmethods are proposed based on this concept. We probe this assumption and claim\nthat taking the similarity between a data point and a more global set of anchor\npoints does not necessarily weaken the coding method as long as the underlying\nstructure of the anchor points are taken into account. Based on this fact, we\npropose to capture this underlying structure by assuming a random walker over\nthe anchor points. We show that our method is a fast approximate learning\nalgorithm based on the diffusion map kernel. The experiments on various\ndatasets show that making different state-of-the-art coding algorithms aware of\nthis structure boosts them in different learning tasks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 04:39:28 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 20:30:13 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Shaban", "Amirreza", ""], ["Rabiee", "Hamid R.", ""], ["Najibi", "Mahyar", ""]]}, {"id": "1311.6591", "submitter": "Guy Van den Broeck", "authors": "Guy Van den Broeck, Adnan Darwiche", "title": "On the Complexity and Approximation of Binary Evidence in Lifted\n  Inference", "comments": "To appear in Advances in Neural Information Processing Systems 26\n  (NIPS), Lake Tahoe, USA, December 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifted inference algorithms exploit symmetries in probabilistic models to\nspeed up inference. They show impressive performance when calculating\nunconditional probabilities in relational models, but often resort to\nnon-lifted inference when computing conditional probabilities. The reason is\nthat conditioning on evidence breaks many of the model's symmetries, which can\npreempt standard lifting techniques. Recent theoretical results show, for\nexample, that conditioning on evidence which corresponds to binary relations is\n#P-hard, suggesting that no lifting is to be expected in the worst case. In\nthis paper, we balance this negative result by identifying the Boolean rank of\nthe evidence as a key parameter for characterizing the complexity of\nconditioning in lifted inference. In particular, we show that conditioning on\nbinary evidence with bounded Boolean rank is efficient. This opens up the\npossibility of approximating evidence by a low-rank Boolean matrix\nfactorization, which we investigate both theoretically and empirically.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 08:39:49 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Broeck", "Guy Van den", ""], ["Darwiche", "Adnan", ""]]}, {"id": "1311.6594", "submitter": "\\'Angela Fern\\'andez Pascual", "authors": "\\'Angela Fern\\'andez, Neta Rabin, Dalia Fishelov, Jos\\'e R. Dorronsoro", "title": "Auto-adaptative Laplacian Pyramids for High-dimensional Data Analysis", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear dimensionality reduction techniques such as manifold learning\nalgorithms have become a common way for processing and analyzing\nhigh-dimensional patterns that often have attached a target that corresponds to\nthe value of an unknown function. Their application to new points consists in\ntwo steps: first, embedding the new data point into the low dimensional space\nand then, estimating the function value on the test point from its neighbors in\nthe embedded space.\n  However, finding the low dimension representation of a test point, while easy\nfor simple but often not powerful enough procedures such as PCA, can be much\nmore complicated for methods that rely on some kind of eigenanalysis, such as\nSpectral Clustering (SC) or Diffusion Maps (DM). Similarly, when a target\nfunction is to be evaluated, averaging methods like nearest neighbors may give\nunstable results if the function is noisy. Thus, the smoothing of the target\nfunction with respect to the intrinsic, low-dimensional representation that\ndescribes the geometric structure of the examined data is a challenging task.\n  In this paper we propose Auto-adaptive Laplacian Pyramids (ALP), an extension\nof the standard Laplacian Pyramids model that incorporates a modified LOOCV\nprocedure that avoids the large cost of the standard one and offers the\nfollowing advantages: (i) it selects automatically the optimal function\nresolution (stopping time) adapted to the data and its noise, (ii) it is easy\nto apply as it does not require parameterization, (iii) it does not overfit the\ntraining set and (iv) it adds no extra cost compared to other classical\ninterpolation methods. We illustrate numerically ALP's behavior on a synthetic\nproblem and apply it to the computation of the DM projection of new patterns\nand to the extension to them of target function values on a radiation\nforecasting problem over very high dimensional patterns.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 09:03:10 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 10:17:31 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Fern\u00e1ndez", "\u00c1ngela", ""], ["Rabin", "Neta", ""], ["Fishelov", "Dalia", ""], ["Dorronsoro", "Jos\u00e9 R.", ""]]}, {"id": "1311.6709", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Debajyoti Mukhopadhyay, Archana Chougule", "title": "A Framework for Semi-automated Web Service Composition in Semantic Web", "comments": "6 pages, 9 figures; CUBE 2013 International Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Number of web services available on Internet and its usage are increasing\nvery fast. In many cases, one service is not enough to complete the business\nrequirement; composition of web services is carried out. Autonomous composition\nof web services to achieve new functionality is generating considerable\nattention in semantic web domain. Development time and effort for new\napplications can be reduced with service composition. Various approaches to\ncarry out automated composition of web services are discussed in literature.\nWeb service composition using ontologies is one of the effective approaches. In\nthis paper we demonstrate how the ontology based composition can be made faster\nfor each customer. We propose a framework to provide precomposed web services\nto fulfil user requirements. We detail how ontology merging can be used for\ncomposition which expedites the whole process. We discuss how framework\nprovides customer specific ontology merging and repository. We also elaborate\non how merging of ontologies is carried out.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 15:41:19 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Mukhopadhyay", "Debajyoti", ""], ["Chougule", "Archana", ""]]}, {"id": "1311.6876", "submitter": "Yuan Yao", "authors": "Yuan Yao, Hanghang Tong, Tao Xie, Leman Akoglu, Feng Xu, Jian Lu", "title": "Want a Good Answer? Ask a Good Question First!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community Question Answering (CQA) websites have become valuable repositories\nwhich host a massive volume of human knowledge. To maximize the utility of such\nknowledge, it is essential to evaluate the quality of an existing question or\nanswer, especially soon after it is posted on the CQA website.\n  In this paper, we study the problem of inferring the quality of questions and\nanswers through a case study of a software CQA (Stack Overflow). Our key\nfinding is that the quality of an answer is strongly positively correlated with\nthat of its question. Armed with this observation, we propose a family of\nalgorithms to jointly predict the quality of questions and answers, for both\nquantifying numerical quality scores and differentiating the high-quality\nquestions/answers from those of low quality. We conduct extensive experimental\nevaluations to demonstrate the effectiveness and efficiency of our methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 06:39:25 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Yao", "Yuan", ""], ["Tong", "Hanghang", ""], ["Xie", "Tao", ""], ["Akoglu", "Leman", ""], ["Xu", "Feng", ""], ["Lu", "Jian", ""]]}, {"id": "1311.6907", "submitter": "Samir Loudni", "authors": "Jean-Philippe M\\'etivier and Samir Loudni and Thierry Charnois", "title": "A Constraint Programming Approach for Mining Sequential Patterns in a\n  Sequence Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint-based pattern discovery is at the core of numerous data mining\ntasks. Patterns are extracted with respect to a given set of constraints\n(frequency, closedness, size, etc). In the context of sequential pattern\nmining, a large number of devoted techniques have been developed for solving\nparticular classes of constraints. The aim of this paper is to investigate the\nuse of Constraint Programming (CP) to model and mine sequential patterns in a\nsequence database. Our CP approach offers a natural way to simultaneously\ncombine in a same framework a large set of constraints coming from various\norigins. Experiments show the feasibility and the interest of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 09:44:43 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["M\u00e9tivier", "Jean-Philippe", ""], ["Loudni", "Samir", ""], ["Charnois", "Thierry", ""]]}, {"id": "1311.7071", "submitter": "Zitao Liu", "authors": "Zitao Liu and Milos Hauskrecht", "title": "Sparse Linear Dynamical System with Its Application in Multivariate\n  Clinical Time Series", "comments": "Appear in Neural Information Processing Systems(NIPS) Workshop on\n  Machine Learning for Clinical Data Analysis and Healthcare 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Linear Dynamical System (LDS) is an elegant mathematical framework for\nmodeling and learning multivariate time series. However, in general, it is\ndifficult to set the dimension of its hidden state space. A small number of\nhidden states may not be able to model the complexities of a time series, while\na large number of hidden states can lead to overfitting. In this paper, we\nstudy methods that impose an $\\ell_1$ regularization on the transition matrix\nof an LDS model to alleviate the problem of choosing the optimal number of\nhidden states. We incorporate a generalized gradient descent method into the\nMaximum a Posteriori (MAP) framework and use Expectation Maximization (EM) to\niteratively achieve sparsity on the transition matrix of an LDS model. We show\nthat our Sparse Linear Dynamical System (SLDS) improves the predictive\nperformance when compared to ordinary LDS on a multivariate clinical time\nseries dataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 18:58:07 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 20:08:28 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Liu", "Zitao", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1311.7139", "submitter": "Florentin Smarandache", "authors": "Florentin Smarandache", "title": "Introduction to Neutrosophic Measure, Neutrosophic Integral, and\n  Neutrosophic Probability", "comments": "140 pages. 10 figures", "journal-ref": "Published as a book by Sitech in 2013", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce for the first time the notions of neutrosophic\nmeasure and neutrosophic integral, and we develop the 1995 notion of\nneutrosophic probability. We present many practical examples. It is possible to\ndefine the neutrosophic measure and consequently the neutrosophic integral and\nneutrosophic probability in many ways, because there are various types of\nindeterminacies, depending on the problem we need to solve. Neutrosophics study\nthe indeterminacy. Indeterminacy is different from randomness. It can be caused\nby physical space materials and type of construction, by items involved in the\nspace, etc.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 18:56:03 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Smarandache", "Florentin", ""]]}, {"id": "1311.7215", "submitter": "Alireza Rezvanian", "authors": "Aylin Mousavian, Alireza Rezvanian, Mohammad Reza Meybodi", "title": "Solving Minimum Vertex Cover Problem Using Learning Automata", "comments": "5 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum vertex cover problem is an NP-Hard problem with the aim of finding\nminimum number of vertices to cover graph. In this paper, a learning automaton\nbased algorithm is proposed to find minimum vertex cover in graph. In the\nproposed algorithm, each vertex of graph is equipped with a learning automaton\nthat has two actions in the candidate or non-candidate of the corresponding\nvertex cover set. Due to characteristics of learning automata, this algorithm\nsignificantly reduces the number of covering vertices of graph. The proposed\nalgorithm based on learning automata iteratively minimize the candidate vertex\ncover through the update its action probability. As the proposed algorithm\nproceeds, a candidate solution nears to optimal solution of the minimum vertex\ncover problem. In order to evaluate the proposed algorithm, several experiments\nconducted on DIMACS dataset which compared to conventional methods.\nExperimental results show the major superiority of the proposed algorithm over\nthe other methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 05:49:34 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Mousavian", "Aylin", ""], ["Rezvanian", "Alireza", ""], ["Meybodi", "Mohammad Reza", ""]]}]