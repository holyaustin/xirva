[{"id": "1710.00262", "submitter": "Tuan Do", "authors": "Tuan Do and James Pustejovsky", "title": "Fine-grained Event Learning of Human-Object Interaction with LSTM-CRF", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event learning is one of the most important problems in AI. However,\nnotwithstanding significant research efforts, it is still a very complex task,\nespecially when the events involve the interaction of humans or agents with\nother objects, as it requires modeling human kinematics and object movements.\nThis study proposes a methodology for learning complex human-object interaction\n(HOI) events, involving the recording, annotation and classification of event\ninteractions. For annotation, we allow multiple interpretations of a motion\ncapture by slicing over its temporal span, for classification, we use\nLong-Short Term Memory (LSTM) sequential models with Conditional Randon Field\n(CRF) for constraints of outputs. Using a setup involving captures of\nhuman-object interaction as three dimensional inputs, we argue that this\napproach could be used for event types involving complex spatio-temporal\ndynamics.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 21:04:25 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Do", "Tuan", ""], ["Pustejovsky", "James", ""]]}, {"id": "1710.00310", "submitter": "Chengwei Huang", "authors": "Yun Liu, Tianmeng Gao, Baolin Song, Chengwei Huang", "title": "Personalized Fuzzy Text Search Using Interest Prediction and Word\n  Vectorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the personalized text search problem. The keyword\nbased search method in conventional algorithms has a low efficiency in\nunderstanding users' intention since the semantic meaning, user profile, user\ninterests are not always considered. Firstly, we propose a novel text search\nalgorithm using a inverse filtering mechanism that is very efficient for label\nbased item search. Secondly, we adopt the Bayesian network to implement the\nuser interest prediction for an improved personalized search. According to user\ninput, it searches the related items using keyword information, predicted user\ninterest. Thirdly, the word vectorization is used to discover potential targets\naccording to the semantic meaning. Experimental results show that the proposed\nsearch engine has an improved efficiency and accuracy and it can operate on\nembedded devices with very limited computational resources.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 08:22:24 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Liu", "Yun", ""], ["Gao", "Tianmeng", ""], ["Song", "Baolin", ""], ["Huang", "Chengwei", ""]]}, {"id": "1710.00336", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu, Hangjun Ye", "title": "Parameter Sharing Deep Deterministic Policy Gradient for Cooperative\n  Multi-agent Reinforcement Learning", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning for multi-agent cooperation and competition has\nbeen a hot topic recently. This paper focuses on cooperative multi-agent\nproblem based on actor-critic methods under local observations settings. Multi\nagent deep deterministic policy gradient obtained state of art results for some\nmulti-agent games, whereas, it cannot scale well with growing amount of agents.\nIn order to boost scalability, we propose a parameter sharing deterministic\npolicy gradient method with three variants based on neural networks, including\nactor-critic sharing, actor sharing and actor sharing with partially shared\ncritic. Benchmarks from rllab show that the proposed method has advantages in\nlearning speed and memory efficiency, well scales with growing amount of\nagents, and moreover, it can make full use of reward sharing and\nexchangeability if possible.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 11:43:10 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 00:47:58 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Ye", "Hangjun", ""]]}, {"id": "1710.00448", "submitter": "Tuan Do", "authors": "Tuan Do and James Pustejovsky", "title": "Learning event representation: As sparse as possible, but not sparser", "comments": "Qualitative reasoning Workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting an optimal event representation is essential for event\nclassification in real world contexts. In this paper, we investigate the\napplication of qualitative spatial reasoning (QSR) frameworks for\nclassification of human-object interaction in three dimensional space, in\ncomparison with the use of quantitative feature extraction approaches for the\nsame purpose. In particular, we modify QSRLib, a library that allows\ncomputation of Qualitative Spatial Relations and Calculi, and employ it for\nfeature extraction, before inputting features into our neural network models.\nUsing an experimental setup involving motion captures of human-object\ninteraction as three dimensional inputs, we observe that the use of qualitative\nspatial features significantly improves the performance of our machine learning\nalgorithm against our baseline, while quantitative features of similar kinds\nfail to deliver similar improvement. We also observe that sequential\nrepresentations of QSR features yield the best classification performance. A\nresult of our learning method is a simple approach to the qualitative\nrepresentation of 3D activities as compositions of 2D actions that can be\nvisualized and learned using 2-dimensional QSR.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 01:18:16 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Do", "Tuan", ""], ["Pustejovsky", "James", ""]]}, {"id": "1710.00459", "submitter": "Melrose Roderick", "authors": "Melrose Roderick, Christopher Grimm, Stefanie Tellex", "title": "Deep Abstract Q-Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the problem of learning and planning on high-dimensional domains\nwith long horizons and sparse rewards. Recent approaches have shown great\nsuccesses in many Atari 2600 domains. However, domains with long horizons and\nsparse rewards, such as Montezuma's Revenge and Venture, remain challenging for\nexisting methods. Methods using abstraction (Dietterich 2000; Sutton, Precup,\nand Singh 1999) have shown to be useful in tackling long-horizon problems. We\ncombine recent techniques of deep reinforcement learning with existing\nmodel-based approaches using an expert-provided state abstraction. We construct\ntoy domains that elucidate the problem of long horizons, sparse rewards and\nhigh-dimensional inputs, and show that our algorithm significantly outperforms\nprevious methods on these domains. Our abstraction-based approach outperforms\nDeep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and\nexhibits backtracking behavior that is absent from previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 02:17:09 GMT"}, {"version": "v2", "created": "Sat, 25 Aug 2018 18:29:32 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Roderick", "Melrose", ""], ["Grimm", "Christopher", ""], ["Tellex", "Stefanie", ""]]}, {"id": "1710.00461", "submitter": "Seng Loke", "authors": "Seng W. Loke", "title": "Cooperative Automated Vehicles: a Review of Opportunities and Challenges\n  in Socially Intelligent Vehicles Beyond Networking", "comments": "10 pages, Accepted for the IEEE Trans. on Intelligent Vehicles (to\n  appear)", "journal-ref": null, "doi": "10.1109/TIV.2019.2938107", "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connected automated vehicle has been often touted as a technology that\nwill become pervasive in society in the near future. One can view an automated\nvehicle as having Artificial Intelligence (AI) capabilities, being able to\nself-drive, sense its surroundings, recognise objects in its vicinity, and\nperform reasoning and decision-making.\n  Rather than being stand alone, we examine the need for automated vehicles to\ncooperate and interact within their socio-cyber-physical environments,\nincluding the problems cooperation will solve, but also the issues and\nchallenges. We review current work in cooperation for automated vehicles, based\non selected examples from the literature. We conclude noting the need for the\nability to behave cooperatively as a form of social-AI capability for automated\nvehicles, beyond sensing the immediate environment and beyond the underlying\nnetworking technology.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 02:22:58 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 09:19:24 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Loke", "Seng W.", ""]]}, {"id": "1710.00489", "submitter": "Arunkumar Byravan", "authors": "Arunkumar Byravan, Felix Leeb, Franziska Meier and Dieter Fox", "title": "SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning\n  and Control", "comments": "8 pages, Initial submission to IEEE International Conference on\n  Robotics and Automation (ICRA) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an approach to deep visuomotor control using\nstructured deep dynamics models. Our deep dynamics model, a variant of\nSE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an\nencoder-decoder structure. Unlike prior work, our dynamics model is structured:\ngiven an input scene, our network explicitly learns to segment salient parts\nand predict their pose-embedding along with their motion modeled as a change in\nthe pose space due to the applied actions. We train our model using a pair of\npoint clouds separated by an action and show that given supervision only in the\nform of point-wise data associations between the frames our network is able to\nlearn a meaningful segmentation of the scene along with consistent poses. We\nfurther show that our model can be used for closed-loop control directly in the\nlearned low-dimensional pose space, where the actions are computed by\nminimizing error in the pose space using gradient-based methods, similar to\ntraditional model-based control. We present results on controlling a Baxter\nrobot from raw depth data in simulation and in the real world and compare\nagainst two baseline deep networks. Our method runs in real-time, achieves good\nprediction of scene dynamics and outperforms the baseline methods on multiple\ncontrol runs. Video results can be found at:\nhttps://rse-lab.cs.washington.edu/se3-structured-deep-ctrl/\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 05:18:12 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Byravan", "Arunkumar", ""], ["Leeb", "Felix", ""], ["Meier", "Franziska", ""], ["Fox", "Dieter", ""]]}, {"id": "1710.00490", "submitter": "Catarina Moreira", "authors": "Catarina Moreira and Emmanuel Haven and Sandro Sozzo and Andreas\n  Wichert", "title": "The Dutch's Real World Financial Institute: Introducing Quantum-Like\n  Bayesian Networks as an Alternative Model to deal with Uncertainty", "comments": "15 images, 33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyse and model a real life financial loan application\nbelonging to a sample bank in the Netherlands. The log is robust in terms of\ndata, containing a total of 262 200 event logs, belonging to 13 087 different\ncredit applications. The dataset is heterogeneous and consists of a mixture of\ncomputer generated automatic processes and manual human tasks. The goal is to\nwork out a decision model, which represents the underlying tasks that make up\nthe loan application service, and to assess potential areas of improvement of\nthe institution's internal processes. To this end we study the impact of\nincomplete event logs for the extraction and analysis of business processes. It\nis quite common that event logs are incomplete with several amounts of missing\ninformation (for instance, workers forget to register their tasks). Absence of\ndata is translated into a drastic decrease of precision and compromises the\ndecision models, leading to biased and unrepresentative results. We investigate\nhow classical probabilistic models are affected by incomplete event logs and we\nexplore quantum-like probabilistic inferences as an alternative mathematical\nmodel to classical probability. This work represents a first step towards\nsystematic investigation of the impact of quantum interference in a real life\nlarge scale decision scenario. The results obtained in this study indicate\nthat, under high levels of uncertainty, the quantum-like models generate\nquantum interference terms, which allow an additional non-linear\nparameterisation of the data. Experimental results attest the efficiency of the\nquantum-like Bayesian networks, since the application of interference terms is\nable to reduce the error percentage of inferences performed over quantum-like\nmodels when compared to inferences produced by classical models.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 05:28:03 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Moreira", "Catarina", ""], ["Haven", "Emmanuel", ""], ["Sozzo", "Sandro", ""], ["Wichert", "Andreas", ""]]}, {"id": "1710.00641", "submitter": "Mirco Ravanelli", "authors": "Mirco Ravanelli, Philemon Brakel, Maurizio Omologo, Yoshua Bengio", "title": "Improving speech recognition by revising gated recurrent units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech recognition is largely taking advantage of deep learning, showing that\nsubstantial benefits can be obtained by modern Recurrent Neural Networks\n(RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which\ntypically reach state-of-the-art performance in many tasks thanks to their\nability to learn long-term dependencies and robustness to vanishing gradients.\nNevertheless, LSTMs have a rather complex design with three multiplicative\ngates, that might impair their efficient implementation. An attempt to simplify\nLSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just\ntwo multiplicative gates.\n  This paper builds on these efforts by further revising GRUs and proposing a\nsimplified architecture potentially more suitable for speech recognition. The\ncontribution of this work is two-fold. First, we suggest to remove the reset\ngate in the GRU design, resulting in a more efficient single-gate architecture.\nSecond, we propose to replace tanh with ReLU activations in the state update\nequations. Results show that, in our implementation, the revised architecture\nreduces the per-epoch training time with more than 30% and consistently\nimproves recognition performance across different tasks, input features, and\nnoisy conditions when compared to a standard GRU.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 12:40:50 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Ravanelli", "Mirco", ""], ["Brakel", "Philemon", ""], ["Omologo", "Maurizio", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1710.00675", "submitter": "Martin Chmel\\'ik", "authors": "Krishnendu Chatterjee, Martin Chmelik, Ufuk Topcu", "title": "Sensor Synthesis for POMDPs with Reachability Objectives", "comments": "arXiv admin note: text overlap with arXiv:1511.08456", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially observable Markov decision processes (POMDPs) are widely used in\nprobabilistic planning problems in which an agent interacts with an environment\nusing noisy and imprecise sensors. We study a setting in which the sensors are\nonly partially defined and the goal is to synthesize \"weakest\" additional\nsensors, such that in the resulting POMDP, there is a small-memory policy for\nthe agent that almost-surely (with probability~1) satisfies a reachability\nobjective. We show that the problem is NP-complete, and present a symbolic\nalgorithm by encoding the problem into SAT instances. We illustrate trade-offs\nbetween the amount of memory of the policy and the number of additional sensors\non a simple example. We have implemented our approach and consider three\nclassical POMDP examples from the literature, and show that in all the examples\nthe number of sensors can be significantly decreased (as compared to the\nexisting solutions in the literature) without increasing the complexity of the\npolicies.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 08:27:24 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Chmelik", "Martin", ""], ["Topcu", "Ufuk", ""]]}, {"id": "1710.00794", "submitter": "Derek Doran", "authors": "Derek Doran, Sarah Schulz, Tarek R. Besold", "title": "What Does Explainable AI Really Mean? A New Conceptualization of\n  Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize three notions of explainable AI that cut across research\nfields: opaque systems that offer no insight into its algo- rithmic mechanisms;\ninterpretable systems where users can mathemat- ically analyze its algorithmic\nmechanisms; and comprehensible systems that emit symbols enabling user-driven\nexplanations of how a conclusion is reached. The paper is motivated by a corpus\nanalysis of NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing differences\nin how work on explainable AI is positioned in various fields. We close by\nintroducing a fourth notion: truly explainable systems, where automated\nreasoning is central to output crafted explanations without requiring human\npost processing as final step of the generative process.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 17:09:38 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Doran", "Derek", ""], ["Schulz", "Sarah", ""], ["Besold", "Tarek R.", ""]]}, {"id": "1710.00892", "submitter": "Joseph Geumlek", "authors": "Joseph Geumlek, Shuang Song, Kamalika Chaudhuri", "title": "R\\'enyi Differential Privacy Mechanisms for Posterior Sampling", "comments": "to be published in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a recently proposed privacy definition of R\\'enyi Differential Privacy\n(RDP), we re-examine the inherent privacy of releasing a single sample from a\nposterior distribution. We exploit the impact of the prior distribution in\nmitigating the influence of individual data points. In particular, we focus on\nsampling from an exponential family and specific generalized linear models,\nsuch as logistic regression. We propose novel RDP mechanisms as well as\noffering a new RDP analysis for an existing method in order to add value to the\nRDP framework. Each method is capable of achieving arbitrary RDP privacy\nguarantees, and we offer experimental results of their efficacy.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 20:15:43 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Geumlek", "Joseph", ""], ["Song", "Shuang", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1710.00978", "submitter": "Naimish Agarwal", "authors": "Naimish Agarwal, G.C. Nandi", "title": "Supervised Q-walk for Learning Vector Representation of Nodes in\n  Networks", "comments": "7 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic feature learning algorithms are at the forefront of modern day\nmachine learning research. We present a novel algorithm, supervised Q-walk,\nwhich applies Q-learning to generate random walks on graphs such that the walks\nprove to be useful for learning node features suitable for tackling with the\nnode classification problem. We present another novel algorithm, k-hops\nneighborhood based confidence values learner, which learns confidence values of\nlabels for unlabelled nodes in the network without first learning the node\nembedding. These confidence values aid in learning an apt reward function for\nQ-learning.\n  We demonstrate the efficacy of supervised Q-walk approach over existing\nstate-of-the-art random walk based node embedding learners in solving the\nsingle / multi-label multi-class node classification problem using several real\nworld datasets.\n  Summarising, our approach represents a novel state-of-the-art technique to\nlearn features, for nodes in networks, tailor-made for dealing with the node\nclassification problem.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 04:25:23 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Agarwal", "Naimish", ""], ["Nandi", "G. C.", ""]]}, {"id": "1710.01079", "submitter": "Andrew Anderson", "authors": "Andrew Anderson and David Gregg", "title": "Optimal DNN Primitive Selection with Partitioned Boolean Quadratic\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) require very large amounts of computation both\nfor training and for inference when deployed in the field. Many different\nalgorithms have been proposed to implement the most computationally expensive\nlayers of DNNs. Further, each of these algorithms has a large number of\nvariants, which offer different trade-offs of parallelism, data locality,\nmemory footprint, and execution time. In addition, specific algorithms operate\nmuch more efficiently on specialized data layouts and formats.\n  We state the problem of optimal primitive selection in the presence of data\nformat transformations, and show that it is NP-hard by demonstrating an\nembedding in the Partitioned Boolean Quadratic Assignment problem (PBQP).\n  We propose an analytic solution via a PBQP solver, and evaluate our approach\nexperimentally by optimizing several popular DNNs using a library of more than\n70 DNN primitives, on an embedded platform and a general purpose platform. We\nshow experimentally that significant gains are possible versus the state of the\nart vendor libraries by using a principled analytic solution to the problem of\nlayout selection in the presence of data format transformations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 11:25:24 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 10:56:03 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Anderson", "Andrew", ""], ["Gregg", "David", ""]]}, {"id": "1710.01275", "submitter": "Stefano Bromuri Dr", "authors": "Stefano Bromuri and Albert Brugues de la Torre and Fabien Duboisson\n  and Michael Schumacher", "title": "Indexing the Event Calculus with Kd-trees to Monitor Diabetes", "comments": "24 pages, preliminary results calculated on an implementation of\n  CECKD, precursor to Journal paper being submitted in 2017, with further\n  indexing and results possibilities, put here for reference and chronological\n  purposes to remember how the idea evolved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal Health Systems (PHS) are mobile solutions tailored to monitoring\npatients affected by chronic non communicable diseases. A patient affected by a\nchronic disease can generate large amounts of events. Type 1 Diabetic patients\ngenerate several glucose events per day, ranging from at least 6 events per day\n(under normal monitoring) to 288 per day when wearing a continuous glucose\nmonitor (CGM) that samples the blood every 5 minutes for several days. This is\na large number of events to monitor for medical doctors, in particular when\nconsidering that they may have to take decisions concerning adjusting the\ntreatment, which may impact the life of the patients for a long time. Given the\nneed to analyse such a large stream of data, doctors need a simple approach\ntowards physiological time series that allows them to promptly transfer their\nknowledge into queries to identify interesting patterns in the data. Achieving\nthis with current technology is not an easy task, as on one hand it cannot be\nexpected that medical doctors have the technical knowledge to query databases\nand on the other hand these time series include thousands of events, which\nrequires to re-think the way data is indexed. In order to tackle the knowledge\nrepresentation and efficiency problem, this contribution presents the kd-tree\ncached event calculus (\\ceckd) an event calculus extension for knowledge\nengineering of temporal rules capable to handle many thousands events produced\nby a diabetic patient. \\ceckd\\ is built as a support to a graphical interface\nto represent monitoring rules for diabetes type 1. In addition, the paper\nevaluates the \\ceckd\\ with respect to the cached event calculus (CEC) to show\nhow indexing events using kd-trees improves scalability with respect to the\ncurrent state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 17:01:54 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Bromuri", "Stefano", ""], ["de la Torre", "Albert Brugues", ""], ["Duboisson", "Fabien", ""], ["Schumacher", "Michael", ""]]}, {"id": "1710.01347", "submitter": "David Di Giorgio", "authors": "David Di Giorgio", "title": "Simple Cortex: A Model of Cells in the Sensory Nervous System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscience research has produced many theories and computational neural\nmodels of sensory nervous systems. Notwithstanding many different perspectives\ntowards developing intelligent machines, artificial intelligence has ultimately\nbeen influenced by neuroscience. Therefore, this paper provides an introduction\nto biologically inspired machine intelligence by exploring the basic principles\nof sensation and perception as well as the structure and behavior of biological\nsensory nervous systems like the neocortex. Concepts like spike timing,\nsynaptic plasticity, inhibition, neural structure, and neural behavior are\napplied to a new model, Simple Cortex (SC). A software implementation of SC has\nbeen built and demonstrates fast observation, learning, and prediction of\nspatio-temporal sensory-motor patterns and sequences. Finally, this paper\nsuggests future areas of improvement and growth for Simple Cortex and other\nrelated machine intelligence models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 18:51:19 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Di Giorgio", "David", ""]]}, {"id": "1710.01437", "submitter": "Elina Robeva Massachusetts Institute of Technology", "authors": "Elina Robeva and Anna Seigal", "title": "Duality of Graphical Models and Tensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI quant-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we show the duality between tensor networks and undirected\ngraphical models with discrete variables. We study tensor networks on\nhypergraphs, which we call tensor hypernetworks. We show that the tensor\nhypernetwork on a hypergraph exactly corresponds to the graphical model given\nby the dual hypergraph. We translate various notions under duality. For\nexample, marginalization in a graphical model is dual to contraction in the\ntensor network. Algorithms also translate under duality. We show that belief\npropagation corresponds to a known algorithm for tensor network contraction.\nThis article is a reminder that the research areas of graphical models and\ntensor networks can benefit from interaction.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 01:55:05 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Robeva", "Elina", ""], ["Seigal", "Anna", ""]]}, {"id": "1710.01447", "submitter": "Hang Ma", "authors": "Hang Ma, Jingxing Yang, Liron Cohen, T. K. Satish Kumar, Sven Koenig", "title": "Feasibility Study: Moving Non-Homogeneous Teams in Congested Video Game\n  Environments", "comments": "To appear in AIIDE 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent path finding (MAPF) is a well-studied problem in artificial\nintelligence, where one needs to find collision-free paths for agents with\ngiven start and goal locations. In video games, agents of different types often\nform teams. In this paper, we demonstrate the usefulness of MAPF algorithms\nfrom artificial intelligence for moving such non-homogeneous teams in congested\nvideo game environments.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 03:14:40 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Ma", "Hang", ""], ["Yang", "Jingxing", ""], ["Cohen", "Liron", ""], ["Kumar", "T. K. Satish", ""], ["Koenig", "Sven", ""]]}, {"id": "1710.01691", "submitter": "Kun Ho Kim", "authors": "Kun Ho Kim, Oisin Mac Aodha, Pietro Perona", "title": "Context Embedding Networks", "comments": "CVPR 2018 spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low dimensional embeddings that capture the main variations of interest in\ncollections of data are important for many applications. One way to construct\nthese embeddings is to acquire estimates of similarity from the crowd. However,\nsimilarity is a multi-dimensional concept that varies from individual to\nindividual. Existing models for learning embeddings from the crowd typically\nmake simplifying assumptions such as all individuals estimate similarity using\nthe same criteria, the list of criteria is known in advance, or that the crowd\nworkers are not influenced by the data that they see. To overcome these\nlimitations we introduce Context Embedding Networks (CENs). In addition to\nlearning interpretable embeddings from images, CENs also model worker biases\nfor different attributes along with the visual context i.e. the visual\nattributes highlighted by a set of images. Experiments on two noisy crowd\nannotated datasets show that modeling both worker bias and visual context\nresults in more interpretable embeddings compared to existing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 18:46:40 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 22:47:50 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 16:32:35 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Kim", "Kun Ho", ""], ["Mac Aodha", "Oisin", ""], ["Perona", "Pietro", ""]]}, {"id": "1710.01692", "submitter": "Dokhyam Hoshen", "authors": "Dokhyam Hoshen, Michael Werman", "title": "IQ of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IQ tests are an accepted method for assessing human intelligence. The tests\nconsist of several parts that must be solved under a time constraint. Of all\nthe tested abilities, pattern recognition has been found to have the highest\ncorrelation with general intelligence. This is primarily because pattern\nrecognition is the ability to find order in a noisy environment, a necessary\nskill for intelligent agents. In this paper, we propose a convolutional neural\nnetwork (CNN) model for solving geometric pattern recognition problems. The CNN\nreceives as input multiple ordered input images and outputs the next image\naccording to the pattern. Our CNN is able to solve problems involving rotation,\nreflection, color, size and shape patterns and score within the top 5% of human\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 11:48:58 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Hoshen", "Dokhyam", ""], ["Werman", "Michael", ""]]}, {"id": "1710.01727", "submitter": "Hamed Haddadi", "authors": "Seyed Ali Osia, Ali Shahin Shamsabadi, Ali Taheri, Kleomenis Katevas,\n  Hamid R. Rabiee, Nicholas D. Lane, Hamed Haddadi", "title": "Privacy-Preserving Deep Inference for Rich User Data on The Cloud", "comments": "arXiv admin note: substantial text overlap with arXiv:1703.02952", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are increasingly being used in a variety of machine\nlearning applications applied to rich user data on the cloud. However, this\napproach introduces a number of privacy and efficiency challenges, as the cloud\noperator can perform secondary inferences on the available data. Recently,\nadvances in edge processing have paved the way for more efficient, and private,\ndata processing at the source for simple tasks and lighter models, though they\nremain a challenge for larger, and more complicated models. In this paper, we\npresent a hybrid approach for breaking down large, complex deep models for\ncooperative, privacy-preserving analytics. We do this by breaking down the\npopular deep architectures and fine-tune them in a particular way. We then\nevaluate the privacy benefits of this approach based on the information exposed\nto the cloud service. We also asses the local inference cost of different\nlayers on a modern handset for mobile applications. Our evaluations show that\nby using certain kind of fine-tuning and embedding techniques and at a small\nprocessing costs, we can greatly reduce the level of information available to\nunintended tasks applied to the data feature on the cloud, and hence achieving\nthe desired tradeoff between privacy and performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 19:15:32 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 09:58:59 GMT"}, {"version": "v3", "created": "Wed, 11 Oct 2017 20:26:15 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Osia", "Seyed Ali", ""], ["Shamsabadi", "Ali Shahin", ""], ["Taheri", "Ali", ""], ["Katevas", "Kleomenis", ""], ["Rabiee", "Hamid R.", ""], ["Lane", "Nicholas D.", ""], ["Haddadi", "Hamed", ""]]}, {"id": "1710.01813", "submitter": "Danfei Xu", "authors": "Danfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei,\n  Silvio Savarese", "title": "Neural Task Programming: Learning to Generalize Across Hierarchical\n  Tasks", "comments": "ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel robot learning framework called Neural Task\nProgramming (NTP), which bridges the idea of few-shot learning from\ndemonstration and neural program induction. NTP takes as input a task\nspecification (e.g., video demonstration of a task) and recursively decomposes\nit into finer sub-task specifications. These specifications are fed to a\nhierarchical neural program, where bottom-level programs are callable\nsubroutines that interact with the environment. We validate our method in three\nrobot manipulation tasks. NTP achieves strong generalization across sequential\ntasks that exhibit hierarchal and compositional structures. The experimental\nresults show that NTP learns to generalize well to- wards unseen tasks with\nincreasing lengths, variable topologies, and changing objectives.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 21:31:49 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 22:04:25 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Xu", "Danfei", ""], ["Nair", "Suraj", ""], ["Zhu", "Yuke", ""], ["Gao", "Julian", ""], ["Garg", "Animesh", ""], ["Fei-Fei", "Li", ""], ["Savarese", "Silvio", ""]]}, {"id": "1710.01823", "submitter": "James O' Neill", "authors": "C\\'ecile Robin, James O'Neill, Paul Buitelaar", "title": "Automatic Taxonomy Generation - A Use-Case in the Legal Domain", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in the legal domain is the adaptation and representation of\nthe legal knowledge expressed through texts, in order for legal practitioners\nand researchers to access this information easier and faster to help with\ncompliance related issues. One way to approach this goal is in the form of a\ntaxonomy of legal concepts. While this task usually requires a manual\nconstruction of terms and their relations by domain experts, this paper\ndescribes a methodology to automatically generate a taxonomy of legal noun\nconcepts. We apply and compare two approaches on a corpus consisting of\nstatutory instruments for UK, Wales, Scotland and Northern Ireland laws.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 23:00:08 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Robin", "C\u00e9cile", ""], ["O'Neill", "James", ""], ["Buitelaar", "Paul", ""]]}, {"id": "1710.02035", "submitter": "Noman Islam Dr.", "authors": "Noman Islam, Zubair A. Shaikh, Aqeel-ur-Rehman, Muhammad Shahab\n  Siddiqui", "title": "HANDY: A Hybrid Association Rules Mining Approach for Network Layer\n  Discovery of Services for Mobile Ad hoc Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Ad hoc Network (MANET) is an infrastructure-less network formed\nbetween a set of mobile nodes. The discovery of services in MANET is a\nchallenging job due to the unique properties of network. In this paper, a novel\nservice discovery framework called Hybrid Association Rules Based Network Layer\nDiscovery of Services for Ad hoc Networks (HANDY) has been proposed. HANDY\nprovides three major research contributions. At first, it adopts a cross-layer\noptimized design for discovery of services that is based on simultaneous\ndiscovery of services and corresponding routes. Secondly, it provides a\nmulti-level ontology-based approach to describe the services. This resolves the\nissue of semantic interoperability among the service consumers in a scalable\nfashion. Finally, to further optimize the performance of the discovery process,\nHANDY recommends exploiting the inherent associations present among the\nservices. These associations are used in two ways. First, periodic service\nadvertisements are performed based on these associations. In addition, when a\nresponse of a service discovery request is generated, correlated services are\nalso attached with the response. The proposed service discovery scheme has been\nimplemented in JIST/SWANS simulator. The results demonstrate that the proposed\nmodifications give rise to improvement in hit ratio of the service consumers\nand latency of discovery process.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 14:49:11 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Islam", "Noman", ""], ["Shaikh", "Zubair A.", ""], ["Aqeel-ur-Rehman", "", ""], ["Siddiqui", "Muhammad Shahab", ""]]}, {"id": "1710.02103", "submitter": "Yu Zhang", "authors": "Yu Zhang, Srikanta Tirthapura, Graham Cormode", "title": "Learning Graphical Models from a Distributed Stream", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A current challenge for data management systems is to support the\nconstruction and maintenance of machine learning models over data that is\nlarge, multi-dimensional, and evolving. While systems that could support these\ntasks are emerging, the need to scale to distributed, streaming data requires\nnew models and algorithms. In this setting, as well as computational\nscalability and model accuracy, we also need to minimize the amount of\ncommunication between distributed processors, which is the chief component of\nlatency. We study Bayesian networks, the workhorse of graphical models, and\npresent a communication-efficient method for continuously learning and\nmaintaining a Bayesian network model over data that is arriving as a\ndistributed stream partitioned across multiple processors. We show a strategy\nfor maintaining model parameters that leads to an exponential reduction in\ncommunication when compared with baseline approaches to maintain the exact MLE\n(maximum likelihood estimation). Meanwhile, our strategy provides similar\nprediction errors for the target distribution and for classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 16:30:33 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Zhang", "Yu", ""], ["Tirthapura", "Srikanta", ""], ["Cormode", "Graham", ""]]}, {"id": "1710.02210", "submitter": "Suraj Narayanan Sasikumar", "authors": "Suraj Narayanan Sasikumar", "title": "Exploration in Feature Space for Reinforcement Learning", "comments": "Masters thesis. Australian National University, May 2017. 65 pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infamous exploration-exploitation dilemma is one of the oldest and most\nimportant problems in reinforcement learning (RL). Deliberate and effective\nexploration is necessary for RL agents to succeed in most environments.\nHowever, until very recently even very sophisticated RL algorithms employed\nsimple, undirected exploration strategies in large-scale RL tasks.\n  We introduce a new optimistic count-based exploration algorithm for RL that\nis feasible in high-dimensional MDPs. The success of RL algorithms in these\ndomains depends crucially on generalization from limited training experience.\nFunction approximation techniques enable RL agents to generalize in order to\nestimate the value of unvisited states, but at present few methods have\nachieved generalization about the agent's uncertainty regarding unvisited\nstates. We present a new method for computing a generalized state visit-count,\nwhich allows the agent to estimate the uncertainty associated with any state.\n  In contrast to existing exploration techniques, our\n$\\phi$-$\\textit{pseudocount}$ achieves generalization by exploiting the feature\nrepresentation of the state space that is used for value function\napproximation. States that have less frequently observed features are deemed\nmore uncertain. The resulting $\\phi$-$\\textit{Exploration-Bonus}$ algorithm\nrewards the agent for exploring in feature space rather than in the original\nstate space. This method is simpler and less computationally expensive than\nsome previous proposals, and achieves near state-of-the-art results on\nhigh-dimensional RL benchmarks. In particular, we report world-class results on\nseveral notoriously difficult Atari 2600 video games, including Montezuma's\nRevenge.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 20:46:47 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Sasikumar", "Suraj Narayanan", ""]]}, {"id": "1710.02221", "submitter": "Ondrej Kuzelka", "authors": "Gustav Sourek, Martin Svatos, Filip Zelezny, Steven Schockaert, Ondrej\n  Kuzelka", "title": "Stacked Structure Learning for Lifted Relational Neural Networks", "comments": "Presented at ILP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifted Relational Neural Networks (LRNNs) describe relational domains using\nweighted first-order rules which act as templates for constructing feed-forward\nneural networks. While previous work has shown that using LRNNs can lead to\nstate-of-the-art results in various ILP tasks, these results depended on\nhand-crafted rules. In this paper, we extend the framework of LRNNs with\nstructure learning, thus enabling a fully automated learning process. Similarly\nto many ILP methods, our structure learning algorithm proceeds in an iterative\nfashion by top-down searching through the hypothesis space of all possible Horn\nclauses, considering the predicates that occur in the training examples as well\nas invented soft concepts entailed by the best weighted rules found so far. In\nthe experiments, we demonstrate the ability to automatically induce useful\nhierarchical soft concepts leading to deep LRNNs with a competitive predictive\npower.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 21:15:45 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Sourek", "Gustav", ""], ["Svatos", "Martin", ""], ["Zelezny", "Filip", ""], ["Schockaert", "Steven", ""], ["Kuzelka", "Ondrej", ""]]}, {"id": "1710.02224", "submitter": "Shiyu Chang", "authors": "Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan,\n  Xiaodong Cui, Michael Witbrock, Mark Hasegawa-Johnson, Thomas S. Huang", "title": "Dilated Recurrent Neural Networks", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with recurrent neural networks (RNNs) on long sequences is a\nnotoriously difficult task. There are three major challenges: 1) complex\ndependencies, 2) vanishing and exploding gradients, and 3) efficient\nparallelization. In this paper, we introduce a simple yet effective RNN\nconnection structure, the DilatedRNN, which simultaneously tackles all of these\nchallenges. The proposed architecture is characterized by multi-resolution\ndilated recurrent skip connections and can be combined flexibly with diverse\nRNN cells. Moreover, the DilatedRNN reduces the number of parameters needed and\nenhances training efficiency significantly, while matching state-of-the-art\nperformance (even with standard RNN cells) in tasks involving very long-term\ndependencies. To provide a theory-based quantification of the architecture's\nadvantages, we introduce a memory capacity measure, the mean recurrent length,\nwhich is more suitable for RNNs with long skip connections than existing\nmeasures. We rigorously prove the advantages of the DilatedRNN over other\nrecurrent neural architectures. The code for our method is publicly available\nat https://github.com/code-terminator/DilatedRNN\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 21:28:01 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 15:46:44 GMT"}, {"version": "v3", "created": "Thu, 2 Nov 2017 01:24:16 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Chang", "Shiyu", ""], ["Zhang", "Yang", ""], ["Han", "Wei", ""], ["Yu", "Mo", ""], ["Guo", "Xiaoxiao", ""], ["Tan", "Wei", ""], ["Cui", "Xiaodong", ""], ["Witbrock", "Michael", ""], ["Hasegawa-Johnson", "Mark", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1710.02238", "submitter": "Garrett Goh", "authors": "Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas,\n  Nathan Baker", "title": "How Much Chemistry Does a Deep Neural Network Need to Know to Make\n  Accurate Predictions?", "comments": "In Proceedings of 2018 IEEE Winter Conference on Applications of\n  Computer Vision (WACV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The meteoric rise of deep learning models in computer vision research, having\nachieved human-level accuracy in image recognition tasks is firm evidence of\nthe impact of representation learning of deep neural networks. In the chemistry\ndomain, recent advances have also led to the development of similar CNN models,\nsuch as Chemception, that is trained to predict chemical properties using\nimages of molecular drawings. In this work, we investigate the effects of\nsystematically removing and adding localized domain-specific information to the\nimage channels of the training data. By augmenting images with only 3\nadditional basic information, and without introducing any architectural\nchanges, we demonstrate that an augmented Chemception (AugChemception)\noutperforms the original model in the prediction of toxicity, activity, and\nsolvation free energy. Then, by altering the information content in the images,\nand examining the resulting model's performance, we also identify two distinct\nlearning patterns in predicting toxicity/activity as compared to solvation free\nenergy. These patterns suggest that Chemception is learning about its tasks in\nthe manner that is consistent with established knowledge. Thus, our work\ndemonstrates that advanced chemical knowledge is not a pre-requisite for deep\nlearning models to accurately predict complex chemical properties.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 23:53:59 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 14:03:12 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Goh", "Garrett B.", ""], ["Siegel", "Charles", ""], ["Vishnu", "Abhinav", ""], ["Hodas", "Nathan O.", ""], ["Baker", "Nathan", ""]]}, {"id": "1710.02248", "submitter": "Chin-Wei Huang", "authors": "Chin-Wei Huang, Ahmed Touati, Laurent Dinh, Michal Drozdzal, Mohammad\n  Havaei, Laurent Charlin, Aaron Courville", "title": "Learnable Explicit Density for Continuous Latent Space and Variational\n  Inference", "comments": "2 figures, 5 pages, submitted to ICML Principled Approaches to Deep\n  Learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study two aspects of the variational autoencoder (VAE): the\nprior distribution over the latent variables and its corresponding posterior.\nFirst, we decompose the learning of VAEs into layerwise density estimation, and\nargue that having a flexible prior is beneficial to both sample generation and\ninference. Second, we analyze the family of inverse autoregressive flows\n(inverse AF) and show that with further improvement, inverse AF could be used\nas universal approximation to any complicated posterior. Our analysis results\nin a unified approach to parameterizing a VAE, without the need to restrict\nourselves to use factorial Gaussians in the latent real space.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 00:51:03 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Huang", "Chin-Wei", ""], ["Touati", "Ahmed", ""], ["Dinh", "Laurent", ""], ["Drozdzal", "Michal", ""], ["Havaei", "Mohammad", ""], ["Charlin", "Laurent", ""], ["Courville", "Aaron", ""]]}, {"id": "1710.02254", "submitter": "Chaitanya Ahuja", "authors": "Chaitanya Ahuja and Louis-Philippe Morency", "title": "Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency\n  for Sequence Modeling", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks have shown remarkable success in modeling\nsequences. However low resource situations still adversely affect the\ngeneralizability of these models. We introduce a new family of models, called\nLattice Recurrent Units (LRU), to address the challenge of learning deep\nmulti-layer recurrent models with limited resources. LRU models achieve this\ngoal by creating distinct (but coupled) flow of information inside the units: a\nfirst flow along time dimension and a second flow along depth dimension. It\nalso offers a symmetry in how information can flow horizontally and vertically.\nWe analyze the effects of decoupling three different components of our LRU\nmodel: Reset Gate, Update Gate and Projected State. We evaluate this family on\nnew LRU models on computational convergence rates and statistical efficiency.\nOur experiments are performed on four publicly-available datasets, comparing\nwith Grid-LSTM and Recurrent Highway networks. Our results show that LRU has\nbetter empirical computational convergence rates and statistical efficiency\nvalues, along with learning more accurate language models.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 01:52:14 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 05:11:17 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Ahuja", "Chaitanya", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1710.02280", "submitter": "Yifei Teng", "authors": "Yifei Teng, An Zhao, Camille Goudeseune", "title": "Generating Nontrivial Melodies for Music as a Service", "comments": "ISMIR 2017 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a hybrid neural network and rule-based system that generates pop\nmusic. Music produced by pure rule-based systems often sounds mechanical. Music\nproduced by machine learning sounds better, but still lacks hierarchical\ntemporal structure. We restore temporal hierarchy by augmenting machine\nlearning with a temporal production grammar, which generates the music's\noverall structure and chord progressions. A compatible melody is then generated\nby a conditional variational recurrent autoencoder. The autoencoder is trained\nwith eight-measure segments from a corpus of 10,000 MIDI files, each of which\nhas had its melody track and chord progressions identified heuristically. The\nautoencoder maps melody into a multi-dimensional feature space, conditioned by\nthe underlying chord progression. A melody is then generated by feeding a\nrandom sample from that space to the autoencoder's decoder, along with the\nchord progression generated by the grammar. The autoencoder can make musically\nplausible variations on an existing melody, suitable for recurring motifs. It\ncan also reharmonize a melody to a new chord progression, keeping the rhythm\nand contour. The generated music compares favorably with that generated by\nother academic and commercial software designed for the music-as-a-service\nindustry.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 05:53:20 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Teng", "Yifei", ""], ["Zhao", "An", ""], ["Goudeseune", "Camille", ""]]}, {"id": "1710.02298", "submitter": "Matteo Hessel", "authors": "Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg\n  Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, David Silver", "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning", "comments": "Under review as a conference paper at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep reinforcement learning community has made several independent\nimprovements to the DQN algorithm. However, it is unclear which of these\nextensions are complementary and can be fruitfully combined. This paper\nexamines six extensions to the DQN algorithm and empirically studies their\ncombination. Our experiments show that the combination provides\nstate-of-the-art performance on the Atari 2600 benchmark, both in terms of data\nefficiency and final performance. We also provide results from a detailed\nablation study that shows the contribution of each component to overall\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 07:45:46 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Hessel", "Matteo", ""], ["Modayil", "Joseph", ""], ["van Hasselt", "Hado", ""], ["Schaul", "Tom", ""], ["Ostrovski", "Georg", ""], ["Dabney", "Will", ""], ["Horgan", "Dan", ""], ["Piot", "Bilal", ""], ["Azar", "Mohammad", ""], ["Silver", "David", ""]]}, {"id": "1710.02338", "submitter": "Lei Huang", "authors": "Lei Huang, Xianglong Liu, Bo Lang and Bo Li", "title": "Projection Based Weight Normalization for Deep Neural Networks", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing deep neural networks (DNNs) often suffers from the ill-conditioned\nproblem. We observe that the scaling-based weight space symmetry property in\nrectified nonlinear network will cause this negative effect. Therefore, we\npropose to constrain the incoming weights of each neuron to be unit-norm, which\nis formulated as an optimization problem over Oblique manifold. A simple yet\nefficient method referred to as projection based weight normalization (PBWN) is\nalso developed to solve this problem. PBWN executes standard gradient updates,\nfollowed by projecting the updated weight back to Oblique manifold. This\nproposed method has the property of regularization and collaborates well with\nthe commonly used batch normalization technique. We conduct comprehensive\nexperiments on several widely-used image datasets including CIFAR-10,\nCIFAR-100, SVHN and ImageNet for supervised learning over the state-of-the-art\nconvolutional neural networks, such as Inception, VGG and residual networks.\nThe results show that our method is able to improve the performance of DNNs\nwith different architectures consistently. We also apply our method to Ladder\nnetwork for semi-supervised learning on permutation invariant MNIST dataset,\nand our method outperforms the state-of-the-art methods: we obtain test errors\nas 2.52%, 1.06%, and 0.91% with only 20, 50, and 100 labeled samples,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 10:24:38 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Huang", "Lei", ""], ["Liu", "Xianglong", ""], ["Lang", "Bo", ""], ["Li", "Bo", ""]]}, {"id": "1710.02511", "submitter": "Hao Li", "authors": "Hao Li and Zhijian Liu", "title": "Performance Prediction and Optimization of Solar Water Heater via a\n  Knowledge-Based Machine Learning Method", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the performance of solar energy and heat transfer systems requires\na lot of time, economic cost and manpower. Meanwhile, directly predicting their\nperformance is challenging due to the complicated internal structures.\nFortunately, a knowledge-based machine learning method can provide a promising\nprediction and optimization strategy for the performance of energy systems. In\nthis Chapter, the authors will show how they utilize the machine learning\nmodels trained from a large experimental database to perform precise prediction\nand optimization on a solar water heater (SWH) system. A new energy system\noptimization strategy based on a high-throughput screening (HTS) process is\nproposed. This Chapter consists of: i) Comparative studies on varieties of\nmachine learning models (artificial neural networks (ANNs), support vector\nmachine (SVM) and extreme learning machine (ELM)) to predict the performances\nof SWHs; ii) Development of an ANN-based software to assist the quick\nprediction and iii) Introduction of a computational HTS method to design a\nhigh-performance SWH system.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 17:39:32 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Li", "Hao", ""], ["Liu", "Zhijian", ""]]}, {"id": "1710.02543", "submitter": "Lei Tai", "authors": "Lei Tai and Jingwei Zhang and Ming Liu and Wolfram Burgard", "title": "Socially Compliant Navigation through Raw Depth Inputs with Generative\n  Adversarial Imitation Learning", "comments": "ICRA 2018 camera-ready version. 7 pages, video link:\n  https://www.youtube.com/watch?v=0hw0GD3lkA8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for mobile robots to learn to navigate in dynamic\nenvironments with pedestrians via raw depth inputs, in a socially compliant\nmanner. To achieve this, we adopt a generative adversarial imitation learning\n(GAIL) strategy, which improves upon a pre-trained behavior cloning policy. Our\napproach overcomes the disadvantages of previous methods, as they heavily\ndepend on the full knowledge of the location and velocity information of nearby\npedestrians, which not only requires specific sensors, but also the extraction\nof such state information from raw sensory input could consume much computation\ntime. In this paper, our proposed GAIL-based model performs directly on raw\ndepth inputs and plans in real-time. Experiments show that our GAIL-based\napproach greatly improves the safety and efficiency of the behavior of mobile\nrobots from pure behavior cloning. The real-world deployment also shows that\nour method is capable of guiding autonomous vehicles to navigate in a socially\ncompliant manner directly through raw depth inputs. In addition, we release a\nsimulation plugin for modeling pedestrian behaviors based on the social force\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 18:29:44 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 05:56:54 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Tai", "Lei", ""], ["Zhang", "Jingwei", ""], ["Liu", "Ming", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1710.02648", "submitter": "Yujian Li", "authors": "Yujian Li", "title": "Can Machines Think in Radio Language?", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People can think in auditory, visual and tactile forms of language, so can\nmachines principally. But is it possible for them to think in radio language?\nAccording to a first principle presented for general intelligence, i.e. the\nprinciple of language's relativity, the answer may give an exceptional solution\nfor robot astronauts to talk with each other in space exploration.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 08:03:58 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 08:49:37 GMT"}, {"version": "v3", "created": "Sun, 17 Dec 2017 12:39:53 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Li", "Yujian", ""]]}, {"id": "1710.02714", "submitter": "Qiaozi Gao", "authors": "Qiaozi Gao, Lanbo She, and Joyce Y. Chai", "title": "Interactive Learning of State Representation through Natural Language\n  Instruction and Explanation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One significant simplification in most previous work on robot learning is the\nclosed-world assumption where the robot is assumed to know ahead of time a\ncomplete set of predicates describing the state of the physical world. However,\nrobots are not likely to have a complete model of the world especially when\nlearning a new task. To address this problem, this extended abstract gives a\nbrief introduction to our on-going work that aims to enable the robot to\nacquire new state representations through language communication with humans.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 17:45:14 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Gao", "Qiaozi", ""], ["She", "Lanbo", ""], ["Chai", "Joyce Y.", ""]]}, {"id": "1710.02754", "submitter": "Jos\\'e Silva Neto", "authors": "Jos\\'e F. S. Neto, Waldson P. N. Leandro, Matheus A. Gadelha, Tiago S.\n  Santos, Bruno M. Carvalho, Edgar Gardu\\~no", "title": "Texture Fuzzy Segmentation using Skew Divergence Adaptive Affinity\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital image segmentation is the process of assigning distinct labels to\ndifferent objects in a digital image, and the fuzzy segmentation algorithm has\nbeen successfully used in the segmentation of images from a wide variety of\nsources. However, the traditional fuzzy segmentation algorithm fails to segment\nobjects that are characterized by textures whose patterns cannot be\nsuccessfully described by simple statistics computed over a very restricted\narea. In this paper, we propose an extension of the fuzzy segmentation\nalgorithm that uses adaptive textural affinity functions to perform the\nsegmentation of such objects on bidimensional images. The adaptive affinity\nfunctions compute their appropriate neighborhood size as they compute the\ntexture descriptors surrounding the seed spels (spatial elements), according to\nthe characteristics of the texture being processed. The algorithm then segments\nthe image with an appropriate neighborhood for each object. We performed\nexperiments on mosaic images that were composed using images from the Brodatz\ndatabase, and compared our results with the ones produced by a recently\npublished texture segmentation algorithm, showing the applicability of our\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 22:10:08 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Neto", "Jos\u00e9 F. S.", ""], ["Leandro", "Waldson P. N.", ""], ["Gadelha", "Matheus A.", ""], ["Santos", "Tiago S.", ""], ["Carvalho", "Bruno M.", ""], ["Gardu\u00f1o", "Edgar", ""]]}, {"id": "1710.02869", "submitter": "Isaac Sledge", "authors": "Isaac J. Sledge, Jose C. Principe", "title": "An Analysis of the Value of Information when Exploring Stochastic,\n  Discrete Multi-Armed Bandits", "comments": "Entropy", "journal-ref": null, "doi": "10.3390/e20030155", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an information-theoretic exploration strategy for\nstochastic, discrete multi-armed bandits that achieves optimal regret. Our\nstrategy is based on the value of information criterion. This criterion\nmeasures the trade-off between policy information and obtainable rewards. High\namounts of policy information are associated with exploration-dominant searches\nof the space and yield high rewards. Low amounts of policy information favor\nthe exploitation of existing knowledge. Information, in this criterion, is\nquantified by a parameter that can be varied during search. We demonstrate that\na simulated-annealing-like update of this parameter, with a sufficiently fast\ncooling schedule, leads to an optimal regret that is logarithmic with respect\nto the number of episodes.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 18:48:48 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 21:01:57 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Sledge", "Isaac J.", ""], ["Principe", "Jose C.", ""]]}, {"id": "1710.02896", "submitter": "Doo Re Song", "authors": "Doo Re Song, Chuanyu Yang, Christopher McGreavy, Zhibin Li", "title": "Recurrent Deterministic Policy Gradient Method for Bipedal Locomotion on\n  Rough Terrain Challenge", "comments": "Published in IEEE proceedings: 2018 15th International Conference on\n  Control, Automation, Robotics and Vision (IEEE-ICARCV)", "journal-ref": "The Institute of Electrical and Electronics Engineers 2018 15th\n  International Conference on Control, Automation, Robotics and Vision\n  (IEEE-ICARCV)", "doi": "10.1109/ICARCV.2018.8581309", "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep learning framework that is capable of solving\npartially observable locomotion tasks based on our novel interpretation of\nRecurrent Deterministic Policy Gradient (RDPG). We study on bias of sampled\nerror measure and its variance induced by the partial observability of\nenvironment and subtrajectory sampling, respectively. Three major improvements\nare introduced in our RDPG based learning framework: tail-step bootstrap of\ninterpolated temporal difference, initialisation of hidden state using past\ntrajectory scanning, and injection of external experiences learned by other\nagents. The proposed learning framework was implemented to solve the\nBipedal-Walker challenge in OpenAI's gym simulation environment where only\npartial state information is available. Our simulation study shows that the\nautonomous behaviors generated by the RDPG agent are highly adaptive to a\nvariety of obstacles and enables the agent to effectively traverse rugged\nterrains for long distance with higher success rate than leading contenders.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 22:38:34 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 10:12:48 GMT"}, {"version": "v3", "created": "Sun, 6 May 2018 16:54:06 GMT"}, {"version": "v4", "created": "Sat, 11 Aug 2018 09:55:39 GMT"}, {"version": "v5", "created": "Thu, 13 Sep 2018 08:40:02 GMT"}, {"version": "v6", "created": "Sun, 15 Dec 2019 11:03:01 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Song", "Doo Re", ""], ["Yang", "Chuanyu", ""], ["McGreavy", "Christopher", ""], ["Li", "Zhibin", ""]]}, {"id": "1710.02913", "submitter": "Mingzhe Chen", "authors": "Mingzhe Chen, Ursula Challita, Walid Saad, Changchuan Yin, and\n  M\\'erouane Debbah", "title": "Artificial Neural Networks-Based Machine Learning for Wireless Networks:\n  A Tutorial", "comments": "This paper has been accepted by IEEE Communications Surveys &\n  Tutorials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation wireless networks must support ultra-reliable, low-latency\ncommunication and intelligently manage a massive number of Internet of Things\n(IoT) devices in real-time, within a highly dynamic environment. This need for\nstringent communication quality-of-service (QoS) requirements as well as mobile\nedge and core intelligence can only be realized by integrating fundamental\nnotions of artificial intelligence (AI) and machine learning across the\nwireless infrastructure and end-user devices. In this context, this paper\nprovides a comprehensive tutorial that introduces the main concepts of machine\nlearning, in general, and artificial neural networks (ANNs), in particular, and\ntheir potential applications in wireless communications. For this purpose, we\npresent a comprehensive overview on a number of key types of neural networks\nthat include feed-forward, recurrent, spiking, and deep neural networks. For\neach type of neural network, we present the basic architecture and training\nprocedure, as well as the associated challenges and opportunities. Then, we\nprovide an in-depth overview on the variety of wireless communication problems\nthat can be addressed using ANNs, ranging from communication using unmanned\naerial vehicles to virtual reality and edge caching.For each individual\napplication, we present the main motivation for using ANNs along with the\nassociated challenges while also providing a detailed example for a use case\nscenario and outlining future works that can be addressed using ANNs. In a\nnutshell, this article constitutes one of the first holistic tutorials on the\ndevelopment of machine learning techniques tailored to the needs of future\nwireless networks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 02:33:43 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 02:21:28 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Chen", "Mingzhe", ""], ["Challita", "Ursula", ""], ["Saad", "Walid", ""], ["Yin", "Changchuan", ""], ["Debbah", "M\u00e9rouane", ""]]}, {"id": "1710.03131", "submitter": "Huikai Wu", "authors": "Huikai Wu, Junge Zhang, Kaiqi Huang", "title": "MSC: A Dataset for Macro-Management in StarCraft II", "comments": "Homepage: https://github.com/wuhuikai/MSC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Macro-management is an important problem in StarCraft, which has been studied\nfor a long time. Various datasets together with assorted methods have been\nproposed in the last few years. But these datasets have some defects for\nboosting the academic and industrial research: 1) There're neither standard\npreprocessing, parsing and feature extraction procedures nor predefined\ntraining, validation and test set in some datasets. 2) Some datasets are only\nspecified for certain tasks in macro-management. 3) Some datasets are either\ntoo small or don't have enough labeled data for modern machine learning\nalgorithms such as deep neural networks. So most previous methods are trained\nwith various features, evaluated on different test sets from the same or\ndifferent datasets, making it difficult to be compared directly. To boost the\nresearch of macro-management in StarCraft, we release a new dataset MSC based\non the platform SC2LE. MSC consists of well-designed feature vectors,\npre-defined high-level actions and final result of each match. We also split\nMSC into training, validation and test set for the convenience of evaluation\nand comparison. Besides the dataset, we propose a baseline model and present\ninitial baseline results for global state evaluation and build order\nprediction, which are two of the key tasks in macro-management. Various\ndownstream tasks and analyses of the dataset are also described for the sake of\nresearch on macro-management in StarCraft II. Homepage:\nhttps://github.com/wuhuikai/MSC.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 14:59:11 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 12:06:34 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Wu", "Huikai", ""], ["Zhang", "Junge", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1710.03163", "submitter": "Mahmoud Nabil", "authors": "Mahmoud Nabil", "title": "Random Projection and Its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Projection is a foundational research topic that connects a bunch of\nmachine learning algorithms under a similar mathematical basis. It is used to\nreduce the dimensionality of the dataset by projecting the data points\nefficiently to a smaller dimensions while preserving the original relative\ndistance between the data points. In this paper, we are intended to explain\nrandom projection method, by explaining its mathematical background and\nfoundation, the applications that are currently adopting it, and an overview on\nits current research perspective.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 15:57:45 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Nabil", "Mahmoud", ""]]}, {"id": "1710.03184", "submitter": "Pratik Gajane", "authors": "Pratik Gajane and Mykola Pechenizkiy", "title": "On Formalizing Fairness in Prediction with Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms for prediction are increasingly being used in\ncritical decisions affecting human lives. Various fairness formalizations, with\nno firm consensus yet, are employed to prevent such algorithms from\nsystematically discriminating against people based on certain attributes\nprotected by law. The aim of this article is to survey how fairness is\nformalized in the machine learning literature for the task of prediction and\npresent these formalizations with their corresponding notions of distributive\njustice from the social sciences literature. We provide theoretical as well as\nempirical critiques of these notions from the social sciences literature and\nexplain how these critiques limit the suitability of the corresponding fairness\nformalizations to certain domains. We also suggest two notions of distributive\njustice which address some of these critiques and discuss avenues for\nprospective fairness formalizations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 16:39:31 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 10:12:23 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 08:22:01 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Gajane", "Pratik", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1710.03189", "submitter": "Muaz Niazi", "authors": "Waseem Akram, Muaz A. Niazi, Laszlo Barna Iantovics", "title": "Towards Agent-Based Model Specification in Smart Grid: A Cognitive\n  Agent-based Computing Approach", "comments": "14 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A smart grid can be considered as a complex network where each node\nrepresents a generation unit or a consumer. Whereas links can be used to\nrepresent transmission lines. One way to study complex systems is by using the\nagent-based modeling (ABM) paradigm. An ABM is a way of representing a complex\nsystem of autonomous agents interacting with each other. Previously, a number\nof studies have been presented in the smart grid domain making use of the ABM\nparadigm. However, to the best of our knowledge, none of these studies have\nfocused on the specification aspect of ABM. An ABM specification is important\nnot only for understanding but also for replication of the model. In this\nstudy, we focus on development as well as specification of ABM for smart grid.\nWe propose an ABM by using a combination of agent-based and complex\nnetwork-based approaches. For ABM specification, we use ODD and DREAM\nspecification approaches. We analyze these two specification approaches\nqualitatively as well as quantitatively. Extensive experiments demonstrate that\nDREAM is a most useful approach as compared with ODD for modeling as well as\nfor replication of models for smart grid.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 02:44:02 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 18:37:59 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Akram", "Waseem", ""], ["Niazi", "Muaz A.", ""], ["Iantovics", "Laszlo Barna", ""]]}, {"id": "1710.03263", "submitter": "Oren Elisha", "authors": "Oren Elisha and Shai Dekel", "title": "Function space analysis of deep learning representation layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a function space approach to Representation Learning\nand the analysis of the representation layers in deep learning architectures.\nWe show how to compute a weak-type Besov smoothness index that quantifies the\ngeometry of the clustering in the feature space. This approach was already\napplied successfully to improve the performance of machine learning algorithms\nsuch as the Random Forest and tree-based Gradient Boosting. Our experiments\ndemonstrate that in well-known and well-performing trained networks, the Besov\nsmoothness of the training set, measured in the corresponding hidden layer\nfeature map representation, increases from layer to layer. We also contribute\nto the understanding of generalization by showing how the Besov smoothness of\nthe representations, decreases as we add more mis-labeling to the training\ndata. We hope this approach will contribute to the de-mystification of some\naspects of deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 18:52:42 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Elisha", "Oren", ""], ["Dekel", "Shai", ""]]}, {"id": "1710.03285", "submitter": "Alejandro Molina", "authors": "Alejandro Molina, Alexander Munteanu, Kristian Kersting", "title": "Coresets for Dependency Networks", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications infer the structure of a probabilistic graphical model from\ndata to elucidate the relationships between variables. But how can we train\ngraphical models on a massive data set? In this paper, we show how to construct\ncoresets -compressed data sets which can be used as proxy for the original data\nand have provably bounded worst case error- for Gaussian dependency networks\n(DNs), i.e., cyclic directed graphical models over Gaussians, where the parents\nof each variable are its Markov blanket. Specifically, we prove that Gaussian\nDNs admit coresets of size independent of the size of the data set.\nUnfortunately, this does not extend to DNs over members of the exponential\nfamily in general. As we will prove, Poisson DNs do not admit small coresets.\nDespite this worst-case result, we will provide an argument why our coreset\nconstruction for DNs can still work well in practice on count data. To\ncorroborate our theoretical results, we empirically evaluated the resulting\nCore DNs on real data sets. The results\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 19:49:11 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 08:45:43 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Molina", "Alejandro", ""], ["Munteanu", "Alexander", ""], ["Kersting", "Kristian", ""]]}, {"id": "1710.03337", "submitter": "Jiajun Lu", "authors": "Jiajun Lu, Hussein Sibai, Evan Fabry, David Forsyth", "title": "Standard detectors aren't (currently) fooled by physical adversarial\n  stop signs", "comments": "Follow up for previous adversarial stop sign paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adversarial example is an example that has been adjusted to produce the\nwrong label when presented to a system at test time. If adversarial examples\nexisted that could fool a detector, they could be used to (for example) wreak\nhavoc on roads populated with smart vehicles. Recently, we described our\ndifficulties creating physical adversarial stop signs that fool a detector.\nMore recently, Evtimov et al. produced a physical adversarial stop sign that\nfools a proxy model of a detector. In this paper, we show that these physical\nadversarial stop signs do not fool two standard detectors (YOLO and Faster\nRCNN) in standard configuration. Evtimov et al.'s construction relies on a crop\nof the image to the stop sign; this crop is then resized and presented to a\nclassifier. We argue that the cropping and resizing procedure largely\neliminates the effects of rescaling and of view angle. Whether an adversarial\nattack is robust under rescaling and change of view direction remains moot. We\nargue that attacking a classifier is very different from attacking a detector,\nand that the structure of detectors - which must search for their own bounding\nbox, and which cannot estimate that box very accurately - likely makes it\ndifficult to make adversarial patterns. Finally, an adversarial pattern on a\nphysical object that could fool a detector would have to be adversarial in the\nface of a wide family of parametric distortions (scale; view angle; box shift\ninside the detector; illumination; and so on). Such a pattern would be of great\ntheoretical and practical interest. There is currently no evidence that such\npatterns exist.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 22:20:59 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 21:53:58 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Lu", "Jiajun", ""], ["Sibai", "Hussein", ""], ["Fabry", "Evan", ""], ["Forsyth", "David", ""]]}, {"id": "1710.03346", "submitter": "Hao Chen", "authors": "Hao Chen, Maria Vasardani, Stephan Winter", "title": "Geo-referencing Place from Everyday Natural Language Descriptions", "comments": "28 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language place descriptions in everyday communication provide a rich\nsource of spatial knowledge about places. An important step to utilize such\nknowledge in information systems is geo-referencing all the places referred to\nin these descriptions. Current techniques for geo-referencing places from text\ndocuments are using place name recognition and disambiguation; however, place\ndescriptions often contain place references that are not known by gazetteers,\nor that are expressed in other, more flexible ways. Hence, the approach for\ngeo-referencing presented in this paper starts from a place graph that contains\nthe place references as well as spatial relationships extracted from place\ndescriptions. Spatial relationships are used to constrain the locations of\nplaces and allow the later best-matching process for geo-referencing. The novel\ngeo-referencing process results in higher precision and recall compared to\nstate-of-art toponym resolution approaches on several tested place description\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 23:06:17 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Chen", "Hao", ""], ["Vasardani", "Maria", ""], ["Winter", "Stephan", ""]]}, {"id": "1710.03390", "submitter": "EPTCS", "authors": "Sjur K Dyrkolbotn (Western Norway University of Applied Sciences)", "title": "On Preemption and Overdetermination in Formal Theories of Causality", "comments": "In Proceedings CREST 2017, arXiv:1710.02770", "journal-ref": "EPTCS 259, 2017, pp. 1-15", "doi": "10.4204/EPTCS.259.1", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges when looking for the causes of a complex event is\nto determine the causal status of factors that are neither individually\nnecessary nor individually sufficient to produce that event. In order to reason\nabout how such factors should be taken into account, we need a vocabulary to\ndistinguish different cases. In philosophy, the concept of overdetermination\nand the concept of preemption serve an important purpose in this regard,\nalthough their exact meaning tends to remain elusive. In this paper, I provide\ntheory-neutral definitions of these concepts using structural equations in the\nHalpern-Pearl tradition. While my definitions do not presuppose any particular\ncausal theory, they take such a theory as a variable parameter. This enables us\nto specify formal constraints on theories of causality, in terms of a\npre-theoretic understanding of what preemption and overdetermination actually\nmean. I demonstrate the usefulness of this by presenting and arguing for what I\ncall the principle of presumption. Roughly speaking, this principle states that\na possible cause can only be regarded as having been preempted if there is\nindependent evidence to support such an inference. I conclude by showing that\nthe principle of presumption is violated by the two main theories of causality\nformulated in the Halpern-Pearl tradition. The paper concludes by defining the\nclass of empirical causal theories, characterised in terms of a fixed-point of\ncounterfactual reasoning about difference-making. It is argued that theories of\nactual causality ought to be empirical.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 03:50:30 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Dyrkolbotn", "Sjur K", "", "Western Norway University of Applied Sciences"]]}, {"id": "1710.03392", "submitter": "EPTCS", "authors": "Marco Bozzano", "title": "Causality and Temporal Dependencies in the Design of Fault Management\n  Systems", "comments": "In Proceedings CREST 2017, arXiv:1710.02770", "journal-ref": "EPTCS 259, 2017, pp. 39-46", "doi": "10.4204/EPTCS.259.4", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about causes and effects naturally arises in the engineering of\nsafety-critical systems. A classical example is Fault Tree Analysis, a\ndeductive technique used for system safety assessment, whereby an undesired\nstate is reduced to the set of its immediate causes. The design of fault\nmanagement systems also requires reasoning on causality relationships. In\nparticular, a fail-operational system needs to ensure timely detection and\nidentification of faults, i.e. recognize the occurrence of run-time faults\nthrough their observable effects on the system. Even more complex scenarios\narise when multiple faults are involved and may interact in subtle ways.\n  In this work, we propose a formal approach to fault management for complex\nsystems. We first introduce the notions of fault tree and minimal cut sets. We\nthen present a formal framework for the specification and analysis of\ndiagnosability, and for the design of fault detection and identification (FDI)\ncomponents. Finally, we review recent advances in fault propagation analysis,\nbased on the Timed Failure Propagation Graphs (TFPG) formalism.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 03:51:47 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Bozzano", "Marco", ""]]}, {"id": "1710.03393", "submitter": "EPTCS", "authors": "Gregor G\\\"ossler (INRIA, France), Oleg Sokolsky (University of\n  Pennsylvania, Philadelphia, USA), Jean-Bernard Stefani (INRIA, France)", "title": "Counterfactual Causality from First Principles?", "comments": "In Proceedings CREST 2017, arXiv:1710.02770", "journal-ref": "EPTCS 259, 2017, pp. 47-53", "doi": "10.4204/EPTCS.259.5", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this position paper we discuss three main shortcomings of existing\napproaches to counterfactual causality from the computer science perspective,\nand sketch lines of work to try and overcome these issues: (1) causality\ndefinitions should be driven by a set of precisely specified requirements\nrather than specific examples; (2) causality frameworks should support system\ndynamics; (3) causality analysis should have a well-understood behavior in\npresence of abstraction.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 03:52:07 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["G\u00f6ssler", "Gregor", "", "INRIA, France"], ["Sokolsky", "Oleg", "", "University of\n  Pennsylvania, Philadelphia, USA"], ["Stefani", "Jean-Bernard", "", "INRIA, France"]]}, {"id": "1710.03399", "submitter": "Sael Lee", "authors": "Vasundhara Dehiya, Jaya Thomas, Lee Sael", "title": "Prior Knowledge based mutation prioritization towards causal variant\n  finding in rare disease", "comments": "21 pages, 5 figures, submitted for journal publication in 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do we determine the mutational effects in exome sequencing data with\nlittle or no statistical evidence? Can protein structural information fill in\nthe gap of not having enough statistical evidence? In this work, we answer the\ntwo questions with the goal towards determining pathogenic effects of rare\nvariants in rare disease. We take the approach of determining the importance of\npoint mutation loci focusing on protein structure features. The proposed\nstructure-based features contain information about geometric, physicochemical,\nand functional information of mutation loci and those of structural neighbors\nof the loci. The performance of the structure-based features trained on 80\\% of\nHumDiv and tested on 20\\% of HumDiv and on ClinVar datasets showed high levels\nof discernibility in the mutation's pathogenic or benign effects: F score of\n0.71 and 0.68 respectively using multi-layer perceptron. Combining structure-\nand sequence-based feature further improve the accuracy: F score of 0.86\n(HumDiv) and 0.75 (ClinVar). Also, careful examination of the rare variants in\nrare diseases cases showed that structure-based features are important in\ndiscerning importance of variant loci.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 04:51:17 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Dehiya", "Vasundhara", ""], ["Thomas", "Jaya", ""], ["Sael", "Lee", ""]]}, {"id": "1710.03414", "submitter": "Chao-Ming Wang", "authors": "Chao-Ming Wang", "title": "Network of Recurrent Neural Networks", "comments": "Under review as a conference paper at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We describe a class of systems theory based neural networks called \"Network\nOf Recurrent neural networks\" (NOR), which introduces a new structure level to\nRNN related models. In NOR, RNNs are viewed as the high-level neurons and are\nused to build the high-level layers. More specifically, we propose several\nmethodologies to design different NOR topologies according to the theory of\nsystem evolution. Then we carry experiments on three different tasks to\nevaluate our implementations. Experimental results show our models outperform\nsimple RNN remarkably under the same number of parameters, and sometimes\nachieve even better results than GRU and LSTM.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 06:14:58 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Wang", "Chao-Ming", ""]]}, {"id": "1710.03430", "submitter": "Seunghyun Yoon", "authors": "Seunghyun Yoon, Joongbo Shin, Kyomin Jung", "title": "Learning to Rank Question-Answer Pairs using Hierarchical Recurrent\n  Encoder with Latent Topic Clustering", "comments": "10 pages, Accepted as a conference paper at NAACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel end-to-end neural architecture for ranking\ncandidate answers, that adapts a hierarchical recurrent neural network and a\nlatent topic clustering module. With our proposed model, a text is encoded to a\nvector representation from an word-level to a chunk-level to effectively\ncapture the entire meaning. In particular, by adapting the hierarchical\nstructure, our model shows very small performance degradations in longer text\ncomprehension while other state-of-the-art recurrent neural network models\nsuffer from it. Additionally, the latent topic clustering module extracts\nsemantic information from target samples. This clustering module is useful for\nany text related tasks by allowing each data sample to find its nearest topic\ncluster, thus helping the neural network model analyze the entire data. We\nevaluate our models on the Ubuntu Dialogue Corpus and consumer electronic\ndomain question answering dataset, which is related to Samsung products. The\nproposed model shows state-of-the-art results for ranking question-answer\npairs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 07:26:50 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 04:33:41 GMT"}, {"version": "v3", "created": "Mon, 9 Apr 2018 10:04:00 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Yoon", "Seunghyun", ""], ["Shin", "Joongbo", ""], ["Jung", "Kyomin", ""]]}, {"id": "1710.03442", "submitter": "Ryo Iwaki", "authors": "Ryo Iwaki and Minoru Asada", "title": "On- and Off-Policy Monotonic Policy Improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monotonic policy improvement and off-policy learning are two main desirable\nproperties for reinforcement learning algorithms. In this paper, by lower\nbounding the performance difference of two policies, we show that the monotonic\npolicy improvement is guaranteed from on- and off-policy mixture samples. An\noptimization procedure which applies the proposed bound can be regarded as an\noff-policy natural policy gradient method. In order to support the theoretical\nresult, we provide a trust region policy optimization method using experience\nreplay as a naive application of our bound, and evaluate its performance in two\nclassical benchmark problems.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 08:18:24 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 08:37:34 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Iwaki", "Ryo", ""], ["Asada", "Minoru", ""]]}, {"id": "1710.03481", "submitter": "Davide Grossi", "authors": "Agneau Belanyek, Davide Grossi, Wiebe van der Hoek", "title": "A Note on Nesting in Dyadic Deontic Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper reports on some results concerning Aqvist's dyadic logic known as\nsystem G, which is one of the most influential logics for reasoning with dyadic\nobligations (\"it ought to be the case that ... if it is the case that ...\").\nAlthough this logic has been known in the literature for a while, many of its\nproperties still await in-depth consideration. In this short paper we show:\nthat any formula in system G including nested modal operators is equivalent to\nsome formula with no nesting; that the universal modality introduced by Aqvist\nin the first presentation of the system is definable in terms of the deontic\nmodality.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 09:45:25 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Belanyek", "Agneau", ""], ["Grossi", "Davide", ""], ["van der Hoek", "Wiebe", ""]]}, {"id": "1710.03592", "submitter": "Kun Li", "authors": "Kun Li, Joel W. Burdick", "title": "Meta Inverse Reinforcement Learning via Maximum Reward Sharing for Human\n  Motion Analysis", "comments": "arXiv admin note: text overlap with arXiv:1707.09394", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work handles the inverse reinforcement learning (IRL) problem where only\na small number of demonstrations are available from a demonstrator for each\nhigh-dimensional task, insufficient to estimate an accurate reward function.\nObserving that each demonstrator has an inherent reward for each state and the\ntask-specific behaviors mainly depend on a small number of key states, we\npropose a meta IRL algorithm that first models the reward function for each\ntask as a distribution conditioned on a baseline reward function shared by all\ntasks and dependent only on the demonstrator, and then finds the most likely\nreward function in the distribution that explains the task-specific behaviors.\nWe test the method in a simulated environment on path planning tasks with\nlimited demonstrations, and show that the accuracy of the learned reward\nfunction is significantly improved. We also apply the method to analyze the\nmotion of a patient under rehabilitation.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 20:22:32 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 20:42:35 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Li", "Kun", ""], ["Burdick", "Joel W.", ""]]}, {"id": "1710.03641", "submitter": "Maruan Al-Shedivat", "authors": "Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor\n  Mordatch, Pieter Abbeel", "title": "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive\n  Environments", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ability to continuously learn and adapt from limited experience in\nnonstationary environments is an important milestone on the path towards\ngeneral intelligence. In this paper, we cast the problem of continuous\nadaptation into the learning-to-learn framework. We develop a simple\ngradient-based meta-learning algorithm suitable for adaptation in dynamically\nchanging and adversarial scenarios. Additionally, we design a new multi-agent\ncompetitive environment, RoboSumo, and define iterated adaptation games for\ntesting various aspects of continuous adaptation strategies. We demonstrate\nthat meta-learning enables significantly more efficient adaptation than\nreactive baselines in the few-shot regime. Our experiments with a population of\nagents that learn and compete suggest that meta-learners are the fittest.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 15:00:37 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 17:27:36 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Bansal", "Trapit", ""], ["Burda", "Yuri", ""], ["Sutskever", "Ilya", ""], ["Mordatch", "Igor", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1710.03740", "submitter": "Sharan Narang", "authors": "Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos,\n  Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev,\n  Ganesh Venkatesh, Hao Wu", "title": "Mixed Precision Training", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have enabled progress in a wide variety of applications.\nGrowing the size of the neural network typically results in improved accuracy.\nAs model sizes grow, the memory and compute requirements for training these\nmodels also increases. We introduce a technique to train deep neural networks\nusing half precision floating point numbers. In our technique, weights,\nactivations and gradients are stored in IEEE half-precision format.\nHalf-precision floating numbers have limited numerical range compared to\nsingle-precision numbers. We propose two techniques to handle this loss of\ninformation. Firstly, we recommend maintaining a single-precision copy of the\nweights that accumulates the gradients after each optimizer step. This\nsingle-precision copy is rounded to half-precision format during training.\nSecondly, we propose scaling the loss appropriately to handle the loss of\ninformation with half-precision gradients. We demonstrate that this approach\nworks for a wide variety of models including convolution neural networks,\nrecurrent neural networks and generative adversarial networks. This technique\nworks for large scale models with more than 100 million parameters trained on\nlarge datasets. Using this approach, we can reduce the memory consumption of\ndeep learning models by nearly 2x. In future processors, we can also expect a\nsignificant computation speedup using half-precision hardware units.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 17:42:04 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 19:09:05 GMT"}, {"version": "v3", "created": "Thu, 15 Feb 2018 20:04:02 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Micikevicius", "Paulius", ""], ["Narang", "Sharan", ""], ["Alben", "Jonah", ""], ["Diamos", "Gregory", ""], ["Elsen", "Erich", ""], ["Garcia", "David", ""], ["Ginsburg", "Boris", ""], ["Houston", "Michael", ""], ["Kuchaiev", "Oleksii", ""], ["Venkatesh", "Ganesh", ""], ["Wu", "Hao", ""]]}, {"id": "1710.03748", "submitter": "Trapit Bansal", "authors": "Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, Igor\n  Mordatch", "title": "Emergent Complexity via Multi-Agent Competition", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning algorithms can train agents that solve problems in\ncomplex, interesting environments. Normally, the complexity of the trained\nagent is closely related to the complexity of the environment. This suggests\nthat a highly capable agent requires a complex environment for training. In\nthis paper, we point out that a competitive multi-agent environment trained\nwith self-play can produce behaviors that are far more complex than the\nenvironment itself. We also point out that such environments come with a\nnatural curriculum, because for any skill level, an environment full of agents\nof this level will have the right level of difficulty. This work introduces\nseveral competitive multi-agent environments where agents compete in a 3D world\nwith simulated physics. The trained agents learn a wide variety of complex and\ninteresting skills, even though the environment themselves are relatively\nsimple. The skills include behaviors such as running, blocking, ducking,\ntackling, fooling opponents, kicking, and defending using both arms and legs. A\nhighlight of the learned behaviors can be found here: https://goo.gl/eR7fbX\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 17:59:41 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 21:49:55 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 21:09:49 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Bansal", "Trapit", ""], ["Pachocki", "Jakub", ""], ["Sidor", "Szymon", ""], ["Sutskever", "Ilya", ""], ["Mordatch", "Igor", ""]]}, {"id": "1710.03753", "submitter": "AbdElRahman ElSaid", "authors": "AbdElRahman ElSaid, Travis Desell, Fatima El Jamiy, James Higgins,\n  Brandon Wild", "title": "Optimizing Long Short-Term Memory Recurrent Neural Networks Using Ant\n  Colony Optimization to Predict Turbine Engine Vibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article expands on research that has been done to develop a recurrent\nneural network (RNN) capable of predicting aircraft engine vibrations using\nlong short-term memory (LSTM) neurons. LSTM RNNs can provide a more\ngeneralizable and robust method for prediction over analytical calculations of\nengine vibration, as analytical calculations must be solved iteratively based\non specific empirical engine parameters, making this approach ungeneralizable\nacross multiple engines. In initial work, multiple LSTM RNN architectures were\nproposed, evaluated and compared. This research improves the performance of the\nmost effective LSTM network design proposed in the previous work by using a\npromising neuroevolution method based on ant colony optimization (ACO) to\ndevelop and enhance the LSTM cell structure of the network. A parallelized\nversion of the ACO neuroevolution algorithm has been developed and the evolved\nLSTM RNNs were compared to the previously used fixed topology. The evolved\nnetworks were trained on a large database of flight data records obtained from\nan airline containing flights that suffered from excessive vibration. Results\nwere obtained using MPI (Message Passing Interface) on a high performance\ncomputing (HPC) cluster, evolving 1000 different LSTM cell structures using 168\ncores over 4 days. The new evolved LSTM cells showed an improvement of 1.35%,\nreducing prediction error from 5.51% to 4.17% when predicting excessive engine\nvibrations 10 seconds in the future, while at the same time dramatically\nreducing the number of weights from 21,170 to 11,810.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 14:09:22 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["ElSaid", "AbdElRahman", ""], ["Desell", "Travis", ""], ["Jamiy", "Fatima El", ""], ["Higgins", "James", ""], ["Wild", "Brandon", ""]]}, {"id": "1710.03774", "submitter": "Hang Ma", "authors": "Hang Ma, Sven Koenig", "title": "AI Buzzwords Explained: Multi-Agent Path Finding (MAPF)", "comments": null, "journal-ref": null, "doi": "10.1145/3137574.3137579", "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanation of the hot topic \"multi-agent path finding\".\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 18:24:34 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 00:21:44 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Ma", "Hang", ""], ["Koenig", "Sven", ""]]}, {"id": "1710.03792", "submitter": "Hongjia Li", "authors": "Hongjia Li, Tianshu Wei, Ao Ren, Qi Zhu, Yanzhi Wang", "title": "Deep Reinforcement Learning: Framework, Applications, and Embedded\n  Implementations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent breakthroughs of deep reinforcement learning (DRL) technique in\nAlpha Go and playing Atari have set a good example in handling large state and\nactions spaces of complicated control problems. The DRL technique is comprised\nof (i) an offline deep neural network (DNN) construction phase, which derives\nthe correlation between each state-action pair of the system and its value\nfunction, and (ii) an online deep Q-learning phase, which adaptively derives\nthe optimal action and updates value estimates. In this paper, we first present\nthe general DRL framework, which can be widely utilized in many applications\nwith different optimization objectives. This is followed by the introduction of\nthree specific applications: the cloud computing resource allocation problem,\nthe residential smart grid task scheduling problem, and building HVAC system\noptimal control problem. The effectiveness of the DRL technique in these three\ncyber-physical applications have been validated. Finally, this paper\ninvestigates the stochastic computing-based hardware implementations of the DRL\nframework, which consumes a significant improvement in area efficiency and\npower consumption compared with binary-based implementation counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 19:22:50 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Li", "Hongjia", ""], ["Wei", "Tianshu", ""], ["Ren", "Ao", ""], ["Zhu", "Qi", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1710.03803", "submitter": "Mohsen Mahoor", "authors": "Mohana Alanazi, Mohsen Mahoor, Amin Khodaei", "title": "Day-Ahead Solar Forecasting Based on Multi-level Solar Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing proliferation in solar deployment, especially at distribution\nlevel, has made the case for power system operators to develop more accurate\nsolar forecasting models. This paper proposes a solar photovoltaic (PV)\ngeneration forecasting model based on multi-level solar measurements and\nutilizing a nonlinear autoregressive with exogenous input (NARX) model to\nimprove the training and achieve better forecasts. The proposed model consists\nof four stages of data preparation, establishment of fitting model, model\ntraining, and forecasting. The model is tested under different weather\nconditions. Numerical simulations exhibit the acceptable performance of the\nmodel when compared to forecasting results obtained from two-level and\nsingle-level studies.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 20:10:05 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Alanazi", "Mohana", ""], ["Mahoor", "Mohsen", ""], ["Khodaei", "Amin", ""]]}, {"id": "1710.03875", "submitter": "Marcell Vazquez-Chanlatte", "authors": "Marcell Vazquez-Chanlatte, Susmit Jha, Ashish Tiwari, Mark K. Ho,\n  Sanjit A. Seshia", "title": "Learning Task Specifications from Demonstrations", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world applications often naturally decompose into several sub-tasks. In\nmany settings (e.g., robotics) demonstrations provide a natural way to specify\nthe sub-tasks. However, most methods for learning from demonstrations either do\nnot provide guarantees that the artifacts learned for the sub-tasks can be\nsafely recombined or limit the types of composition available. Motivated by\nthis deficit, we consider the problem of inferring Boolean non-Markovian\nrewards (also known as logical trace properties or specifications) from\ndemonstrations provided by an agent operating in an uncertain, stochastic\nenvironment. Crucially, specifications admit well-defined composition rules\nthat are typically easy to interpret. In this paper, we formulate the\nspecification inference task as a maximum a posteriori (MAP) probability\ninference problem, apply the principle of maximum entropy to derive an analytic\ndemonstration likelihood model and give an efficient approach to search for the\nmost likely specification in a large candidate pool of specifications. In our\nexperiments, we demonstrate how learning specifications can help avoid common\nproblems that often arise due to ad-hoc reward composition.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 01:31:14 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 06:03:22 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2018 00:32:09 GMT"}, {"version": "v4", "created": "Tue, 14 Aug 2018 03:32:12 GMT"}, {"version": "v5", "created": "Sat, 27 Oct 2018 16:49:13 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Vazquez-Chanlatte", "Marcell", ""], ["Jha", "Susmit", ""], ["Tiwari", "Ashish", ""], ["Ho", "Mark K.", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1710.03937", "submitter": "Aleksandra Faust", "authors": "Aleksandra Faust, Oscar Ramirez, Marek Fiser, Kenneth Oslund, Anthony\n  Francis, James Davidson, and Lydia Tapia", "title": "PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement\n  Learning and Sampling-based Planning", "comments": "9 pages, 7 figures", "journal-ref": "IEEE International Conference on Robotics and Automation (ICRA),\n  2018", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PRM-RL, a hierarchical method for long-range navigation task\ncompletion that combines sampling based path planning with reinforcement\nlearning (RL). The RL agents learn short-range, point-to-point navigation\npolicies that capture robot dynamics and task constraints without knowledge of\nthe large-scale topology. Next, the sampling-based planners provide roadmaps\nwhich connect robot configurations that can be successfully navigated by the RL\nagent. The same RL agents are used to control the robot under the direction of\nthe planning, enabling long-range navigation. We use the Probabilistic Roadmaps\n(PRMs) for the sampling-based planner. The RL agents are constructed using\nfeature-based and deep neural net policies in continuous state and action\nspaces. We evaluate PRM-RL, both in simulation and on-robot, on two navigation\ntasks with non-trivial robot dynamics: end-to-end differential drive indoor\nnavigation in office environments, and aerial cargo delivery in urban\nenvironments with load displacement constraints. Our results show improvement\nin task completion over both RL agents on their own and traditional\nsampling-based planners. In the indoor navigation task, PRM-RL successfully\ncompletes up to 215 m long trajectories under noisy sensor conditions, and the\naerial cargo delivery completes flights over 1000 m without violating the task\nconstraints in an environment 63 million times larger than used in training.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 07:19:17 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 07:05:43 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Faust", "Aleksandra", ""], ["Ramirez", "Oscar", ""], ["Fiser", "Marek", ""], ["Oslund", "Kenneth", ""], ["Francis", "Anthony", ""], ["Davidson", "James", ""], ["Tapia", "Lydia", ""]]}, {"id": "1710.03978", "submitter": "Shamaila Iram", "authors": "Shamaila Iram, Terrence Fernando, May Bassanino", "title": "Exploring Cross-Domain Data Dependencies for Smart Homes to Improve\n  Energy Efficiency", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, the idea of smart homes has been conceived as a\npotential solution to counter energy crises or to at least mitigate its\nintensive destructive consequences in the residential building sector.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 09:40:39 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Iram", "Shamaila", ""], ["Fernando", "Terrence", ""], ["Bassanino", "May", ""]]}, {"id": "1710.04076", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt", "title": "Deep Semantic Abstractions of Everyday Human Activities: On Commonsense\n  Representations of Human Interactions", "comments": "In ROBOT 2017: Third Iberian Robotics Conference. Escuela T\\'ecnica\n  Superior de Ingenier\\'ia, Sevilla (Spain) (November 22-24, 2017).\n  https://grvc.us.es/robot2017/ (to appear). arXiv admin note: substantial text\n  overlap with arXiv:1709.05293", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep semantic characterization of space and motion categorically\nfrom the viewpoint of grounding embodied human-object interactions. Our key\nfocus is on an ontological model that would be adept to formalisation from the\nviewpoint of commonsense knowledge representation, relational learning, and\nqualitative reasoning about space and motion in cognitive robotics settings. We\ndemonstrate key aspects of the space & motion ontology and its formalization as\na representational framework in the backdrop of select examples from a dataset\nof everyday activities. Furthermore, focussing on human-object interaction data\nobtained from RGBD sensors, we also illustrate how declarative\n(spatio-temporal) reasoning in the (constraint) logic programming family may be\nperformed with the developed deep semantic abstractions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 07:40:39 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""]]}, {"id": "1710.04157", "submitter": "Jacob Devlin", "authors": "Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew Hausknecht, Pushmeet\n  Kohli", "title": "Neural Program Meta-Induction", "comments": "8 Pages + 1 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recently proposed methods for Neural Program Induction work under the\nassumption of having a large set of input/output (I/O) examples for learning\nany underlying input-output mapping. This paper aims to address the problem of\ndata and computation efficiency of program induction by leveraging information\nfrom related tasks. Specifically, we propose two approaches for cross-task\nknowledge transfer to improve program induction in limited-data scenarios. In\nour first proposal, portfolio adaptation, a set of induction models is\npretrained on a set of related tasks, and the best model is adapted towards the\nnew task using transfer learning. In our second approach, meta program\ninduction, a $k$-shot learning approach is used to make a model generalize to\nnew tasks without additional training. To test the efficacy of our methods, we\nconstructed a new benchmark of programs written in the Karel programming\nlanguage. Using an extensive experimental evaluation on the Karel benchmark, we\ndemonstrate that our proposals dramatically outperform the baseline induction\nmethod that does not use knowledge transfer. We also analyze the relative\nperformance of the two approaches and study conditions in which they perform\nbest. In particular, meta induction outperforms all existing approaches under\nextreme data sparsity (when a very small number of examples are available),\ni.e., fewer than ten. As the number of available I/O examples increase (i.e. a\nthousand or more), portfolio adapted program induction becomes the best\napproach. For intermediate data sizes, we demonstrate that the combined method\nof adapted meta program induction has the strongest performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 16:29:38 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Devlin", "Jacob", ""], ["Bunel", "Rudy", ""], ["Singh", "Rishabh", ""], ["Hausknecht", "Matthew", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1710.04161", "submitter": "Naveen Sundar Govindarajulu", "authors": "Naveen Sundar Govindarajulu and Selmer Bringsjord", "title": "Counterfactual Conditionals in Quantified Modal Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel formalization of counterfactual conditionals in a\nquantified modal logic. Counterfactual conditionals play a vital role in\nethical and moral reasoning. Prior work has shown that moral reasoning systems\n(and more generally, theory-of-mind reasoning systems) should be at least as\nexpressive as first-order (quantified) modal logic (QML) to be well-behaved.\nWhile existing work on moral reasoning has focused on counterfactual-free QML\nmoral reasoning, we present a fully specified and implemented formal system\nthat includes counterfactual conditionals. We validate our model with two\nprojects. In the first project, we demonstrate that our system can be used to\nmodel a complex moral principle, the doctrine of double effect. In the second\nproject, we use the system to build a data-set with true and false\ncounterfactuals as licensed by our theory, which we believe can be useful for\nother researchers. This project also shows that our model can be\ncomputationally feasible.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 16:32:30 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 23:04:57 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Govindarajulu", "Naveen Sundar", ""], ["Bringsjord", "Selmer", ""]]}, {"id": "1710.04162", "submitter": "Adam Stooke", "authors": "Adam Stooke and Pieter Abbeel", "title": "Synkhronos: a Multi-GPU Theano Extension for Data Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Synkhronos, an extension to Theano for multi-GPU computations\nleveraging data parallelism. Our framework provides automated execution and\nsynchronization across devices, allowing users to continue to write serial\nprograms without risk of race conditions. The NVIDIA Collective Communication\nLibrary is used for high-bandwidth inter-GPU communication. Further\nenhancements to the Theano function interface include input slicing (with\naggregation) and input indexing, which perform common data-parallel computation\npatterns efficiently. One example use case is synchronous SGD, which has\nrecently been shown to scale well for a growing set of deep learning problems.\nWhen training ResNet-50, we achieve a near-linear speedup of 7.5x on an NVIDIA\nDGX-1 using 8 GPUs, relative to Theano-only code running a single GPU in\nisolation. Yet Synkhronos remains general to any data-parallel computation\nprogrammable in Theano. By implementing parallelism at the level of individual\nTheano functions, our framework uniquely addresses a niche between manual\nmulti-device programming and prescribed multi-GPU training routines.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 16:38:58 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Stooke", "Adam", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1710.04312", "submitter": "Kyle Hundman", "authors": "Kyle Hundman, Chris A. Mattmann", "title": "Measurement Context Extraction from Text: Discovering Opportunities and\n  Gaps in Earth Science", "comments": null, "journal-ref": "23rd ACM SIGKDD International Conference on Knowledge Discovery\n  and Data Mining, Data-Driven Discovery Workshop, Halifax, Canada, August 2017", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Marve, a system for extracting measurement values, units, and\nrelated words from natural language text. Marve uses conditional random fields\n(CRF) to identify measurement values and units, followed by a rule-based system\nto find related entities, descriptors and modifiers within a sentence. Sentence\ntokens are represented by an undirected graphical model, and rules are based on\npart-of-speech and word dependency patterns connecting values and units to\ncontextual words. Marve is unique in its focus on measurement context and early\nexperimentation demonstrates Marve's ability to generate high-precision\nextractions with strong recall. We also discuss Marve's role in refining\nmeasurement requirements for NASA's proposed HyspIRI mission, a hyperspectral\ninfrared imaging satellite that will study the world's ecosystems. In general,\nour work with HyspIRI demonstrates the value of semantic measurement\nextractions in characterizing quantitative discussion contained in large\ncorpuses of natural language text. These extractions accelerate broad,\ncross-cutting research and expose scientists new algorithmic approaches and\nexperimental nuances. They also facilitate identification of scientific\nopportunities enabled by HyspIRI leading to more efficient scientific\ninvestment and research.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 21:37:07 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Hundman", "Kyle", ""], ["Mattmann", "Chris A.", ""]]}, {"id": "1710.04324", "submitter": "Md Kamruzzaman Sarker", "authors": "Md Kamruzzaman Sarker, Ning Xie, Derek Doran, Michael Raymer, Pascal\n  Hitzler", "title": "Explaining Trained Neural Networks with Semantic Web Technologies: First\n  Steps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever increasing prevalence of publicly available structured data on the\nWorld Wide Web enables new applications in a variety of domains. In this paper,\nwe provide a conceptual approach that leverages such data in order to explain\nthe input-output behavior of trained artificial neural networks. We apply\nexisting Semantic Web technologies in order to provide an experimental proof of\nconcept.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 22:32:51 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Sarker", "Md Kamruzzaman", ""], ["Xie", "Ning", ""], ["Doran", "Derek", ""], ["Raymer", "Michael", ""], ["Hitzler", "Pascal", ""]]}, {"id": "1710.04334", "submitter": "Allen Nie", "authors": "Allen Nie, Erin D. Bennett, Noah D. Goodman", "title": "DisSent: Sentence Representation Learning from Explicit Discourse\n  Relations", "comments": "13 pages, 4 figures. ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning effective representations of sentences is one of the core missions\nof natural language understanding. Existing models either train on a vast\namount of text, or require costly, manually curated sentence relation datasets.\nWe show that with dependency parsing and rule-based rubrics, we can curate a\nhigh quality sentence relation task by leveraging explicit discourse relations.\nWe show that our curated dataset provides an excellent signal for learning\nvector representations of sentence meaning, representing relations that can\nonly be determined when the meanings of two sentences are combined. We\ndemonstrate that the automatically curated corpus allows a bidirectional LSTM\nsentence encoder to yield high quality sentence embeddings and can serve as a\nsupervised fine-tuning dataset for larger models such as BERT. Our fixed\nsentence embeddings achieve high performance on a variety of transfer tasks,\nincluding SentEval, and we achieve state-of-the-art results on Penn Discourse\nTreebank's implicit relation prediction task.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 00:56:13 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 03:52:37 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 17:21:48 GMT"}, {"version": "v4", "created": "Tue, 4 Jun 2019 07:22:22 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Nie", "Allen", ""], ["Bennett", "Erin D.", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1710.04380", "submitter": "Tsuyoshi Kato", "authors": "Tsuyoshi Kato, Misato Kobayashi, Daisuke Sano", "title": "Sign-Constrained Regularized Loss Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical analysis, domain knowledge about analysis target has often been\naccumulated, although, typically, such knowledge has been discarded in the\nstatistical analysis stage, and the statistical tool has been applied as a\nblack box. In this paper, we introduce sign constraints that are a handy and\nsimple representation for non-experts in generic learning problems. We have\ndeveloped two new optimization algorithms for the sign-constrained regularized\nloss minimization, called the sign-constrained Pegasos (SC-Pega) and the\nsign-constrained SDCA (SC-SDCA), by simply inserting the sign correction step\ninto the original Pegasos and SDCA, respectively. We present theoretical\nanalyses that guarantee that insertion of the sign correction step does not\ndegrade the convergence rate for both algorithms. Two applications, where the\nsign-constrained learning is effective, are presented. The one is exploitation\nof prior information about correlation between explanatory variables and a\ntarget variable. The other is introduction of the sign-constrained to\nSVM-Pairwise method. Experimental results demonstrate significant improvement\nof generalization performance by introducing sign constraints in both\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 06:34:54 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Kato", "Tsuyoshi", ""], ["Kobayashi", "Misato", ""], ["Sano", "Daisuke", ""]]}, {"id": "1710.04382", "submitter": "Richard Everitt", "authors": "Richard G. Everitt and Dennis Prangle and Philip Maybank and Mark Bell", "title": "Marginal sequential Monte Carlo for doubly intractable models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for models that have an intractable partition function is\nknown as a doubly intractable problem, where standard Monte Carlo methods are\nnot applicable. The past decade has seen the development of auxiliary variable\nMonte Carlo techniques (M{\\o}ller et al., 2006; Murray et al., 2006) for\ntackling this problem; these approaches being members of the more general class\nof pseudo-marginal, or exact-approximate, Monte Carlo algorithms (Andrieu and\nRoberts, 2009), which make use of unbiased estimates of intractable posteriors.\nEveritt et al. (2017) investigated the use of exact-approximate importance\nsampling (IS) and sequential Monte Carlo (SMC) in doubly intractable problems,\nbut focussed only on SMC algorithms that used data-point tempering. This paper\ndescribes SMC samplers that may use alternative sequences of distributions, and\ndescribes ways in which likelihood estimates may be improved adaptively as the\nalgorithm progresses, building on ideas from Moores et al. (2015). This\napproach is compared with a number of alternative algorithms for doubly\nintractable problems, including approximate Bayesian computation (ABC), which\nwe show is closely related to the method of M{\\o}ller et al. (2006).\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 06:36:14 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Everitt", "Richard G.", ""], ["Prangle", "Dennis", ""], ["Maybank", "Philip", ""], ["Bell", "Mark", ""]]}, {"id": "1710.04459", "submitter": "Lex Fridman", "authors": "Lex Fridman, Li Ding, Benedikt Jenik, Bryan Reimer", "title": "Arguing Machines: Human Supervision of Black Box AI Systems That Make\n  Life-Critical Decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the paradigm of a black box AI system that makes life-critical\ndecisions. We propose an \"arguing machines\" framework that pairs the primary AI\nsystem with a secondary one that is independently trained to perform the same\ntask. We show that disagreement between the two systems, without any knowledge\nof underlying system design or operation, is sufficient to arbitrarily improve\nthe accuracy of the overall decision pipeline given human supervision over\ndisagreements. We demonstrate this system in two applications: (1) an\nillustrative example of image classification and (2) on large-scale real-world\nsemi-autonomous driving data. For the first application, we apply this\nframework to image classification achieving a reduction from 8.0% to 2.8% top-5\nerror on ImageNet. For the second application, we apply this framework to Tesla\nAutopilot and demonstrate the ability to predict 90.4% of system disengagements\nthat were labeled by human annotators as challenging and needing human\nsupervision.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 11:27:08 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 07:53:53 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Fridman", "Lex", ""], ["Ding", "Li", ""], ["Jenik", "Benedikt", ""], ["Reimer", "Bryan", ""]]}, {"id": "1710.04502", "submitter": "Vadim Sokolov", "authors": "Josh Warren and Jeff Lipkowitz and Vadim Sokolov", "title": "Clusters of Driving Behavior from Observational Smartphone Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding driving behaviors is essential for improving safety and\nmobility of our transportation systems. Data is usually collected via\nsimulator-based studies or naturalistic driving studies. Those techniques allow\nfor understanding relations between demographics, road conditions and safety.\nOn the other hand, they are very costly and time consuming. Thanks to the\nubiquity of smartphones, we have an opportunity to substantially complement\nmore traditional data collection techniques with data extracted from phone\nsensors, such as GPS, accelerometer gyroscope and camera. We developed\nstatistical models that provided insight into driver behavior in the San\nFrancisco metro area based on tens of thousands of driver logs. We used novel\ndata sources to support our work. We used cell phone sensor data drawn from\nfive hundred drivers in San Francisco to understand the speed of traffic across\nthe city as well as the maneuvers of drivers in different areas. Specifically,\nwe clustered drivers based on their driving behavior. We looked at driver norms\nby street and flagged driving behaviors that deviated from the norm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 13:31:21 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 21:09:56 GMT"}, {"version": "v3", "created": "Thu, 11 Jan 2018 20:39:25 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Warren", "Josh", ""], ["Lipkowitz", "Jeff", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1710.04582", "submitter": "Eleni Vasilaki D.Phil.", "authors": "Eleni Vasilaki", "title": "Is Epicurus the father of Reinforcement Learning?", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Epicurean Philosophy is commonly thought as simplistic and hedonistic.\nHere I discuss how this is a misconception and explore its link to\nReinforcement Learning. Based on the letters of Epicurus, I construct an\nobjective function for hedonism which turns out to be equivalent of the\nReinforcement Learning objective function when omitting the discount factor. I\nthen discuss how Plato and Aristotle 's views that can be also loosely linked\nto Reinforcement Learning, as well as their weaknesses in relationship to it.\nFinally, I emphasise the close affinity of the Epicurean views and the Bellman\nequation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 16:07:18 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Vasilaki", "Eleni", ""]]}, {"id": "1710.04584", "submitter": "Yongyu Wang", "authors": "Yongyu Wang, Zhuo Feng", "title": "Towards Scalable Spectral Clustering via Spectrum-Preserving\n  Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The eigendeomposition of nearest-neighbor (NN) graph Laplacian matrices is\nthe main computational bottleneck in spectral clustering. In this work, we\nintroduce a highly-scalable, spectrum-preserving graph sparsification algorithm\nthat enables to build ultra-sparse NN (u-NN) graphs with guaranteed\npreservation of the original graph spectrums, such as the first few\neigenvectors of the original graph Laplacian. Our approach can immediately lead\nto scalable spectral clustering of large data networks without sacrificing\nsolution quality. The proposed method starts from constructing low-stretch\nspanning trees (LSSTs) from the original graphs, which is followed by\niteratively recovering small portions of \"spectrally critical\" off-tree edges\nto the LSSTs by leveraging a spectral off-tree embedding scheme. To determine\nthe suitable amount of off-tree edges to be recovered to the LSSTs, an\neigenvalue stability checking scheme is proposed, which enables to robustly\npreserve the first few Laplacian eigenvectors within the sparsified graph.\nAdditionally, an incremental graph densification scheme is proposed for\nidentifying extra edges that have been missing in the original NN graphs but\ncan still play important roles in spectral clustering tasks. Our experimental\nresults for a variety of well-known data sets show that the proposed method can\ndramatically reduce the complexity of NN graphs, leading to significant\nspeedups in spectral clustering.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 16:09:29 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 00:51:54 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 01:04:10 GMT"}, {"version": "v4", "created": "Thu, 11 Oct 2018 17:59:49 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Wang", "Yongyu", ""], ["Feng", "Zhuo", ""]]}, {"id": "1710.04743", "submitter": "Thanh Tran", "authors": "Thanh Tran, Kyumin Lee, Nguyen Vo, Hongkyu Choi", "title": "Identifying On-time Reward Delivery Projects with Estimating Delivery\n  Duration on Kickstarter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Crowdfunding platforms, people turn their prototype ideas into real\nproducts by raising money from the crowd, or invest in someone else's projects.\nIn reward-based crowdfunding platforms such as Kickstarter and Indiegogo,\nselecting accurate reward delivery duration becomes crucial for creators,\nbackers, and platform providers to keep the trust between the creators and the\nbackers, and the trust between the platform providers and users. According to\nKickstarter, 35% backers did not receive rewards on time. Unfortunately, little\nis known about on-time and late reward delivery projects, and there is no prior\nwork to estimate reward delivery duration. To fill the gap, in this paper, we\n(i) extract novel features that reveal latent difficulty levels of project\nrewards; (ii) build predictive models to identify whether a creator will\ndeliver all rewards in a project on time or not; and (iii) build a regression\nmodel to estimate accurate reward delivery duration (i.e., how long it will\ntake to produce and deliver all the rewards). Experimental results show that\nour models achieve good performance -- 82.5% accuracy, 78.1 RMSE, and 0.108\nNRMSE at the first 5% of the longest reward delivery duration.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 22:47:55 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Tran", "Thanh", ""], ["Lee", "Kyumin", ""], ["Vo", "Nguyen", ""], ["Choi", "Hongkyu", ""]]}, {"id": "1710.04748", "submitter": "Sebastian Risi", "authors": "Jakob Merrild, Mikkel Angaju Rasmussen and Sebastian Risi", "title": "HyperENTM: Evolving Scalable Neural Turing Machines through HyperNEAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments within memory-augmented neural networks have solved\nsequential problems requiring long-term memory, which are intractable for\ntraditional neural networks. However, current approaches still struggle to\nscale to large memory sizes and sequence lengths. In this paper we show how\naccess to memory can be encoded geometrically through a HyperNEAT-based Neural\nTuring Machine (HyperENTM). We demonstrate that using the indirect HyperNEAT\nencoding allows for training on small memory vectors in a bit-vector copy task\nand then applying the knowledge gained from such training to speed up training\non larger size memory vectors. Additionally, we demonstrate that in some\ninstances, networks trained to copy bit-vectors of size 9 can be scaled to\nsizes of 1,000 without further training. While the task in this paper is\nsimple, these results could open up the problems amendable to networks with\nexternal memories to problems with larger memory vectors and theoretically\nunbounded memory sizes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 23:41:02 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Merrild", "Jakob", ""], ["Rasmussen", "Mikkel Angaju", ""], ["Risi", "Sebastian", ""]]}, {"id": "1710.04749", "submitter": "Vijay Manikandan Janakiraman", "authors": "Vijay Manikandan Janakiraman", "title": "Explaining Aviation Safety Incidents Using Deep Temporal Multiple\n  Instance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although aviation accidents are rare, safety incidents occur more frequently\nand require a careful analysis to detect and mitigate risks in a timely manner.\nAnalyzing safety incidents using operational data and producing event-based\nexplanations is invaluable to airline companies as well as to governing\norganizations such as the Federal Aviation Administration (FAA) in the United\nStates. However, this task is challenging because of the complexity involved in\nmining multi-dimensional heterogeneous time series data, the lack of\ntime-step-wise annotation of events in a flight, and the lack of scalable tools\nto perform analysis over a large number of events. In this work, we propose a\nprecursor mining algorithm that identifies events in the multidimensional time\nseries that are correlated with the safety incident. Precursors are valuable to\nsystems health and safety monitoring and in explaining and forecasting safety\nincidents. Current methods suffer from poor scalability to high dimensional\ntime series data and are inefficient in capturing temporal behavior. We propose\nan approach by combining multiple-instance learning (MIL) and deep recurrent\nneural networks (DRNN) to take advantage of MIL's ability to learn using weakly\nsupervised data and DRNN's ability to model temporal behavior. We describe the\nalgorithm, the data, the intuition behind taking a MIL approach, and a\ncomparative analysis of the proposed algorithm with baseline models. We also\ndiscuss the application to a real-world aviation safety problem using data from\na commercial airline company and discuss the model's abilities and\nshortcomings, with some final remarks about possible deployment directions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 23:42:00 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 05:16:08 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Janakiraman", "Vijay Manikandan", ""]]}, {"id": "1710.04759", "submitter": "David Krueger", "authors": "David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre\n  Lacoste, Aaron Courville", "title": "Bayesian Hypernetworks", "comments": "David Krueger and Chin-Wei Huang contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Bayesian hypernetworks: a framework for approximate Bayesian\ninference in neural networks. A Bayesian hypernetwork $\\h$ is a neural network\nwhich learns to transform a simple noise distribution, $p(\\vec\\epsilon) =\n\\N(\\vec 0,\\mat I)$, to a distribution $q(\\pp) := q(h(\\vec\\epsilon))$ over the\nparameters $\\pp$ of another neural network (the \"primary network\")\\@. We train\n$q$ with variational inference, using an invertible $\\h$ to enable efficient\nestimation of the variational lower bound on the posterior $p(\\pp | \\D)$ via\nsampling. In contrast to most methods for Bayesian deep learning, Bayesian\nhypernets can represent a complex multimodal approximate posterior with\ncorrelations between parameters, while enabling cheap iid sampling of~$q(\\pp)$.\nIn practice, Bayesian hypernets can provide a better defense against\nadversarial examples than dropout, and also exhibit competitive performance on\na suite of tasks which evaluate model uncertainty, including regularization,\nactive learning, and anomaly detection.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 00:27:57 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 20:36:16 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Krueger", "David", ""], ["Huang", "Chin-Wei", ""], ["Islam", "Riashat", ""], ["Turner", "Ryan", ""], ["Lacoste", "Alexandre", ""], ["Courville", "Aaron", ""]]}, {"id": "1710.04805", "submitter": "Santiago Ontanon", "authors": "Santiago Onta\\~n\\'on", "title": "Combinatorial Multi-armed Bandits for Real-Time Strategy Games", "comments": null, "journal-ref": "(2017) Journal of Artificial Intelligence Research (JAIR). Volume\n  58, pp 665-702", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Games with large branching factors pose a significant challenge for game tree\nsearch algorithms. In this paper, we address this problem with a sampling\nstrategy for Monte Carlo Tree Search (MCTS) algorithms called {\\em na\\\"{i}ve\nsampling}, based on a variant of the Multi-armed Bandit problem called {\\em\nCombinatorial Multi-armed Bandits} (CMAB). We analyze the theoretical\nproperties of several variants of {\\em na\\\"{i}ve sampling}, and empirically\ncompare it against the other existing strategies in the literature for CMABs.\nWe then evaluate these strategies in the context of real-time strategy (RTS)\ngames, a genre of computer games characterized by their very large branching\nfactors. Our results show that as the branching factor grows, {\\em na\\\"{i}ve\nsampling} outperforms the other sampling strategies.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 05:08:14 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Onta\u00f1\u00f3n", "Santiago", ""]]}, {"id": "1710.04806", "submitter": "Oscar Li", "authors": "Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin", "title": "Deep Learning for Case-Based Reasoning through Prototypes: A Neural\n  Network that Explains Its Predictions", "comments": "The first two authors contributed equally, 8 pages, accepted in AAAI\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are widely used for classification. These deep models\noften suffer from a lack of interpretability -- they are particularly difficult\nto understand because of their non-linear nature. As a result, neural networks\nare often treated as \"black box\" models, and in the past, have been trained\npurely to optimize the accuracy of predictions. In this work, we create a novel\nnetwork architecture for deep learning that naturally explains its own\nreasoning for each prediction. This architecture contains an autoencoder and a\nspecial prototype layer, where each unit of that layer stores a weight vector\nthat resembles an encoded training input. The encoder of the autoencoder allows\nus to do comparisons within the latent space, while the decoder allows us to\nvisualize the learned prototypes. The training objective has four terms: an\naccuracy term, a term that encourages every prototype to be similar to at least\none encoded input, a term that encourages every encoded input to be close to at\nleast one prototype, and a term that encourages faithful reconstruction by the\nautoencoder. The distances computed in the prototype layer are used as part of\nthe classification process. Since the prototypes are learned during training,\nthe learned network naturally comes with explanations for each prediction, and\nthe explanations are loyal to what the network actually computes.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 05:12:03 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 06:43:01 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Li", "Oscar", ""], ["Liu", "Hao", ""], ["Chen", "Chaofan", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1710.04822", "submitter": "Fang Zhang", "authors": "Fang Zhang, Xiaochen Wang, Jingfei Han, Jie Tang, Shiyin Wang,\n  Marie-Francine Moens", "title": "Fast Top-k Area Topics Extraction with Knowledge Base", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What are the most popular research topics in Artificial Intelligence (AI)? We\nformulate the problem as extracting top-$k$ topics that can best represent a\ngiven area with the help of knowledge base. We theoretically prove that the\nproblem is NP-hard and propose an optimization model, FastKATE, to address this\nproblem by combining both explicit and latent representations for each topic.\nWe leverage a large-scale knowledge base (Wikipedia) to generate topic\nembeddings using neural networks and use this kind of representations to help\ncapture the representativeness of topics for given areas. We develop a fast\nheuristic algorithm to efficiently solve the problem with a provable error\nbound. We evaluate the proposed model on three real-world datasets.\nExperimental results demonstrate our model's effectiveness, robustness,\nreal-timeness (return results in $<1$s), and its superiority over several\nalternative methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 06:34:44 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 17:23:41 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Zhang", "Fang", ""], ["Wang", "Xiaochen", ""], ["Han", "Jingfei", ""], ["Tang", "Jie", ""], ["Wang", "Shiyin", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1710.04837", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal, and\n  Shaogang Gong", "title": "Recent Advances in Zero-shot Recognition", "comments": "accepted by IEEE Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent renaissance of deep convolution neural networks, encouraging\nbreakthroughs have been achieved on the supervised recognition tasks, where\neach class has sufficient training data and fully annotated training data.\nHowever, to scale the recognition to a large number of classes with few or now\ntraining samples for each class remains an unsolved problem. One approach to\nscaling up the recognition is to develop models capable of recognizing unseen\ncategories without any training instances, or zero-shot recognition/ learning.\nThis article provides a comprehensive review of existing zero-shot recognition\ntechniques covering various aspects ranging from representations of models, and\nfrom datasets and evaluation settings. We also overview related recognition\ntasks including one-shot and open set recognition which can be used as natural\nextensions of zero-shot recognition when limited number of class samples become\navailable or when zero-shot recognition is implemented in a real-world setting.\nImportantly, we highlight the limitations of existing approaches and point out\nfuture research directions in this existing new research area.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 08:29:29 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Fu", "Yanwei", ""], ["Xiang", "Tao", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""], ["Sigal", "Leonid", ""], ["Gong", "Shaogang", ""]]}, {"id": "1710.04924", "submitter": "Junpei Komiyama", "authors": "Junpei Komiyama and Hajime Shimao", "title": "Two-stage Algorithm for Fairness-aware Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic decision making process now affects many aspects of our lives.\nStandard tools for machine learning, such as classification and regression, are\nsubject to the bias in data, and thus direct application of such off-the-shelf\ntools could lead to a specific group being unfairly discriminated. Removing\nsensitive attributes of data does not solve this problem because a\n\\textit{disparate impact} can arise when non-sensitive attributes and sensitive\nattributes are correlated. Here, we study a fair machine learning algorithm\nthat avoids such a disparate impact when making a decision. Inspired by the\ntwo-stage least squares method that is widely used in the field of economics,\nwe propose a two-stage algorithm that removes bias in the training data. The\nproposed algorithm is conceptually simple. Unlike most of existing fair\nalgorithms that are designed for classification tasks, the proposed method is\nable to (i) deal with regression tasks, (ii) combine explanatory attributes to\nremove reverse discrimination, and (iii) deal with numerical sensitive\nattributes. The performance and fairness of the proposed algorithm are\nevaluated in simulations with synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 13:58:42 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Komiyama", "Junpei", ""], ["Shimao", "Hajime", ""]]}, {"id": "1710.05060", "submitter": "Nate Soares", "authors": "Eliezer Yudkowsky and Nate Soares", "title": "Functional Decision Theory: A New Theory of Instrumental Rationality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes and motivates a new decision theory known as functional\ndecision theory (FDT), as distinct from causal decision theory and evidential\ndecision theory. Functional decision theorists hold that the normative\nprinciple for action is to treat one's decision as the output of a fixed\nmathematical function that answers the question, \"Which output of this very\nfunction would yield the best outcome?\" Adhering to this principle delivers a\nnumber of benefits, including the ability to maximize wealth in an array of\ntraditional decision-theoretic and game-theoretic problems where CDT and EDT\nperform poorly. Using one simple and coherent decision rule, functional\ndecision theorists (for example) achieve more utility than CDT on Newcomb's\nproblem, more utility than EDT on the smoking lesion problem, and more utility\nthan both in Parfit's hitchhiker problem. In this paper, we define FDT, explore\nits prescriptions in a number of different decision problems, compare it to CDT\nand EDT, and give philosophical justifications for FDT as a normative theory of\ndecision-making.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 19:51:38 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 21:07:53 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Yudkowsky", "Eliezer", ""], ["Soares", "Nate", ""]]}, {"id": "1710.05096", "submitter": "Hilmar Lapp", "authors": "David Carral, Pascal Hitzler, Hilmar Lapp, Sebastian Rudolph", "title": "On the Ontological Modeling of Trees", "comments": "Proceedings of the 8th Workshop on Ontology Design and Patterns, WOP\n  2017, co-located with the 16th International Semantic Web Conference,\n  ISWC2017, Vienna, Austria, October 2017. To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trees -- i.e., the type of data structure known under this name -- are\ncentral to many aspects of knowledge organization. We investigate some central\ndesign choices concerning the ontological modeling of such trees. In\nparticular, we consider the limits of what is expressible in the Web Ontology\nLanguage, and provide a reusable ontology design pattern for trees.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 22:58:41 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Carral", "David", ""], ["Hitzler", "Pascal", ""], ["Lapp", "Hilmar", ""], ["Rudolph", "Sebastian", ""]]}, {"id": "1710.05199", "submitter": "Mohammad Mehdi Keikha", "authors": "Mohammad Mehdi Keikha, Maseud Rahgozar, Masoud Asadpour", "title": "Community Aware Random Walk for Network Embedding", "comments": "17 pages, 3 figures, 4 Tables", "journal-ref": "Knowledge-Based Systems Volume 148, 15 May 2018, Pages 47-54", "doi": "10.1016/j.knosys.2018.02.028", "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network analysis provides meaningful information about behavior of\nnetwork members that can be used for diverse applications such as\nclassification, link prediction. However, network analysis is computationally\nexpensive because of feature learning for different applications. In recent\nyears, many researches have focused on feature learning methods in social\nnetworks. Network embedding represents the network in a lower dimensional\nrepresentation space with the same properties which presents a compressed\nrepresentation of the network. In this paper, we introduce a novel algorithm\nnamed \"CARE\" for network embedding that can be used for different types of\nnetworks including weighted, directed and complex. Current methods try to\npreserve local neighborhood information of nodes, whereas the proposed method\nutilizes local neighborhood and community information of network nodes to cover\nboth local and global structure of social networks. CARE builds customized\npaths, which are consisted of local and global structure of network nodes, as a\nbasis for network embedding and uses the Skip-gram model to learn\nrepresentation vector of nodes. Subsequently, stochastic gradient descent is\napplied to optimize our objective function and learn the final representation\nof nodes. Our method can be scalable when new nodes are appended to network\nwithout information loss. Parallelize generation of customized random walks is\nalso used for speeding up CARE. We evaluate the performance of CARE on multi\nlabel classification and link prediction tasks. Experimental results on various\nnetworks indicate that the proposed method outperforms others in both Micro and\nMacro-f1 measures for different size of training data.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 15:37:07 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 15:57:36 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Keikha", "Mohammad Mehdi", ""], ["Rahgozar", "Maseud", ""], ["Asadpour", "Masoud", ""]]}, {"id": "1710.05207", "submitter": "Ivan Brugere", "authors": "Ivan Brugere and Tanya Y. Berger-Wolf", "title": "Network Model Selection Using Task-Focused Minimum Description Length", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are fundamental models for data used in practically every\napplication domain. In most instances, several implicit or explicit choices\nabout the network definition impact the translation of underlying data to a\nnetwork representation, and the subsequent question(s) about the underlying\nsystem being represented. Users of downstream network data may not even be\naware of these choices or their impacts. We propose a task-focused network\nmodel selection methodology which addresses several key challenges. Our\napproach constructs network models from underlying data and uses minimum\ndescription length (MDL) criteria for selection. Our methodology measures\nefficiency, a general and comparable measure of the network's performance of a\nlocal (i.e. node-level) predictive task of interest. Selection on efficiency\nfavors parsimonious (e.g. sparse) models to avoid overfitting and can be\napplied across arbitrary tasks and representations. We show stability,\nsensitivity, and significance testing in our methodology.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 16:27:51 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 02:26:28 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Brugere", "Ivan", ""], ["Berger-Wolf", "Tanya Y.", ""]]}, {"id": "1710.05219", "submitter": "Jian-Qiao Zhu", "authors": "Jian-Qiao Zhu, Adam N. Sanborn, Nick Chater", "title": "Mental Sampling in Multimodal Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both resources in the natural environment and concepts in a semantic space\nare distributed \"patchily\", with large gaps in between the patches. To describe\npeople's internal and external foraging behavior, various random walk models\nhave been proposed. In particular, internal foraging has been modeled as\nsampling: in order to gather relevant information for making a decision, people\ndraw samples from a mental representation using random-walk algorithms such as\nMarkov chain Monte Carlo (MCMC). However, two common empirical observations\nargue against simple sampling algorithms such as MCMC. First, the spatial\nstructure is often best described by a L\\'evy flight distribution: the\nprobability of the distance between two successive locations follows a\npower-law on the distances. Second, the temporal structure of the sampling that\nhumans and other animals produce have long-range, slowly decaying serial\ncorrelations characterized as $1/f$-like fluctuations. We propose that mental\nsampling is not done by simple MCMC, but is instead adapted to multimodal\nrepresentations and is implemented by Metropolis-coupled Markov chain Monte\nCarlo (MC$^3$), one of the first algorithms developed for sampling from\nmultimodal distributions. MC$^3$ involves running multiple Markov chains in\nparallel but with target distributions of different temperatures, and it swaps\nthe states of the chains whenever a better location is found. Heated chains\nmore readily traverse valleys in the probability landscape to propose moves to\nfar-away peaks, while the colder chains make the local steps that explore the\ncurrent peak or patch. We show that MC$^3$ generates distances between\nsuccessive samples that follow a L\\'evy flight distribution and $1/f$-like\nserial correlations, providing a single mechanistic account of these two\npuzzling empirical phenomena.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 18:17:30 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Zhu", "Jian-Qiao", ""], ["Sanborn", "Adam N.", ""], ["Chater", "Nick", ""]]}, {"id": "1710.05233", "submitter": "Jonathan Shafer", "authors": "Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, Amir Yehudayoff", "title": "Learners that Use Little Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study learning algorithms that are restricted to using a small amount of\ninformation from their input sample. We introduce a category of learning\nalgorithms we term $d$-bit information learners, which are algorithms whose\noutput conveys at most $d$ bits of information of their input. A central theme\nin this work is that such algorithms generalize.\n  We focus on the learning capacity of these algorithms, and prove sample\ncomplexity bounds with tight dependencies on the confidence and error\nparameters. We also observe connections with well studied notions such as\nsample compression schemes, Occam's razor, PAC-Bayes and differential privacy.\n  We discuss an approach that allows us to prove upper bounds on the amount of\ninformation that algorithms reveal about their inputs, and also provide a lower\nbound by showing a simple concept class for which every (possibly randomized)\nempirical risk minimizer must reveal a lot of information. On the other hand,\nwe show that in the distribution-dependent setting every VC class has empirical\nrisk minimizers that do not reveal a lot of information.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 20:40:46 GMT"}, {"version": "v2", "created": "Sun, 24 Dec 2017 10:51:35 GMT"}, {"version": "v3", "created": "Wed, 28 Feb 2018 03:14:32 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Bassily", "Raef", ""], ["Moran", "Shay", ""], ["Nachum", "Ido", ""], ["Shafer", "Jonathan", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "1710.05247", "submitter": "Aditya Aniruddha Shrotri", "authors": "Kuldeep S. Meel (1), Aditya A. Shrotri (2), Moshe Y. Vardi (2) ((1)\n  National University of Singapore, (2) Rice University)", "title": "On Hashing-Based Approaches to Approximate DNF-Counting", "comments": "Full version of paper accepted to FSTTCS 2017. 12 pages +\n  Acknowledgements + References + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Propositional model counting is a fundamental problem in artificial\nintelligence with a wide variety of applications, such as probabilistic\ninference, decision making under uncertainty, and probabilistic databases.\nConsequently, the problem is of theoretical as well as practical interest. When\nthe constraints are expressed as DNF formulas, Monte Carlo-based techniques\nhave been shown to provide a fully polynomial randomized approximation scheme\n(FPRAS). For CNF constraints, hashing-based approximation techniques have been\ndemonstrated to be highly successful. Furthermore, it was shown that\nhashing-based techniques also yield an FPRAS for DNF counting without usage of\nMonte Carlo sampling. Our analysis, however, shows that the proposed\nhashing-based approach to DNF counting provides poor time complexity compared\nto the Monte Carlo-based DNF counting techniques. Given the success of\nhashing-based techniques for CNF constraints, it is natural to ask: Can\nhashing-based techniques provide an efficient FPRAS for DNF counting? In this\npaper, we provide a positive answer to this question. To this end, we introduce\ntwo novel algorithmic techniques: \\emph{Symbolic Hashing} and \\emph{Stochastic\nCell Counting}, along with a new hash family of \\emph{Row-Echelon hash\nfunctions}. These innovations allow us to design a hashing-based FPRAS for DNF\ncounting of similar complexity (up to polylog factors) as that of prior works.\nFurthermore, we expect these techniques to have potential applications beyond\nDNF counting.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 23:22:09 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Meel", "Kuldeep S.", ""], ["Shrotri", "Aditya A.", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1710.05257", "submitter": "Tong Wang", "authors": "Tong Wang", "title": "Multi-Value Rule Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present the Multi-vAlue Rule Set (MARS) model for interpretable\nclassification with feature efficient presentations. MARS introduces a more\ngeneralized form of association rules that allows multiple values in a\ncondition. Rules of this form are more concise than traditional single-valued\nrules in capturing and describing patterns in data. MARS mitigates the problem\nof dealing with continuous features and high-cardinality categorical features\nfaced by rule-based models. Our formulation also pursues a higher efficiency of\nfeature utilization, which reduces the cognitive load to understand the\ndecision process. We propose an efficient inference method for learning a\nmaximum a posteriori model, incorporating theoretically grounded bounds to\niteratively reduce the search space to improve search efficiency. Experiments\nwith synthetic and real-world data demonstrate that MARS models have\nsignificantly smaller complexity and fewer features, providing better\ninterpretability while being competitive in predictive accuracy. We conducted a\nusability study with human subjects and results show that MARS is the easiest\nto use compared with other competing rule-based models, in terms of the correct\nrate and response time. Overall, MARS introduces a new approach to rule-based\nmodels that balance accuracy and interpretability with feature-efficient\nrepresentations.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 01:22:56 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Wang", "Tong", ""]]}, {"id": "1710.05268", "submitter": "Frederik Ebert", "authors": "Frederik Ebert, Chelsea Finn, Alex X. Lee, Sergey Levine", "title": "Self-Supervised Visual Planning with Temporal Skip Connections", "comments": "accepted at the Conference on Robot Learning (CoRL) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to autonomously learn wide repertoires of complex skills, robots\nmust be able to learn from their own autonomously collected data, without human\nsupervision. One learning signal that is always available for autonomously\ncollected data is prediction: if a robot can learn to predict the future, it\ncan use this predictive model to take actions to produce desired outcomes, such\nas moving an object to a particular location. However, in complex open-world\nscenarios, designing a representation for prediction is difficult. In this\nwork, we instead aim to enable self-supervised robotic learning through direct\nvideo prediction: instead of attempting to design a good representation, we\ndirectly predict what the robot will see next, and then use this model to\nachieve desired goals. A key challenge in video prediction for robotic\nmanipulation is handling complex spatial arrangements such as occlusions. To\nthat end, we introduce a video prediction model that can keep track of objects\nthrough occlusion by incorporating temporal skip-connections. Together with a\nnovel planning criterion and action space formulation, we demonstrate that this\nmodel substantially outperforms prior work on video prediction-based control.\nOur results show manipulation of objects not seen during training, handling\nmultiple objects, and pushing objects around obstructions. These results\nrepresent a significant advance in the range and complexity of skills that can\nbe performed entirely with self-supervised robotic learning.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 02:58:20 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Ebert", "Frederik", ""], ["Finn", "Chelsea", ""], ["Lee", "Alex X.", ""], ["Levine", "Sergey", ""]]}, {"id": "1710.05270", "submitter": "Wei Ping", "authors": "Wei Ping, Qiang Liu, Alexander Ihler", "title": "Learning Infinite RBMs with Frank-Wolfe", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an infinite restricted Boltzmann machine~(RBM),\nwhose maximum likelihood estimation~(MLE) corresponds to a constrained convex\noptimization. We consider the Frank-Wolfe algorithm to solve the program, which\nprovides a sparse solution that can be interpreted as inserting a hidden unit\nat each iteration, so that the optimization process takes the form of a\nsequence of finite models of increasing complexity. As a side benefit, this can\nbe used to easily and efficiently identify an appropriate number of hidden\nunits during the optimization. The resulting model can also be used as an\ninitialization for typical state-of-the-art RBM training algorithms such as\ncontrastive divergence, leading to models with consistently higher test\nlikelihood than random initialization.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 03:38:32 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Ping", "Wei", ""], ["Liu", "Qiang", ""], ["Ihler", "Alexander", ""]]}, {"id": "1710.05341", "submitter": "Anthony Young", "authors": "Anthony P. Young", "title": "The Complete Extensions do not form a Complete Semilattice", "comments": "10 pages, 2 figures, 11 references [Update 26/10/2017] This note\n  contains an error that invalidates its title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In his seminal paper that inaugurated abstract argumentation, Dung proved\nthat the set of complete extensions forms a complete semilattice with respect\nto set inclusion. In this note we demonstrate that this proof is incorrect with\ncounterexamples. We then trace the error in the proof and explain why it arose.\nWe then examine the implications for the grounded extension.\n  [Reason for withdrawal continued] Page 4, Example 2 is not a counterexample\nto Dung 1995 Theorem 25(3). It was believed to be a counter-example because the\nauthor misunderstood ``glb'' to be set-theoretic intersection. But in this\ncase, ``glb'' is defined to be other than set-theoretic intersection such that\nTheorem 25(3) is true.\n  The author was motivated to fully understand the lattice-theoretic claims of\nDung 1995 in writing this note and was not aware that this issue is probably\nfolklore; the author bears full responsibility for this error.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 14:49:40 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 04:55:17 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Young", "Anthony P.", ""]]}, {"id": "1710.05381", "submitter": "Mateusz Buda", "authors": "Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski", "title": "A systematic study of the class imbalance problem in convolutional\n  neural networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2018.07.011", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we systematically investigate the impact of class imbalance on\nclassification performance of convolutional neural networks (CNNs) and compare\nfrequently used methods to address the issue. Class imbalance is a common\nproblem that has been comprehensively studied in classical machine learning,\nyet very limited systematic research is available in the context of deep\nlearning. In our study, we use three benchmark datasets of increasing\ncomplexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of\nimbalance on classification and perform an extensive comparison of several\nmethods to address the issue: oversampling, undersampling, two-phase training,\nand thresholding that compensates for prior class probabilities. Our main\nevaluation metric is area under the receiver operating characteristic curve\n(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is\nassociated with notable difficulties in the context of imbalanced data. Based\non results from our experiments we conclude that (i) the effect of class\nimbalance on classification performance is detrimental; (ii) the method of\naddressing class imbalance that emerged as dominant in almost all analyzed\nscenarios was oversampling; (iii) oversampling should be applied to the level\nthat completely eliminates the imbalance, whereas the optimal undersampling\nratio depends on the extent of imbalance; (iv) as opposed to some classical\nmachine learning models, oversampling does not cause overfitting of CNNs; (v)\nthresholding should be applied to compensate for prior class probabilities when\noverall number of properly classified cases is of interest.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 19:01:43 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 02:02:17 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Buda", "Mateusz", ""], ["Maki", "Atsuto", ""], ["Mazurowski", "Maciej A.", ""]]}, {"id": "1710.05387", "submitter": "Xinyan Yan", "authors": "Xinyan Yan, Krzysztof Choromanski, Byron Boots, Vikas Sindhwani", "title": "Manifold Regularization for Kernelized LSTD", "comments": "6 pages, CoRL 2017 non-archival track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy evaluation or value function or Q-function approximation is a key\nprocedure in reinforcement learning (RL). It is a necessary component of policy\niteration and can be used for variance reduction in policy gradient methods.\nTherefore its quality has a significant impact on most RL algorithms. Motivated\nby manifold regularized learning, we propose a novel kernelized policy\nevaluation method that takes advantage of the intrinsic geometry of the state\nspace learned from data, in order to achieve better sample efficiency and\nhigher accuracy in Q-function approximation. Applying the proposed method in\nthe Least-Squares Policy Iteration (LSPI) framework, we observe superior\nperformance compared to widely used parametric basis functions on two standard\nbenchmarks in terms of policy quality.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 19:59:13 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Yan", "Xinyan", ""], ["Choromanski", "Krzysztof", ""], ["Boots", "Byron", ""], ["Sindhwani", "Vikas", ""]]}, {"id": "1710.05426", "submitter": "Tong Wang", "authors": "Tong Wang and Cynthia Rudin", "title": "Causal Rule Sets for Identifying Subgroups with Enhanced Treatment\n  Effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key question in causal inference analyses is how to find subgroups with\nelevated treatment effects. This paper takes a machine learning approach and\nintroduces a generative model, Causal Rule Sets (CRS), for interpretable\nsubgroup discovery. A CRS model uses a small set of short decision rules to\ncapture a subgroup where the average treatment effect is elevated. We present a\nBayesian framework for learning a causal rule set. The Bayesian model consists\nof a prior that favors simple models for better interpretability as well as\navoiding overfitting, and a Bayesian logistic regression that captures the\nlikelihood of data, characterizing the relation between outcomes, attributes,\nand subgroup membership. The Bayesian model has tunable parameters that can\ncharacterize subgroups with various sizes, providing users with more flexible\nchoices of models from the \\emph{treatment efficient frontier}. We find maximum\na posteriori models using iterative discrete Monte Carlo steps in the joint\nsolution space of rules sets and parameters. To improve search efficiency, we\nprovide theoretically grounded heuristics and bounding strategies to prune and\nconfine the search space. Experiments show that the search algorithm can\nefficiently recover true underlying subgroups. We apply CRS on public and\nreal-world datasets from domains where interpretability is indispensable. We\ncompare CRS with state-of-the-art rule-based subgroup discovery models. Results\nshow that CRS achieved consistently competitive performance on datasets from\nvarious domains, represented by high treatment efficient frontiers.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 00:30:43 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 13:43:28 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 04:30:54 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wang", "Tong", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1710.05465", "submitter": "Cathy Wu", "authors": "Cathy Wu, Aboudy Kreidieh, Kanaad Parvate, Eugene Vinitsky, Alexandre\n  M Bayen", "title": "Flow: A Modular Learning Framework for Autonomy in Traffic", "comments": "14 pages, 8 figures; new experiments and analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of autonomous vehicles (AVs) holds vast potential for\ntransportation systems through improved safety, efficiency, and access to\nmobility. However, due to numerous technical, political, and human factors\nchallenges, new methodologies are needed to design vehicles and transportation\nsystems for these positive outcomes. This article tackles technical challenges\narising from the partial adoption of autonomy: partial control, partial\nobservation, complex multi-vehicle interactions, and the sheer variety of\ntraffic settings represented by real-world networks. The article presents a\nmodular learning framework which leverages deep Reinforcement Learning methods\nto address complex traffic dynamics. Modules are composed to capture common\ntraffic phenomena (traffic jams, lane changing, intersections). Learned control\nlaws are found to exceed human driving performance by at least 40% with only\n5-10% adoption of AVs. In partially-observed single-lane traffic, a small\nneural network control law can eliminate stop-and-go traffic -- surpassing all\nknown model-based controllers, achieving near-optimal performance, and\ngeneralizing to out-of-distribution traffic densities.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 01:51:51 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 04:17:06 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 16:47:01 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wu", "Cathy", ""], ["Kreidieh", "Aboudy", ""], ["Parvate", "Kanaad", ""], ["Vinitsky", "Eugene", ""], ["Bayen", "Alexandre M", ""]]}, {"id": "1710.05468", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio", "title": "Generalization in Deep Learning", "comments": "To appear in Mathematics of Deep Learning, Cambridge University\n  Press. All previous results remain unchanged", "journal-ref": null, "doi": null, "report-no": "Massachusetts Institute of Technology (MIT), MIT-CSAIL-TR-2018-014", "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides theoretical insights into why and how deep learning can\ngeneralize well, despite its large capacity, complexity, possible algorithmic\ninstability, nonrobustness, and sharp minima, responding to an open question in\nthe literature. We also discuss approaches to provide non-vacuous\ngeneralization guarantees for deep learning. Based on theoretical observations,\nwe propose new open problems and discuss the limitations of our results.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 02:21:24 GMT"}, {"version": "v2", "created": "Sun, 24 Dec 2017 19:44:43 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 23:39:50 GMT"}, {"version": "v4", "created": "Tue, 1 Jan 2019 00:07:45 GMT"}, {"version": "v5", "created": "Fri, 10 May 2019 18:41:13 GMT"}, {"version": "v6", "created": "Mon, 27 Jul 2020 23:01:04 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Kaelbling", "Leslie Pack", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1710.05503", "submitter": "Anoop Aroor", "authors": "Anoop Aroor and Susan L. Epstein", "title": "Toward Crowd-Sensitive Path Planning", "comments": "Accepted at AAAI fall symposium 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If a robot can predict crowds in parts of its environment that are\ninaccessible to its sensors, then it can plan to avoid them. This paper\nproposes a fast, online algorithm that learns average crowd densities in\ndifferent areas. It also describes how these densities can be incorporated into\nexisting navigation architectures. In simulation across multiple challenging\ncrowd scenarios, the robot reaches its target faster, travels less, and risks\nfewer collisions than if it were to plan with the traditional A* algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 04:32:45 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Aroor", "Anoop", ""], ["Epstein", "Susan L.", ""]]}, {"id": "1710.05627", "submitter": "Wei Gao", "authors": "Wei Gao and David Hsu and Wee Sun Lee and Shengmei Shen and Karthikk\n  Subramanian", "title": "Intention-Net: Integrating Planning and Deep Learning for Goal-Directed\n  Autonomous Navigation", "comments": "Published in 1st Annual Conference on Robot Learning (CoRL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can a delivery robot navigate reliably to a destination in a new office\nbuilding, with minimal prior information? To tackle this challenge, this paper\nintroduces a two-level hierarchical approach, which integrates model-free deep\nlearning and model-based path planning. At the low level, a neural-network\nmotion controller, called the intention-net, is trained end-to-end to provide\nrobust local navigation. The intention-net maps images from a single monocular\ncamera and \"intentions\" directly to robot controls. At the high level, a path\nplanner uses a crude map, e.g., a 2-D floor plan, to compute a path from the\nrobot's current location to the goal. The planned path provides intentions to\nthe intention-net. Preliminary experiments suggest that the learned motion\ncontroller is robust against perceptual uncertainty and by integrating with a\npath planner, it generalizes effectively to new environments and goals.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 11:22:32 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 02:24:06 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Gao", "Wei", ""], ["Hsu", "David", ""], ["Lee", "Wee Sun", ""], ["Shen", "Shengmei", ""], ["Subramanian", "Karthikk", ""]]}, {"id": "1710.05693", "submitter": "David Chapela-Campa", "authors": "David Chapela-Campa, Manuel Mucientes, Manuel Lama", "title": "Mining Frequent Patterns in Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining has emerged as a way to analyze the behavior of an\norganization by extracting knowledge from event logs and by offering techniques\nto discover, monitor and enhance real processes. In the discovery of process\nmodels, retrieving a complex one, i.e., a hardly readable process model, can\nhinder the extraction of information. Even in well-structured process models,\nthere is information that cannot be obtained with the current techniques. In\nthis paper, we present WoMine, an algorithm to retrieve frequent behavioural\npatterns from the model. Our approach searches in process models extracting\nstructures with sequences, selections, parallels and loops, which are\nfrequently executed in the logs. This proposal has been validated with a set of\nprocess models, including some from BPI Challenges, and compared with the state\nof the art techniques. Experiments have validated that WoMine can find all\ntypes of patterns, extracting information that cannot be mined with the state\nof the art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 18:33:19 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Chapela-Campa", "David", ""], ["Mucientes", "Manuel", ""], ["Lama", "Manuel", ""]]}, {"id": "1710.05703", "submitter": "Noman Islam Dr.", "authors": "Noman Islam, Zeeshan Islam, Nazia Noor", "title": "A Survey on Optical Character Recognition System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Character Recognition (OCR) has been a topic of interest for many\nyears. It is defined as the process of digitizing a document image into its\nconstituent characters. Despite decades of intense research, developing OCR\nwith capabilities comparable to that of human still remains an open challenge.\nDue to this challenging nature, researchers from industry and academic circles\nhave directed their attentions towards Optical Character Recognition. Over the\nlast few years, the number of academic laboratories and companies involved in\nresearch on Character Recognition has increased dramatically. This research\naims at summarizing the research so far done in the field of OCR. It provides\nan overview of different aspects of OCR and discusses corresponding proposals\naimed at resolving issues of OCR.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 15:08:49 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Islam", "Noman", ""], ["Islam", "Zeeshan", ""], ["Noor", "Nazia", ""]]}, {"id": "1710.05720", "submitter": "EPTCS", "authors": "Simon Rehwald, Amjad Ibrahim, Kristian Beckers, Alexander Pretschner", "title": "ACCBench: A Framework for Comparing Causality Algorithms", "comments": "In Proceedings CREST 2017, arXiv:1710.02770", "journal-ref": "EPTCS 259, 2017, pp. 16-30", "doi": "10.4204/EPTCS.259.2", "report-no": null, "categories": "cs.AI cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern socio-technical systems are increasingly complex. A fundamental\nproblem is that the borders of such systems are often not well-defined\na-priori, which among other problems can lead to unwanted behavior during\nruntime. Ideally, unwanted behavior should be prevented. If this is not\npossible the system shall at least be able to help determine potential cause(s)\na-posterori, identify responsible parties and make them accountable for their\nbehavior. Recently, several algorithms addressing these concepts have been\nproposed. However, the applicability of the corresponding approaches,\nspecifically their effectiveness and performance, is mostly unknown. Therefore,\nin this paper, we propose ACCBench, a benchmark tool that allows to compare and\nevaluate causality algorithms under a consistent setting. Furthermore, we\ncontribute an implementation of the two causality algorithms by G\\\"o{\\ss}ler\nand Metayer and G\\\"o{\\ss}ler and Astefanoaei as well as of a policy compliance\napproach based on some concepts of Main et al. Lastly, we conduct a case study\nof an Intelligent Door Control System, which exposes concrete strengths and\nweaknesses of all algorithms under different aspects. In the course of this, we\nshow that the effectiveness of the algorithms in terms of cause detection as\nwell as their performance differ to some extent. In addition, our analysis\nreports on some qualitative aspects that should be considered when evaluating\neach algorithm. For example, the human effort needed to configure the algorithm\nand model the use case is analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 05:20:33 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Rehwald", "Simon", ""], ["Ibrahim", "Amjad", ""], ["Beckers", "Kristian", ""], ["Pretschner", "Alexander", ""]]}, {"id": "1710.05733", "submitter": "Sobhan Moosavi", "authors": "Sobhan Moosavi, Behrooz Omidvar-Tehrani, R. Bruce Craig, Arnab Nandi,\n  Rajiv Ramnath", "title": "Characterizing Driving Context from Driver Behavior", "comments": "Accepted to be published at The 25th ACM SIGSPATIAL International\n  Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL\n  2017)", "journal-ref": null, "doi": "10.1145/3139958.3139992", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the increasing availability of spatiotemporal data, a variety of\ndata-analytic applications have become possible. Characterizing driving\ncontext, where context may be thought of as a combination of location and time,\nis a new challenging application. An example of such a characterization is\nfinding the correlation between driving behavior and traffic conditions. This\ncontextual information enables analysts to validate observation-based\nhypotheses about the driving of an individual. In this paper, we present\nDriveContext, a novel framework to find the characteristics of a context, by\nextracting significant driving patterns (e.g., a slow-down), and then\nidentifying the set of potential causes behind patterns (e.g., traffic\ncongestion). Our experimental results confirm the feasibility of the framework\nin identifying meaningful driving patterns, with improvements in comparison\nwith the state-of-the-art. We also demonstrate how the framework derives\ninteresting characteristics for different contexts, through real-world\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 17:34:11 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 23:42:05 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Moosavi", "Sobhan", ""], ["Omidvar-Tehrani", "Behrooz", ""], ["Craig", "R. Bruce", ""], ["Nandi", "Arnab", ""], ["Ramnath", "Rajiv", ""]]}, {"id": "1710.05780", "submitter": "Gerasimos Spanakis", "authors": "Alexander Bartl, Gerasimos Spanakis", "title": "A retrieval-based dialogue system utilizing utterance and context\n  embeddings", "comments": "A shorter version is accepted at ICMLA2017 conference;\n  acknowledgement added; typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding semantically rich and computer-understandable representations for\ntextual dialogues, utterances and words is crucial for dialogue systems (or\nconversational agents), as their performance mostly depends on understanding\nthe context of conversations. Recent research aims at finding distributed\nvector representations (embeddings) for words, such that semantically similar\nwords are relatively close within the vector-space. Encoding the \"meaning\" of\ntext into vectors is a current trend, and text can range from words, phrases\nand documents to actual human-to-human conversations. In recent research\napproaches, responses have been generated utilizing a decoder architecture,\ngiven the vector representation of the current conversation. In this paper, the\nutilization of embeddings for answer retrieval is explored by using\nLocality-Sensitive Hashing Forest (LSH Forest), an Approximate Nearest Neighbor\n(ANN) model, to find similar conversations in a corpus and rank possible\ncandidates. Experimental results on the well-known Ubuntu Corpus (in English)\nand a customer service chat dataset (in Dutch) show that, in combination with a\ncandidate selection method, retrieval-based approaches outperform generative\nones and reveal promising future research directions towards the usability of\nsuch a system.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 15:23:56 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 07:32:32 GMT"}, {"version": "v3", "created": "Fri, 20 Oct 2017 10:16:43 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Bartl", "Alexander", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "1710.05944", "submitter": "Godwin Ambukege", "authors": "Godwin Ambukege, Godfrey Justo and Joseph Mushi", "title": "Neuro Fuzzy Modelling for Prediction of Consumer Price Index", "comments": null, "journal-ref": "International Journal of Artificial Intelligence and Applications\n  (IJAIA), Vol.8, No.5, September 2017", "doi": "10.5121/ijaia.2017.8503", "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Economic indicators such as Consumer Price Index (CPI) have frequently used\nin predicting future economic wealth for financial policy makers of respective\ncountry. Most central banks, on guidelines of research studies, have recently\nadopted an inflation targeting monetary policy regime, which accounts for high\nrequirement for effective prediction model of consumer price index. However,\nprediction accuracy by numerous studies is still low, which raises a need for\nimprovement. This manuscript presents findings of study that use neuro fuzzy\ntechnique to design a machine-learning model that train and test data to\npredict a univariate time series CPI. The study establishes a matrix of monthly\nCPI data from secondary data source of Tanzania National Bureau of Statistics\nfrom January 2000 to December 2015 as case study and thereafter conducted\nsimulation experiments on MATLAB whereby ninety five percent (95%) of data used\nto train the model and five percent (5%) for testing. Furthermore, the study\nuse root mean square error (RMSE) and mean absolute percentage error (MAPE) as\nerror metrics for model evaluation. The results show that the neuro fuzzy model\nhave an architecture of 5:74:1 with Gaussian membership functions (2, 2, 2, 2,\n2), provides RMSE of 0.44886 and MAPE 0.23384, which is far better compared to\nexisting research studies.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 07:44:00 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Ambukege", "Godwin", ""], ["Justo", "Godfrey", ""], ["Mushi", "Joseph", ""]]}, {"id": "1710.05958", "submitter": "Sayna Ebrahimi", "authors": "Sayna Ebrahimi, Anna Rohrbach, Trevor Darrell", "title": "Gradient-free Policy Architecture Search and Adaptation", "comments": "Accepted in Conference on Robot Learning, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for policy architecture search and adaptation via\ngradient-free optimization which can learn to perform autonomous driving tasks.\nBy learning from both demonstration and environmental reward we develop a model\nthat can learn with relatively few early catastrophic failures. We first learn\nan architecture of appropriate complexity to perceive aspects of world state\nrelevant to the expert demonstration, and then mitigate the effect of\ndomain-shift during deployment by adapting a policy demonstrated in a source\ndomain to rewards obtained in a target environment. We show that our approach\nallows safer learning than baseline methods, offering a reduced cumulative\ncrash metric over the agent's lifetime as it learns to drive in a realistic\nsimulated environment.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 18:47:35 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Ebrahimi", "Sayna", ""], ["Rohrbach", "Anna", ""], ["Darrell", "Trevor", ""]]}, {"id": "1710.05980", "submitter": "Meng Wang", "authors": "Fang Gong, Meng Wang, Haofen Wang, Sen Wang, Mengyue Liu", "title": "SMR: Medical Knowledge Graph Embedding for Safe Medicine Recommendation", "comments": "8 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing medicine recommendation systems that are mainly based on\nelectronic medical records (EMRs) are significantly assisting doctors to make\nbetter clinical decisions benefiting both patients and caregivers. Even though\nthe growth of EMRs is at a lighting fast speed in the era of big data, content\nlimitations in EMRs restrain the existed recommendation systems to reflect\nrelevant medical facts, such as drug-drug interactions. Many medical knowledge\ngraphs that contain drug-related information, such as DrugBank, may give hope\nfor the recommendation systems. However, the direct use of these knowledge\ngraphs in the systems suffers from robustness caused by the incompleteness of\nthe graphs. To address these challenges, we stand on recent advances in graph\nembedding learning techniques and propose a novel framework, called Safe\nMedicine Recommendation (SMR), in this paper. Specifically, SMR first\nconstructs a high-quality heterogeneous graph by bridging EMRs (MIMIC-III) and\nmedical knowledge graphs (ICD-9 ontology and DrugBank). Then, SMR jointly\nembeds diseases, medicines, patients, and their corresponding relations into a\nshared lower dimensional space. Finally, SMR uses the embeddings to decompose\nthe medicine recommendation into a link prediction process while considering\nthe patient's diagnoses and adverse drug reactions. To our best knowledge, SMR\nis the first to learn embeddings of a patient-disease-medicine graph for\nmedicine recommendation in the world. Extensive experiments on real datasets\nare conducted to evaluate the effectiveness of proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 20:06:13 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 08:02:44 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 05:51:23 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Gong", "Fang", ""], ["Wang", "Meng", ""], ["Wang", "Haofen", ""], ["Wang", "Sen", ""], ["Liu", "Mengyue", ""]]}, {"id": "1710.06055", "submitter": "Tim Taylor", "authors": "Tim Taylor", "title": "Evolution in Virtual Worlds", "comments": "Author's final preprint", "journal-ref": "Chapter 32 of \"The Oxford Handbook of Virtuality\", Mark Grimshaw\n  (ed.), Oxford University Press, 2014. (ISBN 9780199826162)", "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter discusses the possibility of instilling a virtual world with\nmechanisms for evolution and natural selection in order to generate rich\necosystems of complex organisms in a process akin to biological evolution. Some\nprevious work in the area is described, and successes and failures are\ndiscussed. The components of a more comprehensive framework for designing such\nworlds are mapped out, including the design of the individual organisms, the\nproperties and dynamics of the environmental medium in which they are evolving,\nand the representational relationship between organism and environment. Some of\nthe key issues discussed include how to allow organisms to evolve new\nstructures and functions with few restrictions, and how to create an\ninterconnectedness between organisms in order to generate drives for continuing\nevolutionary activity.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 02:13:53 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Taylor", "Tim", ""]]}, {"id": "1710.06061", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Bhaskar Mitra, Matteo Venanzi, Roy Rosemarin,\n  Grzegorz Kukla, Piotr Grudzien, Nicola Cancedda", "title": "Reply With: Proactive Recommendation of Email Attachments", "comments": "CIKM2017. Proceedings of the 26th ACM International Conference on\n  Information and Knowledge Management. 2017", "journal-ref": null, "doi": "10.1145/3132847.3132979", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Email responses often contain items-such as a file or a hyperlink to an\nexternal document-that are attached to or included inline in the body of the\nmessage. Analysis of an enterprise email corpus reveals that 35% of the time\nwhen users include these items as part of their response, the attachable item\nis already present in their inbox or sent folder. A modern email client can\nproactively retrieve relevant attachable items from the user's past emails\nbased on the context of the current conversation, and recommend them for\ninclusion, to reduce the time and effort involved in composing the response. In\nthis paper, we propose a weakly supervised learning framework for recommending\nattachable items to the user. As email search systems are commonly available,\nwe constrain the recommendation task to formulating effective search queries\nfrom the context of the conversations. The query is submitted to an existing IR\nsystem to retrieve relevant items for attachment. We also present a novel\nstrategy for generating labels from an email corpus---without the need for\nmanual annotations---that can be used to train and evaluate the query\nformulation model. In addition, we describe a deep convolutional neural network\nthat demonstrates satisfactory performance on this query formulation task when\nevaluated on the publicly available Avocado dataset and a proprietary dataset\nof internal emails obtained through an employee participation program.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 02:37:18 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 10:29:50 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Van Gysel", "Christophe", ""], ["Mitra", "Bhaskar", ""], ["Venanzi", "Matteo", ""], ["Rosemarin", "Roy", ""], ["Kukla", "Grzegorz", ""], ["Grudzien", "Piotr", ""], ["Cancedda", "Nicola", ""]]}, {"id": "1710.06071", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt, Ji Young Lee", "title": "PubMed 200k RCT: a Dataset for Sequential Sentence Classification in\n  Medical Abstracts", "comments": "Accepted as a conference paper at IJCNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PubMed 200k RCT, a new dataset based on PubMed for sequential\nsentence classification. The dataset consists of approximately 200,000\nabstracts of randomized controlled trials, totaling 2.3 million sentences. Each\nsentence of each abstract is labeled with their role in the abstract using one\nof the following classes: background, objective, method, result, or conclusion.\nThe purpose of releasing this dataset is twofold. First, the majority of\ndatasets for sequential short-text classification (i.e., classification of\nshort texts that appear in sequences) are small: we hope that releasing a new\nlarge dataset will help develop more accurate algorithms for this task. Second,\nfrom an application perspective, researchers need better tools to efficiently\nskim through the literature. Automatically classifying each sentence in an\nabstract would help researchers read abstracts more efficiently, especially in\nfields where abstracts may be long, such as the medical field.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 03:22:00 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Dernoncourt", "Franck", ""], ["Lee", "Ji Young", ""]]}, {"id": "1710.06096", "submitter": "Ricky Fok", "authors": "Ricky Fok, Aijun An, and Xiaogang Wang", "title": "Spontaneous Symmetry Breaking in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework to understand the unprecedented performance and\nrobustness of deep neural networks using field theory. Correlations between the\nweights within the same layer can be described by symmetries in that layer, and\nnetworks generalize better if such symmetries are broken to reduce the\nredundancies of the weights. Using a two parameter field theory, we find that\nthe network can break such symmetries itself towards the end of training in a\nprocess commonly known in physics as spontaneous symmetry breaking. This\ncorresponds to a network generalizing itself without any user input layers to\nbreak the symmetry, but by communication with adjacent layers. In the layer\ndecoupling limit applicable to residual networks (He et al., 2015), we show\nthat the remnant symmetries that survive the non-linear layers are\nspontaneously broken. The Lagrangian for the non-linear and weight layers\ntogether has striking similarities with the one in quantum field theory of a\nscalar. Using results from quantum field theory we show that our framework is\nable to explain many experimentally observed phenomena,such as training on\nrandom labels with zero error (Zhang et al., 2017), the information bottleneck,\nthe phase transition out of it and gradient variance explosion (Shwartz-Ziv &\nTishby, 2017), shattered gradients (Balduzzi et al., 2017), and many more.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 04:55:14 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Fok", "Ricky", ""], ["An", "Aijun", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1710.06117", "submitter": "Ayaka Kume", "authors": "Ayaka Kume, Eiichi Matsumoto, Kuniyuki Takahashi, Wilson Ko and Jethro\n  Tan", "title": "Map-based Multi-Policy Reinforcement Learning: Enhancing Adaptability of\n  Robots by Deep Reinforcement Learning", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for robots to perform mission-critical tasks, it is essential that\nthey are able to quickly adapt to changes in their environment as well as to\ninjuries and or other bodily changes. Deep reinforcement learning has been\nshown to be successful in training robot control policies for operation in\ncomplex environments. However, existing methods typically employ only a single\npolicy. This can limit the adaptability since a large environmental\nmodification might require a completely different behavior compared to the\nlearning environment. To solve this problem, we propose Map-based Multi-Policy\nReinforcement Learning (MMPRL), which aims to search and store multiple\npolicies that encode different behavioral features while maximizing the\nexpected reward in advance of the environment change. Thanks to these policies,\nwhich are stored into a multi-dimensional discrete map according to its\nbehavioral feature, adaptation can be performed within reasonable time without\nretraining the robot. An appropriate pre-trained policy from the map can be\nrecalled using Bayesian optimization. Our experiments show that MMPRL enables\nrobots to quickly adapt to large changes without requiring any prior knowledge\non the type of injuries that could occur. A highlight of the learned behaviors\ncan be found here: https://youtu.be/QwInbilXNOE .\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 06:26:44 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 04:54:31 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Kume", "Ayaka", ""], ["Matsumoto", "Eiichi", ""], ["Takahashi", "Kuniyuki", ""], ["Ko", "Wilson", ""], ["Tan", "Jethro", ""]]}, {"id": "1710.06169", "submitter": "Sarah Tan", "authors": "Sarah Tan, Rich Caruana, Giles Hooker, Yin Lou", "title": "Distill-and-Compare: Auditing Black-Box Models Using Transparent Model\n  Distillation", "comments": "Camera-ready version for AAAI/ACM AIES 2018. Data and pseudocode at\n  https://github.com/shftan/auditblackbox. Previously titled \"Detecting Bias in\n  Black-Box Models Using Transparent Model Distillation\". A short version was\n  presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": "10.1145/3278721.3278725", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box risk scoring models permeate our lives, yet are typically\nproprietary or opaque. We propose Distill-and-Compare, a model distillation and\ncomparison approach to audit such models. To gain insight into black-box\nmodels, we treat them as teachers, training transparent student models to mimic\nthe risk scores assigned by black-box models. We compare the student model\ntrained with distillation to a second un-distilled transparent model trained on\nground-truth outcomes, and use differences between the two models to gain\ninsight into the black-box model. Our approach can be applied in a realistic\nsetting, without probing the black-box model API. We demonstrate the approach\non four public data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending\nClub. We also propose a statistical test to determine if a data set is missing\nkey features used to train the black-box model. Our test finds that the\nProPublica data is likely missing key feature(s) used in COMPAS.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 08:58:59 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 07:54:17 GMT"}, {"version": "v3", "created": "Sat, 24 Feb 2018 05:25:51 GMT"}, {"version": "v4", "created": "Thu, 11 Oct 2018 07:33:54 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Tan", "Sarah", ""], ["Caruana", "Rich", ""], ["Hooker", "Giles", ""], ["Lou", "Yin", ""]]}, {"id": "1710.06331", "submitter": "Wiktor Daszczuk", "authors": "Wiktor B. Daszczuk, Jerzy Mie\\'scicki, Waldemar Grabski", "title": "Distributed algorithm for empty vehicles management in personal rapid\n  transit (PRT) network", "comments": "22 pages, 6 figures, 5 tables", "journal-ref": "Journal of Advanced Transportation, vol. 50 (2016), No.4,\n  pp.608-629", "doi": "10.1002/atr.1365", "report-no": null, "categories": "cs.DC cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an original heuristic algorithm of empty vehicles management\nin personal rapid transit network is presented. The algorithm is used for the\ndelivery of empty vehicles for waiting passengers, for balancing the\ndistribution of empty vehicles within the network, and for providing an empty\nspace for vehicles approaching a station. Each of these tasks involves a\ndecision on the trip that has to be done by a selected empty vehicle from its\nactual location to some determined destination. The decisions are based on a\nmulti-parameter function involving a set of factors and thresholds. An\nimportant feature of the algorithm is that it does not use any central database\nof passenger input (demand) and locations of free vehicles. Instead, it is\nbased on the local exchange of data between stations: on their states and on\nthe vehicles they expect. Therefore, it seems well-tailored for a distributed\nimplementation. The algorithm is uniform, meaning that the same basic procedure\nis used for multiple tasks using a task-specific set of parameters.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 15:03:22 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Daszczuk", "Wiktor B.", ""], ["Mie\u015bcicki", "Jerzy", ""], ["Grabski", "Waldemar", ""]]}, {"id": "1710.06406", "submitter": "Claire Bonial", "authors": "Claire Bonial, Matthew Marge, Ron artstein, Ashley Foots, Felix\n  Gervits, Cory J. Hayes, Cassidy Henry, Susan G. Hill, Anton Leuski, Stephanie\n  M. Lukin, Pooja Moolchandani, Kimberly A. Pollard, David Traum, Clare R. Voss", "title": "Laying Down the Yellow Brick Road: Development of a Wizard-of-Oz\n  Interface for Collecting Human-Robot Dialogue", "comments": "7 pages, 2 figures, accepted for oral presentation at the Symposium\n  on Natural Communication for Human-Robot Collaboration, AAAI Fall Symposium\n  Series, November 9-11, 2017, https://www.aaai.org/ocs/index.php/FSS/FSS17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the adaptation and refinement of a graphical user interface\ndesigned to facilitate a Wizard-of-Oz (WoZ) approach to collecting human-robot\ndialogue data. The data collected will be used to develop a dialogue system for\nrobot navigation. Building on an interface previously used in the development\nof dialogue systems for virtual agents and video playback, we add templates\nwith open parameters which allow the wizard to quickly produce a wide variety\nof utterances. Our research demonstrates that this approach to data collection\nis viable as an intermediate step in developing a dialogue system for physical\nrobots in remote locations from their users - a domain in which the human and\nrobot need to regularly verify and update a shared understanding of the\nphysical environment. We show that our WoZ interface and the fixed set of\nutterances and templates therein provide for a natural pace of dialogue with\ngood coverage of the navigation domain.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 17:34:31 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Bonial", "Claire", ""], ["Marge", "Matthew", ""], ["artstein", "Ron", ""], ["Foots", "Ashley", ""], ["Gervits", "Felix", ""], ["Hayes", "Cory J.", ""], ["Henry", "Cassidy", ""], ["Hill", "Susan G.", ""], ["Leuski", "Anton", ""], ["Lukin", "Stephanie M.", ""], ["Moolchandani", "Pooja", ""], ["Pollard", "Kimberly A.", ""], ["Traum", "David", ""], ["Voss", "Clare R.", ""]]}, {"id": "1710.06422", "submitter": "Kuan Fang", "authors": "Kuan Fang, Yunfei Bai, Stefan Hinterstoisser, Silvio Savarese, Mrinal\n  Kalakrishnan", "title": "Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from\n  Simulation", "comments": "ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based approaches to robotic manipulation are limited by the\nscalability of data collection and accessibility of labels. In this paper, we\npresent a multi-task domain adaptation framework for instance grasping in\ncluttered scenes by utilizing simulated robot experiments. Our neural network\ntakes monocular RGB images and the instance segmentation mask of a specified\ntarget object as inputs, and predicts the probability of successfully grasping\nthe specified object for each candidate motor command. The proposed transfer\nlearning framework trains a model for instance grasping in simulation and uses\na domain-adversarial loss to transfer the trained model to real robots using\nindiscriminate grasping data, which is available both in simulation and the\nreal world. We evaluate our model in real-world robot experiments, comparing it\nwith alternative model architectures as well as an indiscriminate grasping\nbaseline.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 17:54:50 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 04:08:58 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Fang", "Kuan", ""], ["Bai", "Yunfei", ""], ["Hinterstoisser", "Stefan", ""], ["Savarese", "Silvio", ""], ["Kalakrishnan", "Mrinal", ""]]}, {"id": "1710.06451", "submitter": "Samuel L. Smith", "authors": "Samuel L. Smith and Quoc V. Le", "title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent", "comments": "13 pages, 9 figures. Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two questions at the heart of machine learning; how can we\npredict if a minimum will generalize to the test set, and why does stochastic\ngradient descent find minima that generalize well? Our work responds to Zhang\net al. (2016), who showed deep neural networks can easily memorize randomly\nlabeled training data, despite generalizing well on real labels of the same\ninputs. We show that the same phenomenon occurs in small linear models. These\nobservations are explained by the Bayesian evidence, which penalizes sharp\nminima but is invariant to model parameterization. We also demonstrate that,\nwhen one holds the learning rate fixed, there is an optimum batch size which\nmaximizes the test set accuracy. We propose that the noise introduced by small\nmini-batches drives the parameters towards minima whose evidence is large.\nInterpreting stochastic gradient descent as a stochastic differential equation,\nwe identify the \"noise scale\" $g = \\epsilon (\\frac{N}{B} - 1) \\approx \\epsilon\nN/B$, where $\\epsilon$ is the learning rate, $N$ the training set size and $B$\nthe batch size. Consequently the optimum batch size is proportional to both the\nlearning rate and the size of the training set, $B_{opt} \\propto \\epsilon N$.\nWe verify these predictions empirically.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 18:08:04 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 22:07:53 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 19:42:20 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Smith", "Samuel L.", ""], ["Le", "Quoc V.", ""]]}, {"id": "1710.06481", "submitter": "Johannes Welbl", "authors": "Johannes Welbl, Pontus Stenetorp, Sebastian Riedel", "title": "Constructing Datasets for Multi-hop Reading Comprehension Across\n  Documents", "comments": "This paper directly corresponds to the TACL version\n  (https://transacl.org/ojs/index.php/tacl/article/view/1325) apart from minor\n  changes in wording, additional footnotes, and appendices", "journal-ref": "Transactions of the Association for Computational Linguistics\n  (TACL), Vol 6 (2018), pages 287-302", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most Reading Comprehension methods limit themselves to queries which can be\nanswered using a single sentence, paragraph, or document. Enabling models to\ncombine disjoint pieces of textual evidence would extend the scope of machine\ncomprehension methods, but currently there exist no resources to train and test\nthis capability. We propose a novel task to encourage the development of models\nfor text understanding across multiple documents and to investigate the limits\nof existing methods. In our task, a model learns to seek and combine evidence -\neffectively performing multi-hop (alias multi-step) inference. We devise a\nmethodology to produce datasets for this task, given a collection of\nquery-answer pairs and thematically linked documents. Two datasets from\ndifferent domains are induced, and we identify potential pitfalls and devise\ncircumvention strategies. We evaluate two previously proposed competitive\nmodels and find that one can integrate information across documents. However,\nboth models struggle to select relevant information, as providing documents\nguaranteed to be relevant greatly improves their performance. While the models\noutperform several strong baselines, their best accuracy reaches 42.9% compared\nto human performance at 74.0% - leaving ample room for improvement.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 19:35:07 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 17:08:20 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Welbl", "Johannes", ""], ["Stenetorp", "Pontus", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1710.06513", "submitter": "Yuanlu Xu", "authors": "Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, Song-Chun Zhu", "title": "Learning Pose Grammar to Encode Human Body Configuration for 3D Pose\n  Estimation", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a pose grammar to tackle the problem of 3D human\npose estimation. Our model directly takes 2D pose as input and learns a\ngeneralized 2D-3D mapping function. The proposed model consists of a base\nnetwork which efficiently captures pose-aligned features and a hierarchy of\nBi-directional RNNs (BRNN) on the top to explicitly incorporate a set of\nknowledge regarding human body configuration (i.e., kinematics, symmetry, motor\ncoordination). The proposed model thus enforces high-level constraints over\nhuman poses. In learning, we develop a pose sample simulator to augment\ntraining samples in virtual camera views, which further improves our model\ngeneralizability. We validate our method on public 3D human pose benchmarks and\npropose a new evaluation protocol working on cross-view setting to verify the\ngeneralization capability of different methods. We empirically observe that\nmost state-of-the-art methods encounter difficulty under such setting while our\nmethod can well handle such challenges.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 22:05:19 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 10:27:33 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 22:56:40 GMT"}, {"version": "v4", "created": "Tue, 5 Dec 2017 08:49:06 GMT"}, {"version": "v5", "created": "Tue, 12 Dec 2017 21:37:59 GMT"}, {"version": "v6", "created": "Thu, 4 Jan 2018 22:50:45 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Fang", "Haoshu", ""], ["Xu", "Yuanlu", ""], ["Wang", "Wenguan", ""], ["Liu", "Xiaobai", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1710.06525", "submitter": "Trong Nghia Hoang", "authors": "Trong Nghia Hoang, Yuchen Xiao, Kavinayan Sivakumar, Christopher\n  Amato, Jonathan How", "title": "Near-Optimal Adversarial Policy Switching for Decentralized Asynchronous\n  Multi-Agent Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in multi-robot and multi-agent systems is generating\nsolutions that are robust to other self-interested or even adversarial parties\nwho actively try to prevent the agents from achieving their goals. The\npracticality of existing works addressing this challenge is limited to only\nsmall-scale synchronous decision-making scenarios or a single agent planning\nits best response against a single adversary with fixed, procedurally\ncharacterized strategies. In contrast this paper considers a more realistic\nclass of problems where a team of asynchronous agents with limited observation\nand communication capabilities need to compete against multiple strategic\nadversaries with changing strategies. This problem necessitates agents that can\ncoordinate to detect changes in adversary strategies and plan the best response\naccordingly. Our approach first optimizes a set of stratagems that represent\nthese best responses. These optimized stratagems are then integrated into a\nunified policy that can detect and respond when the adversaries change their\nstrategies. The near-optimality of the proposed framework is established\ntheoretically as well as demonstrated empirically in simulation and hardware.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 23:14:17 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Hoang", "Trong Nghia", ""], ["Xiao", "Yuchen", ""], ["Sivakumar", "Kavinayan", ""], ["Amato", "Christopher", ""], ["How", "Jonathan", ""]]}, {"id": "1710.06542", "submitter": "Lerrel Pinto Mr", "authors": "Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba,\n  Pieter Abbeel", "title": "Asymmetric Actor Critic for Image-Based Robot Learning", "comments": "Videos of experiments can be found at http://www.goo.gl/b57WTs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) has proven a powerful technique in many\nsequential decision making domains. However, Robotics poses many challenges for\nRL, most notably training on a physical system can be expensive and dangerous,\nwhich has sparked significant interest in learning control policies using a\nphysics simulator. While several recent works have shown promising results in\ntransferring policies trained in simulation to the real world, they often do\nnot fully utilize the advantage of working with a simulator. In this work, we\nexploit the full state observability in the simulator to train better policies\nwhich take as input only partial observations (RGBD images). We do this by\nemploying an actor-critic training algorithm in which the critic is trained on\nfull states while the actor (or policy) gets rendered images as input. We show\nexperimentally on a range of simulated tasks that using these asymmetric inputs\nsignificantly improves performance. Finally, we combine this method with domain\nrandomization and show real robot experiments for several tasks like picking,\npushing, and moving a block. We achieve this simulation to real world transfer\nwithout training on any real world data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 01:10:37 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Pinto", "Lerrel", ""], ["Andrychowicz", "Marcin", ""], ["Welinder", "Peter", ""], ["Zaremba", "Wojciech", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1710.06574", "submitter": "Ruishan Liu", "authors": "Ruishan Liu, James Zou", "title": "The Effects of Memory Replay in Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experience replay is a key technique behind many recent advances in deep\nreinforcement learning. Allowing the agent to learn from earlier memories can\nspeed up learning and break undesirable temporal correlations. Despite its\nwide-spread application, very little is understood about the properties of\nexperience replay. How does the amount of memory kept affect learning dynamics?\nDoes it help to prioritize certain experiences? In this paper, we address these\nquestions by formulating a dynamical systems ODE model of Q-learning with\nexperience replay. We derive analytic solutions of the ODE for a simple\nsetting. We show that even in this very simple setting, the amount of memory\nkept can substantially affect the agent's performance. Too much or too little\nmemory both slow down learning. Moreover, we characterize regimes where\nprioritized replay harms the agent's learning. We show that our analytic\nsolutions have excellent agreement with experiments. Finally, we propose a\nsimple algorithm for adaptively changing the memory buffer size which achieves\nconsistently good empirical performance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 03:19:55 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Liu", "Ruishan", ""], ["Zou", "James", ""]]}, {"id": "1710.06636", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "Deceased Organ Matching in Australia", "comments": "Proceedings of 5th International Conference on Algorithmic Decision\n  Theory (ADT 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite efforts to increase the supply of organs from living donors, most\nkidney transplants performed in Australia still come from deceased donors. The\nage of these donated organs has increased substantially in recent decades as\nthe rate of fatal accidents on roads has fallen. The Organ and Tissue Authority\nin Australia is therefore looking to design a new mechanism that better matches\nthe age of the organ to the age of the patient. I discuss the design,\naxiomatics and performance of several candidate mechanisms that respect the\nspecial online nature of this fair division problem.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 09:20:12 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "1710.06879", "submitter": "Guolei Sun", "authors": "Guolei Sun, Xiangliang Zhang", "title": "Graph Embedding with Rich Information through Heterogeneous Network", "comments": "9 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding has attracted increasing attention due to its critical\napplication in social network analysis. Most existing algorithms for graph\nembedding only rely on the typology information and fail to use the copious\ninformation in nodes as well as edges. As a result, their performance for many\ntasks may not be satisfactory. In this paper, we proposed a novel and general\nframework of representation learning for graph with rich text information\nthrough constructing a bipartite heterogeneous network. Specially, we designed\na biased random walk to explore the constructed heterogeneous network with the\nnotion of flexible neighborhood. The efficacy of our method is demonstrated by\nextensive comparison experiments with several baselines on various datasets. It\nimproves the Micro-F1 and Macro-F1 of node classification by 10% and 7% on Cora\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 18:06:57 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 08:46:14 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Sun", "Guolei", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "1710.06922", "submitter": "Jason Lee", "authors": "Jason Lee, Kyunghyun Cho, Jason Weston and Douwe Kiela", "title": "Emergent Translation in Multi-Agent Communication", "comments": "Accepted to ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most machine translation systems to date are trained on large parallel\ncorpora, humans learn language in a different way: by being grounded in an\nenvironment and interacting with other humans. In this work, we propose a\ncommunication game where two agents, native speakers of their own respective\nlanguages, jointly learn to solve a visual referential task. We find that the\nability to understand and translate a foreign language emerges as a means to\nachieve shared goals. The emergent translation is interactive and multimodal,\nand crucially does not require parallel corpora, but only monolingual,\nindependent text and corresponding images. Our proposed translation model\nachieves this by grounding the source and target languages into a shared visual\nmodality, and outperforms several baselines on both word-level and\nsentence-level translation tasks. Furthermore, we show that agents in a\nmultilingual community learn to translate better and faster than in a bilingual\ncommunication setting.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 00:37:27 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 03:22:49 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Lee", "Jason", ""], ["Cho", "Kyunghyun", ""], ["Weston", "Jason", ""], ["Kiela", "Douwe", ""]]}, {"id": "1710.06923", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "C. Anantaram and Sunil Kumar Kopparapu", "title": "Adapting general-purpose speech recognition engine output for\n  domain-specific natural language question answering", "comments": "20 opages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-based natural language question-answering interfaces to enterprise\nsystems are gaining a lot of attention. General-purpose speech engines can be\nintegrated with NLP systems to provide such interfaces. Usually,\ngeneral-purpose speech engines are trained on large `general' corpus. However,\nwhen such engines are used for specific domains, they may not recognize\ndomain-specific words well, and may produce erroneous output. Further, the\naccent and the environmental conditions in which the speaker speaks a sentence\nmay induce the speech engine to inaccurately recognize certain words. The\nsubsequent natural language question-answering does not produce the requisite\nresults as the question does not accurately represent what the speaker\nintended. Thus, the speech engine's output may need to be adapted for a domain\nbefore further natural language processing is carried out. We present two\nmechanisms for such an adaptation, one based on evolutionary development and\nthe other based on machine learning, and show how we can repair the\nspeech-output to make the subsequent natural language question-answering\nbetter.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 12:18:16 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Anantaram", "C.", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1710.06975", "submitter": "Alexander Peysakhovich", "authors": "Alexander Peysakhovich, Adam Lerer", "title": "Consequentialist conditional cooperation in social dilemmas with\n  imperfect information", "comments": null, "journal-ref": "Proceedings of the International Conference on Learning\n  Representations 2018", "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social dilemmas, where mutual cooperation can lead to high payoffs but\nparticipants face incentives to cheat, are ubiquitous in multi-agent\ninteraction. We wish to construct agents that cooperate with pure cooperators,\navoid exploitation by pure defectors, and incentivize cooperation from the\nrest. However, often the actions taken by a partner are (partially) unobserved\nor the consequences of individual actions are hard to predict. We show that in\na large class of games good strategies can be constructed by conditioning one's\nbehavior solely on outcomes (ie. one's past rewards). We call this\nconsequentialist conditional cooperation. We show how to construct such\nstrategies using deep reinforcement learning techniques and demonstrate, both\nanalytically and experimentally, that they are effective in social dilemmas\nbeyond simple matrix games. We also show the limitations of relying purely on\nconsequences and discuss the need for understanding both the consequences of\nand the intentions behind an action.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 00:54:53 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 14:54:33 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Peysakhovich", "Alexander", ""], ["Lerer", "Adam", ""]]}, {"id": "1710.07031", "submitter": "Borko Bo\\v{s}kovi\\'c", "authors": "Borko Bo\\v{s}kovi\\'c and Janez Brest", "title": "Protein Folding Optimization using Differential Evolution Extended with\n  Local Search and Component Reinitialization", "comments": "22 pages, 8 figures, 10 tables, journal", "journal-ref": "Information Sciences, Volumes 454-455, 2018, Pages 178-199", "doi": "10.1016/j.ins.2018.04.072", "report-no": null, "categories": "cs.AI cs.NE q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel Differential Evolution algorithm for protein\nfolding optimization that is applied to a three-dimensional AB off-lattice\nmodel. The proposed algorithm includes two new mechanisms. A local search is\nused to improve convergence speed and to reduce the runtime complexity of the\nenergy calculation. For this purpose, a local movement is introduced within the\nlocal search. The designed evolutionary algorithm has fast convergence speed\nand, therefore, when it is trapped into the local optimum or a relatively good\nsolution is located, it is hard to locate a better similar solution. The\nsimilar solution is different from the good solution in only a few components.\nA component reinitialization method is designed to mitigate this problem. Both\nthe new mechanisms and the proposed algorithm were analyzed on well-known amino\nacid sequences that are used frequently in the literature. Experimental results\nshow that the employed new mechanisms improve the efficiency of our algorithm\nand that the proposed algorithm is superior to other state-of-the-art\nalgorithms. It obtained a hit ratio of 100% for sequences up to 18 monomers,\nwithin a budget of $10^{11}$ solution evaluations. New best-known solutions\nwere obtained for most of the sequences. The existence of the symmetric\nbest-known solutions is also demonstrated in the paper.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 08:07:51 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 06:56:02 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Bo\u0161kovi\u0107", "Borko", ""], ["Brest", "Janez", ""]]}, {"id": "1710.07075", "submitter": "Spyros Gkezerlis", "authors": "Spyros Gkezerlis and Dimitris Kalles", "title": "Decision Trees for Helpdesk Advisor Graphs", "comments": null, "journal-ref": "Bulletin of the Technical Committee on Learning Technology, Volume\n  18, Issue 2-3, April 2016", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use decision trees to build a helpdesk agent reference network to\nfacilitate the on-the-job advising of junior or less experienced staff on how\nto better address telecommunication customer fault reports. Such reports\ngenerate field measurements and remote measurements which, when coupled with\nlocation data and client attributes, and fused with organization-level\nstatistics, can produce models of how support should be provided. Beyond\ndecision support, these models can help identify staff who can act as advisors,\nbased on the quality, consistency and predictability of dealing with complex\ntroubleshooting reports. Advisor staff models are then used to guide less\nexperienced staff in their decision making; thus, we advocate the deployment of\na simple mechanism which exploits the availability of staff with a sound track\nrecord at the helpdesk to act as dormant tutors.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 10:48:52 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Gkezerlis", "Spyros", ""], ["Kalles", "Dimitris", ""]]}, {"id": "1710.07114", "submitter": "Jedrzej Potoniec", "authors": "Jedrzej Potoniec and Piotr Jakubowski and Agnieszka {\\L}awrynowicz", "title": "Swift Linked Data Miner: Mining OWL 2 EL class expressions directly from\n  online RDF datasets", "comments": null, "journal-ref": null, "doi": "10.1016/j.websem.2017.08.001", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present Swift Linked Data Miner, an interruptible algorithm\nthat can directly mine an online Linked Data source (e.g., a SPARQL endpoint)\nfor OWL 2 EL class expressions to extend an ontology with new SubClassOf:\naxioms. The algorithm works by downloading only a small part of the Linked Data\nsource at a time, building a smart index in the memory and swiftly iterating\nover the index to mine axioms. We propose a transformation function from mined\naxioms to RDF Data Shapes. We show, by means of a crowdsourcing experiment,\nthat most of the axioms mined by Swift Linked Data Miner are correct and can be\nadded to an ontology. We provide a ready to use Prot\\'eg\\'e plugin implementing\nthe algorithm, to support ontology engineers in their daily modeling work.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 12:25:06 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Potoniec", "Jedrzej", ""], ["Jakubowski", "Piotr", ""], ["\u0141awrynowicz", "Agnieszka", ""]]}, {"id": "1710.07147", "submitter": "Aschkan Omidvar", "authors": "Aschkan Omidvar, Eren Erman Ozguven, O. Arda Vanli, R.\n  Tavakkoli-Moghaddam", "title": "A Two-Phase Safe Vehicle Routing and Scheduling Problem: Formulations\n  and Solution Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two phase time dependent vehicle routing and scheduling\noptimization model that identifies the safest routes, as a substitute for the\nclassical objectives given in the literature such as shortest distance or\ntravel time, through (1) avoiding recurring congestions, and (2) selecting\nroutes that have a lower probability of crash occurrences and non-recurring\ncongestion caused by those crashes. In the first phase, we solve a\nmixed-integer programming model which takes the dynamic speed variations into\naccount on a graph of roadway networks according to the time of day, and\nidentify the routing of a fleet and sequence of nodes on the safest feasible\npaths. Second phase considers each route as an independent transit path (fixed\nroute with fixed node sequences), and tries to avoid congestion by rescheduling\nthe departure times of each vehicle from each node, and by adjusting the\nsub-optimal speed on each arc. A modified simulated annealing (SA) algorithm is\nformulated to solve both complex models iteratively, which is found to be\ncapable of providing solutions in a considerably short amount of time.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 01:58:19 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Omidvar", "Aschkan", ""], ["Ozguven", "Eren Erman", ""], ["Vanli", "O. Arda", ""], ["Tavakkoli-Moghaddam", "R.", ""]]}, {"id": "1710.07214", "submitter": "Georgios Feretzakis", "authors": "Georgios Feretzakis, Dimitris Kalles and Vassilios S. Verykios", "title": "On Using Linear Diophantine Equations to Tune the extent of Look Ahead\n  while Hiding Decision Tree Rules", "comments": "10 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1706.05733", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on preserving the privacy of sensitive pat-terns when\ninducing decision trees. We adopt a record aug-mentation approach for hiding\nsensitive classification rules in binary datasets. Such a hiding methodology is\npreferred over other heuristic solutions like output perturbation or\ncrypto-graphic techniques - which restrict the usability of the data - since\nthe raw data itself is readily available for public use. In this paper, we\npropose a look ahead approach using linear Diophantine equations in order to\nadd the appropriate number of instances while minimally disturbing the initial\nentropy of the nodes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 04:12:59 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Feretzakis", "Georgios", ""], ["Kalles", "Dimitris", ""], ["Verykios", "Vassilios S.", ""]]}, {"id": "1710.07360", "submitter": "Matias Alvarado Dr", "authors": "Mat\\'ias Alvarado, Arturo Yee, Carlos Villarreal", "title": "Go game formal revealing by Ising model", "comments": "19 pages, 9 figures some of them composition of 2 - 5 small ones. 42\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Go gaming is a struggle for territory control between rival, black and white,\nstones on a board. We model the Go dynamics in a game by means of the Ising\nmodel whose interaction coefficients reflect essential rules and tactics\nemployed in Go to build long-term strategies. At any step of the game, the\nenergy functional of the model provides the control degree (strength) of a\nplayer over the board. A close fit between predictions of the model with actual\ngames is obtained.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 21:36:09 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Alvarado", "Mat\u00edas", ""], ["Yee", "Arturo", ""], ["Villarreal", "Carlos", ""]]}, {"id": "1710.07551", "submitter": "Tuka Alhanai", "authors": "Tuka Alhanai, Rhoda Au, and James Glass", "title": "Spoken Language Biomarkers for Detecting Cognitive Impairment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we developed an automated system that evaluates speech and\nlanguage features from audio recordings of neuropsychological examinations of\n92 subjects in the Framingham Heart Study. A total of 265 features were used in\nan elastic-net regularized binomial logistic regression model to classify the\npresence of cognitive impairment, and to select the most predictive features.\nWe compared performance with a demographic model from 6,258 subjects in the\ngreater study cohort (0.79 AUC), and found that a system that incorporated both\naudio and text features performed the best (0.92 AUC), with a True Positive\nRate of 29% (at 0% False Positive Rate) and a good model fit (Hosmer-Lemeshow\ntest > 0.05). We also found that decreasing pitch and jitter, shorter segments\nof speech, and responses phrased as questions were positively associated with\ncognitive impairment.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 14:41:43 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Alhanai", "Tuka", ""], ["Au", "Rhoda", ""], ["Glass", "James", ""]]}, {"id": "1710.07558", "submitter": "Vivek Sharma", "authors": "Vivek Sharma, Ali Diba, Davy Neven, Michael S. Brown, Luc Van Gool,\n  Rainer Stiefelhagen", "title": "Classification Driven Dynamic Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks rely on image texture and structure to serve as\ndiscriminative features to classify the image content. Image enhancement\ntechniques can be used as preprocessing steps to help improve the overall image\nquality and in turn improve the overall effectiveness of a CNN. Existing image\nenhancement methods, however, are designed to improve the perceptual quality of\nan image for a human observer. In this paper, we are interested in learning\nCNNs that can emulate image enhancement and restoration, but with the overall\ngoal to improve image classification and not necessarily human perception. To\nthis end, we present a unified CNN architecture that uses a range of\nenhancement filters that can enhance image-specific details via end-to-end\ndynamic filter learning. We demonstrate the effectiveness of this strategy on\nfour challenging benchmark datasets for fine-grained, object, scene, and\ntexture classification: CUB-200-2011, PASCAL-VOC2007, MIT-Indoor, and DTD.\nExperiments using our proposed enhancement show promising results on all the\ndatasets. In addition, our approach is capable of improving the performance of\nall generic CNN architectures.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 14:54:29 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 19:17:42 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 19:11:33 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Sharma", "Vivek", ""], ["Diba", "Ali", ""], ["Neven", "Davy", ""], ["Brown", "Michael S.", ""], ["Van Gool", "Luc", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1710.07654", "submitter": "Wei Ping", "authors": "Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan,\n  Sharan Narang, Jonathan Raiman, John Miller", "title": "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence\n  Learning", "comments": "Published as a conference paper at ICLR 2018. (v3 changed paper\n  title)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Deep Voice 3, a fully-convolutional attention-based neural\ntext-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural\nspeech synthesis systems in naturalness while training ten times faster. We\nscale Deep Voice 3 to data set sizes unprecedented for TTS, training on more\nthan eight hundred hours of audio from over two thousand speakers. In addition,\nwe identify common error modes of attention-based speech synthesis networks,\ndemonstrate how to mitigate them, and compare several different waveform\nsynthesis methods. We also describe how to scale inference to ten million\nqueries per day on one single-GPU server.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 18:17:23 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 02:50:28 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 06:23:45 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Ping", "Wei", ""], ["Peng", "Kainan", ""], ["Gibiansky", "Andrew", ""], ["Arik", "Sercan O.", ""], ["Kannan", "Ajay", ""], ["Narang", "Sharan", ""], ["Raiman", "Jonathan", ""], ["Miller", "John", ""]]}, {"id": "1710.07659", "submitter": "Andreas St\\\"ockel", "authors": "Andreas St\\\"ockel, Aaron R. Voelker, Chris Eliasmith", "title": "Point Neurons with Conductance-Based Synapses in the Neural Engineering\n  Framework", "comments": "24 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mathematical model underlying the Neural Engineering Framework (NEF)\nexpresses neuronal input as a linear combination of synaptic currents. However,\nin biology, synapses are not perfect current sources and are thus nonlinear.\nDetailed synapse models are based on channel conductances instead of currents,\nwhich require independent handling of excitatory and inhibitory synapses. This,\nin particular, significantly affects the influence of inhibitory signals on the\nneuronal dynamics. In this technical report we first summarize the relevant\nportions of the NEF and conductance-based synapse models. We then discuss a\nna\\\"ive translation between populations of LIF neurons with current- and\nconductance-based synapses based on an estimation of an average membrane\npotential. Experiments show that this simple approach works relatively well for\nfeed-forward communication channels, yet performance degrades for NEF networks\ndescribing more complex dynamics, such as integration.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 18:35:23 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["St\u00f6ckel", "Andreas", ""], ["Voelker", "Aaron R.", ""], ["Eliasmith", "Chris", ""]]}, {"id": "1710.07706", "submitter": "Supriya Kapur", "authors": "Supriya Kapur, Asit Mishra, and Debbie Marr", "title": "Low Precision RNNs: Quantizing RNNs Without Losing Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similar to convolution neural networks, recurrent neural networks (RNNs)\ntypically suffer from over-parameterization. Quantizing bit-widths of weights\nand activations results in runtime efficiency on hardware, yet it often comes\nat the cost of reduced accuracy. This paper proposes a quantization approach\nthat increases model size with bit-width reduction. This approach will allow\nnetworks to perform at their baseline accuracy while still maintaining the\nbenefits of reduced precision and overall model size reduction.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 21:12:30 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Kapur", "Supriya", ""], ["Mishra", "Asit", ""], ["Marr", "Debbie", ""]]}, {"id": "1710.07709", "submitter": "Kalyan Veeramachaneni", "authors": "Roy Wedge, James Max Kanter, Santiago Moral Rubio, Sergio Iglesias\n  Perez, Kalyan Veeramachaneni", "title": "Solving the \"false positives\" problem in fraud prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an automated feature engineering based approach to\ndramatically reduce false positives in fraud prediction. False positives plague\nthe fraud prediction industry. It is estimated that only 1 in 5 declared as\nfraud are actually fraud and roughly 1 in every 6 customers have had a valid\ntransaction declined in the past year. To address this problem, we use the Deep\nFeature Synthesis algorithm to automatically derive behavioral features based\non the historical data of the card associated with a transaction. We generate\n237 features (>100 behavioral patterns) for each transaction, and use a random\nforest to learn a classifier. We tested our machine learning model on data from\na large multinational bank and compared it to their existing solution. On an\nunseen data of 1.852 million transactions, we were able to reduce the false\npositives by 54% and provide a savings of 190K euros. We also assess how to\ndeploy this solution, and whether it necessitates streaming computation for\nreal time scoring. We found that our solution can maintain similar benefits\neven when historical features are computed once every 7 days.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 21:34:49 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Wedge", "Roy", ""], ["Kanter", "James Max", ""], ["Rubio", "Santiago Moral", ""], ["Perez", "Sergio Iglesias", ""], ["Veeramachaneni", "Kalyan", ""]]}, {"id": "1710.07735", "submitter": "Sima Behpour", "authors": "Sima Behpour, Kris M. Kitani, Brian D. Ziebart", "title": "ADA: A Game-Theoretic Perspective on Data Augmentation for Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of random perturbations of ground truth data, such as random\ntranslation or scaling of bounding boxes, is a common heuristic used for data\naugmentation that has been shown to prevent overfitting and improve\ngeneralization. Since the design of data augmentation is largely guided by\nreported best practices, it is difficult to understand if those design choices\nare optimal. To provide a more principled perspective, we develop a\ngame-theoretic interpretation of data augmentation in the context of object\ndetection. We aim to find an optimal adversarial perturbations of the ground\ntruth data (i.e., the worst case perturbations) that forces the object bounding\nbox predictor to learn from the hardest distribution of perturbed examples for\nbetter test-time performance. We establish that the game theoretic solution,\nthe Nash equilibrium, provides both an optimal predictor and optimal data\naugmentation distribution. We show that our adversarial method of training a\npredictor can significantly improve test time performance for the task of\nobject detection. On the ImageNet object detection task, our adversarial\napproach improves performance by over 16\\% compared to the best performing data\naugmentation method\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 00:51:49 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 15:20:22 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Behpour", "Sima", ""], ["Kitani", "Kris M.", ""], ["Ziebart", "Brian D.", ""]]}, {"id": "1710.07783", "submitter": "Aixiang Chen", "authors": "Aixiang Chen, Bingchuan Chen, Xiaolong Chai, Rui Bian, Hengguang Li", "title": "A Novel Stochastic Stratified Average Gradient Method: Convergence Rate\n  and Its Complexity", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SGD (Stochastic Gradient Descent) is a popular algorithm for large scale\noptimization problems due to its low iterative cost. However, SGD can not\nachieve linear convergence rate as FGD (Full Gradient Descent) because of the\ninherent gradient variance. To attack the problem, mini-batch SGD was proposed\nto get a trade-off in terms of convergence rate and iteration cost. In this\npaper, a general CVI (Convergence-Variance Inequality) equation is presented to\nstate formally the interaction of convergence rate and gradient variance. Then\na novel algorithm named SSAG (Stochastic Stratified Average Gradient) is\nintroduced to reduce gradient variance based on two techniques, stratified\nsampling and averaging over iterations that is a key idea in SAG (Stochastic\nAverage Gradient). Furthermore, SSAG can achieve linear convergence rate of\n$\\mathcal {O}((1-\\frac{\\mu}{8CL})^k)$ at smaller storage and iterative costs,\nwhere $C\\geq 2$ is the category number of training data. This convergence rate\ndepends mainly on the variance between classes, but not on the variance within\nthe classes. In the case of $C\\ll N$ ($N$ is the training data size), SSAG's\nconvergence rate is much better than SAG's convergence rate of $\\mathcal\n{O}((1-\\frac{\\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG\nand many other algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 10:45:13 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 08:13:34 GMT"}, {"version": "v3", "created": "Sun, 3 Dec 2017 23:13:26 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Chen", "Aixiang", ""], ["Chen", "Bingchuan", ""], ["Chai", "Xiaolong", ""], ["Bian", "Rui", ""], ["Li", "Hengguang", ""]]}, {"id": "1710.07818", "submitter": "Yue Zhao", "authors": "Yue Zhao, Jianshu Chen, H. Vincent Poor", "title": "A Learning-to-Infer Method for Real-Time Power Grid Multi-Line Outage\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying a potentially large number of simultaneous line outages in power\ntransmission networks in real time is a computationally hard problem. This is\nbecause the number of hypotheses grows exponentially with the network size. A\nnew \"Learning-to-Infer\" method is developed for efficient inference of every\nline status in the network. Optimizing the line outage detector is transformed\nto and solved as a discriminative learning problem based on Monte Carlo samples\ngenerated with power flow simulations. A major advantage of the developed\nLearning-to-Infer method is that the labeled data used for training can be\ngenerated in an arbitrarily large amount rapidly and at very little cost. As a\nresult, the power of offline training is fully exploited to learn very complex\nclassifiers for effective real-time multi-line outage identification. The\nproposed methods are evaluated in the IEEE 30, 118 and 300 bus systems.\nExcellent performance in identifying multi-line outages in real time is\nachieved with a reasonably small amount of data.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 15:58:46 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 00:45:08 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Zhao", "Yue", ""], ["Chen", "Jianshu", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1710.07850", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan, Nina Narodytska, Hongxia Jin", "title": "Deep Neural Network Approximation using Tensor Sketching", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are powerful learning models that achieve\nstate-of-the-art performance on many computer vision, speech, and language\nprocessing tasks. In this paper, we study a fundamental question that arises\nwhen designing deep network architectures: Given a target network architecture\ncan we design a smaller network architecture that approximates the operation of\nthe target network? The question is, in part, motivated by the challenge of\nparameter reduction (compression) in modern deep neural networks, as the ever\nincreasing storage and memory requirements of these networks pose a problem in\nresource constrained environments.\n  In this work, we focus on deep convolutional neural network architectures,\nand propose a novel randomized tensor sketching technique that we utilize to\ndevelop a unified framework for approximating the operation of both the\nconvolutional and fully connected layers. By applying the sketching technique\nalong different tensor dimensions, we design changes to the convolutional and\nfully connected layers that substantially reduce the number of effective\nparameters in a network. We show that the resulting smaller network can be\ntrained directly, and has a classification accuracy that is comparable to the\noriginal network.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 20:14:00 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Narodytska", "Nina", ""], ["Jin", "Hongxia", ""]]}, {"id": "1710.07903", "submitter": "Guillermo P\\'erez", "authors": "Stephane Le Roux and Guillermo A. Perez", "title": "The Complexity of Graph-Based Reductions for Reachability in Markov\n  Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the never-worse relation (NWR) for Markov decision processes with an\ninfinite-horizon reachability objective. A state q is never worse than a state\np if the maximal probability of reaching the target set of states from p is at\nmost the same value from q, regard- less of the probabilities labelling the\ntransitions. Extremal-probability states, end components, and essential states\nare all special cases of the equivalence relation induced by the NWR. Using the\nNWR, states in the same equivalence class can be collapsed. Then, actions\nleading to sub- optimal states can be removed. We show the natural decision\nproblem associated to computing the NWR is coNP-complete. Finally, we ex- tend\na previously known incomplete polynomial-time iterative algorithm to\nunder-approximate the NWR.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 07:40:11 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 12:00:26 GMT"}, {"version": "v3", "created": "Sun, 28 Jan 2018 11:03:47 GMT"}, {"version": "v4", "created": "Sat, 24 Feb 2018 12:26:03 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Roux", "Stephane Le", ""], ["Perez", "Guillermo A.", ""]]}, {"id": "1710.07983", "submitter": "Weichao Zhou", "authors": "Weichao Zhou, Wenchao Li", "title": "Safety-Aware Apprenticeship Learning", "comments": "Accepted by International Conference on Computer Aided Verification\n  (CAV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apprenticeship learning (AL) is a kind of Learning from Demonstration\ntechniques where the reward function of a Markov Decision Process (MDP) is\nunknown to the learning agent and the agent has to derive a good policy by\nobserving an expert's demonstrations. In this paper, we study the problem of\nhow to make AL algorithms inherently safe while still meeting its learning\nobjective. We consider a setting where the unknown reward function is assumed\nto be a linear combination of a set of state features, and the safety property\nis specified in Probabilistic Computation Tree Logic (PCTL). By embedding\nprobabilistic model checking inside AL, we propose a novel\ncounterexample-guided approach that can ensure safety while retaining\nperformance of the learnt policy. We demonstrate the effectiveness of our\napproach on several challenging AL scenarios where safety is essential.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 17:29:16 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 20:48:50 GMT"}, {"version": "v3", "created": "Tue, 6 Feb 2018 18:58:32 GMT"}, {"version": "v4", "created": "Sat, 28 Apr 2018 14:25:44 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Zhou", "Weichao", ""], ["Li", "Wenchao", ""]]}, {"id": "1710.07990", "submitter": "Daniel Larsson", "authors": "Daniel T. Larsson, Daniel Braun, Panagiotis Tsiotras", "title": "Hierarchical State Abstractions for Decision-Making Problems with\n  Computational Constraints", "comments": null, "journal-ref": "2017 IEEE Conference on Decision and Control", "doi": "10.1109/CDC.2017.8263809", "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this semi-tutorial paper, we first review the information-theoretic\napproach to account for the computational costs incurred during the search for\noptimal actions in a sequential decision-making problem. The traditional (MDP)\nframework ignores computational limitations while searching for optimal\npolicies, essentially assuming that the acting agent is perfectly rational and\naims for exact optimality. Using the free-energy, a variational principle is\nintroduced that accounts not only for the value of a policy alone, but also\nconsiders the cost of finding this optimal policy. The solution of the\nvariational equations arising from this formulation can be obtained using\nfamiliar Bellman-like value iterations from dynamic programming (DP) and the\nBlahut-Arimoto (BA) algorithm from rate distortion theory. Finally, we\ndemonstrate the utility of the approach for generating hierarchies of state\nabstractions that can be used to best exploit the available computational\nresources. A numerical example showcases these concepts for a path-planning\nproblem in a grid world environment.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 17:59:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Larsson", "Daniel T.", ""], ["Braun", "Daniel", ""], ["Tsiotras", "Panagiotis", ""]]}, {"id": "1710.08012", "submitter": "Maryam Hashemzadeh", "authors": "Maryam Hashemzadeh, Reshad Hosseini and Majid Nili Ahmadabadi", "title": "Exploiting generalization in the subspaces for faster model-based\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the lack of enough generalization in the state-space, common methods\nin Reinforcement Learning (RL) suffer from slow learning speed especially in\nthe early learning trials. This paper introduces a model-based method in\ndiscrete state-spaces for increasing learning speed in terms of required\nexperience (but not required computational time) by exploiting generalization\nin the experiences of the subspaces. A subspace is formed by choosing a subset\nof features in the original state representation (full-space). Generalization\nand faster learning in a subspace are due to many-to-one mapping of experiences\nfrom the full-space to each state in the subspace. Nevertheless, due to\ninherent perceptual aliasing in the subspaces, the policy suggested by each\nsubspace does not generally converge to the optimal policy. Our approach,\ncalled Model Based Learning with Subspaces (MoBLeS), calculates confidence\nintervals of the estimated Q-values in the full-space and in the subspaces.\nThese confidence intervals are used in the decision making, such that the agent\nbenefits the most from the possible generalization while avoiding from\ndetriment of the perceptual aliasing in the subspaces. Convergence of MoBLeS to\nthe optimal policy is theoretically investigated. Additionally, we show through\nseveral experiments that MoBLeS improves the learning speed in the early\ntrials.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 20:50:52 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 11:51:13 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Hashemzadeh", "Maryam", ""], ["Hosseini", "Reshad", ""], ["Ahmadabadi", "Majid Nili", ""]]}, {"id": "1710.08107", "submitter": "Michael Amir", "authors": "Michael Amir, Alfred M. Bruckstein", "title": "Probabilistic Pursuits on Graphs", "comments": "Corrections and additional details", "journal-ref": null, "doi": "10.1016/j.tcs.2019.08.001", "report-no": null, "categories": "cs.DM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider discrete dynamical systems of \"ant-like\" agents engaged in a\nsequence of pursuits on a graph environment. The agents emerge one by one at\nequal time intervals from a source vertex $s$ and pursue each other by greedily\nattempting to close the distance to their immediate predecessor, the agent that\nemerged just before them from $s$, until they arrive at the destination point\n$t$. Such pursuits have been investigated before in the continuous setting and\nin discrete time when the underlying environment is a regular grid. In both\nthese settings the agents' walks provably converge to a shortest path from $s$\nto $t$. Furthermore, assuming a certain natural probability distribution over\nthe move choices of the agents on the grid (in case there are multiple shortest\npaths between an agent and its predecessor), the walks converge to the uniform\ndistribution over all shortest paths from $s$ to $t$.\n  We study the evolution of agent walks over a general finite graph environment\n$G$. Our model is a natural generalization of the pursuit rule proposed for the\ncase of the grid. The main results are as follows. We show that \"convergence\"\nto the shortest paths in the sense of previous work extends to all\npseudo-modular graphs (i.e. graphs in which every three pairwise intersecting\ndisks have a nonempty intersection), and also to environments obtained by\ntaking graph products, generalizing previous results in two different ways. We\nshow that convergence to the shortest paths is also obtained by chordal graphs,\nand discuss some further positive and negative results for planar graphs. In\nthe most general case, convergence to the shortest paths is not guaranteed, and\nthe agents may get stuck on sets of recurrent, non-optimal walks from $s$ to\n$t$. However, we show that the limiting distributions of the agents' walks will\nalways be uniform distributions over some set of walks of equal length.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 06:52:09 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 13:37:14 GMT"}, {"version": "v3", "created": "Thu, 31 Jan 2019 17:26:48 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Amir", "Michael", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1710.08191", "submitter": "Fabio Massimo Zanzotto", "authors": "Fabio Massimo Zanzotto", "title": "Human-in-the-loop Artificial Intelligence", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research, 2019", "doi": "10.1613/jair.1.11345", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Little by little, newspapers are revealing the bright future that Artificial\nIntelligence (AI) is building. Intelligent machines will help everywhere.\nHowever, this bright future has a dark side: a dramatic job market contraction\nbefore its unpredictable transformation. Hence, in a near future, large numbers\nof job seekers will need financial support while catching up with these novel\nunpredictable jobs. This possible job market crisis has an antidote inside. In\nfact, the rise of AI is sustained by the biggest knowledge theft of the recent\nyears. Learning AI machines are extracting knowledge from unaware skilled or\nunskilled workers by analyzing their interactions. By passionately doing their\njobs, these workers are digging their own graves.\n  In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI)\nas a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward\naware and unaware knowledge producers with a different scheme: decisions of AI\nsystems generating revenues will repay the legitimate owners of the knowledge\nused for taking those decisions. As modern Robin Hoods, HIT-AI researchers\nshould fight for a fairer Artificial Intelligence that gives back what it\nsteals.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 10:37:50 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Zanzotto", "Fabio Massimo", ""]]}, {"id": "1710.08192", "submitter": "Jonghwa Yim", "authors": "Jonghwa Yim, Kyung-Ah Sohn", "title": "Investigating the feature collection for semantic segmentation via\n  single skip connection", "comments": "(In pressing) Journal of KIISE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the study of deep convolutional neural network became prevalent, one of\nthe important discoveries is that a feature map from a convolutional network\ncan be extracted before going into the fully connected layer and can be used as\na saliency map for object detection. Furthermore, the model can use features\nfrom each different layer for accurate object detection: the features from\ndifferent layers can have different properties. As the model goes deeper, it\nhas many latent skip connections and feature maps to elaborate object\ndetection. Although there are many intermediate layers that we can use for\nsemantic segmentation through skip connection, still the characteristics of\neach skip connection and the best skip connection for this task are uncertain.\nTherefore, in this study, we exhaustively research skip connections of\nstate-of-the-art deep convolutional networks and investigate the\ncharacteristics of the features from each intermediate layer. In addition, this\nstudy would suggest how to use a recent deep neural network model for semantic\nsegmentation and it would therefore become a cornerstone for later studies with\nthe state-of-the-art network models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 10:40:58 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Yim", "Jonghwa", ""], ["Sohn", "Kyung-Ah", ""]]}, {"id": "1710.08315", "submitter": "Jinhua Tao", "authors": "Jinhua Tao, Zidong Du, Qi Guo, Huiying Lan, Lei Zhang, Shengyuan Zhou,\n  Lingjie Xu, Cong Liu, Haifeng Liu, Shan Tang, Allen Rush, Willian Chen,\n  Shaoli Liu, Yunji Chen, Tianshi Chen", "title": "BENCHIP: Benchmarking Intelligence Processors", "comments": "37pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing attention on deep learning has tremendously spurred the design\nof intelligence processing hardware. The variety of emerging intelligence\nprocessors requires standard benchmarks for fair comparison and system\noptimization (in both software and hardware). However, existing benchmarks are\nunsuitable for benchmarking intelligence processors due to their non-diversity\nand nonrepresentativeness. Also, the lack of a standard benchmarking\nmethodology further exacerbates this problem. In this paper, we propose\nBENCHIP, a benchmark suite and benchmarking methodology for intelligence\nprocessors. The benchmark suite in BENCHIP consists of two sets of benchmarks:\nmicrobenchmarks and macrobenchmarks. The microbenchmarks consist of\nsingle-layer networks. They are mainly designed for bottleneck analysis and\nsystem optimization. The macrobenchmarks contain state-of-the-art industrial\nnetworks, so as to offer a realistic comparison of different platforms. We also\npropose a standard benchmarking methodology built upon an industrial software\nstack and evaluation metrics that comprehensively reflect the various\ncharacteristics of the evaluated intelligence processors. BENCHIP is utilized\nfor evaluating various hardware platforms, including CPUs, GPUs, and\naccelerators. BENCHIP will be open-sourced soon.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 14:53:54 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 10:37:09 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Tao", "Jinhua", ""], ["Du", "Zidong", ""], ["Guo", "Qi", ""], ["Lan", "Huiying", ""], ["Zhang", "Lei", ""], ["Zhou", "Shengyuan", ""], ["Xu", "Lingjie", ""], ["Liu", "Cong", ""], ["Liu", "Haifeng", ""], ["Tang", "Shan", ""], ["Rush", "Allen", ""], ["Chen", "Willian", ""], ["Liu", "Shaoli", ""], ["Chen", "Yunji", ""], ["Chen", "Tianshi", ""]]}, {"id": "1710.08377", "submitter": "Delip Rao", "authors": "Brian McMahan and Delip Rao", "title": "Listening to the World Improves Speech Command Recognition", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study transfer learning in convolutional network architectures applied to\nthe task of recognizing audio, such as environmental sound events and speech\ncommands. Our key finding is that not only is it possible to transfer\nrepresentations from an unrelated task like environmental sound classification\nto a voice-focused task like speech command recognition, but also that doing so\nimproves accuracies significantly. We also investigate the effect of increased\nmodel capacity for transfer learning audio, by first validating known results\nfrom the field of Computer Vision of achieving better accuracies with\nincreasingly deeper networks on two audio datasets: UrbanSound8k and the newly\nreleased Google Speech Commands dataset. Then we propose a simple multiscale\ninput representation using dilated convolutions and show that it is able to\naggregate larger contexts and increase classification performance. Further, the\nmodels trained using a combination of transfer learning and multiscale input\nrepresentations need only 40% of the training data to achieve similar\naccuracies as a freshly trained model with 100% of the training data. Finally,\nwe demonstrate a positive interaction effect for the multiscale input and\ntransfer learning, making a case for the joint application of the two\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 16:47:05 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["McMahan", "Brian", ""], ["Rao", "Delip", ""]]}, {"id": "1710.08396", "submitter": "Barathi Ganesh H B", "authors": "Vinayakumar R, Barathi Ganesh HB, Anand Kumar M, Soman KP", "title": "Deep Health Care Text Classification", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health related social media mining is a valuable apparatus for the early\nrecognition of the diverse antagonistic medicinal conditions. Mostly, the\nexisting methods are based on machine learning with knowledge-based learning.\nThis working note presents the Recurrent neural network (RNN) and Long\nshort-term memory (LSTM) based embedding for automatic health text\nclassification in the social media mining. For each task, two systems are built\nand that classify the tweet at the tweet level. RNN and LSTM are used for\nextracting features and non-linear activation function at the last layer\nfacilitates to distinguish the tweets of different categories. The experiments\nare conducted on 2nd Social Media Mining for Health Applications Shared Task at\nAMIA 2017. The experiment results are considerable; however the proposed method\nis appropriate for the health text classification. This is primarily due to the\nreason that, it doesn't rely on any feature engineering mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 17:24:12 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["R", "Vinayakumar", ""], ["HB", "Barathi Ganesh", ""], ["M", "Anand Kumar", ""], ["KP", "Soman", ""]]}, {"id": "1710.08543", "submitter": "Sungbin Lim", "authors": "Hyungjoo Cho, Sungbin Lim, Gunho Choi, Hyunseok Min", "title": "Neural Stain-Style Transfer Learning using GAN for Histopathological\n  Images", "comments": "10 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of data-driven network for tumor classification varies with\nstain-style of histopathological images. This article proposes the stain-style\ntransfer (SST) model based on conditional generative adversarial networks\n(GANs) which is to learn not only the certain color distribution but also the\ncorresponding histopathological pattern. Our model considers feature-preserving\nloss in addition to well-known GAN loss. Consequently our model does not only\ntransfers initial stain-styles to the desired one but also prevent the\ndegradation of tumor classifier on transferred images. The model is examined\nusing the CAMELYON16 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 23:02:25 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 11:15:25 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Cho", "Hyungjoo", ""], ["Lim", "Sungbin", ""], ["Choi", "Gunho", ""], ["Min", "Hyunseok", ""]]}, {"id": "1710.08585", "submitter": "Dipan Pal", "authors": "Dipan K. Pal, Ashwin A. Kannan, Gautam Arakalgud, Marios Savvides", "title": "Max-Margin Invariant Features from Transformed Unlabeled Data", "comments": "Accepted at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of representations invariant to common transformations of the data\nis important to learning. Most techniques have focused on local approximate\ninvariance implemented within expensive optimization frameworks lacking\nexplicit theoretical guarantees. In this paper, we study kernels that are\ninvariant to a unitary group while having theoretical guarantees in addressing\nthe important practical issue of unavailability of transformed versions of\nlabelled data. A problem we call the Unlabeled Transformation Problem which is\na special form of semi-supervised learning and one-shot learning. We present a\ntheoretically motivated alternate approach to the invariant kernel SVM based on\nwhich we propose Max-Margin Invariant Features (MMIF) to solve this problem. As\nan illustration, we design an framework for face recognition and demonstrate\nthe efficacy of our approach on a large scale semi-synthetic dataset with\n153,000 images and a new challenging protocol on Labelled Faces in the Wild\n(LFW) while out-performing strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 02:57:37 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Pal", "Dipan K.", ""], ["Kannan", "Ashwin A.", ""], ["Arakalgud", "Gautam", ""], ["Savvides", "Marios", ""]]}, {"id": "1710.08893", "submitter": "Shaojun Zhu", "authors": "Shaojun Zhu, Andrew Kimmel, Kostas E. Bekris and Abdeslam Boularias", "title": "Fast Model Identification via Physics Engines for Data-Efficient Policy\n  Search", "comments": "IJCAI 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for identifying mechanical parameters of robots\nor objects, such as their mass and friction coefficients. Key features are the\nuse of off-the-shelf physics engines and the adaptation of a Bayesian\noptimization technique towards minimizing the number of real-world experiments\nneeded for model-based reinforcement learning. The proposed framework\nreproduces in a physics engine experiments performed on a real robot and\noptimizes the model's mechanical parameters so as to match real-world\ntrajectories. The optimized model is then used for learning a policy in\nsimulation, before real-world deployment. It is well understood, however, that\nit is hard to exactly reproduce real trajectories in simulation. Moreover, a\nnear-optimal policy can be frequently found with an imperfect model. Therefore,\nthis work proposes a strategy for identifying a model that is just good enough\nto approximate the value of a locally optimal policy with a certain confidence,\ninstead of wasting effort on identifying the most accurate model. Evaluations,\nperformed both in simulation and on a real robotic manipulation task, indicate\nthat the proposed strategy results in an overall time-efficient, integrated\nmodel identification and learning solution, which significantly improves the\ndata-efficiency of existing policy search algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 17:08:20 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 19:45:10 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 13:03:41 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Zhu", "Shaojun", ""], ["Kimmel", "Andrew", ""], ["Bekris", "Kostas E.", ""], ["Boularias", "Abdeslam", ""]]}, {"id": "1710.08969", "submitter": "Hideyuki Tachibana", "authors": "Hideyuki Tachibana, Katsuya Uenoyama, Shunsuke Aihara", "title": "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional\n  Networks with Guided Attention", "comments": "5 pages, 3figures, IEEE ICASSP 2018", "journal-ref": "Proc. ICASSP (2018) 4784-4788", "doi": "10.1109/ICASSP.2018.8461829", "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel text-to-speech (TTS) technique based on deep\nconvolutional neural networks (CNN), without use of any recurrent units.\nRecurrent neural networks (RNN) have become a standard technique to model\nsequential data recently, and this technique has been used in some cutting-edge\nneural TTS techniques. However, training RNN components often requires a very\npowerful computer, or a very long time, typically several days or weeks. Recent\nother studies, on the other hand, have shown that CNN-based sequence synthesis\ncan be much faster than RNN-based techniques, because of high\nparallelizability. The objective of this paper is to show that an alternative\nneural TTS based only on CNN alleviate these economic costs of training. In our\nexperiment, the proposed Deep Convolutional TTS was sufficiently trained\novernight (15 hours), using an ordinary gaming PC equipped with two GPUs, while\nthe quality of the synthesized speech was almost acceptable.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 19:56:32 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 05:41:53 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Tachibana", "Hideyuki", ""], ["Uenoyama", "Katsuya", ""], ["Aihara", "Shunsuke", ""]]}, {"id": "1710.08986", "submitter": "Dimitri Scheftelowitsch", "authors": "Dimitri Scheftelowitsch, Peter Buchholz, Vahid Hashemi, Holger\n  Hermanns", "title": "Multi-Objective Approaches to Markov Decision Processes with Uncertain\n  Transition Parameters", "comments": "9 pages, 5 figures, preprint for VALUETOOLS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov decision processes (MDPs) are a popular model for performance analysis\nand optimization of stochastic systems. The parameters of stochastic behavior\nof MDPs are estimates from empirical observations of a system; their values are\nnot known precisely. Different types of MDPs with uncertain, imprecise or\nbounded transition rates or probabilities and rewards exist in the literature.\n  Commonly, analysis of models with uncertainties amounts to searching for the\nmost robust policy which means that the goal is to generate a policy with the\ngreatest lower bound on performance (or, symmetrically, the lowest upper bound\non costs). However, hedging against an unlikely worst case may lead to losses\nin other situations. In general, one is interested in policies that behave well\nin all situations which results in a multi-objective view on decision making.\n  In this paper, we consider policies for the expected discounted reward\nmeasure of MDPs with uncertain parameters. In particular, the approach is\ndefined for bounded-parameter MDPs (BMDPs) [8]. In this setting the worst, best\nand average case performances of a policy are analyzed simultaneously, which\nyields a multi-scenario multi-objective optimization problem. The paper\npresents and evaluates approaches to compute the pure Pareto optimal policies\nin the value vector space.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 07:47:41 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Scheftelowitsch", "Dimitri", ""], ["Buchholz", "Peter", ""], ["Hashemi", "Vahid", ""], ["Hermanns", "Holger", ""]]}, {"id": "1710.09012", "submitter": "Baibhab Chatterjee", "authors": "Baibhab Chatterjee, Priyadarshini Panda, Shovan Maity, Kaushik Roy and\n  Shreyas Sen", "title": "An Energy-Efficient Mixed-Signal Neuron for Inherently Error-Resilient\n  Neuromorphic Systems", "comments": "Accepted in IEEE International Conference on Rebooting Computing\n  (ICRC), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents the design and analysis of a mixed-signal neuron (MS-N)\nfor convolutional neural networks (CNN) and compares its performance with a\ndigital neuron (Dig-N) in terms of operating frequency, power and noise. The\ncircuit-level implementation of the MS-N in 65 nm CMOS technology exhibits 2-3\norders of magnitude better energy-efficiency over Dig-N for neuromorphic\ncomputing applications - especially at low frequencies due to the high leakage\ncurrents from many transistors in Dig-N. The inherent error-resiliency of CNN\nis exploited to handle the thermal and flicker noise of MS-N. A system-level\nanalysis using a cohesive circuit-algorithmic framework on MNIST and CIFAR-10\ndatasets demonstrate an increase of 3% in worst-case classification error for\nMNIST when the integrated noise power in the bandwidth is ~ 1 {\\mu}V2.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 22:43:16 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Chatterjee", "Baibhab", ""], ["Panda", "Priyadarshini", ""], ["Maity", "Shovan", ""], ["Roy", "Kaushik", ""], ["Sen", "Shreyas", ""]]}, {"id": "1710.09102", "submitter": "Robert K\\\"unnemann", "authors": "Robert K\\\"unnemann", "title": "Sufficient and necessary causation are dual", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causation has been the issue of philosophic debate since Hippocrates. Recent\nwork defines actual causation in terms of Pearl/Halpern's causality framework,\nformalizing necessary causes (IJCAI'15). This has inspired causality notions in\nthe security domain (CSF'15), which, perhaps surprisingly, formalize sufficient\ncauses instead. We provide an explicit relation between necessary and\nsufficient causes.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 07:57:56 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["K\u00fcnnemann", "Robert", ""]]}, {"id": "1710.09278", "submitter": "Fabio Lorenzo Traversa Ph.D.", "authors": "Fabio L. Traversa, Pietro Cicotti, Forrest Sheldon, Massimiliano Di\n  Ventra", "title": "Evidence of an exponential speed-up in the solution of hard optimization\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems pervade essentially every scientific discipline and\nindustry. Many such problems require finding a solution that maximizes the\nnumber of constraints satisfied. Often, these problems are particularly\ndifficult to solve because they belong to the NP-hard class, namely algorithms\nthat always find a solution in polynomial time are not known. Over the past\ndecades, research has focused on developing heuristic approaches that attempt\nto find an approximation to the solution. However, despite numerous research\nefforts, in many cases even approximations to the optimal solution are hard to\nfind, as the computational time for further refining a candidate solution grows\nexponentially with input size. Here, we show a non-combinatorial approach to\nhard optimization problems that achieves an exponential speed-up and finds\nbetter approximations than the current state-of-the-art. First, we map the\noptimization problem into a boolean circuit made of specially designed,\nself-organizing logic gates, which can be built with (non-quantum) electronic\ncomponents; the equilibrium points of the circuit represent the approximation\nto the problem at hand. Then, we solve its associated non-linear ordinary\ndifferential equations numerically, towards the equilibrium points. We\ndemonstrate this exponential gain by comparing a sequential MatLab\nimplementation of our solver with the winners of the 2016 Max-SAT competition\non a variety of hard optimization instances. We show empirical evidence that\nour solver scales linearly with the size of the problem, both in time and\nmemory, and argue that this property derives from the collective behavior of\nthe simulated physical circuit. Our approach can be applied to other types of\noptimization problems and the results presented here have far-reaching\nconsequences in many fields.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 06:23:09 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Traversa", "Fabio L.", ""], ["Cicotti", "Pietro", ""], ["Sheldon", "Forrest", ""], ["Di Ventra", "Massimiliano", ""]]}, {"id": "1710.09300", "submitter": "Filipe Alves Neto Verri", "authors": "Filipe Alves Neto Verri, Renato Tin\\'os, Liang Zhao", "title": "Feature learning in feature-sample networks using multi-objective\n  optimization", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": "10.1109/CEC.2018.8477891", "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data and knowledge representation are fundamental concepts in machine\nlearning. The quality of the representation impacts the performance of the\nlearning model directly. Feature learning transforms or enhances raw data to\nstructures that are effectively exploited by those models. In recent years,\nseveral works have been using complex networks for data representation and\nanalysis. However, no feature learning method has been proposed for such\ncategory of techniques. Here, we present an unsupervised feature learning\nmechanism that works on datasets with binary features. First, the dataset is\nmapped into a feature--sample network. Then, a multi-objective optimization\nprocess selects a set of new vertices to produce an enhanced version of the\nnetwork. The new features depend on a nonlinear function of a combination of\npreexisting features. Effectively, the process projects the input data into a\nhigher-dimensional space. To solve the optimization problem, we design two\nmetaheuristics based on the lexicographic genetic algorithm and the improved\nstrength Pareto evolutionary algorithm (SPEA2). We show that the enhanced\nnetwork contains more information and can be exploited to improve the\nperformance of machine learning methods. The advantages and disadvantages of\neach optimization strategy are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 15:18:27 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Verri", "Filipe Alves Neto", ""], ["Tin\u00f3s", "Renato", ""], ["Zhao", "Liang", ""]]}, {"id": "1710.09471", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Ryan A. Rossi, Rong Zhou, John Boaz Lee, Xiangnan\n  Kong, Theodore L. Willke and Hoda Eldardiry", "title": "Inductive Representation Learning in Large Attributed Graphs", "comments": "NIPS WiML", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs (networks) are ubiquitous and allow us to model entities (nodes) and\nthe dependencies (edges) between them. Learning a useful feature representation\nfrom graph data lies at the heart and success of many machine learning tasks\nsuch as classification, anomaly detection, link prediction, among many others.\nMany existing techniques use random walks as a basis for learning features or\nestimating the parameters of a graph model for a downstream prediction task.\nExamples include recent node embedding methods such as DeepWalk, node2vec, as\nwell as graph-based deep learning algorithms. However, the simple random walk\nused by these methods is fundamentally tied to the identity of the node. This\nhas three main disadvantages. First, these approaches are inherently\ntransductive and do not generalize to unseen nodes and other graphs. Second,\nthey are not space-efficient as a feature vector is learned for each node which\nis impractical for large graphs. Third, most of these approaches lack support\nfor attributed graphs.\n  To make these methods more generally applicable, we propose a framework for\ninductive network representation learning based on the notion of attributed\nrandom walk that is not tied to node identity and is instead based on learning\na function $\\Phi : \\mathrm{\\rm \\bf x} \\rightarrow w$ that maps a node attribute\nvector $\\mathrm{\\rm \\bf x}$ to a type $w$. This framework serves as a basis for\ngeneralizing existing methods such as DeepWalk, node2vec, and many other\nprevious methods that leverage traditional random walks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 21:40:57 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 23:51:39 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Rossi", "Ryan A.", ""], ["Zhou", "Rong", ""], ["Lee", "John Boaz", ""], ["Kong", "Xiangnan", ""], ["Willke", "Theodore L.", ""], ["Eldardiry", "Hoda", ""]]}, {"id": "1710.09549", "submitter": "Chong Huang", "authors": "Chong Huang, Peter Kairouz, Xiao Chen, Lalitha Sankar, and Ram\n  Rajagopal", "title": "Context-Aware Generative Adversarial Privacy", "comments": "Improved version of a paper accepted by Entropy Journal, Special\n  Issue on Information Theory in Machine Learning and Data Science", "journal-ref": null, "doi": "10.3390/e19120656", "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.GT cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving the utility of published datasets while simultaneously providing\nprovable privacy guarantees is a well-known challenge. On the one hand,\ncontext-free privacy solutions, such as differential privacy, provide strong\nprivacy guarantees, but often lead to a significant reduction in utility. On\nthe other hand, context-aware privacy solutions, such as information theoretic\nprivacy, achieve an improved privacy-utility tradeoff, but assume that the data\nholder has access to dataset statistics. We circumvent these limitations by\nintroducing a novel context-aware privacy framework called generative\nadversarial privacy (GAP). GAP leverages recent advancements in generative\nadversarial networks (GANs) to allow the data holder to learn privatization\nschemes from the dataset itself. Under GAP, learning the privacy mechanism is\nformulated as a constrained minimax game between two players: a privatizer that\nsanitizes the dataset in a way that limits the risk of inference attacks on the\nindividuals' private variables, and an adversary that tries to infer the\nprivate variables from the sanitized dataset. To evaluate GAP's performance, we\ninvestigate two simple (yet canonical) statistical dataset models: (a) the\nbinary data model, and (b) the binary Gaussian mixture model. For both models,\nwe derive game-theoretically optimal minimax privacy mechanisms, and show that\nthe privacy mechanisms learned from data (in a generative adversarial fashion)\nmatch the theoretically optimal ones. This demonstrates that our framework can\nbe easily applied in practice, even in the absence of dataset statistics.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 05:36:35 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 16:40:45 GMT"}, {"version": "v3", "created": "Sun, 3 Dec 2017 00:17:37 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Huang", "Chong", ""], ["Kairouz", "Peter", ""], ["Chen", "Xiao", ""], ["Sankar", "Lalitha", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1710.09554", "submitter": "Liu Liu", "authors": "Liu Liu, Ji Liu and Dacheng Tao", "title": "Duality-free Methods for Stochastic Composition Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the composition optimization with two expected-value functions in\nthe form of $\\frac{1}{n}\\sum\\nolimits_{i = 1}^n F_i(\\frac{1}{m}\\sum\\nolimits_{j\n= 1}^m G_j(x))+R(x)$, { which formulates many important problems in statistical\nlearning and machine learning such as solving Bellman equations in\nreinforcement learning and nonlinear embedding}. Full Gradient or classical\nstochastic gradient descent based optimization algorithms are unsuitable or\ncomputationally expensive to solve this problem due to the inner expectation\n$\\frac{1}{m}\\sum\\nolimits_{j = 1}^m G_j(x)$. We propose a duality-free based\nstochastic composition method that combines variance reduction methods to\naddress the stochastic composition problem. We apply SVRG and SAGA based\nmethods to estimate the inner function, and duality-free method to estimate the\nouter function. We prove the linear convergence rate not only for the convex\ncomposition problem, but also for the case that the individual outer functions\nare non-convex while the objective function is strongly-convex. We also provide\nthe results of experiments that show the effectiveness of our proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 06:10:44 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Liu", "Liu", ""], ["Liu", "Ji", ""], ["Tao", "Dacheng", ""]]}, {"id": "1710.09627", "submitter": "Imran Khan", "authors": "Charbel El Kaed, Imran Khan, Andre Van Den Berg, Hicham Hossayni and\n  Christophe Saint-Marcel", "title": "SRE: Semantic Rules Engine For the Industrial Internet-Of-Things\n  Gateways", "comments": "Accepted for publication in forthcoming issue of IEEE Transactions on\n  Industrial Informatics. The content is final but has NOT been proof-read", "journal-ref": "IEEE Transactions on Industrial Informatics, 2017", "doi": null, "report-no": null, "categories": "cs.AI cs.NI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Advent of the Internet-of-Things (IoT) paradigm has brought opportunities\nto solve many real-world problems. Energy management, for example, has\nattracted huge interest from academia, industries, governments and regulatory\nbodies. It involves collecting energy usage data, analyzing it, and optimizing\nthe energy consumption by applying control strategies. However, in industrial\nenvironments, performing such optimization is not trivial. The changes in\nbusiness rules, process control, and customer requirements make it much more\nchallenging. In this paper, a Semantic Rules Engine (SRE) for industrial\ngateways is presented that allows implementing dynamic and flexible rule-based\ncontrol strategies. It is simple, expressive, and allows managing rules\non-the-fly without causing any service interruption. Additionally, it can\nhandle semantic queries and provide results by inferring additional knowledge\nfrom previously defined concepts in ontologies. SRE has been validated and\ntested on different hardware platforms and in commercial products. Performance\nevaluations are also presented to validate its conformance to the customer\nrequirements.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 10:17:00 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Kaed", "Charbel El", ""], ["Khan", "Imran", ""], ["Berg", "Andre Van Den", ""], ["Hossayni", "Hicham", ""], ["Saint-Marcel", "Christophe", ""]]}, {"id": "1710.09762", "submitter": "Sarfaraz Hussein", "authors": "Maria J. M. Chuquicusma, Sarfaraz Hussein, Jeremy Burt, and Ulas Bagci", "title": "How to Fool Radiologists with Generative Adversarial Networks? A Visual\n  Turing Test for Lung Cancer Diagnosis", "comments": "Accepted for publication in IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminating lung nodules as malignant or benign is still an underlying\nchallenge. To address this challenge, radiologists need computer aided\ndiagnosis (CAD) systems which can assist in learning discriminative imaging\nfeatures corresponding to malignant and benign nodules. However, learning\nhighly discriminative imaging features is an open problem. In this paper, our\naim is to learn the most discriminative features pertaining to lung nodules by\nusing an adversarial learning methodology. Specifically, we propose to use\nunsupervised learning with Deep Convolutional-Generative Adversarial Networks\n(DC-GANs) to generate lung nodule samples realistically. We hypothesize that\nimaging features of lung nodules will be discriminative if it is hard to\ndifferentiate them (fake) from real (true) nodules. To test this hypothesis, we\npresent Visual Turing tests to two radiologists in order to evaluate the\nquality of the generated (fake) nodules. Extensive comparisons are performed in\ndiscerning real, generated, benign, and malignant nodules. This experimental\nset up allows us to validate the overall quality of the generated nodules,\nwhich can then be used to (1) improve diagnostic decisions by mining highly\ndiscriminative imaging features, (2) train radiologists for educational\npurposes, and (3) generate realistic samples to train deep networks with big\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 15:38:50 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 04:31:54 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Chuquicusma", "Maria J. M.", ""], ["Hussein", "Sarfaraz", ""], ["Burt", "Jeremy", ""], ["Bagci", "Ulas", ""]]}, {"id": "1710.09779", "submitter": "Sarfaraz Hussein", "authors": "Sarfaraz Hussein, Pujan Kandel, Juan E. Corral, Candice W. Bolan,\n  Michael B. Wallace and Ulas Bagci", "title": "Deep Multi-Modal Classification of Intraductal Papillary Mucinous\n  Neoplasms (IPMN) with Canonical Correlation Analysis", "comments": "Accepted for publication in IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.QM q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pancreatic cancer has the poorest prognosis among all cancer types.\nIntraductal Papillary Mucinous Neoplasms (IPMNs) are radiographically\nidentifiable precursors to pancreatic cancer; hence, early detection and\nprecise risk assessment of IPMN are vital. In this work, we propose a\nConvolutional Neural Network (CNN) based computer aided diagnosis (CAD) system\nto perform IPMN diagnosis and risk assessment by utilizing multi-modal MRI. In\nour proposed approach, we use minimum and maximum intensity projections to ease\nthe annotation variations among different slices and type of MRIs. Then, we\npresent a CNN to obtain deep feature representation corresponding to each MRI\nmodality (T1-weighted and T2-weighted). At the final step, we employ canonical\ncorrelation analysis (CCA) to perform a fusion operation at the feature level,\nleading to discriminative canonical correlation features. Extracted features\nare used for classification. Our results indicate significant improvements over\nother potential approaches to solve this important problem. The proposed\napproach doesn't require explicit sample balancing in cases of imbalance\nbetween positive and negative examples. To the best of our knowledge, our study\nis the first to automatically diagnose IPMN using multi-modal MRI.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 16:01:31 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 04:27:29 GMT"}, {"version": "v3", "created": "Fri, 27 Apr 2018 16:47:53 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Hussein", "Sarfaraz", ""], ["Kandel", "Pujan", ""], ["Corral", "Juan E.", ""], ["Bolan", "Candice W.", ""], ["Wallace", "Michael B.", ""], ["Bagci", "Ulas", ""]]}, {"id": "1710.09788", "submitter": "Alessandro Checco", "authors": "Alessandro Checco, Gianluca Demartini, Alexander Loeser, Ines Arous,\n  Mourad Khayati, Matthias Dantone, Richard Koopmanschap, Svetlin Stalinov,\n  Martin Kersten, Ying Zhang", "title": "FashionBrain Project: A Vision for Understanding Europe's Fashion Data\n  Universe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core business in the fashion industry is the understanding and prediction\nof customer needs and trends. Search engines and social networks are at the\nsame time a fundamental bridge and a costly middleman between the customer's\npurchase intention and the retailer. To better exploit Europe's distinctive\ncharacteristics e.g., multiple languages, fashion and cultural differences, it\nis pivotal to reduce retailers' dependence to search engines. This goal can be\nachieved by harnessing various data channels (manufacturers and distribution\nnetworks, online shops, large retailers, social media, market observers, call\ncenters, press/magazines etc.) that retailers can leverage in order to gain\nmore insight about potential buyers, and on the industry trends as a whole.\nThis can enable the creation of novel on-line shopping experiences, the\ndetection of influencers, and the prediction of upcoming fashion trends.\n  In this paper, we provide an overview of the main research challenges and an\nanalysis of the most promising technological solutions that we are\ninvestigating in the FashionBrain project.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 16:18:31 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Checco", "Alessandro", ""], ["Demartini", "Gianluca", ""], ["Loeser", "Alexander", ""], ["Arous", "Ines", ""], ["Khayati", "Mourad", ""], ["Dantone", "Matthias", ""], ["Koopmanschap", "Richard", ""], ["Stalinov", "Svetlin", ""], ["Kersten", "Martin", ""], ["Zhang", "Ying", ""]]}, {"id": "1710.09824", "submitter": "Preeti Bhargava", "authors": "Sarah Ellinger, Prantik Bhattacharyya, Preeti Bhargava, Nemanja\n  Spasojevic", "title": "Klout Topics for Modeling Interests and Expertise of Users Across Social\n  Networks", "comments": "4 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Klout Topics, a lightweight ontology to describe social\nmedia users' topics of interest and expertise. Klout Topics is designed to: be\nhuman-readable and consumer-friendly; cover multiple domains of knowledge in\ndepth; and promote data extensibility via knowledge base entities. We discuss\nwhy this ontology is well-suited for text labeling and interest modeling\napplications, and how it compares to available alternatives. We show its\ncoverage against common social media interest sets, and examples of how it is\nused to model the interests of over 780M social media users on Klout.com.\nFinally, we open the ontology for external use.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 17:42:13 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Ellinger", "Sarah", ""], ["Bhattacharyya", "Prantik", ""], ["Bhargava", "Preeti", ""], ["Spasojevic", "Nemanja", ""]]}, {"id": "1710.09867", "submitter": "Felix Hill Mr", "authors": "Felix Hill, Stephen Clark, Karl Moritz Hermann, Phil Blunsom", "title": "Understanding Early Word Learning in Situated Artificial Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based systems can now learn to locate the referents of words\nand phrases in images, answer questions about visual scenes, and execute\nsymbolic instructions as first-person actors in partially-observable worlds. To\nachieve this so-called grounded language learning, models must overcome\nchallenges that infants face when learning their first words. While it is\nnotable that models with no meaningful prior knowledge overcome these\nobstacles, researchers currently lack a clear understanding of how they do so,\na problem that we attempt to address in this paper. For maximum control and\ngenerality, we focus on a simple neural network-based language learning agent,\ntrained via policy-gradient methods, which can interpret single-word\ninstructions in a simulated 3D world. Whilst the goal is not to explicitly\nmodel infant word learning, we take inspiration from experimental paradigms in\ndevelopmental psychology and apply some of these to the artificial agent,\nexploring the conditions under which established human biases and learning\neffects emerge. We further propose a novel method for visualising semantic\nrepresentations in the agent.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 18:48:20 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 17:43:34 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Hill", "Felix", ""], ["Clark", "Stephen", ""], ["Hermann", "Karl Moritz", ""], ["Blunsom", "Phil", ""]]}, {"id": "1710.09952", "submitter": "Renato Fabbri", "authors": "Renato Fabbri", "title": "Enhancements of linked data expressiveness for ontologies", "comments": null, "journal-ref": "Anais do XX ENMC - Encontro Nacional de Modelagem Computacional e\n  VIII ECTM - Encontro de Ci\\^encias e Tecnologia de Materiais, Nova Friburgo,\n  RJ - 16 a 19 Outubro 2017", "doi": null, "report-no": "ISSN 2527-2357, ISBN 978-85-5676-019-7", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic web has received many contributions of researchers as ontologies\nwhich, in this context, i.e. within RDF linked data, are formalized\nconceptualizations that might use different protocols, such as RDFS, OWL DL and\nOWL FULL. In this article, we describe new expressive techniques which were\nfound necessary after elaborating dozens of OWL ontologies for the scientific\nacademy, the State and the civil society. They consist in: 1) stating possible\nuses a property might have without incurring into axioms or restrictions; 2)\nassigning a level of priority for an element (class, property, triple); 3)\ncorrect depiction in diagrams of relations between classes, between individuals\nwhich are imperative, and between individuals which are optional; 4) a\nconvenient association between OWL classes and SKOS concepts. We propose\nspecific rules to accomplish these enhancements and exemplify both its use and\nthe difficulties that arise because these techniques are currently not\nestablished as standards to the ontology designer.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 00:16:04 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Fabbri", "Renato", ""]]}, {"id": "1710.09954", "submitter": "Renato Fabbri", "authors": "Renato Fabbri and Maria Cristina Ferreira de Oliveira", "title": "Audiovisual Analytics Vocabulary and Ontology (AAVO): initial core and\n  example expansion", "comments": "Scripts in https://github.com/ttm/aavo/", "journal-ref": "Anais do XX ENMC - Encontro Nacional de Modelagem Computacional e\n  VIII ECTM - Encontro de Ci\\^encias e Tecnologia de Materiais, Nova Friburgo,\n  RJ - 16 a 19 Outubro 2017", "doi": null, "report-no": "ISSN 2527-2357, ISBN 978-85-5676-019-7", "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Analytics might be defined as data mining assisted by interactive\nvisual interfaces. The field has been receiving prominent consideration by\nresearchers, developers and the industry. The literature, however, is complex\nbecause it involves multiple fields of knowledge and is considerably recent. In\nthis article we describe an initial tentative organization of the knowledge in\nthe field as an OWL ontology and a SKOS vocabulary. This effort might be useful\nin many ways that include conceptual considerations and software\nimplementations. Within the results and discussions, we expose a core and an\nexample expansion of the conceptualization, and incorporate design issues that\nenhance the expressive power of the abstraction.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 00:30:03 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Fabbri", "Renato", ""], ["de Oliveira", "Maria Cristina Ferreira", ""]]}, {"id": "1710.10035", "submitter": "Bastien Pasdeloup", "authors": "Bastien Pasdeloup, Vincent Gripon, Jean-Charles Vialatte, Dominique\n  Pastor, Pascal Frossard", "title": "Convolutional neural networks on irregular domains based on approximate\n  vertex-domain translations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalization of convolutional neural networks (CNNs) to\nirregular domains, through the use of a translation operator on a graph\nstructure. In regular settings such as images, convolutional layers are\ndesigned by translating a convolutional kernel over all pixels, thus enforcing\ntranslation equivariance. In the case of general graphs however, translation is\nnot a well-defined operation, which makes shifting a convolutional kernel not\nstraightforward. In this article, we introduce a methodology to allow the\ndesign of convolutional layers that are adapted to signals evolving on\nirregular topologies, even in the absence of a natural translation. Using the\ndesigned layers, we build a CNN that we train using the initial set of signals.\nContrary to other approaches that aim at extending CNNs to irregular domains,\nwe incorporate the classical settings of CNNs for 2D signals as a particular\ncase of our approach. Designing convolutional layers in the vertex domain\ndirectly implies weight sharing, which in other approaches is generally\nestimated a posteriori using heuristics.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 09:08:29 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 13:54:48 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Pasdeloup", "Bastien", ""], ["Gripon", "Vincent", ""], ["Vialatte", "Jean-Charles", ""], ["Pastor", "Dominique", ""], ["Frossard", "Pascal", ""]]}, {"id": "1710.10044", "submitter": "Will Dabney", "authors": "Will Dabney, Mark Rowland, Marc G. Bellemare, R\\'emi Munos", "title": "Distributional Reinforcement Learning with Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning an agent interacts with the environment by taking\nactions and observing the next state and reward. When sampled\nprobabilistically, these state transitions, rewards, and actions can all induce\nrandomness in the observed long-term return. Traditionally, reinforcement\nlearning algorithms average over this randomness to estimate the value\nfunction. In this paper, we build on recent work advocating a distributional\napproach to reinforcement learning in which the distribution over returns is\nmodeled explicitly instead of only estimating the mean. That is, we examine\nmethods of learning the value distribution instead of the value function. We\ngive results that close a number of gaps between the theoretical and\nalgorithmic results given by Bellemare, Dabney, and Munos (2017). First, we\nextend existing results to the approximate distribution setting. Second, we\npresent a novel distributional reinforcement learning algorithm consistent with\nour theoretical formulation. Finally, we evaluate this new algorithm on the\nAtari 2600 games, observing that it significantly outperforms many of the\nrecent improvements on DQN, including the related distributional algorithm C51.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 09:35:26 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Dabney", "Will", ""], ["Rowland", "Mark", ""], ["Bellemare", "Marc G.", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1710.10057", "submitter": "Huang Lingxiao", "authors": "L. Elisa Celis, Lingxiao Huang, Nisheeth K. Vishnoi", "title": "Multiwinner Voting with Fairness Constraints", "comments": "The conference version of this paper appears in IJCAI-ECAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiwinner voting rules are used to select a small representative subset of\ncandidates or items from a larger set given the preferences of voters. However,\nif candidates have sensitive attributes such as gender or ethnicity (when\nselecting a committee), or specified types such as political leaning (when\nselecting a subset of news items), an algorithm that chooses a subset by\noptimizing a multiwinner voting rule may be unbalanced in its selection -- it\nmay under or over represent a particular gender or political orientation in the\nexamples above. We introduce an algorithmic framework for multiwinner voting\nproblems when there is an additional requirement that the selected subset\nshould be \"fair\" with respect to a given set of attributes. Our framework\nprovides the flexibility to (1) specify fairness with respect to multiple,\nnon-disjoint attributes (e.g., ethnicity and gender) and (2) specify a score\nfunction. We study the computational complexity of this constrained multiwinner\nvoting problem for monotone and submodular score functions and present several\napproximation algorithms and matching hardness of approximation results for\nvarious attribute group structure and types of score functions. We also present\nsimulations that suggest that adding fairness constraints may not affect the\nscores significantly when compared to the unconstrained case.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 10:13:31 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 19:19:15 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Celis", "L. Elisa", ""], ["Huang", "Lingxiao", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1710.10093", "submitter": "Alejandro Ramos Soto", "authors": "A. Ramos-Soto and M. Pereira-Fari\\~na", "title": "On modeling vagueness and uncertainty in data-to-text systems through\n  fuzzy sets", "comments": "31 pages including references (in a review-friendly format), 4\n  figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vagueness and uncertainty management is counted among one of the challenges\nthat remain unresolved in systems that generate texts from non-linguistic data,\nknown as data-to-text systems. In the last decade, work in fuzzy linguistic\nsummarization and description of data has raised the interest of using fuzzy\nsets to model and manage the imprecision of human language in data-to-text\nsystems. However, despite some research in this direction, there has not been\nan actual clear discussion and justification on how fuzzy sets can contribute\nto data-to-text for modeling vagueness and uncertainty in words and\nexpressions. This paper intends to bridge this gap by answering the following\nquestions: What does vagueness mean in fuzzy sets theory? What does vagueness\nmean in data-to-text contexts? In what ways can fuzzy sets theory contribute to\nimprove data-to-text systems? What are the challenges that researchers from\nboth disciplines need to address for a successful integration of fuzzy sets\ninto data-to-text systems? In what cases should the use of fuzzy sets be\navoided in D2T? For this, we review and discuss the state of the art of\nvagueness modeling in natural language generation and data-to-text, describe\npotential and actual usages of fuzzy sets in data-to-text contexts, and provide\nsome additional insights about the engineering of data-to-text systems that\nmake use of fuzzy set-based techniques.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 11:56:08 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Ramos-Soto", "A.", ""], ["Pereira-Fari\u00f1a", "M.", ""]]}, {"id": "1710.10098", "submitter": "Vincent Mousseau", "authors": "K. Belahc\\`ene, C. Labreuche, N. Maudet, V. Mousseau, W. Ouerdane", "title": "An efficient SAT formulation for learning multiple criteria\n  non-compensatory sorting rules from examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on Multiple Criteria Decision Analysis (MCDA) proposes several\nmethods in order to sort alternatives evaluated on several attributes into\nordered classes. Non Compensatory Sorting models (NCS) assign alternatives to\nclasses based on the way they compare to multicriteria profiles separating the\nconsecutive classes. Previous works have proposed approaches to learn the\nparameters of a NCS model based on a learning set. Exact approaches based on\nmixed integer linear programming ensures that the learning set is best\nrestored, but can only handle datasets of limited size. Heuristic approaches\ncan handle large learning sets, but do not provide any guarantee about the\ninferred model. In this paper, we propose an alternative formulation to learn a\nNCS model. This formulation, based on a SAT problem, guarantees to find a model\nfully consistent with the learning set (whenever it exists), and is\ncomputationally much more efficient than existing exact MIP approaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 12:07:55 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Belahc\u00e8ne", "K.", ""], ["Labreuche", "C.", ""], ["Maudet", "N.", ""], ["Mousseau", "V.", ""], ["Ouerdane", "W.", ""]]}, {"id": "1710.10116", "submitter": "Prashant Doshi", "authors": "Shervin Shahryari and Prashant Doshi", "title": "Inverse Reinforcement Learning Under Noisy Observations", "comments": "Full version of the extended abstract published in AAMAS 2017\n  conference, pages 1733 - 1735", "journal-ref": "In Proceedings of the 16th Conference on Autonomous Agents and\n  MultiAgent Systems (AAMAS '17). International Foundation for Autonomous\n  Agents and Multiagent Systems, Richland, SC, 1733-1735, 2017", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of performing inverse reinforcement learning when the\ntrajectory of the expert is not perfectly observed by the learner. Instead, a\nnoisy continuous-time observation of the trajectory is provided to the learner.\nThis problem exhibits wide-ranging applications and the specific application we\nconsider here is the scenario in which the learner seeks to penetrate a\nperimeter patrolled by a robot. The learner's field of view is limited due to\nwhich it cannot observe the patroller's complete trajectory. Instead, we allow\nthe learner to listen to the expert's movement sound, which it can also use to\nestimate the expert's state and action using an observation model. We treat the\nexpert's state and action as hidden data and present an algorithm based on\nexpectation maximization and maximum entropy principle to solve the non-linear,\nnon-convex problem. Related work considers discrete-time observations and an\nobservation model that does not include actions. In contrast, our technique\ntakes expectations over both state and action of the expert, enabling learning\neven in the presence of extreme noise and broader applications.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 13:10:26 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Shahryari", "Shervin", ""], ["Doshi", "Prashant", ""]]}, {"id": "1710.10164", "submitter": "Fulvio Mastrogiovanni", "authors": "Luca Buoncompagni, Barbara Bruno, Antonella Giuni, Fulvio\n  Mastrogiovanni, Renato Zaccaria", "title": "Towards a new paradigm for assistive technology at home: research\n  challenges, design issues and performance assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing elderly and people with special needs, including those suffering\nfrom physical disabilities and chronic diseases, with the possibility of\nretaining their independence at best is one of the most important challenges\nour society is expected to face. Assistance models based on the home care\nparadigm are being adopted rapidly in almost all industrialized and emerging\ncountries. Such paradigms hypothesize that it is necessary to ensure that the\nso-called Activities of Daily Living are correctly and regularly performed by\nthe assisted person to increase the perception of an improved quality of life.\nThis chapter describes the computational inference engine at the core of\nArianna, a system able to understand whether an assisted person performs a\ngiven set of ADL and to motivate him/her in performing them through a\nspeech-mediated motivational dialogue, using a set of nearables to be installed\nin an apartment, plus a wearable to be worn or fit in garments.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 14:36:44 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Buoncompagni", "Luca", ""], ["Bruno", "Barbara", ""], ["Giuni", "Antonella", ""], ["Mastrogiovanni", "Fulvio", ""], ["Zaccaria", "Renato", ""]]}, {"id": "1710.10335", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Nesreen K. Ahmed, Hoda Eldardiry, and Rong Zhou", "title": "Similarity-based Multi-label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification is an important learning problem with many\napplications. In this work, we propose a principled similarity-based approach\nfor multi-label learning called SML. We also introduce a similarity-based\napproach for predicting the label set size. The experimental results\ndemonstrate the effectiveness of SML for multi-label classification where it is\nshown to compare favorably with a wide variety of existing algorithms across a\nrange of evaluation criterion.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 21:20:31 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Ahmed", "Nesreen K.", ""], ["Eldardiry", "Hoda", ""], ["Zhou", "Rong", ""]]}, {"id": "1710.10381", "submitter": "Isaac Sledge", "authors": "Isaac J. Sledge and Jose C. Principe", "title": "Partitioning Relational Matrices of Similarities or Dissimilarities\n  using the Value of Information", "comments": "Submitted to the IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide an approach to clustering relational matrices whose\nentries correspond to either similarities or dissimilarities between objects.\nOur approach is based on the value of information, a parameterized,\ninformation-theoretic criterion that measures the change in costs associated\nwith changes in information. Optimizing the value of information yields a\ndeterministic annealing style of clustering with many benefits. For instance,\ninvestigators avoid needing to a priori specify the number of clusters, as the\npartitions naturally undergo phase changes, during the annealing process,\nwhereby the number of clusters changes in a data-driven fashion. The\nglobal-best partition can also often be identified.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 03:21:24 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Sledge", "Isaac J.", ""], ["Principe", "Jose C.", ""]]}, {"id": "1710.10386", "submitter": "Changmao Cheng", "authors": "Changmao Cheng, Yanwei Fu, Yu-Gang Jiang, Wei Liu, Wenlian Lu,\n  Jianfeng Feng, Xiangyang Xue", "title": "Dual Skipping Networks", "comments": "CVPR 2018 (poster); fix typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent neuroscience studies on the left-right asymmetry of\nthe human brain in processing low and high spatial frequency information, this\npaper introduces a dual skipping network which carries out coarse-to-fine\nobject categorization. Such a network has two branches to simultaneously deal\nwith both coarse and fine-grained classification tasks. Specifically, we\npropose a layer-skipping mechanism that learns a gating network to predict\nwhich layers to skip in the testing stage. This layer-skipping mechanism endows\nthe network with good flexibility and capability in practice. Evaluations are\nconducted on several widely used coarse-to-fine object categorization\nbenchmarks, and promising results are achieved by our proposed network model.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 04:18:11 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 06:59:48 GMT"}, {"version": "v3", "created": "Sun, 27 May 2018 12:31:43 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Cheng", "Changmao", ""], ["Fu", "Yanwei", ""], ["Jiang", "Yu-Gang", ""], ["Liu", "Wei", ""], ["Lu", "Wenlian", ""], ["Feng", "Jianfeng", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1710.10433", "submitter": "Susel Fern\\'andez", "authors": "Susel Fernandez and Takayuki Ito", "title": "An Ontology to support automated negotiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose an ontology to support automated negotiation in\nmultiagent systems. The ontology can be connected with some domain-specific\nontologies to facilitate the negotiation in different domains, such as\nIntelligent Transportation Systems (ITS), e-commerce, etc. The specific\nnegotiation rules for each type of negotiation strategy can also be defined as\npart of the ontology, reducing the amount of knowledge hardcoded in the agents\nand ensuring the interoperability. The expressiveness of the ontology was\nproved in a multiagent architecture for the automatic traffic light setting\napplication on ITS.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 09:30:53 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Fernandez", "Susel", ""], ["Ito", "Takayuki", ""]]}, {"id": "1710.10466", "submitter": "Andrew Holliday", "authors": "Andrew Holliday and Gregory Dudek", "title": "Scale-Robust Localization Using General Object Landmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization under large changes in scale is an important capability\nin many robotic mapping applications, such as localizing at low altitudes in\nmaps built at high altitudes, or performing loop closure over long distances.\nExisting approaches, however, are robust only up to about a 3x difference in\nscale between map and query images.\n  We propose a novel combination of deep-learning-based object features and\nstate-of-the-art SIFT point-features that yields improved robustness to scale\nchange. This technique is training-free and class-agnostic, and in principle\ncan be deployed in any environment out-of-the-box. We evaluate the proposed\ntechnique on the KITTI Odometry benchmark and on a novel dataset of outdoor\nimages exhibiting changes in visual scale of $7\\times$ and greater, which we\nhave released to the public. Our technique consistently outperforms\nlocalization using either SIFT features or the proposed object features alone,\nachieving both greater accuracy and much lower failure rates under large\nchanges in scale.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 13:48:31 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 19:03:16 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Holliday", "Andrew", ""], ["Dudek", "Gregory", ""]]}, {"id": "1710.10519", "submitter": "Lili Meng", "authors": "Lili Meng, Frederick Tung, James J. Little, Julien Valentin, Clarence\n  de Silva", "title": "Exploiting Points and Lines in Regression Forests for RGB-D Camera\n  Relocalization", "comments": "published as a conference paper at 2018 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera relocalization plays a vital role in many robotics and computer vision\ntasks, such as global localization, recovery from tracking failure and loop\nclosure detection. Recent random forests based methods exploit randomly sampled\npixel comparison features to predict 3D world locations for 2D image locations\nto guide the camera pose optimization. However, these image features are only\nsampled randomly in the images, without considering the spatial structures or\ngeometric information, leading to large errors or failure cases with the\nexistence of poorly textured areas or in motion blur. Line segment features are\nmore robust in these environments. In this work, we propose to jointly exploit\npoints and lines within the framework of uncertainty driven regression forests.\nThe proposed approach is thoroughly evaluated on three publicly available\ndatasets against several strong state-of-the-art baselines in terms of several\ndifferent error metrics. Experimental results prove the efficacy of our method,\nshowing superior or on-par state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 19:37:33 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 05:00:42 GMT"}, {"version": "v3", "created": "Sat, 28 Jul 2018 22:29:09 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Meng", "Lili", ""], ["Tung", "Frederick", ""], ["Little", "James J.", ""], ["Valentin", "Julien", ""], ["de Silva", "Clarence", ""]]}, {"id": "1710.10532", "submitter": "Daniel Kasenberg", "authors": "Daniel Kasenberg, Matthias Scheutz", "title": "Interpretable Apprenticeship Learning with Temporal Logic Specifications", "comments": "Accepted to the 56th IEEE Conference on Decision and Control (CDC\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has addressed using formulas in linear temporal logic (LTL) as\nspecifications for agents planning in Markov Decision Processes (MDPs). We\nconsider the inverse problem: inferring an LTL specification from demonstrated\nbehavior trajectories in MDPs. We formulate this as a multiobjective\noptimization problem, and describe state-based (\"what actually happened\") and\naction-based (\"what the agent expected to happen\") objective functions based on\na notion of \"violation cost\". We demonstrate the efficacy of the approach by\nemploying genetic programming to solve this problem in two simple domains.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 22:01:55 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Kasenberg", "Daniel", ""], ["Scheutz", "Matthias", ""]]}, {"id": "1710.10538", "submitter": "Ramanathan Guha", "authors": "Ramanathan V. Guha", "title": "Partial Knowledge In Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing domain knowledge is crucial for any task. There has been a wide\nrange of techniques developed to represent this knowledge, from older logic\nbased approaches to the more recent deep learning based techniques (i.e.\nembeddings). In this paper, we discuss some of these methods, focusing on the\nrepresentational expressiveness tradeoffs that are often made. In particular,\nwe focus on the the ability of various techniques to encode `partial knowledge'\n- a key component of successful knowledge systems. We introduce and describe\nthe concepts of `ensembles of embeddings' and `aggregate embeddings' and\ndemonstrate how they allow for partial knowledge.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 23:55:33 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Guha", "Ramanathan V.", ""]]}, {"id": "1710.10550", "submitter": "Jaemyung Ahn", "authors": "Dongoo Lee and Jaemyung Ahn", "title": "Vehicle Routing Problem with Vector Profits (VRPVP) with Max-Min\n  Criterion", "comments": "33 pages, submitted to Engineering Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new routing problem referred to as the vehicle\nrouting problem with vector profits. Given a network composed of nodes\n(depot/sites) and arcs connecting the nodes, the problem determines routes that\ndepart from the depot, visit sites to collect profits, and return to the depot.\nThere are multiple stakeholders interested in the mission and each site is\nassociated with a vector whose k-th element represents the profit value for the\nk-th stakeholder. The objective of the problem is to maximize the profit sum\nfor the least satisfied stakeholder, i.e., the stakeholder with the smallest\ntotal profit value. An approach based on the linear programming relaxation and\ncolumn-generation to solve this max-min type routing problem was developed. Two\ncases studies - the planetary surface exploration and the Rome tour cases -\nwere presented to demonstrate the effectiveness of the proposed problem\nformulation and solution methodology.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 02:04:17 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Lee", "Dongoo", ""], ["Ahn", "Jaemyung", ""]]}, {"id": "1710.10600", "submitter": "Daniel Lopez Martinez", "authors": "Daniel Lopez-Martinez", "title": "Regularization approaches for support vector machines with applications\n  to biomedical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine (SVM) is a widely used machine learning tool for\nclassification based on statistical learning theory. Given a set of training\ndata, the SVM finds a hyperplane that separates two different classes of data\npoints by the largest distance. While the standard form of SVM uses L2-norm\nregularization, other regularization approaches are particularly attractive for\nbiomedical datasets where, for example, sparsity and interpretability of the\nclassifier's coefficient values are highly desired features. Therefore, in this\npaper we consider different types of regularization approaches for SVMs, and\nexplore them in both synthetic and real biomedical datasets.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 12:17:19 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Lopez-Martinez", "Daniel", ""]]}, {"id": "1710.10675", "submitter": "Devinder Kumar", "authors": "Devinder Kumar, Graham W. Taylor and Alexander Wong", "title": "Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided\n  Diagnosis of Diabetic Retinopathy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown\nconsiderable promise in recent years as a potential tool for improving clinical\ndecision support in medical oncology, particularly those based around the\nconcept of Discovery Radiomics, where radiomic sequencers are discovered\nthrough the analysis of medical imaging data. One of the main limitations with\ncurrent CAD approaches is that it is very difficult to gain insight or\nrationale as to how decisions are made, thus limiting their utility to\nclinicians. Methods: In this study, we propose CLEAR-DR, a novel interpretable\nCAD system based on the notion of CLass-Enhanced Attentive Response Discovery\nRadiomics for the purpose of clinical decision support for diabetic\nretinopathy. Results: In addition to disease grading via the discovered deep\nradiomic sequencer, the CLEAR-DR system also produces a visual interpretation\nof the decision-making process to provide better insight and understanding into\nthe decision-making process of the system. Conclusion: We demonstrate the\neffectiveness and utility of the proposed CLEAR-DR system of enhancing the\ninterpretability of diagnostic grading results for the application of diabetic\nretinopathy grading. Significance: CLEAR-DR can act as a potential powerful\ntool to address the uninterpretability issue of current CAD systems, thus\nimproving their utility to clinicians.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 19:26:19 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Kumar", "Devinder", ""], ["Taylor", "Graham W.", ""], ["Wong", "Alexander", ""]]}, {"id": "1710.10686", "submitter": "Vladimir Golkov", "authors": "Jan Kuka\\v{c}ka, Vladimir Golkov, Daniel Cremers", "title": "Regularization for Deep Learning: A Taxonomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is one of the crucial ingredients of deep learning, yet the\nterm regularization has various definitions, and regularization methods are\noften studied separately from each other. In our work we present a systematic,\nunifying taxonomy to categorize existing methods. We distinguish methods that\naffect data, network architectures, error terms, regularization terms, and\noptimization procedures. We do not provide all details about the listed\nmethods; instead, we present an overview of how the methods can be sorted into\nmeaningful categories and sub-categories. This helps revealing links and\nfundamental similarities between them. Finally, we include practical\nrecommendations both for users and for developers of new regularization\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 20:27:51 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Kuka\u010dka", "Jan", ""], ["Golkov", "Vladimir", ""], ["Cremers", "Daniel", ""]]}, {"id": "1710.10704", "submitter": "Alireza Bagheri", "authors": "Alireza Bagheri, Osvaldo Simeone, Bipin Rajendran", "title": "Training Probabilistic Spiking Neural Networks with First-to-spike\n  Decoding", "comments": "A shorter version will be published on Proc. IEEE ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at\nharnessing the energy efficiency of spike-domain processing by building on\ncomputing elements that operate on, and exchange, spikes. In this paper, the\nproblem of training a two-layer SNN is studied for the purpose of\nclassification, under a Generalized Linear Model (GLM) probabilistic neural\nmodel that was previously considered within the computational neuroscience\nliterature. Conventional classification rules for SNNs operate offline based on\nthe number of output spikes at each output neuron. In contrast, a novel\ntraining method is proposed here for a first-to-spike decoding rule, whereby\nthe SNN can perform an early classification decision once spike firing is\ndetected at an output neuron. Numerical results bring insights into the optimal\nparameter selection for the GLM neuron and on the accuracy-complexity trade-off\nperformance of conventional and first-to-spike decoding.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 22:13:53 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 07:41:15 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 04:49:44 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Bagheri", "Alireza", ""], ["Simeone", "Osvaldo", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1710.10753", "submitter": "Lane A. Hemaspaandra", "authors": "Lane A. Hemaspaandra", "title": "Computational Social Choice and Computational Complexity: BFFs?", "comments": "A version of this paper will appear in AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the connection between computational social choice (comsoc) and\ncomputational complexity. We stress the work so far on, and urge continued\nfocus on, two less-recognized aspects of this connection. Firstly, this is very\nmuch a two-way street: Everyone knows complexity classification is used in\ncomsoc, but we also highlight benefits to complexity that have arisen from its\nuse in comsoc. Secondly, more subtle, less-known complexity tools often can be\nvery productively used in comsoc.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 03:28:32 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 19:36:27 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Hemaspaandra", "Lane A.", ""]]}, {"id": "1710.10772", "submitter": "Xingwei Cao", "authors": "Xingwei Cao, Xuyang Zhao, Qibin Zhao", "title": "Tensorizing Generative Adversarial Nets", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Network (GAN) and its variants exhibit\nstate-of-the-art performance in the class of generative models. To capture\nhigher-dimensional distributions, the common learning procedure requires high\ncomputational complexity and a large number of parameters. The problem of\nemploying such massive framework arises when deploying it on a platform with\nlimited computational power such as mobile phones. In this paper, we present a\nnew generative adversarial framework by representing each layer as a tensor\nstructure connected by multilinear operations, aiming to reduce the number of\nmodel parameters by a large factor while preserving the generative performance\nand sample quality. To learn the model, we employ an efficient algorithm which\nalternatively optimizes both discriminator and generator. Experimental outcomes\ndemonstrate that our model can achieve high compression rate for model\nparameters up to $35$ times when compared to the original GAN for MNIST\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 05:05:02 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 03:23:03 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Cao", "Xingwei", ""], ["Zhao", "Xuyang", ""], ["Zhao", "Qibin", ""]]}, {"id": "1710.10776", "submitter": "Catherine Wong", "authors": "Catherine Wong and Andrea Gesmundo", "title": "Transfer Learning to Learn with Multitask Neural Model Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models require extensive architecture design exploration and\nhyperparameter optimization to perform well on a given task. The exploration of\nthe model design space is often made by a human expert, and optimized using a\ncombination of grid search and search heuristics over a large space of possible\nchoices. Neural Architecture Search (NAS) is a Reinforcement Learning approach\nthat has been proposed to automate architecture design. NAS has been\nsuccessfully applied to generate Neural Networks that rival the best\nhuman-designed architectures. However, NAS requires sampling, constructing, and\ntraining hundreds to thousands of models to achieve well-performing\narchitectures. This procedure needs to be executed from scratch for each new\ntask. The application of NAS to a wide set of tasks currently lacks a way to\ntransfer generalizable knowledge across tasks. In this paper, we present the\nMultitask Neural Model Search (MNMS) controller. Our goal is to learn a\ngeneralizable framework that can condition model construction on successful\nmodel searches for previously seen tasks, thus significantly speeding up the\nsearch for new tasks. We demonstrate that MNMS can conduct an automated\narchitecture search for multiple tasks simultaneously while still learning\nwell-performing, specialized models for each task. We then show that\npre-trained MNMS controllers can transfer learning to new tasks. By leveraging\nknowledge from previous searches, we find that pre-trained MNMS models start\nfrom a better location in the search space and reduce search time on unseen\ntasks, while still discovering models that outperform published human-designed\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 05:32:50 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Wong", "Catherine", ""], ["Gesmundo", "Andrea", ""]]}, {"id": "1710.10777", "submitter": "Yao Ming", "authors": "Yao Ming and Shaozu Cao and Ruixiang Zhang and Zhen Li and Yuanzhe\n  Chen and Yangqiu Song and Huamin Qu", "title": "Understanding Hidden Memories of Recurrent Neural Networks", "comments": "Published at IEEE Conference on Visual Analytics Science and\n  Technology (IEEE VAST 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have been successfully applied to various\nnatural language processing (NLP) tasks and achieved better results than\nconventional methods. However, the lack of understanding of the mechanisms\nbehind their effectiveness limits further improvements on their architectures.\nIn this paper, we present a visual analytics method for understanding and\ncomparing RNN models for NLP tasks. We propose a technique to explain the\nfunction of individual hidden state units based on their expected response to\ninput texts. We then co-cluster hidden state units and words based on the\nexpected response and visualize co-clustering results as memory chips and word\nclouds to provide more structured knowledge on RNNs' hidden states. We also\npropose a glyph-based sequence visualization based on aggregate information to\nanalyze the behavior of an RNN's hidden state at the sentence-level. The\nusability and effectiveness of our method are demonstrated through case studies\nand reviews from domain experts.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 05:37:25 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Ming", "Yao", ""], ["Cao", "Shaozu", ""], ["Zhang", "Ruixiang", ""], ["Li", "Zhen", ""], ["Chen", "Yuanzhe", ""], ["Song", "Yangqiu", ""], ["Qu", "Huamin", ""]]}, {"id": "1710.10824", "submitter": "Shuliang Xu", "authors": "Lin Feng, Shuliang Xu, Feilong Wang, Shenglan Liu", "title": "Rough extreme learning machine: a new classification method based on\n  uncertainty measure", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extreme learning machine (ELM) is a new single hidden layer feedback neural\nnetwork. The weights of the input layer and the biases of neurons in hidden\nlayer are randomly generated, the weights of the output layer can be\nanalytically determined. ELM has been achieved good results for a large number\nof classification tasks. In this paper, a new extreme learning machine called\nrough extreme learning machine (RELM) was proposed. RELM uses rough set to\ndivide data into upper approximation set and lower approximation set, and the\ntwo approximation sets are utilized to train upper approximation neurons and\nlower approximation neurons. In addition, an attribute reduction is executed in\nthis algorithm to remove redundant attributes. The experimental results showed,\ncomparing with the comparison algorithms, RELM can get a better accuracy and\nrepeatability in most cases, RELM can not only maintain the advantages of fast\nspeed, but also effectively cope with the classification task for\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 09:37:20 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 09:03:36 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Feng", "Lin", ""], ["Xu", "Shuliang", ""], ["Wang", "Feilong", ""], ["Liu", "Shenglan", ""]]}, {"id": "1710.10903", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Petar Veli\\v{c}kovi\\'c, Guillem Cucurull, Arantxa Casanova, Adriana\n  Romero, Pietro Li\\`o, Yoshua Bengio", "title": "Graph Attention Networks", "comments": "To appear at ICLR 2018. 12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present graph attention networks (GATs), novel neural network\narchitectures that operate on graph-structured data, leveraging masked\nself-attentional layers to address the shortcomings of prior methods based on\ngraph convolutions or their approximations. By stacking layers in which nodes\nare able to attend over their neighborhoods' features, we enable (implicitly)\nspecifying different weights to different nodes in a neighborhood, without\nrequiring any kind of costly matrix operation (such as inversion) or depending\non knowing the graph structure upfront. In this way, we address several key\nchallenges of spectral-based graph neural networks simultaneously, and make our\nmodel readily applicable to inductive as well as transductive problems. Our GAT\nmodels have achieved or matched state-of-the-art results across four\nestablished transductive and inductive graph benchmarks: the Cora, Citeseer and\nPubmed citation network datasets, as well as a protein-protein interaction\ndataset (wherein test graphs remain unseen during training).\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 12:41:12 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 11:18:15 GMT"}, {"version": "v3", "created": "Sun, 4 Feb 2018 19:13:29 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Veli\u010dkovi\u0107", "Petar", ""], ["Cucurull", "Guillem", ""], ["Casanova", "Arantxa", ""], ["Romero", "Adriana", ""], ["Li\u00f2", "Pietro", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1710.10916", "submitter": "Han Zhang", "authors": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang,\n  Xiaolei Huang, Dimitris Metaxas", "title": "StackGAN++: Realistic Image Synthesis with Stacked Generative\n  Adversarial Networks", "comments": "In IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI),\n  2018. (16 pages, 15 figures.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Generative Adversarial Networks (GANs) have shown remarkable success\nin various tasks, they still face challenges in generating high quality images.\nIn this paper, we propose Stacked Generative Adversarial Networks (StackGAN)\naiming at generating high-resolution photo-realistic images. First, we propose\na two-stage generative adversarial network architecture, StackGAN-v1, for\ntext-to-image synthesis. The Stage-I GAN sketches the primitive shape and\ncolors of the object based on given text description, yielding low-resolution\nimages. The Stage-II GAN takes Stage-I results and text descriptions as inputs,\nand generates high-resolution images with photo-realistic details. Second, an\nadvanced multi-stage generative adversarial network architecture, StackGAN-v2,\nis proposed for both conditional and unconditional generative tasks. Our\nStackGAN-v2 consists of multiple generators and discriminators in a tree-like\nstructure; images at multiple scales corresponding to the same scene are\ngenerated from different branches of the tree. StackGAN-v2 shows more stable\ntraining behavior than StackGAN-v1 by jointly approximating multiple\ndistributions. Extensive experiments demonstrate that the proposed stacked\ngenerative adversarial networks significantly outperform other state-of-the-art\nmethods in generating photo-realistic images.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 18:45:59 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 06:43:57 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 00:49:19 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Zhang", "Han", ""], ["Xu", "Tao", ""], ["Li", "Hongsheng", ""], ["Zhang", "Shaoting", ""], ["Wang", "Xiaogang", ""], ["Huang", "Xiaolei", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "1710.10928", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen and Matthias Hein", "title": "Optimization Landscape and Expressivity of Deep CNNs", "comments": "Accepted at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the loss landscape and expressiveness of practical deep\nconvolutional neural networks (CNNs) with shared weights and max pooling\nlayers. We show that such CNNs produce linearly independent features at a\n\"wide\" layer which has more neurons than the number of training samples. This\ncondition holds e.g. for the VGG network. Furthermore, we provide for such wide\nCNNs necessary and sufficient conditions for global minima with zero training\nerror. For the case where the wide layer is followed by a fully connected layer\nwe show that almost every critical point of the empirical loss is a global\nminimum with zero training error. Our analysis suggests that both depth and\nwidth are very important in deep learning. While depth brings more\nrepresentational power and allows the network to learn high level features,\nwidth smoothes the optimization landscape of the loss function in the sense\nthat a sufficiently wide network has a well-behaved loss surface with almost no\nbad local minima.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 13:24:28 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 12:49:58 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Nguyen", "Quynh", ""], ["Hein", "Matthias", ""]]}, {"id": "1710.10967", "submitter": "Mitsuru Igami", "authors": "Mitsuru Igami", "title": "Artificial Intelligence as Structural Estimation: Economic\n  Interpretations of Deep Blue, Bonanza, and AlphaGo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) has achieved superhuman performance in a growing\nnumber of tasks, but understanding and explaining AI remain challenging. This\npaper clarifies the connections between machine-learning algorithms to develop\nAIs and the econometrics of dynamic structural models through the case studies\nof three famous game AIs. Chess-playing Deep Blue is a calibrated value\nfunction, whereas shogi-playing Bonanza is an estimated value function via\nRust's (1987) nested fixed-point method. AlphaGo's \"supervised-learning policy\nnetwork\" is a deep neural network implementation of Hotz and Miller's (1993)\nconditional choice probability estimation; its \"reinforcement-learning value\nnetwork\" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional\nchoice simulation method. Relaxing these AIs' implicit econometric assumptions\nwould improve their structural interpretability.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 14:25:39 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 22:48:17 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 20:52:33 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Igami", "Mitsuru", ""]]}, {"id": "1710.11040", "submitter": "Anirudha Majumdar", "authors": "Anirudha Majumdar and Marco Pavone", "title": "How Should a Robot Assess Risk? Towards an Axiomatic Theory of Risk in\n  Robotics", "comments": "Extended version of paper published in International Symposium on\n  Robotics Research (ISRR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endowing robots with the capability of assessing risk and making risk-aware\ndecisions is widely considered a key step toward ensuring safety for robots\noperating under uncertainty. But, how should a robot quantify risk? A natural\nand common approach is to consider the framework whereby costs are assigned to\nstochastic outcomes - an assignment captured by a cost random variable.\nQuantifying risk then corresponds to evaluating a risk metric, i.e., a mapping\nfrom the cost random variable to a real number. Yet, the question of what\nconstitutes a \"good\" risk metric has received little attention within the\nrobotics community. The goal of this paper is to explore and partially address\nthis question by advocating axioms that risk metrics in robotics applications\nshould satisfy in order to be employed as rational assessments of risk. We\ndiscuss general representation theorems that precisely characterize the class\nof metrics that satisfy these axioms (referred to as distortion risk metrics),\nand provide instantiations that can be used in applications. We further discuss\npitfalls of commonly used risk metrics in robotics, and discuss additional\nproperties that one must consider in sequential decision making tasks. Our hope\nis that the ideas presented here will lead to a foundational framework for\nquantifying risk (and hence safety) in robotics applications.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 16:17:26 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 17:59:48 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Majumdar", "Anirudha", ""], ["Pavone", "Marco", ""]]}, {"id": "1710.11041", "submitter": "Mikel Artetxe", "authors": "Mikel Artetxe, Gorka Labaka, Eneko Agirre, Kyunghyun Cho", "title": "Unsupervised Neural Machine Translation", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the recent success of neural machine translation (NMT) in\nstandard benchmarks, the lack of large parallel corpora poses a major practical\nproblem for many language pairs. There have been several proposals to alleviate\nthis issue with, for instance, triangulation and semi-supervised learning\ntechniques, but they still require a strong cross-lingual signal. In this work,\nwe completely remove the need of parallel data and propose a novel method to\ntrain an NMT system in a completely unsupervised manner, relying on nothing but\nmonolingual corpora. Our model builds upon the recent work on unsupervised\nembedding mappings, and consists of a slightly modified attentional\nencoder-decoder model that can be trained on monolingual corpora alone using a\ncombination of denoising and backtranslation. Despite the simplicity of the\napproach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014\nFrench-to-English and German-to-English translation. The model can also profit\nfrom small parallel corpora, and attains 21.81 and 15.24 points when combined\nwith 100,000 parallel sentences, respectively. Our implementation is released\nas an open source project.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 16:17:34 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 16:54:14 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Artetxe", "Mikel", ""], ["Labaka", "Gorka", ""], ["Agirre", "Eneko", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1710.11054", "submitter": "Rishabh Singh", "authors": "Jacob Devlin, Jonathan Uesato, Rishabh Singh, Pushmeet Kohli", "title": "Semantic Code Repair using Neuro-Symbolic Transformation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of semantic code repair, which can be broadly defined as\nautomatically fixing non-syntactic bugs in source code. The majority of past\nwork in semantic code repair assumed access to unit tests against which\ncandidate repairs could be validated. In contrast, the goal here is to develop\na strong statistical model to accurately predict both bug locations and exact\nfixes without access to information about the intended correct behavior of the\nprogram. Achieving such a goal requires a robust contextual repair model, which\nwe train on a large corpus of real-world source code that has been augmented\nwith synthetically injected bugs. Our framework adopts a two-stage approach\nwhere first a large set of repair candidates are generated by rule-based\nprocessors, and then these candidates are scored by a statistical model using a\nnovel neural network architecture which we refer to as Share, Specialize, and\nCompete. Specifically, the architecture (1) generates a shared encoding of the\nsource code using an RNN over the abstract syntax tree, (2) scores each\ncandidate repair using specialized network modules, and (3) then normalizes\nthese scores together so they can compete against one another in comparable\nprobability space. We evaluate our model on a real-world test set gathered from\nGitHub containing four common categories of bugs. Our model is able to predict\nthe exact correct repair 41\\% of the time with a single guess, compared to 13\\%\naccuracy for an attentional sequence-to-sequence model.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 16:32:45 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Devlin", "Jacob", ""], ["Uesato", "Jonathan", ""], ["Singh", "Rishabh", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1710.11089", "submitter": "Marlos C. Machado", "authors": "Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald\n  Tesauro, Murray Campbell", "title": "Eigenoption Discovery through the Deep Successor Representation", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Options in reinforcement learning allow agents to hierarchically decompose a\ntask into subtasks, having the potential to speed up learning and planning.\nHowever, autonomously learning effective sets of options is still a major\nchallenge in the field. In this paper we focus on the recently introduced idea\nof using representation learning methods to guide the option discovery process.\nSpecifically, we look at eigenoptions, options obtained from representations\nthat encode diffusive information flow in the environment. We extend the\nexisting algorithms for eigenoption discovery to settings with stochastic\ntransitions and in which handcrafted features are not available. We propose an\nalgorithm that discovers eigenoptions while learning non-linear state\nrepresentations from raw pixels. It exploits recent successes in the deep\nreinforcement learning literature and the equivalence between proto-value\nfunctions and the successor representation. We use traditional tabular domains\nto provide intuition about our approach and Atari 2600 games to demonstrate its\npotential.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 17:36:19 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 01:48:36 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 21:55:05 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Machado", "Marlos C.", ""], ["Rosenbaum", "Clemens", ""], ["Guo", "Xiaoxiao", ""], ["Liu", "Miao", ""], ["Tesauro", "Gerald", ""], ["Campbell", "Murray", ""]]}, {"id": "1710.11160", "submitter": "Vedran Dunjko", "authors": "Vedran Dunjko, Yi-Kai Liu, Xingyao Wu, Jacob M. Taylor", "title": "Exponential improvements for quantum-accessible reinforcement learning", "comments": "14+13 pages, 4 figures; The updated version is simplified, and more\n  concisely presented", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computers can offer dramatic improvements over classical devices for\ndata analysis tasks such as prediction and classification. However, less is\nknown about the advantages that quantum computers may bring in the setting of\nreinforcement learning, where learning is achieved via interaction with a task\nenvironment. Here, we consider a special case of reinforcement learning, where\nthe task environment allows quantum access. In addition, we impose certain\n\"naturalness\" conditions on the task environment, which rule out the kinds of\noracle problems that are studied in quantum query complexity (and for which\nquantum speedups are well-known). Within this framework of quantum-accessible\nreinforcement learning environments, we demonstrate that quantum agents can\nachieve exponential improvements in learning efficiency, surpassing previous\nresults that showed only quadratic improvements. A key step in the proof is to\nconstruct task environments that encode well-known oracle problems, such as\nSimon's problem and Recursive Fourier Sampling, while satisfying the above\n\"naturalness\" conditions for reinforcement learning. Our results suggest that\nquantum agents may perform well in certain game-playing scenarios, where the\ngame has recursive structure, and the agent can learn by playing against\nitself.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 18:12:10 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 11:49:27 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 18:30:44 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Dunjko", "Vedran", ""], ["Liu", "Yi-Kai", ""], ["Wu", "Xingyao", ""], ["Taylor", "Jacob M.", ""]]}, {"id": "1710.11169", "submitter": "Xiang Ren", "authors": "Zeqiu Wu, Xiang Ren, Frank F. Xu, Ji Li, Jiawei Han", "title": "Indirect Supervision for Relation Extraction using Question-Answer Pairs", "comments": "9 pages + 1 page reference. Accepted to WSDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic relation extraction (RE) for types of interest is of great\nimportance for interpreting massive text corpora in an efficient manner.\nTraditional RE models have heavily relied on human-annotated corpus for\ntraining, which can be costly in generating labeled data and become obstacles\nwhen dealing with more relation types. Thus, more RE extraction systems have\nshifted to be built upon training data automatically acquired by linking to\nknowledge bases (distant supervision). However, due to the incompleteness of\nknowledge bases and the context-agnostic labeling, the training data collected\nvia distant supervision (DS) can be very noisy. In recent years, as increasing\nattention has been brought to tackling question-answering (QA) tasks, user\nfeedback or datasets of such tasks become more accessible. In this paper, we\npropose a novel framework, ReQuest, to leverage question-answer pairs as an\nindirect source of supervision for relation extraction, and study how to use\nsuch supervision to reduce noise induced from DS. Our model jointly embeds\nrelation mentions, types, QA entity mention pairs and text features in two\nlow-dimensional spaces (RE and QA), where objects with same relation types or\nsemantically similar question-answer pairs have similar representations. Shared\nfeatures connect these two spaces, carrying clearer semantic knowledge from\nboth sources. ReQuest, then use these learned embeddings to estimate the types\nof test relation mentions. We formulate a global objective function and adopt a\nnovel margin-based QA loss to reduce noise in DS by exploiting semantic\nevidence from the QA dataset. Our experimental results achieve an average of\n11% improvement in F1 score on two public RE datasets combined with TREC QA\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 18:27:19 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 07:43:31 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Wu", "Zeqiu", ""], ["Ren", "Xiang", ""], ["Xu", "Frank F.", ""], ["Li", "Ji", ""], ["Han", "Jiawei", ""]]}, {"id": "1710.11204", "submitter": "Haoze Wu", "authors": "Haoze Wu", "title": "Improve SAT-solving with Machine Learning", "comments": "2 pages, SIGCSE SRC 2017", "journal-ref": "In Proceedings of the 2017 ACM SIGCSE Technical Symposium on\n  Computer Science Education (SIGCSE '17). ACM, New York, NY, USA, 787-788\n  (2017)", "doi": "10.1145/3017680.3022464", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we aimed to improve the runtime of Minisat, a\nConflict-Driven Clause Learning (CDCL) solver that solves the Propositional\nBoolean Satisfiability (SAT) problem. We first used a logistic regression model\nto predict the satisfiability of propositional boolean formulae after fixing\nthe values of a certain fraction of the variables in each formula. We then\napplied the logistic model and added a preprocessing period to Minisat to\ndetermine the preferable initial value (either true or false) of each boolean\nvariable using a Monte-Carlo approach. Concretely, for each Monte-Carlo trial,\nwe fixed the values of a certain ratio of randomly selected variables, and\ncalculated the confidence that the resulting sub-formula is satisfiable with\nour logistic regression model. The initial value of each variable was set based\non the mean confidence scores of the trials that started from the literals of\nthat variable. We were particularly interested in setting the initial values of\nthe backbone variables correctly, which are variables that have the same value\nin all solutions of a SAT formula. Our Monte-Carlo method was able to set 78%\nof the backbones correctly. Excluding the preprocessing time, compared with the\ndefault setting of Minisat, the runtime of Minisat for satisfiable formulae\ndecreased by 23%. However, our method did not outperform vanilla Minisat in\nruntime, as the decrease in the conflicts was outweighed by the long runtime of\nthe preprocessing period.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 19:13:28 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Wu", "Haoze", ""]]}, {"id": "1710.11223", "submitter": "Yanjun  Qi Dr.", "authors": "Beilun Wang and Arshdeep Sekhon and Yanjun Qi", "title": "Fast and Scalable Learning of Sparse Changes in High-Dimensional\n  Gaussian Graphical Model Structure", "comments": "20pages, 6 figures, 10 tables; at AISTAT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of estimating the change in the dependency structures\nof two $p$-dimensional Gaussian Graphical models (GGMs). Previous studies for\nsparse change estimation in GGMs involve expensive and difficult non-smooth\noptimization. We propose a novel method, DIFFEE for estimating DIFFerential\nnetworks via an Elementary Estimator under a high-dimensional situation. DIFFEE\nis solved through a faster and closed form solution that enables it to work in\nlarge-scale settings. We conduct a rigorous statistical analysis showing that\nsurprisingly DIFFEE achieves the same asymptotic convergence rates as the\nstate-of-the-art estimators that are much more difficult to compute. Our\nexperimental results on multiple synthetic datasets and one real-world data\nabout brain connectivity show strong performance improvements over baselines,\nas well as significant computational benefits.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 20:15:20 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 16:14:25 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 17:57:26 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Wang", "Beilun", ""], ["Sekhon", "Arshdeep", ""], ["Qi", "Yanjun", ""]]}, {"id": "1710.11238", "submitter": "Yanjun  Qi Dr.", "authors": "Jack Lanchantin, Arshdeep Sekhon, Ritambhara Singh, Yanjun Qi", "title": "Prototype Matching Networks for Large-Scale Multi-label Genomic Sequence\n  Classification", "comments": "15 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental tasks in understanding genomics is the problem of\npredicting Transcription Factor Binding Sites (TFBSs). With more than hundreds\nof Transcription Factors (TFs) as labels, genomic-sequence based TFBS\nprediction is a challenging multi-label classification task. There are two\nmajor biological mechanisms for TF binding: (1) sequence-specific binding\npatterns on genomes known as \"motifs\" and (2) interactions among TFs known as\nco-binding effects. In this paper, we propose a novel deep architecture, the\nPrototype Matching Network (PMN) to mimic the TF binding mechanisms. Our PMN\nmodel automatically extracts prototypes (\"motif\"-like features) for each TF\nthrough a novel prototype-matching loss. Borrowing ideas from few-shot matching\nmodels, we use the notion of support set of prototypes and an LSTM to learn how\nTFs interact and bind to genomic sequences. On a reference TFBS dataset with\n$2.1$ $million$ genomic sequences, PMN significantly outperforms baselines and\nvalidates our design choices empirically. To our knowledge, this is the first\ndeep learning architecture that introduces prototype learning and considers\nTF-TF interactions for large-scale TFBS prediction. Not only is the proposed\narchitecture accurate, but it also models the underlying biology.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 21:04:49 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 16:47:26 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Lanchantin", "Jack", ""], ["Sekhon", "Arshdeep", ""], ["Singh", "Ritambhara", ""], ["Qi", "Yanjun", ""]]}, {"id": "1710.11277", "submitter": "Xiujun Li", "authors": "Baolin Peng and Xiujun Li and Jianfeng Gao and Jingjing Liu and\n  Yun-Nung Chen and Kam-Fai Wong", "title": "Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue\n  Policy Learning", "comments": "5 pages, 3 figures, ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method --- adversarial advantage actor-critic\n(Adversarial A2C), which significantly improves the efficiency of dialogue\npolicy learning in task-completion dialogue systems. Inspired by generative\nadversarial networks (GAN), we train a discriminator to differentiate\nresponses/actions generated by dialogue agents from responses/actions by\nexperts. Then, we incorporate the discriminator as another critic into the\nadvantage actor-critic (A2C) framework, to encourage the dialogue agent to\nexplore state-action within the regions where the agent takes actions similar\nto those of the experts. Experimental results in a movie-ticket booking domain\nshow that the proposed Adversarial A2C can accelerate policy exploration\nefficiently.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 00:25:03 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 18:41:05 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Peng", "Baolin", ""], ["Li", "Xiujun", ""], ["Gao", "Jianfeng", ""], ["Liu", "Jingjing", ""], ["Chen", "Yun-Nung", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "1710.11311", "submitter": "Alexander Lambert", "authors": "Alexander Lambert, Amirreza Shaban, Amit Raj, Zhen Liu and Byron Boots", "title": "Deep Forward and Inverse Perceptual Models for Tracking and Prediction", "comments": "8 pages, International Conference on Robotics and Automation (ICRA)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of learning forward models that map state to\nhigh-dimensional images and inverse models that map high-dimensional images to\nstate in robotics. Specifically, we present a perceptual model for generating\nvideo frames from state with deep networks, and provide a framework for its use\nin tracking and prediction tasks. We show that our proposed model greatly\noutperforms standard deconvolutional methods and GANs for image generation,\nproducing clear, photo-realistic images. We also develop a convolutional neural\nnetwork model for state estimation and compare the result to an Extended Kalman\nFilter to estimate robot trajectories. We validate all models on a real robotic\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 03:35:03 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 02:30:14 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Lambert", "Alexander", ""], ["Shaban", "Amirreza", ""], ["Raj", "Amit", ""], ["Liu", "Zhen", ""], ["Boots", "Byron", ""]]}, {"id": "1710.11342", "submitter": "Zhengli Zhao", "authors": "Zhengli Zhao, Dheeru Dua, Sameer Singh", "title": "Generating Natural Adversarial Examples", "comments": "Published as a conference paper at the International Conference on\n  Learning Representations (ICLR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their complex nature, it is hard to characterize the ways in which\nmachine learning models can misbehave or be exploited when deployed. Recent\nwork on adversarial examples, i.e. inputs with minor perturbations that result\nin substantially different model predictions, is helpful in evaluating the\nrobustness of these models by exposing the adversarial scenarios where they\nfail. However, these malicious perturbations are often unnatural, not\nsemantically meaningful, and not applicable to complicated domains such as\nlanguage. In this paper, we propose a framework to generate natural and legible\nadversarial examples that lie on the data manifold, by searching in semantic\nspace of dense and continuous data representation, utilizing the recent\nadvances in generative adversarial networks. We present generated adversaries\nto demonstrate the potential of the proposed approach for black-box classifiers\nfor a wide range of applications such as image classification, textual\nentailment, and machine translation. We include experiments to show that the\ngenerated adversaries are natural, legible to humans, and useful in evaluating\nand analyzing black-box classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 06:22:26 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 23:28:31 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Zhao", "Zhengli", ""], ["Dua", "Dheeru", ""], ["Singh", "Sameer", ""]]}, {"id": "1710.11386", "submitter": "Yannic Kilcher", "authors": "Yannic Kilcher, Gary Becigneul, Thomas Hofmann", "title": "Parametrizing filters of a CNN with a GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly agreed that the use of relevant invariances as a good\nstatistical bias is important in machine-learning. However, most approaches\nthat explicitly incorporate invariances into a model architecture only make use\nof very simple transformations, such as translations and rotations. Hence,\nthere is a need for methods to model and extract richer transformations that\ncapture much higher-level invariances. To that end, we introduce a tool\nallowing to parametrize the set of filters of a trained convolutional neural\nnetwork with the latent space of a generative adversarial network. We then show\nthat the method can capture highly non-linear invariances of the data by\nvisualizing their effect in the data space.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 09:24:39 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Kilcher", "Yannic", ""], ["Becigneul", "Gary", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1710.11417", "submitter": "Gregory Farquhar", "authors": "Gregory Farquhar, Tim Rockt\\\"aschel, Maximilian Igl, Shimon Whiteson", "title": "TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining deep model-free reinforcement learning with on-line planning is a\npromising approach to building on the successes of deep RL. On-line planning\nwith look-ahead trees has proven successful in environments where transition\nmodels are known a priori. However, in complex environments where transition\nmodels need to be learned from data, the deficiencies of learned models have\nlimited their utility for planning. To address these challenges, we propose\nTreeQN, a differentiable, recursive, tree-structured model that serves as a\ndrop-in replacement for any value function network in deep RL with discrete\nactions. TreeQN dynamically constructs a tree by recursively applying a\ntransition model in a learned abstract state space and then aggregating\npredicted rewards and state-values using a tree backup to estimate Q-values. We\nalso propose ATreeC, an actor-critic variant that augments TreeQN with a\nsoftmax layer to form a stochastic policy network. Both approaches are trained\nend-to-end, such that the learned model is optimised for its actual use in the\ntree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a\nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et\nal. 2017) on multiple Atari games. Furthermore, we present ablation studies\nthat demonstrate the effect of different auxiliary losses on learning\ntransition models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 11:54:35 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 17:30:48 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Farquhar", "Gregory", ""], ["Rockt\u00e4schel", "Tim", ""], ["Igl", "Maximilian", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1710.11424", "submitter": "Peter Jin", "authors": "Peter Jin, Kurt Keutzer, Sergey Levine", "title": "Regret Minimization for Partially Observable Deep Reinforcement Learning", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep reinforcement learning algorithms that estimate state and state-action\nvalue functions have been shown to be effective in a variety of challenging\ndomains, including learning control strategies from raw image pixels. However,\nalgorithms that estimate state and state-action value functions typically\nassume a fully observed state and must compensate for partial observations by\nusing finite length observation histories or recurrent networks. In this work,\nwe propose a new deep reinforcement learning algorithm based on counterfactual\nregret minimization that iteratively updates an approximation to an\nadvantage-like function and is robust to partially observed state. We\ndemonstrate that this new algorithm can substantially outperform strong\nbaseline methods on several partially observed reinforcement learning tasks:\nlearning first-person 3D navigation in Doom and Minecraft, and acting in the\npresence of partially observed objects in Doom and Pong.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 12:15:38 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 00:58:42 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Jin", "Peter", ""], ["Keutzer", "Kurt", ""], ["Levine", "Sergey", ""]]}, {"id": "1710.11431", "submitter": "Anuj Karpatne", "authors": "Anuj Karpatne, William Watkins, Jordan Read, and Vipin Kumar", "title": "Physics-guided Neural Networks (PGNN): An Application in Lake\n  Temperature Modeling", "comments": "submitted to ACM SIGKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel framework for combining scientific knowledge of\nphysics-based models with neural networks to advance scientific discovery. This\nframework, termed as physics-guided neural network (PGNN), leverages the output\nof physics-based model simulations along with observational features to\ngenerate predictions using a neural network architecture. Further, this paper\npresents a novel framework for using physics-based loss functions in the\nlearning objective of neural networks, to ensure that the model predictions not\nonly show lower errors on the training set but are also scientifically\nconsistent with the known physics on the unlabeled set. We illustrate the\neffectiveness of PGNN for the problem of lake temperature modeling, where\nphysical relationships between the temperature, density, and depth of water are\nused to design a physics-based loss function. By using scientific knowledge to\nguide the construction and learning of neural networks, we are able to show\nthat the proposed framework ensures better generalizability as well as\nscientific consistency of results.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 12:24:26 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 17:33:48 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Karpatne", "Anuj", ""], ["Watkins", "William", ""], ["Read", "Jordan", ""], ["Kumar", "Vipin", ""]]}, {"id": "1710.11531", "submitter": "Varish Mulwad", "authors": "Paul Cuddihy, Justin McHugh, Jenny Weisenberg Williams, Varish Mulwad,\n  Kareem S. Aggour", "title": "SemTK: An Ontology-first, Open Source Semantic Toolkit for Managing and\n  Querying Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relatively recent adoption of Knowledge Graphs as an enabling technology\nin multiple high-profile artificial intelligence and cognitive applications has\nled to growing interest in the Semantic Web technology stack. Many\nsemantics-related tools, however, are focused on serving experts with a deep\nunderstanding of semantic technologies. For example, triplification of\nrelational data is available but there is no open source tool that allows a\nuser unfamiliar with OWL/RDF to import data into a semantic triple store in an\nintuitive manner. Further, many tools require users to have a working\nunderstanding of SPARQL to query data. Casual users interested in benefiting\nfrom the power of Knowledge Graphs have few tools available for exploring,\nquerying, and managing semantic data. We present SemTK, the Semantics Toolkit,\na user-friendly suite of tools that allow both expert and non-expert semantics\nusers convenient ingestion of relational data, simplified query generation, and\nmore. The exploration of ontologies and instance data is performed through\nSPARQLgraph, an intuitive web-based user interface in SemTK understandable and\nnavigable by a lay user. The open source version of SemTK is available at\nhttp://semtk.research.ge.com\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 15:29:35 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 01:52:39 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Cuddihy", "Paul", ""], ["McHugh", "Justin", ""], ["Williams", "Jenny Weisenberg", ""], ["Mulwad", "Varish", ""], ["Aggour", "Kareem S.", ""]]}, {"id": "1710.11577", "submitter": "Guokun Lai", "authors": "Guokun Lai, Hanxiao Liu, Yiming Yang", "title": "Learning Depthwise Separable Graph Convolution from Data Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution Neural Network (CNN) has gained tremendous success in computer\nvision tasks with its outstanding ability to capture the local latent features.\nRecently, there has been an increasing interest in extending convolution\noperations to the non-Euclidean geometry. Although various types of convolution\noperations have been proposed for graphs or manifolds, their connections with\ntraditional convolution over grid-structured data are not well-understood. In\nthis paper, we show that depthwise separable convolution can be successfully\ngeneralized for the unification of both graph-based and grid-based convolution\nmethods. Based on this insight we propose a novel Depthwise Separable Graph\nConvolution (DSGC) approach which is compatible with the tradition convolution\nnetwork and subsumes existing convolution methods as special cases. It is\nequipped with the combined strengths in model expressiveness, compatibility\n(relatively small number of parameters), modularity and computational\nefficiency in training. Extensive experiments show the outstanding performance\nof DSGC in comparison with strong baselines on multi-domain benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 16:48:12 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 18:32:25 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2018 05:58:53 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Lai", "Guokun", ""], ["Liu", "Hanxiao", ""], ["Yang", "Yiming", ""]]}, {"id": "1710.11601", "submitter": "Lea Frermann", "authors": "Lea Frermann and Shay B. Cohen and Mirella Lapata", "title": "Whodunnit? Crime Drama as a Case for Natural Language Understanding", "comments": "To appear in Transactions of the Association for Computational\n  Linguistics (TACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we argue that crime drama exemplified in television programs\nsuch as CSI:Crime Scene Investigation is an ideal testbed for approximating\nreal-world natural language understanding and the complex inferences associated\nwith it. We propose to treat crime drama as a new inference task, capitalizing\non the fact that each episode poses the same basic question (i.e., who\ncommitted the crime) and naturally provides the answer when the perpetrator is\nrevealed. We develop a new dataset based on CSI episodes, formalize perpetrator\nidentification as a sequence labeling problem, and develop an LSTM-based model\nwhich learns from multi-modal data. Experimental results show that an\nincremental inference strategy is key to making accurate guesses as well as\nlearning from representations fusing textual, visual, and acoustic input.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 17:27:44 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Frermann", "Lea", ""], ["Cohen", "Shay B.", ""], ["Lapata", "Mirella", ""]]}, {"id": "1710.11622", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Sergey Levine", "title": "Meta-Learning and Universality: Deep Representations and Gradient\n  Descent can Approximate any Learning Algorithm", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to learn is a powerful paradigm for enabling models to learn from\ndata more effectively and efficiently. A popular approach to meta-learning is\nto train a recurrent model to read in a training dataset as input and output\nthe parameters of a learned model, or output predictions for new test inputs.\nAlternatively, a more recent approach to meta-learning aims to acquire deep\nrepresentations that can be effectively fine-tuned, via standard gradient\ndescent, to new tasks. In this paper, we consider the meta-learning problem\nfrom the perspective of universality, formalizing the notion of learning\nalgorithm approximation and comparing the expressive power of the\naforementioned recurrent models to the more recent approaches that embed\ngradient descent into the meta-learner. In particular, we seek to answer the\nfollowing question: does deep representation combined with standard gradient\ndescent have sufficient capacity to approximate any learning algorithm? We find\nthat this is indeed true, and further find, in our experiments, that\ngradient-based meta-learning consistently leads to learning strategies that\ngeneralize more widely compared to those represented by recurrent models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 17:55:42 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 01:38:51 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 19:16:20 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}]