[{"id": "0808.0056", "submitter": "Emanuel Diamant", "authors": "Emanuel Diamant", "title": "I'm sorry to say, but your understanding of image processing\n  fundamentals is absolutely wrong", "comments": "To be published as chapter 5 in \"Frontiers in Brain, Vision and AI\",\n  I-TECH Publisher, Viena, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.IR cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing discussion whether modern vision systems have to be viewed as\nvisually-enabled cognitive systems or cognitively-enabled vision systems is\ngroundless, because perceptual and cognitive faculties of vision are separate\ncomponents of human (and consequently, artificial) information processing\nsystem modeling.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2008 04:45:17 GMT"}], "update_date": "2008-09-25", "authors_parsed": [["Diamant", "Emanuel", ""]]}, {"id": "0808.0112", "submitter": "Didier Sornette", "authors": "V.I. Yukalov and D. Sornette", "title": "Mathematical Structure of Quantum Decision Theory", "comments": "40 pages", "journal-ref": "Advances in Complex Systems 13, 659-698 (2010)", "doi": null, "report-no": null, "categories": "cs.AI math-ph math.MP quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most complex systems is the human brain whose formalized\nfunctioning is characterized by decision theory. We present a \"Quantum Decision\nTheory\" of decision making, based on the mathematical theory of separable\nHilbert spaces. This mathematical structure captures the effect of\nsuperposition of composite prospects, including many incorporated intentions,\nwhich allows us to explain a variety of interesting fallacies and anomalies\nthat have been reported to particularize the decision making of real human\nbeings. The theory describes entangled decision making, non-commutativity of\nsubsequent decisions, and intention interference of composite prospects. We\ndemonstrate how the violation of the Savage's sure-thing principle (disjunction\neffect) can be explained as a result of the interference of intentions, when\nmaking decisions under uncertainty. The conjunction fallacy is also explained\nby the presence of the interference terms. We demonstrate that all known\nanomalies and paradoxes, documented in the context of classical decision\ntheory, are reducible to just a few mathematical archetypes, all of which\nfinding straightforward explanations in the frame of the developed quantum\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2008 13:14:20 GMT"}, {"version": "v2", "created": "Wed, 27 Oct 2010 09:05:25 GMT"}, {"version": "v3", "created": "Thu, 28 Oct 2010 10:28:03 GMT"}], "update_date": "2010-10-29", "authors_parsed": [["Yukalov", "V. I.", ""], ["Sornette", "D.", ""]]}, {"id": "0808.0973", "submitter": "Chaitanya Chemudugunta", "authors": "Chaitanya Chemudugunta, Padhraic Smyth and Mark Steyvers", "title": "Text Modeling using Unsupervised Topic Models and Concept Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical topic models provide a general data-driven framework for\nautomated discovery of high-level knowledge from large collections of text\ndocuments. While topic models can potentially discover a broad range of themes\nin a data set, the interpretability of the learned topics is not always ideal.\nHuman-defined concepts, on the other hand, tend to be semantically richer due\nto careful selection of words to define concepts but they tend not to cover the\nthemes in a data set exhaustively. In this paper, we propose a probabilistic\nframework to combine a hierarchy of human-defined semantic concepts with\nstatistical topic models to seek the best of both worlds. Experimental results\nusing two different sources of concept hierarchies and two collections of text\ndocuments indicate that this combination leads to systematic improvements in\nthe quality of the associated language models as well as enabling new\ntechniques for inferring and visualizing the semantics of a document.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2008 07:59:29 GMT"}], "update_date": "2008-08-08", "authors_parsed": [["Chemudugunta", "Chaitanya", ""], ["Smyth", "Padhraic", ""], ["Steyvers", "Mark", ""]]}, {"id": "0808.1125", "submitter": "Omid David-Tabibi", "authors": "Omid David-Tabibi and Nathan S. Netanyahu", "title": "Verified Null-Move Pruning", "comments": "9 pages", "journal-ref": "ICGA Journal, International Computer Games Association, Vol. 25,\n  No. 3, pp. 153--161, September 2002", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we review standard null-move pruning and introduce our\nextended version of it, which we call verified null-move pruning. In verified\nnull-move pruning, whenever the shallow null-move search indicates a fail-high,\ninstead of cutting off the search from the current node, the search is\ncontinued with reduced depth.\n  Our experiments with verified null-move pruning show that on average, it\nconstructs a smaller search tree with greater tactical strength in comparison\nto standard null-move pruning. Moreover, unlike standard null-move pruning,\nwhich fails badly in zugzwang positions, verified null-move pruning manages to\ndetect most zugzwangs and in such cases conducts a re-search to obtain the\ncorrect result. In addition, verified null-move pruning is very easy to\nimplement, and any standard null-move pruning program can use verified\nnull-move pruning by modifying only a few lines of code.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2008 12:44:10 GMT"}], "update_date": "2008-08-11", "authors_parsed": [["David-Tabibi", "Omid", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "0808.1211", "submitter": "W Saba", "authors": "Walid S. Saba", "title": "Commonsense Knowledge, Ontology and Ordinary Language", "comments": "To appear in Int. J. Reasoning-based Intelligent Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over two decades ago a \"quite revolution\" overwhelmingly replaced\nknowledgebased approaches in natural language processing (NLP) by quantitative\n(e.g., statistical, corpus-based, machine learning) methods. Although it is our\nfirm belief that purely quantitative approaches cannot be the only paradigm for\nNLP, dissatisfaction with purely engineering approaches to the construction of\nlarge knowledge bases for NLP are somewhat justified. In this paper we hope to\ndemonstrate that both trends are partly misguided and that the time has come to\nenrich logical semantics with an ontological structure that reflects our\ncommonsense view of the world and the way we talk about in ordinary language.\nIn this paper it will be demonstrated that assuming such an ontological\nstructure a number of challenges in the semantics of natural language (e.g.,\nmetonymy, intensionality, copredication, nominal compounds, etc.) can be\nproperly and uniformly addressed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2008 14:37:45 GMT"}], "update_date": "2008-08-11", "authors_parsed": [["Saba", "Walid S.", ""]]}, {"id": "0808.1508", "submitter": "Michel Rueher", "authors": "H\\'el\\`ene Collavizza (I3S), Michel Rueher (I3S), Pascal Van\n  Hentenryck (Brown University)", "title": "Comparison between CPBPV, ESC/Java, CBMC, Blast, EUREKA and Why for\n  Bounded Program Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes experimental results for a set of benchmarks on program\nverification. It compares the capabilities of CPBVP \"Constraint Programming\nframework for Bounded Program Verification\" [4] with the following frameworks:\nESC/Java, CBMC, Blast, EUREKA and Why.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2008 12:55:19 GMT"}], "update_date": "2008-08-12", "authors_parsed": [["Collavizza", "H\u00e9l\u00e8ne", "", "I3S"], ["Rueher", "Michel", "", "I3S"], ["Van Hentenryck", "Pascal", "", "Brown University"]]}, {"id": "0808.1721", "submitter": "Paul Fodor", "authors": "Paul Fodor", "title": "Initial Results on the F-logic to OWL Bi-directional Translation on a\n  Tabled Prolog Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show our results on the bi-directional data exchange\nbetween the F-logic language supported by the Flora2 system and the OWL\nlanguage. Most of the TBox and ABox axioms are translated preserving the\nsemantics between the two representations, such as: proper inclusion,\nindividual definition, functional properties, while some axioms and\nrestrictions require a change in the semantics, such as: numbered and qualified\ncardinality restrictions. For the second case, we translate the OWL definite\nstyle inference rules into F-logic style constraints. We also describe a set of\nreasoning examples using the above translation, including the reasoning in\nFlora2 of a variety of ABox queries.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2008 19:58:59 GMT"}], "update_date": "2008-08-13", "authors_parsed": [["Fodor", "Paul", ""]]}, {"id": "0808.2984", "submitter": "Sebastien Destercke", "authors": "S\\'ebastien Destercke (IRSN, IRIT), Serge Guillaume (ITAP), Brigitte\n  Charnomordic (ASB)", "title": "Building an interpretable fuzzy rule base from data using Orthogonal\n  Least Squares Application to a depollution problem", "comments": "pre-print of final version published in Fuzzy Sets and Systems", "journal-ref": "Fuzzy Sets and Systems 158, 18 (2007) 2078-2094", "doi": "10.1016/j.fss.2007.04.026", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields where human understanding plays a crucial role, such as\nbioprocesses, the capacity of extracting knowledge from data is of critical\nimportance. Within this framework, fuzzy learning methods, if properly used,\ncan greatly help human experts. Amongst these methods, the aim of orthogonal\ntransformations, which have been proven to be mathematically robust, is to\nbuild rules from a set of training data and to select the most important ones\nby linear regression or rank revealing techniques. The OLS algorithm is a good\nrepresentative of those methods. However, it was originally designed so that it\nonly cared about numerical performance. Thus, we propose some modifications of\nthe original method to take interpretability into account. After recalling the\noriginal algorithm, this paper presents the changes made to the original\nmethod, then discusses some results obtained from benchmark problems. Finally,\nthe algorithm is applied to a real-world fault detection depollution problem.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2008 19:54:04 GMT"}], "update_date": "2008-08-22", "authors_parsed": [["Destercke", "S\u00e9bastien", "", "IRSN, IRIT"], ["Guillaume", "Serge", "", "ITAP"], ["Charnomordic", "Brigitte", "", "ASB"]]}, {"id": "0808.3109", "submitter": "Florentin Smarandache", "authors": "Florentin Smarandache, V. Christianto", "title": "n-ary Fuzzy Logic and Neutrosophic Logic Operators", "comments": "15 pages, 2 fuzzy and neutrosophic value tables, many diagrams", "journal-ref": "Studies in Logic, Grammar and Rethoric [Belarus], 17 (30), pp.\n  1-16, 2009.", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and\nneutrosophic logic binary operators. Then we generalize them to n-ary fuzzy\nlogic and neutrosophic logic operators using the smarandache codification of\nthe Venn diagram and a defined vector neutrosophic law. In such way, new\noperators in neutrosophic logic/set/probability are built.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2008 16:10:36 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2008 02:05:57 GMT"}, {"version": "v3", "created": "Tue, 18 Nov 2008 21:43:56 GMT"}], "update_date": "2010-02-16", "authors_parsed": [["Smarandache", "Florentin", ""], ["Christianto", "V.", ""]]}, {"id": "0808.3231", "submitter": "Zhi-Hua Zhou", "authors": "Zhi-Hua Zhou, Min-Ling Zhang, Sheng-Jun Huang, Yu-Feng Li", "title": "Multi-Instance Multi-Label Learning", "comments": "64 pages, 10 figures; Artificial Intelligence, 2011", "journal-ref": "Artificial Intelligence, 2012, 176(1): 2291-2320", "doi": "10.1016/j.artint.2011.10.002", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the MIML (Multi-Instance Multi-Label learning)\nframework where an example is described by multiple instances and associated\nwith multiple class labels. Compared to traditional learning frameworks, the\nMIML framework is more convenient and natural for representing complicated\nobjects which have multiple semantic meanings. To learn from MIML examples, we\npropose the MimlBoost and MimlSvm algorithms based on a simple degeneration\nstrategy, and experiments show that solving problems involving complicated\nobjects with multiple semantic meanings in the MIML framework can lead to good\nperformance. Considering that the degeneration process may lose information, we\npropose the D-MimlSvm algorithm which tackles MIML problems directly in a\nregularization framework. Moreover, we show that even when we do not have\naccess to the real objects and thus cannot capture more information from real\nobjects by using the MIML representation, MIML is still useful. We propose the\nInsDif and SubCod algorithms. InsDif works by transforming single-instances\ninto the MIML representation for learning, while SubCod works by transforming\nsingle-label examples into the MIML representation for learning. Experiments\nshow that in some tasks they are able to achieve better performance than\nlearning the single-instances or single-label examples directly.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2008 06:31:43 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2009 19:47:42 GMT"}, {"version": "v3", "created": "Thu, 13 Oct 2011 10:51:52 GMT"}, {"version": "v4", "created": "Sun, 23 Oct 2011 16:22:49 GMT"}], "update_date": "2011-10-28", "authors_parsed": [["Zhou", "Zhi-Hua", ""], ["Zhang", "Min-Ling", ""], ["Huang", "Sheng-Jun", ""], ["Li", "Yu-Feng", ""]]}]