[{"id": "1612.00092", "submitter": "Christian Walder Dr", "authors": "Christian Walder and Dongwoo Kim", "title": "Computer Assisted Composition with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence modeling with neural networks has lead to powerful models of\nsymbolic music data. We address the problem of exploiting these models to reach\ncreative musical goals, by combining with human input. To this end we\ngeneralise previous work, which sampled Markovian sequence models under the\nconstraint that the sequence belong to the language of a given finite state\nmachine provided by the human. We consider more expressive non-Markov models,\nthereby requiring approximate sampling which we provide in the form of an\nefficient sequential Monte Carlo method. In addition we provide and compare\nwith a beam search strategy for conditional probability maximisation.\n  Our algorithms are capable of convincingly re-harmonising famous musical\nworks. To demonstrate this we provide visualisations, quantitative experiments,\na human listening test and audio examples. We find both the sampling and\noptimisation procedures to be effective, yet complementary in character. For\nthe case of highly permissive constraint sets, we find that sampling is to be\npreferred due to the overly regular nature of the optimisation based results.\nThe generality of our algorithms permits countless other creative applications.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 00:49:19 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 23:38:35 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Walder", "Christian", ""], ["Kim", "Dongwoo", ""]]}, {"id": "1612.00094", "submitter": "Paul Weng", "authors": "Hugo Gilbert and Paul Weng and Yan Xu", "title": "Optimizing Quantiles in Preference-based Markov Decision Processes", "comments": "Long version of AAAI 2017 paper. arXiv admin note: text overlap with\n  arXiv:1611.00862", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Markov decision process model, policies are usually evaluated by\nexpected cumulative rewards. As this decision criterion is not always suitable,\nwe propose in this paper an algorithm for computing a policy optimal for the\nquantile criterion. Both finite and infinite horizons are considered. Finally\nwe experimentally evaluate our approach on random MDPs and on a data center\ncontrol problem.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 00:55:23 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Gilbert", "Hugo", ""], ["Weng", "Paul", ""], ["Xu", "Yan", ""]]}, {"id": "1612.00104", "submitter": "Xiaojian Wu", "authors": "Xiaojian Wu, Akshat Kumar, Daniel Sheldon, Shlomo Zilberstein", "title": "Robust Optimization for Tree-Structured Stochastic Network Design", "comments": "AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic network design is a general framework for optimizing network\nconnectivity. It has several applications in computational sustainability\nincluding spatial conservation planning, pre-disaster network preparation, and\nriver network optimization. A common assumption in previous work has been made\nthat network parameters (e.g., probability of species colonization) are\nprecisely known, which is unrealistic in real- world settings. We therefore\naddress the robust river network design problem where the goal is to optimize\nriver connectivity for fish movement by removing barriers. We assume that fish\npassability probabilities are known only imprecisely, but are within some\ninterval bounds. We then develop a planning approach that computes the policies\nwith either high robust ratio or low regret. Empirically, our approach scales\nwell to large river networks. We also provide insights into the solutions\ngenerated by our robust approach, which has significantly higher robust ratio\nthan the baseline solution with mean parameter estimates.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 01:21:21 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Wu", "Xiaojian", ""], ["Kumar", "Akshat", ""], ["Sheldon", "Daniel", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1612.00108", "submitter": "Zizhan Zheng", "authors": "Zizhan Zheng, Ness B. Shroff, Prasant Mohapatra", "title": "When to Reset Your Keys: Optimal Timing of Security Updates via Learning", "comments": "9 pages, 2 figures; accepted by the Thirty-First AAAI Conference on\n  Artificial Intelligence (AAAI-17), San Francisco, CA, USA, Feb. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybersecurity is increasingly threatened by advanced and persistent attacks.\nAs these attacks are often designed to disable a system (or a critical\nresource, e.g., a user account) repeatedly, it is crucial for the defender to\nkeep updating its security measures to strike a balance between the risk of\nbeing compromised and the cost of security updates. Moreover, these decisions\noften need to be made with limited and delayed feedback due to the stealthy\nnature of advanced attacks. In addition to targeted attacks, such an optimal\ntiming policy under incomplete information has broad applications in\ncybersecurity. Examples include key rotation, password change, application of\npatches, and virtual machine refreshing. However, rigorous studies of optimal\ntiming are rare. Further, existing solutions typically rely on a pre-defined\nattack model that is known to the defender, which is often not the case in\npractice. In this work, we make an initial effort towards achieving optimal\ntiming of security updates in the face of unknown stealthy attacks. We consider\na variant of the influential FlipIt game model with asymmetric feedback and\nunknown attack time distribution, which provides a general model to consecutive\nsecurity updates. The defender's problem is then modeled as a time associative\nbandit problem with dependent arms. We derive upper confidence bound based\nlearning policies that achieve low regret compared with optimal periodic\ndefense strategies that can only be derived when attack time distributions are\nknown.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 01:43:24 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 18:26:18 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Zheng", "Zizhan", ""], ["Shroff", "Ness B.", ""], ["Mohapatra", "Prasant", ""]]}, {"id": "1612.00132", "submitter": "Jiajun Lu", "authors": "Jiajun Lu, Aditya Deshpande, David Forsyth", "title": "CDVAE: Co-embedding Deep Variational Auto Encoder for Conditional\n  Variational Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems such as predicting a new shading field (Y) for an image (X) are\nambiguous: many very distinct solutions are good. Representing this ambiguity\nrequires building a conditional model P(Y|X) of the prediction, conditioned on\nthe image. Such a model is difficult to train, because we do not usually have\ntraining data containing many different shadings for the same image. As a\nresult, we need different training examples to share data to produce good\nmodels. This presents a danger we call \"code space collapse\" - the training\nprocedure produces a model that has a very good loss score, but which\nrepresents the conditional distribution poorly. We demonstrate an improved\nmethod for building conditional models by exploiting a metric constraint on\ntraining data that prevents code space collapse. We demonstrate our model on\ntwo example tasks using real data: image saturation adjustment, image\nrelighting. We describe quantitative metrics to evaluate ambiguous generation\nresults. Our results quantitatively and qualitatively outperform different\nstrong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 03:40:42 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 03:21:34 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Lu", "Jiajun", ""], ["Deshpande", "Aditya", ""], ["Forsyth", "David", ""]]}, {"id": "1612.00203", "submitter": "Alessia Amelio Dr.", "authors": "Darko Brodi\\'c, Alessia Amelio", "title": "Analysis of the Human-Computer Interaction on the Example of Image-based\n  CAPTCHA by Association Rule Mining", "comments": "13 pages, 2 figures, 6 tables, 5th International Workshop on\n  Symbiotic Interaction (Symbiotic), Padua, Italy, 29-30 September 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper analyzes the interaction between humans and computers in terms of\nresponse time in solving the image-based CAPTCHA. In particular, the analysis\nfocuses on the attitude of the different Internet users in easily solving four\ndifferent types of image-based CAPTCHAs which include facial expressions like:\nanimated character, old woman, surprised face, worried face. To pursue this\ngoal, an experiment is realized involving 100 Internet users in solving the\nfour types of CAPTCHAs, differentiated by age, Internet experience, and\neducation level. The response times are collected for each user. Then,\nassociation rules are extracted from user data, for evaluating the dependence\nof the response time in solving the CAPTCHA from age, education level and\nexperience in internet usage by statistical analysis. The results implicitly\ncapture the users' psychological states showing in what states the users are\nmore sensible. It reveals to be a novelty and a meaningful analysis in the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 11:24:09 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 12:52:21 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Brodi\u0107", "Darko", ""], ["Amelio", "Alessia", ""]]}, {"id": "1612.00222", "submitter": "Peter Battaglia", "authors": "Peter W. Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, Koray\n  Kavukcuoglu", "title": "Interaction Networks for Learning about Objects, Relations and Physics", "comments": "Published in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about objects, relations, and physics is central to human\nintelligence, and a key goal of artificial intelligence. Here we introduce the\ninteraction network, a model which can reason about how objects in complex\nsystems interact, supporting dynamical predictions, as well as inferences about\nthe abstract properties of the system. Our model takes graphs as input,\nperforms object- and relation-centric reasoning in a way that is analogous to a\nsimulation, and is implemented using deep neural networks. We evaluate its\nability to reason about several challenging physical domains: n-body problems,\nrigid-body collision, and non-rigid dynamics. Our results show it can be\ntrained to accurately simulate the physical trajectories of dozens of objects\nover thousands of time steps, estimate abstract quantities such as energy, and\ngeneralize automatically to systems with different numbers and configurations\nof objects and relations. Our interaction network implementation is the first\ngeneral-purpose, learnable physics engine, and a powerful general framework for\nreasoning about object and relations in a wide variety of complex real-world\ndomains.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 12:34:54 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Battaglia", "Peter W.", ""], ["Pascanu", "Razvan", ""], ["Lai", "Matthew", ""], ["Rezende", "Danilo", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1612.00227", "submitter": "Loris Bozzato", "authors": "Stefano Borgo, Loris Bozzato, Alessio Palmero Aprosio, Marco Rospocher\n  and Luciano Serafini", "title": "On Coreferring Text-extracted Event Descriptions with the aid of\n  Ontological Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems for automatic extraction of semantic information about events from\nlarge textual resources are now available: these tools are capable to generate\nRDF datasets about text extracted events and this knowledge can be used to\nreason over the recognized events. On the other hand, text based tasks for\nevent recognition, as for example event coreference (i.e. recognizing whether\ntwo textual descriptions refer to the same event), do not take into account\nontological information of the extracted events in their process. In this\npaper, we propose a method to derive event coreference on text extracted event\ndata using semantic based rule reasoning. We demonstrate our method considering\na limited (yet representative) set of event types: we introduce a formal\nanalysis on their ontological properties and, on the base of this, we define a\nset of coreference criteria. We then implement these criteria as RDF-based\nreasoning rules to be applied on text extracted event data. We evaluate the\neffectiveness of our approach over a standard coreference benchmark dataset.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 12:58:02 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Borgo", "Stefano", ""], ["Bozzato", "Loris", ""], ["Aprosio", "Alessio Palmero", ""], ["Rospocher", "Marco", ""], ["Serafini", "Luciano", ""]]}, {"id": "1612.00240", "submitter": "Kleanthi Georgala", "authors": "Kleanthi Georgala, Micheal Hoffmann and Axel-Cyrille Ngonga Ngomo", "title": "An Evaluation of Models for Runtime Approximation in Link Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-efficient link discovery is of central importance to implement the\nvision of the Semantic Web. Some of the most rapid Link Discovery approaches\nrely internally on planning to execute link specifications. In newer works,\nlinear models have been used to estimate the runtime the fastest planners.\nHowever, no other category of models has been studied for this purpose so far.\nIn this paper, we study non-linear runtime estimation functions for runtime\nestimation. In particular, we study exponential and mixed models for the\nestimation of the runtimes of planners. To this end, we evaluate three\ndifferent models for runtime on six datasets using 400 link specifications. We\nshow that exponential and mixed models achieve better fits when trained but are\nonly to be preferred in some cases. Our evaluation also shows that the use of\nbetter runtime approximation models has a positive impact on the overall\nexecution of link specifications.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 13:33:03 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Georgala", "Kleanthi", ""], ["Hoffmann", "Micheal", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1612.00341", "submitter": "Michael Chang", "authors": "Michael B. Chang, Tomer Ullman, Antonio Torralba, Joshua B. Tenenbaum", "title": "A Compositional Object-Based Approach to Learning Physical Dynamics", "comments": "Published as a conference paper for ICLR 2017. 15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Neural Physics Engine (NPE), a framework for learning\nsimulators of intuitive physics that naturally generalize across variable\nobject count and different scene configurations. We propose a factorization of\na physical scene into composable object-based representations and a neural\nnetwork architecture whose compositional structure factorizes object dynamics\ninto pairwise interactions. Like a symbolic physics engine, the NPE is endowed\nwith generic notions of objects and their interactions; realized as a neural\nnetwork, it can be trained via stochastic gradient descent to adapt to specific\nobject properties and dynamics of different worlds. We evaluate the efficacy of\nour approach on simple rigid body dynamics in two-dimensional worlds. By\ncomparing to less structured architectures, we show that the NPE's\ncompositional representation of the structure in physical interactions improves\nits ability to predict movement, generalize across variable object count and\ndifferent scene configurations, and infer latent properties of objects such as\nmass.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 16:39:04 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 17:44:06 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Chang", "Michael B.", ""], ["Ullman", "Tomer", ""], ["Torralba", "Antonio", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1612.00347", "submitter": "Arash Eshghi", "authors": "Dimitrios Kalatzis, Arash Eshghi, Oliver Lemon", "title": "Bootstrapping incremental dialogue systems: using linguistic knowledge\n  to learn from minimal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for inducing new dialogue systems from very small amounts\nof unannotated dialogue data, showing how word-level exploration using\nReinforcement Learning (RL), combined with an incremental and semantic grammar\n- Dynamic Syntax (DS) - allows systems to discover, generate, and understand\nmany new dialogue variants. The method avoids the use of expensive and\ntime-consuming dialogue act annotations, and supports more natural\n(incremental) dialogues than turn-based systems. Here, language generation and\ndialogue management are treated as a joint decision/optimisation problem, and\nthe MDP model for RL is constructed automatically. With an implemented system,\nwe show that this method enables a wide range of dialogue variations to be\nautomatically captured, even when the system is trained from only a single\ndialogue. The variants include question-answer pairs, over- and\nunder-answering, self- and other-corrections, clarification interaction,\nsplit-utterances, and ellipsis. This generalisation property results from the\nstructural knowledge and constraints present within the DS grammar, and\nhighlights some limitations of recent systems built using machine learning\ntechniques only.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 16:49:04 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Kalatzis", "Dimitrios", ""], ["Eshghi", "Arash", ""], ["Lemon", "Oliver", ""]]}, {"id": "1612.00367", "submitter": "Damien Lefortier", "authors": "Damien Lefortier, Adith Swaminathan, Xiaotao Gu, Thorsten Joachims,\n  Maarten de Rijke", "title": "Large-scale Validation of Counterfactual Learning Methods: A Test-Bed", "comments": "10 pages, What If workshop NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perform effective off-policy learning would revolutionize the\nprocess of building better interactive systems, such as search engines and\nrecommendation systems for e-commerce, computational advertising and news.\nRecent approaches for off-policy evaluation and learning in these settings\nappear promising. With this paper, we provide real-world data and a\nstandardized test-bed to systematically investigate these algorithms using data\nfrom display advertising. In particular, we consider the problem of filling a\nbanner ad with an aggregate of multiple products the user may want to purchase.\nThis paper presents our test-bed, the sanity checks we ran to ensure its\nvalidity, and shows results comparing state-of-the-art off-policy learning\nmethods like doubly robust optimization, POEM, and reductions to supervised\nlearning using regression baselines. Our results show experimental evidence\nthat recent off-policy learning methods can improve upon state-of-the-art\nsupervised learning techniques on a large-scale real-world data set.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 17:59:53 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 11:00:30 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Lefortier", "Damien", ""], ["Swaminathan", "Adith", ""], ["Gu", "Xiaotao", ""], ["Joachims", "Thorsten", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1612.00377", "submitter": "Iulian Vlad Serban", "authors": "Iulian V. Serban, Alexander G. Ororbia II, Joelle Pineau, Aaron\n  Courville", "title": "Piecewise Latent Variables for Neural Variational Text Processing", "comments": "19 pages, 2 figures, 8 tables; EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in neural variational inference have facilitated the learning of\npowerful directed graphical models with continuous latent variables, such as\nvariational autoencoders. The hope is that such models will learn to represent\nrich, multi-modal latent factors in real-world data, such as natural language\ntext. However, current models often assume simplistic priors on the latent\nvariables - such as the uni-modal Gaussian distribution - which are incapable\nof representing complex latent factors efficiently. To overcome this\nrestriction, we propose the simple, but highly flexible, piecewise constant\ndistribution. This distribution has the capacity to represent an exponential\nnumber of modes of a latent target distribution, while remaining mathematically\ntractable. Our results demonstrate that incorporating this new latent\ndistribution into different models yields substantial improvements in natural\nlanguage processing tasks such as document modeling and natural language\ngeneration for dialogue.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 18:49:23 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 03:18:54 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 19:25:58 GMT"}, {"version": "v4", "created": "Sat, 23 Sep 2017 13:33:55 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Serban", "Iulian V.", ""], ["Ororbia", "Alexander G.", "II"], ["Pineau", "Joelle", ""], ["Courville", "Aaron", ""]]}, {"id": "1612.00380", "submitter": "Nantas Nardelli", "authors": "Shehroze Bhatti, Alban Desmaison, Ondrej Miksik, Nantas Nardelli, N.\n  Siddharth, Philip H. S. Torr", "title": "Playing Doom with SLAM-Augmented Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent approaches to policy learning in 2D game domains have been\nsuccessful going directly from raw input images to actions. However when\nemployed in complex 3D environments, they typically suffer from challenges\nrelated to partial observability, combinatorial exploration spaces, path\nplanning, and a scarcity of rewarding scenarios. Inspired from prior work in\nhuman cognition that indicates how humans employ a variety of semantic concepts\nand abstractions (object categories, localisation, etc.) to reason about the\nworld, we build an agent-model that incorporates such abstractions into its\npolicy-learning framework. We augment the raw image input to a Deep Q-Learning\nNetwork (DQN), by adding details of objects and structural elements\nencountered, along with the agent's localisation. The different components are\nautomatically extracted and composed into a topological representation using\non-the-fly object detection and 3D-scene reconstruction.We evaluate the\nefficacy of our approach in Doom, a 3D first-person combat game that exhibits a\nnumber of challenges discussed, and show that our augmented framework\nconsistently learns better, more effective policies.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 18:54:51 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Bhatti", "Shehroze", ""], ["Desmaison", "Alban", ""], ["Miksik", "Ondrej", ""], ["Nardelli", "Nantas", ""], ["Siddharth", "N.", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1612.00429", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Tianhe Yu, Justin Fu, Pieter Abbeel, Sergey Levine", "title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "comments": "ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) can acquire complex behaviors from low-level\ninputs, such as images. However, real-world applications of such methods\nrequire generalizing to the vast variability of the real world. Deep networks\nare known to achieve remarkable generalization when provided with massive\namounts of labeled data, but can we provide this breadth of experience to an RL\nagent, such as a robot? The robot might continuously learn as it explores the\nworld around it, even while deployed. However, this learning requires access to\na reward function, which is often hard to measure in real-world domains, where\nthe reward could depend on, for example, unknown positions of objects or the\nemotional state of the user. Conversely, it is often quite practical to provide\nthe agent with reward functions in a limited set of situations, such as when a\nhuman supervisor is present or in a controlled setting. Can we make use of this\nlimited supervision, and still benefit from the breadth of experience an agent\nmight collect on its own? In this paper, we formalize this problem as\nsemisupervised reinforcement learning, where the reward function can only be\nevaluated in a set of \"labeled\" MDPs, and the agent must generalize its\nbehavior to the wide range of states it might encounter in a set of \"unlabeled\"\nMDPs, by using experience from both settings. Our proposed method infers the\ntask objective in the unlabeled MDPs through an algorithm that resembles\ninverse RL, using the agent's own prior experience in the labeled MDPs as a\nkind of demonstration of optimal behavior. We evaluate our method on\nchallenging tasks that require control directly from images, and show that our\napproach can improve the generalization of a learned deep neural network policy\nby using experience for which no reward function is available. We also show\nthat our method outperforms direct supervised learning of the reward.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 20:48:39 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 19:46:12 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Finn", "Chelsea", ""], ["Yu", "Tianhe", ""], ["Fu", "Justin", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1612.00475", "submitter": "Taylor Killian", "authors": "Taylor Killian, George Konidaris, Finale Doshi-Velez", "title": "Transfer Learning Across Patient Variations with Hidden Parameter Markov\n  Decision Processes", "comments": "Brief abstract for poster submission to Machine Learning for\n  Healthcare workshop at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to physiological variation, patients diagnosed with the same condition\nmay exhibit divergent, but related, responses to the same treatments. Hidden\nParameter Markov Decision Processes (HiP-MDPs) tackle this transfer-learning\nproblem by embedding these tasks into a low-dimensional space. However, the\noriginal formulation of HiP-MDP had a critical flaw: the embedding uncertainty\nwas modeled independently of the agent's state uncertainty, requiring an\nunnatural training procedure in which all tasks visited every part of the state\nspace---possible for robots that can be moved to a particular location,\nimpossible for human patients. We update the HiP-MDP framework and extend it to\nmore robustly develop personalized medicine strategies for HIV treatment.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 21:26:52 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Killian", "Taylor", ""], ["Konidaris", "George", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1612.00563", "submitter": "Steven Rennie", "authors": "Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross and\n  Vaibhava Goel", "title": "Self-critical Sequence Training for Image Captioning", "comments": "CVPR 2017 + additional analysis + fixed baseline results, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it has been shown that policy-gradient methods for reinforcement\nlearning can be utilized to train deep end-to-end systems directly on\nnon-differentiable metrics for the task at hand. In this paper we consider the\nproblem of optimizing image captioning systems using reinforcement learning,\nand show that by carefully optimizing our systems using the test metrics of the\nMSCOCO task, significant gains in performance can be realized. Our systems are\nbuilt using a new optimization approach that we call self-critical sequence\ntraining (SCST). SCST is a form of the popular REINFORCE algorithm that, rather\nthan estimating a \"baseline\" to normalize the rewards and reduce variance,\nutilizes the output of its own test-time inference algorithm to normalize the\nrewards it experiences. Using this approach, estimating the reward signal (as\nactor-critic methods must do) and estimating normalization (as REINFORCE\nalgorithms typically do) is avoided, while at the same time harmonizing the\nmodel with respect to its test-time inference procedure. Empirically we find\nthat directly optimizing the CIDEr metric with SCST and greedy decoding at\ntest-time is highly effective. Our results on the MSCOCO evaluation sever\nestablish a new state-of-the-art on the task, improving the best result in\nterms of CIDEr from 104.9 to 114.7.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 04:37:22 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 02:38:37 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Rennie", "Steven J.", ""], ["Marcheret", "Etienne", ""], ["Mroueh", "Youssef", ""], ["Ross", "Jarret", ""], ["Goel", "Vaibhava", ""]]}, {"id": "1612.00583", "submitter": "Yifei Ma", "authors": "Yifei Ma and Roman Garnett and Jeff Schneider", "title": "Active Search for Sparse Signals with Region Sensing", "comments": "aaai 2017 preprint; nips exhibition of rejections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous systems can be used to search for sparse signals in a large space;\ne.g., aerial robots can be deployed to localize threats, detect gas leaks, or\nrespond to distress calls. Intuitively, search algorithms may increase\nefficiency by collecting aggregate measurements summarizing large contiguous\nregions. However, most existing search methods either ignore the possibility of\nsuch region observations (e.g., Bayesian optimization and multi-armed bandits)\nor make strong assumptions about the sensing mechanism that allow each\nmeasurement to arbitrarily encode all signals in the entire environment (e.g.,\ncompressive sensing). We propose an algorithm that actively collects data to\nsearch for sparse signals using only noisy measurements of the average values\non rectangular regions (including single points), based on the greedy\nmaximization of information gain. We analyze our algorithm in 1d and show that\nit requires $\\tilde{O}(\\frac{n}{\\mu^2}+k^2)$ measurements to recover all of $k$\nsignal locations with small Bayes error, where $\\mu$ and $n$ are the signal\nstrength and the size of the search space, respectively. We also show that\nactive designs can be fundamentally more efficient than passive designs with\nregion sensing, contrasting with the results of Arias-Castro, Candes, and\nDavenport (2013). We demonstrate the empirical performance of our algorithm on\na search problem using satellite image data and in high dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:44:45 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Ma", "Yifei", ""], ["Garnett", "Roman", ""], ["Schneider", "Jeff", ""]]}, {"id": "1612.00653", "submitter": "Antti Kangasr\\\"a\\\"asi\\\"o", "authors": "Antti Kangasr\\\"a\\\"asi\\\"o, Kumaripaba Athukorala, Andrew Howes, Jukka\n  Corander, Samuel Kaski, Antti Oulasvirta", "title": "Inferring Cognitive Models from Data using Approximate Bayesian\n  Computation", "comments": "To appear in CHI'2017", "journal-ref": null, "doi": "10.1145/3025453.3025576", "report-no": null, "categories": "cs.HC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem for HCI researchers is to estimate the parameter values\nof a cognitive model from behavioral data. This is a difficult problem, because\nof the substantial complexity and variety in human behavioral strategies. We\nreport an investigation into a new approach using approximate Bayesian\ncomputation (ABC) to condition model parameters to data and prior knowledge. As\nthe case study we examine menu interaction, where we have click time data only\nto infer a cognitive model that implements a search behaviour with parameters\nsuch as fixation duration and recall probability. Our results demonstrate that\nABC (i) improves estimates of model parameter values, (ii) enables meaningful\ncomparisons between model variants, and (iii) supports fitting models to\nindividual users. ABC provides ample opportunities for theoretical HCI research\nby allowing principled inference of model parameter values and their\nuncertainty.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 12:20:47 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 12:15:47 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Kangasr\u00e4\u00e4si\u00f6", "Antti", ""], ["Athukorala", "Kumaripaba", ""], ["Howes", "Andrew", ""], ["Corander", "Jukka", ""], ["Kaski", "Samuel", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "1612.00712", "submitter": "Jayant Krishnamurthy", "authors": "Kenton W. Murray and Jayant Krishnamurthy", "title": "Probabilistic Neural Programs", "comments": "Appears in NAMPI workshop at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present probabilistic neural programs, a framework for program induction\nthat permits flexible specification of both a computational model and inference\nalgorithm while simultaneously enabling the use of deep neural networks.\nProbabilistic neural programs combine a computation graph for specifying a\nneural network with an operator for weighted nondeterministic choice. Thus, a\nprogram describes both a collection of decisions as well as the neural network\narchitecture used to make each one. We evaluate our approach on a challenging\ndiagram question answering task where probabilistic neural programs correctly\nexecute nearly twice as many programs as a baseline model.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 15:46:09 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Murray", "Kenton W.", ""], ["Krishnamurthy", "Jayant", ""]]}, {"id": "1612.00742", "submitter": "Michael Gr. Voskoglou Prof. Dr.", "authors": "Michael Gr. Voskoglou", "title": "Comparison of the COG Defuzzification Technique and Its Variations to\n  the GPA Index", "comments": "11 pages, 5 figures, 2 tables", "journal-ref": "American Journal of Computational and Applied Mathematics, 6(5),\n  187-193, 2016", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Center of Gravity (COG) method is one of the most popular defuzzification\ntechniques of fuzzy mathematics. In earlier works the COG technique was\nproperly adapted to be used as an assessment model (RFAM)and several variations\nof it (GRFAM, TFAM and TpFAM)were also constructed for the same purpose. In\nthis paper the outcomes of all these models are compared to the corresponding\noutcomes of a traditional assessment method of the bi-valued logic, the Grade\nPoint Average (GPA) Index. Examples are also presented illustrating our\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 07:53:15 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Voskoglou", "Michael Gr.", ""]]}, {"id": "1612.00745", "submitter": "Andras Lorincz", "authors": "Andr\\'as L\\H{o}rincz, M\\'at\\'e Cs\\'akv\\'ari, \\'Aron F\\'othi, Zolt\\'an\n  \\'Ad\\'am Milacski, Andr\\'as S\\'ark\\'any, Zolt\\'an T\\H{o}s\\'er", "title": "Cognitive Deep Machine Can Train Itself", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is making substantial progress in diverse applications. The\nsuccess is mostly due to advances in deep learning. However, deep learning can\nmake mistakes and its generalization abilities to new tasks are questionable.\nWe ask when and how one can combine network outputs, when (i) details of the\nobservations are evaluated by learned deep components and (ii) facts and\nconfirmation rules are available in knowledge based systems. We show that in\nlimited contexts the required number of training samples can be low and\nself-improvement of pre-trained networks in more general context is possible.\nWe argue that the combination of sparse outlier detection with deep components\nthat can support each other diminish the fragility of deep methods, an\nimportant requirement for engineering applications. We argue that supervised\nlearning of labels may be fully eliminated under certain conditions: a\ncomponent based architecture together with a knowledge based system can train\nitself and provide high quality answers. We demonstrate these concepts on the\nState Farm Distracted Driver Detection benchmark. We argue that the view of the\nStudy Panel (2016) may overestimate the requirements on `years of focused\nresearch' and `careful, unique construction' for `AI systems'.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 16:49:07 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["L\u0151rincz", "Andr\u00e1s", ""], ["Cs\u00e1kv\u00e1ri", "M\u00e1t\u00e9", ""], ["F\u00f3thi", "\u00c1ron", ""], ["Milacski", "Zolt\u00e1n \u00c1d\u00e1m", ""], ["S\u00e1rk\u00e1ny", "Andr\u00e1s", ""], ["T\u0151s\u00e9r", "Zolt\u00e1n", ""]]}, {"id": "1612.00767", "submitter": "Jost Tobias Springenberg", "authors": "Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, Frank Hutter", "title": "Asynchronous Stochastic Gradient MCMC with Elastic Coupling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parallel asynchronous Markov Chain Monte Carlo (MCMC) sampling\nfor problems where we can leverage (stochastic) gradients to define continuous\ndynamics which explore the target distribution. We outline a solution strategy\nfor this setting based on stochastic gradient Hamiltonian Monte Carlo sampling\n(SGHMC) which we alter to include an elastic coupling term that ties together\nmultiple MCMC instances. The proposed strategy turns inherently sequential HMC\nalgorithms into asynchronous parallel versions. First experiments empirically\nshow that the resulting parallel sampler significantly speeds up exploration of\nthe target distribution, when compared to standard SGHMC, and is less prone to\nthe harmful effects of stale gradients than a naive parallelization approach.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 17:43:33 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 09:19:30 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Springenberg", "Jost Tobias", ""], ["Klein", "Aaron", ""], ["Falkner", "Stefan", ""], ["Hutter", "Frank", ""]]}, {"id": "1612.00796", "submitter": "Raia Hadsell", "authors": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness,\n  Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho,\n  Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan\n  Kumaran, Raia Hadsell", "title": "Overcoming catastrophic forgetting in neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of artificial intelligence. Neural networks are not, in general,\ncapable of this and it has been widely thought that catastrophic forgetting is\nan inevitable feature of connectionist models. We show that it is possible to\novercome this limitation and train networks that can maintain expertise on\ntasks which they have not experienced for a long time. Our approach remembers\nold tasks by selectively slowing down learning on the weights important for\nthose tasks. We demonstrate our approach is scalable and effective by solving a\nset of classification tasks based on the MNIST hand written digit dataset and\nby learning several Atari 2600 games sequentially.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 19:18:37 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 13:01:51 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Kirkpatrick", "James", ""], ["Pascanu", "Razvan", ""], ["Rabinowitz", "Neil", ""], ["Veness", "Joel", ""], ["Desjardins", "Guillaume", ""], ["Rusu", "Andrei A.", ""], ["Milan", "Kieran", ""], ["Quan", "John", ""], ["Ramalho", "Tiago", ""], ["Grabska-Barwinska", "Agnieszka", ""], ["Hassabis", "Demis", ""], ["Clopath", "Claudia", ""], ["Kumaran", "Dharshan", ""], ["Hadsell", "Raia", ""]]}, {"id": "1612.00817", "submitter": "Alexander Gaunt", "authors": "Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman,\n  Pushmeet Kohli, Jonathan Taylor, Daniel Tarlow", "title": "Summary - TerpreT: A Probabilistic Programming Language for Program\n  Induction", "comments": "7 pages, 2 figures, 4 tables in 1st Workshop on Neural Abstract\n  Machines & Program Induction (NAMPI), @NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study machine learning formulations of inductive program synthesis; that\nis, given input-output examples, synthesize source code that maps inputs to\ncorresponding outputs. Our key contribution is TerpreT, a domain-specific\nlanguage for expressing program synthesis problems. A TerpreT model is composed\nof a specification of a program representation and an interpreter that\ndescribes how programs map inputs to outputs. The inference task is to observe\na set of input-output examples and infer the underlying program. From a TerpreT\nmodel we automatically perform inference using four different back-ends:\ngradient descent (thus each TerpreT model can be seen as defining a\ndifferentiable interpreter), linear program (LP) relaxations for graphical\nmodels, discrete satisfiability solving, and the Sketch program synthesis\nsystem. TerpreT has two main benefits. First, it enables rapid exploration of a\nrange of domains, program representations, and interpreter models. Second, it\nseparates the model specification from the inference algorithm, allowing proper\ncomparisons between different approaches to inference.\n  We illustrate the value of TerpreT by developing several interpreter models\nand performing an extensive empirical comparison between alternative inference\nalgorithms on a variety of program models. To our knowledge, this is the first\nwork to compare gradient-based search over program space to traditional\nsearch-based alternatives. Our key empirical finding is that constraint solvers\ndominate the gradient descent and LP-based formulations.\n  This is a workshop summary of a longer report at arXiv:1608.04428\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 20:08:22 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Gaunt", "Alexander L.", ""], ["Brockschmidt", "Marc", ""], ["Singh", "Rishabh", ""], ["Kushman", "Nate", ""], ["Kohli", "Pushmeet", ""], ["Taylor", "Jonathan", ""], ["Tarlow", "Daniel", ""]]}, {"id": "1612.00837", "submitter": "Yash Goyal", "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in\n  Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems at the intersection of vision and language are of significant\nimportance both as challenging research questions and for the rich set of\napplications they enable. However, inherent structure in our world and bias in\nour language tend to be a simpler signal for learning than visual modalities,\nresulting in models that ignore visual information, leading to an inflated\nsense of their capability.\n  We propose to counter these language priors for the task of Visual Question\nAnswering (VQA) and make vision (the V in VQA) matter! Specifically, we balance\nthe popular VQA dataset by collecting complementary images such that every\nquestion in our balanced dataset is associated with not just a single image,\nbut rather a pair of similar images that result in two different answers to the\nquestion. Our dataset is by construction more balanced than the original VQA\ndataset and has approximately twice the number of image-question pairs. Our\ncomplete balanced dataset is publicly available at www.visualqa.org as part of\nthe 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA\nv2.0).\n  We further benchmark a number of state-of-art VQA models on our balanced\ndataset. All models perform significantly worse on our balanced dataset,\nsuggesting that these models have indeed learned to exploit language priors.\nThis finding provides the first concrete empirical evidence for what seems to\nbe a qualitative sense among practitioners.\n  Finally, our data collection protocol for identifying complementary images\nenables us to develop a novel interpretable model, which in addition to\nproviding an answer to the given (image, question) pair, also provides a\ncounter-example based explanation. Specifically, it identifies an image that is\nsimilar to the original image, but it believes has a different answer to the\nsame question. This can help in building trust for machines among their users.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 20:57:07 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 18:20:13 GMT"}, {"version": "v3", "created": "Mon, 15 May 2017 17:58:49 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Goyal", "Yash", ""], ["Khot", "Tejas", ""], ["Summers-Stay", "Douglas", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1612.00901", "submitter": "Mark Yatskar", "authors": "Mark Yatskar, Vicente Ordonez, Luke Zettlemoyer, Ali Farhadi", "title": "Commonly Uncommon: Semantic Sparsity in Situation Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic sparsity is a common challenge in structured visual classification\nproblems; when the output space is complex, the vast majority of the possible\npredictions are rarely, if ever, seen in the training set. This paper studies\nsemantic sparsity in situation recognition, the task of producing structured\nsummaries of what is happening in images, including activities, objects and the\nroles objects play within the activity. For this problem, we find empirically\nthat most object-role combinations are rare, and current state-of-the-art\nmodels significantly underperform in this sparse data regime. We avoid many\nsuch errors by (1) introducing a novel tensor composition function that learns\nto share examples across role-noun combinations and (2) semantically augmenting\nour training data with automatically gathered examples of rarely observed\noutputs using web data. When integrated within a complete CRF-based structured\nprediction model, the tensor-based approach outperforms existing state of the\nart by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role\naccuracy, respectively. Adding 5 million images with our semantic augmentation\ntechniques gives further relative improvements of 6.23% and 9.57% on top-5 verb\nand noun-role accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 00:31:52 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Yatskar", "Mark", ""], ["Ordonez", "Vicente", ""], ["Zettlemoyer", "Luke", ""], ["Farhadi", "Ali", ""]]}, {"id": "1612.00916", "submitter": "Pierre-Luc Bacon", "authors": "Pierre-Luc Bacon, Doina Precup", "title": "A Matrix Splitting Perspective on Planning with Options", "comments": "The results presented in the previous version of this paper were\n  found be applicable only to \"gating execution\" and not \"call-and-return\". We\n  made this distinction clear in the text and added an extension to the\n  call-and-return model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Bellman operator underlying the options framework leads to a\nmatrix splitting, an approach traditionally used to speed up convergence of\niterative solvers for large linear systems of equations. Based on standard\ncomparison theorems for matrix splittings, we then show how the asymptotic rate\nof convergence varies as a function of the inherent timescales of the options.\nThis new perspective highlights a trade-off between asymptotic performance and\nthe cost of computation associated with building a good set of options.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 02:57:36 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 19:28:32 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Bacon", "Pierre-Luc", ""], ["Precup", "Doina", ""]]}, {"id": "1612.00944", "submitter": "Muthu Kumar Chandrasekaran", "authors": "Muthu Kumar Chandrasekaran, Carrie Demmans Epp, Min-Yen Kan, Diane\n  Litman", "title": "Using Discourse Signals for Robust Instructor Intervention Prediction", "comments": "To appear in proceedings of the 31st AAAI Conference on Artificial\n  Intelligence, San Francisco, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the prediction of instructor intervention in student posts from\ndiscussion forums in Massive Open Online Courses (MOOCs). Our key finding is\nthat using automatically obtained discourse relations improves the prediction\nof when instructors intervene in student discussions, when compared with a\nstate-of-the-art, feature-rich baseline. Our supervised classifier makes use of\nan automatic discourse parser which outputs Penn Discourse Treebank (PDTB) tags\nthat represent in-post discourse features. We show PDTB relation-based features\nincrease the robustness of the classifier and complement baseline features in\nrecalling more diverse instructor intervention patterns. In comprehensive\nexperiments over 14 MOOC offerings from several disciplines, the PDTB discourse\nfeatures improve performance on average. The resultant models are less\ndependent on domain-specific vocabulary, allowing them to better generalize to\nnew courses.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 09:08:51 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Chandrasekaran", "Muthu Kumar", ""], ["Epp", "Carrie Demmans", ""], ["Kan", "Min-Yen", ""], ["Litman", "Diane", ""]]}, {"id": "1612.00959", "submitter": "Karol W\\k{e}grzycki", "authors": "Andrzej Pacuk and Piotr Sankowski and Karol W\\k{e}grzycki and Adam\n  Witkowski and Piotr Wygocki", "title": "RecSys Challenge 2016: job recommendations based on preselection of\n  offers and gradient boosting", "comments": "6 pages, 1 figure, 2 tables, Description of 2nd place winning\n  solution of RecSys 2016 Challange. To be published in RecSys'16 Challange\n  Proceedings", "journal-ref": "Proceedings of the Recommender Systems Challenge, RecSys Challenge\n  '16, Boston, Massachusetts - September 15 - 15, 2016, pages 10:1--10:4", "doi": "10.1145/2987538.2987544", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Mim-Solution's approach to the RecSys Challenge 2016, which\nranked 2nd. The goal of the competition was to prepare job recommendations for\nthe users of the website Xing.com.\n  Our two phase algorithm consists of candidate selection followed by the\ncandidate ranking. We ranked the candidates by the predicted probability that\nthe user will positively interact with the job offer. We have used Gradient\nBoosting Decision Trees as the regression tool.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 11:35:37 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Pacuk", "Andrzej", ""], ["Sankowski", "Piotr", ""], ["W\u0119grzycki", "Karol", ""], ["Witkowski", "Adam", ""], ["Wygocki", "Piotr", ""]]}, {"id": "1612.01010", "submitter": "Ga\\\"etan Hadjeres", "authors": "Ga\\\"etan Hadjeres and Fran\\c{c}ois Pachet and Frank Nielsen", "title": "DeepBach: a Steerable Model for Bach Chorales Generation", "comments": "10 pages, ICML2017 version", "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70:1362-1371, 2017", "doi": null, "report-no": null, "categories": "cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces DeepBach, a graphical model aimed at modeling\npolyphonic music and specifically hymn-like pieces. We claim that, after being\ntrained on the chorale harmonizations by Johann Sebastian Bach, our model is\ncapable of generating highly convincing chorales in the style of Bach.\nDeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an\nadapted representation of musical data. This is in contrast with many automatic\nmusic composition approaches which tend to compose music sequentially. Our\nmodel is also steerable in the sense that a user can constrain the generation\nby imposing positional constraints such as notes, rhythms or cadences in the\ngenerated score. We also provide a plugin on top of the MuseScore music editor\nmaking the interaction with DeepBach easy to use.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 19:17:29 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 17:25:58 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Hadjeres", "Ga\u00ebtan", ""], ["Pachet", "Fran\u00e7ois", ""], ["Nielsen", "Frank", ""]]}, {"id": "1612.01058", "submitter": "Margareta Ackerman", "authors": "Margareta Ackerman and David Loker", "title": "Algorithmic Songwriting with ALYSIA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces ALYSIA: Automated LYrical SongwrIting Application.\nALYSIA is based on a machine learning model using Random Forests, and we\ndiscuss its success at pitch and rhythm prediction. Next, we show how ALYSIA\nwas used to create original pop songs that were subsequently recorded and\nproduced. Finally, we discuss our vision for the future of Automated\nSongwriting for both co-creative and autonomous systems.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 03:36:51 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Ackerman", "Margareta", ""], ["Loker", "David", ""]]}, {"id": "1612.01078", "submitter": "Ali Nassif", "authors": "Ali Bou Nassif, Luiz Fernando Capretz, Danny Ho", "title": "Enhancing Use Case Points Estimation Method Using Soft Computing\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software estimation is a crucial task in software engineering. Software\nestimation encompasses cost, effort, schedule, and size. The importance of\nsoftware estimation becomes critical in the early stages of the software life\ncycle when the details of software have not been revealed yet. Several\ncommercial and non-commercial tools exist to estimate software in the early\nstages. Most software effort estimation methods require software size as one of\nthe important metric inputs and consequently, software size estimation in the\nearly stages becomes essential. One of the approaches that has been used for\nabout two decades in the early size and effort estimation is called use case\npoints. Use case points method relies on the use case diagram to estimate the\nsize and effort of software projects. Although the use case points method has\nbeen widely used, it has some limitations that might adversely affect the\naccuracy of estimation. This paper presents some techniques using fuzzy logic\nand neural networks to improve the accuracy of the use case points method.\nResults showed that an improvement up to 22% can be obtained using the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 06:59:14 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Nassif", "Ali Bou", ""], ["Capretz", "Luiz Fernando", ""], ["Ho", "Danny", ""]]}, {"id": "1612.01086", "submitter": "Bar Hilleli", "authors": "Bar Hilleli and Ran El-Yaniv", "title": "Deep Learning of Robotic Tasks without a Simulator using Strong and Weak\n  Human Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scheme for training a computerized agent to perform complex\nhuman tasks such as highway steering. The scheme is designed to follow a\nnatural learning process whereby a human instructor teaches a computerized\ntrainee. The learning process consists of five elements: (i) unsupervised\nfeature learning; (ii) supervised imitation learning; (iii) supervised reward\ninduction; (iv) supervised safety module construction; and (v) reinforcement\nlearning. We implemented the last four elements of the scheme using deep\nconvolutional networks and applied it to successfully create a computerized\nagent capable of autonomous highway steering over the well-known racing game\nAssetto Corsa. We demonstrate that the use of the last four elements is\nessential to effectively carry out the steering task using vision alone,\nwithout access to a driving simulator internals, and operating in wall-clock\ntime. This is made possible also through the introduction of a safety network,\na novel way for preventing the agent from performing catastrophic mistakes\nduring the reinforcement learning stage.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 08:28:38 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 17:08:16 GMT"}, {"version": "v3", "created": "Sun, 26 Mar 2017 08:43:23 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Hilleli", "Bar", ""], ["El-Yaniv", "Ran", ""]]}, {"id": "1612.01095", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Representing Independence Models with Elementary Triplets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an independence model, the triplets that represent conditional\nindependences between singletons are called elementary. It is known that the\nelementary triplets represent the independence model unambiguously under some\nconditions. In this paper, we show how this representation helps performing\nsome operations with independence models, such as finding the dominant triplets\nor a minimal independence map of an independence model, or computing the union\nor intersection of a pair of independence models, or performing causal\nreasoning. For the latter, we rephrase in terms of conditional independences\nsome of Pearl's results for computing causal effects.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 10:41:33 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1612.01120", "submitter": "Fabio Cozman", "authors": "Fabio Gagliardi Cozman, Denis Deratani Mau\\'a", "title": "The Complexity of Bayesian Networks Specified by Propositional and\n  Relational Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the complexity of inference in Bayesian networks specified by\nlogical languages. We consider representations that range from fragments of\npropositional logic to function-free first-order logic with equality; in doing\nso we cover a variety of plate models and of probabilistic relational models.\nWe study the complexity of inferences when network, query and domain are the\ninput (the inferential and the combined complexity), when the network is fixed\nand query and domain are the input (the query/data complexity), and when the\nnetwork and query are fixed and the domain is the input (the domain\ncomplexity). We draw connections with probabilistic databases and liftability\nresults, and obtain complexity classes that range from polynomial to\nexponential levels.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 13:51:55 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 02:00:14 GMT"}, {"version": "v3", "created": "Fri, 6 Jan 2017 13:07:30 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Cozman", "Fabio Gagliardi", ""], ["Mau\u00e1", "Denis Deratani", ""]]}, {"id": "1612.01197", "submitter": "Chen Liang", "authors": "Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, Ni Lao", "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with\n  Weak Supervision (Short Version)", "comments": "Published in NAMPI workshop at NIPS 2016. Short version of\n  arXiv:1611.00020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending the success of deep neural networks to natural language\nunderstanding and symbolic reasoning requires complex operations and external\nmemory. Recent neural program induction approaches have attempted to address\nthis problem, but are typically limited to differentiable memory, and\nconsequently cannot scale beyond small synthetic tasks. In this work, we\npropose the Manager-Programmer-Computer framework, which integrates neural\nnetworks with non-differentiable memory to support abstract, scalable and\nprecise operations through a friendly neural computer interface. Specifically,\nwe introduce a Neural Symbolic Machine, which contains a sequence-to-sequence\nneural \"programmer\", and a non-differentiable \"computer\" that is a Lisp\ninterpreter with code assist. To successfully apply REINFORCE for training, we\naugment it with approximate gold programs found by an iterative maximum\nlikelihood training process. NSM is able to learn a semantic parser from weak\nsupervision over a large knowledge base. It achieves new state-of-the-art\nperformance on WebQuestionsSP, a challenging semantic parsing dataset, with\nweak supervision. Compared to previous approaches, NSM is end-to-end, therefore\ndoes not rely on feature engineering or domain specific knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 22:29:32 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Liang", "Chen", ""], ["Berant", "Jonathan", ""], ["Le", "Quoc", ""], ["Forbus", "Kenneth D.", ""], ["Lao", "Ni", ""]]}, {"id": "1612.01294", "submitter": "Arnab Ghosh", "authors": "Arnab Ghosh and Viveka Kulharia and Vinay Namboodiri", "title": "Message Passing Multi-Agent GANs", "comments": "The first 2 authors contributed equally for this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communicating and sharing intelligence among agents is an important facet of\nachieving Artificial General Intelligence. As a first step towards this\nchallenge, we introduce a novel framework for image generation: Message Passing\nMulti-Agent Generative Adversarial Networks (MPM GANs). While GANs have\nrecently been shown to be very effective for image generation and other tasks,\nthese networks have been limited to mostly single generator-discriminator\nnetworks. We show that we can obtain multi-agent GANs that communicate through\nmessage passing to achieve better image generation. The objectives of the\nindividual agents in this framework are two fold: a co-operation objective and\na competing objective. The co-operation objective ensures that the message\nsharing mechanism guides the other generator to generate better than itself\nwhile the competing objective encourages each generator to generate better than\nits counterpart. We analyze and visualize the messages that these GANs share\namong themselves in various scenarios. We quantitatively show that the message\nsharing formulation serves as a regularizer for the adversarial training.\nQualitatively, we show that the different generators capture different traits\nof the underlying data distribution.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 10:10:13 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Ghosh", "Arnab", ""], ["Kulharia", "Viveka", ""], ["Namboodiri", "Vinay", ""]]}, {"id": "1612.01399", "submitter": "Hamid Reza Hassanzadeh", "authors": "Hamid Reza Hassanzadeh", "title": "A New Type-II Fuzzy Logic Based Controller for Non-linear Dynamical\n  Systems with Application to a 3-PSP Parallel Robot", "comments": "Master's thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of uncertainty is posed in almost any complex system including\nparallel robots as an outstanding instance of dynamical robotics systems. As\nsuggested by the name, uncertainty, is some missing information that is beyond\nthe knowledge of human thus we may tend to handle it properly to minimize the\nside-effects through the control process.\n  Type-II fuzzy logic has shown its superiority over traditional fuzzy logic\nwhen dealing with uncertainty. Type-II fuzzy logic controllers are however\nnewer and more promising approaches that have been recently applied to various\nfields due to their significant contribution especially when noise (as an\nimportant instance of uncertainty) emerges. During the design of Type-I fuzzy\nlogic systems, we presume that we are almost certain about the fuzzy membership\nfunctions which is not true in many cases. Thus T2FLS as a more realistic\napproach dealing with practical applications might have a lot to offer. Type-II\nfuzzy logic takes into account a higher level of uncertainty, in other words,\nthe membership grade for a type-II fuzzy variable is no longer a crisp number\nbut rather is itself a type-I linguistic term. In this thesis the effects of\nuncertainty in dynamic control of a parallel robot is considered. More\nspecifically, it is intended to incorporate the Type-II Fuzzy Logic paradigm\ninto a model based controller, the so-called computed torque control method,\nand apply the result to a 3 degrees of freedom parallel manipulator.\n  ...\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 15:38:50 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Hassanzadeh", "Hamid Reza", ""]]}, {"id": "1612.01434", "submitter": "Piotr Skowron", "authors": "Piotr Skowron, Martin Lackner, Markus Brill, Dominik Peters, Edith\n  Elkind", "title": "Proportional Rankings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extend the principle of proportional representation to\nrankings. We consider the setting where alternatives need to be ranked based on\napproval preferences. In this setting, proportional representation requires\nthat cohesive groups of voters are represented proportionally in each initial\nsegment of the ranking. Proportional rankings are desirable in situations where\ninitial segments of different lengths may be relevant, e.g., hiring decisions\n(if it is unclear how many positions are to be filled), the presentation of\ncompeting proposals on a liquid democracy platform (if it is unclear how many\nproposals participants are taking into consideration), or recommender systems\n(if a ranking has to accommodate different user types). We study the\nproportional representation provided by several ranking methods and prove\ntheoretical guarantees. Furthermore, we experimentally evaluate these methods\nand present preliminary evidence as to which methods are most suitable for\nproducing proportional rankings.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 17:09:34 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Skowron", "Piotr", ""], ["Lackner", "Martin", ""], ["Brill", "Markus", ""], ["Peters", "Dominik", ""], ["Elkind", "Edith", ""]]}, {"id": "1612.01445", "submitter": "Suleiman Yerima", "authors": "BooJoong Kang, Suleiman Y. Yerima, Sakir Sezer and Kieran McLaughlin", "title": "N-gram Opcode Analysis for Android Malware Detection", "comments": null, "journal-ref": "International Journal on Cyber Situational Awareness, Vol. 1, No.\n  1, pp231-255 (2016)", "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Android malware has been on the rise in recent years due to the increasing\npopularity of Android and the proliferation of third party application markets.\nEmerging Android malware families are increasingly adopting sophisticated\ndetection avoidance techniques and this calls for more effective approaches for\nAndroid malware detection. Hence, in this paper we present and evaluate an\nn-gram opcode features based approach that utilizes machine learning to\nidentify and categorize Android malware. This approach enables automated\nfeature discovery without relying on prior expert or domain knowledge for\npre-determined features. Furthermore, by using a data segmentation technique\nfor feature selection, our analysis is able to scale up to 10-gram opcodes. Our\nexperiments on a dataset of 2520 samples showed an f-measure of 98% using the\nn-gram opcode based approach. We also provide empirical findings that\nillustrate factors that have probable impact on the overall n-gram opcodes\nperformance trends.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 17:33:23 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Kang", "BooJoong", ""], ["Yerima", "Suleiman Y.", ""], ["Sezer", "Sakir", ""], ["McLaughlin", "Kieran", ""]]}, {"id": "1612.01589", "submitter": "Konrad \\.Zo{\\l}na", "authors": "Konrad Zolna", "title": "Improving the Performance of Neural Networks in Regression Tasks Using\n  Drawering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method presented extends a given regression neural network to make its\nperformance improve. The modification affects the learning procedure only,\nhence the extension may be easily omitted during evaluation without any change\nin prediction. It means that the modified model may be evaluated as quickly as\nthe original one but tends to perform better.\n  This improvement is possible because the modification gives better expressive\npower, provides better behaved gradients and works as a regularization. The\nknowledge gained by the temporarily extended neural network is contained in the\nparameters shared with the original neural network.\n  The only cost is an increase in learning time.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 23:28:54 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Zolna", "Konrad", ""]]}, {"id": "1612.01608", "submitter": "Julian Togelius", "authors": "Julian Togelius", "title": "AI Researchers, Video Games Are Your Friends!", "comments": "in Studies in Computational Intelligence Studies in Computational\n  Intelligence, Volume 669 2017. Springer", "journal-ref": null, "doi": "10.1007/978-3-319-48506-5_1", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If you are an artificial intelligence researcher, you should look to video\ngames as ideal testbeds for the work you do. If you are a video game developer,\nyou should look to AI for the technology that makes completely new types of\ngames possible. This chapter lays out the case for both of these propositions.\nIt asks the question \"what can video games do for AI\", and discusses how in\nparticular general video game playing is the ideal testbed for artificial\ngeneral intelligence research. It then asks the question \"what can AI do for\nvideo games\", and lays out a vision for what video games might look like if we\nhad significantly more advanced AI at our disposal. The chapter is based on my\nkeynote at IJCCI 2015, and is written in an attempt to be accessible to a broad\naudience.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 00:46:57 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Togelius", "Julian", ""]]}, {"id": "1612.01691", "submitter": "Arthur Mah\\'eo", "authors": "Arthur Mah\\'eo, Tommaso Urli, Philip Kilby", "title": "Fleet Size and Mix Split-Delivery Vehicle Routing", "comments": "Rich Vehicle Routing, Split Delivery, Fleet Size and Mix, Mixed\n  Integer Programming, Constraint Programming", "journal-ref": null, "doi": null, "report-no": "EP166439", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classic Vehicle Routing Problem (VRP) a fleet of of vehicles has to\nvisit a set of customers while minimising the operations' costs. We study a\nrich variant of the VRP featuring split deliveries, an heterogeneous fleet, and\nvehicle-commodity incompatibility constraints. Our goal is twofold: define the\ncheapest routing and the most adequate fleet.\n  To do so, we split the problem into two interdependent components: a fleet\ndesign component and a routing component. First, we define two Mixed Integer\nProgramming (MIP) formulations for each component. Then we discuss several\nimprovements in the form of valid cuts and symmetry breaking constraints.\n  The main contribution of this paper is a comparison of the four resulting\nmodels for this Rich VRP. We highlight their strengths and weaknesses with\nextensive experiments.\n  Finally, we explore a lightweight integration with Constraint Programming\n(CP). We use a fast CP model which gives good solutions and use the solution to\nwarm-start our models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 07:46:41 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Mah\u00e9o", "Arthur", ""], ["Urli", "Tommaso", ""], ["Kilby", "Philip", ""]]}, {"id": "1612.01746", "submitter": "Peter Karkus", "authors": "Peter Karkus, Andras Kupcsik, David Hsu, Wee Sun Lee", "title": "Factored Contextual Policy Search with Bayesian Optimization", "comments": "BayesOpt 2016, NeurIPS Workshop. A full paper extension is available\n  at arXiv:1904.11761", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scarce data is a major challenge to scaling robot learning to truly complex\ntasks, as we need to generalize locally learned policies over different\n\"contexts\". Bayesian optimization approaches to contextual policy search (CPS)\noffer data-efficient policy learning that generalize over a context space. We\npropose to improve data-efficiency by factoring typically considered contexts\ninto two components: target-type contexts that correspond to a desired outcome\nof the learned behavior, e.g. target position for throwing a ball; and\nenvironment type contexts that correspond to some state of the environment,\ne.g. initial ball position or wind speed. Our key observation is that\nexperience can be directly generalized over target-type contexts. Based on that\nwe introduce Factored Contextual Policy Search with Bayesian Optimization for\nboth passive and active learning settings. Preliminary results show faster\npolicy generalization on a simulated toy problem. A full paper extension is\navailable at arXiv:1904.11761\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 10:51:51 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 04:08:29 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Karkus", "Peter", ""], ["Kupcsik", "Andras", ""], ["Hsu", "David", ""], ["Lee", "Wee Sun", ""]]}, {"id": "1612.01857", "submitter": "Alexa Gopaulsingh Mrs.", "authors": "Alexa Gopaulsingh", "title": "On a Well-behaved Relational Generalisation of Rough Set Approximations", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine non-dual relational extensions of rough set approximations and\nfind an extension which satisfies surprisingly many of the usual rough set\nproperties. We then use this definition to give an explanation for an\nobservation made by Samanta and Chakraborty in their recent paper [P. Samanta\nand M.K. Chakraborty. Interface of rough set systems and modal logics: A\nsurvey. Transactions on Rough Sets XIX, pages 114-137, 2015].\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 08:53:16 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 15:04:20 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Gopaulsingh", "Alexa", ""]]}, {"id": "1612.01887", "submitter": "Jiasen Lu", "authors": "Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher", "title": "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image\n  Captioning", "comments": "12 pages, 11 figures, CVPR2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based neural encoder-decoder frameworks have been widely adopted\nfor image captioning. Most methods force visual attention to be active for\nevery generated word. However, the decoder likely requires little to no visual\ninformation from the image to predict non-visual words such as \"the\" and \"of\".\nOther words that may seem visual can often be predicted reliably just from the\nlanguage model e.g., \"sign\" after \"behind a red stop\" or \"phone\" following\n\"talking on a cell\". In this paper, we propose a novel adaptive attention model\nwith a visual sentinel. At each time step, our model decides whether to attend\nto the image (and if so, to which regions) or to the visual sentinel. The model\ndecides whether to attend to the image and where, in order to extract\nmeaningful information for sequential word generation. We test our method on\nthe COCO image captioning 2015 challenge dataset and Flickr30K. Our approach\nsets the new state-of-the-art by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 16:03:50 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 06:59:15 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Lu", "Jiasen", ""], ["Xiong", "Caiming", ""], ["Parikh", "Devi", ""], ["Socher", "Richard", ""]]}, {"id": "1612.01892", "submitter": "Gautam Singh", "authors": "Gautam Singh, Saemi Jang, Mun Y. Yi", "title": "Cross-Lingual Predicate Mapping Between Linked Data Ontologies", "comments": "11 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies in different natural languages often differ in quality in terms of\nrichness of schema or richness of internal links. This difference is markedly\nvisible when comparing a rich English language ontology with a non-English\nlanguage counterpart. Discovering alignment between them is a useful endeavor\nas it serves as a starting point in bridging the disparity. In particular, our\nwork is motivated by the absence of inter-language links for predicates in the\nlocalised versions of DBpedia. In this paper, we propose and demonstrate an\nad-hoc system to find possible owl:equivalentProperty links between predicates\nin ontologies of different natural languages. We seek to achieve this mapping\nby using pre-existing inter-language links of the resources connected by the\ngiven predicate. Thus, our methodology stresses on semantic similarity rather\nthan lexical. Moreover, through an evaluation, we show that our system is\ncapable of outperforming a baseline system that is similar to the one used in\nrecent OAEI campaigns.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 16:26:33 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Singh", "Gautam", ""], ["Jang", "Saemi", ""], ["Yi", "Mun Y.", ""]]}, {"id": "1612.01895", "submitter": "Xin Wang", "authors": "Xin Wang, Geoffrey Oxholm, Da Zhang, Yuan-Fang Wang", "title": "Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network\n  for Fast Artistic Style Transfer", "comments": "Accepted by CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring artistic styles onto everyday photographs has become an\nextremely popular task in both academia and industry. Recently, offline\ntraining has replaced on-line iterative optimization, enabling nearly real-time\nstylization. When those stylization networks are applied directly to\nhigh-resolution images, however, the style of localized regions often appears\nless similar to the desired artistic style. This is because the transfer\nprocess fails to capture small, intricate textures and maintain correct texture\nscales of the artworks. Here we propose a multimodal convolutional neural\nnetwork that takes into consideration faithful representations of both color\nand luminance channels, and performs stylization hierarchically with multiple\nlosses of increasing scales. Compared to state-of-the-art networks, our network\ncan also perform style transfer in nearly real-time by conducting much more\nsophisticated training offline. By properly handling style and texture cues at\nmultiple scales using several modalities, we can transfer not just large-scale,\nobvious style cues but also subtle, exquisite ones. That is, our scheme can\ngenerate results that are visually pleasing and more similar to multiple\ndesired artistic styles with color and texture cues at multiple scales.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 07:48:25 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 05:27:13 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Wang", "Xin", ""], ["Oxholm", "Geoffrey", ""], ["Zhang", "Da", ""], ["Wang", "Yuan-Fang", ""]]}, {"id": "1612.01939", "submitter": "Baochen Sun", "authors": "Baochen Sun, Jiashi Feng, Kate Saenko", "title": "Correlation Alignment for Unsupervised Domain Adaptation", "comments": "Introduction to CORAL, CORAL-LDA, and Deep CORAL. arXiv admin note:\n  text overlap with arXiv:1511.05547", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we present CORrelation ALignment (CORAL), a simple yet\neffective method for unsupervised domain adaptation. CORAL minimizes domain\nshift by aligning the second-order statistics of source and target\ndistributions, without requiring any target labels. In contrast to subspace\nmanifold methods, it aligns the original feature distributions of the source\nand target domains, rather than the bases of lower-dimensional subspaces. It is\nalso much simpler than other distribution matching methods. CORAL performs\nremarkably well in extensive evaluations on standard benchmark datasets. We\nfirst describe a solution that applies a linear transformation to source\nfeatures to align them with target features before classifier training. For\nlinear classifiers, we propose to equivalently apply CORAL to the classifier\nweights, leading to added efficiency when the number of classifiers is small\nbut the number and dimensionality of target examples are very high. The\nresulting CORAL Linear Discriminant Analysis (CORAL-LDA) outperforms LDA by a\nlarge margin on standard domain adaptation benchmarks. Finally, we extend CORAL\nto learn a nonlinear transformation that aligns correlations of layer\nactivations in deep neural networks (DNNs). The resulting Deep CORAL approach\nworks seamlessly with DNNs and achieves state-of-the-art performance on\nstandard benchmark datasets. Our code is available\nat:~\\url{https://github.com/VisionLearningGroup/CORAL}\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 18:31:57 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Sun", "Baochen", ""], ["Feng", "Jiashi", ""], ["Saenko", "Kate", ""]]}, {"id": "1612.01941", "submitter": "Paolo Dragone", "authors": "Stefano Teso and Paolo Dragone and Andrea Passerini", "title": "Coactive Critiquing: Elicitation of Preferences and Features", "comments": "AAAI'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When faced with complex choices, users refine their own preference criteria\nas they explore the catalogue of options. In this paper we propose an approach\nto preference elicitation suited for this scenario. We extend Coactive\nLearning, which iteratively collects manipulative feedback, to optionally query\nexample critiques. User critiques are integrated into the learning model by\ndynamically extending the feature space. Our formulation natively supports\nconstructive learning tasks, where the option catalogue is generated\non-the-fly. We present an upper bound on the average regret suffered by the\nlearner. Our empirical analysis highlights the promise of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 18:32:40 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Teso", "Stefano", ""], ["Dragone", "Paolo", ""], ["Passerini", "Andrea", ""]]}, {"id": "1612.02088", "submitter": "Shuai Ma", "authors": "Shuai Ma and Jia Yuan Yu", "title": "Transition-based versus State-based Reward Functions for MDPs with\n  Value-at-Risk", "comments": "55th Annual Allerton Conference on Communication, Control, and\n  Computing (Allerton)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, the reward function on current state and action is\nwidely used. When the objective is about the expectation of the (discounted)\ntotal reward only, it works perfectly. However, if the objective involves the\ntotal reward distribution, the result will be wrong. This paper studies\nValue-at-Risk (VaR) problems in short- and long-horizon Markov decision\nprocesses (MDPs) with two reward functions, which share the same expectations.\nFirstly we show that with VaR objective, when the real reward function is\ntransition-based (with respect to action and both current and next states), the\nsimplified (state-based, with respect to action and current state only) reward\nfunction will change the VaR. Secondly, for long-horizon MDPs, we estimate the\nVaR function with the aid of spectral theory and the central limit theorem.\nThirdly, since the estimation method is for a Markov reward process with the\nreward function on current state only, we present a transformation algorithm\nfor the Markov reward process with the reward function on current and next\nstates, in order to estimate the VaR function with an intact total reward\ndistribution.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 01:17:26 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 16:32:47 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 23:50:15 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2018 22:50:03 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Ma", "Shuai", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "1612.02120", "submitter": "Yaron Meirovitch", "authors": "Yaron Meirovitch, Alexander Matveev, Hayk Saribekyan, David Budden,\n  David Rolnick, Gergely Odor, Seymour Knowles-Barley, Thouis Raymond Jones,\n  Hanspeter Pfister, Jeff William Lichtman, Nir Shavit", "title": "A Multi-Pass Approach to Large-Scale Connectomics", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of connectomics faces unprecedented \"big data\" challenges. To\nreconstruct neuronal connectivity, automated pixel-level segmentation is\nrequired for petabytes of streaming electron microscopy data. Existing\nalgorithms provide relatively good accuracy but are unacceptably slow, and\nwould require years to extract connectivity graphs from even a single cubic\nmillimeter of neural tissue. Here we present a viable real-time solution, a\nmulti-pass pipeline optimized for shared-memory multicore systems, capable of\nprocessing data at near the terabyte-per-hour pace of multi-beam electron\nmicroscopes. The pipeline makes an initial fast-pass over the data, and then\nmakes a second slow-pass to iteratively correct errors in the output of the\nfast-pass. We demonstrate the accuracy of a sparse slow-pass reconstruction\nalgorithm and suggest new methods for detecting morphological errors. Our\nfast-pass approach provided many algorithmic challenges, including the design\nand implementation of novel shallow convolutional neural nets and the\nparallelization of watershed and object-merging techniques. We use it to\nreconstruct, from image stack to skeletons, the full dataset of Kasthuri et al.\n(463 GB capturing 120,000 cubic microns) in a matter of hours on a single\nmulticore machine rather than the weeks it has taken in the past on much larger\ndistributed systems.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 05:46:24 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Meirovitch", "Yaron", ""], ["Matveev", "Alexander", ""], ["Saribekyan", "Hayk", ""], ["Budden", "David", ""], ["Rolnick", "David", ""], ["Odor", "Gergely", ""], ["Knowles-Barley", "Seymour", ""], ["Jones", "Thouis Raymond", ""], ["Pfister", "Hanspeter", ""], ["Lichtman", "Jeff William", ""], ["Shavit", "Nir", ""]]}, {"id": "1612.02136", "submitter": "Yanran Li", "authors": "Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, Wenjie Li", "title": "Mode Regularized Generative Adversarial Networks", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Generative Adversarial Networks achieve state-of-the-art results on\na variety of generative tasks, they are regarded as highly unstable and prone\nto miss modes. We argue that these bad behaviors of GANs are due to the very\nparticular functional shape of the trained discriminators in high dimensional\nspaces, which can easily make training stuck or push probability mass in the\nwrong direction, towards that of higher concentration than that of the data\ngenerating distribution. We introduce several ways of regularizing the\nobjective, which can dramatically stabilize the training of GAN models. We also\nshow that our regularizers can help the fair distribution of probability mass\nacross the modes of the data generating distribution, during the early phases\nof training and thus providing a unified solution to the missing modes problem.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 07:45:38 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 06:08:37 GMT"}, {"version": "v3", "created": "Sun, 18 Dec 2016 05:55:22 GMT"}, {"version": "v4", "created": "Mon, 20 Feb 2017 05:01:27 GMT"}, {"version": "v5", "created": "Thu, 2 Mar 2017 06:28:13 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Che", "Tong", ""], ["Li", "Yanran", ""], ["Jacob", "Athul Paul", ""], ["Bengio", "Yoshua", ""], ["Li", "Wenjie", ""]]}, {"id": "1612.02161", "submitter": "Marco Cusumano-Towner", "authors": "Marco F. Cusumano-Towner, Vikash K. Mansinghka", "title": "Measuring the non-asymptotic convergence of sequential Monte Carlo\n  samplers using probabilistic programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key limitation of sampling algorithms for approximate inference is that it\nis difficult to quantify their approximation error. Widely used sampling\nschemes, such as sequential importance sampling with resampling and\nMetropolis-Hastings, produce output samples drawn from a distribution that may\nbe far from the target posterior distribution. This paper shows how to\nupper-bound the symmetric KL divergence between the output distribution of a\nbroad class of sequential Monte Carlo (SMC) samplers and their target posterior\ndistributions, subject to assumptions about the accuracy of a separate\ngold-standard sampler. The proposed method applies to samplers that combine\nmultiple particles, multinomial resampling, and rejuvenation kernels. The\nexperiments show the technique being used to estimate bounds on the divergence\nof SMC samplers for posterior inference in a Bayesian linear regression model\nand a Dirichlet process mixture model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 09:29:58 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 20:33:52 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Cusumano-Towner", "Marco F.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1612.02255", "submitter": "Armando Vieira", "authors": "Armando Vieira", "title": "Knowledge Representation in Graphs using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graphs (KG) constitute a flexible representation of complex\nrelationships between entities particularly useful for biomedical data. These\nKG, however, are very sparse with many missing edges (facts) and the\nvisualisation of the mesh of interactions nontrivial. Here we apply a\ncompositional model to embed nodes and relationships into a vectorised semantic\nspace to perform graph completion. A visualisation tool based on Convolutional\nNeural Networks and Self-Organised Maps (SOM) is proposed to extract high-level\ninsights from the KG. We apply this technique to a subset of CTD, containing\ninteractions of compounds with human genes / proteins and show that the\nperformance is comparable to the one obtained by structural models.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 14:10:56 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Vieira", "Armando", ""]]}, {"id": "1612.02310", "submitter": "Ji Feng", "authors": "Ji Feng, Qingsheng Zhu, Jinlong Huang, Lijun Yang", "title": "Extend natural neighbor: a novel classification method with\n  self-adaptive neighborhood parameters in different stages", "comments": "10 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various kinds of k-nearest neighbor (KNN) based classification methods are\nthe bases of many well-established and high-performance pattern-recognition\ntechniques, but both of them are vulnerable to their parameter choice.\nEssentially, the challenge is to detect the neighborhood of various data sets,\nwhile utterly ignorant of the data characteristic. This article introduces a\nnew supervised classification method: the extend natural neighbor (ENaN)\nmethod, and shows that it provides a better classification result without\nchoosing the neighborhood parameter artificially. Unlike the original KNN based\nmethod which needs a prior k, the ENaNE method predicts different k in\ndifferent stages. Therefore, the ENaNE method is able to learn more from\nflexible neighbor information both in training stage and testing stage, and\nprovide a better classification result.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 16:13:52 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Feng", "Ji", ""], ["Zhu", "Qingsheng", ""], ["Huang", "Jinlong", ""], ["Yang", "Lijun", ""]]}, {"id": "1612.02487", "submitter": "Luana Micallef", "authors": "Luana Micallef, Iiris Sundin, Pekka Marttinen, Muhammad Ammad-ud-din,\n  Tomi Peltola, Marta Soare, Giulio Jacucci, Samuel Kaski", "title": "Interactive Elicitation of Knowledge on Feature Relevance Improves\n  Predictions in Small Data Sets", "comments": "in Proceedings of the 22nd International Conference on Intelligent\n  User Interfaces (IUI 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Providing accurate predictions is challenging for machine learning algorithms\nwhen the number of features is larger than the number of samples in the data.\nPrior knowledge can improve machine learning models by indicating relevant\nvariables and parameter values. Yet, this prior knowledge is often tacit and\nonly available from domain experts. We present a novel approach that uses\ninteractive visualization to elicit the tacit prior knowledge and uses it to\nimprove the accuracy of prediction models. The main component of our approach\nis a user model that models the domain expert's knowledge of the relevance of\ndifferent features for a prediction task. In particular, based on the expert's\nearlier input, the user model guides the selection of the features on which to\nelicit user's knowledge next. The results of a controlled user study show that\nthe user model significantly improves prior knowledge elicitation and\nprediction accuracy, when predicting the relative citation counts of scientific\ndocuments in a specific domain.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 23:31:26 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 15:16:05 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Micallef", "Luana", ""], ["Sundin", "Iiris", ""], ["Marttinen", "Pekka", ""], ["Ammad-ud-din", "Muhammad", ""], ["Peltola", "Tomi", ""], ["Soare", "Marta", ""], ["Jacucci", "Giulio", ""], ["Kaski", "Samuel", ""]]}, {"id": "1612.02516", "submitter": "Yichen Chen", "authors": "Yichen Chen and Mengdi Wang", "title": "Stochastic Primal-Dual Methods and Sample Complexity of Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online estimation of the optimal policy of a Markov decision\nprocess (MDP). We propose a class of Stochastic Primal-Dual (SPD) methods which\nexploit the inherent minimax duality of Bellman equations. The SPD methods\nupdate a few coordinates of the value and policy estimates as a new state\ntransition is observed. These methods use small storage and has low\ncomputational complexity per iteration. The SPD methods find an\nabsolute-$\\epsilon$-optimal policy, with high probability, using\n$\\mathcal{O}\\left(\\frac{|\\mathcal{S}|^4 |\\mathcal{A}|^2\\sigma^2\n}{(1-\\gamma)^6\\epsilon^2} \\right)$ iterations/samples for the infinite-horizon\ndiscounted-reward MDP and $\\mathcal{O}\\left(\\frac{|\\mathcal{S}|^4\n|\\mathcal{A}|^2H^6\\sigma^2 }{\\epsilon^2} \\right)$ for the finite-horizon MDP.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 03:05:41 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Chen", "Yichen", ""], ["Wang", "Mengdi", ""]]}, {"id": "1612.02526", "submitter": "Vatsal Sharan", "authors": "Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant", "title": "Prediction with a Short Memory", "comments": "Updates for STOC camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting the next observation given a sequence\nof past observations, and consider the extent to which accurate prediction\nrequires complex algorithms that explicitly leverage long-range dependencies.\nPerhaps surprisingly, our positive results show that for a broad class of\nsequences, there is an algorithm that predicts well on average, and bases its\npredictions only on the most recent few observation together with a set of\nsimple summary statistics of the past observations. Specifically, we show that\nfor any distribution over observations, if the mutual information between past\nobservations and future observations is upper bounded by $I$, then a simple\nMarkov model over the most recent $I/\\epsilon$ observations obtains expected KL\nerror $\\epsilon$---and hence $\\ell_1$ error $\\sqrt{\\epsilon}$---with respect to\nthe optimal predictor that has access to the entire past and knows the data\ngenerating distribution. For a Hidden Markov Model with $n$ hidden states, $I$\nis bounded by $\\log n$, a quantity that does not depend on the mixing time, and\nwe show that the trivial prediction algorithm based on the empirical\nfrequencies of length $O(\\log n/\\epsilon)$ windows of observations achieves\nthis error, provided the length of the sequence is $d^{\\Omega(\\log\nn/\\epsilon)}$, where $d$ is the size of the observation alphabet.\n  We also establish that this result cannot be improved upon, even for the\nclass of HMMs, in the following two senses: First, for HMMs with $n$ hidden\nstates, a window length of $\\log n/\\epsilon$ is information-theoretically\nnecessary to achieve expected $\\ell_1$ error $\\sqrt{\\epsilon}$. Second, the\n$d^{\\Theta(\\log n/\\epsilon)}$ samples required to estimate the Markov model for\nan observation alphabet of size $d$ is necessary for any computationally\ntractable learning algorithm, assuming the hardness of strongly refuting a\ncertain class of CSPs.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 04:18:09 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 17:51:39 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 07:01:47 GMT"}, {"version": "v4", "created": "Sun, 27 May 2018 01:30:15 GMT"}, {"version": "v5", "created": "Thu, 28 Jun 2018 01:54:04 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Sharan", "Vatsal", ""], ["Kakade", "Sham", ""], ["Liang", "Percy", ""], ["Valiant", "Gregory", ""]]}, {"id": "1612.02587", "submitter": "Juerg Kohlas", "authors": "Juerg Kohlas", "title": "Inverses, Conditionals and Compositional Operators in Separative\n  Valuation Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional models were introduce by Jirousek and Shenoy in the general\nframework of valuation-based systems. They based their theory on an axiomatic\nsystem of valuations involving not only the operations of combination and\nmarginalisation, but also of removal. They claimed that this systems covers\nbesides the classical case of discrete probability distributions, also the\ncases of Gaussian densities and belief functions, and many other systems.\n  Whereas their results on the compositional operator are correct, the\naxiomatic basis is not sufficient to cover the examples claimed above. We\npropose here a different axiomatic system of valuation algebras, which permits\na rigorous mathematical theory of compositional operators in valuation-based\nsystems and covers all the examples mentioned above. It extends the classical\ntheory of inverses in semigroup theory and places thereby the present theory\ninto its proper mathematical frame. Also this theory sheds light on the\ndifferent structures of valuation-based systems, like regular algebras\n(represented by probability potentials), canncellative algebras (Gaussian\npotentials) and general separative algebras (density functions).\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 10:34:16 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Kohlas", "Juerg", ""]]}, {"id": "1612.02660", "submitter": "Maurizio Negri", "authors": "Maurizio Negri", "title": "Decision Theory in an Algebraic Setting", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In decision theory an act is a function from a set of conditions to the set\nof real numbers. The set of conditions is a partition in some algebra of\nevents. The expected value of an act can be calculated when a probability\nmeasure is given. We adopt an algebraic point of view by substituting the\nalgebra of events with a finite distributive lattice and the probability\nmeasure with a lattice valuation. We introduce a partial order on acts that\ngeneralizes the dominance relation and show that the set of acts is a lattice\nwith respect to this order. Finally we analyze some different kinds of\ncomparison between acts, without supposing a common set of conditions for the\nacts to be compared.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 18:42:16 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Negri", "Maurizio", ""]]}, {"id": "1612.02684", "submitter": "Damian Kurpiewski", "authors": "Wojciech Jamroga, Micha{\\l} Knapik, Damian Kurpiewski", "title": "Fixpoint Approximation of Strategic Abilities under Imperfect\n  Information", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model checking of strategic ability under imperfect information is known to\nbe hard. The complexity results range from NP-completeness to undecidability,\ndepending on the precise setup of the problem. No less importantly, fixpoint\nequivalences do not generally hold for imperfect information strategies, which\nseriously hampers incremental synthesis of winning strategies. In this paper,\nwe propose translations of ATLir formulae that provide lower and upper bounds\nfor their truth values, and are cheaper to verify than the original\nspecifications. That is, if the expression is verified as true then the\ncorresponding formula of ATLir should also hold in the given model. We begin by\nshowing where the straightforward approach does not work. Then, we propose how\nit can be modified to obtain guaranteed lower bounds. To this end, we alter the\nnext-step operator in such a way that traversing one's indistinguishability\nrelation is seen as atomic activity. Most interestingly, the lower\napproximation is provided by a fixpoint expression that uses a nonstandard\nvariant of the next-step ability operator. We show the correctness of the\ntranslations, establish their computational complexity, and validate the\napproach by experiments with a scalable scenario of Bridge play.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 15:07:57 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 12:48:34 GMT"}, {"version": "v3", "created": "Mon, 13 Mar 2017 16:08:55 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Jamroga", "Wojciech", ""], ["Knapik", "Micha\u0142", ""], ["Kurpiewski", "Damian", ""]]}, {"id": "1612.02734", "submitter": "Peter Sadowski", "authors": "Pierre Baldi, Peter Sadowski, Zhiqin Lu", "title": "Learning in the Machine: Random Backpropagation and the Deep Learning\n  Channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random backpropagation (RBP) is a variant of the backpropagation algorithm\nfor training neural networks, where the transpose of the forward matrices are\nreplaced by fixed random matrices in the calculation of the weight updates. It\nis remarkable both because of its effectiveness, in spite of using random\nmatrices to communicate error information, and because it completely removes\nthe taxing requirement of maintaining symmetric weights in a physical neural\nsystem. To better understand random backpropagation, we first connect it to the\nnotions of local learning and learning channels. Through this connection, we\nderive several alternatives to RBP, including skipped RBP (SRPB), adaptive RBP\n(ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their\ncomputational complexity. We then study their behavior through simulations\nusing the MNIST and CIFAR-10 bechnmark datasets. These simulations show that\nmost of these variants work robustly, almost as well as backpropagation, and\nthat multiplication by the derivatives of the activation functions is\nimportant. As a follow-up, we study also the low-end of the number of bits\nrequired to communicate error information over the learning channel. We then\nprovide partial intuitive explanations for some of the remarkable properties of\nRBP and its variations. Finally, we prove several mathematical results,\nincluding the convergence to fixed points of linear chains of arbitrary length,\nthe convergence to fixed points of linear autoencoders with decorrelated data,\nthe long-term existence of solutions for linear systems with a single hidden\nlayer and convergence in special cases, and the convergence to fixed points of\nnon-linear chains, when the derivative of the activation functions is included.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 17:15:45 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 17:29:20 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Baldi", "Pierre", ""], ["Sadowski", "Peter", ""], ["Lu", "Zhiqin", ""]]}, {"id": "1612.02739", "submitter": "Martin Pecka", "authors": "Martin Pecka, Karel Zimmermann, Michal Rein\\v{s}tein, Tom\\'a\\v{s}\n  Svoboda", "title": "Controlling Robot Morphology from Incomplete Measurements", "comments": "Accepted into IEEE Transactions to Industrial Electronics, Special\n  Section on Motion Control for Novel Emerging Robotic Devices and Systems", "journal-ref": null, "doi": "10.1109/TIE.2016.2580125", "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile robots with complex morphology are essential for traversing rough\nterrains in Urban Search & Rescue missions (USAR). Since teleoperation of the\ncomplex morphology causes high cognitive load of the operator, the morphology\nis controlled autonomously. The autonomous control measures the robot state and\nsurrounding terrain which is usually only partially observable, and thus the\ndata are often incomplete. We marginalize the control over the missing\nmeasurements and evaluate an explicit safety condition. If the safety condition\nis violated, tactile terrain exploration by the body-mounted robotic arm\ngathers the missing data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 17:40:57 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Pecka", "Martin", ""], ["Zimmermann", "Karel", ""], ["Rein\u0161tein", "Michal", ""], ["Svoboda", "Tom\u00e1\u0161", ""]]}, {"id": "1612.02741", "submitter": "Lili Mou", "authors": "Lili Mou, Zhengdong Lu, Hang Li, Zhi Jin", "title": "Coupling Distributed and Symbolic Execution for Natural Language Queries", "comments": "Accepted by ICML-17; also presented at ICLR-17 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building neural networks to query a knowledge base (a table) with natural\nlanguage is an emerging research topic in deep learning. An executor for table\nquerying typically requires multiple steps of execution because queries may\nhave complicated structures. In previous studies, researchers have developed\neither fully distributed executors or symbolic executors for table querying. A\ndistributed executor can be trained in an end-to-end fashion, but is weak in\nterms of execution efficiency and explicit interpretability. A symbolic\nexecutor is efficient in execution, but is very difficult to train especially\nat initial stages. In this paper, we propose to couple distributed and symbolic\nexecution for natural language queries, where the symbolic executor is\npretrained with the distributed executor's intermediate execution results in a\nstep-by-step fashion. Experiments show that our approach significantly\noutperforms both distributed and symbolic executors, exhibiting high accuracy,\nhigh learning efficiency, high execution efficiency, and high interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 17:45:16 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 11:37:44 GMT"}, {"version": "v3", "created": "Tue, 25 Apr 2017 20:39:57 GMT"}, {"version": "v4", "created": "Fri, 16 Jun 2017 14:33:31 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Mou", "Lili", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Jin", "Zhi", ""]]}, {"id": "1612.02757", "submitter": "Adam Earle", "authors": "Andrew M. Saxe, Adam Earle, Benjamin Rosman", "title": "Hierarchy through Composition with Linearly Solvable Markov Decision\n  Processes", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical architectures are critical to the scalability of reinforcement\nlearning methods. Current hierarchical frameworks execute actions serially,\nwith macro-actions comprising sequences of primitive actions. We propose a\nnovel alternative to these control hierarchies based on concurrent execution of\nmany actions in parallel. Our scheme uses the concurrent compositionality\nprovided by the linearly solvable Markov decision process (LMDP) framework,\nwhich naturally enables a learning agent to draw on several macro-actions\nsimultaneously to solve new tasks. We introduce the Multitask LMDP module,\nwhich maintains a parallel distributed representation of tasks and may be\nstacked to form deep hierarchies abstracted in space and time.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 18:25:31 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Saxe", "Andrew M.", ""], ["Earle", "Adam", ""], ["Rosman", "Benjamin", ""]]}, {"id": "1612.02795", "submitter": "Heejin Ahn", "authors": "Heejin Ahn and Domitilla Del Vecchio", "title": "Safety Verification and Control for Collision Avoidance at Road\n  Intersections", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the design of a supervisory algorithm that monitors\nsafety at road intersections and overrides drivers with a safe input when\nnecessary. The design of the supervisor consists of two parts: safety\nverification and control design. Safety verification is the problem to\ndetermine if vehicles will be able to cross the intersection without colliding\nwith current drivers' inputs. We translate this safety verification problem\ninto a jobshop scheduling problem, which minimizes the maximum lateness and\nevaluates if the optimal cost is zero. The zero optimal cost corresponds to the\ncase in which all vehicles can cross each conflict area without collisions.\nComputing the optimal cost requires solving a Mixed Integer Nonlinear\nProgramming (MINLP) problem due to the nonlinear second-order dynamics of the\nvehicles. We therefore estimate this optimal cost by formulating two related\nMixed Integer Linear Programming (MILP) problems that assume simpler vehicle\ndynamics. We prove that these two MILP problems yield lower and upper bounds of\nthe optimal cost. We also quantify the worst case approximation errors of these\nMILP problems. We design the supervisor to override the vehicles with a safe\ncontrol input if the MILP problem that computes the upper bound yields a\npositive optimal cost. We theoretically demonstrate that the supervisor keeps\nthe intersection safe and is non-blocking. Computer simulations further\nvalidate that the algorithms can run in real time for problems of realistic\nsize.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 20:24:01 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Ahn", "Heejin", ""], ["Del Vecchio", "Domitilla", ""]]}, {"id": "1612.02814", "submitter": "Ting Chen Ting Chen", "authors": "Ting Chen and Yizhou Sun", "title": "Task-Guided and Path-Augmented Heterogeneous Network Embedding for\n  Author Identification", "comments": "Accepted by WSDM 2017. This is an extended version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of author identification under\ndouble-blind review setting, which is to identify potential authors given\ninformation of an anonymized paper. Different from existing approaches that\nrely heavily on feature engineering, we propose to use network embedding\napproach to address the problem, which can automatically represent nodes into\nlower dimensional feature vectors. However, there are two major limitations in\nrecent studies on network embedding: (1) they are usually general-purpose\nembedding methods, which are independent of the specific tasks; and (2) most of\nthese approaches can only deal with homogeneous networks, where the\nheterogeneity of the network is ignored. Hence, challenges faced here are two\nfolds: (1) how to embed the network under the guidance of the author\nidentification task, and (2) how to select the best type of information due to\nthe heterogeneity of the network.\n  To address the challenges, we propose a task-guided and path-augmented\nheterogeneous network embedding model. In our model, nodes are first embedded\nas vectors in latent feature space. Embeddings are then shared and jointly\ntrained according to task-specific and network-general objectives. We extend\nthe existing unsupervised network embedding to incorporate meta paths in\nheterogeneous networks, and select paths according to the specific task. The\nguidance from author identification task for network embedding is provided both\nexplicitly in joint training and implicitly during meta path selection. Our\nexperiments demonstrate that by using path-augmented network embedding with\ntask guidance, our model can obtain significantly better accuracy at\nidentifying the true authors comparing to existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 20:56:48 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 04:20:03 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Chen", "Ting", ""], ["Sun", "Yizhou", ""]]}, {"id": "1612.02879", "submitter": "Vivek Veeriah", "authors": "Vivek Veeriah, Shangtong Zhang, Richard S. Sutton", "title": "Learning Representations by Stochastic Meta-Gradient Descent in Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations are fundamental to artificial intelligence. The performance\nof a learning system depends on the type of representation used for\nrepresenting the data. Typically, these representations are hand-engineered\nusing domain knowledge. More recently, the trend is to learn these\nrepresentations through stochastic gradient descent in multi-layer neural\nnetworks, which is called backprop. Learning the representations directly from\nthe incoming data stream reduces the human labour involved in designing a\nlearning system. More importantly, this allows in scaling of a learning system\nfor difficult tasks. In this paper, we introduce a new incremental learning\nalgorithm called crossprop, which learns incoming weights of hidden units based\non the meta-gradient descent approach, that was previously introduced by Sutton\n(1992) and Schraudolph (1999) for learning step-sizes. The final update\nequation introduces an additional memory parameter for each of these weights\nand generalizes the backprop update equation. From our experiments, we show\nthat crossprop learns and reuses its feature representation while tackling new\nand unseen tasks whereas backprop relearns a new feature representation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 00:56:42 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 14:53:00 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Veeriah", "Vivek", ""], ["Zhang", "Shangtong", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1612.02904", "submitter": "Davoud Mougouei", "authors": "Davoud Mougouei and David Powers", "title": "GOTM: a Goal-Oriented Framework for Capturing Uncertainty of Medical\n  Treatments", "comments": "Idea Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been widely recognized that uncertainty is an inevitable aspect of\ndiagnosis and treatment of medical disorders. Such uncertainties hence, need to\nbe considered in computerized medical models. The existing medical modeling\ntechniques however, have mainly focused on capturing uncertainty associated\nwith diagnosis of medical disorders while ignoring uncertainty of treatments.\nTo tackle this issue, we have proposed using a fuzzy-based modeling and\ndescription technique for capturing uncertainties in treatment plans. We have\nfurther contributed a formal framework which allows for goal-oriented modeling\nand analysis of medical treatments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 04:02:34 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 05:51:20 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mougouei", "Davoud", ""], ["Powers", "David", ""]]}, {"id": "1612.03055", "submitter": "Jessa Bekker", "authors": "Jessa Bekker, Arjen Hommersom, Martijn Lappenschaar, Jesse Davis", "title": "Measuring Adverse Drug Effects on Multimorbity using Tractable Bayesian\n  Networks", "comments": "Machine Learning for Health @ NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing patients with multimorbidity often results in polypharmacy: the\nprescription of multiple drugs. However, the long-term effects of specific\ncombinations of drugs and diseases are typically unknown. In particular, drugs\nprescribed for one condition may result in adverse effects for the other. To\ninvestigate which types of drugs may affect the further progression of\nmultimorbidity, we query models of diseases and prescriptions that are learned\nfrom primary care data. State-of-the-art tractable Bayesian network\nrepresentations, on which such complex queries can be computed efficiently, are\nemployed for these large medical networks. Our results confirm that\nprescriptions may lead to unintended negative consequences in further\ndevelopment of multimorbidity in cardiovascular diseases. Moreover, a drug\ntreatment for one disease group may affect diseases of another group.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 15:25:03 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Bekker", "Jessa", ""], ["Hommersom", "Arjen", ""], ["Lappenschaar", "Martijn", ""], ["Davis", "Jesse", ""]]}, {"id": "1612.03117", "submitter": "Kim Peter Wabersich", "authors": "Kim Peter Wabersich and Marc Toussaint", "title": "Advancing Bayesian Optimization: The Mixed-Global-Local (MGL) Kernel and\n  Length-Scale Cool Down", "comments": "Long version of accepted NIPS BayesOpt 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Optimization (BO) has become a core method for solving expensive\nblack-box optimization problems. While much research focussed on the choice of\nthe acquisition function, we focus on online length-scale adaption and the\nchoice of kernel function. Instead of choosing hyperparameters in view of\nmaximum likelihood on past data, we propose to use the acquisition function to\ndecide on hyperparameter adaptation more robustly and in view of the future\noptimization progress. Further, we propose a particular kernel function that\nincludes non-stationarity and local anisotropy and thereby implicitly\nintegrates the efficiency of local convex optimization with global Bayesian\noptimization. Comparisons to state-of-the art BO methods underline the\nefficiency of these mechanisms on global optimization benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 18:20:00 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Wabersich", "Kim Peter", ""], ["Toussaint", "Marc", ""]]}, {"id": "1612.03211", "submitter": "Xiaolin Andy Li", "authors": "Rajendra Rana Bhat, Vivek Viswanath, Xiaolin Li", "title": "DeepCancer: Detecting Cancer through Gene Expressions via Deep\n  Generative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcriptional profiling on microarrays to obtain gene expressions has been\nused to facilitate cancer diagnosis. We propose a deep generative machine\nlearning architecture (called DeepCancer) that learn features from unlabeled\nmicroarray data. These models have been used in conjunction with conventional\nclassifiers that perform classification of the tissue samples as either being\ncancerous or non-cancerous. The proposed model has been tested on two different\nclinical datasets. The evaluation demonstrates that DeepCancer model achieves a\nvery high precision score, while significantly controlling the false positive\nand false negative scores.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 23:01:12 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 16:27:34 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Bhat", "Rajendra Rana", ""], ["Viswanath", "Vivek", ""], ["Li", "Xiaolin", ""]]}, {"id": "1612.03224", "submitter": "Zhe Yu", "authors": "Zhe Yu, Nicholas A. Kraft, Tim Menzies", "title": "Finding Better Active Learners for Faster Literature Reviews", "comments": "23 pages, 5 figures, 3 tables, accepted for publication in EMSE\n  journal", "journal-ref": null, "doi": "10.1007/s10664-017-9587-0", "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature reviews can be time-consuming and tedious to complete. By\ncataloging and refactoring three state-of-the-art active learning techniques\nfrom evidence-based medicine and legal electronic discovery, this paper finds\nand implements FASTREAD, a faster technique for studying a large corpus of\ndocuments. This paper assesses FASTREAD using datasets generated from existing\nSE literature reviews (Hall, Wahono, Radjenovi\\'c, Kitchenham et al.). Compared\nto manual methods, FASTREAD lets researchers find 95% relevant studies after\nreviewing an order of magnitude fewer papers. Compared to other\nstate-of-the-art automatic methods, FASTREAD reviews 20-50% fewer studies while\nfinding same number of relevant primary studies in a systematic literature\nreview.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 00:04:22 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 19:45:11 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 13:28:16 GMT"}, {"version": "v4", "created": "Tue, 28 Nov 2017 20:49:50 GMT"}, {"version": "v5", "created": "Fri, 2 Feb 2018 15:12:36 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Yu", "Zhe", ""], ["Kraft", "Nicholas A.", ""], ["Menzies", "Tim", ""]]}, {"id": "1612.03242", "submitter": "Han Zhang", "authors": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang,\n  Xiaolei Huang, Dimitris Metaxas", "title": "StackGAN: Text to Photo-realistic Image Synthesis with Stacked\n  Generative Adversarial Networks", "comments": "ICCV 2017 Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing high-quality images from text descriptions is a challenging\nproblem in computer vision and has many practical applications. Samples\ngenerated by existing text-to-image approaches can roughly reflect the meaning\nof the given descriptions, but they fail to contain necessary details and vivid\nobject parts. In this paper, we propose Stacked Generative Adversarial Networks\n(StackGAN) to generate 256x256 photo-realistic images conditioned on text\ndescriptions. We decompose the hard problem into more manageable sub-problems\nthrough a sketch-refinement process. The Stage-I GAN sketches the primitive\nshape and colors of the object based on the given text description, yielding\nStage-I low-resolution images. The Stage-II GAN takes Stage-I results and text\ndescriptions as inputs, and generates high-resolution images with\nphoto-realistic details. It is able to rectify defects in Stage-I results and\nadd compelling details with the refinement process. To improve the diversity of\nthe synthesized images and stabilize the training of the conditional-GAN, we\nintroduce a novel Conditioning Augmentation technique that encourages\nsmoothness in the latent conditioning manifold. Extensive experiments and\ncomparisons with state-of-the-arts on benchmark datasets demonstrate that the\nproposed method achieves significant improvements on generating photo-realistic\nimages conditioned on text descriptions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 03:11:37 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 02:18:21 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zhang", "Han", ""], ["Xu", "Tao", ""], ["Li", "Hongsheng", ""], ["Zhang", "Shaoting", ""], ["Wang", "Xiaogang", ""], ["Huang", "Xiaolei", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "1612.03284", "submitter": "Yongqing Liang", "authors": "Yongqing Liang", "title": "Salient Object Detection with Convex Hull Overlap", "comments": "Published in: 2018 IEEE International Conference on Big Data (Big\n  Data)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection plays an important part in a vision system to detect\nimportant regions. Convolutional neural network (CNN) based methods directly\ntrain their models with large-scale datasets, but what is the crucial feature\nfor saliency is still a problem. In this paper, we establish a novel bottom-up\nfeature named convex hull overlap (CHO), combining with appearance contrast\nfeatures, to detect salient objects. CHO feature is a kind of enhanced Gestalt\ncue. Psychologists believe that surroundedness reflects objects overlap\nrelationship. An object which is on the top of the others is attractive. Our\nmethod significantly differs from other earlier works in (1) We set up a\nhand-crafted feature to detect salient object that our model does not need to\nbe trained by large-scale datasets; (2) Previous works only focus on appearance\nfeatures, while our CHO feature makes up the gap between the spatial object\ncovering and the object's saliency. Our experiments on a large number of public\ndatasets have obtained very positive results.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 12:42:10 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 18:14:11 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Liang", "Yongqing", ""]]}, {"id": "1612.03328", "submitter": "Pedram Daee", "authors": "Pedram Daee, Tomi Peltola, Marta Soare, Samuel Kaski", "title": "Knowledge Elicitation via Sequential Probabilistic Inference for\n  High-Dimensional Prediction", "comments": "22 pages, 9 figures. The paper is published in Machine Learning\n  journal (http://rdcu.be/t9KF). Codes and data available at\n  https://github.com/HIIT/knowledge-elicitation-for-linear-regression, Machine\n  Learning, (2017)", "journal-ref": null, "doi": "10.1007/s10994-017-5651-7", "report-no": null, "categories": "cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction in a small-sized sample with a large number of covariates, the\n\"small n, large p\" problem, is challenging. This setting is encountered in\nmultiple applications, such as precision medicine, where obtaining additional\nsamples can be extremely costly or even impossible, and extensive research\neffort has recently been dedicated to finding principled solutions for accurate\nprediction. However, a valuable source of additional information, domain\nexperts, has not yet been efficiently exploited. We formulate knowledge\nelicitation generally as a probabilistic inference process, where expert\nknowledge is sequentially queried to improve predictions. In the specific case\nof sparse linear regression, where we assume the expert has knowledge about the\nvalues of the regression coefficients or about the relevance of the features,\nwe propose an algorithm and computational approximation for fast and efficient\ninteraction, which sequentially identifies the most informative features on\nwhich to query expert knowledge. Evaluations of our method in experiments with\nsimulated and real users show improved prediction accuracy already with a small\neffort from the expert.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 18:11:32 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 13:08:56 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Daee", "Pedram", ""], ["Peltola", "Tomi", ""], ["Soare", "Marta", ""], ["Kaski", "Samuel", ""]]}, {"id": "1612.03353", "submitter": "Seiji Isotani", "authors": "Judson Bandeira, Ig Ibert Bittencourt, Patricia Espinheira and Seiji\n  Isotani", "title": "FOCA: A Methodology for Ontology Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling an ontology is a hard and time-consuming task. Although\nmethodologies are useful for ontologists to create good ontologies, they do not\nhelp with the task of evaluating the quality of the ontology to be reused. For\nthese reasons, it is imperative to evaluate the quality of the ontology after\nconstructing it or before reusing it. Few studies usually present only a set of\ncriteria and questions, but no guidelines to evaluate the ontology. The effort\nto evaluate an ontology is very high as there is a huge dependence on the\nevaluator's expertise to understand the criteria and questions in depth.\nMoreover, the evaluation is still very subjective. This study presents a novel\nmethodology for ontology evaluation, taking into account three fundamental\nprinciples: i) it is based on the Goal, Question, Metric approach for empirical\nevaluation; ii) the goals of the methodologies are based on the roles of\nknowledge representations combined with specific evaluation criteria; iii) each\nontology is evaluated according to the type of ontology. The methodology was\nempirically evaluated using different ontologists and ontologies of the same\ndomain. The main contributions of this study are: i) defining a step-by-step\napproach to evaluate the quality of an ontology; ii) proposing an evaluation\nbased on the roles of knowledge representations; iii) the explicit difference\nof the evaluation according to the type of the ontology iii) a questionnaire to\nevaluate the ontologies; iv) a statistical model that automatically calculates\nthe quality of the ontologies.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 22:38:42 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 18:21:55 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Bandeira", "Judson", ""], ["Bittencourt", "Ig Ibert", ""], ["Espinheira", "Patricia", ""], ["Isotani", "Seiji", ""]]}, {"id": "1612.03364", "submitter": "Baojian Zhou", "authors": "Feng Chen, Baojian Zhou", "title": "Technical Report: A Generalized Matching Pursuit Approach for\n  Graph-Structured Sparsity", "comments": "International Joint Conference on Artificial Intelligence, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sparsity-constrained optimization is an important and challenging problem\nthat has wide applicability in data mining, machine learning, and statistics.\nIn this paper, we focus on sparsity-constrained optimization in cases where the\ncost function is a general nonlinear function and, in particular, the sparsity\nconstraint is defined by a graph-structured sparsity model. Existing methods\nexplore this problem in the context of sparse estimation in linear models. To\nthe best of our knowledge, this is the first work to present an efficient\napproximation algorithm, namely, Graph-structured Matching Pursuit (Graph-Mp),\nto optimize a general nonlinear function subject to graph-structured\nconstraints. We prove that our algorithm enjoys the strong guarantees analogous\nto those designed for linear models in terms of convergence rate and\napproximation accuracy. As a case study, we specialize Graph-Mp to optimize a\nnumber of well-known graph scan statistic models for the connected subgraph\ndetection task, and empirical evidence demonstrates that our general algorithm\nperforms superior over state-of-the-art methods that are designed specifically\nfor the task of connected subgraph detection.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 01:57:50 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Chen", "Feng", ""], ["Zhou", "Baojian", ""]]}, {"id": "1612.03365", "submitter": "Marc-Andr\\'e Carbonneau", "authors": "Marc-Andr\\'e Carbonneau, Veronika Cheplygina, Eric Granger and\n  Ghyslain Gagnon", "title": "Multiple Instance Learning: A Survey of Problem Characteristics and\n  Applications", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2017.10.009", "report-no": null, "categories": "cs.CV cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) is a form of weakly supervised learning\nwhere training instances are arranged in sets, called bags, and a label is\nprovided for the entire bag. This formulation is gaining interest because it\nnaturally fits various problems and allows to leverage weakly labeled data.\nConsequently, it has been used in diverse application fields such as computer\nvision and document classification. However, learning from bags raises\nimportant challenges that are unique to MIL. This paper provides a\ncomprehensive survey of the characteristics which define and differentiate the\ntypes of MIL problems. Until now, these problem characteristics have not been\nformally identified and described. As a result, the variations in performance\nof MIL algorithms from one data set to another are difficult to explain. In\nthis paper, MIL problem characteristics are grouped into four broad categories:\nthe composition of the bags, the types of data distribution, the ambiguity of\ninstance labels, and the task to be performed. Methods specialized to address\neach category are reviewed. Then, the extent to which these characteristics\nmanifest themselves in key MIL application areas are described. Finally,\nexperiments are conducted to compare the performance of 16 state-of-the-art MIL\nmethods on selected problem characteristics. This paper provides insight on how\nthe problem characteristics affect MIL algorithms, recommendations for future\nbenchmarking and promising avenues for research.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 02:19:22 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Carbonneau", "Marc-Andr\u00e9", ""], ["Cheplygina", "Veronika", ""], ["Granger", "Eric", ""], ["Gagnon", "Ghyslain", ""]]}, {"id": "1612.03433", "submitter": "Michael Crosscombe", "authors": "Michael Crosscombe and Jonathan Lawry", "title": "A Model of Multi-Agent Consensus for Vague and Uncertain Beliefs", "comments": null, "journal-ref": "Adaptive Behavior. 24 (4). pp 249-260. 2016", "doi": "10.1177/1059712316661395", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consensus formation is investigated for multi-agent systems in which agents'\nbeliefs are both vague and uncertain. Vagueness is represented by a third truth\nstate meaning \\emph{borderline}. This is combined with a probabilistic model of\nuncertainty. A belief combination operator is then proposed which exploits\nborderline truth values to enable agents with conflicting beliefs to reach a\ncompromise. A number of simulation experiments are carried out in which agents\napply this operator in pairwise interactions, under the bounded confidence\nrestriction that the two agents' beliefs must be sufficiently consistent with\neach other before agreement can be reached. As well as studying the consensus\noperator in isolation we also investigate scenarios in which agents are\ninfluenced either directly or indirectly by the state of the world. For the\nformer we conduct simulations which combine consensus formation with belief\nupdating based on evidence. For the latter we investigate the effect of\nassuming that the closer an agent's beliefs are to the truth the more visible\nthey are in the consensus building process. In all cases applying the consensus\noperators results in the population converging to a single shared belief which\nis both crisp and certain. Furthermore, simulations which combine consensus\nformation with evidential updating converge faster to a shared opinion which is\ncloser to the actual state of the world than those in which beliefs are only\nchanged as a result of directly receiving new evidence. Finally, if agent\ninteractions are guided by belief quality measured as similarity to the true\nstate of the world, then applying the consensus operator alone results in the\npopulation converging to a high quality shared belief.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 16:22:27 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 16:08:05 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Crosscombe", "Michael", ""], ["Lawry", "Jonathan", ""]]}, {"id": "1612.03471", "submitter": "Xiao Li", "authors": "Xiao Li, Cristian-Ioan Vasile and Calin Belta", "title": "Reinforcement Learning With Temporal Logic Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) depends critically on the choice of reward\nfunctions used to capture the de- sired behavior and constraints of a robot.\nUsually, these are handcrafted by a expert designer and represent heuristics\nfor relatively simple tasks. Real world applications typically involve more\ncomplex tasks with rich temporal and logical structure. In this paper we take\nadvantage of the expressive power of temporal logic (TL) to specify complex\nrules the robot should follow, and incorporate domain knowledge into learning.\nWe propose Truncated Linear Temporal Logic (TLTL) as specifications language,\nthat is arguably well suited for the robotics applications, together with\nquantitative semantics, i.e., robustness degree. We propose a RL approach to\nlearn tasks expressed as TLTL formulae that uses their associated robustness\ndegree as reward functions, instead of the manually crafted heuristics trying\nto capture the same specifications. We show in simulated trials that learning\nis faster and policies obtained using the proposed approach outperform the ones\nlearned using heuristic rewards in terms of the robustness degree, i.e., how\nwell the tasks are satisfied. Furthermore, we demonstrate the proposed RL\napproach in a toast-placing task learned by a Baxter robot.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 20:43:30 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 15:39:11 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Li", "Xiao", ""], ["Vasile", "Cristian-Ioan", ""], ["Belta", "Calin", ""]]}, {"id": "1612.03494", "submitter": "Vasileios Lampos", "authors": "Vasileios Lampos", "title": "Flu Detector: Estimating influenza-like illness rates from online\n  user-generated content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a brief technical description of an online platform for disease\nmonitoring, titled as the Flu Detector (fludetector.cs.ucl.ac.uk). Flu\nDetector, in its current version (v.0.5), uses either Twitter or Google search\ndata in conjunction with statistical Natural Language Processing models to\nestimate the rate of influenza-like illness in the population of England. Its\nback-end is a live service that collects online data, utilises modern\ntechnologies for large-scale text processing, and finally applies statistical\ninference models that are trained offline. The front-end visualises the various\ndisease rate estimates. Notably, the models based on Google data achieve a high\nlevel of accuracy with respect to the most recent four flu seasons in England\n(2012/13 to 2015/16). This highlighted Flu Detector as having a great potential\nof becoming a complementary source to the domestic traditional flu surveillance\nschemes.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 22:27:37 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Lampos", "Vasileios", ""]]}, {"id": "1612.03551", "submitter": "Xun Wang", "authors": "Xun Wang, Katsuhito Sudoh, Masaaki Nagata, Tomohide Shibata, Daisuke\n  Kawahara and Sadao Kurohashi", "title": "Reading Comprehension using Entity-based Memory Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel neural network model for question answering,\nthe \\emph{entity-based memory network}. It enhances neural networks' ability of\nrepresenting and calculating information over a long period by keeping records\nof entities contained in text. The core component is a memory pool which\ncomprises entities' states. These entities' states are continuously updated\naccording to the input text. Questions with regard to the input text are used\nto search the memory pool for related entities and answers are further\npredicted based on the states of retrieved entities. Compared with previous\nmemory network models, the proposed model is capable of handling fine-grained\ninformation and more sophisticated relations based on entities. We formulated\nseveral different tasks as question answering problems and tested the proposed\nmodel. Experiments reported satisfying results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 06:19:32 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2017 06:09:20 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 09:13:25 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Wang", "Xun", ""], ["Sudoh", "Katsuhito", ""], ["Nagata", "Masaaki", ""], ["Shibata", "Tomohide", ""], ["Kawahara", "Daisuke", ""], ["Kurohashi", "Sadao", ""]]}, {"id": "1612.03653", "submitter": "Sahand Sharifzadeh", "authors": "Sahand Sharifzadeh, Ioannis Chiotellis, Rudolph Triebel, Daniel\n  Cremers", "title": "Learning to Drive using Inverse Reinforcement Learning and Deep\n  Q-Networks", "comments": "NIPS workshop on Deep Learning for Action and Interaction, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an inverse reinforcement learning (IRL) approach using Deep\nQ-Networks to extract the rewards in problems with large state spaces. We\nevaluate the performance of this approach in a simulation-based autonomous\ndriving scenario. Our results resemble the intuitive relation between the\nreward function and readings of distance sensors mounted at different poses on\nthe car. We also show that, after a few learning rounds, our simulated agent\ngenerates collision-free motions and performs human-like lane change behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 12:57:10 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 14:44:37 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Sharifzadeh", "Sahand", ""], ["Chiotellis", "Ioannis", ""], ["Triebel", "Rudolph", ""], ["Cremers", "Daniel", ""]]}, {"id": "1612.03705", "submitter": "Francisco Aparecido Rodrigues", "authors": "Oscar A. C. Linares, Glenda Michele Botelho, Francisco Aparecido\n  Rodrigues, Jo\\~ao Batista Neto", "title": "Segmentation of large images based on super-pixels and community\n  detection in graphs", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation has many applications which range from machine learning to\nmedical diagnosis. In this paper, we propose a framework for the segmentation\nof images based on super-pixels and algorithms for community identification in\ngraphs. The super-pixel pre-segmentation step reduces the number of nodes in\nthe graph, rendering the method the ability to process large images. Moreover,\ncommunity detection algorithms provide more accurate segmentation than\ntraditional approaches, such as those based on spectral graph partition. We\nalso compare our method with two algorithms: a) the graph-based approach by\nFelzenszwalb and Huttenlocher and b) the contour-based method by Arbelaez.\nResults have shown that our method provides more precise segmentation and is\nfaster than both of them.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 14:31:29 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Linares", "Oscar A. C.", ""], ["Botelho", "Glenda Michele", ""], ["Rodrigues", "Francisco Aparecido", ""], ["Neto", "Jo\u00e3o Batista", ""]]}, {"id": "1612.03769", "submitter": "Yushi Yao", "authors": "Yushi Yao, Guangjian Li", "title": "Context-aware Sentiment Word Identification: sentiword2vec", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional sentiment analysis often uses sentiment dictionary to extract\nsentiment information in text and classify documents. However, emerging\ninformal words and phrases in user generated content call for analysis aware to\nthe context. Usually, they have special meanings in a particular context.\nBecause of its great performance in representing inter-word relation, we use\nsentiment word vectors to identify the special words. Based on the distributed\nlanguage model word2vec, in this paper we represent a novel method about\nsentiment representation of word under particular context, to be detailed, to\nidentify the words with abnormal sentiment polarity in long answers. Result\nshows the improved model shows better performance in representing the words\nwith special meaning, while keep doing well in representing special idiomatic\npattern. Finally, we will discuss the meaning of vectors representing in the\nfield of sentiment, which may be different from general object-based\nconditions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 16:25:08 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Yao", "Yushi", ""], ["Li", "Guangjian", ""]]}, {"id": "1612.03780", "submitter": "Ludovic Hofer", "authors": "Ludovic Hofer, Hugo Gimbert", "title": "Online Reinforcement Learning for Real-Time Exploration in Continuous\n  State and Action Markov Decision Processes", "comments": null, "journal-ref": "ICAPS 26th, PlanRob 4th (Workshop) (2016) 37-48", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method to learn online policies in continuous\nstate, continuous action, model-free Markov decision processes, with two\nproperties that are crucial for practical applications. First, the policies are\nimplementable with a very low computational cost: once the policy is computed,\nthe action corresponding to a given state is obtained in logarithmic time with\nrespect to the number of samples used. Second, our method is versatile: it does\nnot rely on any a priori knowledge of the structure of optimal policies. We\nbuild upon the Fitted Q-iteration algorithm which represents the $Q$-value as\nthe average of several regression trees. Our algorithm, the Fitted Policy\nForest algorithm (FPF), computes a regression forest representing the Q-value\nand transforms it into a single tree representing the policy, while keeping\ncontrol on the size of the policy using resampling and leaf merging. We\nintroduce an adaptation of Multi-Resolution Exploration (MRE) which is\nparticularly suited to FPF. We assess the performance of FPF on three classical\nbenchmarks for reinforcement learning: the \"Inverted Pendulum\", the \"Double\nIntegrator\" and \"Car on the Hill\" and show that FPF equals or outperforms other\nalgorithms, although these algorithms rely on the use of particular\nrepresentations of the policies, especially chosen in order to fit each of the\nthree problems. Finally, we exhibit that the combination of FPF and MRE allows\nto find nearly optimal solutions in problems where $\\epsilon$-greedy approaches\nwould fail.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 16:52:01 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Hofer", "Ludovic", ""], ["Gimbert", "Hugo", ""]]}, {"id": "1612.03789", "submitter": "Mason Bretan", "authors": "Mason Bretan, Gil Weinberg, and Larry Heck", "title": "A Unit Selection Methodology for Music Generation Using Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods exist for a computer to generate music based on data\nincluding Markov chains, recurrent neural networks, recombinancy, and grammars.\nWe explore the use of unit selection and concatenation as a means of generating\nmusic using a procedure based on ranking, where, we consider a unit to be a\nvariable length number of measures of music. We first examine whether a unit\nselection method, that is restricted to a finite size unit library, can be\nsufficient for encompassing a wide spectrum of music. We do this by developing\na deep autoencoder that encodes a musical input and reconstructs the input by\nselecting from the library. We then describe a generative model that combines a\ndeep structured semantic model (DSSM) with an LSTM to predict the next unit,\nwhere units consist of four, two, and one measures of music. We evaluate the\ngenerative model using objective metrics including mean rank and accuracy and\nwith a subjective listening test in which expert musicians are asked to\ncomplete a forced-choiced ranking task. We compare our model to a note-level\ngenerative baseline that consists of a stacked LSTM trained to predict forward\nby one note.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 17:06:19 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Bretan", "Mason", ""], ["Weinberg", "Gil", ""], ["Heck", "Larry", ""]]}, {"id": "1612.03801", "submitter": "Stig Petersen", "authors": "Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus\n  Wainwright, Heinrich K\\\"uttler, Andrew Lefrancq, Simon Green, V\\'ictor\n  Vald\\'es, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max\n  Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis,\n  Shane Legg and Stig Petersen", "title": "DeepMind Lab", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeepMind Lab is a first-person 3D game platform designed for research and\ndevelopment of general artificial intelligence and machine learning systems.\nDeepMind Lab can be used to study how autonomous artificial agents may learn\ncomplex tasks in large, partially observed, and visually diverse worlds.\nDeepMind Lab has a simple and flexible API enabling creative task-designs and\nnovel AI-designs to be explored and quickly iterated upon. It is powered by a\nfast and widely recognised game engine, and tailored for effective use by the\nresearch community.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 17:32:49 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 12:19:48 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Beattie", "Charles", ""], ["Leibo", "Joel Z.", ""], ["Teplyashin", "Denis", ""], ["Ward", "Tom", ""], ["Wainwright", "Marcus", ""], ["K\u00fcttler", "Heinrich", ""], ["Lefrancq", "Andrew", ""], ["Green", "Simon", ""], ["Vald\u00e9s", "V\u00edctor", ""], ["Sadik", "Amir", ""], ["Schrittwieser", "Julian", ""], ["Anderson", "Keith", ""], ["York", "Sarah", ""], ["Cant", "Max", ""], ["Cain", "Adam", ""], ["Bolton", "Adrian", ""], ["Gaffney", "Stephen", ""], ["King", "Helen", ""], ["Hassabis", "Demis", ""], ["Legg", "Shane", ""], ["Petersen", "Stig", ""]]}, {"id": "1612.03871", "submitter": "Ashish Sabharwal", "authors": "Hanie Sedghi and Ashish Sabharwal", "title": "Knowledge Completion for Generics using Guided Tensor Factorization", "comments": "To appear in TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a knowledge base or KB containing (noisy) facts about common nouns or\ngenerics, such as \"all trees produce oxygen\" or \"some animals live in forests\",\nwe consider the problem of inferring additional such facts at a precision\nsimilar to that of the starting KB. Such KBs capture general knowledge about\nthe world, and are crucial for various applications such as question answering.\nDifferent from commonly studied named entity KBs such as Freebase, generics KBs\ninvolve quantification, have more complex underlying regularities, tend to be\nmore incomplete, and violate the commonly used locally closed world assumption\n(LCWA). We show that existing KB completion methods struggle with this new\ntask, and present the first approach that is successful. Our results\ndemonstrate that external information, such as relation schemas and entity\ntaxonomies, if used appropriately, can be a surprisingly powerful tool in this\nsetting. First, our simple yet effective knowledge guided tensor factorization\napproach achieves state-of-the-art results on two generics KBs (80% precise)\nfor science, doubling their size at 74%-86% precision. Second, our novel\ntaxonomy guided, submodular, active learning method for collecting annotations\nabout rare entities (e.g., oriole, a bird) is 6x more effective at inferring\nfurther new facts about them than multiple active learning baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 19:53:04 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 09:08:44 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 18:58:58 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Sedghi", "Hanie", ""], ["Sabharwal", "Ashish", ""]]}, {"id": "1612.03929", "submitter": "Nabiha Asghar", "authors": "Nabiha Asghar, Pascal Poupart, Xin Jiang, Hang Li", "title": "Deep Active Learning for Dialogue Generation", "comments": "Accepted at 6th Joint Conference on Lexical and Computational\n  Semantics (*SEM) 2017 (Previously titled \"Online Sequence-to-Sequence Active\n  Learning for Open-Domain Dialogue Generation\" on ArXiv)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an online, end-to-end, neural generative conversational model for\nopen-domain dialogue. It is trained using a unique combination of offline\ntwo-phase supervised learning and online human-in-the-loop active learning.\nWhile most existing research proposes offline supervision or hand-crafted\nreward functions for online reinforcement, we devise a novel interactive\nlearning mechanism based on hamming-diverse beam search for response generation\nand one-character user-feedback at each step. Experiments show that our model\ninherently promotes the generation of semantically relevant and interesting\nresponses, and can be used to train agents with customized personas, moods and\nconversational styles.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 21:19:51 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 01:58:21 GMT"}, {"version": "v3", "created": "Fri, 16 Dec 2016 20:44:43 GMT"}, {"version": "v4", "created": "Wed, 1 Feb 2017 18:19:50 GMT"}, {"version": "v5", "created": "Fri, 16 Jun 2017 14:08:31 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Asghar", "Nabiha", ""], ["Poupart", "Pascal", ""], ["Jiang", "Xin", ""], ["Li", "Hang", ""]]}, {"id": "1612.03981", "submitter": "Brett Israelsen", "authors": "Brett Israelsen and Nisar Ahmed", "title": "Hybrid Repeat/Multi-point Sampling for Highly Volatile Objective\n  Functions", "comments": null, "journal-ref": "BayesOpt Workshop, NIPS 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key drawback of the current generation of artificial decision-makers is\nthat they do not adapt well to changes in unexpected situations. This paper\naddresses the situation in which an AI for aerial dog fighting, with tunable\nparameters that govern its behavior, will optimize behavior with respect to an\nobjective function that must be evaluated and learned through simulations. Once\nthis objective function has been modeled, the agent can then choose its desired\nbehavior in different situations. Bayesian optimization with a Gaussian Process\nsurrogate is used as the method for investigating the objective function. One\nkey benefit is that during optimization the Gaussian Process learns a global\nestimate of the true objective function, with predicted outcomes and a\nstatistical measure of confidence in areas that haven't been investigated yet.\nHowever, standard Bayesian optimization does not perform consistently or\nprovide an accurate Gaussian Process surrogate function for highly volatile\nobjective functions. We treat these problems by introducing a novel sampling\ntechnique called Hybrid Repeat/Multi-point Sampling. This technique gives the\nAI ability to learn optimum behaviors in a highly uncertain environment. More\nimportantly, it not only improves the reliability of the optimization, but also\ncreates a better model of the entire objective surface. With this improved\nmodel the agent is equipped to better adapt behaviors.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 00:21:45 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Israelsen", "Brett", ""], ["Ahmed", "Nisar", ""]]}, {"id": "1612.04023", "submitter": "EPTCS", "authors": "Mehdi Kargahi (University of Tehran), Ashutosh Trivedi (University of\n  Colorado Boulder)", "title": "Proceedings of the The First Workshop on Verification and Validation of\n  Cyber-Physical Systems", "comments": null, "journal-ref": "EPTCS 232, 2016", "doi": "10.4204/EPTCS.232", "report-no": null, "categories": "cs.SY cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first International Workshop on Verification and Validation of\nCyber-Physical Systems (V2CPS-16) was held in conjunction with the 12th\nInternational Conference on integration of Formal Methods (iFM 2016) in\nReykjavik, Iceland. The purpose of V2CPS-16 was to bring together researchers\nand experts of the fields of formal verification and cyber-physical systems\n(CPS) to cover the theme of this workshop, namely a wide spectrum of\nverification and validation methods including (but not limited to) control,\nsimulation, formal methods, etc.\n  A CPS is an integration of networked computational and physical processes\nwith meaningful inter-effects; the former monitors, controls, and affects the\nlatter, while the latter also impacts the former. CPSs have applications in a\nwide-range of systems spanning robotics, transportation, communication,\ninfrastructure, energy, and manufacturing. Many safety-critical systems such as\nchemical processes, medical devices, aircraft flight control, and automotive\nsystems, are indeed CPS. The advanced capabilities of CPS require complex\nsoftware and synthesis algorithms, which are hard to verify. In fact, many\nproblems in this area are undecidable. Thus, a major step is to find particular\nabstractions of such systems which might be algorithmically verifiable\nregarding specific properties of such systems, describing the partial/overall\nbehaviors of CPSs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 04:27:32 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Kargahi", "Mehdi", "", "University of Tehran"], ["Trivedi", "Ashutosh", "", "University of\n  Colorado Boulder"]]}, {"id": "1612.04286", "submitter": "Peter Christen", "authors": "Peter Christen", "title": "Application of Advanced Record Linkage Techniques for Complex Population\n  Reconstruction", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is the process of identifying records that refer to the same\nentities from several databases. This process is challenging because commonly\nno unique entity identifiers are available. Linkage therefore has to rely on\npartially identifying attributes, such as names and addresses of people. Recent\nyears have seen the development of novel techniques for linking data from\ndiverse application areas, where a major focus has been on linking complex data\nthat contain records about different types of entities. Advanced approaches\nthat exploit both the similarities between record attributes as well as the\nrelationships between entities to identify clusters of matching records have\nbeen developed.\n  In this application paper we study the novel problem where rather than\ndifferent types of entities we have databases where the same entity can have\ndifferent roles, and where these roles change over time. We specifically\ndevelop novel techniques for linking historical birth, death, marriage and\ncensus records with the aim to reconstruct the population covered by these\nrecords over a period of several decades. Our experimental evaluation on real\nScottish data shows that even with advanced linkage techniques that consider\ngroup, relationship, and temporal aspects it is challenging to achieve high\nquality linkage from such complex data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 17:10:11 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Christen", "Peter", ""]]}, {"id": "1612.04299", "submitter": "Filippo Bistaffa", "authors": "Filippo Bistaffa, Alessandro Farinelli, Jes\\'us Cerquides, Juan A.\n  Rodr\\'iguez-Aguilar, Sarvapali D. Ramchurn", "title": "Algorithms for Graph-Constrained Coalition Formation in the Real World", "comments": "Accepted for publication, cite as \"in press\"", "journal-ref": "ACM Transactions on Intelligent Systems and Technology, 2017,\n  Volume 8, Issue 4", "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coalition formation typically involves the coming together of multiple,\nheterogeneous, agents to achieve both their individual and collective goals. In\nthis paper, we focus on a special case of coalition formation known as\nGraph-Constrained Coalition Formation (GCCF) whereby a network connecting the\nagents constrains the formation of coalitions. We focus on this type of problem\ngiven that in many real-world applications, agents may be connected by a\ncommunication network or only trust certain peers in their social network. We\npropose a novel representation of this problem based on the concept of edge\ncontraction, which allows us to model the search space induced by the GCCF\nproblem as a rooted tree. Then, we propose an anytime solution algorithm\n(CFSS), which is particularly efficient when applied to a general class of\ncharacteristic functions called $m+a$ functions. Moreover, we show how CFSS can\nbe efficiently parallelised to solve GCCF using a non-redundant partition of\nthe search space. We benchmark CFSS on both synthetic and realistic scenarios,\nusing a real-world dataset consisting of the energy consumption of a large\nnumber of households in the UK. Our results show that, in the best case, the\nserial version of CFSS is 4 orders of magnitude faster than the state of the\nart, while the parallel version is 9.44 times faster than the serial version on\na 12-core machine. Moreover, CFSS is the first approach to provide anytime\napproximate solutions with quality guarantees for very large systems of agents\n(i.e., with more than 2700 agents).\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 17:51:59 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Bistaffa", "Filippo", ""], ["Farinelli", "Alessandro", ""], ["Cerquides", "Jes\u00fas", ""], ["Rodr\u00edguez-Aguilar", "Juan A.", ""], ["Ramchurn", "Sarvapali D.", ""]]}, {"id": "1612.04315", "submitter": "Brett Israelsen", "authors": "Brett W. Israelsen, Nisar Ahmed, Kenneth Center, Roderick Green,\n  Winston Bennett Jr", "title": "Towards Adaptive Training of Agent-based Sparring Partners for Fighter\n  Pilots", "comments": "submitted copy", "journal-ref": "SciTech 2017, paper 2545524", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement for the current generation of artificial decision-makers is\nthat they should adapt well to changes in unexpected situations. This paper\naddresses the situation in which an AI for aerial dog fighting, with tunable\nparameters that govern its behavior, must optimize behavior with respect to an\nobjective function that is evaluated and learned through simulations. Bayesian\noptimization with a Gaussian Process surrogate is used as the method for\ninvestigating the objective function. One key benefit is that during\noptimization, the Gaussian Process learns a global estimate of the true\nobjective function, with predicted outcomes and a statistical measure of\nconfidence in areas that haven't been investigated yet. Having a model of the\nobjective function is important for being able to understand possible outcomes\nin the decision space; for example this is crucial for training and providing\nfeedback to human pilots. However, standard Bayesian optimization does not\nperform consistently or provide an accurate Gaussian Process surrogate function\nfor highly volatile objective functions. We treat these problems by introducing\na novel sampling technique called Hybrid Repeat/Multi-point Sampling. This\ntechnique gives the AI ability to learn optimum behaviors in a highly uncertain\nenvironment. More importantly, it not only improves the reliability of the\noptimization, but also creates a better model of the entire objective surface.\nWith this improved model the agent is equipped to more accurately/efficiently\npredict performance in unexplored scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 18:43:59 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Israelsen", "Brett W.", ""], ["Ahmed", "Nisar", ""], ["Center", "Kenneth", ""], ["Green", "Roderick", ""], ["Bennett", "Winston", "Jr"]]}, {"id": "1612.04318", "submitter": "Markus Wulfmeier", "authors": "Markus Wulfmeier, Dushyant Rao, Ingmar Posner", "title": "Incorporating Human Domain Knowledge into Large Scale Cost Function\n  Learning", "comments": "Neural Information Processing Systems 2016, Deep Reinforcement\n  Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances have shown the capability of Fully Convolutional Neural\nNetworks (FCN) to model cost functions for motion planning in the context of\nlearning driving preferences purely based on demonstration data from human\ndrivers. While pure learning from demonstrations in the framework of Inverse\nReinforcement Learning (IRL) is a promising approach, we can benefit from well\ninformed human priors and incorporate them into the learning process. Our work\nachieves this by pretraining a model to regress to a manual cost function and\nrefining it based on Maximum Entropy Deep Inverse Reinforcement Learning. When\ninjecting prior knowledge as pretraining for the network, we achieve higher\nrobustness, more visually distinct obstacle boundaries, and the ability to\ncapture instances of obstacles that elude models that purely learn from\ndemonstration data. Furthermore, by exploiting these human priors, the\nresulting model can more accurately handle corner cases that are scarcely seen\nin the demonstration data, such as stairs, slopes, and underpasses.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 18:56:03 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Wulfmeier", "Markus", ""], ["Rao", "Dushyant", ""], ["Posner", "Ingmar", ""]]}, {"id": "1612.04432", "submitter": "Christian Stra{\\ss}er", "authors": "Annemarie Borg, Daniel Frey, Dunja \\v{S}e\\v{s}elja, Christian\n  Stra{\\ss}er", "title": "An argumentative agent-based model of scientific inquiry", "comments": "14 page, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an agent-based model (ABM) of scientific inquiry\naimed at investigating how different social networks impact the efficiency of\nscientists in acquiring knowledge. As such, the ABM is a computational tool for\ntackling issues in the domain of scientific methodology and science policy. In\ncontrast to existing ABMs of science, our model aims to represent the\nargumentative dynamics that underlies scientific practice. To this end we\nemploy abstract argumentation theory as the core design feature of the model.\nThis helps to avoid a number of problematic idealizations which are present in\nother ABMs of science and which impede their relevance for actual scientific\npractice.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 23:56:58 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Borg", "Annemarie", ""], ["Frey", "Daniel", ""], ["\u0160e\u0161elja", "Dunja", ""], ["Stra\u00dfer", "Christian", ""]]}, {"id": "1612.04468", "submitter": "Jason J Corso", "authors": "Parker Koch and Jason J. Corso", "title": "Sparse Factorization Layers for Neural Networks with Limited Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas CNNs have demonstrated immense progress in many vision problems, they\nsuffer from a dependence on monumental amounts of labeled training data. On the\nother hand, dictionary learning does not scale to the size of problems that\nCNNs can handle, despite being very effective at low-level vision tasks such as\ndenoising and inpainting. Recently, interest has grown in adapting dictionary\nlearning methods for supervised tasks such as classification and inverse\nproblems. We propose two new network layers that are based on dictionary\nlearning: a sparse factorization layer and a convolutional sparse factorization\nlayer, analogous to fully-connected and convolutional layers, respectively.\nUsing our derivations, these layers can be dropped in to existing CNNs, trained\ntogether in an end-to-end fashion with back-propagation, and leverage\nsemisupervision in ways classical CNNs cannot. We experimentally compare\nnetworks with these two new layers against a baseline CNN. Our results\ndemonstrate that networks with either of the sparse factorization layers are\nable to outperform classical CNNs when supervised data are few. They also show\nperformance improvements in certain tasks when compared to the CNN with no\nsparse factorization layers with the same exact number of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 03:13:29 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Koch", "Parker", ""], ["Corso", "Jason J.", ""]]}, {"id": "1612.04469", "submitter": "Kenrick Kenrick", "authors": "Kenrick", "title": "Web-based Argumentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Assumption-Based Argumentation (ABA) is an argumentation framework that has\nbeen proposed in the late 20th century. Since then, there was still no solver\nimplemented in a programming language which is easy to setup and no solver have\nbeen interfaced to the web, which impedes the interests of the public. This\nproject aims to implement an ABA solver in a modern programming language that\nperforms reasonably well and interface it to the web for easier access by the\npublic. This project has demonstrated the novelty of development of an ABA\nsolver, that computes conflict-free, stable, admissible, grounded, ideal, and\ncomplete semantics, in Python programming language which can be used via an\neasy-to-use web interface for visualization of the argument and dispute trees.\nExperiments were conducted to determine the project's best configurations and\nto compare this project with proxdd, a state-of-the-art ABA solver, which has\nno web interface and computes less number of semantics. From the results of the\nexperiments, this project's best configuration is achieved by utilizing\n\"pickle\" technique and tree caching technique. Using this project's best\nconfiguration, this project achieved a lower average runtime compared to\nproxdd. On other aspect, this project encountered more cases with exceptions\ncompared to proxdd, which might be caused by this project computing more\nsemantics and hence requires more resources to do so. Hence, it can be said\nthat this project run comparably well to the state-of-the-art ABA solver\nproxdd. Future works of this project include computational complexity analysis\nand efficiency analysis of algorithms implemented, implementation of more\nsemantics in argumentation framework, and usability testing of the web\ninterface.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 03:21:32 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Kenrick", "", ""]]}, {"id": "1612.04687", "submitter": "Memo Akten", "authors": "Memo Akten and Mick Grierson", "title": "Real-time interactive sequence generation and control with Recurrent\n  Neural Network ensembles", "comments": "Demo presentation at NIPS 2016, and poster presentation at the RNN\n  Symposium at NIPS 2016. 7 pages including 1 page references, 1 page appendix,\n  2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNN), particularly Long Short Term Memory (LSTM)\nRNNs, are a popular and very successful method for learning and generating\nsequences. However, current generative RNN techniques do not allow real-time\ninteractive control of the sequence generation process, thus aren't well suited\nfor live creative expression. We propose a method of real-time continuous\ncontrol and 'steering' of sequence generation using an ensemble of RNNs and\ndynamically altering the mixture weights of the models. We demonstrate the\nmethod using character based LSTM networks and a gestural interface allowing\nusers to 'conduct' the generation of text.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 15:22:57 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 21:25:53 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Akten", "Memo", ""], ["Grierson", "Mick", ""]]}, {"id": "1612.04742", "submitter": "Stefan Lattner", "authors": "Stefan Lattner, Maarten Grachten, Gerhard Widmer", "title": "Imposing higher-level Structure in Polyphonic Music Generation using\n  Convolutional Restricted Boltzmann Machines and Constraints", "comments": "31 pages, 11 figures", "journal-ref": "Journal of Creative Music Systems, Volume 2, Issue 1, March 2018", "doi": "10.5920/jcms.2018.01", "report-no": null, "categories": "cs.SD cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for imposing higher-level structure on generated,\npolyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a\ngenerative model is combined with gradient descent constraint optimisation to\nprovide further control over the generation process. Among other things, this\nallows for the use of a \"template\" piece, from which some structural properties\ncan be extracted, and transferred as constraints to the newly generated\nmaterial. The sampling process is guided with Simulated Annealing to avoid\nlocal optima, and to find solutions that both satisfy the constraints, and are\nrelatively stable with respect to the C-RBM. Results show that with this\napproach it is possible to control the higher-level self-similarity structure,\nthe meter, and the tonal properties of the resulting musical piece, while\npreserving its local musical coherence.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 17:33:38 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 01:48:52 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 15:21:05 GMT"}, {"version": "v4", "created": "Sat, 14 Apr 2018 12:43:15 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Lattner", "Stefan", ""], ["Grachten", "Maarten", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1612.04757", "submitter": "Marcus Rohrbach", "authors": "Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Bernt Schiele,\n  Trevor Darrell, Marcus Rohrbach", "title": "Attentive Explanations: Justifying Decisions and Pointing to the\n  Evidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep models are the defacto standard in visual decision models due to their\nimpressive performance on a wide array of visual tasks. However, they are\nfrequently seen as opaque and are unable to explain their decisions. In\ncontrast, humans can justify their decisions with natural language and point to\nthe evidence in the visual world which led to their decisions. We postulate\nthat deep models can do this as well and propose our Pointing and Justification\n(PJ-X) model which can justify its decision with a sentence and point to the\nevidence by introspecting its decision and explanation process using an\nattention mechanism. Unfortunately there is no dataset available with reference\nexplanations for visual decision making. We thus collect two datasets in two\ndomains where it is interesting and challenging to explain decisions. First, we\nextend the visual question answering task to not only provide an answer but\nalso a natural language explanation for the answer. Second, we focus on\nexplaining human activities which is traditionally more challenging than object\nclassification. We extensively evaluate our PJ-X model, both on the\njustification and pointing tasks, by comparing it to prior models and ablations\nusing both automatic and human evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 18:12:47 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 09:33:03 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Park", "Dong Huk", ""], ["Hendricks", "Lisa Anne", ""], ["Akata", "Zeynep", ""], ["Schiele", "Bernt", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1612.04759", "submitter": "Marco Cusumano-Towner", "authors": "Marco F. Cusumano-Towner, Vikash K. Mansinghka", "title": "Encapsulating models and approximate inference programs in probabilistic\n  modules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the probabilistic module interface, which allows\nencapsulation of complex probabilistic models with latent variables alongside\ncustom stochastic approximate inference machinery, and provides a\nplatform-agnostic abstraction barrier separating the model internals from the\nhost probabilistic inference system. The interface can be seen as a stochastic\ngeneralization of a standard simulation and density interface for probabilistic\nprimitives. We show that sound approximate inference algorithms can be\nconstructed for networks of probabilistic modules, and we demonstrate that the\ninterface can be implemented using learned stochastic inference networks and\nMCMC and SMC approximate inference programs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 18:14:59 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 21:13:42 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Cusumano-Towner", "Marco F.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1612.04791", "submitter": "Patrick Rodler", "authors": "Patrick Rodler and Wolfgang Schmid and Kostyantyn Shchekotykhin", "title": "Scalable Computation of Optimized Queries for Sequential Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many model-based diagnosis applications it is impossible to provide such a\nset of observations and/or measurements that allow to identify the real cause\nof a fault. Therefore, diagnosis systems often return many possible candidates,\nleaving the burden of selecting the correct diagnosis to a user. Sequential\ndiagnosis techniques solve this problem by automatically generating a sequence\nof queries to some oracle. The answers to these queries provide additional\ninformation necessary to gradually restrict the search space by removing\ndiagnosis candidates inconsistent with the answers.\n  During query computation, existing sequential diagnosis methods often require\nthe generation of many unnecessary query candidates and strongly rely on\nexpensive logical reasoners. We tackle this issue by devising efficient\nheuristic query search methods. The proposed methods enable for the first time\na completely reasoner-free query generation while at the same time guaranteeing\noptimality conditions, e.g. minimal cardinality or best understandability, of\nthe returned query that existing methods cannot realize. Hence, the performance\nof this approach is independent of the (complexity of the) diagnosed system.\nExperiments conducted using real-world problems show that the new approach is\nhighly scalable and outperforms existing methods by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 20:15:36 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 18:24:55 GMT"}, {"version": "v3", "created": "Fri, 16 Dec 2016 17:26:02 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Rodler", "Patrick", ""], ["Schmid", "Wolfgang", ""], ["Shchekotykhin", "Kostyantyn", ""]]}, {"id": "1612.04804", "submitter": "Asaf Shabtai", "authors": "Asaf Shabtai", "title": "Anomaly Detection Using the Knowledge-based Temporal Abstraction Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth in stored time-oriented data necessitates the development of\nnew methods for handling, processing, and interpreting large amounts of\ntemporal data. One important example of such processing is detecting anomalies\nin time-oriented data. The Knowledge-Based Temporal Abstraction method was\npreviously proposed for intelligent interpretation of temporal data based on\npredefined domain knowledge. In this study we propose a framework that\nintegrates the KBTA method with a temporal pattern mining process for anomaly\ndetection. According to the proposed method a temporal pattern mining process\nis applied on a dataset of basic temporal abstraction database in order to\nextract patterns representing normal behavior. These patterns are then analyzed\nin order to identify abnormal time periods characterized by a significantly\nsmall number of normal patterns. The proposed approach was demonstrated using a\ndataset collected from a real server.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 20:50:48 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Shabtai", "Asaf", ""]]}, {"id": "1612.04868", "submitter": "I\\~nigo Lopez-Gazpio", "authors": "I. Lopez-Gazpio and M. Maritxalar and A. Gonzalez-Agirre and G. Rigau\n  and L. Uria and E. Agirre", "title": "Interpretable Semantic Textual Similarity: Finding and explaining\n  differences between sentences", "comments": "Preprint version, Knowledge-Based Systems (ISSN: 0950-7051). (2016)", "journal-ref": null, "doi": "10.1016/j.knosys.2016.12.013", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User acceptance of artificial intelligence agents might depend on their\nability to explain their reasoning, which requires adding an interpretability\nlayer that fa- cilitates users to understand their behavior. This paper focuses\non adding an in- terpretable layer on top of Semantic Textual Similarity (STS),\nwhich measures the degree of semantic equivalence between two sentences. The\ninterpretability layer is formalized as the alignment between pairs of segments\nacross the two sentences, where the relation between the segments is labeled\nwith a relation type and a similarity score. We present a publicly available\ndataset of sentence pairs annotated following the formalization. We then\ndevelop a system trained on this dataset which, given a sentence pair, explains\nwhat is similar and different, in the form of graded and typed segment\nalignments. When evaluated on the dataset, the system performs better than an\ninformed baseline, showing that the dataset and task are well-defined and\nfeasible. Most importantly, two user studies show how the system output can be\nused to automatically produce explanations in natural language. Users performed\nbetter when having access to the explanations, pro- viding preliminary evidence\nthat our dataset and method to automatically produce explanations is useful in\nreal applications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 22:22:33 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Lopez-Gazpio", "I.", ""], ["Maritxalar", "M.", ""], ["Gonzalez-Agirre", "A.", ""], ["Rigau", "G.", ""], ["Uria", "L.", ""], ["Agirre", "E.", ""]]}, {"id": "1612.04876", "submitter": "Memo Akten", "authors": "Memo Akten and Mick Grierson", "title": "Collaborative creativity with Monte-Carlo Tree Search and Convolutional\n  Neural Networks", "comments": "Presented at the Constructive Machine Learning workshop at NIPS 2016\n  as a poster and spotlight talk. 8 pages including 2 page references, 2 page\n  appendix, 3 figures. Blog post (including videos) at\n  https://medium.com/@memoakten/collaborative-creativity-with-monte-carlo-tree-search-and-convolutional-neural-networks-and-other-69d7107385a0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a human-machine collaborative drawing environment in which an\nautonomous agent sketches images while optionally allowing a user to directly\ninfluence the agent's trajectory. We combine Monte Carlo Tree Search with image\nclassifiers and test both shallow models (e.g. multinomial logistic regression)\nand deep Convolutional Neural Networks (e.g. LeNet, Inception v3). We found\nthat using the shallow model, the agent produces a limited variety of images,\nwhich are noticably recogonisable by humans. However, using the deeper models,\nthe agent produces a more diverse range of images, and while the agent remains\nvery confident (99.99%) in having achieved its objective, to humans they mostly\nresemble unrecognisable 'random' noise. We relate this to recent research which\nalso discovered that 'deep neural networks are easily fooled' \\cite{Nguyen2015}\nand we discuss possible solutions and future directions for the research.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 23:13:26 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Akten", "Memo", ""], ["Grierson", "Mick", ""]]}, {"id": "1612.04883", "submitter": "Tara Safavi", "authors": "Yike Liu, Tara Safavi, Abhilash Dighe, Danai Koutra", "title": "Graph Summarization Methods and Applications: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While advances in computing resources have made processing enormous amounts\nof data possible, human ability to identify patterns in such data has not\nscaled accordingly. Efficient computational methods for condensing and\nsimplifying data are thus becoming vital for extracting actionable insights. In\nparticular, while data summarization techniques have been studied extensively,\nonly recently has summarizing interconnected data, or graphs, become popular.\nThis survey is a structured, comprehensive overview of the state-of-the-art\nmethods for summarizing graph data. We first broach the motivation behind, and\nthe challenges of, graph summarization. We then categorize summarization\napproaches by the type of graphs taken as input and further organize each\ncategory by core methodology. Finally, we discuss applications of summarization\non real-world graphs and conclude by describing some open problems in the\nfield.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 23:39:45 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 03:32:02 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 18:10:26 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Liu", "Yike", ""], ["Safavi", "Tara", ""], ["Dighe", "Abhilash", ""], ["Koutra", "Danai", ""]]}, {"id": "1612.04885", "submitter": "Rupert Freeman", "authors": "Rupert Freeman, Sebastien Lahaie, David M. Pennock", "title": "Crowdsourced Outcome Determination in Prediction Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prediction market is a useful means of aggregating information about a\nfuture event. To function, the market needs a trusted entity who will verify\nthe true outcome in the end. Motivated by the recent introduction of\ndecentralized prediction markets, we introduce a mechanism that allows for the\noutcome to be determined by the votes of a group of arbiters who may themselves\nhold stakes in the market. Despite the potential conflict of interest, we\nderive conditions under which we can incentivize arbiters to vote truthfully by\nusing funds raised from market fees to implement a peer prediction mechanism.\nFinally, we investigate what parameter values could be used in a real-world\nimplementation of our mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 23:47:27 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Freeman", "Rupert", ""], ["Lahaie", "Sebastien", ""], ["Pennock", "David M.", ""]]}, {"id": "1612.04933", "submitter": "Benjamin Jantzen", "authors": "Benjamin C. Jantzen", "title": "Dynamical Kinds and their Discovery", "comments": "Accepted for the proceedings of the Causation: Foundation to\n  Application Workshop, UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the possibility of classifying causal systems into kinds that\nshare a common structure without first constructing an explicit dynamical model\nor using prior knowledge of the system dynamics. The algorithmic ability to\ndetermine whether arbitrary systems are governed by causal relations of the\nsame form offers significant practical applications in the development and\nvalidation of dynamical models. It is also of theoretical interest as an\nessential stage in the scientific inference of laws from empirical data. The\nalgorithm presented is based on the dynamical symmetry approach to dynamical\nkinds. A dynamical symmetry with respect to time is an intervention on one or\nmore variables of a system that commutes with the time evolution of the system.\nA dynamical kind is a class of systems sharing a set of dynamical symmetries.\nThe algorithm presented classifies deterministic, time-dependent causal systems\nby directly comparing their exhibited symmetries. Using simulated, noisy data\nfrom a variety of nonlinear systems, we show that this algorithm correctly\nsorts systems into dynamical kinds. It is robust under significant sampling\nerror, is immune to violations of normality in sampling error, and fails\ngracefully with increasing dynamical similarity. The algorithm we demonstrate\nis the first to address this aspect of automated scientific discovery.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 05:25:41 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Jantzen", "Benjamin C.", ""]]}, {"id": "1612.04936", "submitter": "Jason  Weston", "authors": "Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc'Aurelio Ranzato,\n  Jason Weston", "title": "Learning through Dialogue Interactions by Asking Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good dialogue agent should have the ability to interact with users by both\nresponding to questions and by asking questions, and importantly to learn from\nboth types of interaction. In this work, we explore this direction by designing\na simulator and a set of synthetic tasks in the movie domain that allow such\ninteractions between a learner and a teacher. We investigate how a learner can\nbenefit from asking questions in both offline and online reinforcement learning\nsettings, and demonstrate that the learner improves when asking questions.\nFinally, real experiments with Mechanical Turk validate the approach. Our work\nrepresents a first step in developing such end-to-end learned interactive\ndialogue agents.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 05:46:27 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 19:47:12 GMT"}, {"version": "v3", "created": "Fri, 13 Jan 2017 21:07:04 GMT"}, {"version": "v4", "created": "Mon, 13 Feb 2017 17:30:42 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Li", "Jiwei", ""], ["Miller", "Alexander H.", ""], ["Chopra", "Sumit", ""], ["Ranzato", "Marc'Aurelio", ""], ["Weston", "Jason", ""]]}, {"id": "1612.04988", "submitter": "Maya Ramanath", "authors": "Prajna Upadhyay and Tanuma Patra and Ashwini Purkar and Maya Ramanath", "title": "TeKnowbase: Towards Construction of a Knowledge-base of Technical\n  Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the construction of TeKnowbase, a knowledge-base\nof technical concepts in computer science. Our main information sources are\ntechnical websites such as Webopedia and Techtarget as well as Wikipedia and\nonline textbooks. We divide the knowledge-base construction problem into two\nparts -- the acquisition of entities and the extraction of relationships among\nthese entities. Our knowledge-base consists of approximately 100,000 triples.\nWe conducted an evaluation on a sample of triples and report an accuracy of a\nlittle over 90\\%. We additionally conducted classification experiments on\nStackOverflow data with features from TeKnowbase and achieved improved\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 09:14:35 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Upadhyay", "Prajna", ""], ["Patra", "Tanuma", ""], ["Purkar", "Ashwini", ""], ["Ramanath", "Maya", ""]]}, {"id": "1612.05028", "submitter": "Oliver Kutz", "authors": "Mihai Codescu, Eugen Kuksa, Oliver Kutz, Till Mossakowski, Fabian\n  Neuhaus", "title": "Ontohub: A semantic repository for heterogeneous ontologies", "comments": "Preprint, journal special issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontohub is a repository engine for managing distributed heterogeneous\nontologies. The distributed nature enables communities to share and exchange\ntheir contributions easily. The heterogeneous nature makes it possible to\nintegrate ontologies written in various ontology languages. Ontohub supports a\nwide range of formal logical and ontology languages, as well as various\nstructuring and modularity constructs and inter-theory (concept) mappings,\nbuilding on the OMG-standardized DOL language. Ontohub repositories are\norganised as Git repositories, thus inheriting all features of this popular\nversion control system. Moreover, Ontohub is the first repository engine\nmeeting a substantial amount of the requirements formulated in the context of\nthe Open Ontology Repository (OOR) initiative, including an API for federation\nas well as support for logical inference and axiom selection.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 11:48:13 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Codescu", "Mihai", ""], ["Kuksa", "Eugen", ""], ["Kutz", "Oliver", ""], ["Mossakowski", "Till", ""], ["Neuhaus", "Fabian", ""]]}, {"id": "1612.05048", "submitter": "Theofanis Karaletsos", "authors": "Theofanis Karaletsos", "title": "Adversarial Message Passing For Graphical Models", "comments": "(12 pages, 2 figures) Presented at NIPS Advances In Approximate\n  Inference 2016 (AABI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference on structured models typically relies on the ability to\ninfer posterior distributions of underlying hidden variables. However,\ninference in implicit models or complex posterior distributions is hard. A\npopular tool for learning implicit models are generative adversarial networks\n(GANs) which learn parameters of generators by fooling discriminators.\nTypically, GANs are considered to be models themselves and are not understood\nin the context of inference. Current techniques rely on inefficient global\ndiscrimination of joint distributions to perform learning, or only consider\ndiscriminating a single output variable. We overcome these limitations by\ntreating GANs as a basis for likelihood-free inference in generative models and\ngeneralize them to Bayesian posterior inference over factor graphs. We propose\nlocal learning rules based on message passing minimizing a global divergence\ncriterion involving cooperating local adversaries used to sidestep explicit\nlikelihood evaluations. This allows us to compose models and yields a unified\ninference and learning framework for adversarial learning. Our framework treats\nmodel specification and inference separately and facilitates richly structured\nmodels within the family of Directed Acyclic Graphs, including components such\nas intractable likelihoods, non-differentiable models, simulators and generally\ncumbersome models. A key result of our treatment is the insight that Bayesian\ninference on structured models can be performed only with sampling and\ndiscrimination when using nonparametric variational families, without access to\nexplicit distributions. As a side-result, we discuss the link to likelihood\nmaximization. These approaches hold promise to be useful in the toolbox of\nprobabilistic modelers and enrich the gamut of current probabilistic\nprogramming applications.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 12:59:33 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Karaletsos", "Theofanis", ""]]}, {"id": "1612.05159", "submitter": "Harm van Seijen", "authors": "Harm van Seijen and Mehdi Fatemi and Joshua Romoff and Romain Laroche", "title": "Separation of Concerns in Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a framework for solving a single-agent task by\nusing multiple agents, each focusing on different aspects of the task. This\napproach has two main advantages: 1) it allows for training specialized agents\non different parts of the task, and 2) it provides a new way to transfer\nknowledge, by transferring trained agents. Our framework generalizes the\ntraditional hierarchical decomposition, in which, at any moment in time, a\nsingle agent has control until it has solved its particular subtask. We\nillustrate our framework with empirical experiments on two domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 17:41:41 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 19:43:48 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["van Seijen", "Harm", ""], ["Fatemi", "Mehdi", ""], ["Romoff", "Joshua", ""], ["Laroche", "Romain", ""]]}, {"id": "1612.05251", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt, Ji Young Lee, Peter Szolovits", "title": "Neural Networks for Joint Sentence Classification in Medical Paper\n  Abstracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing models based on artificial neural networks (ANNs) for sentence\nclassification often do not incorporate the context in which sentences appear,\nand classify sentences individually. However, traditional sentence\nclassification approaches have been shown to greatly benefit from jointly\nclassifying subsequent sentences, such as with conditional random fields. In\nthis work, we present an ANN architecture that combines the effectiveness of\ntypical ANN models to classify sentences in isolation, with the strength of\nstructured prediction. Our model achieves state-of-the-art results on two\ndifferent datasets for sequential sentence classification in medical abstracts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 20:57:56 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dernoncourt", "Franck", ""], ["Lee", "Ji Young", ""], ["Szolovits", "Peter", ""]]}, {"id": "1612.05299", "submitter": "Karl Ridgeway", "authors": "Karl Ridgeway", "title": "A Survey of Inductive Biases for Factorial Representation-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the resurgence of interest in neural networks, representation learning\nhas re-emerged as a central focus in artificial intelligence. Representation\nlearning refers to the discovery of useful encodings of data that make\ndomain-relevant information explicit. Factorial representations identify\nunderlying independent causal factors of variation in data. A factorial\nrepresentation is compact and faithful, makes the causal factors explicit, and\nfacilitates human interpretation of data. Factorial representations support a\nvariety of applications, including the generation of novel examples, indexing\nand search, novelty detection, and transfer learning.\n  This article surveys various constraints that encourage a learning algorithm\nto discover factorial representations. I dichotomize the constraints in terms\nof unsupervised and supervised inductive bias. Unsupervised inductive biases\nexploit assumptions about the environment, such as the statistical distribution\nof factor coefficients, assumptions about the perturbations a factor should be\ninvariant to (e.g. a representation of an object can be invariant to rotation,\ntranslation or scaling), and assumptions about how factors are combined to\nsynthesize an observation. Supervised inductive biases are constraints on the\nrepresentations based on additional information connected to observations.\nSupervisory labels come in variety of types, which vary in how strongly they\nconstrain the representation, how many factors are labeled, how many\nobservations are labeled, and whether or not we know the associations between\nthe constraints and the factors they are related to.\n  This survey brings together a wide variety of models that all touch on the\nproblem of learning factorial representations and lays out a framework for\ncomparing these models based on the strengths of the underlying supervised and\nunsupervised inductive biases.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 22:55:41 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Ridgeway", "Karl", ""]]}, {"id": "1612.05309", "submitter": "Hang Ma", "authors": "Hang Ma, T. K. Satish Kumar, Sven Koenig", "title": "Multi-Agent Path Finding with Delay Probabilities", "comments": "To appear in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recently developed Multi-Agent Path Finding (MAPF) solvers scale to\nlarge MAPF instances by searching for MAPF plans on 2 levels: The high-level\nsearch resolves collisions between agents, and the low-level search plans paths\nfor single agents under the constraints imposed by the high-level search. We\nmake the following contributions to solve the MAPF problem with imperfect plan\nexecution with small average makespans: First, we formalize the MAPF Problem\nwith Delay Probabilities (MAPF-DP), define valid MAPF-DP plans and propose the\nuse of robust plan-execution policies for valid MAPF-DP plans to control how\neach agent proceeds along its path. Second, we discuss 2 classes of\ndecentralized robust plan-execution policies (called Fully Synchronized\nPolicies and Minimal Communication Policies) that prevent collisions during\nplan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP\nsolver (called Approximate Minimization in Expectation) that generates valid\nMAPF-DP plans.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 23:33:41 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Ma", "Hang", ""], ["Kumar", "T. K. Satish", ""], ["Koenig", "Sven", ""]]}, {"id": "1612.05348", "submitter": "Ndapandula Nakashole", "authors": "Ndapandula Nakashole, Tom M. Mitchell", "title": "Machine Reading with Background Knowledge", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent systems capable of automatically understanding natural language\ntext are important for many artificial intelligence applications including\nmobile phone voice assistants, computer vision, and robotics. Understanding\nlanguage often constitutes fitting new information into a previously acquired\nview of the world. However, many machine reading systems rely on the text alone\nto infer its meaning. In this paper, we pursue a different approach; machine\nreading methods that make use of background knowledge to facilitate language\nunderstanding. To this end, we have developed two methods: The first method\naddresses prepositional phrase attachment ambiguity. It uses background\nknowledge within a semi-supervised machine learning algorithm that learns from\nboth labeled and unlabeled data. This approach yields state-of-the-art results\non two datasets against strong baselines; The second method extracts\nrelationships from compound nouns. Our knowledge-aware method for compound noun\nanalysis accurately extracts relationships and significantly outperforms a\nbaseline that does not make use of background knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 03:33:07 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Nakashole", "Ndapandula", ""], ["Mitchell", "Tom M.", ""]]}, {"id": "1612.05497", "submitter": "Wen Jiang", "authors": "Wen Jiang", "title": "A correlation coefficient of belief functions", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to manage conflict is still an open issue in Dempster-Shafer evidence\ntheory. The correlation coefficient can be used to measure the similarity of\nevidence in Dempster-Shafer evidence theory. However, existing correlation\ncoefficients of belief functions have some shortcomings. In this paper, a new\ncorrelation coefficient is proposed with many desirable properties. One of its\napplications is to measure the conflict degree among belief functions. Some\nnumerical examples and comparisons demonstrate the effectiveness of the\ncorrelation coefficient.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 14:58:17 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 03:29:42 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Jiang", "Wen", ""]]}, {"id": "1612.05502", "submitter": "Neil Seward", "authors": "Neil Seward", "title": "Defensive Player Classification in the National Basketball Association", "comments": "Methods provided in paper do not go into enough detail into how\n  defensive formations are beneficial to teams. Needs more explanation behind\n  formations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The National Basketball Association(NBA) has expanded their data gathering\nand have heavily invested in new technologies to gather advanced performance\nmetrics on players. This expanded data set allows analysts to use unique\nperformance metrics in models to estimate and classify player performance.\nInstead of grouping players together based on physical attributes and positions\nplayed, analysts can group together players that play similar to each other\nbased on these tracked metrics. Existing methods for player classification have\ntypically used offensive metrics for clustering [1]. There have been attempts\nto classify players using past defensive metrics, but the lack of quality\nmetrics has not produced promising results. The classifications presented in\nthe paper use newly introduced defensive metrics to find different defensive\npositions for each player. Without knowing the number of categories that\nplayers can be cast into, Gaussian Mixture Models (GMM) can be applied to find\nthe optimal number of clusters. In the model presented, five different\ndefensive player types can be identified.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:22:00 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 18:53:29 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Seward", "Neil", ""]]}, {"id": "1612.05533", "submitter": "Jingwei Zhang", "authors": "Jingwei Zhang, Jost Tobias Springenberg, Joschka Boedecker, Wolfram\n  Burgard", "title": "Deep Reinforcement Learning with Successor Features for Navigation\n  across Similar Environments", "comments": "Camera ready version for IROS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of robot navigation in simple maze-like\nenvironments where the robot has to rely on its onboard sensors to perform the\nnavigation task. In particular, we are interested in solutions to this problem\nthat do not require localization, mapping or planning. Additionally, we require\nthat our solution can quickly adapt to new situations (e.g., changing\nnavigation goals and environments). To meet these criteria we frame this\nproblem as a sequence of related reinforcement learning tasks. We propose a\nsuccessor feature based deep reinforcement learning algorithm that can learn to\ntransfer knowledge from previously mastered navigation tasks to new problem\ninstances. Our algorithm substantially decreases the required learning time\nafter the first task instance has been solved, which makes it easily adaptable\nto changing environments. We validate our method in both simulated and real\nrobot experiments with a Robotino and compare it to a set of baseline methods\nincluding classical planning-based navigation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 16:15:26 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 21:11:04 GMT"}, {"version": "v3", "created": "Sun, 23 Jul 2017 16:36:33 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zhang", "Jingwei", ""], ["Springenberg", "Jost Tobias", ""], ["Boedecker", "Joschka", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1612.05535", "submitter": "Lucas Lamata", "authors": "Unai Alvarez-Rodriguez, Lucas Lamata, Pablo Escandell-Montero, Jos\\'e\n  D. Mart\\'in-Guerrero, Enrique Solano", "title": "Supervised Quantum Learning without Measurements", "comments": null, "journal-ref": "Scientific Reports 7, 13645 (2017)", "doi": "10.1038/s41598-017-13378-0", "report-no": null, "categories": "quant-ph cond-mat.mes-hall cond-mat.supr-con cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a quantum machine learning algorithm for efficiently solving a\nclass of problems encoded in quantum controlled unitary operations. The central\nphysical mechanism of the protocol is the iteration of a quantum time-delayed\nequation that introduces feedback in the dynamics and eliminates the necessity\nof intermediate measurements. The performance of the quantum algorithm is\nanalyzed by comparing the results obtained in numerical simulations with the\noutcome of classical machine learning methods for the same problem. The use of\ntime-delayed equations enhances the toolbox of the field of quantum machine\nlearning, which may enable unprecedented applications in quantum technologies.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 16:15:45 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 07:44:58 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Alvarez-Rodriguez", "Unai", ""], ["Lamata", "Lucas", ""], ["Escandell-Montero", "Pablo", ""], ["Mart\u00edn-Guerrero", "Jos\u00e9 D.", ""], ["Solano", "Enrique", ""]]}, {"id": "1612.05596", "submitter": "Emre Neftci", "authors": "Emre Neftci, Charles Augustine, Somnath Paul, Georgios Detorakis", "title": "Neuromorphic Deep Learning Machines", "comments": null, "journal-ref": null, "doi": "10.3389/fnins.2017.00324", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ongoing challenge in neuromorphic computing is to devise general and\ncomputationally efficient models of inference and learning which are compatible\nwith the spatial and temporal constraints of the brain. One increasingly\npopular and successful approach is to take inspiration from inference and\nlearning algorithms used in deep neural networks. However, the workhorse of\ndeep learning, the gradient descent Back Propagation (BP) rule, often relies on\nthe immediate availability of network-wide information stored with\nhigh-precision memory, and precise operations that are difficult to realize in\nneuromorphic hardware. Remarkably, recent work showed that exact backpropagated\nweights are not essential for learning deep representations. Random BP replaces\nfeedback weights with random ones and encourages the network to adjust its\nfeed-forward weights to learn pseudo-inverses of the (random) feedback weights.\nBuilding on these results, we demonstrate an event-driven random BP (eRBP) rule\nthat uses an error-modulated synaptic plasticity for learning deep\nrepresentations in neuromorphic computing hardware. The rule requires only one\naddition and two comparisons for each synaptic weight using a two-compartment\nleaky Integrate & Fire (I&F) neuron, making it very suitable for implementation\nin digital or mixed-signal neuromorphic hardware. Our results show that using\neRBP, deep representations are rapidly learned, achieving nearly identical\nclassification accuracies compared to artificial neural network simulations on\nGPUs, while being robust to neural and synaptic state quantizations during\nlearning.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 19:06:35 GMT"}, {"version": "v2", "created": "Sat, 21 Jan 2017 07:45:32 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Neftci", "Emre", ""], ["Augustine", "Charles", ""], ["Paul", "Somnath", ""], ["Detorakis", "Georgios", ""]]}, {"id": "1612.05628", "submitter": "Kavosh Asadi", "authors": "Kavosh Asadi, Michael L. Littman", "title": "An Alternative Softmax Operator for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A softmax operator applied to a set of values acts somewhat like the\nmaximization function and somewhat like an average. In sequential decision\nmaking, softmax is often used in settings where it is necessary to maximize\nutility but also to hedge against problems that arise from putting all of one's\nweight behind a single maximum utility decision. The Boltzmann softmax operator\nis the most commonly used softmax operator in this setting, but we show that\nthis operator is prone to misbehavior. In this work, we study a differentiable\nsoftmax operator that, among other properties, is a non-expansion ensuring a\nconvergent behavior in learning and planning. We introduce a variant of SARSA\nalgorithm that, by utilizing the new operator, computes a Boltzmann policy with\na state-dependent temperature parameter. We show that the algorithm is\nconvergent and that it performs favorably in practice.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 20:49:35 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 19:02:06 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 02:05:31 GMT"}, {"version": "v4", "created": "Tue, 13 Jun 2017 05:28:04 GMT"}, {"version": "v5", "created": "Wed, 14 Jun 2017 14:29:04 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Asadi", "Kavosh", ""], ["Littman", "Michael L.", ""]]}, {"id": "1612.05688", "submitter": "Xiujun Li", "authors": "Xiujun Li, Zachary C. Lipton, Bhuwan Dhingra, Lihong Li, Jianfeng Gao,\n  Yun-Nung Chen", "title": "A User Simulator for Task-Completion Dialogues", "comments": "14 pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite widespread interests in reinforcement-learning for task-oriented\ndialogue systems, several obstacles can frustrate research and development\nprogress. First, reinforcement learners typically require interaction with the\nenvironment, so conventional dialogue corpora cannot be used directly. Second,\neach task presents specific challenges, requiring separate corpus of\ntask-specific annotated data. Third, collecting and annotating human-machine or\nhuman-human conversations for task-oriented dialogues requires extensive domain\nknowledge. Because building an appropriate dataset can be both financially\ncostly and time-consuming, one popular approach is to build a user simulator\nbased upon a corpus of example dialogues. Then, one can train reinforcement\nlearning agents in an online fashion as they interact with the simulator.\nDialogue agents trained on these simulators can serve as an effective starting\npoint. Once agents master the simulator, they may be deployed in a real\nenvironment to interact with humans, and continue to be trained online. To ease\nempirical algorithmic comparisons in dialogues, this paper introduces a new,\npublicly available simulation framework, where our simulator, designed for the\nmovie-booking domain, leverages both rules and collected data. The simulator\nsupports two tasks: movie ticket booking and movie seeking. Finally, we\ndemonstrate several agents and detail the procedure to add and test your own\nagent in the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 01:03:55 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 20:04:30 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 05:52:42 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Li", "Xiujun", ""], ["Lipton", "Zachary C.", ""], ["Dhingra", "Bhuwan", ""], ["Li", "Lihong", ""], ["Gao", "Jianfeng", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "1612.05693", "submitter": "Hang Ma", "authors": "Hang Ma, Sven Koenig", "title": "Optimal Target Assignment and Path Finding for Teams of Agents", "comments": "In AAMAS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the TAPF (combined target-assignment and path-finding) problem for\nteams of agents in known terrain, which generalizes both the anonymous and\nnon-anonymous multi-agent path-finding problems. Each of the teams is given the\nsame number of targets as there are agents in the team. Each agent has to move\nto exactly one target given to its team such that all targets are visited. The\nTAPF problem is to first assign agents to targets and then plan collision-free\npaths for the agents to their targets in a way such that the makespan is\nminimized. We present the CBM (Conflict-Based Min-Cost-Flow) algorithm, a\nhierarchical algorithm that solves TAPF instances optimally by combining ideas\nfrom anonymous and non-anonymous multi-agent path-finding algorithms. On the\nlow level, CBM uses a min-cost max-flow algorithm on a time-expanded network to\nassign all agents in a single team to targets and plan their paths. On the high\nlevel, CBM uses conflict-based search to resolve collisions among agents in\ndifferent teams. Theoretically, we prove that CBM is correct, complete and\noptimal. Experimentally, we show the scalability of CBM to TAPF instances with\ndozens of teams and hundreds of agents and adapt it to a simulated warehouse\nsystem.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 01:54:23 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Ma", "Hang", ""], ["Koenig", "Sven", ""]]}, {"id": "1612.05695", "submitter": "Pooya Ronagh", "authors": "Daniel Crawford, Anna Levit, Navid Ghadermarzy, Jaspreet S. Oberoi,\n  Pooya Ronagh", "title": "Reinforcement Learning Using Quantum Boltzmann Machines", "comments": null, "journal-ref": "Quantum Information & Computation, Volume 18 (1-2), pp. 0051-0074\n  (2018)", "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate whether quantum annealers with select chip layouts can\noutperform classical computers in reinforcement learning tasks. We associate a\ntransverse field Ising spin Hamiltonian with a layout of qubits similar to that\nof a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to\nnumerically simulate quantum sampling from this system. We design a\nreinforcement learning algorithm in which the set of visible nodes representing\nthe states and actions of an optimal policy are the first and last layers of\nthe deep network. In absence of a transverse field, our simulations show that\nDBMs are trained more effectively than restricted Boltzmann machines (RBM) with\nthe same number of nodes. We then develop a framework for training the network\nas a quantum Boltzmann machine (QBM) in the presence of a significant\ntransverse field for reinforcement learning. This method also outperforms the\nreinforcement learning method that uses RBMs.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 02:33:41 GMT"}, {"version": "v2", "created": "Sun, 25 Dec 2016 08:18:19 GMT"}, {"version": "v3", "created": "Thu, 3 Jan 2019 20:49:47 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Crawford", "Daniel", ""], ["Levit", "Anna", ""], ["Ghadermarzy", "Navid", ""], ["Oberoi", "Jaspreet S.", ""], ["Ronagh", "Pooya", ""]]}, {"id": "1612.05729", "submitter": "Mirko Polato", "authors": "Mirko Polato and Fabio Aiolli", "title": "Exploiting sparsity to build efficient kernel based collaborative\n  filtering for top-N item recommendation", "comments": "Under revision for Neurocomputing (Elsevier Journal)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of implicit feedback datasets has raised the\ninterest in developing effective collaborative filtering techniques able to\ndeal asymmetrically with unambiguous positive feedback and ambiguous negative\nfeedback. In this paper, we propose a principled kernel-based collaborative\nfiltering method for top-N item recommendation with implicit feedback. We\npresent an efficient implementation using the linear kernel, and we show how to\ngeneralize it to kernels of the dot product family preserving the efficiency.\nWe also investigate on the elements which influence the sparsity of a standard\ncosine kernel. This analysis shows that the sparsity of the kernel strongly\ndepends on the properties of the dataset, in particular on the long tail\ndistribution. We compare our method with state-of-the-art algorithms achieving\ngood results both in terms of efficiency and effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 10:50:41 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Polato", "Mirko", ""], ["Aiolli", "Fabio", ""]]}, {"id": "1612.05734", "submitter": "Valentina Franzoni", "authors": "Valentina Franzoni, Giulio Biondi, Alfredo Milani, Yuanxi Li", "title": "Web-based Semantic Similarity for Emotion Recognition in Web Objects", "comments": "Authors preprint, including revision differences with respect to the\n  main publication 'Web-based Similarity for Emotion Recognition in Web\n  Objects' published in the UCC '16 workshop in IEEE UCC, December 06 - 09,\n  2016, Shanghai, China. DOI: http://dx.doi.org/10.1145/2996890.3007883", "journal-ref": "In Proc 9th International Conference on Utility and Cloud\n  Computing (UCC 2016). ACM, New York, NY, USA, 327-332", "doi": "10.1145/2996890.3007883", "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project we propose a new approach for emotion recognition using\nweb-based similarity (e.g. confidence, PMI and PMING). We aim to extract basic\nemotions from short sentences with emotional content (e.g. news titles, tweets,\ncaptions), performing a web-based quantitative evaluation of semantic proximity\nbetween each word of the analyzed sentence and each emotion of a psychological\nmodel (e.g. Plutchik, Ekman, Lovheim). The phases of the extraction include:\ntext preprocessing (tokenization, stop words, filtering), search engine\nautomated query, HTML parsing of results (i.e. scraping), estimation of\nsemantic proximity, ranking of emotions according to proximity measures. The\nmain idea is that, since it is possible to generalize semantic similarity under\nthe assumption that similar concepts co-occur in documents indexed in search\nengines, therefore also emotions can be generalized in the same way, through\ntags or terms that express them in a particular language, ranking emotions.\nTraining results are compared to human evaluation, then additional comparative\ntests on results are performed, both for the global ranking correlation (e.g.\nKendall, Spearman, Pearson) both for the evaluation of the emotion linked to\neach single word. Different from sentiment analysis, our approach works at a\ndeeper level of abstraction, aiming at recognizing specific emotions and not\nonly the positive/negative sentiment, in order to predict emotions as semantic\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 11:36:06 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Franzoni", "Valentina", ""], ["Biondi", "Giulio", ""], ["Milani", "Alfredo", ""], ["Li", "Yuanxi", ""]]}, {"id": "1612.05756", "submitter": "Karl Schlechta", "authors": "Karl Schlechta (LIF)", "title": "A Comment on Argumentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the theory of defaults and their meaning of [GS16] to develop (the\noutline of a) new theory of argumentation.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 13:36:33 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Schlechta", "Karl", "", "LIF"]]}, {"id": "1612.05971", "submitter": "Fanlin Meng Dr", "authors": "Fanlin Meng, Xiao-Jun Zeng, Yan Zhang, Chris J. Dent, and Dunwei Gong", "title": "An Integrated Optimization + Learning Approach to Optimal Dynamic\n  Pricing for the Retailer with Multi-type Customers in Smart Grids", "comments": "38 pages, 6 figures", "journal-ref": null, "doi": "10.1016/j.ins.2018.03.039", "report-no": null, "categories": "cs.SY cs.AI cs.GT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a realistic and meaningful scenario in the context\nof smart grids where an electricity retailer serves three different types of\ncustomers, i.e., customers with an optimal home energy management system\nembedded in their smart meters (C-HEMS), customers with only smart meters\n(C-SM), and customers without smart meters (C-NONE). The main objective of this\npaper is to support the retailer to make optimal day-ahead dynamic pricing\ndecisions in such a mixed customer pool. To this end, we propose a two-level\ndecision-making framework where the retailer acting as upper-level agent\nfirstly announces its electricity prices of next 24 hours and customers acting\nas lower-level agents subsequently schedule their energy usages accordingly.\nFor the lower level problem, we model the price responsiveness of different\ncustomers according to their unique characteristics. For the upper level\nproblem, we optimize the dynamic prices for the retailer to maximize its profit\nsubject to realistic market constraints. The above two-level model is tackled\nby genetic algorithms (GA) based distributed optimization methods while its\nfeasibility and effectiveness are confirmed via simulation results.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 18:44:49 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 11:46:21 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 12:25:04 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Meng", "Fanlin", ""], ["Zeng", "Xiao-Jun", ""], ["Zhang", "Yan", ""], ["Dent", "Chris J.", ""], ["Gong", "Dunwei", ""]]}, {"id": "1612.06000", "submitter": "Kavosh Asadi", "authors": "Kavosh Asadi, Jason D. Williams", "title": "Sample-efficient Deep Reinforcement Learning for Dialog Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing a dialog policy as a recurrent neural network (RNN) is\nattractive because it handles partial observability, infers a latent\nrepresentation of state, and can be optimized with supervised learning (SL) or\nreinforcement learning (RL). For RL, a policy gradient approach is natural, but\nis sample inefficient. In this paper, we present 3 methods for reducing the\nnumber of dialogs required to optimize an RNN-based dialog policy with RL. The\nkey idea is to maintain a second RNN which predicts the value of the current\npolicy, and to apply experience replay to both networks. On two tasks, these\nmethods reduce the number of dialogs/episodes required by about a third, vs.\nstandard policy gradient methods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 21:51:10 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Asadi", "Kavosh", ""], ["Williams", "Jason D.", ""]]}, {"id": "1612.06007", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa and Mihaela van der Schaar", "title": "A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal\n  Data: Learning and Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling continuous-time physiological processes that manifest a patient's\nevolving clinical states is a key step in approaching many problems in\nhealthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model\n(HASMM): a versatile probabilistic model that is capable of capturing the\nmodern electronic health record (EHR) data. Unlike exist- ing models, an HASMM\naccommodates irregularly sampled, temporally correlated, and informatively\ncensored physiological data, and can describe non-stationary clinical state\ntransitions. Learning an HASMM from the EHR data is achieved via a novel\nforward- filtering backward-sampling Monte-Carlo EM algorithm that exploits the\nknowledge of the end-point clinical outcomes (informative censoring) in the EHR\ndata, and implements the E-step by sequentially sampling the patients' clinical\nstates in the reverse-time direction while conditioning on the future states.\nReal-time inferences are drawn via a forward- filtering algorithm that operates\non a virtually constructed discrete-time embedded Markov chain that mirrors the\npatient's continuous-time state trajectory. We demonstrate the di- agnostic and\nprognostic utility of the HASMM in a critical care prognosis setting using a\nreal-world dataset for patients admitted to the Ronald Reagan UCLA Medical\nCenter.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 23:02:02 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 13:44:59 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1612.06018", "submitter": "Erik Talvitie", "authors": "Erik Talvitie", "title": "Self-Correcting Models for Model-Based Reinforcement Learning", "comments": "Original paper appeared in Proceedings of the 31st AAAI Conference on\n  Artificial Intelligence, 2017. This version incorporates the appendix into\n  document (rather than as supplementary material), corrects a minor error in\n  Lemma 1, and fixes some type-os", "journal-ref": "Proceedings of the Thirty-First AAAI Conference on Artificial\n  Intelligence, 2597-2603 (2017)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an agent cannot represent a perfectly accurate model of its\nenvironment's dynamics, model-based reinforcement learning (MBRL) can fail\ncatastrophically. Planning involves composing the predictions of the model;\nwhen flawed predictions are composed, even minor errors can compound and render\nthe model useless for planning. Hallucinated Replay (Talvitie 2014) trains the\nmodel to \"correct\" itself when it produces errors, substantially improving MBRL\nwith flawed models. This paper theoretically analyzes this approach,\nilluminates settings in which it is likely to be effective or ineffective, and\npresents a novel error bound, showing that a model's ability to self-correct is\nmore tightly related to MBRL performance than one-step prediction error. These\nresults inspire an MBRL algorithm for deterministic MDPs with performance\nguarantees that are robust to model class limitations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 01:09:23 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 18:53:51 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Talvitie", "Erik", ""]]}, {"id": "1612.06038", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Jonito Aerts Argu\\\"elles, Lester Beltran, Lyneth\n  Beltran, Massimiliano Sassoli de Bianchi, Sandro Sozzo and Tomas Veloz", "title": "Context and Interference Effects in the Combinations of Natural Concepts", "comments": "12 pages, no figures", "journal-ref": "In: Br\\'ezillon P., Turner R., Penco C. (eds) Modeling and Using\n  Context. CONTEXT 2017. Lecture Notes in Computer Science, vol 10257.\n  Springer, Cham (2017)", "doi": "10.1007/978-3-319-57837-8_54", "report-no": null, "categories": "cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mathematical formalism of quantum theory exhibits significant\neffectiveness when applied to cognitive phenomena that have resisted\ntraditional (set theoretical) modeling. Relying on a decade of research on the\noperational foundations of micro-physical and conceptual entities, we present a\ntheoretical framework for the representation of concepts and their conjunctions\nand disjunctions that uses the quantum formalism. This framework provides a\nunified solution to the 'conceptual combinations problem' of cognitive\npsychology, explaining the observed deviations from classical (Boolean, fuzzy\nset and Kolmogorovian) structures in terms of genuine quantum effects. In\nparticular, natural concepts 'interfere' when they combine to form more complex\nconceptual entities, and they also exhibit a 'quantum-type context-dependence',\nwhich are responsible of the 'over- and under-extension' that are\nsystematically observed in experiments on membership judgments.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 04:01:55 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Aerts", "Diederik", ""], ["Argu\u00eblles", "Jonito Aerts", ""], ["Beltran", "Lester", ""], ["Beltran", "Lyneth", ""], ["de Bianchi", "Massimiliano Sassoli", ""], ["Sozzo", "Sandro", ""], ["Veloz", "Tomas", ""]]}, {"id": "1612.06043", "submitter": "Raphael Shu", "authors": "Raphael Shu and Hideki Nakayama", "title": "An Empirical Study of Adequate Vision Span for Attention-Based Neural\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the attention mechanism plays a key role to achieve high\nperformance for Neural Machine Translation models. However, as it computes a\nscore function for the encoder states in all positions at each decoding step,\nthe attention model greatly increases the computational complexity. In this\npaper, we investigate the adequate vision span of attention models in the\ncontext of machine translation, by proposing a novel attention framework that\nis capable of reducing redundant score computation dynamically. The term\n\"vision span\" means a window of the encoder states considered by the attention\nmodel in one step. In our experiments, we found that the average window size of\nvision span can be reduced by over 50% with modest loss in accuracy on\nEnglish-Japanese and German-English translation tasks.% This results indicate\nthat the conventional attention mechanism performs a significant amount of\nredundant computation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 04:23:22 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 01:43:35 GMT"}, {"version": "v3", "created": "Mon, 17 Apr 2017 05:58:12 GMT"}, {"version": "v4", "created": "Thu, 8 Jun 2017 07:52:36 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Shu", "Raphael", ""], ["Nakayama", "Hideki", ""]]}, {"id": "1612.06062", "submitter": "Ganesh J", "authors": "Ganesh J, Manish Gupta and Vasudeva Varma", "title": "Improving Tweet Representations using Temporal and User Context", "comments": "To be presented at European Conference on Information Retrieval\n  (ECIR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel representation learning model which computes\nsemantic representations for tweets accurately. Our model systematically\nexploits the chronologically adjacent tweets ('context') from users' Twitter\ntimelines for this task. Further, we make our model user-aware so that it can\ndo well in modeling the target tweet by exploiting the rich knowledge about the\nuser such as the way the user writes the post and also summarizing the topics\non which the user writes. We empirically demonstrate that the proposed models\noutperform the state-of-the-art models in predicting the user profile\nattributes like spouse, education and job by 19.66%, 2.27% and 2.22%\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 07:06:34 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["J", "Ganesh", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1612.06174", "submitter": "Wen Jiang", "authors": "Shuai Xu, Wen Jiang, Yehang Shou", "title": "A modified Physarum-inspired model for the user equilibrium traffic\n  assignment problem", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The user equilibrium traffic assignment principle is very important in the\ntraffic assignment problem. Mathematical programming models are designed to\nsolve the user equilibrium problem in traditional algorithms. Recently, the\nPhysarum shows the ability to address the user equilibrium and system\noptimization traffic assignment problems. However, the Physarum model are not\nefficient in real traffic networks with two-way traffic characteristics and\nmultiple origin-destination pairs. In this article, a modified\nPhysarum-inspired model for the user equilibrium problem is proposed. By\ndecomposing traffic flux based on origin nodes, the traffic flux from different\norigin-destination pairs can be distinguished in the proposed model. The\nPhysarum can obtain the equilibrium traffic flux when no shorter path can be\ndiscovered between each origin-destination pair. Finally, numerical examples\nuse the Sioux Falls network to demonstrate the rationality and convergence\nproperties of the proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 13:36:09 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Xu", "Shuai", ""], ["Jiang", "Wen", ""], ["Shou", "Yehang", ""]]}, {"id": "1612.06340", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried and Farzana Yusuf", "title": "Computing Human-Understandable Strategies", "comments": "Earlier version appeared in Proceedings of the Workshop on Computer\n  Poker and Imperfect Information Games at AAAI Conference on Artificial\n  Intelligence, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for equilibrium computation generally make no attempt to ensure\nthat the computed strategies are understandable by humans. For instance the\nstrategies for the strongest poker agents are represented as massive binary\nfiles. In many situations, we would like to compute strategies that can\nactually be implemented by humans, who may have computational limitations and\nmay only be able to remember a small number of features or components of the\nstrategies that have been computed. We study poker games where private\ninformation distributions can be arbitrary. We create a large training set of\ngame instances and solutions, by randomly selecting the information\nprobabilities, and present algorithms that learn from the training instances in\norder to perform well in games with unseen information distributions. We are\nable to conclude several new fundamental rules about poker strategy that can be\neasily implemented by humans.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 20:40:19 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 17:54:11 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Ganzfried", "Sam", ""], ["Yusuf", "Farzana", ""]]}, {"id": "1612.06370", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Ross Girshick, Piotr Doll\\'ar, Trevor Darrell, Bharath\n  Hariharan", "title": "Learning Features by Watching Objects Move", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel yet intuitive approach to unsupervised feature\nlearning. Inspired by the human visual system, we explore whether low-level\nmotion-based grouping cues can be used to learn an effective visual\nrepresentation. Specifically, we use unsupervised motion-based segmentation on\nvideos to obtain segments, which we use as 'pseudo ground truth' to train a\nconvolutional network to segment objects from a single frame. Given the\nextensive evidence that motion plays a key role in the development of the human\nvisual system, we hope that this straightforward approach to unsupervised\nlearning will be more effective than cleverly designed 'pretext' tasks studied\nin the literature. Indeed, our extensive experiments show that this is the\ncase. When used for transfer learning on object detection, our representation\nsignificantly outperforms previous unsupervised approaches across multiple\nsettings, especially when training data for the target task is scarce.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 20:56:04 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 04:28:47 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Pathak", "Deepak", ""], ["Girshick", "Ross", ""], ["Doll\u00e1r", "Piotr", ""], ["Darrell", "Trevor", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1612.06476", "submitter": "Haris Aziz", "authors": "Haris Aziz and Shenwei Huang", "title": "Computational Complexity of Testing Proportional Justified\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a committee voting setting in which each voter approves of a\nsubset of candidates and based on the approvals, a target number of candidates\nare selected. Aziz et al. (2015) proposed two representation axioms called\njustified representation and extended justified representation. Whereas the\nformer can be tested as well as achieved in polynomial time, the latter\nproperty is coNP-complete to test and no polynomial-time algorithm is known to\nachieve it. Interestingly, S{\\'a}nchez-Fern{\\'a}ndez et~al. (2016) proposed an\nintermediate property called proportional justified representation that admits\na polynomial-time algorithm to achieve. The complexity of testing proportional\njustified representation has remained an open problem. In this paper, we settle\nthe complexity by proving that testing proportional justified representation is\ncoNP-complete. We complement the complexity result by showing that the problem\nadmits efficient algorithms if any of the following parameters are bounded: (1)\nnumber of voters (2) number of candidates (3) maximum number of candidates\napproved by a voter (4) maximum number of voters approving a given candidate.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 01:23:18 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 22:56:43 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Aziz", "Haris", ""], ["Huang", "Shenwei", ""]]}, {"id": "1612.06505", "submitter": "Kim Batselier", "authors": "Zhongming Chen, Kim Batselier, Johan A.K. Suykens, Ngai Wong", "title": "Parallelized Tensor Train Learning of Polynomial Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In pattern classification, polynomial classifiers are well-studied methods as\nthey are capable of generating complex decision surfaces. Unfortunately, the\nuse of multivariate polynomials is limited to kernels as in support vector\nmachines, because polynomials quickly become impractical for high-dimensional\nproblems. In this paper, we effectively overcome the curse of dimensionality by\nemploying the tensor train format to represent a polynomial classifier. Based\non the structure of tensor trains, two learning algorithms are proposed which\ninvolve solving different optimization problems of low computational\ncomplexity. Furthermore, we show how both regularization to prevent overfitting\nand parallelization, which enables the use of large training sets, are\nincorporated into these methods. Both the efficiency and efficacy of our\ntensor-based polynomial classifier are then demonstrated on the two popular\ndatasets USPS and MNIST.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 04:54:49 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 08:37:39 GMT"}, {"version": "v3", "created": "Fri, 23 Dec 2016 12:48:53 GMT"}, {"version": "v4", "created": "Mon, 6 Nov 2017 01:03:44 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Chen", "Zhongming", ""], ["Batselier", "Kim", ""], ["Suykens", "Johan A. K.", ""], ["Wong", "Ngai", ""]]}, {"id": "1612.06528", "submitter": "Sarmimala Saikia", "authors": "Sarmimala Saikia, Lovekesh Vig, Ashwin Srinivasan, Gautam Shroff,\n  Puneet Agarwal, Richa Rawat", "title": "Neuro-symbolic EDA-based Optimisation using ILP-enhanced DBNs", "comments": "9 pages, 7 figures, Cognitive Computation: Integrating Neural and\n  Symbolic Approaches (Workshop at 30th Conference on Neural Information\n  Processing Systems (NIPS 2016), Barcelona, Spain.),\n  http://daselab.cs.wright.edu/nesy/CoCo2016/coco_nips_2016_pre-proceedings.pdf\n  (page 78-86). arXiv admin note: substantial text overlap with\n  arXiv:1608.01093", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate solving discrete optimisation problems using the estimation of\ndistribution (EDA) approach via a novel combination of deep belief\nnetworks(DBN) and inductive logic programming (ILP).While DBNs are used to\nlearn the structure of successively better feasible solutions,ILP enables the\nincorporation of domain-based background knowledge related to the goodness of\nsolutions.Recent work showed that ILP could be an effective way to use domain\nknowledge in an EDA scenario.However,in a purely ILP-based EDA,sampling\nsuccessive populations is either inefficient or not straightforward.In our\nNeuro-symbolic EDA,an ILP engine is used to construct a model for good\nsolutions using domain-based background knowledge.These rules are introduced as\nBoolean features in the last hidden layer of DBNs used for EDA-based\noptimization.This incorporation of logical ILP features requires some changes\nwhile training and sampling from DBNs: (a)our DBNs need to be trained with data\nfor units at the input layer as well as some units in an otherwise hidden\nlayer, and (b)we would like the samples generated to be drawn from instances\nentailed by the logical model.We demonstrate the viability of our approach on\ninstances of two optimisation problems: predicting optimal depth-of-win for the\nKRK endgame,and jobshop scheduling.Our results are promising: (i)On each\niteration of distribution estimation,samples obtained with an ILP-assisted DBN\nhave a substantially greater proportion of good solutions than samples\ngenerated using a DBN without ILP features, and (ii)On termination of\ndistribution estimation,samples obtained using an ILP-assisted DBN contain more\nnear-optimal samples than samples from a DBN without ILP features.These results\nsuggest that the use of ILP-constructed theories could be useful for\nincorporating complex domain-knowledge into deep models for estimation of\ndistribution based procedures.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 06:56:12 GMT"}], "update_date": "2017-01-01", "authors_parsed": [["Saikia", "Sarmimala", ""], ["Vig", "Lovekesh", ""], ["Srinivasan", "Ashwin", ""], ["Shroff", "Gautam", ""], ["Agarwal", "Puneet", ""], ["Rawat", "Richa", ""]]}, {"id": "1612.06589", "submitter": "Noriyoshi Sukegawa", "authors": "Naoki Nishimura, Noriyoshi Sukegawa, Yuichi Takano, Jiro Iwanaga", "title": "A Latent-class Model for Estimating Product-choice Probabilities from\n  Clickstream Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes customer product-choice behavior based on the recency and\nfrequency of each customer's page views on e-commerce sites. Recently, we\ndevised an optimization model for estimating product-choice probabilities that\nsatisfy monotonicity, convexity, and concavity constraints with respect to\nrecency and frequency. This shape-restricted model delivered high predictive\nperformance even when there were few training samples. However, typical\ne-commerce sites deal in many different varieties of products, so the\npredictive performance of the model can be further improved by integration of\nsuch product heterogeneity. For this purpose, we develop a novel latent-class\nshape-restricted model for estimating product-choice probabilities for each\nlatent class of products. We also give a tailored expectation-maximization\nalgorithm for parameter estimation. Computational results demonstrate that\nhigher predictive performance is achieved with our latent-class model than with\nthe previous shape-restricted model and common latent-class logistic\nregression.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 10:22:37 GMT"}], "update_date": "2017-01-01", "authors_parsed": [["Nishimura", "Naoki", ""], ["Sukegawa", "Noriyoshi", ""], ["Takano", "Yuichi", ""], ["Iwanaga", "Jiro", ""]]}, {"id": "1612.06704", "submitter": "Donggeun Yoo", "authors": "Donggeun Yoo, Sunggyun Park, Kyunghyun Paeng, Joon-Young Lee, In So\n  Kweon", "title": "Action-Driven Object Detection with Top-Down Visual Attentions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dominant paradigm for deep learning based object detection relies on a\n\"bottom-up\" approach using \"passive\" scoring of class agnostic proposals. These\napproaches are efficient but lack of holistic analysis of scene-level context.\nIn this paper, we present an \"action-driven\" detection mechanism using our\n\"top-down\" visual attention model. We localize an object by taking sequential\nactions that the attention model provides. The attention model conditioned with\nan image region provides required actions to get closer toward a target object.\nAn action at each time step is weak itself but an ensemble of the sequential\nactions makes a bounding-box accurately converge to a target object boundary.\nThis attention model we call AttentionNet is composed of a convolutional neural\nnetwork. During our whole detection procedure, we only utilize the actions from\na single AttentionNet without any modules for object proposals nor post\nbounding-box regression. We evaluate our top-down detection mechanism over the\nPASCAL VOC series and ILSVRC CLS-LOC dataset, and achieve state-of-the-art\nperformances compared to the major bottom-up detection methods. In particular,\nour detection mechanism shows a strong advantage in elaborate localization by\noutperforming Faster R-CNN with a margin of +7.1% over PASCAL VOC 2007 when we\nincrease the IoU threshold for positive detection to 0.7.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 15:24:46 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Yoo", "Donggeun", ""], ["Park", "Sunggyun", ""], ["Paeng", "Kyunghyun", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "1612.06915", "submitter": "Neil Burch", "authors": "Neil Burch, Martin Schmid, Matej Morav\\v{c}\\'ik, Michael Bowling", "title": "AIVAT: A New Variance Reduction Technique for Agent Evaluation in\n  Imperfect Information Games", "comments": "To appear at AAAI-17 Workshop on Computer Poker and Imperfect\n  Information Games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating agent performance when outcomes are stochastic and agents use\nrandomized strategies can be challenging when there is limited data available.\nThe variance of sampled outcomes may make the simple approach of Monte Carlo\nsampling inadequate. This is the case for agents playing heads-up no-limit\nTexas hold'em poker, where man-machine competitions have involved multiple days\nof consistent play and still not resulted in statistically significant\nconclusions even when the winner's margin is substantial. In this paper, we\nintroduce AIVAT, a low variance, provably unbiased value assessment tool that\nuses an arbitrary heuristic estimate of state value, as well as the explicit\nstrategy of a subset of the agents. Unlike existing techniques which reduce the\nvariance from chance events, or only consider game ending actions, AIVAT\nreduces the variance both from choices by nature and by players with a known\nstrategy. The resulting estimator in no-limit poker can reduce the number of\nhands needed to draw statistical conclusions by more than a factor of 10.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 23:09:40 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 21:22:12 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Burch", "Neil", ""], ["Schmid", "Martin", ""], ["Morav\u010d\u00edk", "Matej", ""], ["Bowling", "Michael", ""]]}, {"id": "1612.06962", "submitter": "Zijun Wu", "authors": "Zijun Wu, Rolf Moehring, Jianhui Lai", "title": "Stochastic Runtime Analysis of a Cross Entropy Algorithm for Traveling\n  Salesman Problems", "comments": "40 pages, 7 figures", "journal-ref": "Theoretical Computer Science, 2017", "doi": "10.1016/j.tcs.2017.10.012", "report-no": null, "categories": "cs.DS cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article analyzes the stochastic runtime of a Cross-Entropy Algorithm on\ntwo classes of traveling salesman problems. The algorithm shares main features\nof the famous Max-Min Ant System with iteration-best reinforcement.\n  For simple instances that have a $\\{1,n\\}$-valued distance function and a\nunique optimal solution, we prove a stochastic runtime of $O(n^{6+\\epsilon})$\nwith the vertex-based random solution generation, and a stochastic runtime of\n$O(n^{3+\\epsilon}\\ln n)$ with the edge-based random solution generation for an\narbitrary $\\epsilon\\in (0,1)$. These runtimes are very close to the known\nexpected runtime for variants of Max-Min Ant System with best-so-far\nreinforcement. They are obtained for the stronger notion of stochastic runtime,\nwhich means that an optimal solution is obtained in that time with an\noverwhelming probability, i.e., a probability tending exponentially fast to one\nwith growing problem size.\n  We also inspect more complex instances with $n$ vertices positioned on an\n$m\\times m$ grid. When the $n$ vertices span a convex polygon, we obtain a\nstochastic runtime of $O(n^{3}m^{5+\\epsilon})$ with the vertex-based random\nsolution generation, and a stochastic runtime of $O(n^{2}m^{5+\\epsilon})$ for\nthe edge-based random solution generation. When there are $k = O(1)$ many\nvertices inside a convex polygon spanned by the other $n-k$ vertices, we obtain\na stochastic runtime of $O(n^{4}m^{5+\\epsilon}+n^{6k-1}m^{\\epsilon})$ with the\nvertex-based random solution generation, and a stochastic runtime of\n$O(n^{3}m^{5+\\epsilon}+n^{3k}m^{\\epsilon})$ with the edge-based random solution\ngeneration. These runtimes are better than the expected runtime for the\nso-called $(\\mu\\!+\\!\\lambda)$ EA reported in a recent article, and again\nobtained for the stronger notion of stochastic runtime.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 03:24:26 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 03:24:57 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Wu", "Zijun", ""], ["Moehring", "Rolf", ""], ["Lai", "Jianhui", ""]]}, {"id": "1612.07025", "submitter": "Mirko Polato", "authors": "Mirko Polato and Fabio Aiolli", "title": "Boolean kernels for collaborative filtering in top-N item recommendation", "comments": "24 pages, 28 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many personalized recommendation problems available data consists only of\npositive interactions (implicit feedback) between users and items. This problem\nis also known as One-Class Collaborative Filtering (OC-CF). Linear models\nusually achieve state-of-the-art performances on OC-CF problems and many\nefforts have been devoted to build more expressive and complex representations\nable to improve the recommendations. Recent analysis show that collaborative\nfiltering (CF) datasets have peculiar characteristics such as high sparsity and\na long tailed distribution of the ratings. In this paper we propose a boolean\nkernel, called Disjunctive kernel, which is less expressive than the linear one\nbut it is able to alleviate the sparsity issue in CF contexts. The embedding of\nthis kernel is composed by all the combinations of a certain arity d of the\ninput variables, and these combined features are semantically interpreted as\ndisjunctions of the input variables. Experiments on several CF datasets show\nthe effectiveness and the efficiency of the proposed kernel.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 09:29:09 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 10:16:20 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Polato", "Mirko", ""], ["Aiolli", "Fabio", ""]]}, {"id": "1612.07059", "submitter": "Anna Lukina", "authors": "Anna Lukina, Lukas Esterle, Christian Hirsch, Ezio Bartocci, Junxing\n  Yang, Ashish Tiwari, Scott A. Smolka, and Radu Grosu", "title": "ARES: Adaptive Receding-Horizon Synthesis of Optimal Plans", "comments": "submitted to TACAS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ARES, an efficient approximation algorithm for generating\noptimal plans (action sequences) that take an initial state of a Markov\nDecision Process (MDP) to a state whose cost is below a specified (convergence)\nthreshold. ARES uses Particle Swarm Optimization, with adaptive sizing for both\nthe receding horizon and the particle swarm. Inspired by Importance Splitting,\nthe length of the horizon and the number of particles are chosen such that at\nleast one particle reaches a next-level state, that is, a state where the cost\ndecreases by a required delta from the previous-level state. The level relation\non states and the plans constructed by ARES implicitly define a Lyapunov\nfunction and an optimal policy, respectively, both of which could be explicitly\ngenerated by applying ARES to all states of the MDP, up to some topological\nequivalence relation. We also assess the effectiveness of ARES by statistically\nevaluating its rate of success in generating optimal plans. The ARES algorithm\nresulted from our desire to clarify if flying in V-formation is a flocking\npolicy that optimizes energy conservation, clear view, and velocity alignment.\nThat is, we were interested to see if one could find optimal plans that bring a\nflock from an arbitrary initial state to a state exhibiting a single connected\nV-formation. For flocks with 7 birds, ARES is able to generate a plan that\nleads to a V-formation in 95% of the 8,000 random initial configurations within\n63 seconds, on average. ARES can also be easily customized into a\nmodel-predictive controller (MPC) with an adaptive receding horizon and\nstatistical guarantees of convergence. To the best of our knowledge, our\nadaptive-sizing approach is the first to provide convergence guarantees in\nreceding-horizon techniques.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 11:16:50 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Lukina", "Anna", ""], ["Esterle", "Lukas", ""], ["Hirsch", "Christian", ""], ["Bartocci", "Ezio", ""], ["Yang", "Junxing", ""], ["Tiwari", "Ashish", ""], ["Smolka", "Scott A.", ""], ["Grosu", "Radu", ""]]}, {"id": "1612.07139", "submitter": "Lei Tai", "authors": "Lei Tai and Jingwei Zhang and Ming Liu and Joschka Boedecker and\n  Wolfram Burgard", "title": "A Survey of Deep Network Solutions for Learning Control in Robotics:\n  From Reinforcement to Imitation", "comments": "19 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have been widely applied, achieving state-of-the-art\nresults in various fields of study. This survey focuses on deep learning\nsolutions that target learning control policies for robotics applications. We\ncarry out our discussions on the two main paradigms for learning control with\ndeep networks: deep reinforcement learning and imitation learning. For deep\nreinforcement learning (DRL), we begin from traditional reinforcement learning\nalgorithms, showing how they are extended to the deep context and effective\nmechanisms that could be added on top of the DRL algorithms. We then introduce\nrepresentative works that utilize DRL to solve navigation and manipulation\ntasks in robotics. We continue our discussion on methods addressing the\nchallenge of the reality gap for transferring DRL policies trained in\nsimulation to real-world scenarios, and summarize robotics simulation platforms\nfor conducting DRL research. For imitation leaning, we go through its three\nmain categories, behavior cloning, inverse reinforcement learning and\ngenerative adversarial imitation learning, by introducing their formulations\nand their corresponding robotics applications. Finally, we discuss the open\nchallenges and research frontiers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 14:31:47 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 09:20:29 GMT"}, {"version": "v3", "created": "Sun, 1 Jan 2017 09:47:49 GMT"}, {"version": "v4", "created": "Mon, 9 Apr 2018 03:46:53 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Tai", "Lei", ""], ["Zhang", "Jingwei", ""], ["Liu", "Ming", ""], ["Boedecker", "Joschka", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1612.07294", "submitter": "Aleksander Lodwich", "authors": "Aleksander Lodwich", "title": "Understanding Error Correction and its Role as Part of the Communication\n  Channel in Environments composed of Self-Integrating Systems", "comments": "60 pages, 55 figures, gray literature", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The raise of complexity of technical systems also raises knowledge required\nto set them up and to maintain them. The cost to evolve such systems can be\nprohibitive. In the field of Autonomic Computing, technical systems should\ntherefore have various self-healing capabilities allowing system owners to\nprovide only partial, potentially inconsistent updates of the system. The\nself-healing or self-integrating system shall find out the remaining changes to\ncommunications and functionalities in order to accommodate change and yet still\nrestore function. This issue becomes even more interesting in context of\nInternet of Things and Industrial Internet where previously unexpected device\ncombinations can be assembled in order to provide a surprising new function. In\norder to pursue higher levels of self-integration capabilities I propose to\nthink of self-integration as sophisticated error correcting communications.\nTherefore, this paper discusses an extended scope of error correction with the\npurpose to emphasize error correction's role as an integrated element of\nbi-directional communication channels in self-integrating, autonomic\ncommunication scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 19:45:13 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Lodwich", "Aleksander", ""]]}, {"id": "1612.07512", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na, Marcus Bendtsen", "title": "Causal Effect Identification in Acyclic Directed Mixed Graphs and Gated\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of graphical models that consists of graphs with\npossibly directed, undirected and bidirected edges but without directed cycles.\nWe show that these models are suitable for representing causal models with\nadditive error terms. We provide a set of sufficient graphical criteria for the\nidentification of arbitrary causal effects when the new models contain directed\nand undirected edges but no bidirected edge. We also provide a necessary and\nsufficient graphical criterion for the identification of the causal effect of a\nsingle variable on the rest of the variables. Moreover, we develop an exact\nalgorithm for learning the new models from observational and interventional\ndata via answer set programming. Finally, we introduce gated models for causal\neffect identification, a new family of graphical models that exploits context\nspecific independences to identify additional causal effects.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 10:00:38 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 12:44:27 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""], ["Bendtsen", "Marcus", ""]]}, {"id": "1612.07548", "submitter": "Wendelin B\\\"ohmer", "authors": "Wendelin B\\\"ohmer and Rong Guo and Klaus Obermayer", "title": "Non-Deterministic Policy Improvement Stabilizes Approximated\n  Reinforcement Learning", "comments": "This paper has been presented at the 13th European Workshop on\n  Reinforcement Learning (EWRL 2016) on the 3rd and 4th of December 2016 in\n  Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a type of instability that is linked to the greedy\npolicy improvement in approximated reinforcement learning. We show empirically\nthat non-deterministic policy improvement can stabilize methods like LSPI by\ncontrolling the improvements' stochasticity. Additionally we show that a\nsuitable representation of the value function also stabilizes the solution to\nsome degree. The presented approach is simple and should also be easily\ntransferable to more sophisticated algorithms like deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 11:30:35 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["B\u00f6hmer", "Wendelin", ""], ["Guo", "Rong", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1612.07555", "submitter": "J. G. Wolff", "authors": "J Gerard Wolff", "title": "The SP Theory of Intelligence as a Foundation for the Development of a\n  General, Human-Level Thinking Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarises how the \"SP theory of intelligence\" and its realisation\nin the \"SP computer model\" simplifies and integrates concepts across artificial\nintelligence and related areas, and thus provides a promising foundation for\nthe development of a general, human-level thinking machine, in accordance with\nthe main goal of research in artificial general intelligence.\n  The key to this simplification and integration is the powerful concept of\n\"multiple alignment\", borrowed and adapted from bioinformatics. This concept\nhas the potential to be the \"double helix\" of intelligence, with as much\nsignificance for human-level intelligence as has DNA for biological sciences.\n  Strengths of the SP system include: versatility in the representation of\ndiverse kinds of knowledge; versatility in aspects of intelligence (including:\nstrengths in unsupervised learning; the processing of natural language; pattern\nrecognition at multiple levels of abstraction that is robust in the face of\nerrors in data; several kinds of reasoning (including: one-step `deductive'\nreasoning; chains of reasoning; abductive reasoning; reasoning with\nprobabilistic networks and trees; reasoning with 'rules'; nonmonotonic\nreasoning and reasoning with default values; Bayesian reasoning with\n'explaining away'; and more); planning; problem solving; and more); seamless\nintegration of diverse kinds of knowledge and diverse aspects of intelligence\nin any combination; and potential for application in several areas (including:\nhelping to solve nine problems with big data; helping to develop human-level\nintelligence in autonomous robots; serving as a database with intelligence and\nwith versatility in the representation and integration of several forms of\nknowledge; serving as a vehicle for medical knowledge and as an aid to medical\ndiagnosis; and several more).\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 11:50:47 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Wolff", "J Gerard", ""]]}, {"id": "1612.07589", "submitter": "Wolfgang Faber", "authors": "Wolfgang Faber, Mauro Vallati, Federico Cerutti, Massimiliano Giacomin", "title": "Solving Set Optimization Problems by Cardinality Optimization via Weak\n  Constraints with an Application to Argumentation", "comments": "Informal proceedings of the 1st Workshop on Trends and Applications\n  of Answer Set Programming (TAASP 2016), Klagenfurt, Austria, 26 September\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization - minimization or maximization - in the lattice of subsets is a\nfrequent operation in Artificial Intelligence tasks. Examples are\nsubset-minimal model-based diagnosis, nonmonotonic reasoning by means of\ncircumscription, or preferred extensions in abstract argumentation. Finding the\noptimum among many admissible solutions is often harder than finding admissible\nsolutions with respect to both computational complexity and methodology. This\npaper addresses the former issue by means of an effective method for finding\nsubset-optimal solutions. It is based on the relationship between\ncardinality-optimal and subset-optimal solutions, and the fact that many\nlogic-based declarative programming systems provide constructs for finding\ncardinality-optimal solutions, for example maximum satisfiability (MaxSAT) or\nweak constraints in Answer Set Programming (ASP). Clearly each\ncardinality-optimal solution is also a subset-optimal one, and if the language\nalso allows for the addition of particular restricting constructs (both MaxSAT\nand ASP do) then all subset-optimal solutions can be found by an iterative\ncomputation of cardinality-optimal solutions. As a showcase, the computation of\npreferred extensions of abstract argumentation frameworks using the proposed\nmethod is studied.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 13:20:02 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Faber", "Wolfgang", ""], ["Vallati", "Mauro", ""], ["Cerutti", "Federico", ""], ["Giacomin", "Massimiliano", ""]]}, {"id": "1612.07601", "submitter": "Johannes Klaus Fichte", "authors": "Johannes Fichte and Markus Hecher and Michael Morak and Stefan Woltran", "title": "Counting Answer Sets via Dynamic Programming", "comments": "Informal proceedings of the 1st Workshop on Trends and Applications\n  of Answer Set Programming (TAASP 2016), Klagenfurt, Austria, 26 September\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the solution counting problem for propositional satisfiability (#SAT)\nhas received renewed attention in recent years, this research trend has not\naffected other AI solving paradigms like answer set programming (ASP). Although\nASP solvers are designed to enumerate all solutions, and counting can therefore\nbe easily done, the involved materialization of all solutions is a clear\nbottleneck for the counting problem of ASP (#ASP). In this paper we propose\ndynamic programming-based #ASP algorithms that exploit the structure of the\nunderlying (ground) ASP program. Experimental results for a prototype\nimplementation show promise when compared to existing solvers.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 14:03:21 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Fichte", "Johannes", ""], ["Hecher", "Markus", ""], ["Morak", "Michael", ""], ["Woltran", "Stefan", ""]]}, {"id": "1612.07602", "submitter": "Hai Ye", "authors": "Hai Ye, Wenhan Chao, Zhunchen Luo, Zhoujun Li", "title": "Jointly Extracting Relations with Class Ties via Effective Deep Ranking", "comments": "To appear in ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connections between relations in relation extraction, which we call class\nties, are common. In distantly supervised scenario, one entity tuple may have\nmultiple relation facts. Exploiting class ties between relations of one entity\ntuple will be promising for distantly supervised relation extraction. However,\nprevious models are not effective or ignore to model this property. In this\nwork, to effectively leverage class ties, we propose to make joint relation\nextraction with a unified model that integrates convolutional neural network\n(CNN) with a general pairwise ranking framework, in which three novel ranking\nloss functions are introduced. Additionally, an effective method is presented\nto relieve the severe class imbalance problem from NR (not relation) for model\ntraining. Experiments on a widely used dataset show that leveraging class ties\nwill enhance extraction and demonstrate the effectiveness of our model to learn\nclass ties. Our model outperforms the baselines significantly, achieving\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 14:08:08 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 11:15:55 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 08:21:37 GMT"}, {"version": "v4", "created": "Sat, 5 Aug 2017 12:03:18 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Ye", "Hai", ""], ["Chao", "Wenhan", ""], ["Luo", "Zhunchen", ""], ["Li", "Zhoujun", ""]]}, {"id": "1612.07603", "submitter": "Wenji Li", "authors": "Zhun Fan, Wenji Li, Xinye Cai, Hui Li, Caimin Wei, Qingfu Zhang,\n  Kalyanmoy Deb and Erik D. Goodman", "title": "Difficulty Adjustable and Scalable Constrained Multi-objective Test\n  Problem Toolkit", "comments": "28 pages,8 figures, 7 tables", "journal-ref": null, "doi": "10.1162/evco_a_00259", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-objective evolutionary algorithms (MOEAs) have progressed significantly\nin recent decades, but most of them are designed to solve unconstrained\nmulti-objective optimization problems. In fact, many real-world multi-objective\nproblems contain a number of constraints. To promote research on constrained\nmulti-objective optimization, we first propose a problem classification scheme\nwith three primary types of difficulty, which reflect various types of\nchallenges presented by real-world optimization problems, in order to\ncharacterize the constraint functions in constrained multi-objective\noptimization problems (CMOPs). These are feasibility-hardness,\nconvergence-hardness and diversity-hardness. We then develop a general toolkit\nto construct difficulty-adjustable and scalable CMOPs (DAS-CMOPs, or DAS-CMaOPs\nwhen the number of objectives is greater than three) with three types of\nparameterized constraint functions developed to capture the three proposed\ntypes of difficulty. Based on this toolkit, we suggest nine\ndifficulty-adjustable and scalable CMOPs and nine CMaOPs. The experimental\nresults reveal that mechanisms in MOEA/D-CDP may be more effective in solving\nconvergence-hard DAS-CMOPs, while mechanisms of NSGA-II-CDP may be more\neffective in solving DAS-CMOPs with simultaneous diversity-, feasibility- and\nconvergence-hardness. Mechanisms in C-NSGA-III may be more effective in solving\nfeasibility-hard CMaOPs, while mechanisms of C-MOEA/DD may be more effective in\nsolving CMaOPs with convergence-hardness. In addition, none of them can solve\nthese problems efficiently, which stimulates us to continue to develop new\nCMOEAs and CMaOEAs to solve the suggested DAS-CMOPs and DAS-CMaOPs.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 14:36:29 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 07:09:08 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 08:46:23 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Fan", "Zhun", ""], ["Li", "Wenji", ""], ["Cai", "Xinye", ""], ["Li", "Hui", ""], ["Wei", "Caimin", ""], ["Zhang", "Qingfu", ""], ["Deb", "Kalyanmoy", ""], ["Goodman", "Erik D.", ""]]}, {"id": "1612.07771", "submitter": "Klaus Greff", "authors": "Klaus Greff and Rupesh K. Srivastava and J\\\"urgen Schmidhuber", "title": "Highway and Residual Networks learn Unrolled Iterative Estimation", "comments": "10 + 4 pages, accepted for ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The past year saw the introduction of new architectures such as Highway\nnetworks and Residual networks which, for the first time, enabled the training\nof feedforward networks with dozens to hundreds of layers using simple gradient\ndescent. While depth of representation has been posited as a primary reason for\ntheir success, there are indications that these architectures defy a popular\nview of deep learning as a hierarchical computation of increasingly abstract\nfeatures at each layer.\n  In this report, we argue that this view is incomplete and does not adequately\nexplain several recent findings. We propose an alternative viewpoint based on\nunrolled iterative estimation -- a group of successive layers iteratively\nrefine their estimates of the same features instead of computing an entirely\nnew representation. We demonstrate that this viewpoint directly leads to the\nconstruction of Highway and Residual networks. Finally we provide preliminary\nexperiments to discuss the similarities and differences between the two\narchitectures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 19:57:35 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 19:52:47 GMT"}, {"version": "v3", "created": "Tue, 14 Mar 2017 21:27:03 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Greff", "Klaus", ""], ["Srivastava", "Rupesh K.", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1612.07837", "submitter": "Soroush Mehri", "authors": "Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham\n  Jain, Jose Sotelo, Aaron Courville and Yoshua Bengio", "title": "SampleRNN: An Unconditional End-to-End Neural Audio Generation Model", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel model for unconditional audio generation\nbased on generating one audio sample at a time. We show that our model, which\nprofits from combining memory-less modules, namely autoregressive multilayer\nperceptrons, and stateful recurrent neural networks in a hierarchical structure\nis able to capture underlying sources of variations in the temporal sequences\nover very long time spans, on three datasets of different nature. Human\nevaluation on the generated samples indicate that our model is preferred over\ncompeting models. We also show how each component of the model contributes to\nthe exhibited performance.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 23:28:47 GMT"}, {"version": "v2", "created": "Sat, 11 Feb 2017 20:04:46 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Mehri", "Soroush", ""], ["Kumar", "Kundan", ""], ["Gulrajani", "Ishaan", ""], ["Kumar", "Rithesh", ""], ["Jain", "Shubham", ""], ["Sotelo", "Jose", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1612.07896", "submitter": "Christopher Burges", "authors": "C.J.C. Burges, T. Hart, Z. Yang, S. Cucerzan, R.W. White, A.\n  Pastusiak, J. Lewis", "title": "A Base Camp for Scaling AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern statistical machine learning (SML) methods share a major limitation\nwith the early approaches to AI: there is no scalable way to adapt them to new\ndomains. Human learning solves this in part by leveraging a rich, shared,\nupdateable world model. Such scalability requires modularity: updating part of\nthe world model should not impact unrelated parts. We have argued that such\nmodularity will require both \"correctability\" (so that errors can be corrected\nwithout introducing new errors) and \"interpretability\" (so that we can\nunderstand what components need correcting).\n  To achieve this, one could attempt to adapt state of the art SML systems to\nbe interpretable and correctable; or one could see how far the simplest\npossible interpretable, correctable learning methods can take us, and try to\ncontrol the limitations of SML methods by applying them only where needed. Here\nwe focus on the latter approach and we investigate two main ideas: \"Teacher\nAssisted Learning\", which leverages crowd sourcing to learn language; and\n\"Factored Dialog Learning\", which factors the process of application\ndevelopment into roles where the language competencies needed are isolated,\nenabling non-experts to quickly create new applications.\n  We test these ideas in an \"Automated Personal Assistant\" (APA) setting, with\ntwo scenarios: that of detecting user intent from a user-APA dialog; and that\nof creating a class of event reminder applications, where a non-expert\n\"teacher\" can then create specific apps. For the intent detection task, we use\na dataset of a thousand labeled utterances from user dialogs with Cortana, and\nwe show that our approach matches state of the art SML methods, but in addition\nprovides full transparency: the whole (editable) model can be summarized on one\nhuman-readable page. For the reminder app task, we ran small user studies to\nverify the efficacy of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 08:03:20 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Burges", "C. J. C.", ""], ["Hart", "T.", ""], ["Yang", "Z.", ""], ["Cucerzan", "S.", ""], ["White", "R. W.", ""], ["Pastusiak", "A.", ""], ["Lewis", "J.", ""]]}, {"id": "1612.07919", "submitter": "Mehdi S. M. Sajjadi", "authors": "Mehdi S. M. Sajjadi and Bernhard Sch\\\"olkopf and Michael Hirsch", "title": "EnhanceNet: Single Image Super-Resolution Through Automated Texture\n  Synthesis", "comments": "main paper and supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution is the task of inferring a high-resolution\nimage from a single low-resolution input. Traditionally, the performance of\nalgorithms for this task is measured using pixel-wise reconstruction measures\nsuch as peak signal-to-noise ratio (PSNR) which have been shown to correlate\npoorly with the human perception of image quality. As a result, algorithms\nminimizing these metrics tend to produce over-smoothed images that lack\nhigh-frequency textures and do not look natural despite yielding high PSNR\nvalues.\n  We propose a novel application of automated texture synthesis in combination\nwith a perceptual loss focusing on creating realistic textures rather than\noptimizing for a pixel-accurate reproduction of ground truth images during\ntraining. By using feed-forward fully convolutional neural networks in an\nadversarial training setting, we achieve a significant boost in image quality\nat high magnification ratios. Extensive experiments on a number of datasets\nshow the effectiveness of our approach, yielding state-of-the-art results in\nboth quantitative and qualitative benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 10:16:26 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 21:52:23 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Sajjadi", "Mehdi S. M.", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Hirsch", "Michael", ""]]}, {"id": "1612.08048", "submitter": "Davide Grossi", "authors": "Zo\\'e Christoff and Davide Grossi", "title": "Liquid Democracy: An Analysis in Binary Aggregation and Diffusion", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes an analysis of liquid democracy (or, delegable proxy\nvoting) from the perspective of binary aggregation and of binary diffusion\nmodels. We show how liquid democracy on binary issues can be embedded into the\nframework of binary aggregation with abstentions, enabling the transfer of\nknown results about the latter---such as impossibility theorems---to the\nformer. This embedding also sheds light on the relation between delegation\ncycles in liquid democracy and the probability of collective abstentions, as\nwell as the issue of individual rationality in a delegable proxy voting\nsetting. We then show how liquid democracy on binary issues can be modeled and\nanalyzed also as a specific process of dynamics of binary opinions on networks.\nThese processes---called Boolean DeGroot processes---are a special case of the\nDeGroot stochastic model of opinion diffusion. We establish the convergence\nconditions of such processes and show they provide some novel insights on how\nthe effects of delegation cycles and individual rationality could be mitigated\nwithin liquid democracy.\n  The study is a first attempt to provide theoretical foundations to the\ndelgable proxy features of the liquid democracy voting system. Our analysis\nsuggests recommendations on how the system may be modified to make it more\nresilient with respect to the handling of delegation cycles and of inconsistent\nmajorities.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 17:36:22 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 16:03:42 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Christoff", "Zo\u00e9", ""], ["Grossi", "Davide", ""]]}, {"id": "1612.08109", "submitter": "Ashish Mani Dr.", "authors": "Nija Mani, Gursaran, and Ashish Mani", "title": "Solving Combinatorial Optimization problems with Quantum inspired\n  Evolutionary Algorithm Tuned using a Novel Heuristic Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum inspired Evolutionary Algorithms were proposed more than a decade ago\nand have been employed for solving a wide range of difficult search and\noptimization problems. A number of changes have been proposed to improve\nperformance of canonical QEA. However, canonical QEA is one of the few\nevolutionary algorithms, which uses a search operator with relatively large\nnumber of parameters. It is well known that performance of evolutionary\nalgorithms is dependent on specific value of parameters for a given problem.\nThe advantage of having large number of parameters in an operator is that the\nsearch process can be made more powerful even with a single operator without\nrequiring a combination of other operators for exploration and exploitation.\nHowever, the tuning of operators with large number of parameters is complex and\ncomputationally expensive. This paper proposes a novel heuristic method for\ntuning parameters of canonical QEA. The tuned QEA outperforms canonical QEA on\na class of discrete combinatorial optimization problems which, validates the\ndesign of the proposed parameter tuning framework. The proposed framework can\nbe used for tuning other algorithms with both large and small number of tunable\nparameters.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 22:51:09 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 07:05:42 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Mani", "Nija", ""], ["Gursaran", "", ""], ["Mani", "Ashish", ""]]}, {"id": "1612.08544", "submitter": "Anuj Karpatne", "authors": "Anuj Karpatne, Gowtham Atluri, James Faghmous, Michael Steinbach,\n  Arindam Banerjee, Auroop Ganguly, Shashi Shekhar, Nagiza Samatova, and Vipin\n  Kumar", "title": "Theory-guided Data Science: A New Paradigm for Scientific Discovery from\n  Data", "comments": null, "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, 29(10),\n  pp.2318-2331. 2017", "doi": "10.1109/TKDE.2017.2720168", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science models, although successful in a number of commercial domains,\nhave had limited applicability in scientific problems involving complex\nphysical phenomena. Theory-guided data science (TGDS) is an emerging paradigm\nthat aims to leverage the wealth of scientific knowledge for improving the\neffectiveness of data science models in enabling scientific discovery. The\noverarching vision of TGDS is to introduce scientific consistency as an\nessential component for learning generalizable models. Further, by producing\nscientifically interpretable models, TGDS aims to advance our scientific\nunderstanding by discovering novel domain insights. Indeed, the paradigm of\nTGDS has started to gain prominence in a number of scientific disciplines such\nas turbulence modeling, material discovery, quantum chemistry, bio-medical\nscience, bio-marker discovery, climate science, and hydrology. In this paper,\nwe formally conceptualize the paradigm of TGDS and present a taxonomy of\nresearch themes in TGDS. We describe several approaches for integrating domain\nknowledge in different research themes using illustrative examples from\ndifferent disciplines. We also highlight some of the promising avenues of novel\nresearch for realizing the full potential of theory-guided data science.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 09:14:16 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 17:42:12 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Karpatne", "Anuj", ""], ["Atluri", "Gowtham", ""], ["Faghmous", "James", ""], ["Steinbach", "Michael", ""], ["Banerjee", "Arindam", ""], ["Ganguly", "Auroop", ""], ["Shekhar", "Shashi", ""], ["Samatova", "Nagiza", ""], ["Kumar", "Vipin", ""]]}, {"id": "1612.08555", "submitter": "Samuel L. Smith", "authors": "Samuel L Smith", "title": "Monte Carlo Sort for unreliable human comparisons", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms which sort lists of real numbers into ascending order have been\nstudied for decades. They are typically based on a series of pairwise\ncomparisons and run entirely on chip. However people routinely sort lists which\ndepend on subjective or complex judgements that cannot be automated. Examples\ninclude marketing research; where surveys are used to learn about customer\npreferences for products, the recruiting process; where interviewers attempt to\nrank potential employees, and sporting tournaments; where we infer team\nrankings from a series of one on one matches. We develop a novel sorting\nalgorithm, where each pairwise comparison reflects a subjective human judgement\nabout which element is bigger or better. We introduce a finite and large error\nrate to each judgement, and we take the cost of each comparison to\nsignificantly exceed the cost of other computational steps. The algorithm must\nrequest the most informative sequence of comparisons from the user; in order to\nidentify the correct sorted list with minimum human input. Our Discrete\nAdiabatic Monte Carlo approach exploits the gradual acquisition of information\nby tracking a set of plausible hypotheses which are updated after each\nadditional comparison.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 09:54:07 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Smith", "Samuel L", ""]]}, {"id": "1612.08633", "submitter": "Vishal Kakkar", "authors": "Vishal Kakkar, Shirish K. Shevade, S Sundararajan, Dinesh Garg", "title": "A Sparse Nonlinear Classifier Design Using AUC Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AUC (Area under the ROC curve) is an important performance measure for\napplications where the data is highly imbalanced. Learning to maximize AUC\nperformance is thus an important research problem. Using a max-margin based\nsurrogate loss function, AUC optimization problem can be approximated as a\npairwise rankSVM learning problem. Batch learning methods for solving the\nkernelized version of this problem suffer from scalability and may not result\nin sparse classifiers. Recent years have witnessed an increased interest in the\ndevelopment of online or single-pass online learning algorithms that design a\nclassifier by maximizing the AUC performance. The AUC performance of nonlinear\nclassifiers, designed using online methods, is not comparable with that of\nnonlinear classifiers designed using batch learning algorithms on many\nreal-world datasets. Motivated by these observations, we design a scalable\nalgorithm for maximizing AUC performance by greedily adding the required number\nof basis functions into the classifier model. The resulting sparse classifiers\nperform faster inference. Our experimental results show that the level of\nsparsity achievable can be order of magnitude smaller than the Kernel RankSVM\nmodel without affecting the AUC performance much.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 13:52:56 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Kakkar", "Vishal", ""], ["Shevade", "Shirish K.", ""], ["Sundararajan", "S", ""], ["Garg", "Dinesh", ""]]}, {"id": "1612.08657", "submitter": "Olivier Auber", "authors": "Antoine Saillenfest, Jean-Louis Dessalles, Olivier Auber", "title": "Role of Simplicity in Creative Behaviour: The Case of the Poietic\n  Generator", "comments": "This study was supported by grants from the programme Futur&Ruptures\n  and from the 'Chaire Modelisation des Imaginaires, Innovation et Creation',\n  http://www.computationalcreativity.net/iccc2016/posters-and-demos/", "journal-ref": "Proceedings of the Seventh International Conference on\n  Computational Creativity (ICCC-2016). Paris, France", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose to apply Simplicity Theory (ST) to model interest in creative\nsituations. ST has been designed to describe and predict interest in\ncommunication. Here we use ST to derive a decision rule that we apply to a\nsimplified version of a creative game, the Poietic Generator. The decision rule\nproduces what can be regarded as an elementary form of creativity. This study\nis meant as a proof of principle. It suggests that some creative actions may be\nmotivated by the search for unexpected simplicity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 12:56:07 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Saillenfest", "Antoine", ""], ["Dessalles", "Jean-Louis", ""], ["Auber", "Olivier", ""]]}, {"id": "1612.08777", "submitter": "Joshua Friedman", "authors": "Joshua S. Friedman", "title": "Automated timetabling for small colleges and high schools using huge\n  integer programs", "comments": "Errors corrected from version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate an integer program to solve a highly constrained academic\ntimetabling problem at the United States Merchant Marine Academy. The IP\ninstance that results from our real case study has approximately both 170,000\nrows and columns and solves to optimality in 4--24 hours using a commercial\nsolver on a portable computer (near optimal feasible solutions were often found\nin 4--12 hours). Our model is applicable to both high schools and small\ncolleges who wish to deviate from group scheduling. We also solve a necessary\npreprocessing student subgrouping problem, which breaks up big groups of\nstudents into small groups so they can optimally fit into small capacity\nclasses.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 00:50:16 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 18:24:48 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Friedman", "Joshua S.", ""]]}, {"id": "1612.08810", "submitter": "Hado van Hasselt", "authors": "David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur\n  Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz,\n  Andre Barreto, Thomas Degris", "title": "The Predictron: End-To-End Learning and Planning", "comments": "Camera-ready version, ICML 2017, with supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges of artificial intelligence is to learn models that\nare effective in the context of planning. In this document we introduce the\npredictron architecture. The predictron consists of a fully abstract model,\nrepresented by a Markov reward process, that can be rolled forward multiple\n\"imagined\" planning steps. Each forward pass of the predictron accumulates\ninternal rewards and values over multiple planning depths. The predictron is\ntrained end-to-end so as to make these accumulated values accurately\napproximate the true value function. We applied the predictron to procedurally\ngenerated random mazes and a simulator for the game of pool. The predictron\nyielded significantly more accurate predictions than conventional deep neural\nnetwork architectures.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 06:47:15 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 14:57:31 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 09:21:54 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Silver", "David", ""], ["van Hasselt", "Hado", ""], ["Hessel", "Matteo", ""], ["Schaul", "Tom", ""], ["Guez", "Arthur", ""], ["Harley", "Tim", ""], ["Dulac-Arnold", "Gabriel", ""], ["Reichert", "David", ""], ["Rabinowitz", "Neil", ""], ["Barreto", "Andre", ""], ["Degris", "Thomas", ""]]}, {"id": "1612.08825", "submitter": "Alexander Amini", "authors": "Alexander Amini, Berthold Horn, Alan Edelman", "title": "Accelerated Convolutions for Efficient Multi-Scale Time to Contact\n  Computation in Julia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutions have long been regarded as fundamental to applied mathematics,\nphysics and engineering. Their mathematical elegance allows for common tasks\nsuch as numerical differentiation to be computed efficiently on large data\nsets. Efficient computation of convolutions is critical to artificial\nintelligence in real-time applications, like machine vision, where convolutions\nmust be continuously and efficiently computed on tens to hundreds of kilobytes\nper second. In this paper, we explore how convolutions are used in fundamental\nmachine vision applications. We present an accelerated n-dimensional\nconvolution package in the high performance computing language, Julia, and\ndemonstrate its efficacy in solving the time to contact problem for machine\nvision. Results are measured against synthetically generated videos and\nquantitatively assessed according to their mean squared error from the ground\ntruth. We achieve over an order of magnitude decrease in compute time and\nallocated memory for comparable machine vision applications. All code is\npackaged and integrated into the official Julia Package Manager to be used in\nvarious other scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 08:46:21 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Amini", "Alexander", ""], ["Horn", "Berthold", ""], ["Edelman", "Alan", ""]]}, {"id": "1612.08843", "submitter": "Hexiang Hu", "authors": "Hexiang Hu, Shiyi Lan, Yuning Jiang, Zhimin Cao, Fei Sha", "title": "FastMask: Segment Multi-scale Object Candidates in One Shot", "comments": "Accepted as CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects appear to scale differently in natural images. This fact requires\nmethods dealing with object-centric tasks (e.g. object proposal) to have robust\nperformance over variances in object scales. In the paper, we present a novel\nsegment proposal framework, namely FastMask, which takes advantage of\nhierarchical features in deep convolutional neural networks to segment\nmulti-scale objects in one shot. Innovatively, we adapt segment proposal\nnetwork into three different functional components (body, neck and head). We\nfurther propose a weight-shared residual neck module as well as a\nscale-tolerant attentional head module for efficient one-shot inference. On MS\nCOCO benchmark, the proposed FastMask outperforms all state-of-the-art segment\nproposal methods in average recall being 2~5 times faster. Moreover, with a\nslight trade-off in accuracy, FastMask can segment objects in near real time\n(~13 fps) with 800*600 resolution images, demonstrating its potential in\npractical applications. Our implementation is available on\nhttps://github.com/voidrank/FastMask.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 10:24:42 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 08:03:28 GMT"}, {"version": "v3", "created": "Mon, 16 Jan 2017 06:46:35 GMT"}, {"version": "v4", "created": "Tue, 11 Apr 2017 21:20:57 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Hu", "Hexiang", ""], ["Lan", "Shiyi", ""], ["Jiang", "Yuning", ""], ["Cao", "Zhimin", ""], ["Sha", "Fei", ""]]}, {"id": "1612.08845", "submitter": "Toni Heidenreich", "authors": "Toni Heidenreich", "title": "The formal-logical characterisation of lies, deception, and associated\n  notions", "comments": "Literature review prepared as a student at King's College London", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defining various dishonest notions in a formal way is a key step to enable\nintelligent agents to act in untrustworthy environments. This review evaluates\nthe literature for this topic by looking at formal definitions based on modal\nlogic as well as other formal approaches. Criteria from philosophical\ngroundwork is used to assess the definitions for correctness and completeness.\nThe key contribution of this review is to show that only a few definitions\nfully comply with this gold standard and to point out the missing steps towards\na successful application of these definitions in an actual agent environment.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 10:35:05 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Heidenreich", "Toni", ""]]}, {"id": "1612.08967", "submitter": "Nicolas Le Roux", "authors": "Nicolas Le Roux", "title": "Efficient iterative policy optimization", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the issue of finding a good policy when the number of policy\nupdates is limited. This is done by approximating the expected policy reward as\na sequence of concave lower bounds which can be efficiently maximized,\ndrastically reducing the number of policy updates required to achieve good\nperformance. We also extend existing methods to negative rewards, enabling the\nuse of control variates.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 19:53:08 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Roux", "Nicolas Le", ""]]}, {"id": "1612.09030", "submitter": "Vikas Garg", "authors": "Vikas K. Garg and Adam Tauman Kalai", "title": "Meta-Unsupervised-Learning: A supervised approach to unsupervised\n  learning", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new paradigm to investigate unsupervised learning, reducing\nunsupervised learning to supervised learning. Specifically, we mitigate the\nsubjectivity in unsupervised decision-making by leveraging knowledge acquired\nfrom prior, possibly heterogeneous, supervised learning tasks. We demonstrate\nthe versatility of our framework via comprehensive expositions and detailed\nexperiments on several unsupervised problems such as (a) clustering, (b)\noutlier detection, and (c) similarity prediction under a common umbrella of\nmeta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds to\nestablish the theoretical foundations of our framework, and show that our\nframing of meta-clustering circumvents Kleinberg's impossibility theorem for\nclustering.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 03:20:33 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 17:34:39 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Garg", "Vikas K.", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "1612.09134", "submitter": "Antonio Manuel Lopez Pe\\~na", "authors": "Antonio M. Lopez, Jiaolong Xu, Jose L. Gomez, David Vazquez, German\n  Ros", "title": "From Virtual to Real World Visual Perception using Domain Adaptation --\n  The DPM as Example", "comments": "Invited book chapter to appear in \"Domain Adaptation in Computer\n  Vision Applications\", Springer Series: Advances in Computer Vision and\n  Pattern Recognition, Edited by Gabriela Csurka", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning tends to produce more accurate classifiers than\nunsupervised learning in general. This implies that training data is preferred\nwith annotations. When addressing visual perception challenges, such as\nlocalizing certain object classes within an image, the learning of the involved\nclassifiers turns out to be a practical bottleneck. The reason is that, at\nleast, we have to frame object examples with bounding boxes in thousands of\nimages. A priori, the more complex the model is regarding its number of\nparameters, the more annotated examples are required. This annotation task is\nperformed by human oracles, which ends up in inaccuracies and errors in the\nannotations (aka ground truth) since the task is inherently very cumbersome and\nsometimes ambiguous. As an alternative we have pioneered the use of virtual\nworlds for collecting such annotations automatically and with high precision.\nHowever, since the models learned with virtual data must operate in the real\nworld, we still need to perform domain adaptation (DA). In this chapter we\nrevisit the DA of a deformable part-based model (DPM) as an exemplifying case\nof virtual- to-real-world DA. As a use case, we address the challenge of\nvehicle detection for driver assistance, using different publicly available\nvirtual-world data. While doing so, we investigate questions such as: how does\nthe domain gap behave due to virtual-vs-real data with respect to dominant\nobject appearance per domain, as well as the role of photo-realism in the\nvirtual world.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 13:16:22 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Lopez", "Antonio M.", ""], ["Xu", "Jiaolong", ""], ["Gomez", "Jose L.", ""], ["Vazquez", "David", ""], ["Ros", "German", ""]]}, {"id": "1612.09205", "submitter": "Tamas Madl", "authors": "Tamas Madl", "title": "Deep neural heart rate variability analysis", "comments": "6 pages in NIPS 2016 Workshop on Machine Learning for Health (ML4HC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite of the pain and limited accuracy of blood tests for early recognition\nof cardiovascular disease, they dominate risk screening and triage. On the\nother hand, heart rate variability is non-invasive and cheap, but not\nconsidered accurate enough for clinical practice. Here, we tackle heart beat\ninterval based classification with deep learning. We introduce an end to end\ndifferentiable hybrid architecture, consisting of a layer of biological neuron\nmodels of cardiac dynamics (modified FitzHugh Nagumo neurons) and several\nlayers of a standard feed-forward neural network. The proposed model is\nevaluated on ECGs from 474 stable at-risk (coronary artery disease) patients,\nand 1172 chest pain patients of an emergency department. We show that it can\nsignificantly outperform models based on traditional heart rate variability\npredictors, as well as approaching or in some cases outperforming clinical\nblood tests, based only on 60 seconds of inter-beat intervals.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 17:14:05 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Madl", "Tamas", ""]]}, {"id": "1612.09212", "submitter": "Rouven Bauer", "authors": "Rouven Bauer", "title": "A hybrid approach to supervised machine learning for algorithmic melody\n  composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present an algorithm for composing monophonic melodies\nsimilar in style to those of a given, phrase annotated, sample of melodies. For\nimplementation, a hybrid approach incorporating parametric Markov models of\nhigher order and a contour concept of phrases is used. This work is based on\nthe master thesis of Thayabaran Kathiresan (2015). An online listening test\nconducted shows that enhancing a pure Markov model with musically relevant\ncontext, like count and planed melody contour, improves the result\nsignificantly.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 17:36:05 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Bauer", "Rouven", ""]]}, {"id": "1612.09251", "submitter": "Evgenia (Eugenia) Ternovska", "authors": "Eugenia Ternovska", "title": "Lifted Relational Algebra with Recursion and Connections to Modal Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new formalism for specifying and reasoning about problems that\ninvolve heterogeneous \"pieces of information\" -- large collections of data,\ndecision procedures of any kind and complexity and connections between them.\nThe essence of our proposal is to lift Codd's relational algebra from\noperations on relational tables to operations on classes of structures (with\nrecursion), and to add a direction of information propagation. We observe the\npresence of information propagation in several formalisms for efficient\nreasoning and use it to express unary negation and operations used in graph\ndatabases. We carefully analyze several reasoning tasks and establish a precise\nconnection between a generalized query evaluation and temporal logic model\nchecking. Our development allows us to reveal a general correspondence between\nclassical and modal logics and may shed a new light on the good computational\nproperties of modal logics and related formalisms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 19:17:31 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Ternovska", "Eugenia", ""]]}, {"id": "1612.09327", "submitter": "Moonish Maknojia", "authors": "Ahlam Ansari, Moonish Maknojia and Altamash Shaikh", "title": "Intelligent information extraction based on artificial neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering System (QAS) is used for information retrieval and natural\nlanguage processing (NLP) to reduce human effort. There are numerous QAS based\non the user documents present today, but they all are limited to providing\nobjective answers and process simple questions only. Complex questions cannot\nbe answered by the existing QAS, as they require interpretation of the current\nand old data as well as the question asked by the user. The above limitations\ncan be overcome by using deep cases and neural network. Hence we propose a\nmodified QAS in which we create a deep artificial neural network with\nassociative memory from text documents. The modified QAS processes the contents\nof the text document provided to it and find the answer to even complex\nquestions in the documents.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 06:09:36 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Ansari", "Ahlam", ""], ["Maknojia", "Moonish", ""], ["Shaikh", "Altamash", ""]]}, {"id": "1612.09433", "submitter": "Cedric Buron", "authors": "C\\'edric Buron (LIP6), Sylvain Ductor (LIP6), Zahia Guessoum (LIP6,\n  CRESTIC)", "title": "Curiosity-Aware Bargaining", "comments": null, "journal-ref": "STAIRS 2016, Aug 2016, The Hague, Netherlands. IOS Press, 284, pp.\n  27 -- 38, Frontiers in Artificial Intelligence and Applications", "doi": "10.3233/978-1-61499-682-8-27", "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opponent modeling consists in modeling the strategy or preferences of an\nagent thanks to the data it provides. In the context of automated negotiation\nand with machine learning, it can result in an advantage so overwhelming that\nit may restrain some casual agents to be part of the bargaining process. We\nqualify as \"curious\" an agent driven by the desire of negotiating in order to\ncollect information and improve its opponent model. However, neither\ncuriosity-based rational-ity nor curiosity-robust protocol have been studied in\nautomatic negotiation. In this paper, we rely on mechanism design to propose\nthree extensions of the standard bargaining protocol that limit information\nleak. Those extensions are supported by an enhanced rationality model, that\nconsiders the exchanged information. Also, they are theoretically analyzed and\nexperimentally evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 09:31:17 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Buron", "C\u00e9dric", "", "LIP6"], ["Ductor", "Sylvain", "", "LIP6"], ["Guessoum", "Zahia", "", "LIP6,\n  CRESTIC"]]}, {"id": "1612.09465", "submitter": "Hugo Penedones", "authors": "Timothy A. Mann and Hugo Penedones and Shie Mannor and Todd Hester", "title": "Adaptive Lambda Least-Squares Temporal Difference Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Difference learning or TD($\\lambda$) is a fundamental algorithm in\nthe field of reinforcement learning. However, setting TD's $\\lambda$ parameter,\nwhich controls the timescale of TD updates, is generally left up to the\npractitioner. We formalize the $\\lambda$ selection problem as a bias-variance\ntrade-off where the solution is the value of $\\lambda$ that leads to the\nsmallest Mean Squared Value Error (MSVE). To solve this trade-off we suggest\napplying Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to search the\nspace of $\\lambda$ values. Unfortunately, this approach is too computationally\nexpensive for most practical applications. For Least Squares TD (LSTD) we show\nthat LOTO-CV can be implemented efficiently to automatically tune $\\lambda$ and\napply function optimization methods to efficiently search the space of\n$\\lambda$ values. The resulting algorithm, ALLSTD, is parameter free and our\nexperiments demonstrate that ALLSTD is significantly computationally faster\nthan the na\\\"{i}ve LOTO-CV implementation while achieving similar performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 11:51:14 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Mann", "Timothy A.", ""], ["Penedones", "Hugo", ""], ["Mannor", "Shie", ""], ["Hester", "Todd", ""]]}, {"id": "1612.09542", "submitter": "Licheng Yu", "authors": "Licheng Yu, Hao Tan, Mohit Bansal, Tamara L. Berg", "title": "A Joint Speaker-Listener-Reinforcer Model for Referring Expressions", "comments": "Some typo fixed; comprehension results on refcocog updated; more\n  human evaluation results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expressions are natural language constructions used to identify\nparticular objects within a scene. In this paper, we propose a unified\nframework for the tasks of referring expression comprehension and generation.\nOur model is composed of three modules: speaker, listener, and reinforcer. The\nspeaker generates referring expressions, the listener comprehends referring\nexpressions, and the reinforcer introduces a reward function to guide sampling\nof more discriminative expressions. The listener-speaker modules are trained\njointly in an end-to-end learning framework, allowing the modules to be aware\nof one another during learning while also benefiting from the discriminative\nreinforcer's feedback. We demonstrate that this unified framework and training\nachieves state-of-the-art results for both comprehension and generation on\nthree referring expression datasets. Project and demo page:\nhttps://vision.cs.unc.edu/refer\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 17:39:19 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 20:13:49 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Yu", "Licheng", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1612.09591", "submitter": "Matthias Nickles", "authors": "Matthias Nickles", "title": "PrASP Report", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report describes the usage, syntax, semantics and core\nalgorithms of the probabilistic inductive logic programming framework PrASP.\nPrASP is a research software which integrates non-monotonic reasoning based on\nAnswer Set Programming (ASP), probabilistic inference and parameter learning.\nIn contrast to traditional approaches to Probabilistic (Inductive) Logic\nProgramming, our framework imposes only little restrictions on probabilistic\nlogic programs. In particular, PrASP allows for ASP as well as First-Order\nLogic syntax, and for the annotation of formulas with point probabilities as\nwell as interval probabilities. A range of widely configurable inference\nalgorithms can be combined in a pipeline-like fashion, in order to cover a\nvariety of use cases.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 20:45:28 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Nickles", "Matthias", ""]]}, {"id": "1612.09592", "submitter": "Erik Hoel", "authors": "Erik P Hoel", "title": "When the map is better than the territory", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": "10.3390/e19050188", "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal structure of any system can be analyzed at a multitude of spatial\nand temporal scales. It has long been thought that while higher scale (macro)\ndescriptions of causal structure may be useful to observers, they are at best a\ncompressed description and at worse leave out critical information. However,\nrecent research applying information theory to causal analysis has shown that\nthe causal structure of some systems can actually come into focus (be more\ninformative) at a macroscale (Hoel et al. 2013). That is, a macro model of a\nsystem (a map) can be more informative than a fully detailed model of the\nsystem (the territory). This has been called causal emergence. While causal\nemergence may at first glance seem counterintuitive, this paper grounds the\nphenomenon in a classic concept from information theory: Shannon's discovery of\nthe channel capacity. I argue that systems have a particular causal capacity,\nand that different causal models of those systems take advantage of that\ncapacity to various degrees. For some systems, only macroscale causal models\nuse the full causal capacity. Such macroscale causal models can either be\ncoarse-grains, or may leave variables and states out of the model (exogenous)\nin various ways, which can improve the model's efficacy and its informativeness\nvia the same mathematical principles of how error-correcting codes take\nadvantage of an information channel's capacity. As model choice increase, the\ncausal capacity of a system approaches the channel capacity. Ultimately, this\nprovides a general framework for understanding how the causal structure of some\nsystems cannot be fully captured by even the most detailed microscopic model.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 20:46:22 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Hoel", "Erik P", ""]]}, {"id": "1612.09593", "submitter": "Hamid Reza Hassanzadeh", "authors": "Hamid Reza Hassanzadeh, Hadi Sadoghi Yazdi, Abedin Vahedian", "title": "Fuzzy Constraints Linear Discriminant Analysis", "comments": null, "journal-ref": "3rd Iranian Joint Congress on Intelligent Systems and Fuzzy\n  Systems, 2009", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a fuzzy constraint linear discriminant analysis\n(FC-LDA). The FC-LDA tries to minimize misclassification error based on\nmodified perceptron criterion that benefits handling the uncertainty near the\ndecision boundary by means of a fuzzy linear programming approach with fuzzy\nresources. The method proposed has low computational complexity because of its\nlinear characteristics and the ability to deal with noisy data with different\ndegrees of tolerance. Obtained results verify the success of the algorithm when\ndealing with different problems. Comparing FC-LDA and LDA shows superiority in\nclassification task.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 20:48:33 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Hassanzadeh", "Hamid Reza", ""], ["Yazdi", "Hadi Sadoghi", ""], ["Vahedian", "Abedin", ""]]}]