[{"id": "1310.0576", "submitter": "Christian Retor\\'e", "authors": "Roberto Bonato and Christian Retor\\'e", "title": "Learning Lambek grammars from proof frames", "comments": "A revised version will appear in a volume in honour of Lambek 90th\n  birthday", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to their limpid interface with semantics, categorial grammars\nenjoy another important property: learnability. This was first noticed by\nBuskowsky and Penn and further studied by Kanazawa, for Bar-Hillel categorial\ngrammars.\n  What about Lambek categorial grammars? In a previous paper we showed that\nproduct free Lambek grammars where learnable from structured sentences, the\nstructures being incomplete natural deductions. These grammars were shown to be\nunlearnable from strings by Foret and Le Nir. In the present paper we show that\nLambek grammars, possibly with product, are learnable from proof frames that\nare incomplete proof nets.\n  After a short reminder on grammatical inference \\`a la Gold, we provide an\nalgorithm that learns Lambek grammars with product from proof frames and we\nprove its convergence. We do so for 1-valued also known as rigid Lambek\ngrammars with product, since standard techniques can extend our result to\n$k$-valued grammars. Because of the correspondence between cut-free proof nets\nand normal natural deductions, our initial result on product free Lambek\ngrammars can be recovered.\n  We are sad to dedicate the present paper to Philippe Darondeau, with whom we\nstarted to study such questions in Rennes at the beginning of the millennium,\nand who passed away prematurely.\n  We are glad to dedicate the present paper to Jim Lambek for his 90 birthday:\nhe is the living proof that research is an eternal learning process.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 06:06:02 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Bonato", "Roberto", ""], ["Retor\u00e9", "Christian", ""]]}, {"id": "1310.0602", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger", "title": "Iterated Variable Neighborhood Search for the resource constrained\n  multi-mode multi-project scheduling problem", "comments": null, "journal-ref": "In: Graham Kendall, Greet Vanden Berghe, and Barry McCollum\n  (editors): Proceedings of the 6th Multidisciplinary International Conference\n  on Scheduling: Theory and Applications, August 27-29, 2013, Gent, Belgium,\n  pages 807-811", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resource constrained multi-mode multi-project scheduling problem\n(RCMMMPSP) is a notoriously difficult combinatorial optimization problem. For a\ngiven set of activities, feasible execution mode assignments and execution\nstarting times must be found such that some optimization function, e.g. the\nmakespan, is optimized. When determining an optimal (or at least feasible)\nassignment of decision variable values, a set of side constraints, such as\nresource availabilities, precedence constraints, etc., has to be respected.\n  In 2013, the MISTA 2013 Challenge stipulated research in the RCMMMPSP. It's\ngoal was the solution of a given set of instances under running time\nrestrictions. We have contributed to this challenge with the here presented\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 07:18:34 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Geiger", "Martin Josef", ""]]}, {"id": "1310.0927", "submitter": "Jussi Rintanen", "authors": "Jukka Corander, Tomi Janhunen, Jussi Rintanen, Henrik Nyman, Johan\n  Pensar", "title": "Learning Chordal Markov Networks by Constraint Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of learning the structure of a Markov network from\ndata. It is shown that the structure of such networks can be described in terms\nof constraints which enables the use of existing solver technology with\noptimization capabilities to compute optimal networks starting from initial\nscores computed from the data. To achieve efficient encodings, we develop a\nnovel characterization of Markov network structure using a balancing condition\non the separators between cliques forming the network. The resulting\ntranslations into propositional satisfiability and its extensions such as\nmaximum satisfiability, satisfiability modulo theories, and answer set\nprogramming, enable us to prove optimal certain network structures which have\nbeen previously found by stochastic search.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 09:01:39 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Corander", "Jukka", ""], ["Janhunen", "Tomi", ""], ["Rintanen", "Jussi", ""], ["Nyman", "Henrik", ""], ["Pensar", "Johan", ""]]}, {"id": "1310.0967", "submitter": "Marco Bardoscia", "authors": "Marco Bardoscia, Daniel Nagaj, Antonello Scardicchio", "title": "The SAT-UNSAT transition in the adversarial SAT problem", "comments": "13 pages, 8 figures", "journal-ref": "Phys. Rev. E 89, 032128 (2014)", "doi": "10.1103/PhysRevE.89.032128", "report-no": null, "categories": "cs.CC cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial SAT (AdSAT) is a generalization of the satisfiability (SAT)\nproblem in which two players try to make a boolean formula true (resp. false)\nby controlling their respective sets of variables. AdSAT belongs to a higher\ncomplexity class in the polynomial hierarchy than SAT and therefore the nature\nof the critical region and the transition are not easily paralleled to those of\nSAT and worth of independent study. AdSAT also provides an upper bound for the\ntransition threshold of the quantum satisfiability problem (QSAT). We present a\ncomplete algorithm for AdSAT, show that 2-AdSAT is in $\\mathbf{P}$, and then\nstudy two stochastic algorithms (simulated annealing and its improved variant)\nand compare their performances in detail for 3-AdSAT. Varying the density of\nclauses $\\alpha$ we find a sharp SAT-UNSAT transition at a critical value whose\nupper bound is $\\alpha_c \\lesssim 1.5$, thus providing a much stricter upper\nbound for the QSAT transition than those previously found.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 13:03:20 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2013 05:28:38 GMT"}, {"version": "v3", "created": "Fri, 7 Mar 2014 19:07:59 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Bardoscia", "Marco", ""], ["Nagaj", "Daniel", ""], ["Scardicchio", "Antonello", ""]]}, {"id": "1310.1137", "submitter": "Jeremiah Blocki", "authors": "Jeremiah Blocki and Manuel Blum and Anupam Datta", "title": "GOTCHA Password Hackers!", "comments": "2013 ACM Workshop on Artificial Intelligence and Security (AISec)", "journal-ref": null, "doi": "10.1145/2517312.2517319", "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GOTCHAs (Generating panOptic Turing Tests to Tell Computers and\nHumans Apart) as a way of preventing automated offline dictionary attacks\nagainst user selected passwords. A GOTCHA is a randomized puzzle generation\nprotocol, which involves interaction between a computer and a human.\nInformally, a GOTCHA should satisfy two key properties: (1) The puzzles are\neasy for the human to solve. (2) The puzzles are hard for a computer to solve\neven if it has the random bits used by the computer to generate the final\npuzzle --- unlike a CAPTCHA. Our main theorem demonstrates that GOTCHAs can be\nused to mitigate the threat of offline dictionary attacks against passwords by\nensuring that a password cracker must receive constant feedback from a human\nbeing while mounting an attack. Finally, we provide a candidate construction of\nGOTCHAs based on Inkblot images. Our construction relies on the usability\nassumption that users can recognize the phrases that they originally used to\ndescribe each Inkblot image --- a much weaker usability assumption than\nprevious password systems based on Inkblots which required users to recall\ntheir phrase exactly. We conduct a user study to evaluate the usability of our\nGOTCHA construction. We also generate a GOTCHA challenge where we encourage\nartificial intelligence and security researchers to try to crack several\npasswords protected with our scheme.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 00:29:48 GMT"}], "update_date": "2013-10-07", "authors_parsed": [["Blocki", "Jeremiah", ""], ["Blum", "Manuel", ""], ["Datta", "Anupam", ""]]}, {"id": "1310.1187", "submitter": "Johan Pensar", "authors": "Johan Pensar, Henrik Nyman, Timo Koski and Jukka Corander", "title": "Labeled Directed Acyclic Graphs: a generalization of context-specific\n  independence in directed graphical models", "comments": "26 pages, 17 figures", "journal-ref": null, "doi": "10.1007/s10618-014-0355-0", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel class of labeled directed acyclic graph (LDAG) models\nfor finite sets of discrete variables. LDAGs generalize earlier proposals for\nallowing local structures in the conditional probability distribution of a\nnode, such that unrestricted label sets determine which edges can be deleted\nfrom the underlying directed acyclic graph (DAG) for a given context. Several\nproperties of these models are derived, including a generalization of the\nconcept of Markov equivalence classes. Efficient Bayesian learning of LDAGs is\nenabled by introducing an LDAG-based factorization of the Dirichlet prior for\nthe model parameters, such that the marginal likelihood can be calculated\nanalytically. In addition, we develop a novel prior distribution for the model\nstructures that can appropriately penalize a model for its labeling complexity.\nA non-reversible Markov chain Monte Carlo algorithm combined with a greedy hill\nclimbing approach is used for illustrating the useful properties of LDAG models\nfor both real and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 07:29:08 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Pensar", "Johan", ""], ["Nyman", "Henrik", ""], ["Koski", "Timo", ""], ["Corander", "Jukka", ""]]}, {"id": "1310.1328", "submitter": "Ernest Davis", "authors": "Ernest Davis", "title": "The Relevance of Proofs of the Rationality of Probability Theory to\n  Automated Reasoning and Cognitive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of well-known theorems, such as Cox's theorem and de Finetti's\ntheorem. prove that any model of reasoning with uncertain information that\nsatisfies specified conditions of \"rationality\" must satisfy the axioms of\nprobability theory. I argue here that these theorems do not in themselves\ndemonstrate that probabilistic models are in fact suitable for any specific\ntask in automated reasoning or plausible for cognitive models. First, the\ntheorems only establish that there exists some probabilistic model; they do not\nestablish that there exists a useful probabilistic model, i.e. one with a\ntractably small number of numerical parameters and a large number of\nindependence assumptions. Second, there are in general many different\nprobabilistic models for a given situation, many of which may be far more\nirrational, in the usual sense of the term, than a model that violates the\naxioms of probability theory. I illustrate this second point with an extended\nexamples of two tasks of induction, of a similar structure, where the\nreasonable probabilistic models are very different.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 16:04:08 GMT"}], "update_date": "2013-10-07", "authors_parsed": [["Davis", "Ernest", ""]]}, {"id": "1310.1537", "submitter": "Alireza Mahani", "authors": "Alireza S. Mahani, Mansour T.A. Sharabiani", "title": "SIMD Parallel MCMC Sampling with Applications for Big-Data Bayesian\n  Analytics", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2015.02.010", "report-no": null, "categories": "stat.CO cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational intensity and sequential nature of estimation techniques for\nBayesian methods in statistics and machine learning, combined with their\nincreasing applications for big data analytics, necessitate both the\nidentification of potential opportunities to parallelize techniques such as\nMCMC sampling, and the development of general strategies for mapping such\nparallel algorithms to modern CPUs in order to elicit the performance up the\ncompute-based and/or memory-based hardware limits. Two opportunities for\nSingle-Instruction Multiple-Data (SIMD) parallelization of MCMC sampling for\nprobabilistic graphical models are presented. In exchangeable models with many\nobservations such as Bayesian Generalized Linear Models, child-node\ncontributions to the conditional posterior of each node can be calculated\nconcurrently. In undirected graphs with discrete nodes, concurrent sampling of\nconditionally-independent nodes can be transformed into a SIMD form.\nHigh-performance libraries with multi-threading and vectorization capabilities\ncan be readily applied to such SIMD opportunities to gain decent speedup, while\na series of high-level source-code and runtime modifications provide further\nperformance boost by reducing parallelization overhead and increasing data\nlocality for NUMA architectures. For big-data Bayesian GLM graphs, the\nend-result is a routine for evaluating the conditional posterior and its\ngradient vector that is 5 times faster than a naive implementation using\n(built-in) multi-threaded Intel MKL BLAS, and reaches within the striking\ndistance of the memory-bandwidth-induced hardware limit. The proposed\noptimization strategies improve the scaling of performance with number of cores\nand width of vector units (applicable to many-core SIMD processors such as\nIntel Xeon Phi and GPUs), resulting in cost-effectiveness, energy efficiency,\nand higher speed on multi-core x86 processors.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2013 04:02:35 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 22:40:39 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Mahani", "Alireza S.", ""], ["Sharabiani", "Mansour T. A.", ""]]}, {"id": "1310.1597", "submitter": "Mengqiu Wang", "authors": "Mengqiu Wang and Christopher D. Manning", "title": "Cross-lingual Pseudo-Projected Expectation Regularization for Weakly\n  Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multilingual weakly supervised learning scenario where\nknowledge from annotated corpora in a resource-rich language is transferred via\nbitext to guide the learning in other languages. Past approaches project labels\nacross bitext and use them as features or gold labels for training. We propose\na new method that projects model expectations rather than labels, which\nfacilities transfer of model uncertainty across language boundaries. We encode\nexpectations as constraints and train a discriminative CRF model using\nGeneralized Expectation Criteria (Mann and McCallum, 2010). Evaluated on\nstandard Chinese-English and German-English NER datasets, our method\ndemonstrates F1 scores of 64% and 60% when no labeled data is used. Attaining\nthe same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences.\nFurthermore, when combined with labeled examples, our method yields significant\nimprovements over state-of-the-art supervised methods, achieving best reported\nnumbers to date on Chinese OntoNotes and German CoNLL-03 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2013 16:34:30 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Wang", "Mengqiu", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1310.1863", "submitter": "Christoph Salge", "authors": "Christoph Salge, Cornelius Glackin and Daniel Polani", "title": "Empowerment -- an Introduction", "comments": "46 pages, 8 figures, to be published in Prokopenko, M., editor,\n  Guided Self-Organization: Inception. Springer. In Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This book chapter is an introduction to and an overview of the\ninformation-theoretic, task independent utility function \"Empowerment\", which\nis defined as the channel capacity between an agent's actions and an agent's\nsensors. It quantifies how much influence and control an agent has over the\nworld it can perceive. This book chapter discusses the general idea behind\nempowerment as an intrinsic motivation and showcases several previous\napplications of empowerment to demonstrate how empowerment can be applied to\ndifferent sensor-motor configuration, and how the same formalism can lead to\ndifferent observed behaviors. Furthermore, we also present a fast approximation\nfor empowerment in the continuous domain.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 17:15:36 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 09:10:58 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Salge", "Christoph", ""], ["Glackin", "Cornelius", ""], ["Polani", "Daniel", ""]]}, {"id": "1310.1947", "submitter": "Frank Hutter", "authors": "Frank Hutter and Holger Hoos and Kevin Leyton-Brown", "title": "Bayesian Optimization With Censored Response Data", "comments": "Extended version of NIPS 2011 workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) aims to minimize a given blackbox function using a\nmodel that is updated whenever new evidence about the function becomes\navailable. Here, we address the problem of BO under partially right-censored\nresponse data, where in some evaluations we only obtain a lower bound on the\nfunction value. The ability to handle such response data allows us to\nadaptively censor costly function evaluations in minimization problems where\nthe cost of a function evaluation corresponds to the function value. One\nimportant application giving rise to such censored data is the\nruntime-minimizing variant of the algorithm configuration problem: finding\nsettings of a given parametric algorithm that minimize the runtime required for\nsolving problem instances from a given distribution. We demonstrate that\nterminating slow algorithm runs prematurely and handling the resulting\nright-censored observations can substantially improve the state of the art in\nmodel-based algorithm configuration.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 20:43:16 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Hutter", "Frank", ""], ["Hoos", "Holger", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "1310.2089", "submitter": "Mir Mohammad Ettefagh", "authors": "Habib Emdadi, Mahsa Yazdanian, Mir Mohammad Ettefagh and Mohammad-Reza\n  Feizi-Derakhshi", "title": "Double four-bar crank-slider mechanism dynamic balancing by\n  meta-heuristic algorithms", "comments": "18 pages-19 figures", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol. 4, No. 5, September 2013", "doi": "10.5121/ijaia.2013.4501", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new method for dynamic balancing of double four-bar crank\nslider mechanism by meta- heuristic-based optimization algorithms is proposed.\nFor this purpose, a proper objective function which is necessary for balancing\nof this mechanism and corresponding constraints has been obtained by dynamic\nmodeling of the mechanism. Then PSO, ABC, BGA and HGAPSO algorithms have been\napplied for minimizing the defined cost function in optimization step. The\noptimization results have been studied completely by extracting the cost\nfunction, fitness, convergence speed and runtime values of applied algorithms.\nIt has been shown that PSO and ABC are more efficient than BGA and HGAPSO in\nterms of convergence speed and result quality. Also, a laboratory scale\nexperimental doublefour-bar crank-slider mechanism was provided for validating\nthe proposed balancing method practically.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 10:47:32 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Emdadi", "Habib", ""], ["Yazdanian", "Mahsa", ""], ["Ettefagh", "Mir Mohammad", ""], ["Feizi-Derakhshi", "Mohammad-Reza", ""]]}, {"id": "1310.2098", "submitter": "Xinyang Deng", "authors": "Xinyang Deng, Yong Deng", "title": "A short note on the axiomatic requirements of uncertainty measure", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we argue that the axiomatic requirement of range to the measure\nof aggregated total uncertainty (ATU) in Dempster-Shafer theory is not\nreasonable.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 11:17:25 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Deng", "Xinyang", ""], ["Deng", "Yong", ""]]}, {"id": "1310.2298", "submitter": "Anton Belov", "authors": "Anton Belov and Antonio Morgado and Joao Marques-Silva", "title": "SAT-based Preprocessing for MaxSAT (extended version)", "comments": "Extended version of LPAR'19 paper with the same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art algorithms for industrial instances of MaxSAT problem rely\non iterative calls to a SAT solver. Preprocessing is crucial for the\nacceleration of SAT solving, and the key preprocessing techniques rely on the\napplication of resolution and subsumption elimination. Additionally,\nsatisfiability-preserving clause elimination procedures are often used. Since\nMaxSAT computation typically involves a large number of SAT calls, we are\ninterested in whether an input instance to a MaxSAT problem can be preprocessed\nup-front, i.e. prior to running the MaxSAT solver, rather than (or, in addition\nto) during each iterative SAT solver call. The key requirement in this setting\nis that the preprocessing has to be sound, i.e. so that the solution can be\nreconstructed correctly and efficiently after the execution of a MaxSAT\nalgorithm on the preprocessed instance. While, as we demonstrate in this paper,\ncertain clause elimination procedures are sound for MaxSAT, it is well-known\nthat this is not the case for resolution and subsumption elimination. In this\npaper we show how to adapt these preprocessing techniques to MaxSAT. To achieve\nthis we recast the MaxSAT problem in a recently introduced labelled-CNF\nframework, and show that within the framework the preprocessing techniques can\nbe applied soundly. Furthermore, we show that MaxSAT algorithms restated in the\nframework have a natural implementation on top of an incremental SAT solver. We\nevaluate the prototype implementation of a MaxSAT algorithm WMSU1 in this\nsetting, demonstrate the effectiveness of preprocessing, and show overall\nimprovement with respect to non-incremental versions of the algorithm on some\nclasses of problems.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 22:33:38 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2013 09:15:07 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Belov", "Anton", ""], ["Morgado", "Antonio", ""], ["Marques-Silva", "Joao", ""]]}, {"id": "1310.2350", "submitter": "Cm Pintea", "authors": "Camelia-M. Pintea, Petrica C. Pop, Camelia Chira", "title": "The Generalized Traveling Salesman Problem solved with Ant Algorithms", "comments": "indexed in Scopus, ORCID", "journal-ref": "Complex Adaptive Systems Modeling 5(1), 8 pages, 2017", "doi": "10.1186/s40294-017-0048-9", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well known N P-hard problem called the Generalized Traveling Salesman\nProblem (GTSP) is considered. In GTSP the nodes of a complete undirected graph\nare partitioned into clusters. The objective is to find a minimum cost tour\npassing through exactly one node from each cluster. An exact exponential time\nalgorithm and an effective meta-heuristic algorithm for the problem are\npresented. The meta-heuristic proposed is a modified Ant Colony System (ACS)\nalgorithm called Reinforcing Ant Colony System (RACS) which introduces new\ncorrection rules in the ACS algorithm. Computational results are reported for\nmany standard test problems. The proposed algorithm is competitive with the\nother already proposed heuristics for the GTSP in both solution quality and\ncomputational time.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 05:17:16 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Pintea", "Camelia-M.", ""], ["Pop", "Petrica C.", ""], ["Chira", "Camelia", ""]]}, {"id": "1310.2396", "submitter": "Hua Yao", "authors": "Hua Yao, William Zhu", "title": "A necessary and sufficient condition for two relations to induce the\n  same definable set family", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Pawlak rough sets, the structure of the definable set families is simple\nand clear, but in generalizing rough sets, the structure of the definable set\nfamilies is a bit more complex. There has been much research work focusing on\nthis topic. However, as a fundamental issue in relation based rough sets, under\nwhat condition two relations induce the same definable set family has not been\ndiscussed. In this paper, based on the concept of the closure of relations, we\npresent a necessary and sufficient condition for two relations to induce the\nsame definable set family.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 08:46:01 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Yao", "Hua", ""], ["Zhu", "William", ""]]}, {"id": "1310.2493", "submitter": "George Vouros VOUROS GEORGE", "authors": "George A. Vouros and Georgios Santipantakis", "title": "Combining Ontologies with Correspondences and Link Relations: The E-SHIQ\n  Representation Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining knowledge and beliefs of autonomous peers in distributed settings,\nis a ma- jor challenge. In this paper we consider peers that combine ontologies\nand reason jointly with their coupled knowledge. Ontologies are within the SHIQ\nfragment of Description Logics. Although there are several representation\nframeworks for modular Description Log- ics, each one makes crucial assumptions\nconcerning the subjectivity of peers' knowledge, the relation between the\ndomains over which ontologies are interpreted, the expressivity of the\nconstructors used for combining knowledge, and the way peers share their\nknowledge. However in settings where autonomous peers can evolve and extend\ntheir knowledge and beliefs independently from others, these assumptions may\nnot hold. In this article, we moti- vate the need for a representation\nframework that allows peers to combine their knowledge in various ways,\nmaintaining the subjectivity of their own knowledge and beliefs, and that\nreason collaboratively, constructing a tableau that is distributed among them,\njointly. The paper presents the proposed E-SHIQ representation framework, the\nimplementation of the E-SHIQ distributed tableau reasoner, and discusses the\nefficiency of this reasoner.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 14:26:23 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Vouros", "George A.", ""], ["Santipantakis", "Georgios", ""]]}, {"id": "1310.2627", "submitter": "Dani Yogatama", "authors": "Dani Yogatama and Bryan R. Routledge and Noah A. Smith", "title": "A Sparse and Adaptive Prior for Time-Dependent Model Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the scenario where the parameters of a probabilistic model are\nexpected to vary over time. We construct a novel prior distribution that\npromotes sparsity and adapts the strength of correlation between parameters at\nsuccessive timesteps, based on the data. We derive approximate variational\ninference procedures for learning and prediction with this prior. We test the\napproach on two tasks: forecasting financial quantities from relevant text, and\nmodeling language contingent on time-varying financial measurements.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 20:39:08 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 05:11:48 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Yogatama", "Dani", ""], ["Routledge", "Bryan R.", ""], ["Smith", "Noah A.", ""]]}, {"id": "1310.2743", "submitter": "Valmi Dufour-Lussier", "authors": "Valmi Dufour-Lussier (INRIA Nancy - Grand Est / LORIA), Florence Le\n  Ber (LHyGeS), Jean Lieber (INRIA Nancy - Grand Est / LORIA), Laura Martin\n  (ASTER Mirecourt)", "title": "Case Adaptation with Qualitative Algebras", "comments": null, "journal-ref": "International Joint Conferences on Artificial Intelligence (2013)\n  3002-3006", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach for the adaptation of spatial or temporal\ncases in a case-based reasoning system. Qualitative algebras are used as\nspatial and temporal knowledge representation languages. The intuition behind\nthis adaptation approach is to apply a substitution and then repair potential\ninconsistencies, thanks to belief revision on qualitative algebras. A temporal\nexample from the cooking domain is given. (The paper on which this extended\nabstract is based was the recipient of the best paper award of the 2012\nInternational Conference on Case-Based Reasoning.)\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 09:28:20 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Dufour-Lussier", "Valmi", "", "INRIA Nancy - Grand Est / LORIA"], ["Ber", "Florence Le", "", "LHyGeS"], ["Lieber", "Jean", "", "INRIA Nancy - Grand Est / LORIA"], ["Martin", "Laura", "", "ASTER Mirecourt"]]}, {"id": "1310.2797", "submitter": "Josef Urban", "authors": "Cezary Kaliszyk and Josef Urban", "title": "Lemma Mining over HOL Light", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large formal mathematical libraries consist of millions of atomic inference\nsteps that give rise to a corresponding number of proved statements (lemmas).\nAnalogously to the informal mathematical practice, only a tiny fraction of such\nstatements is named and re-used in later proofs by formal mathematicians. In\nthis work, we suggest and implement criteria defining the estimated usefulness\nof the HOL Light lemmas for proving further theorems. We use these criteria to\nmine the large inference graph of all lemmas in the core HOL Light library,\nadding thousands of the best lemmas to the pool of named statements that can be\nre-used in later proofs. The usefulness of the new lemmas is then evaluated by\ncomparing the performance of automated proving of the core HOL Light theorems\nwith and without such added lemmas.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 12:53:04 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""]]}, {"id": "1310.2805", "submitter": "Josef Urban", "authors": "Cezary Kaliszyk and Josef Urban", "title": "MizAR 40 for Mizar 40", "comments": null, "journal-ref": "J. Automated Reasoning 55(3): 245-256 (2015)", "doi": "10.1007/s10817-015-9330-8", "report-no": null, "categories": "cs.AI cs.DL cs.LG cs.LO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a present to Mizar on its 40th anniversary, we develop an AI/ATP system\nthat in 30 seconds of real time on a 14-CPU machine automatically proves 40% of\nthe theorems in the latest official version of the Mizar Mathematical Library\n(MML). This is a considerable improvement over previous performance of large-\ntheory AI/ATP methods measured on the whole MML. To achieve that, a large suite\nof AI/ATP methods is employed and further developed. We implement the most\nuseful methods efficiently, to scale them to the 150000 formulas in MML. This\nreduces the training times over the corpus to 1-3 seconds, allowing a simple\npractical deployment of the methods in the online automated reasoning service\nfor the Mizar users (MizAR).\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 13:24:07 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""]]}, {"id": "1310.2955", "submitter": "Marc Pickett", "authors": "Marc Pickett and David W. Aha", "title": "Spontaneous Analogy by Piggybacking on a Perceptual System", "comments": "Proceedings of the 35th Meeting of the Cognitive Science Society,\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most computational models of analogy assume they are given a delineated\nsource domain and often a specified target domain. These systems do not address\nhow analogs can be isolated from large domains and spontaneously retrieved from\nlong-term memory, a process we call spontaneous analogy. We present a system\nthat represents relational structures as feature bags. Using this\nrepresentation, our system leverages perceptual algorithms to automatically\ncreate an ontology of relational structures and to efficiently retrieve analogs\nfor new relational structures from long-term memory. We provide a demonstration\nof our approach that takes a set of unsegmented stories, constructs an ontology\nof analogical schemas (corresponding to plot devices), and uses this ontology\nto efficiently find analogs within new stories, yielding significant\ntime-savings over linear analog retrieval at a small accuracy cost.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 20:22:33 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Pickett", "Marc", ""], ["Aha", "David W.", ""]]}, {"id": "1310.3174", "submitter": "Manuel Lopes", "authors": "Benjamin Clement, Didier Roy, Pierre-Yves Oudeyer, Manuel Lopes", "title": "Multi-Armed Bandits for Intelligent Tutoring Systems", "comments": null, "journal-ref": "Journal of Educational Data Mining, 7(2), 20-48 (2015)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to Intelligent Tutoring Systems which adaptively\npersonalizes sequences of learning activities to maximize skills acquired by\nstudents, taking into account the limited time and motivational resources. At a\ngiven point in time, the system proposes to the students the activity which\nmakes them progress faster. We introduce two algorithms that rely on the\nempirical estimation of the learning progress, RiARiT that uses information\nabout the difficulty of each exercise and ZPDES that uses much less knowledge\nabout the problem.\n  The system is based on the combination of three approaches. First, it\nleverages recent models of intrinsically motivated learning by transposing them\nto active teaching, relying on empirical estimation of learning progress\nprovided by specific activities to particular students. Second, it uses\nstate-of-the-art Multi-Arm Bandit (MAB) techniques to efficiently manage the\nexploration/exploitation challenge of this optimization process. Third, it\nleverages expert knowledge to constrain and bootstrap initial exploration of\nthe MAB, while requiring only coarse guidance information of the expert and\nallowing the system to deal with didactic gaps in its knowledge. The system is\nevaluated in a scenario where 7-8 year old schoolchildren learn how to\ndecompose numbers while manipulating money. Systematic experiments are\npresented with simulated students, followed by results of a user study across a\npopulation of 400 school children.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 15:47:41 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 21:38:13 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Clement", "Benjamin", ""], ["Roy", "Didier", ""], ["Oudeyer", "Pierre-Yves", ""], ["Lopes", "Manuel", ""]]}, {"id": "1310.3225", "submitter": "Seth Lloyd", "authors": "Seth Lloyd", "title": "A Turing test for free will", "comments": "20 pages, plain TeX", "journal-ref": "Phil. Trans. Roy. Soc. A 28, 3597-3610 (2012)", "doi": "10.1098/rsta.2011.0331", "report-no": null, "categories": "quant-ph cs.AI physics.hist-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Before Alan Turing made his crucial contributions to the theory of\ncomputation, he studied the question of whether quantum mechanics could throw\nlight on the nature of free will. This article investigates the roles of\nquantum mechanics and computation in free will. Although quantum mechanics\nimplies that events are intrinsically unpredictable, the `pure stochasticity'\nof quantum mechanics adds only randomness to decision making processes, not\nfreedom. By contrast, the theory of computation implies that even when our\ndecisions arise from a completely deterministic decision-making process, the\noutcomes of that process can be intrinsically unpredictable, even to --\nespecially to -- ourselves. I argue that this intrinsic computational\nunpredictability of the decision making process is what give rise to our\nimpression that we possess free will. Finally, I propose a `Turing test' for\nfree will: a decision maker who passes this test will tend to believe that he,\nshe, or it possesses free will, whether the world is deterministic or not.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 18:10:20 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Lloyd", "Seth", ""]]}, {"id": "1310.3692", "submitter": "Christoph Salge", "authors": "Christoph Salge and Daniel Polani", "title": "Changing the Environment based on Intrinsic Motivation", "comments": "3 page, 1 figure, extended abstract of work presented at the Workshop\n  for \"Guided Self-Organization\" 2013 (http://prokopenko.net/gso6.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the remarkable feats of intelligent life is that it restructures the\nworld it lives in for its own benefit. This extended abstract outlines how the\ninformation-theoretic principle of empowerment, as an intrinsic motivation, can\nbe used to restructure the environment an agent lives in. We present a first\nqualitative evaluation of how an agent in a 3d-gridworld builds a\nstaircase-like structure, which reflects the agent's embodiment.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 14:21:17 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Salge", "Christoph", ""], ["Polani", "Daniel", ""]]}, {"id": "1310.3781", "submitter": "Liane Gabora", "authors": "Liane Gabora and Maryam Saberi", "title": "An Agent-based Model of the Cognitive Mechanisms Underlying the Origins\n  of Creative Cultural Evolution", "comments": "8 pages. arXiv admin note: text overlap with arXiv:1308.5032,\n  arXiv:1005.1516, arXiv:1309.7407, arXiv:0911.2390, arXiv:0811.2551,\n  arXiv:1310.0522", "journal-ref": "(2011). In A. Goel, F. Harrell, B. Magerko, & J. Prophet (Eds.),\n  Proceedings of the 8th ACM Conference on Cognition & Creativity (pp.\n  299-306). New York: Association for Computing Machinery (ACM)", "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human culture is uniquely cumulative and open-ended. Using a computational\nmodel of cultural evolution in which neural network based agents evolve ideas\nfor actions through invention and imitation, we tested the hypothesis that this\nis due to the capacity for recursive recall. We compared runs in which agents\nwere limited to single-step actions to runs in which they used recursive recall\nto chain simple actions into complex ones. Chaining resulted in higher cultural\ndiversity, open-ended generation of novelty, and no ceiling on the mean fitness\nof actions. Both chaining and no-chaining runs exhibited convergence on optimal\nactions, but without chaining this set was static while with chaining it was\never-changing. Chaining increased the ability to capitalize on the capacity for\nlearning. These findings show that the recursive recall hypothesis provides a\ncomputationally plausible explanation of why humans alone have evolved the\ncultural means to transform this planet.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 18:47:36 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 22:16:25 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Gabora", "Liane", ""], ["Saberi", "Maryam", ""]]}, {"id": "1310.4086", "submitter": "Liane Gabora", "authors": "Liane Gabora, Wei Wen Chia, and Hadi Firouzi", "title": "A Computational Model of Two Cognitive Transitions Underlying Cultural\n  Evolution", "comments": "arXiv admin note: text overlap with arXiv:1309.7407, arXiv:1308.5032,\n  arXiv:1310.3781", "journal-ref": "(2013). Proceedings of the Annual Meeting of the Cognitive Science\n  Society. July 31-3, Berlin. Austin TX: Cognitive Science Society", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tested the computational feasibility of the proposal that open-ended\ncultural evolution was made possible by two cognitive transitions: (1) onset of\nthe capacity to chain thoughts together, followed by (2) onset of contextual\nfocus (CF): the capacity to shift between a divergent mode of thought conducive\nto 'breaking out of a rut' and a convergent mode of thought conducive to minor\nmodifications. These transitions were simulated in EVOC, an agent-based model\nof cultural evolution, in which the fitness of agents' actions increases as\nagents invent ideas for new actions, and imitate the fittest of their\nneighbors' actions. Both mean fitness and diversity of actions across the\nsociety increased with chaining, and even more so with CF, as hypothesized. CF\nwas only effective when the fitness function changed, which supports its\nhypothesized role in generating and refining ideas.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 15:36:52 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Gabora", "Liane", ""], ["Chia", "Wei Wen", ""], ["Firouzi", "Hadi", ""]]}, {"id": "1310.4156", "submitter": "Hong Sun", "authors": "Hong Sun, Jos De Roo, Marc Twagirumukiza, Giovanni Mels, Kristof\n  Depraetere, Boris De Vloed, Dirk Colaert", "title": "Validation Rules for Assessing and Improving SKOS Mapping Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Simple Knowledge Organization System (SKOS) is popular for expressing\ncontrolled vocabularies, such as taxonomies, classifications, etc., for their\nuse in Semantic Web applications. Using SKOS, concepts can be linked to other\nconcepts and organized into hierarchies inside a single terminology system.\nMeanwhile, expressing mappings between concepts in different terminology\nsystems is also possible. This paper discusses potential quality issues in\nusing SKOS to express these terminology mappings. Problematic patterns are\ndefined and corresponding rules are developed to automatically detect\nsituations where the mappings either result in 'SKOS Vocabulary Hijacking' to\nthe source vocabularies or cause conflicts. An example of using the rules to\nvalidate sample mappings between two clinical terminologies is given. The\nvalidation rules, expressed in N3 format, are available as open source.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 19:28:04 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Sun", "Hong", ""], ["De Roo", "Jos", ""], ["Twagirumukiza", "Marc", ""], ["Mels", "Giovanni", ""], ["Depraetere", "Kristof", ""], ["De Vloed", "Boris", ""], ["Colaert", "Dirk", ""]]}, {"id": "1310.4342", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babuhor, SSSN Usha Devi N3", "title": "An Extensive Report on Cellular Automata Based Artificial Immune System\n  for Strengthening Automated Protein Prediction", "comments": "arXiv admin note: text overlap with arXiv:0801.4312 by other authors", "journal-ref": "Advances in Biomedical Engineering Research (ABER) Volume 1 Issue\n  3, September 2013", "doi": null, "report-no": null, "categories": "cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Immune System (AIS-MACA) a novel computational intelligence\ntechnique is can be used for strengthening the automated protein prediction\nsystem with more adaptability and incorporating more parallelism to the system.\nMost of the existing approaches are sequential which will classify the input\ninto four major classes and these are designed for similar sequences. AIS-MACA\nis designed to identify ten classes from the sequences that share twilight zone\nsimilarity and identity with the training sequences with mixed and hybrid\nvariations. This method also predicts three states (helix, strand, and coil)\nfor the secondary structure. Our comprehensive design considers 10 feature\nselection methods and 4 classifiers to develop MACA (Multiple Attractor\nCellular Automata) based classifiers that are build for each of the ten\nclasses. We have tested the proposed classifier with twilight-zone and\n1-high-similarity benchmark datasets with over three dozens of modern competing\npredictors shows that AIS-MACA provides the best overall accuracy that ranges\nbetween 80% and 89.8% depending on the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 12:14:48 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babuhor", "Inampudi Ramesh", ""], ["N3", "SSSN Usha Devi", ""]]}, {"id": "1310.4756", "submitter": "Andreas Wotzlaw", "authors": "Andreas Wotzlaw, Alexander van der Grinten, Ewald Speckenmeyer", "title": "Effectiveness of pre- and inprocessing for CDCL-based SAT solving", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying pre- and inprocessing techniques to simplify CNF formulas both\nbefore and during search can considerably improve the performance of modern SAT\nsolvers. These algorithms mostly aim at reducing the number of clauses,\nliterals, and variables in the formula. However, to be worthwhile, it is\nnecessary that their additional runtime does not exceed the runtime saved\nduring the subsequent SAT solver execution. In this paper we investigate the\nefficiency and the practicability of selected simplification algorithms for\nCDCL-based SAT solving. We first analyze them by means of their expected impact\non the CNF formula and SAT solving at all. While testing them on real-world and\ncombinatorial SAT instances, we show which techniques and combinations of them\nyield a desirable speedup and which ones should be avoided.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 15:49:32 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["Wotzlaw", "Andreas", ""], ["van der Grinten", "Alexander", ""], ["Speckenmeyer", "Ewald", ""]]}, {"id": "1310.4938", "submitter": "Andreas Wotzlaw", "authors": "Andreas Wotzlaw and Ravi Coote", "title": "A Logic-based Approach for Recognizing Textual Entailment Supported by\n  Ontological Background Knowledge", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the architecture and the evaluation of a new system for\nrecognizing textual entailment (RTE). In RTE we want to identify automatically\nthe type of a logical relation between two input texts. In particular, we are\ninterested in proving the existence of an entailment between them. We conceive\nour system as a modular environment allowing for a high-coverage syntactic and\nsemantic text analysis combined with logical inference. For the syntactic and\nsemantic analysis we combine a deep semantic analysis with a shallow one\nsupported by statistical models in order to increase the quality and the\naccuracy of results. For RTE we use logical inference of first-order employing\nmodel-theoretic techniques and automated reasoning tools. The inference is\nsupported with problem-relevant background knowledge extracted automatically\nand on demand from external sources like, e.g., WordNet, YAGO, and OpenCyc, or\nother, more experimental sources with, e.g., manually defined presupposition\nresolutions, or with axiomatized general and common sense knowledge. The\nresults show that fine-grained and consistent knowledge coming from diverse\nsources is a necessary condition determining the correctness and traceability\nof results.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 08:10:32 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Wotzlaw", "Andreas", ""], ["Coote", "Ravi", ""]]}, {"id": "1310.4986", "submitter": "Federico Cerutti", "authors": "Federico Cerutti, Paul E. Dunne, Massimiliano Giacomin, Mauro Vallati", "title": "Computing Preferred Extensions in Abstract Argumentation: a SAT-based\n  Approach", "comments": "Preprint of TAFA'13 post proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel SAT-based approach for the computation of\nextensions in abstract argumentation, with focus on preferred semantics, and an\nempirical evaluation of its performances. The approach is based on the idea of\nreducing the problem of computing complete extensions to a SAT problem and then\nusing a depth-first search method to derive preferred extensions. The proposed\napproach has been tested using two distinct SAT solvers and compared with three\nstate-of-the-art systems for preferred extension computation. It turns out that\nthe proposed approach delivers significantly better performances in the large\nmajority of the considered cases.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 12:14:31 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2013 09:53:43 GMT"}], "update_date": "2013-10-24", "authors_parsed": [["Cerutti", "Federico", ""], ["Dunne", "Paul E.", ""], ["Giacomin", "Massimiliano", ""], ["Vallati", "Mauro", ""]]}, {"id": "1310.5042", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Distributional semantics beyond words: Supervised learning of analogy\n  and paraphrase", "comments": null, "journal-ref": "Transactions of the Association for Computational Linguistics\n  (TACL), (2013), 1, 353-366", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been several efforts to extend distributional semantics beyond\nindividual words, to measure the similarity of word pairs, phrases, and\nsentences (briefly, tuples; ordered sets of words, contiguous or\nnoncontiguous). One way to extend beyond words is to compare two tuples using a\nfunction that combines pairwise similarities between the component words in the\ntuples. A strength of this approach is that it works with both relational\nsimilarity (analogy) and compositional similarity (paraphrase). However, past\nwork required hand-coding the combination function for different tasks. The\nmain contribution of this paper is that combination functions are generated by\nsupervised learning. We achieve state-of-the-art results in measuring\nrelational similarity between word pairs (SAT analogies and SemEval~2012 Task\n2) and measuring compositional similarity between noun-modifier phrases and\nunigrams (multiple-choice paraphrase questions).\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 14:50:39 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1310.5288", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Elad Gilboa, Arye Nehorai, John P. Cunningham", "title": "GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian\n  Processes", "comments": "13 Pages, 9 Figures, 1 Table. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are typically used for smoothing and interpolation on\nsmall datasets. We introduce a new Bayesian nonparametric framework -- GPatt --\nenabling automatic pattern extrapolation with Gaussian processes on large\nmultidimensional datasets. GPatt unifies and extends highly expressive kernels\nand fast exact inference techniques. Without human intervention -- no hand\ncrafting of kernel features, and no sophisticated initialisation procedures --\nwe show that GPatt can solve large scale pattern extrapolation, inpainting, and\nkernel discovery problems, including a problem with 383400 training points. We\nfind that GPatt significantly outperforms popular alternative scalable Gaussian\nprocess methods in speed and accuracy. Moreover, we discover profound\ndifferences between each of these methods, suggesting expressive kernels,\nnonparametric representations, and exact inference are useful for modelling\nlarge scale multidimensional patterns.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2013 01:26:45 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2013 16:58:35 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2013 14:10:34 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Gilboa", "Elad", ""], ["Nehorai", "Arye", ""], ["Cunningham", "John P.", ""]]}, {"id": "1310.5463", "submitter": "Ioanna Lykourentzou", "authors": "Muhammad Imran, Ioanna Lykourentzou, Yannick Naudet, Carlos Castillo", "title": "Engineering Crowdsourced Stream Processing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crowdsourced stream processing system (CSP) is a system that incorporates\ncrowdsourced tasks in the processing of a data stream. This can be seen as\nenabling crowdsourcing work to be applied on a sample of large-scale data at\nhigh speed, or equivalently, enabling stream processing to employ human\nintelligence. It also leads to a substantial expansion of the capabilities of\ndata processing systems. Engineering a CSP system requires the combination of\nhuman and machine computation elements. From a general systems theory\nperspective, this means taking into account inherited as well as emerging\nproperties from both these elements. In this paper, we position CSP systems\nwithin a broader taxonomy, outline a series of design principles and evaluation\nmetrics, present an extensible framework for their design, and describe several\ndesign patterns. We showcase the capabilities of CSP systems by performing a\ncase study that applies our proposed framework to the design and analysis of a\nreal system (AIDR) that classifies social media messages during time-critical\ncrisis events. Results show that compared to a pure stream processing system,\nAIDR can achieve a higher data classification accuracy, while compared to a\npure crowdsourcing solution, the system makes better use of human workers by\nrequiring much less manual work effort.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 08:46:29 GMT"}, {"version": "v2", "created": "Fri, 25 Jul 2014 21:59:16 GMT"}, {"version": "v3", "created": "Mon, 4 Aug 2014 08:54:40 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Imran", "Muhammad", ""], ["Lykourentzou", "Ioanna", ""], ["Naudet", "Yannick", ""], ["Castillo", "Carlos", ""]]}, {"id": "1310.5488", "submitter": "Wim Pessemier", "authors": "Wim Pessemier, Gert Raskin, Hans Van Winckel, Geert Deconinck,\n  Philippe Saey", "title": "A practical approach to ontology-enabled control systems for\n  astronomical instrumentation", "comments": "Proceedings of ICALEPCS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though modern service-oriented and data-oriented architectures promise\nto deliver loosely coupled control systems, they are inherently brittle as they\ncommonly depend on a priori agreed interfaces and data models. At the same\ntime, the Semantic Web and a whole set of accompanying standards and tools are\nemerging, advocating ontologies as the basis for knowledge exchange. In this\npaper we aim to identify a number of key ideas from the myriad of\nknowledge-based practices that can readily be implemented by control systems\ntoday. We demonstrate with a practical example (a three-channel imager for the\nMercator Telescope) how ontologies developed in the Web Ontology Language (OWL)\ncan serve as a meta-model for our instrument, covering as many engineering\naspects of the project as needed. We show how a concrete system model can be\nbuilt on top of this meta-model via a set of Domain Specific Languages (DSLs),\nsupporting both formal verification and the generation of software and\ndocumentation artifacts. Finally we reason how the available semantics can be\nexposed at run-time by adding a \"semantic layer\" that can be browsed, queried,\nmonitored etc. by any OPC UA-enabled client.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 10:20:28 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Pessemier", "Wim", ""], ["Raskin", "Gert", ""], ["Van Winckel", "Hans", ""], ["Deconinck", "Geert", ""], ["Saey", "Philippe", ""]]}, {"id": "1310.5781", "submitter": "David Budden", "authors": "Madison Flannery, Shannon Fenn and David Budden", "title": "RANSAC: Identification of Higher-Order Geometric Features and\n  Applications in Humanoid Robot Soccer", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability for an autonomous agent to self-localise is directly proportional\nto the accuracy and precision with which it can perceive salient features\nwithin its local environment. The identification of such features by\nrecognising geometric profile allows robustness against lighting variations,\nwhich is necessary in most industrial robotics applications. This paper details\na framework by which the random sample consensus (RANSAC) algorithm, often\napplied to parameter fitting in linear models, can be extended to identify\nhigher-order geometric features. Goalpost identification within humanoid robot\nsoccer is investigated as an application, with the developed system yielding an\norder-of-magnitude improvement in classification performance relative to a\ntraditional histogramming methodology.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 02:38:30 GMT"}], "update_date": "2013-10-23", "authors_parsed": [["Flannery", "Madison", ""], ["Fenn", "Shannon", ""], ["Budden", "David", ""]]}, {"id": "1310.5793", "submitter": "Chinmay Dhekne Mr", "authors": "Snehal Mulay, Chinmay Dhekne, Rucha Bapat, Tanmay Budukh, Soham Gadgil", "title": "Intelligent City Traffic Management and Public Transportation System", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 3, No 1, May 2013 ISSN (Print): 1694-0814 | ISSN (Online): 1694-0784\n  Reference: IJCSI-10-3-1-46-50", "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent Transportation System in case of cities is controlling traffic\ncongestion and regulating the traffic flow. This paper presents three modules\nthat will help in managing city traffic issues and ultimately gives advanced\ndevelopment in transportation system. First module, Congestion Detection and\nManagement will provide user real time information about congestion on the road\ntowards his destination, Second module, Intelligent Public Transport System\nwill provide user real time public transport information,i.e, local buses, and\nthe third module, Signal Synchronization will help in controlling congestion at\nsignals, with real time adjustments of signal timers according to the\ncongestion. All the information that user is getting about the traffic or\npublic transportation will be provided on users day to day device that is\nmobile through Android application or SMS. Moreover, communication can also be\ndone via Website for Clients having internet access. And all these modules will\nbe fully automated without any human intervention at server side.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 03:47:47 GMT"}], "update_date": "2013-10-23", "authors_parsed": [["Mulay", "Snehal", ""], ["Dhekne", "Chinmay", ""], ["Bapat", "Rucha", ""], ["Budukh", "Tanmay", ""], ["Gadgil", "Soham", ""]]}, {"id": "1310.6257", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer, Dan Suciu", "title": "Dissociation and Propagation for Approximate Lifted Inference with\n  Standard Relational Database Management Systems", "comments": "33 pages, 27 figures, pre-print for VLDBJ full version of\n  arXiv:1412.1069 [PVLDB 8(5):629-640, 2015: \"Approximate lifted inference with\n  probabilistic databases\", http://www.vldb.org/pvldb/vol8/p629-gatterbauer.pdf\n  ]. Former working title: \"Dissociation and Propagation for Efficient Query\n  Evaluation over Probabilistic Databases\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic inference over large data sets is a challenging data management\nproblem since exact inference is generally #P-hard and is most often solved\napproximately with sampling-based methods today. This paper proposes an\nalternative approach for approximate evaluation of conjunctive queries with\nstandard relational databases: In our approach, every query is evaluated\nentirely in the database engine by evaluating a fixed number of query plans,\neach providing an upper bound on the true probability, then taking their\nminimum. We provide an algorithm that takes into account important schema\ninformation to enumerate only the minimal necessary plans among all possible\nplans. Importantly, this algorithm is a strict generalization of all known\nPTIME self-join-free conjunctive queries: A query is in PTIME if and only if\nour algorithm returns one single plan. Furthermore, our approach is a\ngeneralization of a family of efficient ranking methods from graphs to\nhypergraphs. We also adapt three relational query optimization techniques to\nevaluate all necessary plans very fast. We give a detailed experimental\nevaluation of our approach and, in the process, provide a new way of thinking\nabout the value of probabilistic methods over non-probabilistic methods for\nranking query answers. We also note that the techniques developed in this paper\napply immediately to lifted inference from statistical relational models since\nlifted inference corresponds to PTIME plans in probabilistic databases.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 15:14:41 GMT"}, {"version": "v2", "created": "Tue, 12 Aug 2014 16:40:04 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2016 15:10:53 GMT"}, {"version": "v4", "created": "Tue, 14 Jun 2016 15:22:13 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Gatterbauer", "Wolfgang", ""], ["Suciu", "Dan", ""]]}, {"id": "1310.6288", "submitter": "Hao Zhang", "authors": "Hao Zhang and Liqing Zhang", "title": "Spatial-Spectral Boosting Analysis for Stroke Patients' Motor Imagery\n  EEG in Rehabilitation Training", "comments": "10 pages,3 figures", "journal-ref": null, "doi": "10.3233/978-1-61499-419-0-537", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current studies about motor imagery based rehabilitation training systems for\nstroke subjects lack an appropriate analytic method, which can achieve a\nconsiderable classification accuracy, at the same time detects gradual changes\nof imagery patterns during rehabilitation process and disinters potential\nmechanisms about motor function recovery. In this study, we propose an adaptive\nboosting algorithm based on the cortex plasticity and spectral band shifts.\nThis approach models the usually predetermined spatial-spectral configurations\nin EEG study into variable preconditions, and introduces a new heuristic of\nstochastic gradient boost for training base learners under these preconditions.\nWe compare our proposed algorithm with commonly used methods on datasets\ncollected from 2 months' clinical experiments. The simulation results\ndemonstrate the effectiveness of the method in detecting the variations of\nstroke patients' EEG patterns. By chronologically reorganizing the weight\nparameters of the learned additive model, we verify the spatial compensatory\nmechanism on impaired cortex and detect the changes of accentuation bands in\nspectral domain, which may contribute important prior knowledge for\nrehabilitation practice.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 16:43:59 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Zhang", "Hao", ""], ["Zhang", "Liqing", ""]]}, {"id": "1310.6323", "submitter": "Burkhard C. Schipper", "authors": "Rineke Verbrugge", "title": "Logic in the Lab", "comments": "2 pages, Plenary talk at TARK 2013 (arXiv:1310.6382)\n  http://www.tark.org", "journal-ref": null, "doi": null, "report-no": "TARK/2013/p4", "categories": "cs.AI cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This file summarizes the plenary talk on laboratory experiments on logic at\nthe TARK 2013 - 14th Conference on Theoretical Aspects of Rationality and\nKnowledge.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 18:34:56 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Verbrugge", "Rineke", ""]]}, {"id": "1310.6343", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora and Aditya Bhaskara and Rong Ge and Tengyu Ma", "title": "Provable Bounds for Learning Some Deep Representations", "comments": "The first 18 pages serve as an extended abstract and a 36 pages long\n  technical appendix follows", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give algorithms with provable guarantees that learn a class of deep nets\nin the generative model view popularized by Hinton and others. Our generative\nmodel is an $n$ node multilayer neural net that has degree at most $n^{\\gamma}$\nfor some $\\gamma <1$ and each edge has a random edge weight in $[-1,1]$. Our\nalgorithm learns {\\em almost all} networks in this class with polynomial\nrunning time. The sample complexity is quadratic or cubic depending upon the\ndetails of the model.\n  The algorithm uses layerwise learning. It is based upon a novel idea of\nobserving correlations among features and using these to infer the underlying\nedge structure via a global graph recovery procedure. The analysis of the\nalgorithm reveals interesting structure of neural networks with random edge\nweights.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 19:49:32 GMT"}], "update_date": "2013-10-25", "authors_parsed": [["Arora", "Sanjeev", ""], ["Bhaskara", "Aditya", ""], ["Ge", "Rong", ""], ["Ma", "Tengyu", ""]]}, {"id": "1310.6429", "submitter": "Burkhard C. Schipper", "authors": "Jerome Lang, Bruno Zanuttini", "title": "Knowledge-Based Programs as Plans: Succinctness and the Complexity of\n  Plan Existence", "comments": "10 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)\n  http://www.tark.org", "journal-ref": null, "doi": null, "report-no": "TARK/2013/p138", "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge-based programs (KBPs) are high-level protocols describing the\ncourse of action an agent should perform as a function of its knowledge. The\nuse of KBPs for expressing action policies in AI planning has been surprisingly\noverlooked. Given that to each KBP corresponds an equivalent plan and vice\nversa, KBPs are typically more succinct than standard plans, but imply more\non-line computation time. Here we make this argument formal, and prove that\nthere exists an exponential succinctness gap between knowledge-based programs\nand standard plans. Then we address the complexity of plan existence. Some\nresults trivially follow from results already known from the literature on\nplanning under incomplete knowledge, but many were unknown so far.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 23:25:44 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Lang", "Jerome", ""], ["Zanuttini", "Bruno", ""]]}, {"id": "1310.6432", "submitter": "Burkhard C. Schipper", "authors": "Eric Pacuit, Arthur Paul Pedersen, Jan-Willem Romeijn", "title": "When is an Example a Counterexample?", "comments": "10 pages, Contributed talk at TARK 2013 (arXiv:1310.6382)\n  http://www.tark.org", "journal-ref": null, "doi": null, "report-no": "TARK/2013/p156", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this extended abstract, we carefully examine a purported counterexample to\na postulate of iterated belief revision. We suggest that the example is better\nseen as a failure to apply the theory of belief revision in sufficient detail.\nThe main contribution is conceptual aiming at the literature on the\nphilosophical foundations of the AGM theory of belief revision [1]. Our\ndiscussion is centered around the observation that it is often unclear whether\na specific example is a \"genuine\" counterexample to an abstract theory or a\nmisapplication of that theory to a concrete case.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 23:32:30 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Pacuit", "Eric", ""], ["Pedersen", "Arthur Paul", ""], ["Romeijn", "Jan-Willem", ""]]}, {"id": "1310.6775", "submitter": "Linas Vepstas PhD", "authors": "Linas Vepstas", "title": "Durkheim Project Data Analysis Report", "comments": "43 pages, to appear as appendix of primary science publication\n  Poulin, et al \"Predicting the risk of suicide by analyzing the text of\n  clinical notes\"", "journal-ref": null, "doi": "10.1371/journal.pone.0085733.s001", "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the suicidality prediction models created under the\nDARPA DCAPS program in association with the Durkheim Project\n[http://durkheimproject.org/]. The models were built primarily from\nunstructured text (free-format clinician notes) for several hundred patient\nrecords obtained from the Veterans Health Administration (VHA). The models were\nconstructed using a genetic programming algorithm applied to bag-of-words and\nbag-of-phrases datasets. The influence of additional structured data was\nexplored but was found to be minor. Given the small dataset size,\nclassification between cohorts was high fidelity (98%). Cross-validation\nsuggests these models are reasonably predictive, with an accuracy of 50% to 69%\non five rotating folds, with ensemble averages of 58% to 67%. One particularly\nnoteworthy result is that word-pairs can dramatically improve classification\naccuracy; but this is the case only when one of the words in the pair is\nalready known to have a high predictive value. By contrast, the set of all\npossible word-pairs does not improve on a simple bag-of-words model.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 21:10:53 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Vepstas", "Linas", ""]]}, {"id": "1310.7115", "submitter": "Mohammad AlHawarat Dr.", "authors": "Mohammad Alhawarat, Waleed Nazih and Mohammad Eldesouki", "title": "Studying a Chaotic Spiking Neural Model", "comments": "Journal, 13 pages", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol. 4, No. 5, September 2013", "doi": "10.5121/ijaia.2013.4508", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamics of a chaotic spiking neuron model are being studied mathematically\nand experimentally. The Nonlinear Dynamic State neuron (NDS) is analysed to\nfurther understand the model and improve it. Chaos has many interesting\nproperties such as sensitivity to initial conditions, space filling, control\nand synchronization. As suggested by biologists, these properties may be\nexploited and play vital role in carrying out computational tasks in human\nbrain. The NDS model has some limitations; in thus paper the model is\ninvestigated to overcome some of these limitations in order to enhance the\nmodel. Therefore, the models parameters are tuned and the resulted dynamics are\nstudied. Also, the discretization method of the model is considered. Moreover,\na mathematical analysis is carried out to reveal the underlying dynamics of the\nmodel after tuning of its parameters. The results of the aforementioned methods\nrevealed some facts regarding the NDS attractor and suggest the stabilization\nof a large number of unstable periodic orbits (UPOs) which might correspond to\nmemories in phase space.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2013 14:21:10 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Alhawarat", "Mohammad", ""], ["Nazih", "Waleed", ""], ["Eldesouki", "Mohammad", ""]]}, {"id": "1310.7163", "submitter": "Lihong Li", "authors": "Lihong Li", "title": "Generalized Thompson Sampling for Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson Sampling, one of the oldest heuristics for solving multi-armed\nbandits, has recently been shown to demonstrate state-of-the-art performance.\nThe empirical success has led to great interests in theoretical understanding\nof this heuristic. In this paper, we approach this problem in a way very\ndifferent from existing efforts. In particular, motivated by the connection\nbetween Thompson Sampling and exponentiated updates, we propose a new family of\nalgorithms called Generalized Thompson Sampling in the expert-learning\nframework, which includes Thompson Sampling as a special case. Similar to most\nexpert-learning algorithms, Generalized Thompson Sampling uses a loss function\nto adjust the experts' weights. General regret bounds are derived, which are\nalso instantiated to two important loss functions: square loss and logarithmic\nloss. In contrast to existing bounds, our results apply to quite general\ncontextual bandits. More importantly, they quantify the effect of the \"prior\"\ndistribution on the regret bounds.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2013 06:29:55 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Li", "Lihong", ""]]}, {"id": "1310.7367", "submitter": "Thabet Slimani", "authors": "Thabet Slimani", "title": "Semantic Description of Web Services", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 1, No 3, January 2013, ISSN (Print): 1694-0784 | ISSN (Online):\n  1694-0814", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tasks of semantic web service (discovery, selection, composition, and\nexecution) are supposed to enable seamless interoperation between systems,\nwhereby human intervention is kept at a minimum. In the field of Web service\ndescription research, the exploitation of descriptions of services through\nsemantics is a better support for the life-cycle of Web services. The large\nnumber of developed ontologies, languages of representations, and integrated\nframeworks supporting the discovery, composition and invocation of services is\na good indicator that research in the field of Semantic Web Services (SWS) has\nbeen considerably active. We provide in this paper a detailed classification of\nthe approaches and solutions, indicating their core characteristics and\nobjectives required and provide indicators for the interested reader to follow\nup further insights and details about these solutions and related software.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 10:32:06 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Slimani", "Thabet", ""]]}, {"id": "1310.7442", "submitter": "Xinyang Deng", "authors": "Yuxian Du, Shiyu Chen, Yong Hu, Felix T.S. Chan, Sankaran Mahadevan,\n  Yong Deng", "title": "Ranking basic belief assignments in decision making under uncertain\n  environment", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dempster-Shafer theory (D-S theory) is widely used in decision making under\nthe uncertain environment. Ranking basic belief assignments (BBAs) now is an\nopen issue. Existing evidence distance measures cannot rank the BBAs in the\nsituations when the propositions have their own ranking order or their inherent\nmeasure of closeness. To address this issue, a new ranking evidence distance\n(RED) measure is proposed. Compared with the existing evidence distance\nmeasures including the Jousselme's distance and the distance between betting\ncommitments, the proposed RED measure is much more general due to the fact that\nthe order of the propositions in the systems is taken into consideration. If\nthere is no order or no inherent measure of closeness in the propositions, our\nproposed RED measure is reduced to the existing evidence distance. Numerical\nexamples show that the proposed RED measure is an efficient alternative to rank\nBBAs in decision making under uncertain environment.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 14:59:53 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Du", "Yuxian", ""], ["Chen", "Shiyu", ""], ["Hu", "Yong", ""], ["Chan", "Felix T. S.", ""], ["Mahadevan", "Sankaran", ""], ["Deng", "Yong", ""]]}, {"id": "1310.7610", "submitter": "Adwaitvedant Mathkar", "authors": "Adwaitvedant S. Mathkar and Vivek S. Borkar", "title": "Distributed Reinforcement Learning via Gossip", "comments": "18 pages, 3 figures, Submitted to Discrete Event Dynamic Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical TD(0) algorithm implemented on a network of agents\nwherein the agents also incorporate the updates received from neighboring\nagents using a gossip-like mechanism. The combined scheme is shown to converge\nfor both discounted and average cost problems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 20:23:57 GMT"}], "update_date": "2013-10-30", "authors_parsed": [["Mathkar", "Adwaitvedant S.", ""], ["Borkar", "Vivek S.", ""]]}, {"id": "1310.7682", "submitter": "Liane Gabora", "authors": "Liane Gabora and Diederik Aerts", "title": "Contextualizing concepts using a mathematical generalization of the\n  quantum formalism", "comments": "31 pages. arXiv admin note: substantial text overlap with\n  arXiv:quant-ph/0205161", "journal-ref": "Journal of Experimental and Theoretical Artificial Intelligence,\n  14(4), 327-358", "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline the rationale and preliminary results of using the state context\nproperty (SCOP) formalism, originally developed as a generalization of quantum\nmechanics, to describe the contextual manner in which concepts are evoked, used\nand combined to generate meaning. The quantum formalism was developed to cope\nwith problems arising in the description of (i) the measurement process, and\n(ii) the generation of new states with new properties when particles become\nentangled. Similar problems arising with concepts motivated the formal\ntreatment introduced here. Concepts are viewed not as fixed representations,\nbut entities existing in states of potentiality that require interaction with a\ncontext-a stimulus or another concept-to 'collapse' to an instantiated form\n(e.g. exemplar, prototype, or other possibly imaginary instance). The stimulus\nsituation plays the role of the measurement in physics, acting as context that\ninduces a change of the cognitive state from superposition state to collapsed\nstate. The collapsed state is more likely to consist of a conjunction of\nconcepts for associative than analytic thought because more stimulus or concept\nproperties take part in the collapse. We provide two contextual measures of\nconceptual distance-one using collapse probabilities and the other weighted\nproperties-and show how they can be applied to conjunctions using the pet fish\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 04:40:53 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 22:17:52 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Gabora", "Liane", ""], ["Aerts", "Diederik", ""]]}, {"id": "1310.7828", "submitter": "Sebastian Ordyniak", "authors": "Christer Baeckstroem, Peter Jonsson, Sebastian Ordyniak, Stefan\n  Szeider", "title": "A Complete Parameterized Complexity Analysis of Bounded Planning", "comments": "The paper is a combined and extended version of the papers \"The\n  Complexity of Planning Revisited - A Parameterized Analysis\" (AAAI 2012,\n  arXiv:1208.2566) and \"Parameterized Complexity and Kernel Bounds for Hard\n  Planning Problems\" (CIAC 2013, arXiv:1211.0479)", "journal-ref": "Proc. AAAI'12, pp. 1735-1741, AAAI Press 2012 and Proc. CIAC'13,\n  pp. 13-24, Springer", "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The propositional planning problem is a notoriously difficult computational\nproblem, which remains hard even under strong syntactical and structural\nrestrictions. Given its difficulty it becomes natural to study planning in the\ncontext of parameterized complexity. In this paper we continue the work\ninitiated by Downey, Fellows and Stege on the parameterized complexity of\nplanning with respect to the parameter \"length of the solution plan.\" We\nprovide a complete classification of the parameterized complexity of the\nplanning problem under two of the most prominent syntactical restrictions,\ni.e., the so called PUBS restrictions introduced by Baeckstroem and Nebel and\nrestrictions on the number of preconditions and effects as introduced by\nBylander. We also determine which of the considered fixed-parameter tractable\nproblems admit a polynomial kernel and which don't.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 15:12:15 GMT"}], "update_date": "2013-10-30", "authors_parsed": [["Baeckstroem", "Christer", ""], ["Jonsson", "Peter", ""], ["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1310.7950", "submitter": "Austin Jones M.S.", "authors": "Austin Jones, Mac Schwager, and Calin Belta", "title": "Technical Report: Distribution Temporal Logic: Combining Correctness\n  with Quality of Estimation", "comments": "More expanded version of \"Distribution Temporal Logic: Combining\n  Correctness with Quality of Estimation\" to appear in IEEE CDC 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new temporal logic called Distribution Temporal Logic (DTL)\ndefined over predicates of belief states and hidden states of partially\nobservable systems. DTL can express properties involving uncertainty and\nlikelihood that cannot be described by existing logics. A co-safe formulation\nof DTL is defined and algorithmic procedures are given for monitoring\nexecutions of a partially observable Markov decision process with respect to\nsuch formulae. A simulation case study of a rescue robotics application\noutlines our approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 18:06:50 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Jones", "Austin", ""], ["Schwager", "Mac", ""], ["Belta", "Calin", ""]]}, {"id": "1310.7961", "submitter": "Seyyed Reza Khaze", "authors": "Seyyed Reza Khaze, Isa maleki, Sohrab Hojjatkhah and Ali Bagherinia", "title": "Evaluation the efficiency of artificial bee colony and the firefly\n  algorithm in solving the continuous optimization problem", "comments": null, "journal-ref": "International Journal on Computational Sciences & Applications\n  (IJCSA) Vol.3, No.4, August 2013", "doi": "10.5121/ijcsa.2013.3403", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Now the Meta-Heuristic algorithms have been used vastly in solving the\nproblem of continuous optimization. In this paper the Artificial Bee Colony\n(ABC) algorithm and the Firefly Algorithm (FA) are valuated. And for presenting\nthe efficiency of the algorithms and also for more analysis of them, the\ncontinuous optimization problems which are of the type of the problems of vast\nlimit of answer and the close optimized points are tested. So, in this paper\nthe efficiency of the ABC algorithm and FA are presented for solving the\ncontinuous optimization problems and also the said algorithms are studied from\nthe accuracy in reaching the optimized solution and the resulting time and the\nreliability of the optimized answer points of view.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 09:26:21 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Khaze", "Seyyed Reza", ""], ["maleki", "Isa", ""], ["Hojjatkhah", "Sohrab", ""], ["Bagherinia", "Ali", ""]]}, {"id": "1310.7981", "submitter": "Liane Gabora", "authors": "Tomas Veloz, Liane Gabora, Mark Eyjolfson, and Diederik Aerts", "title": "Toward a Formal Model of the Shifting Relationship between Concepts and\n  Contexts during Associative Thought", "comments": "11 pages; 2 figures", "journal-ref": "(2011). Lecture Notes in Computer Science 7052: Proceedings Fifth\n  International Symposium on Quantum Interaction. June 27-29, Aberdeen, UK.\n  Berlin: Springer", "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantum inspired State Context Property (SCOP) theory of concepts is\nunique amongst theories of concepts in offering a means of incorporating that\nfor each concept in each different context there are an unlimited number of\nexemplars, or states, of varying degrees of typicality. Working with data from\na study in which participants were asked to rate the typicality of exemplars of\na concept for different contexts, and introducing an exemplar typicality\nthreshold, we built a SCOP model of how states of a concept arise differently\nin associative versus analytic (or divergent and convergent) modes of thought.\nIntroducing measures of state robustness and context relevance, we show that by\nvarying the threshold, the relevance of different contexts changes, and\nseemingly atypical states can become typical. The formalism provides a pivotal\nstep toward a formal explanation of creative thought proesses.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 22:57:54 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Veloz", "Tomas", ""], ["Gabora", "Liane", ""], ["Eyjolfson", "Mark", ""], ["Aerts", "Diederik", ""]]}, {"id": "1310.8120", "submitter": "Fabrizio Angiulli", "authors": "Fabrizio Angiulli and Rachel Ben-Eliyahu-Zohary and Fabio Fassetti and\n  Luigi Palopoli", "title": "On the Tractability of Minimal Model Computation for Some CNF Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing algorithms capable of efficiently constructing minimal models of\nCNFs is an important task in AI. This paper provides new results along this\nresearch line and presents new algorithms for performing minimal model finding\nand checking over positive propositional CNFs and model minimization over\npropositional CNFs. An algorithmic schema, called the Generalized Elimination\nAlgorithm (GEA) is presented, that computes a minimal model of any positive\nCNF. The schema generalizes the Elimination Algorithm (EA) [BP97], which\ncomputes a minimal model of positive head-cycle-free (HCF) CNF theories. While\nthe EA always runs in polynomial time in the size of the input HCF CNF, the\ncomplexity of the GEA depends on the complexity of the specific eliminating\noperator invoked therein, which may in general turn out to be exponential.\nTherefore, a specific eliminating operator is defined by which the GEA\ncomputes, in polynomial time, a minimal model for a class of CNF that strictly\nincludes head-elementary-set-free (HEF) CNF theories [GLL06], which form, in\ntheir turn, a strict superset of HCF theories. Furthermore, in order to deal\nwith the high complexity associated with recognizing HEF theories, an\n\"incomplete\" variant of the GEA (called IGEA) is proposed: the resulting\nschema, once instantiated with an appropriate elimination operator, always\nconstructs a model of the input CNF, which is guaranteed to be minimal if the\ninput theory is HEF. In the light of the above results, the main contribution\nof this work is the enlargement of the tractability frontier for the minimal\nmodel finding and checking and the model minimization problems.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 12:37:06 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Angiulli", "Fabrizio", ""], ["Ben-Eliyahu-Zohary", "Rachel", ""], ["Fassetti", "Fabio", ""], ["Palopoli", "Luigi", ""]]}, {"id": "1310.8583", "submitter": "Swakkhar Shatabda", "authors": "Swakkhar Shatabda, M.A. Hakim Newton, Duc Nghia Pham and Abdul Sattar", "title": "A Hybrid Local Search for Simplified Protein Structure Prediction", "comments": null, "journal-ref": "Proceedings of the International Conference on Bioinformatics\n  Models, Methods and Algorithms, Barcelona, Spain, 11 - 14 February, 2013.\n  SciTePress 2013 ISBN 978-989-8565-35-8 pages:158-163", "doi": null, "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein structure prediction based on Hydrophobic-Polar energy model\nessentially becomes searching for a conformation having a compact hydrophobic\ncore at the center. The hydrophobic core minimizes the interaction energy\nbetween the amino acids of the given protein. Local search algorithms can\nquickly find very good conformations by moving repeatedly from the current\nsolution to its \"best\" neighbor. However, once such a compact hydrophobic core\nis found, the search stagnates and spends enormous effort in quest of an\nalternative core. In this paper, we attempt to restructure segments of a\nconformation with such compact core. We select one large segment or a number of\nsmall segments and apply exhaustive local search. We also apply a mix of\nheuristics so that one heuristic can help escape local minima of another. We\nevaluated our algorithm by using Face Centered Cubic (FCC) Lattice on a set of\nstandard benchmark proteins and obtain significantly better results than that\nof the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 16:41:44 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Shatabda", "Swakkhar", ""], ["Newton", "M. A. Hakim", ""], ["Pham", "Duc Nghia", ""], ["Sattar", "Abdul", ""]]}, {"id": "1310.8588", "submitter": "Abdoun Otman", "authors": "Tkatek Said, Abdoun Otman, Abouchabaka Jaafar and Rafalia Najat", "title": "A Meta-heuristically Approach of the Spatial Assignment Problem of Human\n  Resources in Multi-sites Enterprise", "comments": null, "journal-ref": "International Journal of Computer Applications (0975 - 8887)\n  Volume 78 - Number 7 September 2013", "doi": "10.5120/13500-1248", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to present a meta-heuristically approach of the\nspatial assignment problem of human resources in multi-sites enterprise.\nUsually, this problem consists to move employees from one site to another based\non one or more criteria. Our goal in this new approach is to improve the\nquality of service and performance of all sites with maximizing an objective\nfunction under some managers imposed constraints. The formulation presented\nhere of this problem coincides perfectly with a Combinatorial Optimization\nProblem (COP) which is in the most cases NP-hard to solve optimally. To avoid\nthis difficulty, we have opted to use a meta-heuristic popular method, which is\nthe genetic algorithm, to solve this problem in concrete cases. The results\nobtained have shown the effectiveness of our approach, which remains until now\nvery costly in time. But the reduction of the time can be obtained by different\nways that we plan to do in the next work.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2013 11:10:57 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Said", "Tkatek", ""], ["Otman", "Abdoun", ""], ["Jaafar", "Abouchabaka", ""], ["Najat", "Rafalia", ""]]}, {"id": "1310.8599", "submitter": "J. G. Wolff", "authors": "J. Gerard Wolff", "title": "Information Compression, Intelligence, Computing, and Mathematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents evidence for the idea that much of artificial\nintelligence, human perception and cognition, mainstream computing, and\nmathematics, may be understood as compression of information via the matching\nand unification of patterns. This is the basis for the \"SP theory of\nintelligence\", outlined in the paper and fully described elsewhere. Relevant\nevidence may be seen: in empirical support for the SP theory; in some\nadvantages of information compression (IC) in terms of biology and engineering;\nin our use of shorthands and ordinary words in language; in how we merge\nsuccessive views of any one thing; in visual recognition; in binocular vision;\nin visual adaptation; in how we learn lexical and grammatical structures in\nlanguage; and in perceptual constancies. IC via the matching and unification of\npatterns may be seen in both computing and mathematics: in IC via equations; in\nthe matching and unification of names; in the reduction or removal of\nredundancy from unary numbers; in the workings of Post's Canonical System and\nthe transition function in the Universal Turing Machine; in the way computers\nretrieve information from memory; in systems like Prolog; and in the\nquery-by-example technique for information retrieval. The chunking-with-codes\ntechnique for IC may be seen in the use of named functions to avoid repetition\nof computer code. The schema-plus-correction technique may be seen in functions\nwith parameters and in the use of classes in object-oriented programming. And\nthe run-length coding technique may be seen in multiplication, in division, and\nin several other devices in mathematics and computing. The SP theory resolves\nthe apparent paradox of \"decompression by compression\". And computing and\ncognition as IC is compatible with the uses of redundancy in such things as\nbackup copies to safeguard data and understanding speech in a noisy\nenvironment.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 17:18:17 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 11:17:17 GMT"}, {"version": "v3", "created": "Tue, 29 Apr 2014 18:16:35 GMT"}, {"version": "v4", "created": "Mon, 13 Jul 2015 08:59:41 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Wolff", "J. Gerard", ""]]}]