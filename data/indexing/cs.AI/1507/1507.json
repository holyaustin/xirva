[{"id": "1507.00043", "submitter": "Athanasios N. Nikolakopoulos", "authors": "Athanasios N. Nikolakopoulos and John D. Garofalakis", "title": "Top-N recommendations in the presence of sparsity: An NCD-based approach", "comments": "To appear in the Web Intelligence Journal as a regular paper", "journal-ref": null, "doi": "10.3233/WEB-150324", "report-no": null, "categories": "cs.IR cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making recommendations in the presence of sparsity is known to present one of\nthe most challenging problems faced by collaborative filtering methods. In this\nwork we tackle this problem by exploiting the innately hierarchical structure\nof the item space following an approach inspired by the theory of\nDecomposability. We view the itemspace as a Nearly Decomposable system and we\ndefine blocks of closely related elements and corresponding indirect proximity\ncomponents. We study the theoretical properties of the decomposition and we\nderive sufficient conditions that guarantee full item space coverage even in\ncold-start recommendation scenarios. A comprehensive set of experiments on the\nMovieLens and the Yahoo!R2Music datasets, using several widely applied\nperformance metrics, support our model's theoretically predicted properties and\nverify that NCDREC outperforms several state-of-the-art algorithms, in terms of\nrecommendation accuracy, diversity and sparseness insensitivity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 21:34:53 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 13:55:35 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Nikolakopoulos", "Athanasios N.", ""], ["Garofalakis", "John D.", ""]]}, {"id": "1507.00066", "submitter": "Pooria Joulani", "authors": "Pooria Joulani, Andr\\'as Gy\\\"orgy, Csaba Szepesv\\'ari", "title": "Fast Cross-Validation for Incremental Learning", "comments": "Appearing in the International Joint Conference on Artificial\n  Intelligence (IJCAI-2015), Buenos Aires, Argentina, July 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation (CV) is one of the main tools for performance estimation and\nparameter tuning in machine learning. The general recipe for computing CV\nestimate is to run a learning algorithm separately for each CV fold, a\ncomputationally expensive process. In this paper, we propose a new approach to\nreduce the computational burden of CV-based performance estimation. As opposed\nto all previous attempts, which are specific to a particular learning model or\nproblem domain, we propose a general method applicable to a large class of\nincremental learning algorithms, which are uniquely fitted to big data\nproblems. In particular, our method applies to a wide range of supervised and\nunsupervised learning tasks with different performance criteria, as long as the\nbase learning algorithm is incremental. We show that the running time of the\nalgorithm scales logarithmically, rather than linearly, in the number of CV\nfolds. Furthermore, the algorithm has favorable properties for parallel and\ndistributed implementation. Experiments with state-of-the-art incremental\nlearning algorithms confirm the practicality of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 23:30:28 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Joulani", "Pooria", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1507.00142", "submitter": "Cunjing Ge", "authors": "Cunjing Ge, Feifei Ma and Jian Zhang", "title": "A Tool for Computing and Estimating the Volume of the Solution Space of\n  SMT(LA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are already quite a few tools for solving the Satisfiability Modulo\nTheories (SMT) problems. In this paper, we present \\texttt{VolCE}, a tool for\ncounting the solutions of SMT constraints, or in other words, for computing the\nvolume of the solution space. Its input is essentially a set of Boolean\ncombinations of linear constraints, where the numeric variables are either all\nintegers or all reals, and each variable is bounded. The tool extends SMT\nsolving with integer solution counting and volume computation/estimation for\nconvex polytopes. Effective heuristics are adopted, which enable the tool to\ndeal with high-dimensional problem instances efficiently and accurately.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 08:06:33 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Ge", "Cunjing", ""], ["Ma", "Feifei", ""], ["Zhang", "Jian", ""]]}, {"id": "1507.00257", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi and Babak Salimi", "title": "From Causes for Database Queries to Repairs and Model-Based Diagnosis\n  and Back", "comments": "To appear in Theory of Computing Systems. By invitation to special\n  issue with extended papers from ICDT 2015 (paper arXiv:1412.4311)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we establish and investigate connections between causes for\nquery answers in databases, database repairs wrt. denial constraints, and\nconsistency-based diagnosis. The first two are relatively new research areas in\ndatabases, and the third one is an established subject in knowledge\nrepresentation. We show how to obtain database repairs from causes, and the\nother way around. Causality problems are formulated as diagnosis problems, and\nthe diagnoses provide causes and their responsibilities. The vast body of\nresearch on database repairs can be applied to the newer problems of computing\nactual causes for query answers and their responsibilities. These connections,\nwhich are interesting per se, allow us, after a transition -inspired by\nconsistency-based diagnosis- to computational problems on hitting sets and\nvertex covers in hypergraphs, to obtain several new algorithmic and complexity\nresults for database causality.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 15:20:29 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 17:52:10 GMT"}, {"version": "v3", "created": "Sun, 23 Oct 2016 17:37:58 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Salimi", "Babak", ""]]}, {"id": "1507.00353", "submitter": "Harm van Seijen", "authors": "Harm van Seijen, A. Rupam Mahmood, Patrick M. Pilarski, Richard S.\n  Sutton", "title": "An Empirical Evaluation of True Online TD({\\lambda})", "comments": "European Workshop on Reinforcement Learning (EWRL) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The true online TD({\\lambda}) algorithm has recently been proposed (van\nSeijen and Sutton, 2014) as a universal replacement for the popular\nTD({\\lambda}) algorithm, in temporal-difference learning and reinforcement\nlearning. True online TD({\\lambda}) has better theoretical properties than\nconventional TD({\\lambda}), and the expectation is that it also results in\nfaster learning. In this paper, we put this hypothesis to the test.\nSpecifically, we compare the performance of true online TD({\\lambda}) with that\nof TD({\\lambda}) on challenging examples, random Markov reward processes, and a\nreal-world myoelectric prosthetic arm. We use linear function approximation\nwith tabular, binary, and non-binary features. We assess the algorithms along\nthree dimensions: computational cost, learning speed, and ease of use. Our\nresults confirm the strength of true online TD({\\lambda}): 1) for sparse\nfeature vectors, the computational overhead with respect to TD({\\lambda}) is\nminimal; for non-sparse features the computation time is at most twice that of\nTD({\\lambda}), 2) across all domains/representations the learning speed of true\nonline TD({\\lambda}) is often better, but never worse than that of\nTD({\\lambda}), and 3) true online TD({\\lambda}) is easier to use, because it\ndoes not require choosing between trace types, and it is generally more stable\nwith respect to the step-size. Overall, our results suggest that true online\nTD({\\lambda}) should be the first choice when looking for an efficient,\ngeneral-purpose TD method.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 20:03:49 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["van Seijen", "Harm", ""], ["Mahmood", "A. Rupam", ""], ["Pilarski", "Patrick M.", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1507.00407", "submitter": "Vasilis Syrgkanis", "authors": "Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, Robert E. Schapire", "title": "Fast Convergence of Regularized Learning in Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that natural classes of regularized learning algorithms with a form\nof recency bias achieve faster convergence rates to approximate efficiency and\nto coarse correlated equilibria in multiplayer normal form games. When each\nplayer in a game uses an algorithm from our class, their individual regret\ndecays at $O(T^{-3/4})$, while the sum of utilities converges to an approximate\noptimum at $O(T^{-1})$--an improvement upon the worst case $O(T^{-1/2})$ rates.\nWe show a black-box reduction for any algorithm in the class to achieve\n$\\tilde{O}(T^{-1/2})$ rates against an adversary, while maintaining the faster\nrates against algorithms in the class. Our results extend those of [Rakhlin and\nShridharan 2013] and [Daskalakis et al. 2014], who only analyzed two-player\nzero-sum games for specific algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 01:55:40 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 21:58:16 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2015 14:51:56 GMT"}, {"version": "v4", "created": "Tue, 8 Dec 2015 17:22:43 GMT"}, {"version": "v5", "created": "Thu, 10 Dec 2015 21:52:29 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Syrgkanis", "Vasilis", ""], ["Agarwal", "Alekh", ""], ["Luo", "Haipeng", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1507.00436", "submitter": "Yusen Zhan", "authors": "Yusen Zhan and Matthew E. Taylor", "title": "Online Transfer Learning in Reinforcement Learning Domains", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes an online transfer framework to capture the interaction\namong agents and shows that current transfer learning in reinforcement learning\nis a special case of online transfer. Furthermore, this paper re-characterizes\nexisting agents-teaching-agents methods as online transfer and analyze one such\nteaching method in three ways. First, the convergence of Q-learning and Sarsa\nwith tabular representation with a finite budget is proven. Second, the\nconvergence of Q-learning and Sarsa with linear function approximation is\nestablished. Third, the we show the asymptotic performance cannot be hurt\nthrough teaching. Additionally, all theoretical results are empirically\nvalidated.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 06:05:44 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 00:12:10 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Zhan", "Yusen", ""], ["Taylor", "Matthew E.", ""]]}, {"id": "1507.00567", "submitter": "Pooyan Jamshidi", "authors": "Pooyan Jamshidi, Amir Sharifloo, Claus Pahl, Andreas Metzger, Giovani\n  Estrada", "title": "Self-Learning Cloud Controllers: Fuzzy Q-Learning for Knowledge\n  Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI cs.DC cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud controllers aim at responding to application demands by automatically\nscaling the compute resources at runtime to meet performance guarantees and\nminimize resource costs. Existing cloud controllers often resort to scaling\nstrategies that are codified as a set of adaptation rules. However, for a cloud\nprovider, applications running on top of the cloud infrastructure are more or\nless black-boxes, making it difficult at design time to define optimal or\npre-emptive adaptation rules. Thus, the burden of taking adaptation decisions\noften is delegated to the cloud application. Yet, in most cases, application\ndevelopers in turn have limited knowledge of the cloud infrastructure. In this\npaper, we propose learning adaptation rules during runtime. To this end, we\nintroduce FQL4KE, a self-learning fuzzy cloud controller. In particular, FQL4KE\nlearns and modifies fuzzy rules at runtime. The benefit is that for designing\ncloud controllers, we do not have to rely solely on precise design-time\nknowledge, which may be difficult to acquire. FQL4KE empowers users to specify\ncloud controllers by simply adjusting weights representing priorities in system\ngoals instead of specifying complex adaptation rules. The applicability of\nFQL4KE has been experimentally assessed as part of the cloud application\nframework ElasticBench. The experimental results indicate that FQL4KE\noutperforms our previously developed fuzzy controller without learning\nmechanisms and the native Azure auto-scaling.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 13:11:22 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Jamshidi", "Pooyan", ""], ["Sharifloo", "Amir", ""], ["Pahl", "Claus", ""], ["Metzger", "Andreas", ""], ["Estrada", "Giovani", ""]]}, {"id": "1507.00814", "submitter": "Bradly Stadie", "authors": "Bradly C. Stadie, Sergey Levine, Pieter Abbeel", "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving efficient and scalable exploration in complex domains poses a major\nchallenge in reinforcement learning. While Bayesian and PAC-MDP approaches to\nthe exploration problem offer strong formal guarantees, they are often\nimpractical in higher dimensions due to their reliance on enumerating the\nstate-action space. Hence, exploration in complex domains is often performed\nwith simple epsilon-greedy methods. In this paper, we consider the challenging\nAtari games domain, which requires processing raw pixel inputs and delayed\nrewards. We evaluate several more sophisticated exploration strategies,\nincluding Thompson sampling and Boltzman exploration, and propose a new\nexploration method based on assigning exploration bonuses from a concurrently\nlearned model of the system dynamics. By parameterizing our learned model with\na neural network, we are able to develop a scalable and efficient approach to\nexploration bonuses that can be applied to tasks with complex, high-dimensional\nstate spaces. In the Atari domain, our method provides the most consistent\nimprovement across a range of games that pose a major challenge for prior\nmethods. In addition to raw game-scores, we also develop an AUC-100 metric for\nthe Atari Learning domain to evaluate the impact of exploration on this\nbenchmark.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 04:11:15 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 23:05:34 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2015 22:40:30 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Stadie", "Bradly C.", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1507.00862", "submitter": "Alexander Semenov", "authors": "Alexander Semenov and Oleg Zaikin", "title": "Using Monte Carlo method for searching partitionings of hard variants of\n  Boolean satisfiability problem", "comments": "The reduced version of this paper was accepted for publication in\n  proceedings of the PaCT 2015 conference (LNCS Vol. 9251). arXiv admin note:\n  substantial text overlap with arXiv:1411.5433", "journal-ref": "LNCS 9251 (2015) 222-230", "doi": "10.1007/978-3-319-21909-7_21", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the approach for constructing partitionings of hard\nvariants of the Boolean satisfiability problem (SAT). Such partitionings can be\nused for solving corresponding SAT instances in parallel. For the same SAT\ninstance one can construct different partitionings, each of them is a set of\nsimplified versions of the original SAT instance. The effectiveness of an\narbitrary partitioning is determined by the total time of solving of all SAT\ninstances from it. We suggest the approach, based on the Monte Carlo method,\nfor estimating time of processing of an arbitrary partitioning. With each\npartitioning we associate a point in the special finite search space. The\nestimation of effectiveness of the particular partitioning is the value of\npredictive function in the corresponding point of this space. The problem of\nsearch for an effective partitioning can be formulated as a problem of\noptimization of the predictive function. We use metaheuristic algorithms\n(simulated annealing and tabu search) to move from point to point in the search\nspace. In our computational experiments we found partitionings for SAT\ninstances encoding problems of inversion of some cryptographic functions.\nSeveral of these SAT instances with realistic predicted solving time were\nsuccessfully solved on a computing cluster and in the volunteer computing\nproject SAT@home. The solving time agrees well with estimations obtained by the\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 10:18:01 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Semenov", "Alexander", ""], ["Zaikin", "Oleg", ""]]}, {"id": "1507.00996", "submitter": "Jan-Willem van de Meent", "authors": "Frank Wood, Jan Willem van de Meent, Vikash Mansinghka", "title": "A New Approach to Probabilistic Programming Inference", "comments": "Updated version of the 2014 AISTATS paper (to reflect changes in new\n  language syntax). 10 pages, 3 figures. Proceedings of the Seventeenth\n  International Conference on Artificial Intelligence and Statistics, JMLR\n  Workshop and Conference Proceedings, Vol 33, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and demonstrate a new approach to inference in expressive\nprobabilistic programming languages based on particle Markov chain Monte Carlo.\nOur approach is simple to implement and easy to parallelize. It applies to\nTuring-complete probabilistic programming languages and supports accurate\ninference in models that make use of complex control flow, including stochastic\nrecursion. It also includes primitives from Bayesian nonparametric statistics.\nOur experiments show that this approach can be more efficient than previously\nintroduced single-site Metropolis-Hastings methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 19:52:58 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 10:31:26 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Wood", "Frank", ""], ["van de Meent", "Jan Willem", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1507.01122", "submitter": "Gabriel Makdah", "authors": "Gabriel Makdah", "title": "Modeling the Mind: A brief review", "comments": "Part 1 in the Modeling the Mind review series. 52 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain is a powerful tool used to achieve amazing feats. There have been\nseveral significant advances in neuroscience and artificial brain research in\nthe past two decades. This article is a review of such advances, ranging from\nthe concepts of connectionism, to neural network architectures and\nhigh-dimensional representations. There have also been advances in biologically\ninspired cognitive architectures of which we will cite a few. We will be\npositioning relatively specific models in a much broader perspective, while\ncomparing and contrasting their advantages and weaknesses. The projects\npresented are targeted to model the brain at different levels, utilizing\ndifferent methodologies.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 15:54:50 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 04:46:21 GMT"}, {"version": "v3", "created": "Fri, 13 May 2016 05:52:00 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Makdah", "Gabriel", ""]]}, {"id": "1507.01193", "submitter": "Piotr Mirowski", "authors": "Piotr Mirowski, Andreas Vlachos", "title": "Dependency Recurrent Neural Language Models for Sentence Completion", "comments": "Accepted for publication at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on language modelling has shifted focus from count-based models\nto neural models. In these works, the words in each sentence are always\nconsidered in a left-to-right order. In this paper we show how we can improve\nthe performance of the recurrent neural network (RNN) language model by\nincorporating the syntactic dependencies of a sentence, which have the effect\nof bringing relevant contexts closer to the word being predicted. We evaluate\nour approach on the Microsoft Research Sentence Completion Challenge and show\nthat the dependency RNN proposed improves over the RNN by about 10 points in\naccuracy. Furthermore, we achieve results comparable with the state-of-the-art\nmodels on this task.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 11:10:24 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Mirowski", "Piotr", ""], ["Vlachos", "Andreas", ""]]}, {"id": "1507.01269", "submitter": "Tianpei Xie", "authors": "Tianpei Xie, Nasser M. Nasrabadi and Alfred O. Hero III", "title": "Semi-supervised Multi-sensor Classification via Consensus-based\n  Multi-View Maximum Entropy Discrimination", "comments": "5 pages, 4 figures, Accepted in 40th IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP 15)", "journal-ref": null, "doi": "10.1109/ICASSP.2015.7178308", "report-no": null, "categories": "cs.IT cs.AI cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider multi-sensor classification when there is a large\nnumber of unlabeled samples. The problem is formulated under the multi-view\nlearning framework and a Consensus-based Multi-View Maximum Entropy\nDiscrimination (CMV-MED) algorithm is proposed. By iteratively maximizing the\nstochastic agreement between multiple classifiers on the unlabeled dataset, the\nalgorithm simultaneously learns multiple high accuracy classifiers. We\ndemonstrate that our proposed method can yield improved performance over\nprevious multi-view learning approaches by comparing performance on three real\nmulti-sensor data sets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 20:23:22 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Xie", "Tianpei", ""], ["Nasrabadi", "Nasser M.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1507.01384", "submitter": "Christopher A. Tucker", "authors": "Christopher A. Tucker", "title": "The method of artificial systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This document is written with the intention to describe in detail a method\nand means by which a computer program can reason about the world and in so\ndoing, increase its analogue to a living system. As the literature is rife and\nit is apparent we, as scientists and engineers, have not found the solution,\nthis document will attempt the solution by grounding its intellectual arguments\nwithin tenets of human cognition in Western philosophy. The result will be a\ncharacteristic description of a method to describe an artificial system\nanalogous to that performed for a human. The approach was the substance of my\nMaster's thesis, explored more deeply during the course of my postdoc research.\nIt focuses primarily on context awareness and choice set within a boundary of\navailable epistemology, which serves to describe it. Expanded upon, such a\ndescription strives to discover agreement with Kant's critique of reason to\nunderstand how it could be applied to define the architecture of its design.\nThe intention has never been to mimic human or biological systems, rather, to\nunderstand the profoundly fundamental rules, when leveraged correctly, results\nin an artificial consciousness as noumenon while in keeping with the perception\nof it as phenomenon.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 10:52:08 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 13:37:02 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Tucker", "Christopher A.", ""]]}, {"id": "1507.01425", "submitter": "Ryuta Arisaka", "authors": "Ryuta Arisaka", "title": "Latent Belief Theory and Belief Dependencies: A Solution to the Recovery\n  Problem in the Belief Set Theories", "comments": "Corrected the following: 1. in Definition 1, earlier versions had\n  2^Props x 2^Props x N, but clearly it should be 2^{Props x Props x N}. 2. in\n  Definition 1, one disjunctive case was missing. The 5th item is newly added\n  to complete. 3. On page 3, in the right column, the 2nd axiom for Compactness\n  has a typo. It is not P \\in X, but should be P \\in L(X)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AGM recovery postulate says: assume a set of propositions X; assume that\nit is consistent and that it is closed under logical consequences; remove a\nbelief P from the set minimally, but make sure that the resultant set is again\nsome set of propositions X' which is closed under the logical consequences; now\nadd P again and close the set under the logical consequences; and we should get\na set of propositions that contains all the propositions that were in X. This\npostulate has since met objections; many have observed that it could bear\ncounter-intuitive results. Nevertheless, the attempts that have been made so\nfar to amend it either recovered the postulate in full, had to relinquish the\nassumption of the logical closure altogether, or else had to introduce fresh\ncontroversies of their own. We provide a solution to the recovery paradox in\nthis work. Our theoretical basis is the recently proposed belief theory with\nlatent beliefs (simply the latent belief theory for short). Firstly, through\nexamples, we will illustrate that the vanilla latent belief theory can be made\nmore expressive. We will identify that a latent belief, when it becomes\nvisible, may remain visible only while the beliefs that triggered it into the\nagent's consciousness are in the agent's belief set. In order that such\nsituations can be also handled, we will enrich the latent belief theory with\nbelief dependencies among attributive beliefs, recording the information as to\nwhich belief is supported of its existence by which beliefs. We will show that\nthe enriched latent belief theory does not possess the recovery property. The\nclosure by logical consequences is maintained in the theory, however. Hence it\nserves as a solution to the open problem in the belief set theories.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 12:48:59 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 16:59:42 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2015 04:13:51 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2016 03:03:43 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Arisaka", "Ryuta", ""]]}, {"id": "1507.01451", "submitter": "Peter Sch\\\"uller", "authors": "Thomas Eiter, Michael Fink, Giovambattista Ianni, Thomas Krennwallner,\n  Christoph Redl, Peter Sch\\\"uller", "title": "A model building framework for Answer Set Programming with external\n  computations", "comments": "57 pages, 9 figures, 3 tables, 6 algorithms, to appear in Theory and\n  Practice of Logic Programming (accepted in June 2015)", "journal-ref": "Theory and Practice of Logic Programming 16 (2016) 418-464", "doi": "10.1017/S1471068415000113", "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As software systems are getting increasingly connected, there is a need for\nequipping nonmonotonic logic programs with access to external sources that are\npossibly remote and may contain information in heterogeneous formats. To cater\nfor this need, HEX programs were designed as a generalization of answer set\nprograms with an API style interface that allows to access arbitrary external\nsources, providing great flexibility. Efficient evaluation of such programs\nhowever is challenging, and it requires to interleave external computation and\nmodel building; to decide when to switch between these tasks is difficult, and\nexisting approaches have limited scalability in many real-world application\nscenarios. We present a new approach for the evaluation of logic programs with\nexternal source access, which is based on a configurable framework for dividing\nthe non-ground program into possibly overlapping smaller parts called\nevaluation units. The latter will be processed by interleaving external\nevaluation and model building using an evaluation graph and a model graph,\nrespectively, and by combining intermediate results. Experiments with our\nprototype implementation show a significant improvement compared to previous\napproaches. While designed for HEX-programs, the new evaluation approach may be\ndeployed to related rule-based formalisms as well.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 13:31:39 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2015 22:20:15 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Eiter", "Thomas", ""], ["Fink", "Michael", ""], ["Ianni", "Giovambattista", ""], ["Krennwallner", "Thomas", ""], ["Redl", "Christoph", ""], ["Sch\u00fcller", "Peter", ""]]}, {"id": "1507.01569", "submitter": "Ashique Rupam Mahmood", "authors": "A. Rupam Mahmood, Huizhen Yu, Martha White, Richard S. Sutton", "title": "Emphatic Temporal-Difference Learning", "comments": "9 pages, accepted for presentation at European Workshop on\n  Reinforcement Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emphatic algorithms are temporal-difference learning algorithms that change\ntheir effective state distribution by selectively emphasizing and\nde-emphasizing their updates on different time steps. Recent works by Sutton,\nMahmood and White (2015), and Yu (2015) show that by varying the emphasis in a\nparticular way, these algorithms become stable and convergent under off-policy\ntraining with linear function approximation. This paper serves as a unified\nsummary of the available results from both works. In addition, we demonstrate\nthe empirical benefits from the flexibility of emphatic algorithms, including\nstate-dependent discounting, state-dependent bootstrapping, and the\nuser-specified allocation of function approximation resources.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 19:28:36 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Mahmood", "A. Rupam", ""], ["Yu", "Huizhen", ""], ["White", "Martha", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1507.01731", "submitter": "Kumar Sankar Ray", "authors": "Kumar Sankar Ray and Mandrita Mondal", "title": "Prediction of Radiation Fog by DNA Computing", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.AI cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a wet lab algorithm for prediction of radiation fog\nby DNA computing. The concept of DNA computing is essentially exploited for\ngenerating the classifier algorithm in the wet lab. The classifier is based on\na new concept of similarity based fuzzy reasoning suitable for wet lab\nimplementation. This new concept of similarity based fuzzy reasoning is\ndifferent from conventional approach to fuzzy reasoning based on similarity\nmeasure and also replaces the logical aspect of classical fuzzy reasoning by\nDNA chemistry. Thus, we add a new dimension to existing forms of fuzzy\nreasoning by bringing it down to nanoscale. We exploit the concept of massive\nparallelism of DNA computing by designing this new classifier in the wet lab.\nThis newly designed classifier is very much generalized in nature and apart\nfrom prediction of radiation fog this methodology can be applied to other types\nof data also. To achieve our goal we first fuzzify the given observed\nparameters in a form of synthetic DNA sequence which is called fuzzy DNA and\nwhich handles the vague concept of human reasoning.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 09:57:44 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Ray", "Kumar Sankar", ""], ["Mondal", "Mandrita", ""]]}, {"id": "1507.01839", "submitter": "Mingbo Ma", "authors": "Mingbo Ma and Liang Huang and Bing Xiang and Bowen Zhou", "title": "Dependency-based Convolutional Neural Networks for Sentence Embedding", "comments": "this paper has been accepted by ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sentence modeling and classification, convolutional neural network\napproaches have recently achieved state-of-the-art results, but all such\nefforts process word vectors sequentially and neglect long-distance\ndependencies. To exploit both deep learning and linguistic structures, we\npropose a tree-based convolutional neural network model which exploit various\nlong-distance relationships between words. Our model improves the sequential\nbaselines on all three sentiment and question classification tasks, and\nachieves the highest published accuracy on TREC.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 15:20:36 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 15:36:45 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Ma", "Mingbo", ""], ["Huang", "Liang", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1507.01986", "submitter": "Nathaniel Soares", "authors": "Nate Soares and Benja Fallenstein", "title": "Toward Idealized Decision Theory", "comments": "This is an extended version of a paper accepted to AGI-2015", "journal-ref": null, "doi": null, "report-no": "2014-7", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper motivates the study of decision theory as necessary for aligning\nsmarter-than-human artificial systems with human interests. We discuss the\nshortcomings of two standard formulations of decision theory, and demonstrate\nthat they cannot be used to describe an idealized decision procedure suitable\nfor approximation by artificial systems. We then explore the notions of policy\nselection and logical counterfactuals, two recent insights into decision theory\nthat point the way toward promising paths for future research.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 23:06:59 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Soares", "Nate", ""], ["Fallenstein", "Benja", ""]]}, {"id": "1507.02020", "submitter": "Thierry Poibeau", "authors": "Thierry Poibeau (LaTTICe), Pablo Ruiz (LaTTICe)", "title": "Generating Navigable Semantic Maps from Social Sciences Corpora", "comments": "in Digital Humanities 2015, Jun 2015, Sydney, Australia. Actes de la\n  Conf{\\'e}rence Digital Humanities 2015. arXiv admin note: text overlap with\n  arXiv:1406.4211", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now commonplace to observe that we are facing a deluge of online\ninformation. Researchers have of course long acknowledged the potential value\nof this information since digital traces make it possible to directly observe,\ndescribe and analyze social facts, and above all the co-evolution of ideas and\ncommunities over time. However, most online information is expressed through\ntext, which means it is not directly usable by machines, since computers\nrequire structured, organized and typed information in order to be able to\nmanipulate it. Our goal is thus twofold: 1. Provide new natural language\nprocessing techniques aiming at automatically extracting relevant information\nfrom texts, especially in the context of social sciences, and connect these\npieces of information so as to obtain relevant socio-semantic networks; 2.\nProvide new ways of exploring these socio-semantic networks, thanks to tools\nallowing one to dynamically navigate these networks, de-construct and\nre-construct them interactively, from different points of view following the\nneeds expressed by domain experts.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 04:27:48 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Poibeau", "Thierry", "", "LaTTICe"], ["Ruiz", "Pablo", "", "LaTTICe"]]}, {"id": "1507.02021", "submitter": "Thierry Poibeau", "authors": "Fr\\'ed\\'erique M\\'elanie-Becquet (LaTTICe), Johan Ferguth (LaTTICe),\n  Katherine Gruel, Thierry Poibeau (LaTTICe)", "title": "Archaeology in the Digital Age: From Paper to Databases", "comments": "Digital Humanities 2015, Jun 2015, Sydney, Australia. 2015,\n  Proceedings of the conference \"Digital Humanities 2015\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research units in archaeology often manage large and precious archives\ncontaining various documents, including reports on fieldwork, scholarly studies\nand reference books. These archives are of course invaluable, recording decades\nof work, but are generally hard to consult and access. In this context,\ndigitizing full text documents is not enough: information must be formalized,\nstructured and easy to access thanks to friendly user interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 04:28:18 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["M\u00e9lanie-Becquet", "Fr\u00e9d\u00e9rique", "", "LaTTICe"], ["Ferguth", "Johan", "", "LaTTICe"], ["Gruel", "Katherine", "", "LaTTICe"], ["Poibeau", "Thierry", "", "LaTTICe"]]}, {"id": "1507.02084", "submitter": "Iago Landesa-V\\'azquez", "authors": "Iago Landesa-V\\'azquez, Jos\\'e Luis Alba-Castro", "title": "Shedding Light on the Asymmetric Learning Capability of AdaBoost", "comments": null, "journal-ref": "Pattern Recognition Letters 33 (2012) 247-255", "doi": "10.1016/j.patrec.2011.10.022", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a different insight to analyze AdaBoost. This\nanalysis reveals that, beyond some preconceptions, AdaBoost can be directly\nused as an asymmetric learning algorithm, preserving all its theoretical\nproperties. A novel class-conditional description of AdaBoost, which models the\nactual asymmetric behavior of the algorithm, is presented.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 09:58:06 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Landesa-V\u00e1zquez", "Iago", ""], ["Alba-Castro", "Jos\u00e9 Luis", ""]]}, {"id": "1507.02086", "submitter": "Shashishekar Ramakrishna", "authors": "Shashishekar Ramakrishna and Lukasz Gorski and Adrian Paschke", "title": "The Role of Pragmatics in Legal Norm Representation", "comments": "International Workshop On Legal Domain And Semantic Web Applications\n  (LeDA-SWAn 2015), held during the 12th Extended Semantic Web Conference (ESWC\n  2015), June 1, 2015, Portoroz, Slovenia. in CEUR Workshop Proceedings 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the 'apparent clarity' of a given legal provision, its application\nmay result in an outcome that does not exactly conform to the semantic level of\na statute. The vagueness within a legal text is induced intentionally to\naccommodate all possible scenarios under which such norms should be applied,\nthus making the role of pragmatics an important aspect also in the\nrepresentation of a legal norm and reasoning on top of it. The notion of\npragmatics considered in this paper does not focus on the aspects associated\nwith judicial decision making. The paper aims to shed light on the aspects of\npragmatics in legal linguistics, mainly focusing on the domain of patent law,\nonly from a knowledge representation perspective. The philosophical discussions\npresented in this paper are grounded based on the legal theories from Grice and\nMarmor.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 10:04:14 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Ramakrishna", "Shashishekar", ""], ["Gorski", "Lukasz", ""], ["Paschke", "Adrian", ""]]}, {"id": "1507.02139", "submitter": "Giuseppe Carbone Dr.", "authors": "Giuseppe Carbone, Ilaria Giannoccaro", "title": "Model of human collective decision-making in complex environments", "comments": "12 pages, 8 figues in European Physical Journal B, 2015", "journal-ref": "European Physical Journal B, 88 (12), 339, 2015", "doi": "10.1140/epjb/e2015-60609-0", "report-no": null, "categories": "cs.MA cs.AI nlin.AO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A continuous-time Markov process is proposed to analyze how a group of humans\nsolves a complex task, consisting in the search of the optimal set of decisions\non a fitness landscape. Individuals change their opinions driven by two\ndifferent forces: (i) the self-interest, which pushes them to increase their\nown fitness values, and (ii) the social interactions, which push individuals to\nreduce the diversity of their opinions in order to reach consensus. Results\nshow that the performance of the group is strongly affected by the strength of\nsocial interactions and by the level of knowledge of the individuals.\nIncreasing the strength of social interactions improves the performance of the\nteam. However, too strong social interactions slow down the search of the\noptimal solution and worsen the performance of the group. In particular, we\nfind that the threshold value of the social interaction strength, which leads\nto the emergence of a superior intelligence of the group, is just the critical\nthreshold at which the consensus among the members sets in. We also prove that\na moderate level of knowledge is already enough to guarantee high performance\nof the group in making decisions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 13:14:16 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 15:06:52 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Carbone", "Giuseppe", ""], ["Giannoccaro", "Ilaria", ""]]}, {"id": "1507.02154", "submitter": "Iago Landesa-V\\'azquez", "authors": "Iago Landesa-V\\'azquez, Jos\\'e Luis Alba-Castro", "title": "Double-Base Asymmetric AdaBoost", "comments": null, "journal-ref": "Neurocomputing 118 (2013) 101-114", "doi": "10.1016/j.neucom.2013.02.019", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the use of different exponential bases to define class-dependent\nerror bounds, a new and highly efficient asymmetric boosting scheme, coined as\nAdaBoostDB (Double-Base), is proposed. Supported by a fully theoretical\nderivation procedure, unlike most of the other approaches in the literature,\nour algorithm preserves all the formal guarantees and properties of original\n(cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-Sensitive\nAdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novel\nderivation scheme enables an extremely efficient conditional search procedure,\ndramatically improving and simplifying the training phase of the algorithm.\nExperiments, both over synthetic and real datasets, reveal that AdaBoostDB is\nable to save over 99% training time with regard to Cost-Sensitive AdaBoost,\nproviding the same cost-sensitive results. This computational advantage of\nAdaBoostDB can make a difference in problems managing huge pools of weak\nclassifiers in which boosting techniques are commonly used.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 13:44:34 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Landesa-V\u00e1zquez", "Iago", ""], ["Alba-Castro", "Jos\u00e9 Luis", ""]]}, {"id": "1507.02347", "submitter": "Jungsik Hwang", "authors": "Jungsik Hwang, Minju Jung, Naveen Madapana, Jinhyung Kim, Minkyu Choi\n  and Jun Tani", "title": "Achieving Synergy in Cognitive Behavior of Humanoids via Deep Learning\n  of Dynamic Visuo-Motor-Attentional Coordination", "comments": "submitted to 2015 IEEE-RAS International Conference on Humanoid\n  Robots", "journal-ref": null, "doi": "10.1109/HUMANOIDS.2015.7363448", "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current study examines how adequate coordination among different\ncognitive processes including visual recognition, attention switching, action\npreparation and generation can be developed via learning of robots by\nintroducing a novel model, the Visuo-Motor Deep Dynamic Neural Network (VMDNN).\nThe proposed model is built on coupling of a dynamic vision network, a motor\ngeneration network, and a higher level network allocated on top of these two.\nThe simulation experiments using the iCub simulator were conducted for\ncognitive tasks including visual object manipulation responding to human\ngestures. The results showed that synergetic coordination can be developed via\niterative learning through the whole network when spatio-temporal hierarchy and\ntemporal one can be self-organized in the visual pathway and in the motor\npathway, respectively, such that the higher level can manipulate them with\nabstraction.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 02:10:03 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Hwang", "Jungsik", ""], ["Jung", "Minju", ""], ["Madapana", "Naveen", ""], ["Kim", "Jinhyung", ""], ["Choi", "Minkyu", ""], ["Tani", "Jun", ""]]}, {"id": "1507.02439", "submitter": "Manish Joshi", "authors": "Oludayo O. Olugbara, Manish Joshi, Michael M. Modiba, and\n  Virendrakumar C. Bhavsar", "title": "Automated Matchmaking to Improve Accuracy of Applicant Selection for\n  University Education System", "comments": "14 pages, 5 Text boxes, 2 tables and a figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate applicant selection for university education is imperative to\nensure fairness and optimal use of institutional resources. Although various\napproaches are operational in tertiary educational institutions for selecting\napplicants, a novel method of automated matchmaking is explored in the current\nstudy. The method functions by matching a prospective students skills profile\nto a programmes requisites profile.\n  Empirical comparisons of the results, calculated by automated matchmaking and\ntwo other selection methods, show matchmaking to be a viable alternative for\naccurate selection of applicants. Matchmaking offers a unique advantage that it\nneither requires data from other applicants nor compares applicants with each\nother. Instead, it emphasises norms that define admissibility to a programme.\n  We have proposed the use of technology to minimize the gap between students\naspirations, skill sets and course requirements. It is a solution to minimize\nthe number of students who get frustrated because of mismatched course\nselection.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 10:00:36 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Olugbara", "Oludayo O.", ""], ["Joshi", "Manish", ""], ["Modiba", "Michael M.", ""], ["Bhavsar", "Virendrakumar C.", ""]]}, {"id": "1507.02456", "submitter": "Melisachew Wudage Chekol", "authors": "Melisachew Wudage Chekol and Jakob Huber and Heiner Stuckenschmidt", "title": "Towards Log-Linear Logics with Concrete Domains", "comments": "StarAI2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present $\\mathcal{MEL}^{++}$ (M denotes Markov logic networks) an\nextension of the log-linear description logics $\\mathcal{EL}^{++}$-LL with\nconcrete domains, nominals, and instances. We use Markov logic networks (MLNs)\nin order to find the most probable, classified and coherent $\\mathcal{EL}^{++}$\nontology from an $\\mathcal{MEL}^{++}$ knowledge base. In particular, we develop\na novel way to deal with concrete domains (also known as datatypes) by\nextending MLN's cutting plane inference (CPI) algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 11:02:38 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 08:29:23 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Chekol", "Melisachew Wudage", ""], ["Huber", "Jakob", ""], ["Stuckenschmidt", "Heiner", ""]]}, {"id": "1507.02563", "submitter": "Wen Shen", "authors": "Wen Shen and Cristina Lopes", "title": "Managing Autonomous Mobility on Demand Systems for Better Passenger\n  Experience", "comments": null, "journal-ref": "Proceedings of the 18th International Conference on Principles and\n  Practice of Multi-Agent Systems (PRIMA 2015). pp 20-35. Lecture Notes in\n  Computer Science, vol 9387. Springer", "doi": "10.1007/978-3-319-25524-8_2", "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous mobility on demand systems, though still in their infancy, have\nvery promising prospects in providing urban population with sustainable and\nsafe personal mobility in the near future. While much research has been\nconducted on both autonomous vehicles and mobility on demand systems, to the\nbest of our knowledge, this is the first work that shows how to manage\nautonomous mobility on demand systems for better passenger experience. We\nintroduce the Expand and Target algorithm which can be easily integrated with\nthree different scheduling strategies for dispatching autonomous vehicles. We\nimplement an agent-based simulation platform and empirically evaluate the\nproposed approaches with the New York City taxi data. Experimental results\ndemonstrate that the algorithm significantly improve passengers' experience by\nreducing the average passenger waiting time by up to 29.82% and increasing the\ntrip success rate by up to 7.65%.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 15:43:17 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Shen", "Wen", ""], ["Lopes", "Cristina", ""]]}, {"id": "1507.02870", "submitter": "Khalid Raza", "authors": "Khalid Raza", "title": "Analysis of Microarray Data using Artificial Intelligence Based\n  Techniques", "comments": "32 pages, 4 figures", "journal-ref": "Handbook of Research on Computational Intelligence Applications in\n  Bioinformatics, 216-239, 2016", "doi": "10.4018/978-1-5225-0427-6.ch011", "report-no": null, "categories": "cs.AI cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray is one of the essential technologies used by the biologist to\nmeasure genome-wide expression levels of genes in a particular organism under\nsome particular conditions or stimuli. As microarrays technologies have become\nmore prevalent, the challenges of analyzing these data for getting better\ninsight about biological processes have essentially increased. Due to\navailability of artificial intelligence based sophisticated computational\ntechniques, such as artificial neural networks, fuzzy logic, genetic\nalgorithms, and many other nature-inspired algorithms, it is possible to\nanalyse microarray gene expression data in more better way. Here, we reviewed\nartificial intelligence based techniques for the analysis of microarray gene\nexpression data. Further, challenges in the field and future work direction\nhave also been suggested.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 12:25:58 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Raza", "Khalid", ""]]}, {"id": "1507.02873", "submitter": "Joris Renkens", "authors": "Joris Renkens and Angelika Kimmig and Luc De Raedt", "title": "Lazy Explanation-Based Approximation for Probabilistic Logic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a lazy approach to the explanation-based approximation of\nprobabilistic logic programs. It uses only the most significant part of the\nprogram when searching for explanations. The result is a fast and anytime\napproximate inference algorithm which returns hard lower and upper bounds on\nthe exact probability. We experimentally show that this method outperforms\nstate-of-the-art approximate inference.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 12:29:47 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Renkens", "Joris", ""], ["Kimmig", "Angelika", ""], ["De Raedt", "Luc", ""]]}, {"id": "1507.02912", "submitter": "James Cussens", "authors": "James Cussens", "title": "First-order integer programming for MAP problems", "comments": "corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the most probable (MAP) model in SRL frameworks such as Markov logic\nand Problog can, in principle, be solved by encoding the problem as a\n`grounded-out' mixed integer program (MIP). However, useful first-order\nstructure disappears in this process motivating the development of first-order\nMIP approaches. Here we present mfoilp, one such approach. Since the syntax and\nsemantics of mfoilp is essentially the same as existing approaches we focus\nhere mainly on implementation and algorithmic issues. We start with the\n(conceptually) simple problem of using a logic program to generate a MIP\ninstance before considering more ambitious exploitation of first-order\nrepresentations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 14:13:31 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 05:48:01 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Cussens", "James", ""]]}, {"id": "1507.03045", "submitter": "Tushar Khot", "authors": "Tushar Khot, Niranjan Balasubramanian, Eric Gribkoff, Ashish\n  Sabharwal, Peter Clark, Oren Etzioni", "title": "Markov Logic Networks for Natural Language Question Answering", "comments": "7 pages, 1 figure, StarAI workshop at UAI'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to answer elementary-level science questions using knowledge\nextracted automatically from science textbooks, expressed in a subset of\nfirst-order logic. Given the incomplete and noisy nature of these automatically\nextracted rules, Markov Logic Networks (MLNs) seem a natural model to use, but\nthe exact way of leveraging MLNs is by no means obvious. We investigate three\nways of applying MLNs to our task. In the first, we simply use the extracted\nscience rules directly as MLN clauses. Unlike typical MLN applications, our\ndomain has long and complex rules, leading to an unmanageable number of\ngroundings. We exploit the structure present in hard constraints to improve\ntractability, but the formulation remains ineffective. In the second approach,\nwe instead interpret science rules as describing prototypical entities, thus\nmapping rules directly to grounded MLN assertions, whose constants are then\nclustered using existing entity resolution methods. This drastically simplifies\nthe network, but still suffers from brittleness. Finally, our third approach,\ncalled Praline, uses MLNs to align the lexical elements as well as define and\ncontrol how inference should be performed in this task. Our experiments,\ndemonstrating a 15\\% accuracy boost and a 10x reduction in runtime, suggest\nthat the flexibility and different inference semantics of Praline are a better\nfit for the natural language question answering task.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 23:17:53 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Khot", "Tushar", ""], ["Balasubramanian", "Niranjan", ""], ["Gribkoff", "Eric", ""], ["Sabharwal", "Ashish", ""], ["Clark", "Peter", ""], ["Etzioni", "Oren", ""]]}, {"id": "1507.03097", "submitter": "Shangpu Jiang", "authors": "Shangpu Jiang, Daniel Lowd, Dejing Dou", "title": "Ontology Matching with Knowledge Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology matching is the process of automatically determining the semantic\nequivalences between the concepts of two ontologies. Most ontology matching\nalgorithms are based on two types of strategies: terminology-based strategies,\nwhich align concepts based on their names or descriptions, and structure-based\nstrategies, which exploit concept hierarchies to find the alignment. In many\ndomains, there is additional information about the relationships of concepts\nrepresented in various ways, such as Bayesian networks, decision trees, and\nassociation rules. We propose to use the similarities between these\nrelationships to find more accurate alignments. We accomplish this by defining\nsoft constraints that prefer alignments where corresponding concepts have the\nsame local relationships encoded as knowledge rules. We use a probabilistic\nframework to integrate this new knowledge-based strategy with standard\nterminology-based and structure-based strategies. Furthermore, our method is\nparticularly effective in identifying correspondences between complex concepts.\nOur method achieves substantially better F-score than the previous\nstate-of-the-art on three ontology matching domains.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 11:19:36 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Jiang", "Shangpu", ""], ["Lowd", "Daniel", ""], ["Dou", "Dejing", ""]]}, {"id": "1507.03168", "submitter": "Pablo Robles-Granda", "authors": "Pablo Robles-Granda and Sebastian Moreno and Jennifer Neville", "title": "Using Bayesian Network Representations for Effective Sampling from\n  Generative Network Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks (BNs) are used for inference and sampling by exploiting\nconditional independence among random variables. Context specific independence\n(CSI) is a property of graphical models where additional independence relations\narise in the context of particular values of random variables (RVs).\nIdentifying and exploiting CSI properties can simplify inference. Some\ngenerative network models (models that generate social/information network\nsamples from a network distribution P(G)), with complex interactions among a\nset of RVs, can be represented with probabilistic graphical models, in\nparticular with BNs. In the present work we show one such a case. We discuss\nhow a mixed Kronecker Product Graph Model can be represented as a BN, and study\nits BN properties that can be used for efficient sampling. Specifically, we\nshow that instead of exhibiting CSI properties, the model has deterministic\ncontext-specific dependence (DCSD). Exploiting this property focuses the\nsampling method on a subset of the sampling space that improves efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 23:10:17 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Robles-Granda", "Pablo", ""], ["Moreno", "Sebastian", ""], ["Neville", "Jennifer", ""]]}, {"id": "1507.03181", "submitter": "Shangpu Jiang", "authors": "Shangpu Jiang, Daniel Lowd, Dejing Dou", "title": "A Probabilistic Approach to Knowledge Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on a novel knowledge reuse scenario where the\nknowledge in the source schema needs to be translated to a semantically\nheterogeneous target schema. We refer to this task as \"knowledge translation\"\n(KT). Unlike data translation and transfer learning, KT does not require any\ndata from the source or target schema. We adopt a probabilistic approach to KT\nby representing the knowledge in the source schema, the mapping between the\nsource and target schemas, and the resulting knowledge in the target schema all\nas probability distributions, specially using Markov random fields and Markov\nlogic networks. Given the source knowledge and mappings, we use standard\nlearning and inference algorithms for probabilistic graphical models to find an\nexplicit probability distribution in the target schema that minimizes the\nKullback-Leibler divergence from the implicit distribution. This gives us a\ncompact probabilistic model that represents knowledge from the source schema as\nwell as possible, respecting the uncertainty in both the source knowledge and\nthe mapping. In experiments on both propositional and relational domains, we\nfind that the knowledge obtained by KT is comparable to other approaches that\nrequire data, demonstrating that knowledge can be reused without data.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 03:24:21 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Jiang", "Shangpu", ""], ["Lowd", "Daniel", ""], ["Dou", "Dejing", ""]]}, {"id": "1507.03257", "submitter": "Michael Gr. Voskoglou Prof. Dr.", "authors": "Michael Voskoglou", "title": "Use of the Triangular Fuzzy Numbers for Student Assessment", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an earlier work we have used the Triangular Fuzzy Numbers (TFNs)as an\nassessment tool of student skills.This approach led to an approximate\nlinguistic characterization of the students' overall performance, but it was\nnot proved to be sufficient in all cases for comparing the performance of two\ndifferent student groups, since tywo TFNs are not always comparable. In the\npresent paper we complete the above fuzzy assessment approach by presenting a\ndefuzzification method of TFNS based on the Center of Gravity (COG) technique,\nwhich enables the required comparison. In addition we extend our results by\nusing the Trapezoidal Fuzzy Numbers (TpFNs) too, which are a generalization of\nthe TFNs, for student assessment and we present suitable examples illustrating\nour new results in practice.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 17:57:50 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2015 22:27:49 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Voskoglou", "Michael", ""]]}, {"id": "1507.03638", "submitter": "Giuseppe Tommaso Costanzo", "authors": "Giuseppe Tommaso Costanzo, Sandro Iacovella, Frederik Ruelens, T.\n  Leurs and Bert Claessens", "title": "Experimental analysis of data-driven control for a building heating\n  system", "comments": "12 pages, 8 figures, pending for publication in Elsevier SEGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the opportunity to harvest the flexibility related to building\nclimate control for demand response applications, this work presents a\ndata-driven control approach building upon recent advancements in reinforcement\nlearning. More specifically, model assisted batch reinforcement learning is\napplied to the setting of building climate control subjected to a dynamic\npricing. The underlying sequential decision making problem is cast on a markov\ndecision problem, after which the control algorithm is detailed. In this work,\nfitted Q-iteration is used to construct a policy from a batch of experimental\ntuples. In those regions of the state space where the experimental sample\ndensity is low, virtual support samples are added using an artificial neural\nnetwork. Finally, the resulting policy is shaped using domain knowledge. The\ncontrol approach has been evaluated quantitatively using a simulation and\nqualitatively in a living lab. From the quantitative analysis it has been found\nthat the control approach converges in approximately 20 days to obtain a\ncontrol policy with a performance within 90% of the mathematical optimum. The\nexperimental analysis confirms that within 10 to 20 days sensible policies are\nobtained that can be used for different outside temperature regimes.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 22:19:41 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 21:43:03 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Costanzo", "Giuseppe Tommaso", ""], ["Iacovella", "Sandro", ""], ["Ruelens", "Frederik", ""], ["Leurs", "T.", ""], ["Claessens", "Bert", ""]]}, {"id": "1507.03663", "submitter": "Frederic Maris", "authors": "Khaled Skander Ben Slimane, Alexis Comte, Olivier Gasquet, Abdelwahab\n  Heba, Olivier Lezaud, Frederic Maris and Mael Valais", "title": "Twist your logic with TouIST", "comments": "Proceedings of the Fourth International Conference on Tools for\n  Teaching Logic (TTL2015), Rennes, France, June 9-12, 2015. Editors: M.\n  Antonia Huertas, Jo\\~ao Marcos, Mar\\'ia Manzano, Sophie Pinchinat,\n  Fran\\c{c}ois Schwarzentruber", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SAT provers are powerful tools for solving real-sized logic problems, but\nusing them requires solid programming knowledge and may be seen w.r.t.\\ logic\nlike assembly language w.r.t.\\ programming. Something like a high level\nlanguage was missing to ease various users to take benefit of these tools. {\\sc\n\\texttt {TouIST}}\\ aims at filling this gap. It is devoted to propositional\nlogic and its main features are 1) to offer a high-level logic langage for\nexpressing succintly complex formulas (e.g.\\ formulas describing Sudoku rules,\nplanification problems,\\ldots) and 2) to find models to these formulas by using\nthe adequate powerful prover, which the user has no need to know about. It\nconsists in a friendly interface that offers several syntactic facilities and\nwhich is connected with some sufficiently powerful provers allowing to\nautomatically solve big instances of difficult problems (such as time-tables or\nSudokus). It can interact with various provers: pure SAT solver but also SMT\nprovers (SAT modulo theories - like linear theory of reals, etc) and thus may\nalso be used by beginners for experiencing with pure propositional problems up\nto graduate students or even researchers for solving planification problems\ninvolving big sets of fluents and numerical constraints on them.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 00:47:36 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Slimane", "Khaled Skander Ben", ""], ["Comte", "Alexis", ""], ["Gasquet", "Olivier", ""], ["Heba", "Abdelwahab", ""], ["Lezaud", "Olivier", ""], ["Maris", "Frederic", ""], ["Valais", "Mael", ""]]}, {"id": "1507.03719", "submitter": "Huy Nguyen", "authors": "Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, Justin Ward", "title": "A New Framework for Distributed Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of problems in machine learning, including exemplar\nclustering, document summarization, and sensor placement, can be cast as\nconstrained submodular maximization problems. A lot of recent effort has been\ndevoted to developing distributed algorithms for these problems. However, these\nresults suffer from high number of rounds, suboptimal approximation ratios, or\nboth. We develop a framework for bringing existing algorithms in the sequential\nsetting to the distributed setting, achieving near optimal approximation ratios\nfor many settings in only a constant number of MapReduce rounds. Our techniques\nalso give a fast sequential algorithm for non-monotone maximization subject to\na matroid constraint.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 04:46:01 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 21:20:02 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Barbosa", "Rafael da Ponte", ""], ["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""], ["Ward", "Justin", ""]]}, {"id": "1507.03920", "submitter": "Mario Alviano", "authors": "Mario Alviano and Rafael Penaloza", "title": "Fuzzy Answer Set Computation via Satisfiability Modulo Theories", "comments": null, "journal-ref": "Theory and Practice of Logic Programming 15 (2015) 588-603", "doi": "10.1017/S1471068415000241", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy answer set programming (FASP) combines two declarative frameworks,\nanswer set programming and fuzzy logic, in order to model reasoning by default\nover imprecise information. Several connectives are available to combine\ndifferent expressions; in particular the \\Godel and \\Luka fuzzy connectives are\nusually considered, due to their properties. Although the \\Godel conjunction\ncan be easily eliminated from rule heads, we show through complexity arguments\nthat such a simplification is infeasible in general for all other connectives.\n%, even if bodies are restricted to \\Luka or \\Godel conjunctions. The paper\nanalyzes a translation of FASP programs into satisfiability modulo\ntheories~(SMT), which in general produces quantified formulas because of the\nminimality of the semantics. Structural properties of many FASP programs allow\nto eliminate the quantification, or to sensibly reduce the number of quantified\nvariables. Indeed, integrality constraints can replace recursive rules commonly\nused to force Boolean interpretations, and completion subformulas can guarantee\nminimality for acyclic programs with atomic heads. Moreover, head cycle free\nrules can be replaced by shifted subprograms, whose structure depends on the\neliminated head connective, so that ordered completion may replace the\nminimality check if also \\Luka disjunction in rule bodies is acyclic. The paper\nalso presents and evaluates a prototype system implementing these translations.\n  To appear in Theory and Practice of Logic Programming (TPLP), Proceedings of\nICLP 2015.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 16:52:05 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Alviano", "Mario", ""], ["Penaloza", "Rafael", ""]]}, {"id": "1507.03922", "submitter": "Mario Alviano", "authors": "Mario Alviano and Nicola Leone", "title": "Complexity and Compilation of GZ-Aggregates in Answer Set Programming", "comments": null, "journal-ref": "Theory and Practice of Logic Programming 15 (2015) 574-587", "doi": "10.1017/S147106841500023X", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gelfond and Zhang recently proposed a new stable model semantics based on\nVicious Circle Principle in order to improve the interpretation of logic\nprograms with aggregates. The paper focuses on this proposal, and analyzes the\ncomplexity of both coherence testing and cautious reasoning under the new\nsemantics. Some surprising results highlight similarities and differences\nversus mainstream stable model semantics for aggregates. Moreover, the paper\nreports on the design of compilation techniques for implementing the new\nsemantics on top of existing ASP solvers, which eventually lead to realize a\nprototype system that allows for experimenting with Gelfond-Zhang's aggregates.\n  To appear in Theory and Practice of Logic Programming (TPLP), Proceedings of\nICLP 2015.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 16:54:36 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Alviano", "Mario", ""], ["Leone", "Nicola", ""]]}, {"id": "1507.03923", "submitter": "Mario Alviano", "authors": "Mario Alviano and Wolfgang Faber and Martin Gebser", "title": "Rewriting recursive aggregates in answer set programming: back to\n  monotonicity", "comments": null, "journal-ref": "Theory and Practice of Logic Programming 15 (2015) 559-573", "doi": "10.1017/S1471068415000228", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregation functions are widely used in answer set programming for\nrepresenting and reasoning on knowledge involving sets of objects collectively.\nCurrent implementations simplify the structure of programs in order to optimize\nthe overall performance. In particular, aggregates are rewritten into simpler\nforms known as monotone aggregates. Since the evaluation of normal programs\nwith monotone aggregates is in general on a lower complexity level than the\nevaluation of normal programs with arbitrary aggregates, any faithful\ntranslation function must introduce disjunction in rule heads in some cases.\nHowever, no function of this kind is known. The paper closes this gap by\nintroducing a polynomial, faithful, and modular translation for rewriting\ncommon aggregation functions into the simpler form accepted by current solvers.\nA prototype system allows for experimenting with arbitrary recursive\naggregates, which are also supported in the recent version 4.5 of the grounder\n\\textsc{gringo}, using the methods presented in this paper.\n  To appear in Theory and Practice of Logic Programming (TPLP), Proceedings of\nICLP 2015.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 16:57:33 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Alviano", "Mario", ""], ["Faber", "Wolfgang", ""], ["Gebser", "Martin", ""]]}, {"id": "1507.03979", "submitter": "Neng-Fa Zhou", "authors": "Neng-Fa Zhou, Roman Bartak and Agostino Dovier", "title": "Planning as Tabled Logic Programming", "comments": "27 pages in TPLP 2015", "journal-ref": "Theory and Practice of Logic Programming 15 (2015) 543-558", "doi": "10.1017/S1471068415000216", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Picat's planner, its implementation, and planning models\nfor several domains used in International Planning Competition (IPC) 2014.\nPicat's planner is implemented by use of tabling. During search, every state\nencountered is tabled, and tabled states are used to effectively perform\nresource-bounded search. In Picat, structured data can be used to avoid\nenumerating all possible permutations of objects, and term sharing is used to\navoid duplication of common state data. This paper presents several modeling\ntechniques through the example models, ranging from designing state\nrepresentations to facilitate data sharing and symmetry breaking, encoding\nactions with operations for efficient precondition checking and state updating,\nto incorporating domain knowledge and heuristics. Broadly, this paper\ndemonstrates the effectiveness of tabled logic programming for planning, and\nargues the importance of modeling despite recent significant progress in\ndomain-independent PDDL planners.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 19:41:26 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Zhou", "Neng-Fa", ""], ["Bartak", "Roman", ""], ["Dovier", "Agostino", ""]]}, {"id": "1507.04091", "submitter": "Kuang Zhou", "authors": "Kuang Zhou (DRUID), Arnaud Martin (DRUID), Quan Pan, Zhun-Ga Liu", "title": "Evidential relational clustering using medoids", "comments": "in The 18th International Conference on Information Fusion, July\n  2015, Washington, DC, USA , Jul 2015, Washington, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real clustering applications, proximity data, in which only pairwise\nsimilarities or dissimilarities are known, is more general than object data, in\nwhich each pattern is described explicitly by a list of attributes.\nMedoid-based clustering algorithms, which assume the prototypes of classes are\nobjects, are of great value for partitioning relational data sets. In this\npaper a new prototype-based clustering method, named Evidential C-Medoids\n(ECMdd), which is an extension of Fuzzy C-Medoids (FCMdd) on the theoretical\nframework of belief functions is proposed. In ECMdd, medoids are utilized as\nthe prototypes to represent the detected classes, including specific classes\nand imprecise classes. Specific classes are for the data which are distinctly\nfar from the prototypes of other classes, while imprecise classes accept the\nobjects that may be close to the prototypes of more than one class. This soft\ndecision mechanism could make the clustering results more cautious and reduce\nthe misclassification rates. Experiments in synthetic and real data sets are\nused to illustrate the performance of ECMdd. The results show that ECMdd could\ncapture well the uncertainty in the internal data structure. Moreover, it is\nmore robust to the initializations compared with FCMdd.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 05:49:43 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Zhou", "Kuang", "", "DRUID"], ["Martin", "Arnaud", "", "DRUID"], ["Pan", "Quan", ""], ["Liu", "Zhun-Ga", ""]]}, {"id": "1507.04121", "submitter": "Jan Leike", "authors": "Jan Leike and Marcus Hutter", "title": "Solomonoff Induction Violates Nicod's Criterion", "comments": "ALT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nicod's criterion states that observing a black raven is evidence for the\nhypothesis H that all ravens are black. We show that Solomonoff induction does\nnot satisfy Nicod's criterion: there are time steps in which observing black\nravens decreases the belief in H. Moreover, while observing any computable\ninfinite string compatible with H, the belief in H decreases infinitely often\nwhen using the unnormalized Solomonoff prior, but only finitely often when\nusing the normalized Solomonoff prior. We argue that the fault is not with\nSolomonoff induction; instead we should reject Nicod's criterion.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:37:52 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Leike", "Jan", ""], ["Hutter", "Marcus", ""]]}, {"id": "1507.04124", "submitter": "Jan Leike", "authors": "Jan Leike and Marcus Hutter", "title": "On the Computability of Solomonoff Induction and Knowledge-Seeking", "comments": "ALT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solomonoff induction is held as a gold standard for learning, but it is known\nto be incomputable. We quantify its incomputability by placing various flavors\nof Solomonoff's prior M in the arithmetical hierarchy. We also derive\ncomputability bounds for knowledge-seeking agents, and give a limit-computable\nweakly asymptotically optimal reinforcement learning agent.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:46:06 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Leike", "Jan", ""], ["Hutter", "Marcus", ""]]}, {"id": "1507.04125", "submitter": "Iago Landesa-V\\'azquez", "authors": "Iago Landesa-V\\'azquez, Jos\\'e Luis Alba-Castro", "title": "Untangling AdaBoost-based Cost-Sensitive Classification. Part I:\n  Theoretical Perspective", "comments": "Extended version of paper submitted to Pattern Recognition (Revised\n  in July 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting algorithms have been widely used to tackle a plethora of problems.\nIn the last few years, a lot of approaches have been proposed to provide\nstandard AdaBoost with cost-sensitive capabilities, each with a different\nfocus. However, for the researcher, these algorithms shape a tangled set with\ndiffuse differences and properties, lacking a unifying analysis to jointly\ncompare, classify, evaluate and discuss those approaches on a common basis. In\nthis series of two papers we aim to revisit the various proposals, both from\ntheoretical (Part I) and practical (Part II) perspectives, in order to analyze\ntheir specific properties and behavior, with the final goal of identifying the\nalgorithm providing the best and soundest results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:50:09 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 17:44:11 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Landesa-V\u00e1zquez", "Iago", ""], ["Alba-Castro", "Jos\u00e9 Luis", ""]]}, {"id": "1507.04126", "submitter": "Iago Landesa-V\\'azquez", "authors": "Iago Landesa-V\\'azquez, Jos\\'e Luis Alba-Castro", "title": "Untangling AdaBoost-based Cost-Sensitive Classification. Part II:\n  Empirical Analysis", "comments": "Extended version of paper submitted to Pattern Recognition (Revised\n  in July 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of approaches, each following a different strategy, have been proposed\nin the literature to provide AdaBoost with cost-sensitive properties. In the\nfirst part of this series of two papers, we have presented these algorithms in\na homogeneous notational framework, proposed a clustering scheme for them and\nperformed a thorough theoretical analysis of those approaches with a fully\ntheoretical foundation. The present paper, in order to complete our analysis,\nis focused on the empirical study of all the algorithms previously presented\nover a wide range of heterogeneous classification problems. The results of our\nexperiments, confirming the theoretical conclusions, seem to reveal that the\nsimplest approach, just based on cost-sensitive weight initialization, is the\none showing the best and soundest results, despite having been recurrently\noverlooked in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:51:18 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 17:44:33 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Landesa-V\u00e1zquez", "Iago", ""], ["Alba-Castro", "Jos\u00e9 Luis", ""]]}, {"id": "1507.04285", "submitter": "Nina Gierasimczuk", "authors": "Thomas Bolander and Nina Gierasimczuk", "title": "Learning Action Models: Qualitative Approach", "comments": "18 pages, accepted for LORI-V: The Fifth International Conference on\n  Logic, Rationality and Interaction, October 28-31, 2015, National Taiwan\n  University, Taipei, Taiwan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dynamic epistemic logic, actions are described using action models. In\nthis paper we introduce a framework for studying learnability of action models\nfrom observations. We present first results concerning propositional action\nmodels. First we check two basic learnability criteria: finite identifiability\n(conclusively inferring the appropriate action model in finite time) and\nidentifiability in the limit (inconclusive convergence to the right action\nmodel). We show that deterministic actions are finitely identifiable, while\nnon-deterministic actions require more learning power-they are identifiable in\nthe limit. We then move on to a particular learning method, which proceeds via\nrestriction of a space of events within a learning-specific action model. This\nway of learning closely resembles the well-known update method from dynamic\nepistemic logic. We introduce several different learning methods suited for\nfinite identifiability of particular types of deterministic actions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 16:32:03 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Bolander", "Thomas", ""], ["Gierasimczuk", "Nina", ""]]}, {"id": "1507.04296", "submitter": "Arun Nair", "authors": "Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory\n  Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman,\n  Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Koray\n  Kavukcuoglu, David Silver", "title": "Massively Parallel Methods for Deep Reinforcement Learning", "comments": "Presented at the Deep Learning Workshop, International Conference on\n  Machine Learning, Lille, France, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first massively distributed architecture for deep\nreinforcement learning. This architecture uses four main components: parallel\nactors that generate new behaviour; parallel learners that are trained from\nstored experience; a distributed neural network to represent the value function\nor behaviour policy; and a distributed store of experience. We used our\narchitecture to implement the Deep Q-Network algorithm (DQN). Our distributed\nalgorithm was applied to 49 games from Atari 2600 games from the Arcade\nLearning Environment, using identical hyperparameters. Our performance\nsurpassed non-distributed DQN in 41 of the 49 games and also reduced the\nwall-time required to achieve these results by an order of magnitude on most\ngames.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 16:56:56 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 09:27:06 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Nair", "Arun", ""], ["Srinivasan", "Praveen", ""], ["Blackwell", "Sam", ""], ["Alcicek", "Cagdas", ""], ["Fearon", "Rory", ""], ["De Maria", "Alessandro", ""], ["Panneershelvam", "Vedavyas", ""], ["Suleyman", "Mustafa", ""], ["Beattie", "Charles", ""], ["Petersen", "Stig", ""], ["Legg", "Shane", ""], ["Mnih", "Volodymyr", ""], ["Kavukcuoglu", "Koray", ""], ["Silver", "David", ""]]}, {"id": "1507.04630", "submitter": "Iliana Petrova", "authors": "Piero Andrea Bonatti and Iliana Mineva Petrova and Luigi Sauro", "title": "Optimizing the computation of overriding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce optimization techniques for reasoning in DLN---a recently\nintroduced family of nonmonotonic description logics whose characterizing\nfeatures appear well-suited to model the applicative examples naturally arising\nin biomedical domains and semantic web access control policies. Such\noptimizations are validated experimentally on large KBs with more than 30K\naxioms. Speedups exceed 1 order of magnitude. For the first time, response\ntimes compatible with real-time reasoning are obtained with nonmonotonic KBs of\nthis size.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 16:05:47 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Bonatti", "Piero Andrea", ""], ["Petrova", "Iliana Mineva", ""], ["Sauro", "Luigi", ""]]}, {"id": "1507.04635", "submitter": "Jan-Willem van de Meent", "authors": "Jan-Willem van de Meent, Brooks Paige, David Tolpin, Frank Wood", "title": "Black-Box Policy Search with Probabilistic Programs", "comments": null, "journal-ref": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics (2016) 1195-1204", "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore how probabilistic programs can be used to represent\npolicies in sequential decision problems. In this formulation, a probabilistic\nprogram is a black-box stochastic simulator for both the problem domain and the\nagent. We relate classic policy gradient techniques to recently introduced\nblack-box variational methods which generalize to probabilistic program\ninference. We present case studies in the Canadian traveler problem, Rock\nSample, and a benchmark for optimal diagnosis inspired by Guess Who. Each study\nillustrates how programs can efficiently represent policies using moderate\nnumbers of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 16:18:44 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2015 17:49:52 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2015 21:32:03 GMT"}, {"version": "v4", "created": "Thu, 4 Aug 2016 10:56:37 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["van de Meent", "Jan-Willem", ""], ["Paige", "Brooks", ""], ["Tolpin", "David", ""], ["Wood", "Frank", ""]]}, {"id": "1507.04808", "submitter": "Iulian Vlad Serban", "authors": "Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville\n  and Joelle Pineau", "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical\n  Neural Network Models", "comments": "8 pages with references; Published in AAAI 2016 (Special Track on\n  Cognitive Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the task of building open domain, conversational dialogue\nsystems based on large dialogue corpora using generative models. Generative\nmodels produce system responses that are autonomously generated word-by-word,\nopening up the possibility for realistic, flexible interactions. In support of\nthis goal, we extend the recently proposed hierarchical recurrent\nencoder-decoder neural network to the dialogue domain, and demonstrate that\nthis model is competitive with state-of-the-art neural language models and\nback-off n-gram models. We investigate the limitations of this and similar\napproaches, and show how its performance can be improved by bootstrapping the\nlearning from a larger question-answer pair corpus and from pretrained word\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 00:21:39 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 19:49:39 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 23:20:41 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Serban", "Iulian V.", ""], ["Sordoni", "Alessandro", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""], ["Pineau", "Joelle", ""]]}, {"id": "1507.04811", "submitter": "Jian Xu", "authors": "Jian Xu, Xuhui Shao, Jianjie Ma, Kuang-chih Lee, Hang Qi, Quan Lu", "title": "Lift-Based Bidding in Ad Selection", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time bidding (RTB) has become one of the largest online advertising\nmarkets in the world. Today the bid price per ad impression is typically\ndecided by the expected value of how it can lead to a desired action event\n(e.g., registering an account or placing a purchase order) to the advertiser.\nHowever, this industry standard approach to decide the bid price does not\nconsider the actual effect of the ad shown to the user, which should be\nmeasured based on the performance lift among users who have been or have not\nbeen exposed to a certain treatment of ads. In this paper, we propose a new\nbidding strategy and prove that if the bid price is decided based on the\nperformance lift rather than absolute performance value, advertisers can\nactually gain more action events. We describe the modeling methodology to\npredict the performance lift and demonstrate the actual performance gain\nthrough blind A/B test with real ad campaigns in an industry-leading\nDemand-Side Platform (DSP). We also discuss the relationship between\nattribution models and bidding strategies. We prove that, to move the DSPs to\nbid based on performance lift, they should be rewarded according to the\nrelative performance lift they contribute.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 01:28:54 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 03:29:21 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2016 01:56:20 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Xu", "Jian", ""], ["Shao", "Xuhui", ""], ["Ma", "Jianjie", ""], ["Lee", "Kuang-chih", ""], ["Qi", "Hang", ""], ["Lu", "Quan", ""]]}, {"id": "1507.04913", "submitter": "Weiyao Lin", "authors": "Xintong Han, Chongyang Zhang, Weiyao Lin, Mingliang Xu, Bin Sheng, Tao\n  Mei", "title": "Tree-based Visualization and Optimization for Image Collection", "comments": "This manuscript is the accepted version for T-CYB (IEEE Transactions\n  on Cybernetics) IEEE Trans. Cybernetics, 2015", "journal-ref": null, "doi": "10.1109/TCYB.2015.2448236", "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visualization of an image collection is the process of displaying a\ncollection of images on a screen under some specific layout requirements. This\npaper focuses on an important problem that is not well addressed by the\nprevious methods: visualizing image collections into arbitrary layout shapes\nwhile arranging images according to user-defined semantic or visual\ncorrelations (e.g., color or object category). To this end, we first propose a\nproperty-based tree construction scheme to organize images of a collection into\na tree structure according to user-defined properties. In this way, images can\nbe adaptively placed with the desired semantic or visual correlations in the\nfinal visualization layout. Then, we design a two-step visualization\noptimization scheme to further optimize image layouts. As a result, multiple\nlayout effects including layout shape and image overlap ratio can be\neffectively controlled to guarantee a satisfactory visualization. Finally, we\nalso propose a tree-transfer scheme such that visualization layouts can be\nadaptively changed when users select different \"images of interest\". We\ndemonstrate the effectiveness of our proposed approach through the comparisons\nwith state-of-the-art visualization techniques.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 10:45:26 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Han", "Xintong", ""], ["Zhang", "Chongyang", ""], ["Lin", "Weiyao", ""], ["Xu", "Mingliang", ""], ["Sheng", "Bin", ""], ["Mei", "Tao", ""]]}, {"id": "1507.04928", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "A Brain-like Cognitive Process with Shared Methods", "comments": null, "journal-ref": "Int. J. Advanced Intelligence Paradigms, Vol. 18, No. 4, 2021,\n  pp.481-501, Inderscience", "doi": "10.1504/IJAIP.2018.10033335", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new entropy-style of equation that may be useful in a\ngeneral sense, but can be applied to a cognitive model with related processes.\nThe model is based on the human brain, with automatic and distributed pattern\nactivity. Methods for carrying out the different processes are suggested. The\nmain purpose of this paper is to reaffirm earlier research on different\nknowledge-based and experience-based clustering techniques. The overall\narchitecture has stayed essentially the same and so it is the localised\nprocesses or smaller details that have been updated. For example, a counting\nmechanism is used slightly differently, to measure a level of 'cohesion'\ninstead of a 'correct' classification, over pattern instances. The introduction\nof features has further enhanced the architecture and the new entropy-style\nequation is proposed. While an earlier paper defined three levels of functional\nrequirement, this paper re-defines the levels in a more human vernacular, with\nhigher-level goals described in terms of action-result pairs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 11:24:07 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 10:06:58 GMT"}, {"version": "v3", "created": "Sat, 23 Jul 2016 16:00:42 GMT"}, {"version": "v4", "created": "Wed, 23 Nov 2016 14:44:04 GMT"}, {"version": "v5", "created": "Tue, 4 Apr 2017 13:46:24 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1507.04997", "submitter": "Ismael Rodr\\'iguez-Fdez M.Sc", "authors": "I. Rodr\\'iguez-Fdez, M. Mucientes, A. Bugar\\'in", "title": "FRULER: Fuzzy Rule Learning through Evolution for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression problems, the use of TSK fuzzy systems is widely extended due\nto the precision of the obtained models. Moreover, the use of simple linear TSK\nmodels is a good choice in many real problems due to the easy understanding of\nthe relationship between the output and input variables. In this paper we\npresent FRULER, a new genetic fuzzy system for automatically learning accurate\nand simple linguistic TSK fuzzy rule bases for regression problems. In order to\nreduce the complexity of the learned models while keeping a high accuracy, the\nalgorithm consists of three stages: instance selection, multi-granularity fuzzy\ndiscretization of the input variables, and the evolutionary learning of the\nrule base that uses the Elastic Net regularization to obtain the consequents of\nthe rules. Each stage was validated using 28 real-world datasets and FRULER was\ncompared with three state of the art enetic fuzzy systems. Experimental results\nshow that FRULER achieves the most accurate and simple models compared even\nwith approximative approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 15:26:06 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Rodr\u00edguez-Fdez", "I.", ""], ["Mucientes", "M.", ""], ["Bugar\u00edn", "A.", ""]]}, {"id": "1507.05019", "submitter": "Paudel Subodh", "authors": "Subodh Paudel (Mines Nantes), Phuong H. Nguyen, Wil L. Kling, Mohamed\n  Elmitri, Bruno Lacarri\\`ere (Mines Nantes), Olivier Le Corre (Mines Nantes)", "title": "Support Vector Machine in Prediction of Building Energy Demand Using\n  Pseudo Dynamic Approach", "comments": "Proceedings of ECOS 2015-The 28th International Conference on\n  Efficiency, Cost, Optimization, Simulation and Environmental Impact of Energy\n  Systems , Jun 2015, Pau, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building's energy consumption prediction is a major concern in the recent\nyears and many efforts have been achieved in order to improve the energy\nmanagement of buildings. In particular, the prediction of energy consumption in\nbuilding is essential for the energy operator to build an optimal operating\nstrategy, which could be integrated to building's energy management system\n(BEMS). This paper proposes a prediction model for building energy consumption\nusing support vector machine (SVM). Data-driven model, for instance, SVM is\nvery sensitive to the selection of training data. Thus the relevant days data\nselection method based on Dynamic Time Warping is used to train SVM model. In\naddition, to encompass thermal inertia of building, pseudo dynamic model is\napplied since it takes into account information of transition of energy\nconsumption effects and occupancy profile. Relevant days data selection and\nwhole training data model is applied to the case studies of Ecole des Mines de\nNantes, France Office building. The results showed that support vector machine\nbased on relevant data selection method is able to predict the energy\nconsumption of building with a high accuracy in compare to whole data training.\nIn addition, relevant data selection method is computationally cheaper (around\n8 minute training time) in contrast to whole data training (around 31 hour for\nweekend and 116 hour for working days) and reveals realistic control\nimplementation for online system as well.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 16:19:11 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Paudel", "Subodh", "", "Mines Nantes"], ["Nguyen", "Phuong H.", "", "Mines Nantes"], ["Kling", "Wil L.", "", "Mines Nantes"], ["Elmitri", "Mohamed", "", "Mines Nantes"], ["Lacarri\u00e8re", "Bruno", "", "Mines Nantes"], ["Corre", "Olivier Le", "", "Mines Nantes"]]}, {"id": "1507.05122", "submitter": "Feng Lin", "authors": "Yingxiao Wu, Yan Zhuang, Xi Long, Feng Lin, Wenyao Xu", "title": "Human Gender Classification: A Review", "comments": "This paper has been withdrawn by the author due to several literature\n  mistakes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender contains a wide range of information regarding to the characteristics\ndifference between male and female. Successful gender recognition is essential\nand critical for many applications in the commercial domains such as\napplications of human-computer interaction and computer-aided physiological or\npsychological analysis. Some have proposed various approaches for automatic\ngender classification using the features derived from human bodies and/or\nbehaviors. First, this paper introduces the challenge and application for\ngender classification research. Then, the development and framework of gender\nclassification are described. Besides, we compare these state-of-the-art\napproaches, including vision-based methods, biological information-based\nmethod, and social network information-based method, to provide a comprehensive\nreview in the area of gender classification. In mean time, we highlight the\nstrength and discuss the limitation of each method. Finally, this review also\ndiscusses several promising applications for the future work.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 21:58:01 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 14:48:45 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Wu", "Yingxiao", ""], ["Zhuang", "Yan", ""], ["Long", "Xi", ""], ["Lin", "Feng", ""], ["Xu", "Wenyao", ""]]}, {"id": "1507.05268", "submitter": "Gal Dalal", "authors": "Gal Dalal, Shie Mannor", "title": "Reinforcement Learning for the Unit Commitment Problem", "comments": "Accepted and presented in IEEE PES PowerTech, Eindhoven 2015, paper\n  ID 462731", "journal-ref": null, "doi": "10.1109/PTC.2015.7232646", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we solve the day-ahead unit commitment (UC) problem, by\nformulating it as a Markov decision process (MDP) and finding a low-cost policy\nfor generation scheduling. We present two reinforcement learning algorithms,\nand devise a third one. We compare our results to previous work that uses\nsimulated annealing (SA), and show a 27% improvement in operation costs, with\nrunning time of 2.5 minutes (compared to 2.5 hours of existing\nstate-of-the-art).\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 09:32:40 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Dalal", "Gal", ""], ["Mannor", "Shie", ""]]}, {"id": "1507.05272", "submitter": "Emilia Oikarinen", "authors": "Laura Koponen and Emilia Oikarinen and Tomi Janhunen and Laura\n  S\\\"ail\\\"a", "title": "Optimizing Phylogenetic Supertrees Using Answer Set Programming", "comments": "To appear in Theory and Practice of Logic Programming (TPLP),\n  Proceedings of ICLP 2015", "journal-ref": "Theory and Practice of Logic Programming 15 (2015) 604-619", "doi": "10.1017/S1471068415000265", "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The supertree construction problem is about combining several phylogenetic\ntrees with possibly conflicting information into a single tree that has all the\nleaves of the source trees as its leaves and the relationships between the\nleaves are as consistent with the source trees as possible. This leads to an\noptimization problem that is computationally challenging and typically\nheuristic methods, such as matrix representation with parsimony (MRP), are\nused. In this paper we consider the use of answer set programming to solve the\nsupertree construction problem in terms of two alternative encodings. The first\nis based on an existing encoding of trees using substructures known as\nquartets, while the other novel encoding captures the relationships present in\ntrees through direct projections. We use these encodings to compute a\ngenus-level supertree for the family of cats (Felidae). Furthermore, we compare\nour results to recent supertrees obtained by the MRP method.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 10:19:08 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Koponen", "Laura", ""], ["Oikarinen", "Emilia", ""], ["Janhunen", "Tomi", ""], ["S\u00e4il\u00e4", "Laura", ""]]}, {"id": "1507.05275", "submitter": "Swakkhar Shatabda", "authors": "Shanjida Khatun, Hasib Ul Alam and Swakkhar Shatabda", "title": "An Efficient Genetic Algorithm for Discovering Diverse-Frequent Patterns", "comments": "2015 International Conference on Electrical Engineering and\n  Information Communication Technology (ICEEICT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Working with exhaustive search on large dataset is infeasible for several\nreasons. Recently, developed techniques that made pattern set mining feasible\nby a general solver with long execution time that supports heuristic search and\nare limited to small datasets only. In this paper, we investigate an approach\nwhich aims to find diverse set of patterns using genetic algorithm to mine\ndiverse frequent patterns. We propose a fast heuristic search algorithm that\noutperforms state-of-the-art methods on a standard set of benchmarks and\ncapable to produce satisfactory results within a short period of time. Our\nproposed algorithm uses a relative encoding scheme for the patterns and an\neffective twin removal technique to ensure diversity throughout the search.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 10:55:09 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Khatun", "Shanjida", ""], ["Alam", "Hasib Ul", ""], ["Shatabda", "Swakkhar", ""]]}, {"id": "1507.05388", "submitter": "Johannes Klaus Fichte", "authors": "Johannes K. Fichte and Miroslaw Truszczynski and Stefan Woltran", "title": "Dual-normal Logic Programs - the Forgotten Class", "comments": "This is the author's self-archived copy including detailed proofs. To\n  appear in Theory and Practice of Logic Programming (TPLP), Proceedings of the\n  31st International Conference on Logic Programming (ICLP 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disjunctive Answer Set Programming is a powerful declarative programming\nparadigm with complexity beyond NP. Identifying classes of programs for which\nthe consistency problem is in NP is of interest from the theoretical standpoint\nand can potentially lead to improvements in the design of answer set\nprogramming solvers. One of such classes consists of dual-normal programs,\nwhere the number of positive body atoms in proper rules is at most one. Unlike\nother classes of programs, dual-normal programs have received little attention\nso far. In this paper we study this class. We relate dual-normal programs to\npropositional theories and to normal programs by presenting several\ninter-translations. With the translation from dual-normal to normal programs at\nhand, we introduce the novel class of body-cycle free programs, which are in\nmany respects dual to head-cycle free programs. We establish the expressive\npower of dual-normal programs in terms of SE- and UE-models, and compare them\nto normal programs. We also discuss the complexity of deciding whether\ndual-normal programs are strongly and uniformly equivalent.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 05:28:50 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Truszczynski", "Miroslaw", ""], ["Woltran", "Stefan", ""]]}, {"id": "1507.05497", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov and Denis Kornilov", "title": "RAPS: A Recommender Algorithm Based on Pattern Structures", "comments": "The paper presented at FCA4AI 2015 in conjunction with IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for recommender systems with numeric ratings which\nis based on Pattern Structures (RAPS). As the input the algorithm takes rating\nmatrix, e.g., such that it contains movies rated by users. For a target user,\nthe algorithm returns a rated list of items (movies) based on its previous\nratings and ratings of other users. We compare the results of the proposed\nalgorithm in terms of precision and recall measures with Slope One, one of the\nstate-of-the-art item-based algorithms, on Movie Lens dataset and RAPS\ndemonstrates the best or comparable quality.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 13:58:30 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Ignatov", "Dmitry I.", ""], ["Kornilov", "Denis", ""]]}, {"id": "1507.05722", "submitter": "Manish Joshi", "authors": "Manish Joshi, Theyazn Hassn Hadi", "title": "A Review of Network Traffic Analysis and Prediction Techniques", "comments": "23 pages, 3 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis and prediction of network traffic has applications in wide\ncomprehensive set of areas and has newly attracted significant number of\nstudies. Different kinds of experiments are conducted and summarized to\nidentify various problems in existing computer network applications. Network\ntraffic analysis and prediction is a proactive approach to ensure secure,\nreliable and qualitative network communication. Various techniques are proposed\nand experimented for analyzing network traffic including neural network based\ntechniques to data mining techniques. Similarly, various Linear and non-linear\nmodels are proposed for network traffic prediction. Several interesting\ncombinations of network analysis and prediction techniques are implemented to\nattain efficient and effective results.\n  This paper presents a survey on various such network analysis and traffic\nprediction techniques. The uniqueness and rules of previous studies are\ninvestigated. Moreover, various accomplished areas of analysis and prediction\nof network traffic have been summed.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 06:42:32 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 09:54:11 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Joshi", "Manish", ""], ["Hadi", "Theyazn Hassn", ""]]}, {"id": "1507.05875", "submitter": "Tarek Richard Besold", "authors": "Arne Recknagel and Tarek R. Besold", "title": "Efficient Dodgson-Score Calculation Using Heuristics and Parallel\n  Computing", "comments": "Additional references; partially rewritten text for improved\n  readability; minor corrections of typos/language issues etc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conflict of interest is the permanent companion of any population of agents\n(computational or biological). For that reason, the ability to compromise is of\nparamount importance, making voting a key element of societal mechanisms. One\nof the voting procedures most often discussed in the literature and, due to its\nintuitiveness, also conceptually quite appealing is Charles Dodgson's scoring\nrule, basically using the respective closeness to being a Condorcet winner for\nevaluating competing alternatives. In this paper, we offer insights on the\npractical limits of algorithms computing the exact Dodgson scores from a number\nof votes. While the problem itself is theoretically intractable, this work\nproposes and analyses five different solutions which try distinct approaches to\npractically solve the issue in an effective manner. Additionally, three of the\ndiscussed procedures can be run in parallel which has the potential of\ndrastically reducing the problem size.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 15:29:28 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 14:44:41 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Recknagel", "Arne", ""], ["Besold", "Tarek R.", ""]]}, {"id": "1507.05895", "submitter": "Song-Ju Kim Dr.", "authors": "Song-Ju Kim, Tohru Tsuruoka, Tsuyoshi Hasegawa, and Masakazu Aono", "title": "Decision Maker based on Atomic Switches", "comments": "10 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1412.6141", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cond-mat.mtrl-sci", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple model for an atomic switch-based decision maker (ASDM),\nand show that, as long as its total volume of precipitated Ag atoms is\nconserved when coupled with suitable operations, an atomic switch system\nprovides a sophisticated \"decision-making\" capability that is known to be one\nof the most important intellectual abilities in human beings. We considered the\nmulti-armed bandit problem (MAB); the problem of finding, as accurately and\nquickly as possible, the most profitable option from a set of options that\ngives stochastic rewards. These decisions are made as dictated by each volume\nof precipitated Ag atoms, which is moved in a manner similar to the\nfluctuations of a rigid body in a tug-of-war game. The \"tug-of-war (TOW)\ndynamics\" of the ASDM exhibits higher efficiency than conventional MAB solvers.\nWe show analytical calculations that validate the statistical reasons for the\nASDM dynamics to produce such high performance, despite its simplicity. These\nresults imply that various physical systems, in which some conservation law\nholds, can be used to implement efficient \"decision-making objects.\" Efficient\nMAB solvers are useful for many practical applications, because MAB abstracts a\nvariety of decision-making problems in real- world situations where an\nefficient trial-and-error is required. The proposed scheme will introduce a new\nphysics-based analog computing paradigm, which will include such things as\n\"intelligent nano devices\" and \"intelligent information networks\" based on\nself-detection and self-judgment.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 16:22:53 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Kim", "Song-Ju", ""], ["Tsuruoka", "Tohru", ""], ["Hasegawa", "Tsuyoshi", ""], ["Aono", "Masakazu", ""]]}, {"id": "1507.05920", "submitter": "Saurabh Joshi", "authors": "Saurabh Joshi, Ruben Martins and Vasco Manquinho", "title": "Generalized Totalizer Encoding for Pseudo-Boolean Constraints", "comments": "10 pages, 2 figures, 2 tables. To be published in 21st International\n  Conference on Principles and Practice of Constraint Programming 2015", "journal-ref": null, "doi": "10.1007/978-3-319-23219-5_15", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-Boolean constraints, also known as 0-1 Integer Linear Constraints, are\nused to model many real-world problems. A common approach to solve these\nconstraints is to encode them into a SAT formula. The runtime of the SAT solver\non such formula is sensitive to the manner in which the given pseudo-Boolean\nconstraints are encoded. In this paper, we propose generalized Totalizer\nencoding (GTE), which is an arc-consistency preserving extension of the\nTotalizer encoding to pseudo-Boolean constraints. Unlike some other encodings,\nthe number of auxiliary variables required for GTE does not depend on the\nmagnitudes of the coefficients. Instead, it depends on the number of distinct\ncombinations of these coefficients. We show the superiority of GTE with respect\nto other encodings when large pseudo-Boolean constraints have low number of\ndistinct coefficients. Our experimental results also show that GTE remains\ncompetitive even when the pseudo-Boolean constraints do not have this\ncharacteristic.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 17:28:59 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Joshi", "Saurabh", ""], ["Martins", "Ruben", ""], ["Manquinho", "Vasco", ""]]}, {"id": "1507.05956", "submitter": "Thomas Lynch", "authors": "Thomas W. Lynch", "title": "Towards a Better Understanding of CAR, CDR, CADR and the Others", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the IBM 704 architecture and the genesis of the names\nfor CAR, and CDR, which, as it turns out, probably don't quite make sense. The\npaper suggests that this may not be all bad, as the names lend themselves to\ncompounding. Indeed that the compound function names , such as CADR, or even\nCADADR, etc. may be read as little access programs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 14:24:28 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 09:59:33 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2015 12:33:25 GMT"}, {"version": "v4", "created": "Mon, 7 Dec 2015 06:31:09 GMT"}, {"version": "v5", "created": "Wed, 6 Apr 2016 15:52:45 GMT"}, {"version": "v6", "created": "Thu, 7 Apr 2016 04:48:00 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Lynch", "Thomas W.", ""]]}, {"id": "1507.06045", "submitter": "Gregory Hasseler", "authors": "Gregory Hasseler", "title": "Adapting Stochastic Search For Real-time Dynamic Weighted Constraint\n  Satisfaction", "comments": "187 pages, Master's Thesis submitted to State University of New York\n  Institute of Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents two new algorithms for performing constraint satisfaction.\nThe first algorithm presented, DMaxWalkSat, is a constraint solver specialized\nfor solving dynamic, weighted constraint satisfaction problems. The second\nalgorithm, RDMaxWalkSat, is a derivative of DMaxWalkSat that has been modified\ninto an anytime algorithm, and hence support realtime constraint satisfaction.\nDMaxWalkSat is shown to offer performance advantages in terms of solution\nquality and runtime over its parent constraint solver, MaxWalkSat. RDMaxWalkSat\nis shown to support anytime operation. The introduction of these algorithms\nbrings another tool to the areas of computer science that naturally represent\nproblems as constraint satisfaction problems, an example of which is the robust\ncoherence algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 03:32:52 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Hasseler", "Gregory", ""]]}, {"id": "1507.06103", "submitter": "Marco Manna", "authors": "Marco Manna and Francesco Ricca and Giorgio Terracina", "title": "Taming Primary Key Violations to Query Large Inconsistent Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistent query answering over a database that violates primary key\nconstraints is a classical hard problem in database research that has been\ntraditionally dealt with logic programming. However, the applicability of\nexisting logic-based solutions is restricted to data sets of moderate size.\nThis paper presents a novel decomposition and pruning strategy that reduces, in\npolynomial time, the problem of computing the consistent answer to a\nconjunctive query over a database subject to primary key constraints to a\ncollection of smaller problems of the same sort that can be solved\nindependently. The new strategy is naturally modeled and implemented using\nAnswer Set Programming (ASP). An experiment run on benchmarks from the database\nworld prove the effectiveness and efficiency of our ASP-based approach also on\nlarge data sets. To appear in Theory and Practice of Logic Programming (TPLP),\nProceedings of ICLP 2015.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 08:56:03 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Manna", "Marco", ""], ["Ricca", "Francesco", ""], ["Terracina", "Giorgio", ""]]}, {"id": "1507.06500", "submitter": "Hai Zhuge Mr", "authors": "Hai Zhuge", "title": "Mapping Big Data into Knowledge Space with Cognitive\n  Cyber-Infrastructure", "comments": "59 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data research has attracted great attention in science, technology,\nindustry and society. It is developing with the evolving scientific paradigm,\nthe fourth industrial revolution, and the transformational innovation of\ntechnologies. However, its nature and fundamental challenge have not been\nrecognized, and its own methodology has not been formed. This paper explores\nand answers the following questions: What is big data? What are the basic\nmethods for representing, managing and analyzing big data? What is the\nrelationship between big data and knowledge? Can we find a mapping from big\ndata into knowledge space? What kind of infrastructure is required to support\nnot only big data management and analysis but also knowledge discovery, sharing\nand management? What is the relationship between big data and science paradigm?\nWhat is the nature and fundamental challenge of big data computing? A\nmulti-dimensional perspective is presented toward a methodology of big data\ncomputing.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 21:38:21 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Zhuge", "Hai", ""]]}, {"id": "1507.06566", "submitter": "Mark Law", "authors": "Mark Law, Alessandra Russo and Krysia Broda", "title": "Learning Weak Constraints in Answer Set Programming", "comments": "To appear in Theory and Practice of Logic Programming (TPLP),\n  Proceedings of ICLP 2015", "journal-ref": "Theory and Practice of Logic Programming 15 (2015) 511-525", "doi": "10.1017/S1471068415000198", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to the area of inductive logic programming by\npresenting a new learning framework that allows the learning of weak\nconstraints in Answer Set Programming (ASP). The framework, called Learning\nfrom Ordered Answer Sets, generalises our previous work on learning ASP\nprograms without weak constraints, by considering a new notion of examples as\nordered pairs of partial answer sets that exemplify which answer sets of a\nlearned hypothesis (together with a given background knowledge) are preferred\nto others. In this new learning task inductive solutions are searched within a\nhypothesis space of normal rules, choice rules, and hard and weak constraints.\nWe propose a new algorithm, ILASP2, which is sound and complete with respect to\nour new learning framework. We investigate its applicability to learning\npreferences in an interview scheduling problem and also demonstrate that when\nrestricted to the task of learning ASP programs without weak constraints,\nILASP2 can be much more efficient than our previously proposed system.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 17:03:39 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Law", "Mark", ""], ["Russo", "Alessandra", ""], ["Broda", "Krysia", ""]]}, {"id": "1507.06689", "submitter": "Sarah Alice Gaggl", "authors": "Sarah A. Gaggl, Norbert Manthey, Alessandro Ronca, Johannes P.\n  Wallner, Stefan Woltran", "title": "Improved Answer-Set Programming Encodings for Abstract Argumentation", "comments": "To appear in Theory and Practice of Logic Programming (TPLP),\n  Proceedings of ICLP 2015", "journal-ref": "Theory and Practice of Logic Programming 15 (2015) 434-448", "doi": "10.1017/S1471068415000149", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of efficient solutions for abstract argumentation problems is a\ncrucial step towards advanced argumentation systems. One of the most prominent\napproaches in the literature is to use Answer-Set Programming (ASP) for this\nendeavor. In this paper, we present new encodings for three prominent\nargumentation semantics using the concept of conditional literals in\ndisjunctions as provided by the ASP-system clingo. Our new encodings are not\nonly more succinct than previous versions, but also outperform them on standard\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 21:43:48 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 13:54:18 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Gaggl", "Sarah A.", ""], ["Manthey", "Norbert", ""], ["Ronca", "Alessandro", ""], ["Wallner", "Johannes P.", ""], ["Woltran", "Stefan", ""]]}, {"id": "1507.06837", "submitter": "Jeremy Fix", "authors": "Jeremy Fix and Herve Frezza-buet", "title": "YARBUS : Yet Another Rule Based belief Update System", "comments": "Source code at : https://github.com/jeremyfix/dstc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new rule based system for belief tracking in dialog systems.\nDespite the simplicity of the rules being considered, the proposed belief\ntracker ranks favourably compared to the previous submissions on the second and\nthird Dialog State Tracking challenges. The results of this simple tracker\nallows to reconsider the performances of previous submissions using more\nelaborate techniques.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 13:21:59 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Fix", "Jeremy", ""], ["Frezza-buet", "Herve", ""]]}, {"id": "1507.07045", "submitter": "Vijay Kamble", "authors": "Vijay Kamble, David Marn, Nihar Shah, Abhay Parekh and Kannan\n  Ramachandran", "title": "A Truth Serum for Large-Scale Evaluations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in obtaining large-scale evaluations, e.g., product or\nservice reviews on online platforms, labeling images, grading in online\ncourses, etc., is that of eliciting honest responses from agents in the absence\nof verifiability. We propose a new reward mechanism with strong incentive\nproperties applicable in a wide variety of such settings. This mechanism has a\nsimple and intuitive output agreement structure: an agent gets a reward only if\nher response for an evaluation matches that of her peer. But instead of the\nreward being the same across different answers, it is inversely proportional to\na popularity index of each answer. This index is a second order population\nstatistic that captures how frequently two agents performing the same\nevaluation agree on the particular answer. Rare agreements thus earn a higher\nreward than agreements that are relatively more common.\n  In the regime where there are a large number of evaluation tasks, we show\nthat truthful behavior is a strict Bayes-Nash equilibrium of the game induced\nby the mechanism. Further, we show that the truthful equilibrium is\napproximately optimal in terms of expected payoffs to the agents across all\nsymmetric equilibria, where the approximation error vanishes in the number of\nevaluation tasks. Moreover, under a mild condition on strategy space, we show\nthat any symmetric equilibrium that gives a higher expected payoff than the\ntruthful equilibrium must be close to being fully informative if the number of\nevaluations is large. These last two results are driven by a new notion of an\nagreement measure that is shown to be monotonic in information loss. This\nnotion and its properties are of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 00:41:00 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 22:49:57 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 19:49:59 GMT"}, {"version": "v4", "created": "Fri, 16 Feb 2018 22:20:30 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Kamble", "Vijay", ""], ["Marn", "David", ""], ["Shah", "Nihar", ""], ["Parekh", "Abhay", ""], ["Ramachandran", "Kannan", ""]]}, {"id": "1507.07058", "submitter": "Azlan Iqbal", "authors": "Azlan Iqbal, Matej Guid, Simon Colton, Jana Krivec, Shazril Azman,\n  Boshra Haghighi", "title": "The Digital Synaptic Neural Substrate: A New Approach to Computational\n  Creativity", "comments": "39 pages, 5 appendices. Full version:\n  http://www.springer.com/gp/book/9783319280783", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new artificial intelligence (AI) approach called, the 'Digital\nSynaptic Neural Substrate' (DSNS). It uses selected attributes from objects in\nvarious domains (e.g. chess problems, classical music, renowned artworks) and\nrecombines them in such a way as to generate new attributes that can then, in\nprinciple, be used to create novel objects of creative value to humans relating\nto any one of the source domains. This allows some of the burden of creative\ncontent generation to be passed from humans to machines. The approach was\ntested in the domain of chess problem composition. We used it to automatically\ncompose numerous sets of chess problems based on attributes extracted and\nrecombined from chess problems and tournament games by humans, renowned\npaintings, computer-evolved abstract art, photographs of people, and classical\nmusic tracks. The quality of these generated chess problems was then assessed\nautomatically using an existing and experimentally-validated computational\nchess aesthetics model. They were also assessed by human experts in the domain.\nThe results suggest that attributes collected and recombined from chess and\nother domains using the DSNS approach can indeed be used to automatically\ngenerate chess problems of reasonably high aesthetic quality. In particular, a\nlow quality chess source (i.e. tournament game sequences between weak players)\nused in combination with actual photographs of people was able to produce\nthree-move chess problems of comparable quality or better to those generated\nusing a high quality chess source (i.e. published compositions by human\nexperts), and more efficiently as well. Why information from a foreign domain\ncan be integrated and functional in this way remains an open question for now.\nThe DSNS approach is, in principle, scalable and applicable to any domain in\nwhich objects have attributes that can be represented using real numbers.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 03:00:31 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 08:10:17 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Iqbal", "Azlan", ""], ["Guid", "Matej", ""], ["Colton", "Simon", ""], ["Krivec", "Jana", ""], ["Azman", "Shazril", ""], ["Haghighi", "Boshra", ""]]}, {"id": "1507.07295", "submitter": "Kirill Dyagilev", "authors": "Kirill Dyagilev, Suchi Saria", "title": "Learning (Predictive) Risk Scores in the Presence of Censoring due to\n  Interventions", "comments": null, "journal-ref": "Machine Learning Journal, Special Issue on on Machine Learning for\n  Health and Medicine, pp. 1-26, 2015", "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large and diverse set of measurements are regularly collected during a\npatient's hospital stay to monitor their health status. Tools for integrating\nthese measurements into severity scores, that accurately track changes in\nillness severity, can improve clinicians ability to provide timely\ninterventions. Existing approaches for creating such scores either 1) rely on\nexperts to fully specify the severity score, or 2) train a predictive score,\nusing supervised learning, by regressing against a surrogate marker of severity\nsuch as the presence of downstream adverse events. The first approach does not\nextend to diseases where an accurate score cannot be elicited from experts. The\nsecond approach often produces scores that suffer from bias due to\ntreatment-related censoring (Paxton, 2013). We propose a novel ranking based\nframework for disease severity score learning (DSSL). DSSL exploits the\nfollowing key observation: while it is challenging for experts to quantify the\ndisease severity at any given time, it is often easy to compare the disease\nseverity at two different times. Extending existing ranking algorithms, DSSL\nlearns a function that maps a vector of patient's measurements to a scalar\nseverity score such that the resulting score is temporally smooth and\nconsistent with the expert's ranking of pairs of disease states. We apply DSSL\nto the problem of learning a sepsis severity score using a large, real-world\ndataset. The learned scores significantly outperform state-of-the-art clinical\nscores in ranking patient states by severity and in early detection of future\nadverse events. We also show that the learned disease severity trajectories are\nconsistent with clinical expectations of disease evolution. Further, using\nsimulated datasets, we show that DSSL exhibits better generalization\nperformance to changes in treatment patterns compared to the above approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 03:56:37 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Dyagilev", "Kirill", ""], ["Saria", "Suchi", ""]]}, {"id": "1507.07374", "submitter": "Maxim Borisyak", "authors": "Maxim Borisyak, Andrey Ustyuzhanin", "title": "A genetic algorithm for autonomous navigation in partially observable\n  domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of autonomous navigation is one of the basic problems for\nrobotics. Although, in general, it may be challenging when an autonomous\nvehicle is placed into partially observable domain. In this paper we consider\nsimplistic environment model and introduce a navigation algorithm based on\nLearning Classifier System.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 11:50:44 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Borisyak", "Maxim", ""], ["Ustyuzhanin", "Andrey", ""]]}, {"id": "1507.07462", "submitter": "Florentin Smarandache", "authors": "Florentin Smarandache", "title": "Unification of Fusion Theories, Rules, Filters, Image Fusion and Target\n  Tracking Methods (UFT)", "comments": "79 pages, a diagram. arXiv admin note: substantial text overlap with\n  arXiv:cs/0409040, arXiv:0901.1289, arXiv:cs/0410033", "journal-ref": "International Journal of Applied Mathematics & Statistics, Vol. 2,\n  1-14, 2004", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The author has pledged in various papers, conference or seminar\npresentations, and scientific grant applications (between 2004-2015) for the\nunification of fusion theories, combinations of fusion rules, image fusion\nprocedures, filter algorithms, and target tracking methods for more accurate\napplications to our real world problems - since neither fusion theory nor\nfusion rule fully satisfy all needed applications. For each particular\napplication, one selects the most appropriate fusion space and fusion model,\nthen the fusion rules, and the algorithms of implementation. He has worked in\nthe Unification of the Fusion Theories (UFT), which looks like a cooking\nrecipe, better one could say like a logical chart for a computer programmer,\nbut one does not see another method to comprise/unify all things. The\nunification scenario presented herein, which is now in an incipient form,\nshould periodically be updated incorporating new discoveries from the fusion\nand engineering research.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 15:59:03 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Smarandache", "Florentin", ""]]}, {"id": "1507.07648", "submitter": "Rehan Abdul Aziz", "authors": "Rehan Abdul Aziz and Geoffrey Chu and Christian Muise and Peter\n  Stuckey", "title": "Projected Model Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model counting is the task of computing the number of assignments to\nvariables V that satisfy a given propositional theory F. Model counting is an\nessential tool in probabilistic reasoning. In this paper, we introduce the\nproblem of model counting projected on a subset P of original variables that we\ncall 'priority' variables. The task is to compute the number of assignments to\nP such that there exists an extension to 'non-priority' variables V\\P that\nsatisfies F. Projected model counting arises when some parts of the model are\nirrelevant to the counts, in particular when we require additional variables to\nmodel the problem we are counting in SAT. We discuss three different approaches\nto projected model counting (two of which are novel), and compare their\nperformance on different benchmark problems.\n  To appear in 18th International Conference on Theory and Applications of\nSatisfiability Testing, September 24-27, 2015, Austin, Texas, USA\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 05:45:05 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Aziz", "Rehan Abdul", ""], ["Chu", "Geoffrey", ""], ["Muise", "Christian", ""], ["Stuckey", "Peter", ""]]}, {"id": "1507.07677", "submitter": "Branislav Bosansky", "authors": "Branislav Bosansky and Simina Branzei and Kristoffer Arnsfelt Hansen\n  and Peter Bro Miltersen and Troels Bjerre Sorensen", "title": "Computation of Stackelberg Equilibria of Finite Sequential Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Stackelberg equilibrium solution concept describes optimal strategies to\ncommit to: Player 1 (termed the leader) publicly commits to a strategy and\nPlayer 2 (termed the follower) plays a best response to this strategy (ties are\nbroken in favor of the leader). We study Stackelberg equilibria in finite\nsequential games (or extensive-form games) and provide new exact algorithms,\napproximate algorithms, and hardness results for several classes of these\nsequential games.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 08:16:56 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 18:26:23 GMT"}, {"version": "v3", "created": "Tue, 23 Aug 2016 16:28:19 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Bosansky", "Branislav", ""], ["Branzei", "Simina", ""], ["Hansen", "Kristoffer Arnsfelt", ""], ["Miltersen", "Peter Bro", ""], ["Sorensen", "Troels Bjerre", ""]]}, {"id": "1507.07688", "submitter": "Stefano Albrecht", "authors": "Stefano V. Albrecht, Jacob W. Crandall, Subramanian Ramamoorthy", "title": "Belief and Truth in Hypothesised Behaviours", "comments": "44 pages; final manuscript published in Artificial Intelligence (AIJ)", "journal-ref": null, "doi": "10.1016/j.artint.2016.02.004", "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a long history in game theory on the topic of Bayesian or \"rational\"\nlearning, in which each player maintains beliefs over a set of alternative\nbehaviours, or types, for the other players. This idea has gained increasing\ninterest in the artificial intelligence (AI) community, where it is used as a\nmethod to control a single agent in a system composed of multiple agents with\nunknown behaviours. The idea is to hypothesise a set of types, each specifying\na possible behaviour for the other agents, and to plan our own actions with\nrespect to those types which we believe are most likely, given the observed\nactions of the agents. The game theory literature studies this idea primarily\nin the context of equilibrium attainment. In contrast, many AI applications\nhave a focus on task completion and payoff maximisation. With this perspective\nin mind, we identify and address a spectrum of questions pertaining to belief\nand truth in hypothesised types. We formulate three basic ways to incorporate\nevidence into posterior beliefs and show when the resulting beliefs are\ncorrect, and when they may fail to be correct. Moreover, we demonstrate that\nprior beliefs can have a significant impact on our ability to maximise payoffs\nin the long-term, and that they can be computed automatically with consistent\nperformance effects. Furthermore, we analyse the conditions under which we are\nable complete our task optimally, despite inaccuracies in the hypothesised\ntypes. Finally, we show how the correctness of hypothesised types can be\nascertained during the interaction via an automated statistical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 08:52:45 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 14:39:45 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 18:38:32 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Albrecht", "Stefano V.", ""], ["Crandall", "Jacob W.", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1507.07749", "submitter": "Joseph Ramsey", "authors": "Joseph D. Ramsey", "title": "Scaling up Greedy Causal Search for Continuous Variables", "comments": "12 pages, 2 figures, tech report for Center for Causal Discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As standardly implemented in R or the Tetrad program, causal search\nalgorithms used most widely or effectively by scientists have severe\ndimensionality constraints that make them inappropriate for big data problems\nwithout sacrificing accuracy. However, implementation improvements are\npossible. We explore optimizations for the Greedy Equivalence Search that allow\nsearch on 50,000-variable problems in 13 minutes for sparse models with 1000\nsamples on a four-processor, 16G laptop computer. We finish a problem with 1000\nsamples on 1,000,000 variables in 18 hours for sparse models on a supercomputer\nnode at the Pittsburgh Supercomputing Center with 40 processors and 384 G RAM.\nThe same algorithm can be applied to discrete data, with a slower discrete\nscore, though the discrete implementation currently does not scale as well in\nour experiments; we have managed to scale up to about 10,000 variables in\nsparse models with 1000 samples.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 12:59:19 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 22:55:28 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Ramsey", "Joseph D.", ""]]}, {"id": "1507.07870", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist and Peter Sarlin", "title": "Detect & Describe: Deep learning of bank stress in the news", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.AI cs.LG cs.NE q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  News is a pertinent source of information on financial risks and stress\nfactors, which nevertheless is challenging to harness due to the sparse and\nunstructured nature of natural text. We propose an approach based on\ndistributional semantics and deep learning with neural networks to model and\nlink text to a scarce set of bank distress events. Through unsupervised\ntraining, we learn semantic vector representations of news articles as\npredictors of distress events. The predictive model that we learn can signal\ncoinciding stress with an aggregated index at bank or European level, while\ncrucially allowing for automatic extraction of text descriptions of the events,\nbased on passages with high stress levels. The method offers insight that\nmodels based on other types of data cannot provide, while offering a general\nmeans for interpreting this type of semantic-predictive model. We model bank\ndistress with data on 243 events and 6.6M news articles for 101 large European\nbanks.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 18:47:09 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Sarlin", "Peter", ""]]}, {"id": "1507.07998", "submitter": "Andrew Dai", "authors": "Andrew M. Dai and Christopher Olah and Quoc V. Le", "title": "Document Embedding with Paragraph Vectors", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paragraph Vectors has been recently proposed as an unsupervised method for\nlearning distributed representations for pieces of texts. In their work, the\nauthors showed that the method can learn an embedding of movie review texts\nwhich can be leveraged for sentiment analysis. That proof of concept, while\nencouraging, was rather narrow. Here we consider tasks other than sentiment\nanalysis, provide a more thorough comparison of Paragraph Vectors to other\ndocument modelling algorithms such as Latent Dirichlet Allocation, and evaluate\nperformance of the method as we vary the dimensionality of the learned\nrepresentation. We benchmarked the models on two document similarity data sets,\none from Wikipedia, one from arXiv. We observe that the Paragraph Vector method\nperforms significantly better than other methods, and propose a simple\nimprovement to enhance embedding quality. Somewhat surprisingly, we also show\nthat much like word embeddings, vector operations on Paragraph Vectors can\nperform useful semantic results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 01:04:28 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Dai", "Andrew M.", ""], ["Olah", "Christopher", ""], ["Le", "Quoc V.", ""]]}, {"id": "1507.08073", "submitter": "Jian Yu", "authors": "Jian Yu", "title": "Communication: Words and Conceptual Systems", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Words (phrases or symbols) play a key role in human life. Word (phrase or\nsymbol) representation is the fundamental problem for knowledge representation\nand understanding. A word (phrase or symbol) usually represents a name of a\ncategory. However, it is always a challenge that how to represent a category\ncan make it easily understood. In this paper, a new representation for a\ncategory is discussed, which can be considered a generalization of classic set.\nIn order to reduce representation complexity, the economy principle of category\nrepresentation is proposed. The proposed category representation provides a\npowerful tool for analyzing conceptual systems, relations between words,\ncommunication, knowledge, situations. More specifically, the conceptual system,\nword relations and communication are mathematically defined and classified such\nas ideal conceptual system, perfect communication and so on; relation between\nwords and sentences is also studied, which shows that knowledge are words.\nFurthermore, how conceptual systems and words depend on situations is\npresented, and how truth is defined is also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 09:21:15 GMT"}, {"version": "v10", "created": "Tue, 15 Sep 2015 09:23:02 GMT"}, {"version": "v11", "created": "Wed, 16 Sep 2015 02:13:24 GMT"}, {"version": "v12", "created": "Wed, 28 Oct 2015 00:56:45 GMT"}, {"version": "v13", "created": "Mon, 16 Nov 2015 02:12:17 GMT"}, {"version": "v14", "created": "Fri, 4 Dec 2015 03:36:06 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2015 12:13:07 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2015 14:24:38 GMT"}, {"version": "v4", "created": "Tue, 25 Aug 2015 14:02:14 GMT"}, {"version": "v5", "created": "Wed, 26 Aug 2015 16:58:08 GMT"}, {"version": "v6", "created": "Thu, 27 Aug 2015 14:39:39 GMT"}, {"version": "v7", "created": "Mon, 31 Aug 2015 03:35:03 GMT"}, {"version": "v8", "created": "Sun, 6 Sep 2015 22:23:44 GMT"}, {"version": "v9", "created": "Wed, 9 Sep 2015 09:37:39 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Yu", "Jian", ""]]}, {"id": "1507.08271", "submitter": "Guy Lever Dr", "authors": "Thomas Furmston and Guy Lever", "title": "A Gauss-Newton Method for Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Newton methods are a standard optimization tool which aim to\nmaintain the benefits of Newton's method, such as a fast rate of convergence,\nwhilst alleviating its drawbacks, such as computationally expensive calculation\nor estimation of the inverse Hessian. In this work we investigate approximate\nNewton methods for policy optimization in Markov Decision Processes (MDPs). We\nfirst analyse the structure of the Hessian of the objective function for MDPs.\nWe show that, like the gradient, the Hessian exhibits useful structure in the\ncontext of MDPs and we use this analysis to motivate two Gauss-Newton Methods\nfor MDPs. Like the Gauss-Newton method for non-linear least squares, these\nmethods involve approximating the Hessian by ignoring certain terms in the\nHessian which are difficult to estimate. The approximate Hessians possess\ndesirable properties, such as negative definiteness, and we demonstrate several\nimportant performance guarantees including guaranteed ascent directions,\ninvariance to affine transformation of the parameter space, and convergence\nguarantees. We finally provide a unifying perspective of key policy search\nalgorithms, demonstrating that our second Gauss-Newton algorithm is closely\nrelated to both the EM-algorithm and natural gradient ascent applied to MDPs,\nbut performs significantly better in practice on a range of challenging\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 19:37:24 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 19:08:37 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2015 17:33:39 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2015 14:02:01 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Furmston", "Thomas", ""], ["Lever", "Guy", ""]]}, {"id": "1507.08444", "submitter": "Indre Zliobaite", "authors": "Indre Zliobaite and Mikhail Khokhlov", "title": "Optimal estimates for short horizon travel time prediction in urban\n  areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing popularity of mobile route planning applications based on GPS\ntechnology provides opportunities for collecting traffic data in urban\nenvironments. One of the main challenges for travel time estimation and\nprediction in such a setting is how to aggregate data from vehicles that have\nfollowed different routes, and predict travel time for other routes of\ninterest. One approach is to predict travel times for route segments, and sum\nthose estimates to obtain a prediction for the whole route. We study how to\nobtain optimal predictions in this scenario. It appears that the optimal\nestimate, minimizing the expected mean absolute error, is a combination of the\nmean and the median travel times on each segment, where the combination\nfunction depends on the number of segments in the route of interest. We present\na methodology for obtaining such predictions, and demonstrate its effectiveness\nwith a case study using travel time data from a district of St. Petersburg\ncollected over one year. The proposed methodology can be applied for real-time\nprediction of expected travel times in an urban road network.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 10:46:52 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 08:45:42 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Zliobaite", "Indre", ""], ["Khokhlov", "Mikhail", ""]]}, {"id": "1507.08467", "submitter": "David Sousa-Rodrigues", "authors": "Cristian Jimenez-Romero and David Sousa-Rodrigues and Jeffrey H.\n  Johnson and Vitorino Ramos", "title": "A Model for Foraging Ants, Controlled by Spiking Neural Networks and\n  Double Pheromones", "comments": "This work has been accepted for presentation at the UK Workshop on\n  Computational Intelligence --- University of Exeter, September 2015\n  http://www.ukci2015.ex.ac.uk/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model of an Ant System where ants are controlled by a spiking neural\ncircuit and a second order pheromone mechanism in a foraging task is presented.\nA neural circuit is trained for individual ants and subsequently the ants are\nexposed to a virtual environment where a swarm of ants performed a resource\nforaging task. The model comprises an associative and unsupervised learning\nstrategy for the neural circuit of the ant. The neural circuit adapts to the\nenvironment by means of classical conditioning. The initially unknown\nenvironment includes different types of stimuli representing food and obstacles\nwhich, when they come in direct contact with the ant, elicit a reflex response\nin the motor neural system of the ant: moving towards or away from the source\nof the stimulus. The ants are released on a landscape with multiple food\nsources where one ant alone would have difficulty harvesting the landscape to\nmaximum efficiency. The introduction of a double pheromone mechanism yields\nbetter results than traditional ant colony optimization strategies. Traditional\nant systems include mainly a positive reinforcement pheromone. This approach\nuses a second pheromone that acts as a marker for forbidden paths (negative\nfeedback). This blockade is not permanent and is controlled by the evaporation\nrate of the pheromones. The combined action of both pheromones acts as a\ncollective stigmergic memory of the swarm, which reduces the search space of\nthe problem. This paper explores how the adaptation and learning abilities\nobserved in biologically inspired cognitive architectures is synergistically\nenhanced by swarm optimization strategies. The model portraits two forms of\nartificial intelligent behaviour: at the individual level the spiking neural\nnetwork is the main controller and at the collective level the pheromone\ndistribution is a map towards the solution emerged by the colony.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 11:57:54 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 09:25:03 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2015 14:17:39 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Jimenez-Romero", "Cristian", ""], ["Sousa-Rodrigues", "David", ""], ["Johnson", "Jeffrey H.", ""], ["Ramos", "Vitorino", ""]]}, {"id": "1507.08482", "submitter": "Vedran Dunjko", "authors": "Vedran Dunjko, Jacob M. Taylor and Hans J. Briegel", "title": "Framework for learning agents in quantum environments", "comments": "39 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a broad framework for describing learning agents in\ngeneral quantum environments. We analyze the types of classically specified\nenvironments which allow for quantum enhancements in learning, by contrasting\nenvironments to quantum oracles. We show that whether or not quantum\nimprovements are at all possible depends on the internal structure of the\nquantum environment. If the environments are constructed and the internal\nstructure is appropriately chosen, or if the agent has limited capacities to\ninfluence the internal states of the environment, we show that improvements in\nlearning times are possible in a broad range of scenarios. Such scenarios we\ncall luck-favoring settings. The case of constructed environments is\nparticularly relevant for the class of model-based learning agents, where our\nresults imply a near-generic improvement.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 12:58:14 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Dunjko", "Vedran", ""], ["Taylor", "Jacob M.", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1507.08559", "submitter": "Ganesh Ram Santhanam", "authors": "Ganesh Ram Santhanam and Samik Basu and Vasant Honavar", "title": "CRISNER: A Practically Efficient Reasoner for Qualitative Preferences", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CRISNER (Conditional & Relative Importance Statement Network\nPrEference Reasoner), a tool that provides practically efficient as well as\nexact reasoning about qualitative preferences in popular ceteris paribus\npreference languages such as CP-nets, TCP-nets, CP-theories, etc. The tool uses\na model checking engine to translate preference specifications and queries into\nappropriate Kripke models and verifiable properties over them respectively. The\ndistinguishing features of the tool are: (1) exact and provably correct query\nanswering for testing dominance, consistency with respect to a preference\nspecification, and testing equivalence and subsumption of two sets of\npreferences; (2) automatic generation of proofs evidencing the correctness of\nanswer produced by CRISNER to any of the above queries; (3) XML inputs and\noutputs that make it portable and pluggable into other applications. We also\ndescribe the extensible architecture of CRISNER, which can be extended to new\nreference formalisms based on ceteris paribus semantics that may be developed\nin the future.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 16:03:48 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Santhanam", "Ganesh Ram", ""], ["Basu", "Samik", ""], ["Honavar", "Vasant", ""]]}, {"id": "1507.08717", "submitter": "EPTCS", "authors": "Christoph Benzm\\\"uller (Freie Universit\\\"at Berlin, Germany),\n  Maximilian Claus (Freie Universit\\\"at Berlin, Germany), Nik Sultana\n  (Cambridge University, UK)", "title": "Systematic Verification of the Modal Logic Cube in Isabelle/HOL", "comments": "In Proceedings PxTP 2015, arXiv:1507.08375", "journal-ref": "EPTCS 186, 2015, pp. 27-41", "doi": "10.4204/EPTCS.186.5", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an automated verification of the well-known modal logic cube in\nIsabelle/HOL, in which we prove the inclusion relations between the cube's\nlogics using automated reasoning tools. Prior work addresses this problem but\nwithout restriction to the modal logic cube, and using encodings in first-order\nlogic in combination with first-order automated theorem provers. In contrast,\nour solution is more elegant, transparent and effective. It employs an\nembedding of quantified modal logic in classical higher-order logic. Automated\nreasoning tools, such as Sledgehammer with LEO-II, Satallax and CVC4, Metis and\nNitpick, are employed to achieve full automation. Though successful, the\nexperiments also motivate some technical improvements in the Isabelle/HOL tool.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 00:58:44 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Benzm\u00fcller", "Christoph", "", "Freie Universit\u00e4t Berlin, Germany"], ["Claus", "Maximilian", "", "Freie Universit\u00e4t Berlin, Germany"], ["Sultana", "Nik", "", "Cambridge University, UK"]]}, {"id": "1507.08726", "submitter": "Jelena Bradic", "authors": "Jelena Bradic", "title": "Robustness in sparse linear models: relative efficiency based on robust\n  approximate message passing", "comments": "49 pages, 10 figures", "journal-ref": "Electronic Journal of Statistics, Volume 10, Number 2 (2016),\n  3894-3944", "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding efficiency in high dimensional linear models is a longstanding\nproblem of interest. Classical work with smaller dimensional problems dating\nback to Huber and Bickel has illustrated the benefits of efficient loss\nfunctions. When the number of parameters $p$ is of the same order as the sample\nsize $n$, $p \\approx n$, an efficiency pattern different from the one of Huber\nwas recently established. In this work, we consider the effects of model\nselection on the estimation efficiency of penalized methods. In particular, we\nexplore whether sparsity, results in new efficiency patterns when $p > n$. In\nthe interest of deriving the asymptotic mean squared error for regularized\nM-estimators, we use the powerful framework of approximate message passing. We\npropose a novel, robust and sparse approximate message passing algorithm\n(RAMP), that is adaptive to the error distribution. Our algorithm includes many\nnon-quadratic and non-differentiable loss functions. We derive its asymptotic\nmean squared error and show its convergence, while allowing $p, n, s \\to\n\\infty$, with $n/p \\in (0,1)$ and $n/s \\in (1,\\infty)$. We identify new\npatterns of relative efficiency regarding a number of penalized $M$ estimators,\nwhen $p$ is much larger than $n$. We show that the classical information bound\nis no longer reachable, even for light--tailed error distributions. We show\nthat the penalized least absolute deviation estimator dominates the penalized\nleast square estimator, in cases of heavy--tailed distributions. We observe\nthis pattern for all choices of the number of non-zero parameters $s$, both $s\n\\leq n$ and $s \\approx n$. In non-penalized problems where $s =p \\approx n$,\nthe opposite regime holds. Therefore, we discover that the presence of model\nselection significantly changes the efficiency patterns.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 01:31:24 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 00:27:46 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Bradic", "Jelena", ""]]}, {"id": "1507.08750", "submitter": "Junhyuk Oh", "authors": "Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh", "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games", "comments": "Published at NIPS 2015 (Advances in Neural Information Processing\n  Systems 28)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by vision-based reinforcement learning (RL) problems, in particular\nAtari games from the recent benchmark Aracade Learning Environment (ALE), we\nconsider spatio-temporal prediction problems where future (image-)frames are\ndependent on control variables or actions as well as previous frames. While not\ncomposed of natural scenes, frames in Atari games are high-dimensional in size,\ncan involve tens of objects with one or more objects being controlled by the\nactions directly and many other objects being influenced indirectly, can\ninvolve entry and departure of objects, and can involve deep partial\nobservability. We propose and evaluate two deep neural network architectures\nthat consist of encoding, action-conditional transformation, and decoding\nlayers based on convolutional neural networks and recurrent neural networks.\nExperimental results show that the proposed architectures are able to generate\nvisually-realistic frames that are also useful for control over approximately\n100-step action-conditional futures in some games. To the best of our\nknowledge, this paper is the first to make and evaluate long-term predictions\non high-dimensional video conditioned by control inputs.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 04:43:30 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 04:26:54 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Oh", "Junhyuk", ""], ["Guo", "Xiaoxiao", ""], ["Lee", "Honglak", ""], ["Lewis", "Richard", ""], ["Singh", "Satinder", ""]]}, {"id": "1507.08826", "submitter": "Matteo Brunelli", "authors": "Matteo Brunelli", "title": "Studying a set of properties of inconsistency indices for pairwise\n  comparisons", "comments": null, "journal-ref": null, "doi": "10.1007/s10479-016-2166-8", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise comparisons between alternatives are a well-established tool to\ndecompose decision problems into smaller and more easily tractable\nsub-problems. However, due to our limited rationality, the subjective\npreferences expressed by decision makers over pairs of alternatives can hardly\never be consistent. Therefore, several inconsistency indices have been proposed\nin the literature to quantify the extent of the deviation from complete\nconsistency. Only recently, a set of properties has been proposed to define a\nfamily of functions representing inconsistency indices. The scope of this paper\nis twofold. Firstly, it expands the set of properties by adding and justifying\na new one. Secondly, it continues the study of inconsistency indices to check\nwhether or not they satisfy the above mentioned properties. Out of the four\nindices considered in this paper, in its present form, two fail to satisfy some\nproperties. An adjusted version of one index is proposed so that it fulfills\nthem.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 10:56:40 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 08:36:12 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Brunelli", "Matteo", ""]]}]