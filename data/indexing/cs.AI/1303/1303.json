[{"id": "1303.0213", "submitter": "Phillip Lord Dr", "authors": "Phillip Lord", "title": "The Semantic Web takes Wing: Programming Ontologies with Tawny-OWL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Tawny-OWL library provides a fully-programmatic environment for ontology\nbuilding; it enables the use of a rich set of tools for ontology development,\nby recasting development as a form of programming. It is built in Clojure - a\nmodern Lisp dialect, and is backed by the OWL API. Used simply, it has a\nsimilar syntax to OWL Manchester syntax, but it provides arbitrary\nextensibility and abstraction. It builds on existing facilities for Clojure,\nwhich provides a rich and modern programming tool chain, for versioning,\ndistributed development, build, testing and continuous integration. In this\npaper, we describe the library, this environment and the its potential\nimplications for the ontology development process.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 16:35:19 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["Lord", "Phillip", ""]]}, {"id": "1303.0691", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Learning AMP Chain Graphs and some Marginal Models Thereof under\n  Faithfulness: Extended Version", "comments": "Changes from v1 to v2: The interpretation of the antecedent of the\n  rule R3 changed, which in turn implied modifying Lemma 6 and Theorem 1.\n  Changes from v2 to v3: Minor improvements in the first 12 pages. A shorter\n  version is to appear in International Journal of Approximate Reasoning, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with chain graphs under the Andersson-Madigan-Perlman (AMP)\ninterpretation. In particular, we present a constraint based algorithm for\nlearning an AMP chain graph a given probability distribution is faithful to.\nMoreover, we show that the extension of Meek's conjecture to AMP chain graphs\ndoes not hold, which compromises the development of efficient and correct\nscore+search learning algorithms under assumptions weaker than faithfulness.\n  We also introduce a new family of graphical models that consists of\nundirected and bidirected edges. We name this new family maximal\ncovariance-concentration graphs (MCCGs) because it includes both covariance and\nconcentration graphs as subfamilies. However, every MCCG can be seen as the\nresult of marginalizing out some nodes in an AMP CG. We describe global, local\nand pairwise Markov properties for MCCGs and prove their equivalence. We\ncharacterize when two MCCGs are Markov equivalent, and show that every Markov\nequivalence class of MCCGs has a distinguished member. We present a constraint\nbased algorithm for learning a MCCG a given probability distribution is\nfaithful to.\n  Finally, we present a graphical criterion for reading dependencies from a\nMCCG of a probability distribution that satisfies the graphoid properties, weak\ntransitivity and composition. We prove that the criterion is sound and complete\nin certain sense.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 13:02:49 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2013 11:55:54 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2014 13:07:42 GMT"}], "update_date": "2014-01-20", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1303.0787", "submitter": "EPTCS", "authors": "Umberto Grandi (University of Padova), Andrea Loreggia (University of\n  Padova), Francesca Rossi (University of Padova), Kristen Brent Venable\n  (Tulane University and IHMC), Toby Walsh (NICTA and UNSW)", "title": "Restricted Manipulation in Iterative Voting: Convergence and Condorcet\n  Efficiency", "comments": "In Proceedings SR 2013, arXiv:1303.0071", "journal-ref": "EPTCS 112, 2013, pp. 17-24", "doi": "10.4204/EPTCS.112.6", "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In collective decision making, where a voting rule is used to take a\ncollective decision among a group of agents, manipulation by one or more agents\nis usually considered negative behavior to be avoided, or at least to be made\ncomputationally difficult for the agents to perform. However, there are\nscenarios in which a restricted form of manipulation can instead be beneficial.\nIn this paper we consider the iterative version of several voting rules, where\nat each step one agent is allowed to manipulate by modifying his ballot\naccording to a set of restricted manipulation moves which are computationally\neasy and require little information to be performed. We prove convergence of\niterative voting rules when restricted manipulation is allowed, and we present\nexperiments showing that most iterative voting rules have a higher Condorcet\nefficiency than their non-iterative version.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 19:05:45 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Grandi", "Umberto", "", "University of Padova"], ["Loreggia", "Andrea", "", "University of\n  Padova"], ["Rossi", "Francesca", "", "University of Padova"], ["Venable", "Kristen Brent", "", "Tulane University and IHMC"], ["Walsh", "Toby", "", "NICTA and UNSW"]]}, {"id": "1303.0794", "submitter": "EPTCS", "authors": "Dimitar P. Guelev (Institute of Mathematics and Informatics, Bulgarian\n  Academy of Sciences)", "title": "Reducing Validity in Epistemic ATL to Validity in Epistemic CTL", "comments": "In Proceedings SR 2013, arXiv:1303.0071", "journal-ref": "EPTCS 112, 2013, pp. 81-89", "doi": "10.4204/EPTCS.112.13", "report-no": null, "categories": "cs.LO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a validity preserving translation from a subset of epistemic\nAlternating-time Temporal Logic (ATL) to epistemic Computation Tree Logic\n(CTL). The considered subset of epistemic ATL is known to have the finite model\nproperty and decidable model-checking. This entails the decidability of\nvalidity but the implied algorithm is unfeasible. Reducing the validity problem\nto that in a corresponding system of CTL makes the techniques for automated\ndeduction for that logic available for the handling of the apparently more\ncomplex system of ATL.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 19:06:40 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Guelev", "Dimitar P.", "", "Institute of Mathematics and Informatics, Bulgarian\n  Academy of Sciences"]]}, {"id": "1303.0875", "submitter": "Sergio Romano", "authors": "Sergio Romano, Mariano Sigman, Santiago Figueira", "title": "LT^2C^2: A language of thought with Turing-computable Kolmogorov\n  complexity", "comments": "14 pages, 4 figures", "journal-ref": "Papers in Physics 5, 050001 (2013)", "doi": "10.4279/PIP.050001", "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we present a theoretical effort to connect the theory of\nprogram size to psychology by implementing a concrete language of thought with\nTuring-computable Kolmogorov complexity (LT^2C^2) satisfying the following\nrequirements: 1) to be simple enough so that the complexity of any given finite\nbinary sequence can be computed, 2) to be based on tangible operations of human\nreasoning (printing, repeating,...), 3) to be sufficiently powerful to generate\nall possible sequences but not too powerful as to identify regularities which\nwould be invisible to humans. We first formalize LT^2C^2, giving its syntax and\nsemantics and defining an adequate notion of program size. Our setting leads to\na Kolmogorov complexity function relative to LT^2C^2 which is computable in\npolynomial time, and it also induces a prediction algorithm in the spirit of\nSolomonoff's inductive inference theory. We then prove the efficacy of this\nlanguage by investigating regularities in strings produced by participants\nattempting to generate random strings. Participants had a profound\nunderstanding of randomness and hence avoided typical misconceptions such as\nexaggerating the number of alternations. We reasoned that remaining\nregularities would express the algorithmic nature of human thoughts, revealed\nin the form of specific patterns. Kolmogorov complexity relative to LT^2C^2\npassed three expected tests examined here: 1) human sequences were less complex\nthan control PRNG sequences, 2) human sequences were not stationary, showing\ndecreasing values of complexity resulting from fatigue, 3) each individual\nshowed traces of algorithmic stability since fitting of partial sequences was\nmore effective to predict subsequent sequences than average fits. This work\nextends on previous efforts to combine notions of Kolmogorov complexity theory\nand algorithmic information theory to psychology, by explicitly ...\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 21:51:55 GMT"}], "update_date": "2013-03-06", "authors_parsed": [["Romano", "Sergio", ""], ["Sigman", "Mariano", ""], ["Figueira", "Santiago", ""]]}, {"id": "1303.0934", "submitter": "Matteo Santoro", "authors": "Andrea Tacchetti, Pavan K Mallapragada, Matteo Santoro, Lorenzo\n  Rosasco", "title": "GURLS: a Least Squares Library for Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GURLS, a least squares, modular, easy-to-extend software library\nfor efficient supervised learning. GURLS is targeted to machine learning\npractitioners, as well as non-specialists. It offers a number state-of-the-art\ntraining strategies for medium and large-scale learning, and routines for\nefficient model selection. The library is particularly well suited for\nmulti-output problems (multi-category/multi-label). GURLS is currently\navailable in two independent implementations: Matlab and C++. It takes\nadvantage of the favorable properties of regularized least squares algorithm to\nexploit advanced tools in linear algebra. Routines to handle computations with\nvery large matrices by means of memory-mapped storage and distributed task\nexecution are available. The package is distributed under the BSD licence and\nis available for download at https://github.com/CBCL/GURLS.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2013 05:55:59 GMT"}], "update_date": "2013-03-06", "authors_parsed": [["Tacchetti", "Andrea", ""], ["Mallapragada", "Pavan K", ""], ["Santoro", "Matteo", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1303.1232", "submitter": "Jessica C. Ram\\'irez", "authors": "Jessica Ram\\'irez, Masayuki Asahara, Yuji Matsumoto", "title": "Japanese-Spanish Thesaurus Construction Using English as a Pivot", "comments": null, "journal-ref": "In Proceeding of The Third International Joint Conference on\n  Natural Language Processing (IJCNLP-08), Hyderabad, India. pages 473-480,\n  2008", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the results of research with the goal of automatically creating a\nmultilingual thesaurus based on the freely available resources of Wikipedia and\nWordNet. Our goal is to increase resources for natural language processing\ntasks such as machine translation targeting the Japanese-Spanish language pair.\nGiven the scarcity of resources, we use existing English resources as a pivot\nfor creating a trilingual Japanese-Spanish-English thesaurus. Our approach\nconsists of extracting the translation tuples from Wikipedia, disambiguating\nthem by mapping them to WordNet word senses. We present results comparing two\nmethods of disambiguation, the first using VSM on Wikipedia article texts and\nWordNet definitions, and the second using categorical information extracted\nfrom Wikipedia, We find that mixing the two methods produces favorable results.\nUsing the proposed method, we have constructed a multilingual\nSpanish-Japanese-English thesaurus consisting of 25,375 entries. The same\nmethod can be applied to any pair of languages that are linked to English in\nWikipedia.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 01:30:58 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Ram\u00edrez", "Jessica", ""], ["Asahara", "Masayuki", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "1303.1384", "submitter": "Silvia Crafa", "authors": "Silvia Crafa and Federica Russo", "title": "Causality in concurrent systems", "comments": "This is an interdisciplinary paper. It addresses a class of causal\n  models developed in computer science from an epistemic perspective, namely in\n  terms of philosophy of causality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent systems identify systems, either software, hardware or even\nbiological systems, that are characterized by sets of independent actions that\ncan be executed in any order or simultaneously. Computer scientists resort to a\ncausal terminology to describe and analyse the relations between the actions in\nthese systems. However, a thorough discussion about the meaning of causality in\nsuch a context has not been developed yet. This paper aims to fill the gap.\nFirst, the paper analyses the notion of causation in concurrent systems and\nattempts to build bridges with the existing philosophical literature,\nhighlighting similarities and divergences between them. Second, the paper\nanalyses the use of counterfactual reasoning in ex-post analysis in concurrent\nsystems (i.e. execution trace analysis).\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 16:50:06 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Crafa", "Silvia", ""], ["Russo", "Federica", ""]]}, {"id": "1303.1454", "submitter": "Marek J. Druzdzel", "authors": "Marek J. Druzdzel, Herbert A. Simon", "title": "Causality in Bayesian Belief Networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-3-11", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of causal interpretation of the graphical structure of\nBayesian belief networks (BBNs). We review the concept of causality explicated\nin the domain of structural equations models and show that it is applicable to\nBBNs. In this view, which we call mechanism-based, causality is defined within\nmodels and causal asymmetries arise when mechanisms are placed in the context\nof a system. We lay the link between structural equations models and BBNs\nmodels and formulate the conditions under which the latter can be given causal\ninterpretation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:18:23 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Druzdzel", "Marek J.", ""], ["Simon", "Herbert A.", ""]]}, {"id": "1303.1455", "submitter": "Judea Pearl", "authors": "Judea Pearl", "title": "From Conditional Oughts to Qualitative Decision Theory", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-12-20", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary theme of this investigation is a decision theoretic account of\nconditional ought statements (e.g., \"You ought to do A, if C\") that rectifies\nglaring deficiencies in classical deontic logic. The resulting account forms a\nsound basis for qualitative decision theory, thus providing a framework for\nqualitative planning under uncertainty. In particular, we show that adding\ncausal relationships (in the form of a single graph) as part of an epistemic\nstate is sufficient to facilitate the analysis of action sequences, their\nconsequences, their interaction with observations, their expected utilities\nand, hence, the synthesis of plans and strategies under uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:18:29 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Pearl", "Judea", ""]]}, {"id": "1303.1456", "submitter": "Russ B. Altman", "authors": "Russ B. Altman", "title": "A Probabilistic Algorithm for Calculating Structure: Borrowing from\n  Simulated Annealing", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-23-31", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a general Bayesian algorithm for determining the\ncoordinates of points in a three-dimensional space. The algorithm takes as\ninput a set of probabilistic constraints on the coordinates of the points, and\nan a priori distribution for each point location. The output is a\nmaximum-likelihood estimate of the location of each point. We use the extended,\niterated Kalman filter, and add a search heuristic for optimizing its solution\nunder nonlinear conditions. This heuristic is based on the same principle as\nthe simulated annealing heuristic for other optimization problems. Our method\nuses any probabilistic constraints that can be expressed as a function of the\npoint coordinates (for example, distance, angles, dihedral angles, and\nplanarity). It assumes that all constraints have Gaussian noise. In this paper,\nwe describe the algorithm and show its performance on a set of synthetic data\nto illustrate its convergence properties, and its applicability to domains such\nng molecular structure determination.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:18:35 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Altman", "Russ B.", ""]]}, {"id": "1303.1457", "submitter": "Scott A. Musman", "authors": "Scott A. Musman, L. W. Chang", "title": "A Study of Scaling Issues in Bayesian Belief Networks for Ship\n  Classification", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-32-39", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problems associated with scaling involve active and challenging research\ntopics in the area of artificial intelligence. The purpose is to solve real\nworld problems by means of AI technologies, in cases where the complexity of\nrepresentation of the real world problem is potentially combinatorial. In this\npaper, we present a novel approach to cope with the scaling issues in Bayesian\nbelief networks for ship classification. The proposed approach divides the\nconceptual model of a complex ship classification problem into a set of small\nmodules that work together to solve the classification problem while preserving\nthe functionality of the original model. The possible ways of explaining sensor\nreturns (e.g., the evidence) for some features, such as portholes along the\nlength of a ship, are sometimes combinatorial. Thus, using an exhaustive\napproach, which entails the enumeration of all possible explanations, is\nimpractical for larger problems. We present a network structure (referred to as\nSequential Decomposition, SD) in which each observation is associated with a\nset of legitimate outcomes which are consistent with the explanation of each\nobserved piece of evidence. The results show that the SD approach allows one to\nrepresent feature-observation relations in a manageable way and achieve the\nsame explanatory power as an exhaustive approach.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:18:41 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Musman", "Scott A.", ""], ["Chang", "L. W.", ""]]}, {"id": "1303.1458", "submitter": "Gregory M. Provan", "authors": "Gregory M. Provan", "title": "Tradeoffs in Constructing and Evaluating Temporal Influence Diagrams", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-40-47", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the tradeoffs which need to be considered in reasoning\nusing probabilistic network representations, such as Influence Diagrams (IDs).\nIn particular, we examine the tradeoffs entailed in using Temporal Influence\nDiagrams (TIDs) which adequately capture the temporal evolution of a dynamic\nsystem without prohibitive data and computational requirements. Three\napproaches for TID construction which make different tradeoffs are examined:\n(1) tailoring the network at each time interval to the data available (rather\nthen just copying the original Bayes Network for all time intervals); (2)\nmodeling the evolution of a parsimonious subset of variables (rather than all\nvariables); and (3) model selection approaches, which seek to minimize some\nmeasure of the predictive accuracy of the model without introducing too many\nparameters, which might cause \"overfitting\" of the model. Methods of evaluating\nthe accuracy/efficiency of the tradeoffs are proposed.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:18:46 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Provan", "Gregory M.", ""]]}, {"id": "1303.1459", "submitter": "Harold P. Lehmann", "authors": "Harold P. Lehmann, Ross D. Shachter", "title": "End-User Construction of Influence Diagrams for Bayesian Statistics", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-48-54", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence diagrams are ideal knowledge representations for Bayesian\nstatistical models. However, these diagrams are difficult for end users to\ninterpret and to manipulate. We present a user-based architecture that enables\nend users to create and to manipulate the knowledge representation. We use the\nproblem of physicians' interpretation of two-arm parallel randomized clinical\ntrials (TAPRCT) to illustrate the architecture and its use. There are three\nprimary data structures. Elements of statistical models are encoded as\nsubgraphs of a restricted class of influence diagram. The interpretations of\nthose elements are mapped into users' language in a domain-specific, user-based\nsemantic interface, called a patient-flow diagram, in the TAPRCT problem.\nPennitted transformations of the statistical model that maintain the semantic\nrelationships of the model are encoded in a metadata-state diagram, called the\ncohort-state diagram, in the TAPRCT problem. The algorithm that runs the system\nuses modular actions called construction steps. This framework has been\nimplemented in a system called THOMAS, that allows physicians to interpret the\ndata reported from a TAPRCT.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:18:52 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Lehmann", "Harold P.", ""], ["Shachter", "Ross D.", ""]]}, {"id": "1303.1460", "submitter": "Steven M. LaValle", "authors": "Steven M. LaValle, Seth A. Hutchinson", "title": "On Considering Uncertainty and Alternatives in Low-Level Vision", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-55-63", "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the uncertainty issues involved in the low-level\nvision task of image segmentation. Researchers in computer vision have worked\nextensively on this problem, in which the goal is to partition (or segment) an\nimage into regions that are homogeneous or uniform in some sense. This\nsegmentation is often utilized by some higher level process, such as an object\nrecognition system. We show that by considering uncertainty in a Bayesian\nformalism, we can use statistical image models to build an approximate\nrepresentation of a probability distribution over a space of alternative\nsegmentations. We give detailed descriptions of the various levels of\nuncertainty associated with this problem, discuss the interaction of prior and\nposterior distributions, and provide the operations for constructing this\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:18:58 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["LaValle", "Steven M.", ""], ["Hutchinson", "Seth A.", ""]]}, {"id": "1303.1461", "submitter": "Paul Dagum", "authors": "Paul Dagum, Adam Galper", "title": "Forecasting Sleep Apnea with Dynamic Network Models", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-64-71", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic network models (DNMs) are belief networks for temporal reasoning. The\nDNM methodology combines techniques from time series analysis and probabilistic\nreasoning to provide (1) a knowledge representation that integrates\nnoncontemporaneous and contemporaneous dependencies and (2) methods for\niteratively refining these dependencies in response to the effects of exogenous\ninfluences. We use belief-network inference algorithms to perform forecasting,\ncontrol, and discrete event simulation on DNMs. The belief network formulation\nallows us to move beyond the traditional assumptions of linearity in the\nrelationships among time-dependent variables and of normality in their\nprobability distributions. We demonstrate the DNM methodology on an important\nforecasting problem in medicine. We conclude with a discussion of how the\nmethodology addresses several limitations found in traditional time series\nanalyses.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:19:03 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Dagum", "Paul", ""], ["Galper", "Adam", ""]]}, {"id": "1303.1462", "submitter": "Peter J. Regan", "authors": "Peter J. Regan", "title": "Normative Engineering Risk Management Systems", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-72-79", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a normative system design that incorporates diagnosis,\ndynamic evolution, decision making, and information gathering. A single\ninfluence diagram demonstrates the design's coherence, yet each activity is\nmore effectively modeled and evaluated separately. Application to offshore oil\nplatforms illustrates the design. For this application, the normative system is\nembedded in a real-time expert system.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:19:10 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Regan", "Peter J.", ""]]}, {"id": "1303.1463", "submitter": "David Heckerman", "authors": "David Heckerman, Michael Shwe", "title": "Diagnosis of Multiple Faults: A Sensitivity Analysis", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-80-87", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the diagnostic accuracy of three diagnostic inference models: the\nsimple Bayes model, the multimembership Bayes model, which is isomorphic to the\nparallel combination function in the certainty-factor model, and a model that\nincorporates the noisy OR-gate interaction. The comparison is done on 20\nclinicopathological conference (CPC) cases from the American Journal of\nMedicine-challenging cases describing actual patients often with multiple\ndisorders. We find that the distributions produced by the noisy OR model agree\nmost closely with the gold-standard diagnoses, although substantial differences\nexist between the distributions and the diagnoses. In addition, we find that\nthe multimembership Bayes model tends to significantly overestimate the\nposterior probabilities of diseases, whereas the simple Bayes model tends to\nsignificantly underestimate the posterior probabilities. Our results suggest\nthat additional work to refine the noisy OR model for internal medicine will be\nworthwhile.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:19:15 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:52:47 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Heckerman", "David", ""], ["Shwe", "Michael", ""]]}, {"id": "1303.1464", "submitter": "Paul Dagum", "authors": "Paul Dagum, Adam Galper", "title": "Additive Belief-Network Models", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-91-98", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inherent intractability of probabilistic inference has hindered the\napplication of belief networks to large domains. Noisy OR-gates [30] and\nprobabilistic similarity networks [18, 17] escape the complexity of inference\nby restricting model expressiveness. Recent work in the application of\nbelief-network models to time-series analysis and forecasting [9, 10] has given\nrise to the additive belief network model (ABNM). We (1) discuss the nature and\nimplications of the approximations made by an additive decomposition of a\nbelief network, (2) show greater efficiency in the induction of additive models\nwhen available data are scarce, (3) generalize probabilistic inference\nalgorithms to exploit the additive decomposition of ABNMs, (4) show greater\nefficiency of inference, and (5) compare results on inference with a simple\nadditive belief network.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:19:21 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Dagum", "Paul", ""], ["Galper", "Adam", ""]]}, {"id": "1303.1465", "submitter": "Francisco Javier Diez", "authors": "Francisco Javier Diez", "title": "Parameter Adjustment in Bayes Networks. The generalized noisy OR-gate", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-99-105", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiegelhalter and Lauritzen [15] studied sequential learning in Bayesian\nnetworks and proposed three models for the representation of conditional\nprobabilities. A forth model, shown here, assumes that the parameter\ndistribution is given by a product of Gaussian functions and updates them from\nthe _ and _r messages of evidence propagation. We also generalize the noisy\nOR-gate for multivalued variables, develop the algorithm to compute probability\nin time proportional to the number of parents (even in networks with loops) and\napply the learning model to this gate.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:19:27 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Diez", "Francisco Javier", ""]]}, {"id": "1303.1466", "submitter": "Didier Dubois", "authors": "Didier Dubois, Henri Prade", "title": "A fuzzy relation-based extension of Reggia's relational model for\n  diagnosis handling uncertain and incomplete information", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-106-113", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational models for diagnosis are based on a direct description of the\nassociation between disorders and manifestations. This type of model has been\nspecially used and developed by Reggia and his co-workers in the late eighties\nas a basic starting point for approaching diagnosis problems. The paper\nproposes a new relational model which includes Reggia's model as a particular\ncase and which allows for a more expressive representation of the observations\nand of the manifestations associated with disorders. The model distinguishes,\ni) between manifestations which are certainly absent and those which are not\n(yet) observed, and ii) between manifestations which cannot be caused by a\ngiven disorder and manifestations for which we do not know if they can or\ncannot be caused by this disorder. This new model, which can handle uncertainty\nin a non-probabilistic way, is based on possibility theory and so-called\ntwofold fuzzy sets, previously introduced by the authors.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:19:33 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Dubois", "Didier", ""], ["Prade", "Henri", ""]]}, {"id": "1303.1467", "submitter": "Morten Elvang-G{\\o}ransson", "authors": "Morten Elvang-G{\\o}ransson, Paul J. Krause, John Fox", "title": "Dialectic Reasoning with Inconsistent Information", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-114-121", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From an inconsistent database non-trivial arguments may be constructed both\nfor a proposition, and for the contrary of that proposition. Therefore,\ninconsistency in a logical database causes uncertainty about which conclusions\nto accept. This kind of uncertainty is called logical uncertainty. We define a\nconcept of \"acceptability\", which induces a means for differentiating\narguments. The more acceptable an argument, the more confident we are in it. A\nspecific interest is to use the acceptability classes to assign linguistic\nqualifiers to propositions, such that the qualifier assigned to a propositions\nreflects its logical uncertainty. A more general interest is to understand how\nclasses of acceptability can be defined for arguments constructed from an\ninconsistent database, and how this notion of acceptability can be devised to\nreflect different criteria. Whilst concentrating on the aspects of assigning\nlinguistic qualifiers to propositions, we also indicate the more general\nsignificance of the notion of acceptability.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:19:38 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Elvang-G\u00f8ransson", "Morten", ""], ["Krause", "Paul J.", ""], ["Fox", "John", ""]]}, {"id": "1303.1468", "submitter": "David Heckerman", "authors": "David Heckerman", "title": "Causal Independence for Knowledge Acquisition and Inference", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-122-127", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce a temporal belief-network representation of causal independence\nthat a knowledge engineer can use to elicit probabilistic models. Like the\ncurrent, atemporal belief-network representation of causal independence, the\nnew representation makes knowledge acquisition tractable. Unlike the atemproal\nrepresentation, however, the temporal representation can simplify inference,\nand does not require the use of unobservable variables. The representation is\nless general than is the atemporal representation, but appears to be useful for\nmany practical applications.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:19:44 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:51:05 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Heckerman", "David", ""]]}, {"id": "1303.1469", "submitter": "Eric J. Horvitz", "authors": "Eric J. Horvitz, Adrian Klein", "title": "Utility-Based Abstraction and Categorization", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-128-135", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a utility-based approach to categorization. We construct\ngeneralizations about events and actions by considering losses associated with\nfailing to distinguish among detailed distinctions in a decision model. The\nutility-based methods transform detailed states of the world into more abstract\ncategories comprised of disjunctions of the states. We show how we can cluster\ndistinctions into groups of distinctions at progressively higher levels of\nabstraction, and describe rules for decision making with the abstractions. The\ntechniques introduce a utility-based perspective on the nature of concepts, and\nprovide a means of simplifying decision models used in automated reasoning\nsystems. We demonstrate the techniques by describing the capabilities and\noutput of TUBA, a program for utility-based abstraction.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:19:50 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Horvitz", "Eric J.", ""], ["Klein", "Adrian", ""]]}, {"id": "1303.1470", "submitter": "Kathryn Blackmond Laskey", "authors": "Kathryn Blackmond Laskey", "title": "Sensitivity Analysis for Probability Assessments in Bayesian Networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-136-142", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When eliciting probability models from experts, knowledge engineers may\ncompare the results of the model with expert judgment on test scenarios, then\nadjust model parameters to bring the behavior of the model more in line with\nthe expert's intuition. This paper presents a methodology for analytic\ncomputation of sensitivity values to measure the impact of small changes in a\nnetwork parameter on a target probability value or distribution. These values\ncan be used to guide knowledge elicitation. They can also be used in a gradient\ndescent algorithm to estimate parameter values that maximize a measure of\ngoodness-of-fit to both local and holistic probability assessments.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:19:56 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Laskey", "Kathryn Blackmond", ""]]}, {"id": "1303.1471", "submitter": "John F. Lemmer", "authors": "John F. Lemmer", "title": "Causal Modeling", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-143-151", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal Models are like Dependency Graphs and Belief Nets in that they provide\na structure and a set of assumptions from which a joint distribution can, in\nprinciple, be computed. Unlike Dependency Graphs, Causal Models are models of\nhierarchical and/or parallel processes, rather than models of distributions\n(partially) known to a model builder through some sort of gestalt. As such,\nCausal Models are more modular, easier to build, more intuitive, and easier to\nunderstand than Dependency Graph Models. Causal Models are formally defined and\nDependency Graph Models are shown to be a special case of them. Algorithms\nsupporting inference are presented. Parsimonious methods for eliciting\ndependent probabilities are presented.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:20:02 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Lemmer", "John F.", ""]]}, {"id": "1303.1472", "submitter": "Izhar Matzkevich", "authors": "Izhar Matzkevich, Bruce Abramson", "title": "Some Complexity Considerations in the Combination of Belief Networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-152-158", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One topic that is likely to attract an increasing amount of attention within\nthe Knowledge-base systems research community is the coordination of\ninformation provided by multiple experts. We envision a situation in which\nseveral experts independently encode information as belief networks. A\npotential user must then coordinate the conclusions and recommendations of\nthese networks to derive some sort of consensus. One approach to such a\nconsensus is the fusion of the contributed networks into a single, consensus\nmodel prior to the consideration of any case-specific data (specific\nobservations, test results). This approach requires two types of combination\nprocedures, one for probabilities, and one for graphs. Since the combination of\nprobabilities is relatively well understood, the key barriers to this approach\nlie in the realm of graph theory. This paper provides formal definitions of\nsome of the operations necessary to effect the necessary graphical\ncombinations, and provides complexity analyses of these procedures. The paper's\nkey result is that most of these operations are NP-hard, and its primary\nmessage is that the derivation of ?good? consensus networks must be done\nheuristically.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:20:07 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Matzkevich", "Izhar", ""], ["Abramson", "Bruce", ""]]}, {"id": "1303.1473", "submitter": "Izhar Matzkevich", "authors": "Izhar Matzkevich, Bruce Abramson", "title": "Deriving a Minimal I-map of a Belief Network Relative to a Target\n  Ordering of its Nodes", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-159-165", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper identifies and solves a new optimization problem: Given a belief\nnetwork (BN) and a target ordering on its variables, how can we efficiently\nderive its minimal I-map whose arcs are consistent with the target ordering? We\npresent three solutions to this problem, all of which lead to directed acyclic\ngraphs based on the original BN's recursive basis relative to the specified\nordering (such a DAG is sometimes termed the boundary DAG drawn from the given\nBN relative to the said ordering [5]). Along the way, we also uncover an\nimportant general principal about arc reversals: when reordering a BN according\nto some target ordering, (while attempting to minimize the number of arcs\ngenerated), the sequence of arc reversals should follow the topological\nordering induced by the original belief network's arcs to as great an extent as\npossible. These results promise to have a significant impact on the derivation\nof consensus models, as well as on other algorithms that require the\nreconfiguration and/or combination of BN's.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:20:12 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Matzkevich", "Izhar", ""], ["Abramson", "Bruce", ""]]}, {"id": "1303.1474", "submitter": "Kim-Leng Poh", "authors": "Kim-Leng Poh, Michael R. Fehling", "title": "Probabilistic Conceptual Network: A Belief Representation Scheme for\n  Utility-Based Categorization", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-166-173", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic conceptual network is a knowledge representation scheme\ndesigned for reasoning about concepts and categorical abstractions in\nutility-based categorization. The scheme combines the formalisms of abstraction\nand inheritance hierarchies from artificial intelligence, and probabilistic\nnetworks from decision analysis. It provides a common framework for\nrepresenting conceptual knowledge, hierarchical knowledge, and uncertainty. It\nfacilitates dynamic construction of categorization decision models at varying\nlevels of abstraction. The scheme is applied to an automated machining problem\nfor reasoning about the state of the machine at varying levels of abstraction\nin support of actions for maintaining competitiveness of the plant.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:20:18 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Poh", "Kim-Leng", ""], ["Fehling", "Michael R.", ""]]}, {"id": "1303.1475", "submitter": "Kim-Leng Poh", "authors": "Kim-Leng Poh, Eric J. Horvitz", "title": "Reasoning about the Value of Decision-Model Refinement: Methods and\n  Application", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-174-182", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the value of extending the completeness of a decision model\nalong different dimensions of refinement. Specifically, we analyze the expected\nvalue of quantitative, conceptual, and structural refinement of decision\nmodels. We illustrate the key dimensions of refinement with examples. The\nanalyses of value of model refinement can be used to focus the attention of an\nanalyst or an automated reasoning system on extensions of a decision model\nassociated with the greatest expected value.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:20:23 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Poh", "Kim-Leng", ""], ["Horvitz", "Eric J.", ""]]}, {"id": "1303.1476", "submitter": "William B. Poland", "authors": "William B. Poland, Ross D. Shachter", "title": "Mixtures of Gaussians and Minimum Relative Entropy Techniques for\n  Modeling Continuous Uncertainties", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-183-190", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems of probabilistic inference and decision making under uncertainty\ncommonly involve continuous random variables. Often these are discretized to a\nfew points, to simplify assessments and computations. An alternative\napproximation is to fit analytically tractable continuous probability\ndistributions. This approach has potential simplicity and accuracy advantages,\nespecially if variables can be transformed first. This paper shows how a\nminimum relative entropy criterion can drive both transformation and fitting,\nillustrating with a power and logarithm family of transformations and mixtures\nof Gaussian (normal) distributions, which allow use of efficient influence\ndiagram methods. The fitting procedure in this case is the well-known EM\nalgorithm. The selection of the number of components in a fitted mixture\ndistribution is automated with an objective that trades off accuracy and\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:20:29 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Poland", "William B.", ""], ["Shachter", "Ross D.", ""]]}, {"id": "1303.1477", "submitter": "Prakash P. Shenoy", "authors": "Prakash P. Shenoy", "title": "Valuation Networks and Conditional Independence", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-191-199", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valuation networks have been proposed as graphical representations of\nvaluation-based systems (VBSs). The VBS framework is able to capture many\nuncertainty calculi including probability theory, Dempster-Shafer's\nbelief-function theory, Spohn's epistemic belief theory, and Zadeh's\npossibility theory. In this paper, we show how valuation networks encode\nconditional independence relations. For the probabilistic case, the class of\nprobability models encoded by valuation networks includes undirected graph\nmodels, directed acyclic graph models, directed balloon graph models, and\nrecursive causal graph models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:20:35 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Shenoy", "Prakash P.", ""]]}, {"id": "1303.1478", "submitter": "Solomon Eyal Shimony", "authors": "Solomon Eyal Shimony", "title": "Relevant Explanations: Allowing Disjunctive Assignments", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-200-207", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relevance-based explanation is a scheme in which partial assignments to\nBayesian belief network variables are explanations (abductive conclusions). We\nallow variables to remain unassigned in explanations as long as they are\nirrelevant to the explanation, where irrelevance is defined in terms of\nstatistical independence. When multiple-valued variables exist in the system,\nespecially when subsets of values correspond to natural types of events, the\nover specification problem, alleviated by independence-based explanation,\nresurfaces. As a solution to that, as well as for addressing the question of\nexplanation specificity, it is desirable to collapse such a subset of values\ninto a single value on the fly. The equivalent method, which is adopted here,\nis to generalize the notion of assignments to allow disjunctive assignments. We\nproceed to define generalized independence based explanations as maximum\nposterior probability independence based generalized assignments (GIB-MAPs).\nGIB assignments are shown to have certain properties that ease the design of\nalgorithms for computing GIB-MAPs. One such algorithm is discussed here, as\nwell as suggestions for how other algorithms may be adapted to compute\nGIB-MAPs. GIB-MAP explanations still suffer from instability, a problem which\nmay be addressed using ?approximate? conditional independence as a condition\nfor irrelevance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:20:41 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Shimony", "Solomon Eyal", ""]]}, {"id": "1303.1479", "submitter": "Sampath Srinivas", "authors": "Sampath Srinivas", "title": "A Generalization of the Noisy-Or Model", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-208-215", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Noisy-Or model is convenient for describing a class of uncertain\nrelationships in Bayesian networks [Pearl 1988]. Pearl describes the Noisy-Or\nmodel for Boolean variables. Here we generalize the model to nary input and\noutput variables and to arbitrary functions other than the Boolean OR function.\nThis generalization is a useful modeling aid for construction of Bayesian\nnetworks. We illustrate with some examples including digital circuit diagnosis\nand network reliability analysis.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:20:46 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Srinivas", "Sampath", ""]]}, {"id": "1303.1480", "submitter": "Fahiem Bacchus", "authors": "Fahiem Bacchus", "title": "Using First-Order Probability Logic for the Construction of Bayesian\n  Networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-219-226", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mechanism for constructing graphical models, specifically\nBayesian networks, from a knowledge base of general probabilistic information.\nThe unique feature of our approach is that it uses a powerful first-order\nprobabilistic logic for expressing the general knowledge base. This logic\nallows for the representation of a wide range of logical and probabilistic\ninformation. The model construction procedure we propose uses notions from\ndirect inference to identify pieces of local statistical information from the\nknowledge base that are most appropriate to the particular event we want to\nreason about. These pieces are composed to generate a joint probability\ndistribution specified as a Bayesian network. Although there are fundamental\ndifficulties in dealing with fully general knowledge, our procedure is\npractical for quite rich knowledge bases and it supports the construction of a\nfar wider range of networks than allowed for by current template technology.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:20:51 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Bacchus", "Fahiem", ""]]}, {"id": "1303.1481", "submitter": "Marie desJardins", "authors": "Marie desJardins", "title": "Representing and Reasoning With Probabilistic Knowledge: A Bayesian\n  Approach", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-227-234", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PAGODA (Probabilistic Autonomous Goal-Directed Agent) is a model for\nautonomous learning in probabilistic domains [desJardins, 1992] that\nincorporates innovative techniques for using the agent's existing knowledge to\nguide and constrain the learning process and for representing, reasoning with,\nand learning probabilistic knowledge. This paper describes the probabilistic\nrepresentation and inference mechanism used in PAGODA. PAGODA forms theories\nabout the effects of its actions and the world state on the environment over\ntime. These theories are represented as conditional probability distributions.\nA restriction is imposed on the structure of the theories that allows the\ninference mechanism to find a unique predicted distribution for any action and\nworld state description. These restricted theories are called uniquely\npredictive theories. The inference mechanism, Probability Combination using\nIndependence (PCI), uses minimal independence assumptions to combine the\nprobabilities in a theory to make probabilistic predictions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:20:56 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["desJardins", "Marie", ""]]}, {"id": "1303.1482", "submitter": "John W. Egar", "authors": "John W. Egar, Mark A. Musen", "title": "Graph-Grammar Assistance for Automated Generation of Influence Diagrams", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-235-242", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most difficult aspects of modeling complex dilemmas in\ndecision-analytic terms is composing a diagram of relevance relations from a\nset of domain concepts. Decision models in domains such as medicine, however,\nexhibit certain prototypical patterns that can guide the modeling process.\nMedical concepts can be classified according to semantic types that have\ncharacteristic positions and typical roles in an influence-diagram model. We\nhave developed a graph-grammar production system that uses such inherent\ninterrelationships among medical terms to facilitate the modeling of medical\ndecisions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:21:02 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Egar", "John W.", ""], ["Musen", "Mark A.", ""]]}, {"id": "1303.1483", "submitter": "Wai Lam", "authors": "Wai Lam, Fahiem Bacchus", "title": "Using Causal Information and Local Measures to Learn Bayesian Networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-243-250", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work we developed a method of learning Bayesian Network models\nfrom raw data. This method relies on the well known minimal description length\n(MDL) principle. The MDL principle is particularly well suited to this task as\nit allows us to tradeoff, in a principled way, the accuracy of the learned\nnetwork against its practical usefulness. In this paper we present some new\nresults that have arisen from our work. In particular, we present a new local\nway of computing the description length. This allows us to make significant\nimprovements in our search algorithm. In addition, we modify our algorithm so\nthat it can take into account partial domain information that might be provided\nby a domain expert. The local computation of description length also opens the\ndoor for local refinement of an existent network. The feasibility of our\napproach is demonstrated by experiments involving networks of a practical size.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:21:10 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Lam", "Wai", ""], ["Bacchus", "Fahiem", ""]]}, {"id": "1303.1484", "submitter": "Ron Musick", "authors": "Ron Musick", "title": "Minimal Assumption Distribution Propagation in Belief Networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-251-258", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As belief networks are used to model increasingly complex situations, the\nneed to automatically construct them from large databases will become\nparamount. This paper concentrates on solving a part of the belief network\ninduction problem: that of learning the quantitative structure (the conditional\nprobabilities), given the qualitative structure. In particular, a theory is\npresented that shows how to propagate inference distributions in a belief\nnetwork, with the only assumption being that the given qualitative structure is\ncorrect. Most inference algorithms must make at least this assumption. The\ntheory is based on four network transformations that are sufficient for any\ninference in a belief network. Furthermore, the claim is made that contrary to\npopular belief, error will not necessarily grow as the inference chain grows.\nInstead, for QBN belief nets induced from large enough samples, the error is\nmore likely to decrease as the size of the inference chain increases.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:21:15 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Musick", "Ron", ""]]}, {"id": "1303.1485", "submitter": "Moninder Singh", "authors": "Moninder Singh, Marco Valtorta", "title": "An Algorithm for the Construction of Bayesian Network Structures from\n  Data", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-259-265", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous algorithms for the construction of Bayesian belief network\nstructures from data have been either highly dependent on conditional\nindependence (CI) tests, or have required an ordering on the nodes to be\nsupplied by the user. We present an algorithm that integrates these two\napproaches - CI tests are used to generate an ordering on the nodes from the\ndatabase which is then used to recover the underlying Bayesian network\nstructure using a non CI based method. Results of preliminary evaluation of the\nalgorithm on two networks (ALARM and LED) are presented. We also discuss some\nalgorithm performance issues and open problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:21:21 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Singh", "Moninder", ""], ["Valtorta", "Marco", ""]]}, {"id": "1303.1486", "submitter": "Joe Suzuki", "authors": "Joe Suzuki", "title": "A Construction of Bayesian Networks from Databases Based on an MDL\n  Principle", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-266-273", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses learning stochastic rules especially on an\ninter-attribute relation based on a Minimum Description Length (MDL) principle\nwith a finite number of examples, assuming an application to the design of\nintelligent relational database systems. The stochastic rule in this paper\nconsists of a model giving the structure like the dependencies of a Bayesian\nBelief Network (BBN) and some stochastic parameters each indicating a\nconditional probability of an attribute value given the state determined by the\nother attributes' values in the same record. Especially, we propose the\nextended version of the algorithm of Chow and Liu in that our learning\nalgorithm selects the model in the range where the dependencies among the\nattributes are represented by some general plural number of trees.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:21:27 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Suzuki", "Joe", ""]]}, {"id": "1303.1487", "submitter": "Soe-Tsyr Yuan", "authors": "Soe-Tsyr Yuan", "title": "Knowledge-Based Decision Model Construction for Hierarchical Diagnosis:\n  A Preliminary Report", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-274-281", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous methods for probabilistic reasoning in large, complex belief or\ndecision networks are currently being developed. There has been little research\non automating the dynamic, incremental construction of decision models. A\nuniform value-driven method of decision model construction is proposed for the\nhierarchical complete diagnosis. Hierarchical complete diagnostic reasoning is\nformulated as a stochastic process and modeled using influence diagrams. Given\nobservations, this method creates decision models in order to obtain the best\nactions sequentially for locating and repairing a fault at minimum cost. This\nmethod construct decision models incrementally, interleaving probe actions with\nmodel construction and evaluation. The method treats meta-level and baselevel\ntasks uniformly. That is, the method takes a decision-theoretic look at the\ncontrol of search in causal pathways and structural hierarchies.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:21:32 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Yuan", "Soe-Tsyr", ""]]}, {"id": "1303.1488", "submitter": "Lisa J. Burnell", "authors": "Lisa J. Burnell, Eric J. Horvitz", "title": "A Synthesis of Logical and Probabilistic Reasoning for Program\n  Understanding and Debugging", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-285-291", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the integration of logical and uncertain reasoning methods to\nidentify the likely source and location of software problems. To date, software\nengineers have had few tools for identifying the sources of error in complex\nsoftware packages. We describe a method for diagnosing software problems\nthrough combining logical and uncertain reasoning analyses. Our preliminary\nresults suggest that such methods can be of value in directing the attention of\nsoftware engineers to paths of an algorithm that have the highest likelihood of\nharboring a programming error.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:21:38 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Burnell", "Lisa J.", ""], ["Horvitz", "Eric J.", ""]]}, {"id": "1303.1489", "submitter": "Peter Che", "authors": "Peter Che, Richard E. Neapolitan, James Kenevan, Martha Evens", "title": "An Implementation of a Method for Computing the Uncertainty in Inferred\n  Probabilities in Belief Networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-292-300", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the belief network has been used increasingly to model\nsystems in Al that must perform uncertain inference. The development of\nefficient algorithms for probabilistic inference in belief networks has been a\nfocus of much research in AI. Efficient algorithms for certain classes of\nbelief networks have been developed, but the problem of reporting the\nuncertainty in inferred probabilities has received little attention. A system\nshould not only be capable of reporting the values of inferred probabilities\nand/or the favorable choices of a decision; it should report the range of\npossible error in the inferred probabilities and/or choices. Two methods have\nbeen developed and implemented for determining the variance in inferred\nprobabilities in belief networks. These methods, the Approximate Propagation\nMethod and the Monte Carlo Integration Method are discussed and compared in\nthis paper.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:21:44 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Che", "Peter", ""], ["Neapolitan", "Richard E.", ""], ["Kenevan", "James", ""], ["Evens", "Martha", ""]]}, {"id": "1303.1490", "submitter": "Bruce D'Ambrosio", "authors": "Bruce D'Ambrosio", "title": "Incremental Probabilistic Inference", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-301-308", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propositional representation services such as truth maintenance systems offer\npowerful support for incremental, interleaved, problem-model construction and\nevaluation. Probabilistic inference systems, in contrast, have lagged behind in\nsupporting this incrementality typically demanded by problem solvers. The\nproblem, we argue, is that the basic task of probabilistic inference is\ntypically formulated at too large a grain-size. We show how a system built\naround a smaller grain-size inference task can have the desired incrementality\nand serve as the basis for a low-level (propositional) probabilistic\nrepresentation service.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:21:48 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["D'Ambrosio", "Bruce", ""]]}, {"id": "1303.1491", "submitter": "Thomas L. Dean", "authors": "Thomas L. Dean, Leslie Pack Kaelbling, Jak Kirman, Ann Nicholson", "title": "Deliberation Scheduling for Time-Critical Sequential Decision Making", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-309-316", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for time-critical decision making involving sequential\ntasks and stochastic processes. The method employs several iterative refinement\nroutines for solving different aspects of the decision making problem. This\npaper concentrates on the meta-level control problem of deliberation\nscheduling, allocating computational resources to these routines. We provide\ndifferent models corresponding to optimization problems that capture the\ndifferent circumstances and computational strategies for decision making under\ntime constraints. We consider precursor models in which all decision making is\nperformed prior to execution and recurrent models in which decision making is\nperformed in parallel with execution, accounting for the states observed during\nexecution and anticipating future states. We describe algorithms for precursor\nand recurrent models and provide the results of our empirical investigations to\ndate.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:21:54 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Dean", "Thomas L.", ""], ["Kaelbling", "Leslie Pack", ""], ["Kirman", "Jak", ""], ["Nicholson", "Ann", ""]]}, {"id": "1303.1492", "submitter": "Marek J. Druzdzel", "authors": "Marek J. Druzdzel, Max Henrion", "title": "Intercausal Reasoning with Uninstantiated Ancestor Nodes", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-317-325", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intercausal reasoning is a common inference pattern involving probabilistic\ndependence of causes of an observed common effect. The sign of this dependence\nis captured by a qualitative property called product synergy. The current\ndefinition of product synergy is insufficient for intercausal reasoning where\nthere are additional uninstantiated causes of the common effect. We propose a\nnew definition of product synergy and prove its adequacy for intercausal\nreasoning with direct and indirect evidence for the common effect. The new\ndefinition is based on a new property matrix half positive semi-definiteness, a\nweakened form of matrix positive semi-definiteness.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:22:00 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Druzdzel", "Marek J.", ""], ["Henrion", "Max", ""]]}, {"id": "1303.1493", "submitter": "Dan Geiger", "authors": "Dan Geiger, David Heckerman", "title": "Inference Algorithms for Similarity Networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-326-334", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine two types of similarity networks each based on a distinct notion\nof relevance. For both types of similarity networks we present an efficient\ninference algorithm that works under the assumption that every event has a\nnonzero probability of occurrence. Another inference algorithm is developed for\ntype 1 similarity networks that works under no restriction, albeit less\nefficiently.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:22:08 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:49:37 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Geiger", "Dan", ""], ["Heckerman", "David", ""]]}, {"id": "1303.1494", "submitter": "Paul E. Lehner", "authors": "Paul E. Lehner, Azar Sadigh", "title": "Two Procedures for Compiling Influence Diagrams", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-335-341", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two algorithms are presented for \"compiling\" influence diagrams into a set of\nsimple decision rules. These decision rules define simple-to-execute, complete,\nconsistent, and near-optimal decision procedures. These compilation algorithms\ncan be used to derive decision procedures for human teams solving time\nconstrained decision problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:22:13 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Lehner", "Paul E.", ""], ["Sadigh", "Azar", ""]]}, {"id": "1303.1495", "submitter": "Zhaoyu Li", "authors": "Zhaoyu Li, Bruce D'Ambrosio", "title": "An efficient approach for finding the MPE in belief networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-342-349", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a belief network with evidence, the task of finding the I most probable\nexplanations (MPE) in the belief network is that of identifying and ordering\nthe I most probable instantiations of the non-evidence nodes of the belief\nnetwork. Although many approaches have been proposed for solving this problem,\nmost work only for restricted topologies (i.e., singly connected belief\nnetworks). In this paper, we will present a new approach for finding I MPEs in\nan arbitrary belief network. First, we will present an algorithm for finding\nthe MPE in a belief network. Then, we will present a linear time algorithm for\nfinding the next MPE after finding the first MPE. And finally, we will discuss\nthe problem of finding the MPE for a subset of variables of a belief network,\nand show that the problem can be efficiently solved by this approach.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:22:18 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Li", "Zhaoyu", ""], ["D'Ambrosio", "Bruce", ""]]}, {"id": "1303.1496", "submitter": "Todd Michael Mansell", "authors": "Todd Michael Mansell", "title": "A Method for Planning Given Uncertain and Incomplete Information", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-350-358", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes ongoing research into planning in an uncertain\nenvironment. In particular, it introduces U-Plan, a planning system that\nconstructs quantitatively ranked plans given an incomplete description of the\nstate of the world. U-Plan uses a DempsterShafer interval to characterise\nuncertain and incomplete information about the state of the world. The planner\ntakes as input what is known about the world, and constructs a number of\npossible initial states with representations at different abstraction levels. A\nplan is constructed for the initial state with the greatest support, and this\nplan is tested to see if it will work for other possible initial states. All,\npart, or none of the existing plans may be used in the generation of the plans\nfor the remaining possible worlds. Planning takes place in an abstraction\nhierarchy where strategic decisions are made before tactical decisions. A\nsuper-plan is then constructed, based on merging the set of plans and the\nappropriately timed acquisition of essential knowledge, which is used to decide\nbetween plan alternatives. U-Plan usually produces a super-plan in less time\nthan a classical planner would take to produce a set of plans, one for each\npossible world.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:22:24 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Mansell", "Todd Michael", ""]]}, {"id": "1303.1497", "submitter": "David L Poole", "authors": "David L. Poole", "title": "The use of conflicts in searching Bayesian networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-359-367", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses how conflicts (as used by the consistency-based\ndiagnosis community) can be adapted to be used in a search-based algorithm for\ncomputing prior and posterior probabilities in discrete Bayesian Networks. This\nis an \"anytime\" algorithm, that at any stage can estimate the probabilities and\ngive an error bound. Whereas the most popular Bayesian net algorithms exploit\nthe structure of the network for efficiency, we exploit probability\ndistributions for efficiency; this algorithm is most suited to the case with\nextreme probabilities. This paper presents a solution to the inefficiencies\nfound in naive algorithms, and shows how the tools of the consistency-based\ndiagnosis community (namely conflicts) can be used effectively to improve the\nefficiency. Empirical results with networks having tens of thousands of nodes\nare presented.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:22:29 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Poole", "David L.", ""]]}, {"id": "1303.1498", "submitter": "Carlos Rojas-Guzman", "authors": "Carlos Rojas-Guzman, Mark A. Kramer", "title": "GALGO: A Genetic ALGOrithm Decision Support Tool for Complex Uncertain\n  Systems Modeled with Bayesian Belief Networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-368-375", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian belief networks can be used to represent and to reason about complex\nsystems with uncertain, incomplete and conflicting information. Belief networks\nare graphs encoding and quantifying probabilistic dependence and conditional\nindependence among variables. One type of reasoning of interest in diagnosis is\ncalled abductive inference (determination of the global most probable system\ndescription given the values of any partial subset of variables). In some\ncases, abductive inference can be performed with exact algorithms using\ndistributed network computations but it is an NP-hard problem and complexity\nincreases drastically with the presence of undirected cycles, number of\ndiscrete states per variable, and number of variables in the network. This\npaper describes an approximate method based on genetic algorithms to perform\nabductive inference in large, multiply connected networks for which complexity\nis a concern when using most exact methods and for which systematic search\nmethods are not feasible. The theoretical adequacy of the method is discussed\nand preliminary experimental results are presented.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:22:36 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Rojas-Guzman", "Carlos", ""], ["Kramer", "Mark A.", ""]]}, {"id": "1303.1499", "submitter": "Sumit Sarkar", "authors": "Sumit Sarkar", "title": "Using Tree-Decomposable Structures to Approximate Belief Networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-376-382", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree structures have been shown to provide an efficient framework for\npropagating beliefs [Pearl,1986]. This paper studies the problem of finding an\noptimal approximating tree. The star decomposition scheme for sets of three\nbinary variables [Lazarsfeld,1966; Pearl,1986] is shown to enhance the class of\nprobability distributions that can support tree structures; such structures are\ncalled tree-decomposable structures. The logarithm scoring rule is found to be\nan appropriate optimality criterion to evaluate different tree-decomposable\nstructures. Characteristics of such structures closest to the actual belief\nnetwork are identified using the logarithm rule, and greedy and exact\ntechniques are developed to find the optimal approximation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:22:41 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Sarkar", "Sumit", ""]]}, {"id": "1303.1500", "submitter": "Ross D. Shachter", "authors": "Ross D. Shachter, Pierre Ndilikilikesha", "title": "Using Potential Influence Diagrams for Probabilistic Inference and\n  Decision Making", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-383-390", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential influence diagram is a generalization of the standard\n\"conditional\" influence diagram, a directed network representation for\nprobabilistic inference and decision analysis [Ndilikilikesha, 1991]. It allows\nefficient inference calculations corresponding exactly to those on undirected\ngraphs. In this paper, we explore the relationship between potential and\nconditional influence diagrams and provide insight into the properties of the\npotential influence diagram. In particular, we show how to convert a potential\ninfluence diagram into a conditional influence diagram, and how to view the\npotential influence diagram operations in terms of the conditional influence\ndiagram.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:22:47 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Shachter", "Ross D.", ""], ["Ndilikilikesha", "Pierre", ""]]}, {"id": "1303.1501", "submitter": "Tom S. Verma", "authors": "Tom S. Verma, Judea Pearl", "title": "Deciding Morality of Graphs is NP-complete", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-391-399", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to find a causal explanation for data presented in the form of\ncovariance and concentration matrices it is necessary to decide if the graph\nformed by such associations is a projection of a directed acyclic graph (dag).\nWe show that the general problem of deciding whether such a dag exists is\nNP-complete.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:22:53 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Verma", "Tom S.", ""], ["Pearl", "Judea", ""]]}, {"id": "1303.1502", "submitter": "Nevin Lianwen Zhang", "authors": "Nevin Lianwen Zhang, Runping Qi, David L. Poole", "title": "Incremental computation of the value of perfect information in\n  stepwise-decomposable influence diagrams", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-400-407", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To determine the value of perfect information in an influence diagram, one\nneeds first to modify the diagram to reflect the change in information\navailability, and then to compute the optimal expected values of both the\noriginal diagram and the modified diagram. The value of perfect information is\nthe difference between the two optimal expected values. This paper is about how\nto speed up the computation of the optimal expected value of the modified\ndiagram by making use of the intermediate computation results obtained when\ncomputing the optimal expected value of the original diagram.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:22:59 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Zhang", "Nevin Lianwen", ""], ["Qi", "Runping", ""], ["Poole", "David L.", ""]]}, {"id": "1303.1503", "submitter": "Salem Benferhat", "authors": "Salem Benferhat, Didier Dubois, Henri Prade", "title": "Argumentative inference in uncertain and inconsistent knowledge bases", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-411-419", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents and discusses several methods for reasoning from\ninconsistent knowledge bases. A so-called argumentative-consequence relation\ntaking into account the existence of consistent arguments in favor of a\nconclusion and the absence of consistent arguments in favor of its contrary, is\nparticularly investigated. Flat knowledge bases, i.e. without any priority\nbetween their elements, as well as prioritized ones where some elements are\nconsidered as more strongly entrenched than others are studied under different\nconsequence relations. Lastly a paraconsistent-like treatment of prioritized\nknowledge bases is proposed, where both the level of entrenchment and the level\nof paraconsistency attached to a formula are propagated. The priority levels\nare handled in the framework of possibility theory.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:23:05 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Benferhat", "Salem", ""], ["Dubois", "Didier", ""], ["Prade", "Henri", ""]]}, {"id": "1303.1504", "submitter": "Adnan Darwiche", "authors": "Adnan Darwiche", "title": "Argument Calculus and Networks", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-420-427", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major reason behind the success of probability calculus is that it\npossesses a number of valuable tools, which are based on the notion of\nprobabilistic independence. In this paper, I identify a notion of logical\nindependence that makes some of these tools available to a class of\npropositional databases, called argument databases. Specifically, I suggest a\ngraphical representation of argument databases, called argument networks, which\nresemble Bayesian networks. I also suggest an algorithm for reasoning with\nargument networks, which resembles a basic algorithm for reasoning with\nBayesian networks. Finally, I show that argument networks have several\napplications: Nonmonotonic reasoning, truth maintenance, and diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:23:11 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Darwiche", "Adnan", ""]]}, {"id": "1303.1505", "submitter": "John Fox", "authors": "John Fox, Paul J. Krause, Morten Elvang-G{\\o}ransson", "title": "Argumentation as a General Framework for Uncertain Reasoning", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-428-434", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argumentation is the process of constructing arguments about propositions,\nand the assignment of statements of confidence to those propositions based on\nthe nature and relative strength of their supporting arguments. The process is\nmodelled as a labelled deductive system, in which propositions are doubly\nlabelled with the grounds on which they are based and a representation of the\nconfidence attached to the argument. Argument construction is captured by a\ngeneralized argument consequence relation based on the ^,--fragment of minimal\nlogic. Arguments can be aggregated by a variety of numeric and symbolic\nflattening functions. This approach appears to shed light on the common logical\nstructure of a variety of quantitative, qualitative and defeasible uncertainty\ncalculi.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:23:15 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Fox", "John", ""], ["Krause", "Paul J.", ""], ["Elvang-G\u00f8ransson", "Morten", ""]]}, {"id": "1303.1506", "submitter": "Simon Parsons", "authors": "Simon Parsons, E. H. Mamdani", "title": "On reasoning in networks with qualitative uncertainty", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-435-442", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper some initial work towards a new approach to qualitative\nreasoning under uncertainty is presented. This method is not only applicable to\nqualitative probabilistic reasoning, as is the case with other methods, but\nalso allows the qualitative propagation within networks of values based upon\npossibility theory and Dempster-Shafer evidence theory. The method is applied\nto two simple networks from which a large class of directed graphs may be\nconstructed. The results of this analysis are used to compare the qualitative\nbehaviour of the three major quantitative uncertainty handling formalisms, and\nto demonstrate that the qualitative integration of the formalisms is possible\nunder certain assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:23:21 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Parsons", "Simon", ""], ["Mamdani", "E. H.", ""]]}, {"id": "1303.1507", "submitter": "Michael S. K. M. Wong", "authors": "Michael S. K. M. Wong, Z. W. Wang", "title": "Qualitative Measures of Ambiguity", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-443-450", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a qualitative measure of ambiguity and analyses its\nrelationship with other measures of uncertainty. Probability measures relative\nlikelihoods, while ambiguity measures vagueness surrounding those judgments.\nAmbiguity is an important representation of uncertain knowledge. It deals with\na different, type of uncertainty modeled by subjective probability or belief.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:23:27 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Wong", "Michael S. K. M.", ""], ["Wang", "Z. W.", ""]]}, {"id": "1303.1508", "submitter": "Robert F. Bordley", "authors": "Robert F. Bordley", "title": "A Bayesian Variant of Shafer's Commonalities For Modelling Unforeseen\n  Events", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-453-460", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shafer's theory of belief and the Bayesian theory of probability are two\nalternative and mutually inconsistent approaches toward modelling uncertainty\nin artificial intelligence. To help reduce the conflict between these two\napproaches, this paper reexamines expected utility theory-from which Bayesian\nprobability theory is derived. Expected utility theory requires the decision\nmaker to assign a utility to each decision conditioned on every possible event\nthat might occur. But frequently the decision maker cannot foresee all the\nevents that might occur, i.e., one of the possible events is the occurrence of\nan unforeseen event. So once we acknowledge the existence of unforeseen events,\nwe need to develop some way of assigning utilities to decisions conditioned on\nunforeseen events. The commonsensical solution to this problem is to assign\nsimilar utilities to events which are similar. Implementing this commonsensical\nsolution is equivalent to replacing Bayesian subjective probabilities over the\nspace of foreseen and unforeseen events by random set theory probabilities over\nthe space of foreseen events. This leads to an expected utility principle in\nwhich normalized variants of Shafer's commonalities play the role of subjective\nprobabilities. Hence allowing for unforeseen events in decision analysis causes\nBayesian probability theory to become much more similar to Shaferian theory.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:23:32 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Bordley", "Robert F.", ""]]}, {"id": "1303.1509", "submitter": "Craig Boutilier", "authors": "Craig Boutilier", "title": "The Probability of a Possibility: Adding Uncertainty to Default Rules", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-461-468", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semantics for adding uncertainty to conditional logics for\ndefault reasoning and belief revision. We are able to treat conditional\nsentences as statements of conditional probability, and express rules for\nrevision such as \"If A were believed, then B would be believed to degree p.\"\nThis method of revision extends conditionalization by allowing meaningful\nrevision by sentences whose probability is zero. This is achieved through the\nuse of counterfactual probabilities. Thus, our system accounts for the best\nproperties of qualitative methods of update (in particular, the AGM theory of\nrevision) and probabilistic methods. We also show how our system can be viewed\nas a unification of probability theory and possibility theory, highlighting\ntheir orthogonality and providing a means for expressing the probability of a\npossibility. We also demonstrate the connection to Lewis's method of imaging.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:23:38 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Boutilier", "Craig", ""]]}, {"id": "1303.1510", "submitter": "Dimiter Driankov", "authors": "Dimiter Driankov, Jerome Lang", "title": "Possibilistic decreasing persistence", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-469-476", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key issue in the handling of temporal data is the treatment of persistence;\nin most approaches it consists in inferring defeasible confusions by\nextrapolating from the actual knowledge of the history of the world; we propose\nhere a gradual modelling of persistence, following the idea that persistence is\ndecreasing (the further we are from the last time point where a fluent is known\nto be true, the less certainly true the fluent is); it is based on possibility\ntheory, which has strong relations with other well-known ordering-based\napproaches to nonmonotonic reasoning. We compare our approach with Dean and\nKanazawa's probabilistic projection. We give a formal modelling of the\ndecreasing persistence problem. Lastly, we show how to infer nonmonotonic\nconclusions using the principle of decreasing persistence.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:23:43 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Driankov", "Dimiter", ""], ["Lang", "Jerome", ""]]}, {"id": "1303.1511", "submitter": "Jiwen W. Guan", "authors": "Jiwen W. Guan, David A. Bell", "title": "Discounting and Combination Operations in Evidential Reasoning", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-477-484", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidential reasoning is now a leading topic in Artificial Intelligence.\nEvidence is represented by a variety of evidential functions. Evidential\nreasoning is carried out by certain kinds of fundamental operation on these\nfunctions. This paper discusses two of the basic operations on evidential\nfunctions, the discount operation and the well-known orthogonal sum operation.\nWe show that the discount operation is not commutative with the orthogonal sum\noperation, and derive expressions for the two operations applied to the various\nevidential function.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:23:49 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Guan", "Jiwen W.", ""], ["Bell", "David A.", ""]]}, {"id": "1303.1512", "submitter": "Jurg Kohlas", "authors": "Jurg Kohlas, Paul-Andre Monney", "title": "Probabilistic Assumption-Based Reasoning", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-485-491", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical propositional assumption-based model is extended to incorporate\nprobabilities for the assumptions. Then it is placed into the framework of\nevidence theory. Several authors like Laskey, Lehner (1989) and Provan (1990)\nalready proposed a similar point of view, but the first paper is not as much\nconcerned with mathematical foundations, and Provan's paper develops into a\ndifferent direction. Here we thoroughly develop and present the mathematical\nfoundations of this theory, together with computational methods adapted from\nReiter, De Kleer (1987) and Inoue (1992). Finally, recently proposed techniques\nfor computing degrees of support are presented.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:23:56 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Kohlas", "Jurg", ""], ["Monney", "Paul-Andre", ""]]}, {"id": "1303.1513", "submitter": "Serafin Moral", "authors": "Serafin Moral, Luis M. de Campos", "title": "Partially Specified Belief Functions", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-492-499", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a procedure to determine a complete belief function from\nthe known values of belief for some of the subsets of the frame of discerment.\nThe method is based on the principle of minimum commitment and a new principle\ncalled the focusing principle. This additional principle is based on the idea\nthat belief is specified for the most relevant sets: the focal elements. The\nresulting procedure is compared with existing methods of building complete\nbelief functions: the minimum specificity principle and the least commitment\nprinciple.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:24:02 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Moral", "Serafin", ""], ["de Campos", "Luis M.", ""]]}, {"id": "1303.1514", "submitter": "Philippe Smets", "authors": "Philippe Smets", "title": "Jeffrey's rule of conditioning generalized to belief functions", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-500-505", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jeffrey's rule of conditioning has been proposed in order to revise a\nprobability measure by another probability function. We generalize it within\nthe framework of the models based on belief functions. We show that several\nforms of Jeffrey's conditionings can be defined that correspond to the\ngeometrical rule of conditioning and to Dempster's rule of conditioning,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:24:07 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Smets", "Philippe", ""]]}, {"id": "1303.1515", "submitter": "Fengming Song", "authors": "Fengming Song, Ping Liang", "title": "Inference with Possibilistic Evidence", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-506-514", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the concept of possibilistic evidence which is a possibility\ndistribution as well as a body of evidence is proposed over an infinite\nuniverse of discourse. The inference with possibilistic evidence is\ninvestigated based on a unified inference framework maintaining both the\ncompatibility of concepts and the consistency of the probability logic.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:24:13 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Song", "Fengming", ""], ["Liang", "Ping", ""]]}, {"id": "1303.1516", "submitter": "Carl G. Wagner", "authors": "Carl G. Wagner, Bruce Tonn", "title": "Constructing Lower Probabilities", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-515-518", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An elaboration of Dempster's method of constructing belief functions suggests\na broadly applicable strategy for constructing lower probabilities under a\nvariety of evidentiary constraints.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:24:18 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Wagner", "Carl G.", ""], ["Tonn", "Bruce", ""]]}, {"id": "1303.1517", "submitter": "Pei Wang", "authors": "Pei Wang", "title": "Belief Revision in Probability Theory", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-519-526", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a probability-based reasoning system, Bayes' theorem and its variations\nare often used to revise the system's beliefs. However, if the explicit\nconditions and the implicit conditions of probability assignments `me properly\ndistinguished, it follows that Bayes' theorem is not a generally applicable\nrevision rule. Upon properly distinguishing belief revision from belief\nupdating, we see that Jeffrey's rule and its variations are not revision rules,\neither. Without these distinctions, the limitation of the Bayesian approach is\noften ignored or underestimated. Revision, in its general form, cannot be done\nin the Bayesian approach, because a probability distribution function alone\ndoes not contain the information needed by the operation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:24:24 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Wang", "Pei", ""]]}, {"id": "1303.1518", "submitter": "Nic Wilson", "authors": "Nic Wilson", "title": "The Assumptions Behind Dempster's Rule", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-527-534", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the concept of a combination rule for belief functions.\nIt is shown that two fairly simple and apparently reasonable assumptions\ndetermine Dempster's rule, giving a new justification for it.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:24:29 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Wilson", "Nic", ""]]}, {"id": "1303.1519", "submitter": "Hong Xu", "authors": "Hong Xu, Yen-Teh Hsia, Philippe Smets", "title": "A Belief-Function Based Decision Support System", "comments": "Appears in Proceedings of the Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI1993)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1993-PG-535-542", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a decision support system based on belief functions\nand the pignistic transformation. The system is an integration of an evidential\nsystem for belief function propagation and a valuation-based system for\nBayesian decision analysis. The two subsystems are connected through the\npignistic transformation. The system takes as inputs the user's \"gut feelings\"\nabout a situation and suggests what, if any, are to be tested and in what\norder, and it does so with a user friendly interface.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 14:24:35 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Xu", "Hong", ""], ["Hsia", "Yen-Teh", ""], ["Smets", "Philippe", ""]]}, {"id": "1303.1700", "submitter": "Boris Campillo-Gimenez", "authors": "Boris Campillo-Gimenez, Wassim Jouini, Sahar Bayat, Marc Cuggia", "title": "K-Nearest Neighbour algorithm coupled with logistic regression in\n  medical case-based reasoning systems. Application to prediction of access to\n  the renal transplant waiting list in Brittany", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction. Case Based Reasoning (CBR) is an emerg- ing decision making\nparadigm in medical research where new cases are solved relying on previously\nsolved similar cases. Usually, a database of solved cases is provided, and\nevery case is described through a set of attributes (inputs) and a label\n(output). Extracting useful information from this database can help the CBR\nsystem providing more reliable results on the yet to be solved cases.\nObjective. For that purpose we suggest a general frame- work where a CBR\nsystem, viz. K-Nearest Neighbor (K-NN) algorithm, is combined with various\ninformation obtained from a Logistic Regression (LR) model. Methods. LR is\napplied, on the case database, to assign weights to the attributes as well as\nthe solved cases. Thus, five possible decision making systems based on K-NN\nand/or LR were identified: a standalone K-NN, a standalone LR and three soft\nK-NN algorithms that rely on the weights based on the results of the LR. The\nevaluation of the described approaches is performed in the field of renal\ntransplant access waiting list. Results and conclusion. The results show that\nour suggested approach, where the K-NN algorithm relies on both weighted\nattributes and cases, can efficiently deal with non relevant attributes,\nwhereas the four other approaches suffer from this kind of noisy setups. The\nrobustness of this approach suggests interesting perspectives for medical\nproblem solving tools using CBR methodology.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 14:25:23 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Campillo-Gimenez", "Boris", ""], ["Jouini", "Wassim", ""], ["Bayat", "Sahar", ""], ["Cuggia", "Marc", ""]]}, {"id": "1303.2013", "submitter": "J. G. Wolff", "authors": "J Gerard Wolff", "title": "Computing as compression: the SP theory of intelligence", "comments": "8 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1212.0229", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an overview of the SP theory of intelligence and its\ncentral idea that artificial intelligence, mainstream computing, and much of\nhuman perception and cognition, may be understood as information compression.\n  The background and origins of the SP theory are described, and the main\nelements of the theory, including the key concept of multiple alignment,\nborrowed from bioinformatics but with important differences. Associated with\nthe SP theory is the idea that redundancy in information may be understood as\nrepetition of patterns, that compression of information may be achieved via the\nmatching and unification (merging) of patterns, and that computing and\ninformation compression are both fundamentally probabilistic. It appears that\nthe SP system is Turing-equivalent in the sense that anything that may be\ncomputed with a Turing machine may, in principle, also be computed with an SP\nmachine.\n  One of the main strengths of the SP theory and the multiple alignment concept\nis in modelling concepts and phenomena in artificial intelligence. Within that\narea, the SP theory provides a simple but versatile means of representing\ndifferent kinds of knowledge, it can model both the parsing and production of\nnatural language, with potential for the understanding and translation of\nnatural languages, it has strengths in pattern recognition, with potential in\ncomputer vision, it can model several kinds of reasoning, and it has\ncapabilities in planning, problem solving, and unsupervised learning.\n  The paper includes two examples showing how alternative parsings of an\nambiguous sentence may be modelled as multiple alignments, and another example\nshowing how the concept of multiple alignment may be applied in medical\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 14:52:24 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Wolff", "J Gerard", ""]]}, {"id": "1303.2071", "submitter": "J. G. Wolff", "authors": "J. Gerard Wolff", "title": "Application of the SP theory of intelligence to the understanding of\n  natural vision and the development of computer vision", "comments": "40 pages, 16 figures", "journal-ref": "SpringerPlus, 3, 552, 2014", "doi": "10.1186/2193-1801-3-552", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SP theory of intelligence aims to simplify and integrate concepts in\ncomputing and cognition, with information compression as a unifying theme. This\narticle discusses how it may be applied to the understanding of natural vision\nand the development of computer vision. The theory, which is described quite\nfully elsewhere, is described here in outline but with enough detail to ensure\nthat the rest of the article makes sense.\n  Low level perceptual features such as edges or corners may be identified by\nthe extraction of redundancy in uniform areas in a manner that is comparable\nwith the run-length encoding technique for information compression.\n  The concept of multiple alignment in the SP theory may be applied to the\nrecognition of objects, and to scene analysis, with a hierarchy of parts and\nsub-parts, and at multiple levels of abstraction.\n  The theory has potential for the unsupervised learning of visual objects and\nclasses of objects, and suggests how coherent concepts may be derived from\nfragments.\n  As in natural vision, both recognition and learning in the SP system is\nrobust in the face of errors of omission, commission and substitution.\n  The theory suggests how, via vision, we may piece together a knowledge of the\nthree-dimensional structure of objects and of our environment, it provides an\naccount of how we may see things that are not objectively present in an image,\nand how we recognise something despite variations in the size of its retinal\nimage. And it has things to say about the phenomena of lightness constancy and\ncolour constancy, the role of context in recognition, and ambiguities in visual\nperception.\n  A strength of the SP theory is that it provides for the integration of vision\nwith other sensory modalities and with other aspects of intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 18:21:17 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 09:59:14 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Wolff", "J. Gerard", ""]]}, {"id": "1303.2096", "submitter": "Alfredo Garcia Woods Prog.", "authors": "Alfredo Garcia Woods", "title": "Gene-Machine, a new search heuristic algorithm", "comments": "GeneMachine uses the chromosome notion, genes and evolution, but it\n  differs from genetic algorithms, in that it does not use mutation, nor\n  population of individuals, neither the notion of generation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Gene-Machine, an efficient and new search heuristic\nalgorithm, based in the building-block hypothesis. It is inspired by natural\nevolution, but does not use some of the concepts present in genetic algorithms\nlike population, mutation and generation. This heuristic exhibits good\nperformance in comparison with genetic algorithms, and can be used to generate\nuseful solutions to optimization and search problems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 20:16:02 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Woods", "Alfredo Garcia", ""]]}, {"id": "1303.2430", "submitter": "Diederik Aerts", "authors": "Diederik Aerts", "title": "Quantum and Concept Combination, Entangled Measurements and Prototype\n  Theory", "comments": "5 pages, 1 figure", "journal-ref": "Topics in Cognitive Science, 6, pp. 129-137, 2014", "doi": "10.1111/tops.12073", "report-no": null, "categories": "cs.AI cs.CL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the meaning of the violation of the marginal probability law for\nsituations of correlation measurements where entanglement is identified. We\nshow that for quantum theory applied to the cognitive realm such a violation\ndoes not lead to the type of problems commonly believed to occur in situations\nof quantum theory applied to the physical realm. We briefly situate our quantum\napproach for modeling concepts and their combinations with respect to the\nnotions of 'extension' and 'intension' in theories of meaning, and in existing\nconcept theories.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 05:36:34 GMT"}, {"version": "v2", "created": "Sun, 19 May 2013 12:50:05 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Aerts", "Diederik", ""]]}, {"id": "1303.2860", "submitter": "Moritz M\\\"uhlenthaler", "authors": "Moritz M\\\"uhlenthaler, Rolf Wanka", "title": "Fairness in Academic Course Timetabling", "comments": "appeared in PATAT 2012, pp. 114-130", "journal-ref": null, "doi": "10.1007/s10479-014-1553-2", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of creating fair course timetables in the setting of\na university. Our motivation is to improve the overall satisfaction of\nindividuals concerned (students, teachers, etc.) by providing a fair timetable\nto them. The central idea is that undesirable arrangements in the course\ntimetable, i.e., violations of soft constraints, should be distributed in a\nfair way among the individuals. We propose two formulations for the fair course\ntimetabling problem that are based on max-min fairness and Jain's fairness\nindex, respectively. Furthermore, we present and experimentally evaluate an\noptimization algorithm based on simulated annealing for solving max-min fair\ncourse timetabling problems. The new contribution is concerned with measuring\nthe energy difference between two timetables, i.e., how much worse a timetable\nis compared to another timetable with respect to max-min fairness. We introduce\nthree different energy difference measures and evaluate their impact on the\noverall algorithm performance. The second proposed problem formulation focuses\non the tradeoff between fairness and the total amount of soft constraint\nviolations. Our experimental evaluation shows that the known best solutions to\nthe ITC2007 curriculum-based course timetabling instances are quite fair with\nrespect to Jain's fairness index. However, the experiments also show that the\nfairness can be improved further for only a rather small increase in the total\namount of soft constraint violations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 12:46:54 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["M\u00fchlenthaler", "Moritz", ""], ["Wanka", "Rolf", ""]]}, {"id": "1303.2912", "submitter": "Roger Frigola", "authors": "Roger Frigola and Carl Edward Rasmussen", "title": "Integrated Pre-Processing for Bayesian Nonlinear System Identification\n  with Gaussian Processes", "comments": "Proceedings of the 52th IEEE International Conference on Decision and\n  Control (CDC), Firenze, Italy, December 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GP-FNARX: a new model for nonlinear system identification based\non a nonlinear autoregressive exogenous model (NARX) with filtered regressors\n(F) where the nonlinear regression problem is tackled using sparse Gaussian\nprocesses (GP). We integrate data pre-processing with system identification\ninto a fully automated procedure that goes from raw data to an identified\nmodel. Both pre-processing parameters and GP hyper-parameters are tuned by\nmaximizing the marginal likelihood of the probabilistic model. We obtain a\nBayesian model of the system's dynamics which is able to report its uncertainty\nin regions where the data is scarce. The automated approach, the modeling of\nuncertainty and its relatively low computational cost make of GP-FNARX a good\ncandidate for applications in robotics and adaptive control.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 15:18:00 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2013 16:25:57 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2013 12:43:24 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Frigola", "Roger", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1303.2975", "submitter": "Gudmund Grov PhD", "authors": "Gudmund Grov and Ewen Maclean", "title": "Towards Automated Proof Strategy Generalisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to automatically generalise (interactive) proofs and use such\ngeneralisations to discharge related conjectures is a very hard problem which\nremains unsolved. Here, we develop a notion of goal types to capture key\nproperties of goals, which enables abstractions over the specific order and\nnumber of sub-goals arising when composing tactics. We show that the goal types\nform a lattice, and utilise this property in the techniques we develop to\nautomatically generalise proof strategies in order to reuse it for proofs of\nrelated conjectures. We illustrate our approach with an example.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 18:18:33 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2013 15:39:10 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Grov", "Gudmund", ""], ["Maclean", "Ewen", ""]]}, {"id": "1303.3163", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi and Mauricio Araya", "title": "A Greedy Approximation of Bayesian Reinforcement Learning with Probably\n  Optimistic Transition Model", "comments": "the 13th International Workshop on Adaptive and Learning Agents at\n  AAMAS'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Reinforcement Learning (RL) is capable of not only incorporating\ndomain knowledge, but also solving the exploration-exploitation dilemma in a\nnatural way. As Bayesian RL is intractable except for special cases, previous\nwork has proposed several approximation methods. However, these methods are\nusually too sensitive to parameter values, and finding an acceptable parameter\nsetting is practically impossible in many applications. In this paper, we\npropose a new algorithm that greedily approximates Bayesian RL to achieve\nrobustness in parameter space. We show that for a desired learning behavior,\nour proposed algorithm has a polynomial sample complexity that is lower than\nthose of existing algorithms. We also demonstrate that the proposed algorithm\nnaturally outperforms other existing algorithms when the prior distributions\nare not significantly misleading. On the other hand, the proposed algorithm\ncannot handle greatly misspecified priors as well as the other algorithms can.\nThis is a natural consequence of the fact that the proposed algorithm is\ngreedier than the other algorithms. Accordingly, we discuss a way to select an\nappropriate algorithm for different tasks based on the algorithms' greediness.\nWe also introduce a new way of simplifying Bayesian planning, based on which\nfuture work would be able to derive new algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 14:06:21 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2013 03:01:40 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2013 01:04:03 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Araya", "Mauricio", ""]]}, {"id": "1303.3319", "submitter": "Anhui Tan", "authors": "Anhui Tan", "title": "A new type of judgement theorems for attribute characters in information\n  system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research of attribute characters in information system which contains\ncore, necessary, unnecessary is a basic and important issue in attribute\nreduct. Many methods for the judgement of attribute characters are based on the\nrelationship between the objects and attributes. In this paper, a new type of\njudgement theorems which are absolutely based on the relationship among\nattributes is proposed for the judgement of attribute characters. The method is\nthrough comparing the two new attribute sets $E(a)$ and $N(a)$ with respect to\nthe designated attribute $a$ which is proposed in this paper. We conclude that\nwhich type of the attribute $a$ belongs to is determined by the relationship\nbetween $E(a)$ and $N(a)$ in essence. Secondly, more concise and clear results\nare given about the judgment of the attribute characters through analyzing the\nproperties of refinement and precise-refinement between $E(a)$ and $N(a)$ in\ntopology. In addition, the relationship among attributes are discussed which is\nuseful for constructing a reduct in the last section of this paper. In the\nlast, we propose a reduct algorithm based on $E(a)$, and this algorithm is an\nextended application of the analysis of attribute characters above.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 00:35:54 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Tan", "Anhui", ""]]}, {"id": "1303.3761", "submitter": "Christoph Benzmueller", "authors": "Christoph Benzm\\\"uller, Nik Sultana", "title": "Update report: LEO-II version 1.5", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent improvements of the LEO-II theorem prover are presented. These\nimprovements include a revised ATP interface, new translations into first-order\nlogic, rule support for the axiom of choice, detection of defined equality, and\nmore flexible strategy scheduling.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2013 13:03:40 GMT"}, {"version": "v2", "created": "Tue, 14 May 2013 07:51:31 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Benzm\u00fcller", "Christoph", ""], ["Sultana", "Nik", ""]]}, {"id": "1303.4017", "submitter": "Bernard Yannou", "authors": "Benachir Medjdoub (LGI, The Martin Centre), Bernard Yannou (LGI)", "title": "Separating Topology and Geometry in Space Planning", "comments": null, "journal-ref": "Computer Aided-Design 32, 1 (2000) 39-61", "doi": "10.1016/S0010-4485(99)00084-6", "report-no": null, "categories": "cs.AI physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are dealing with the problem of space layout planning here. We present an\narchitectural conceptual CAD approach. Starting with design specifications in\nterms of constraints over spaces, a specific enumeration heuristics leads to a\ncomplete set of consistent conceptual design solutions named topological\nsolutions. These topological solutions which do not presume any precise\ndefinitive dimension correspond to the sketching step that an architect carries\nout from the Design specifications on a preliminary design phase in\narchitecture.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2013 20:12:49 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Medjdoub", "Benachir", "", "LGI, The Martin Centre"], ["Yannou", "Bernard", "", "LGI"]]}, {"id": "1303.4183", "submitter": "Lukasz Swierczewski", "authors": "Lukasz Swierczewski", "title": "Generating extrema approximation of analytically incomputable functions\n  through usage of parallel computer aided genetic algorithms", "comments": "16 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper presents capabilities of using genetic algorithms to find\napproximations of function extrema, which cannot be found using analytic ways.\nTo enhance effectiveness of calculations, algorithm has been parallelized using\nOpenMP library. We gained much increase in speed on platforms using\nmultithreaded processors with shared memory free access. During analysis we\nused different modifications of genetic operator, using them we obtained varied\nevolution process of potential solutions. Results allow to choose best methods\namong many applied in genetic algorithms and observation of acceleration on\nYorkfield, Bloomfield, Westmere-EX and most recent Sandy Bridge cores.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 08:49:48 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Swierczewski", "Lukasz", ""]]}, {"id": "1303.4431", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega, Daniel A. Braun", "title": "Generalized Thompson Sampling for Sequential Decision-Making and Causal\n  Inference", "comments": "28 pages, 5 figures", "journal-ref": "Complex Adaptive Systems Modeling 2014, 2:2", "doi": "10.1186/2194-3206-2-2", "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been shown how sampling actions from the predictive\ndistribution over the optimal action-sometimes called Thompson sampling-can be\napplied to solve sequential adaptive control problems, when the optimal policy\nis known for each possible environment. The predictive distribution can then be\nconstructed by a Bayesian superposition of the optimal policies weighted by\ntheir posterior probability that is updated by Bayesian inference and causal\ncalculus. Here we discuss three important features of this approach. First, we\ndiscuss in how far such Thompson sampling can be regarded as a natural\nconsequence of the Bayesian modeling of policy uncertainty. Second, we show how\nThompson sampling can be used to study interactions between multiple adaptive\nagents, thus, opening up an avenue of game-theoretic analysis. Third, we show\nhow Thompson sampling can be applied to infer causal relationships when\ninteracting with an environment in a sequential fashion. In summary, our\nresults suggest that Thompson sampling might not merely be a useful heuristic,\nbut a principled method to address problems of adaptive sequential\ndecision-making and causal inference.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 21:34:06 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1303.5016", "submitter": "Giuseppe Sanfilippo", "authors": "Angelo Gilio and Giuseppe Sanfilippo", "title": "Quasi Conjunction, Quasi Disjunction, T-norms and T-conorms:\n  Probabilistic Aspects", "comments": null, "journal-ref": "Information Sciences, vol. 245, pp. 146 - 167, 2013", "doi": "10.1016/j.ins.2013.03.019", "report-no": null, "categories": "math.PR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make a probabilistic analysis related to some inference rules which play\nan important role in nonmonotonic reasoning. In a coherence-based setting, we\nstudy the extensions of a probability assessment defined on $n$ conditional\nevents to their quasi conjunction, and by exploiting duality, to their quasi\ndisjunction. The lower and upper bounds coincide with some well known t-norms\nand t-conorms: minimum, product, Lukasiewicz, and Hamacher t-norms and their\ndual t-conorms. On this basis we obtain Quasi And and Quasi Or rules. These are\nrules for which any finite family of conditional events p-entails the\nassociated quasi conjunction and quasi disjunction. We examine some cases of\nlogical dependencies, and we study the relations among coherence, inclusion for\nconditional events, and p-entailment. We also consider the Or rule, where quasi\nconjunction and quasi disjunction of premises coincide with the conclusion. We\nanalyze further aspects of quasi conjunction and quasi disjunction, by\ncomputing probabilistic bounds on premises from bounds on conclusions. Finally,\nwe consider biconditional events, and we introduce the notion of an\n$n$-conditional event. Then we give a probabilistic interpretation for a\ngeneralized Loop rule. In an appendix we provide explicit expressions for the\nHamacher t-norm and t-conorm in the unitary hypercube.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 18:31:14 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Gilio", "Angelo", ""], ["Sanfilippo", "Giuseppe", ""]]}, {"id": "1303.5132", "submitter": "Vania Bogorny", "authors": "Vitor Cunha Fontes and Vania Bogorny", "title": "Discovering Semantic Spatial and Spatio-Temporal Outliers from Moving\n  Object Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several algorithms have been proposed for discovering patterns from\ntrajectories of moving objects, but only a few have concentrated on outlier\ndetection. Existing approaches, in general, discover spatial outliers, and do\nnot provide any further analysis of the patterns. In this paper we introduce\nsemantic spatial and spatio-temporal outliers and propose a new algorithm for\ntrajectory outlier detection. Semantic outliers are computed between regions of\ninterest, where objects have similar movement intention, and there exist\nstandard paths which connect the regions. We show with experiments on real data\nthat the method finds semantic outliers from trajectory data that are not\ndiscovered by similar approaches.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 00:28:41 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Fontes", "Vitor Cunha", ""], ["Bogorny", "Vania", ""]]}, {"id": "1303.5177", "submitter": "Nabila Shikoun", "authors": "Nabila Shikoun, Mohamed El Nahas and Samar Kassim", "title": "Model Based Framework for Estimating Mutation Rate of Hepatitis C Virus\n  in Egypt", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hepatitis C virus (HCV) is a widely spread disease all over the world. HCV\nhas very high mutation rate that makes it resistant to antibodies. Modeling HCV\nto identify the virus mutation process is essential to its detection and\npredicting its evolution. This paper presents a model based framework for\nestimating mutation rate of HCV in two steps. Firstly profile hidden Markov\nmodel (PHMM) architecture was builder to select the sequences which represents\nsequence per year. Secondly mutation rate was calculated by using pair-wise\ndistance method between sequences. A pilot study is conducted on NS5B zone of\nHCV dataset of genotype 4 subtype a (HCV4a) in Egypt.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 06:49:05 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Shikoun", "Nabila", ""], ["Nahas", "Mohamed El", ""], ["Kassim", "Samar", ""]]}, {"id": "1303.5391", "submitter": "Zhi An", "authors": "Zhi An, David A. Bell, John G. Hughes", "title": "RES - a Relative Method for Evidential Reasoning", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-1-8", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a novel method for evidential reasoning [1]. It\ninvolves modelling the process of evidential reasoning in three steps, namely,\nevidence structure construction, evidence accumulation, and decision making.\nThe proposed method, called RES, is novel in that evidence strength is\nassociated with an evidential support relationship (an argument) between a pair\nof statements and such strength is carried by comparison between arguments.\nThis is in contrast to the onventional approaches, where evidence strength is\nrepresented numerically and is associated with a statement.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:51:26 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["An", "Zhi", ""], ["Bell", "David A.", ""], ["Hughes", "John G.", ""]]}, {"id": "1303.5392", "submitter": "Remco R. Bouckaert", "authors": "Remco R. Bouckaert", "title": "Optimizing Causal Orderings for Generating DAGs from Data", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-9-16", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for generating the structure of a directed acyclic graph from\ndata using the notion of causal input lists is presented. The algorithm\nmanipulates the ordering of the variables with operations which very much\nresemble arc reversal. Operations are only applied if the DAG after the\noperation represents at least the independencies represented by the DAG before\nthe operation until no more arcs can be removed from the DAG. The resulting DAG\nis a minimal l-map.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:51:32 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Bouckaert", "Remco R.", ""]]}, {"id": "1303.5393", "submitter": "Craig Boutilier", "authors": "Craig Boutilier", "title": "Modal Logics for Qualitative Possibility and Beliefs", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-17-24", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Possibilistic logic has been proposed as a numerical formalism for reasoning\nwith uncertainty. There has been interest in developing qualitative accounts of\npossibility, as well as an explanation of the relationship between possibility\nand modal logics. We present two modal logics that can be used to represent and\nreason with qualitative statements of possibility and necessity. Within this\nmodal framework, we are able to identify interesting relationships between\npossibilistic logic, beliefs and conditionals. In particular, the most natural\nconditional definable via possibilistic means for default reasoning is\nidentical to Pearl's conditional for e-semantics.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:51:38 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Boutilier", "Craig", ""]]}, {"id": "1303.5394", "submitter": "Brian Y. Chan", "authors": "Brian Y. Chan, Ross D. Shachter", "title": "Structural Controllability and Observability in Influence Diagrams", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-25-32", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence diagram is a graphical representation of belief networks with\nuncertainty. This article studies the structural properties of a probabilistic\nmodel in an influence diagram. In particular, structural controllability\ntheorems and structural observability theorems are developed and algorithms are\nformulated. Controllability and observability are fundamental concepts in\ndynamic systems (Luenberger 1979). Controllability corresponds to the ability\nto control a system while observability analyzes the inferability of its\nvariables. Both properties can be determined by the ranks of the system\nmatrices. Structural controllability and observability, on the other hand,\nanalyze the property of a system with its structure only, without the specific\nknowledge of the values of its elements (tin 1974, Shields and Pearson 1976).\nThe structural analysis explores the connection between the structure of a\nmodel and the functional dependence among its elements. It is useful in\ncomprehending problem and formulating solution by challenging the underlying\nintuitions and detecting inconsistency in a model. This type of qualitative\nreasoning can sometimes provide insight even when there is insufficient\nnumerical information in a model.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:51:44 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Chan", "Brian Y.", ""], ["Shachter", "Ross D.", ""]]}, {"id": "1303.5395", "submitter": "Philippe Chatalic", "authors": "Philippe Chatalic, Christine Froidevaux", "title": "Lattice-Based Graded Logic: a Multimodal Approach", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-33-40", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experts do not always feel very, comfortable when they have to give precise\nnumerical estimations of certainty degrees. In this paper we present a\nqualitative approach which allows for attaching partially ordered symbolic\ngrades to logical formulas. Uncertain information is expressed by means of\nparameterized modal operators. We propose a semantics for this multimodal logic\nand give a sound and complete axiomatization. We study the links with related\napproaches and suggest how this framework might be used to manage both\nuncertain and incomplere knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:51:51 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Chatalic", "Philippe", ""], ["Froidevaux", "Christine", ""]]}, {"id": "1303.5396", "submitter": "Paul Dagum", "authors": "Paul Dagum, Adam Galper, Eric J. Horvitz", "title": "Dynamic Network Models for Forecasting", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-41-48", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a probabilistic forecasting methodology through a synthesis\nof belief network models and classical time-series analysis. We present the\ndynamic network model (DNM) and describe methods for constructing, refining,\nand performing inference with this representation of temporal probabilistic\nknowledge. The DNM representation extends static belief-network models to more\ngeneral dynamic forecasting models by integrating and iteratively refining\ncontemporaneous and time-lagged dependencies. We discuss key concepts in terms\nof a model for forecasting U.S. car sales in Japan.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:51:57 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Dagum", "Paul", ""], ["Galper", "Adam", ""], ["Horvitz", "Eric J.", ""]]}, {"id": "1303.5397", "submitter": "Paul Dagum", "authors": "Paul Dagum, Eric J. Horvitz", "title": "Reformulating Inference Problems Through Selective Conditioning", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-49-54", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe how we selectively reformulate portions of a belief network that\npose difficulties for solution with a stochastic-simulation algorithm. With\nemploy the selective conditioning approach to target specific nodes in a belief\nnetwork for decomposition, based on the contribution the nodes make to the\ntractability of stochastic simulation. We review previous work on BNRAS\nalgorithms- randomized approximation algorithms for probabilistic inference. We\nshow how selective conditioning can be employed to reformulate a single BNRAS\nproblem into multiple tractable BNRAS simulation problems. We discuss how we\ncan use another simulation algorithm-logic sampling-to solve a component of the\ninference problem that provides a means for knitting the solutions of\nindividual subproblems into a final result. Finally, we analyze tradeoffs among\nthe computational subtasks associated with the selective conditioning approach\nto reformulation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:52:03 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Dagum", "Paul", ""], ["Horvitz", "Eric J.", ""]]}, {"id": "1303.5398", "submitter": "Norman C. Dalkey", "authors": "Norman C. Dalkey", "title": "Entropy and Belief Networks", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-55-58", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The product expansion of conditional probabilities for belief nets is not\nmaximum entropy. This appears to deny a desirable kind of assurance for the\nmodel. However, a kind of guarantee that is almost as strong as maximum entropy\ncan be derived. Surprisingly, a variant model also exhibits the guarantee, and\nfor many cases obtains a higher performance score than the product expansion.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:52:07 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Dalkey", "Norman C.", ""]]}, {"id": "1303.5399", "submitter": "Bruce D'Ambrosio", "authors": "Bruce D'Ambrosio, Tony Fountain, Zhaoyu Li", "title": "Parallelizing Probabilistic Inference: Some Early Explorations", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-59-66", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on an experimental investigation into opportunities for parallelism\nin beliefnet inference. Specifically, we report on a study performed of the\navailable parallelism, on hypercube style machines, of a set of randomly\ngenerated belief nets, using factoring (SPI) style inference algorithms. Our\nresults indicate that substantial speedup is available, but that it is\navailable only through parallelization of individual conformal product\noperations, and depends critically on finding an appropriate factoring. We find\nnegligible opportunity for parallelism at the topological, or clustering tree,\nlevel.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:52:13 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["D'Ambrosio", "Bruce", ""], ["Fountain", "Tony", ""], ["Li", "Zhaoyu", ""]]}, {"id": "1303.5400", "submitter": "Adnan Darwiche", "authors": "Adnan Darwiche", "title": "Objection-Based Causal Networks", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-67-73", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the notion of objection-based causal networks which\nresemble probabilistic causal networks except that they are quantified using\nobjections. An objection is a logical sentence and denotes a condition under\nwhich a, causal dependency does not exist. Objection-based causal networks\nenjoy almost all the properties that make probabilistic causal networks\npopular, with the added advantage that objections are, arguably more intuitive\nthan probabilities.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:52:19 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Darwiche", "Adnan", ""]]}, {"id": "1303.5401", "submitter": "Didier Dubois", "authors": "Didier Dubois, Henri Prade, Lluis Godo, Ramon Lopez de Mantaras", "title": "A Symbolic Approach to Reasoning with Linguistic Quantifiers", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-74-82", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the possibility of performing automated reasoning in\nprobabilistic logic when probabilities are expressed by means of linguistic\nquantifiers. Each linguistic term is expressed as a prescribed interval of\nproportions. Then instead of propagating numbers, qualitative terms are\npropagated in accordance with the numerical interpretation of these terms. The\nquantified syllogism, modelling the chaining of probabilistic rules, is studied\nin this context. It is shown that a qualitative counterpart of this syllogism\nmakes sense, and is relatively independent of the threshold defining the\nlinguistically meaningful intervals, provided that these threshold values\nremain in accordance with the intuition. The inference power is less than that\nof a full-fledged probabilistic con-quaint propagation device but better\ncorresponds to what could be thought of as commonsense probabilistic reasoning.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:52:25 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Dubois", "Didier", ""], ["Prade", "Henri", ""], ["Godo", "Lluis", ""], ["de Mantaras", "Ramon Lopez", ""]]}, {"id": "1303.5402", "submitter": "Francesco Fulvio Monai", "authors": "Francesco Fulvio Monai, Thomas Chehire", "title": "Possibilistic Assumption based Truth Maintenance System, Validation in a\n  Data Fusion Application", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-83-91", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data fusion allows the elaboration and the evaluation of a situation\nsynthesized from low level informations provided by different kinds of sensors.\nThe fusion of the collected data will result in fewer and higher level\ninformations more easily assessed by a human operator and that will assist him\neffectively in his decision process. In this paper we present the suitability\nand the advantages of using a Possibilistic Assumption based Truth Maintenance\nSystem (n-ATMS) in a data fusion military application. We first describe the\nproblem, the needed knowledge representation formalisms and problem solving\nparadigms. Then we remind the reader of the basic concepts of ATMSs,\nPossibilistic Logic and 11-ATMSs. Finally we detail the solution to the given\ndata fusion problem and conclude with the results and comparison with a\nnon-possibilistic solution.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:52:32 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Monai", "Francesco Fulvio", ""], ["Chehire", "Thomas", ""]]}, {"id": "1303.5403", "submitter": "Dan Geiger", "authors": "Dan Geiger", "title": "An Entropy-based Learning Algorithm of Bayesian Conditional Trees", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-92-97", "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article offers a modification of Chow and Liu's learning algorithm in\nthe context of handwritten digit recognition. The modified algorithm directs\nthe user to group digits into several classes consisting of digits that are\nhard to distinguish and then constructing an optimal conditional tree\nrepresentation for each class of digits instead of for each single digit as\ndone by Chow and Liu (1968). Advantages and extensions of the new method are\ndiscussed. Related works of Wong and Wang (1977) and Wong and Poon (1989) which\noffer a different entropy-based learning algorithm are shown to rest on\ninappropriate assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:52:37 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Geiger", "Dan", ""]]}, {"id": "1303.5404", "submitter": "Angelo Gilio", "authors": "Angelo Gilio, Fulvio Spezzaferri", "title": "Knowledge Integration for Conditional Probability Assessments", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-98-103", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the probabilistic approach to uncertainty management the input knowledge\nis usually represented by means of some probability distributions. In this\npaper we assume that the input knowledge is given by two discrete conditional\nprobability distributions, represented by two stochastic matrices P and Q. The\nconsistency of the knowledge base is analyzed. Coherence conditions and\nexplicit formulas for the extension to marginal distributions are obtained in\nsome special cases.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:52:43 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Gilio", "Angelo", ""], ["Spezzaferri", "Fulvio", ""]]}, {"id": "1303.5405", "submitter": "Robert P. Goldman", "authors": "Robert P. Goldman, John S. Breese", "title": "Integrating Model Construction and Evaluation", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-104-111", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, most probabilistic reasoning systems have relied on a fixed belief\nnetwork constructed at design time. The network is used by an application\nprogram as a representation of (in)dependencies in the domain. Probabilistic\ninference algorithms operate over the network to answer queries. Recognizing\nthe inflexibility of fixed models has led researchers to develop automated\nnetwork construction procedures that use an expressive knowledge base to\ngenerate a network that can answer a query. Although more flexible than fixed\nmodel approaches, these construction procedures separate construction and\nevaluation into distinct phases. In this paper we develop an approach to\ncombining incremental construction and evaluation of a partial probability\nmodel. The combined method holds promise for improved methods for control of\nmodel construction based on a trade-off between fidelity of results and cost of\nconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:52:48 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Goldman", "Robert P.", ""], ["Breese", "John S.", ""]]}, {"id": "1303.5406", "submitter": "Moises Goldszmidt", "authors": "Moises Goldszmidt, Judea Pearl", "title": "Reasoning With Qualitative Probabilities Can Be Tractable", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-112-120", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recently described a formalism for reasoning with if-then rules that re\nexpressed with different levels of firmness [18]. The formalism interprets\nthese rules as extreme conditional probability statements, specifying orders of\nmagnitude of disbelief, which impose constraints over possible rankings of\nworlds. It was shown that, once we compute a priority function Z+ on the rules,\nthe degree to which a given query is confirmed or denied can be computed in\nO(log n`) propositional satisfiability tests, where n is the number of rules in\nthe knowledge base. In this paper, we show that computing Z+ requires O(n2 X\nlog n) satisfiability tests, not an exponential number as was conjectured in\n[18], which reduces to polynomial complexity in the case of Horn expressions.\nWe also show how reasoning with imprecise observations can be incorporated in\nour formalism and how the popular notions of belief revision and epistemic\nentrenchment are embodied naturally and tractably.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:52:54 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Goldszmidt", "Moises", ""], ["Pearl", "Judea", ""]]}, {"id": "1303.5407", "submitter": "Uffe Kj{\\ae}rulff", "authors": "Uffe Kj{\\ae}rulff", "title": "A computational scheme for Reasoning in Dynamic Probabilistic Networks", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-121-129", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computational scheme for reasoning about dynamic systems using (causal)\nprobabilistic networks is presented. The scheme is based on the framework of\nLauritzen and Spiegelhalter (1988), and may be viewed as a generalization of\nthe inference methods of classical time-series analysis in the sense that it\nallows description of non-linear, multivariate dynamic systems with complex\nconditional independence structures. Further, the scheme provides a method for\nefficient backward smoothing and possibilities for efficient, approximate\nforecasting methods. The scheme has been implemented on top of the HUGIN shell.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:53:00 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Kj\u00e6rulff", "Uffe", ""]]}, {"id": "1303.5408", "submitter": "Frank Klawonn", "authors": "Frank Klawonn, Philippe Smets", "title": "The Dynamic of Belief in the Transferable Belief Model and\n  Specialization-Generalization Matrices", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-130-137", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental updating process in the transferable belief model is related\nto the concept of specialization and can be described by a specialization\nmatrix. The degree of belief in the truth of a proposition is a degree of\njustified support. The Principle of Minimal Commitment implies that one should\nnever give more support to the truth of a proposition than justified. We show\nthat Dempster's rule of conditioning corresponds essentially to the least\ncommitted specialization, and that Dempster's rule of combination results\nessentially from commutativity requirements. The concept of generalization,\ndual to thc concept of specialization, is described.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:53:05 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Klawonn", "Frank", ""], ["Smets", "Philippe", ""]]}, {"id": "1303.5409", "submitter": "George J. Klir", "authors": "George J. Klir, Behzad Parviz", "title": "A Note on the Measure of Discord", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-138-141", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new entropy-like measure as well as a new measure of total uncertainty\npertaining to the Dempster-Shafer theory are introduced. It is argued that\nthese measures are better justified than any of the previously proposed\ncandidates.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:53:11 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Klir", "George J.", ""], ["Parviz", "Behzad", ""]]}, {"id": "1303.5410", "submitter": "Henry E. Kyburg Jr.", "authors": "Henry E. Kyburg Jr", "title": "Semantics for Probabilistic Inference", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-142-148", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of writers(Joseph Halpern and Fahiem Bacchus among them) have\noffered semantics for formal languages in which inferences concerning\nprobabilities can be made. Our concern is different. This paper provides a\nformalization of nonmonotonic inferences in which the conclusion is supported\nonly to a certain degree. Such inferences are clearly 'invalid' since they must\nallow the falsity of a conclusion even when the premises are true.\nNevertheless, such inferences can be characterized both syntactically and\nsemantically. The 'premises' of probabilistic arguments are sets of statements\n(as in a database or knowledge base), the conclusions categorical statements in\nthe language. We provide standards for both this form of inference, for which\nhigh probability is required, and for an inference in which the conclusion is\nqualified by an intermediate interval of support.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:53:17 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Kyburg", "Henry E.", "Jr"]]}, {"id": "1303.5411", "submitter": "Henry E. Kyburg Jr.", "authors": "Henry E. Kyburg Jr., Michael Pittarelli", "title": "Some Problems for Convex Bayesians", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-149-154", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss problems for convex Bayesian decision making and uncertainty\nrepresentation. These include the inability to accommodate various natural and\nuseful constraints and the possibility of an analog of the classical Dutch Book\nbeing made against an agent behaving in accordance with convex Bayesian\nprescriptions. A more general set-based Bayesianism may be as tractable and\nwould avoid the difficulties we raise.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:53:22 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Kyburg", "Henry E.", "Jr."], ["Pittarelli", "Michael", ""]]}, {"id": "1303.5412", "submitter": "Kathryn Blackmond Laskey", "authors": "Kathryn Blackmond Laskey", "title": "Bayesian Meta-Reasoning: Determining Model Adequacy from Within a Small\n  World", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-155-158", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian framework for assessing the adequacy of a\nmodel without the necessity of explicitly enumerating a specific alternate\nmodel. A test statistic is developed for tracking the performance of the model\nacross repeated problem instances. Asymptotic methods are used to derive an\napproximate distribution for the test statistic. When the model is rejected,\nthe individual components of the test statistic can be used to guide search for\nan alternate model.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:53:27 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Laskey", "Kathryn Blackmond", ""]]}, {"id": "1303.5413", "submitter": "Kathryn Blackmond Laskey", "authors": "Kathryn Blackmond Laskey", "title": "The Bounded Bayesian", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-159-165", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ideal Bayesian agent reasons from a global probability model, but real\nagents are restricted to simplified models which they know to be adequate only\nin restricted circumstances. Very little formal theory has been developed to\nhelp fallibly rational agents manage the process of constructing and revising\nsmall world models. The goal of this paper is to present a theoretical\nframework for analyzing model management approaches. For a probability\nforecasting problem, a search process over small world models is analyzed as an\napproximation to a larger-world model which the agent cannot explicitly\nenumerate or compute. Conditions are given under which the sequence of\nsmall-world models converges to the larger-world probabilities.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:53:33 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Laskey", "Kathryn Blackmond", ""]]}, {"id": "1303.5414", "submitter": "Tze-Yun Leong", "authors": "Tze-Yun Leong", "title": "Representing Context-Sensitive Knowledge in a Network Formalism: A\n  Preliminary Report", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-166-173", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated decision making is often complicated by the complexity of the\nknowledge involved. Much of this complexity arises from the context sensitive\nvariations of the underlying phenomena. We propose a framework for representing\ndescriptive, context-sensitive knowledge. Our approach attempts to integrate\ncategorical and uncertain knowledge in a network formalism. This paper outlines\nthe basic representation constructs, examines their expressiveness and\nefficiency, and discusses the potential applications of the framework.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:53:40 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Leong", "Tze-Yun", ""]]}, {"id": "1303.5415", "submitter": "Dekang Lin", "authors": "Dekang Lin", "title": "A Probabilistic Network of Predicates", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-174-181", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are directed acyclic graphs representing independence\nrelationships among a set of random variables. A random variable can be\nregarded as a set of exhaustive and mutually exclusive propositions. We argue\nthat there are several drawbacks resulting from the propositional nature and\nacyclic structure of Bayesian networks. To remedy these shortcomings, we\npropose a probabilistic network where nodes represent unary predicates and\nwhich may contain directed cycles. The proposed representation allows us to\nrepresent domain knowledge in a single static network even though we cannot\ndetermine the instantiations of the predicates before hand. The ability to deal\nwith cycles also enables us to handle cyclic causal tendencies and to recognize\nrecursive plans.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:53:47 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Lin", "Dekang", ""]]}, {"id": "1303.5416", "submitter": "Weiru Liu", "authors": "Weiru Liu, John G. Hughes, Michael F. McTear", "title": "Representing Heuristic Knowledge in D-S Theory", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-182-190", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dempster-Shafer theory of evidence has been used intensively to deal with\nuncertainty in knowledge-based systems. However the representation of uncertain\nrelationships between evidence and hypothesis groups (heuristic knowledge) is\nstill a major research problem. This paper presents an approach to representing\nsuch heuristic knowledge by evidential mappings which are defined on the basis\nof mass functions. The relationships between evidential mappings and multi\nvalued mappings, as well as between evidential mappings and Bayesian multi-\nvalued causal link models in Bayesian theory are discussed. Following this the\ndetailed procedures for constructing evidential mappings for any set of\nheuristic rules are introduced. Several situations of belief propagation are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:53:54 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Liu", "Weiru", ""], ["Hughes", "John G.", ""], ["McTear", "Michael F.", ""]]}, {"id": "1303.5417", "submitter": "Izhar Matzkevich", "authors": "Izhar Matzkevich, Bruce Abramson", "title": "The Topological Fusion of Bayes Nets", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-191-198", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayes nets are relatively recent innovations. As a result, most of their\ntheoretical development has focused on the simplest class of single-author\nmodels. The introduction of more sophisticated multiple-author settings raises\na variety of interesting questions. One such question involves the nature of\ncompromise and consensus. Posterior compromises let each model process all data\nto arrive at an independent response, and then split the difference. Prior\ncompromises, on the other hand, force compromise to be reached on all points\nbefore data is observed. This paper introduces prior compromises in a Bayes net\nsetting. It outlines the problem and develops an efficient algorithm for fusing\ntwo directed acyclic graphs into a single, consensus structure, which may then\nbe used as the basis of a prior compromise.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:53:59 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Matzkevich", "Izhar", ""], ["Abramson", "Bruce", ""]]}, {"id": "1303.5418", "submitter": "Serafin Moral", "authors": "Serafin Moral", "title": "Calculating Uncertainty Intervals From Conditional Convex Sets of\n  Probabilities", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-199-206", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Moral, Campos (1991) and Cano, Moral, Verdegay-Lopez (1991) a new method\nof conditioning convex sets of probabilities has been proposed. The result of\nit is a convex set of non-necessarily normalized probability distributions. The\nnormalizing factor of each probability distribution is interpreted as the\npossibility assigned to it by the conditioning information. From this, it is\ndeduced that the natural value for the conditional probability of an event is a\npossibility distribution. The aim of this paper is to study methods of\ntransforming this possibility distribution into a probability (or uncertainty)\ninterval. These methods will be based on the use of Sugeno and Choquet\nintegrals. Their behaviour will be compared in basis to some selected examples.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:54:05 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Moral", "Serafin", ""]]}, {"id": "1303.5419", "submitter": "Ann Nicholson", "authors": "Ann Nicholson, J. M. Brady", "title": "Sensor Validation Using Dynamic Belief Networks", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-207-214", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trajectory of a robot is monitored in a restricted dynamic environment\nusing light beam sensor data. We have a Dynamic Belief Network (DBN), based on\na discrete model of the domain, which provides discrete monitoring analogous to\nconventional quantitative filter techniques. Sensor observations are added to\nthe basic DBN in the form of specific evidence. However, sensor data is often\npartially or totally incorrect. We show how the basic DBN, which infers only an\nimpossible combination of evidence, may be modified to handle specific types of\nincorrect data which may occur in the domain. We then present an extension to\nthe DBN, the addition of an invalidating node, which models the status of the\nsensor as working or defective. This node provides a qualitative explanation of\ninconsistent data: it is caused by a defective sensor. The connection of\nsuccessive instances of the invalidating node models the status of a sensor\nover time, allowing the DBN to handle both persistent and intermittent faults.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:54:11 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Nicholson", "Ann", ""], ["Brady", "J. M.", ""]]}, {"id": "1303.5420", "submitter": "Raymond T. Ng", "authors": "Raymond T. Ng, V. S. Subrahmanian", "title": "Empirical Probabilities in Monadic Deductive Databases", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-215-222", "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of supporting empirical probabilities in monadic logic\ndatabases. Though the semantics of multivalued logic programs has been studied\nextensively, the treatment of probabilities as results of statistical findings\nhas not been studied in logic programming/deductive databases. We develop a\nmodel-theoretic characterization of logic databases that facilitates such a\ntreatment. We present an algorithm for checking consistency of such databases\nand prove its total correctness. We develop a sound and complete query\nprocessing procedure for handling queries to such databases.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:54:16 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Ng", "Raymond T.", ""], ["Subrahmanian", "V. S.", ""]]}, {"id": "1303.5421", "submitter": "Kristian G. Olesen", "authors": "Kristian G. Olesen, Steffen L. Lauritzen, Finn Verner Jensen", "title": "aHUGIN: A System Creating Adaptive Causal Probabilistic Networks", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-223-229", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes aHUGIN, a tool for creating adaptive systems. aHUGIN is\nan extension of the HUGIN shell, and is based on the methods reported by\nSpiegelhalter and Lauritzen (1990a). The adaptive systems resulting from aHUGIN\nare able to adjust the C011ditional probabilities in the model. A short\nanalysis of the adaptation task is given and the features of aHUGIN are\ndescribed. Finally a session with experiments is reported and the results are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:54:22 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Olesen", "Kristian G.", ""], ["Lauritzen", "Steffen L.", ""], ["Jensen", "Finn Verner", ""]]}, {"id": "1303.5422", "submitter": "Gerhard Paa{\\ss}", "authors": "Gerhard Paa{\\ss}", "title": "MESA: Maximum Entropy by Simulated Annealing", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-230-237", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic reasoning systems combine different probabilistic rules and\nprobabilistic facts to arrive at the desired probability values of\nconsequences. In this paper we describe the MESA-algorithm (Maximum Entropy by\nSimulated Annealing) that derives a joint distribution of variables or\npropositions. It takes into account the reliability of probability values and\ncan resolve conflicts between contradictory statements. The joint distribution\nis represented in terms of marginal distributions and therefore allows to\nprocess large inference networks and to determine desired probability values\nwith high precision. The procedure derives a maximum entropy distribution\nsubject to the given constraints. It can be applied to inference networks of\narbitrary topology and may be extended into a number of directions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:54:28 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Paa\u00df", "Gerhard", ""]]}, {"id": "1303.5423", "submitter": "Thomas S. Paterson", "authors": "Thomas S. Paterson, Michael R. Fehling", "title": "Decision Methods for Adaptive Task-Sharing in Associate Systems", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-238-243", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes some results of research on associate systems:\nknowledge-based systems that flexibly and adaptively support their human users\nin carrying out complex, time-dependent problem-solving tasks under\nuncertainty. Based on principles derived from decision theory and decision\nanalysis, a problem-solving approach is presented which can overcome many of\nthe limitations of traditional expert-systems. This approach implements an\nexplicit model of the human user's problem-solving capabilities as an integral\nelement in the overall problem solving architecture. This integrated model,\nrepresented as an influence diagram, is the basis for achieving adaptive task\nsharing behavior between the associate system and the human user. This\nassociate system model has been applied toward ongoing research on a Mars Rover\nManager's Associate (MRMA). MRMA's role would be to manage a small fleet of\nrobotic rovers on the Martian surface. The paper describes results for a\nspecific scenario where MRMA examines the benefits and costs of consulting\nhuman experts on Earth to assist a Mars rover with a complex resource\nmanagement decision.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:54:34 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Paterson", "Thomas S.", ""], ["Fehling", "Michael R.", ""]]}, {"id": "1303.5424", "submitter": "Luigi Portinale", "authors": "Luigi Portinale", "title": "Modeling Uncertain Temporal Evolutions in Model-Based Diagnosis", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-244-251", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the notion of diagnostic problem has been extensively investigated\nin the context of static systems, in most practical applications the behavior\nof the modeled system is significantly variable during time. The goal of the\npaper is to propose a novel approach to the modeling of uncertainty about\ntemporal evolutions of time-varying systems and a characterization of\nmodel-based temporal diagnosis. Since in most real world cases knowledge about\nthe temporal evolution of the system to be diagnosed is uncertain, we consider\nthe case when probabilistic temporal knowledge is available for each component\nof the system and we choose to model it by means of Markov chains. In fact, we\naim at exploiting the statistical assumptions underlying reliability theory in\nthe context of the diagnosis of timevarying systems. We finally show how to\nexploit Markov chain theory in order to discard, in the diagnostic process,\nvery unlikely diagnoses.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:54:40 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Portinale", "Luigi", ""]]}, {"id": "1303.5425", "submitter": "Yuping Qiu", "authors": "Yuping Qiu, Louis Anthony Cox, Jr., Lawrence Davis", "title": "Guess-And-Verify Heuristics for Reducing Uncertainties in Expert\n  Classification Systems", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-252-258", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An expert classification system having statistical information about the\nprior probabilities of the different classes should be able to use this\nknowledge to reduce the amount of additional information that it must collect,\ne.g., through questions, in order to make a correct classification. This paper\nexamines how best to use such prior information and additional\ninformation-collection opportunities to reduce uncertainty about the class to\nwhich a case belongs, thus minimizing the average cost or effort required to\ncorrectly classify new cases.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:54:46 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Qiu", "Yuping", ""], ["Cox,", "Louis Anthony", "Jr."], ["Davis", "Lawrence", ""]]}, {"id": "1303.5426", "submitter": "Peter J. Regan", "authors": "Peter J. Regan, Samuel Holtzman", "title": "R&D Analyst: An Interactive Approach to Normative Decision System Model\n  Construction", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-259-267", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the architecture of R&D Analyst, a commercial\nintelligent decision system for evaluating corporate research and development\nprojects and portfolios. In analyzing projects, R&D Analyst interactively\nguides a user in constructing an influence diagram model for an individual\nresearch project. The system's interactive approach can be clearly explained\nfrom a blackboard system perspective. The opportunistic reasoning emphasis of\nblackboard systems satisfies the flexibility requirements of model\nconstruction, thereby suggesting that a similar architecture would be valuable\nfor developing normative decision systems in other domains. Current research is\naimed at extending the system architecture to explicitly consider of sequential\ndecisions involving limited temporal, financial, and physical resources.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:54:52 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Regan", "Peter J.", ""], ["Holtzman", "Samuel", ""]]}, {"id": "1303.5427", "submitter": "Thomas Schiex", "authors": "Thomas Schiex", "title": "Possibilistic Constraint Satisfaction Problems or \"How to handle soft\n  constraints?\"", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-268-275", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many AI synthesis problems such as planning or scheduling may be modelized as\nconstraint satisfaction problems (CSP). A CSP is typically defined as the\nproblem of finding any consistent labeling for a fixed set of variables\nsatisfying all given constraints between these variables. However, for many\nreal tasks such as job-shop scheduling, time-table scheduling, design?, all\nthese constraints have not the same significance and have not to be necessarily\nsatisfied. A first distinction can be made between hard constraints, which\nevery solution should satisfy and soft constraints, whose satisfaction has not\nto be certain. In this paper, we formalize the notion of possibilistic\nconstraint satisfaction problems that allows the modeling of uncertainly\nsatisfied constraints. We use a possibility distribution over labelings to\nrepresent respective possibilities of each labeling. Necessity-valued\nconstraints allow a simple expression of the respective certainty degrees of\neach constraint. The main advantage of our approach is its integration in the\nCSP technical framework. Most classical techniques, such as Backtracking (BT),\narcconsistency enforcing (AC) or Forward Checking have been extended to handle\npossibilistics CSP and are effectively implemented. The utility of our approach\nis demonstrated on a simple design problem.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:54:58 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Schiex", "Thomas", ""]]}, {"id": "1303.5428", "submitter": "Ross D. Shachter", "authors": "Ross D. Shachter, Mark Alan Peot", "title": "Decision Making Using Probabilistic Inference Methods", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-276-283", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of decision making under uncertainty is closely related to the\nanalysis of probabilistic inference. Indeed, much of the research into\nefficient methods for probabilistic inference in expert systems has been\nmotivated by the fundamental normative arguments of decision theory. In this\npaper we show how the developments underlying those efficient methods can be\napplied immediately to decision problems. In addition to general approaches\nwhich need know nothing about the actual probabilistic inference method, we\nsuggest some simple modifications to the clustering family of algorithms in\norder to efficiently incorporate decision making capabilities.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:55:04 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Shachter", "Ross D.", ""], ["Peot", "Mark Alan", ""]]}, {"id": "1303.5429", "submitter": "Prakash P. Shenoy", "authors": "Prakash P. Shenoy", "title": "Conditional Independence in Uncertainty Theories", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-284-291", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the notions of independence and conditional\nindependence in valuation-based systems (VBS). VBS is an axiomatic framework\ncapable of representing many different uncertainty calculi. We define\nindependence and conditional independence in terms of factorization of the\njoint valuation. The definitions of independence and conditional independence\nin VBS generalize the corresponding definitions in probability theory. Our\ndefinitions apply not only to probability theory, but also to Dempster-Shafer's\nbelief-function theory, Spohn's epistemic-belief theory, and Zadeh's\npossibility theory. In fact, they apply to any uncertainty calculi that fit in\nthe framework of valuation-based systems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:55:11 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Shenoy", "Prakash P.", ""]]}, {"id": "1303.5430", "submitter": "Philippe Smets", "authors": "Philippe Smets", "title": "The Nature of the Unnormalized Beliefs Encountered in the Transferable\n  Belief Model", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-292-297", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the transferable belief model, positive basic belief masses can be\nallocated to the empty set, leading to unnormalized belief functions. The\nnature of these unnormalized beliefs is analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:55:17 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Smets", "Philippe", ""]]}, {"id": "1303.5431", "submitter": "Paul Snow", "authors": "Paul Snow", "title": "Intuitions about Ordered Beliefs Leading to Probabilistic Models", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-298-302", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general use of subjective probabilities to model belief has been\njustified using many axiomatic schemes. For example, ?consistent betting\nbehavior' arguments are well-known. To those not already convinced of the\nunique fitness and generality of probability models, such justifications are\noften unconvincing. The present paper explores another rationale for\nprobability models. ?Qualitative probability,' which is known to provide\nstringent constraints on belief representation schemes, is derived from five\nsimple assumptions about relationships among beliefs. While counterparts of\nfamiliar rationality concepts such as transitivity, dominance, and consistency\nare used, the betting context is avoided. The gap between qualitative\nprobability and probability proper can be bridged by any of several additional\nassumptions. The discussion here relies on results common in the recent AI\nliterature, introducing a sixth simple assumption. The narrative emphasizes\nmodels based on unique complete orderings, but the rationale extends easily to\nmotivate set-valued representations of partial orderings as well.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:55:23 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Snow", "Paul", ""]]}, {"id": "1303.5432", "submitter": "Luis Enrique Sucar", "authors": "Luis Enrique Sucar, Duncan F. Gillies", "title": "Expressing Relational and Temporal Knowledge in Visual Probabilistic\n  Networks", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-303-309", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks have been used extensively in diagnostic tasks such as\nmedicine, where they represent the dependency relations between a set of\nsymptoms and a set of diseases. A criticism of this type of knowledge\nrepresentation is that it is restricted to this kind of task, and that it\ncannot cope with the knowledge required in other artificial intelligence\napplications. For example, in computer vision, we require the ability to model\ncomplex knowledge, including temporal and relational factors. In this paper we\nextend Bayesian networks to model relational and temporal knowledge for\nhigh-level vision. These extended networks have a simple structure which\npermits us to propagate probability efficiently. We have applied them to the\ndomain of endoscopy, illustrating how the general modelling principles can be\nused in specific cases.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:55:28 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Sucar", "Luis Enrique", ""], ["Gillies", "Duncan F.", ""]]}, {"id": "1303.5433", "submitter": "Chin-Wang Tao", "authors": "Chin-Wang Tao, Wiley E. Thompson", "title": "A Fuzzy Logic Approach to Target Tracking", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-310-314", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a target tracking problem in which no dynamic\nmathematical model is explicitly assumed. A nonlinear filter based on the fuzzy\nIf-then rules is developed. A comparison with a Kalman filter is made, and\nempirical results show that the performance of the fuzzy filter is better.\nIntensive simulations suggest that theoretical justification of the empirical\nresults is possible.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:55:34 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Tao", "Chin-Wang", ""], ["Thompson", "Wiley E.", ""]]}, {"id": "1303.5434", "submitter": "Helmut Thone", "authors": "Helmut Thone, Ulrich Guntzer, Werner Kiessling", "title": "Towards Precision of Probabilistic Bounds Propagation", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-315-322", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DUCK-calculus presented here is a recent approach to cope with\nprobabilistic uncertainty in a sound and efficient way. Uncertain rules with\nbounds for probabilities and explicit conditional independences can be\nmaintained incrementally. The basic inference mechanism relies on local bounds\npropagation, implementable by deductive databases with a bottom-up fixpoint\nevaluation. In situations, where no precise bounds are deducible, it can be\ncombined with simple operations research techniques on a local scope. In\nparticular, we provide new precise analytical bounds for probabilistic\nentailment.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:55:40 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Thone", "Helmut", ""], ["Guntzer", "Ulrich", ""], ["Kiessling", "Werner", ""]]}, {"id": "1303.5435", "submitter": "Tom S. Verma", "authors": "Tom S. Verma, Judea Pearl", "title": "An Algorithm for Deciding if a Set of Observed Independencies Has a\n  Causal Explanation", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-323-330", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous paper [Pearl and Verma, 1991] we presented an algorithm for\nextracting causal influences from independence information, where a causal\ninfluence was defined as the existence of a directed arc in all minimal causal\nmodels consistent with the data. In this paper we address the question of\ndeciding whether there exists a causal model that explains ALL the observed\ndependencies and independencies. Formally, given a list M of conditional\nindependence statements, it is required to decide whether there exists a\ndirected acyclic graph (dag) D that is perfectly consistent with M, namely,\nevery statement in M, and no other, is reflected via dseparation in D. We\npresent and analyze an effective algorithm that tests for the existence of such\na day, and produces one, if it exists.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:55:46 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Verma", "Tom S.", ""], ["Pearl", "Judea", ""]]}, {"id": "1303.5436", "submitter": "Carl G. Wagner", "authors": "Carl G. Wagner", "title": "Generalizing Jeffrey Conditionalization", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-331-335", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jeffrey's rule has been generalized by Wagner to the case in which new\nevidence bounds the possible revisions of a prior probability below by a\nDempsterian lower probability. Classical probability kinematics arises within\nthis generalization as the special case in which the evidentiary focal elements\nof the bounding lower probability are pairwise disjoint. We discuss a twofold\nextension of this generalization, first allowing the lower bound to be any\ntwo-monotone capacity and then allowing the prior to be a lower envelope.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:55:52 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Wagner", "Carl G.", ""]]}, {"id": "1303.5437", "submitter": "Michael S. K. M. Wong", "authors": "Michael S. K. M. Wong, L. S. Wang, Y. Y. Yao", "title": "Interval Structure: A Framework for Representing Uncertain Information", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-336-343", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a unified framework for representing uncertain information\nbased on the notion of an interval structure is proposed. It is shown that the\nlower and upper approximations of the rough-set model, the lower and upper\nbounds of incidence calculus, and the belief and plausibility functions all\nobey the axioms of an interval structure. An interval structure can be used to\nsynthesize the decision rules provided by the experts. An efficient algorithm\nto find the desirable set of rules is developed from a set of sound and\ncomplete inference axioms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:55:58 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Wong", "Michael S. K. M.", ""], ["Wang", "L. S.", ""], ["Yao", "Y. Y.", ""]]}, {"id": "1303.5438", "submitter": "Yang Xiang", "authors": "Yang Xiang, David L. Poole, Michael P. Beddoes", "title": "Exploring Localization in Bayesian Networks for Large Expert Systems", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-344-351", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current Bayesian net representations do not consider structure in the domain\nand include all variables in a homogeneous network. At any time, a human\nreasoner in a large domain may direct his attention to only one of a number of\nnatural subdomains, i.e., there is ?localization' of queries and evidence. In\nsuch a case, propagating evidence through a homogeneous network is inefficient\nsince the entire network has to be updated each time. This paper presents\nmultiply sectioned Bayesian networks that enable a (localization preserving)\nrepresentation of natural subdomains by separate Bayesian subnets. The subnets\nare transformed into a set of permanent junction trees such that evidential\nreasoning takes place at only one of them at a time. Probabilities obtained are\nidentical to those that would be obtained from the homogeneous network. We\ndiscuss attention shift to a different junction tree and propagation of\npreviously acquired evidence. Although the overall system can be large,\ncomputational requirements are governed by the size of only one junction tree.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:56:04 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Xiang", "Yang", ""], ["Poole", "David L.", ""], ["Beddoes", "Michael P.", ""]]}, {"id": "1303.5439", "submitter": "Hong Xu", "authors": "Hong Xu", "title": "A Decision Calculus for Belief Functions in Valuation-Based Systems", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-352-359", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valuation-based system (VBS) provides a general framework for representing\nknowledge and drawing inferences under uncertainty. Recent studies have shown\nthat the semantics of VBS can represent and solve Bayesian decision problems\n(Shenoy, 1991a). The purpose of this paper is to propose a decision calculus\nfor Dempster-Shafer (D-S) theory in the framework of VBS. The proposed calculus\nuses a weighting factor whose role is similar to the probabilistic\ninterpretation of an assumption that disambiguates decision problems\nrepresented with belief functions (Strat 1990). It will be shown that with the\npresented calculus, if the decision problems are represented in the valuation\nnetwork properly, we can solve the problems by using fusion algorithm (Shenoy\n1991a). It will also be shown the presented decision calculus can be reduced to\nthe calculus for Bayesian probability theory when probabilities, instead of\nbelief functions, are given.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:56:10 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Xu", "Hong", ""]]}, {"id": "1303.5440", "submitter": "Nevin Lianwen Zhang", "authors": "Nevin Lianwen Zhang, David L. Poole", "title": "Sidestepping the Triangulation Problem in Bayesian Net Computations", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-360-367", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for computing posterior probabilities in\nBayesian nets, which sidesteps the triangulation problem. The current state of\nart is the clique tree propagation approach. When the underlying graph of a\nBayesian net is triangulated, this approach arranges its cliques into a tree\nand computes posterior probabilities by appropriately passing around messages\nin that tree. The computation in each clique is simply direct marginalization.\nWhen the underlying graph is not triangulated, one has to first triangulated it\nby adding edges. Referred to as the triangulation problem, the problem of\nfinding an optimal or even a ?good? triangulation proves to be difficult. In\nthis paper, we propose to first decompose a Bayesian net into smaller\ncomponents by making use of Tarjan's algorithm for decomposing an undirected\ngraph at all its minimal complete separators. Then, the components are arranged\ninto a tree and posterior probabilities are computed by appropriately passing\naround messages in that tree. The computation in each component is carried out\nby repeating the whole procedure from the beginning. Thus the triangulation\nproblem is sidestepped.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:56:16 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Zhang", "Nevin Lianwen", ""], ["Poole", "David L.", ""]]}, {"id": "1303.5659", "submitter": "Taisuke Sato", "authors": "Taisuke Sato and Keiichi Kubota", "title": "Viterbi training in PRISM", "comments": "23 pages, 1 figure", "journal-ref": "Theory and Practice of Logic Programming 15 (2015) 147-168", "doi": "10.1017/S1471068413000677", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VT (Viterbi training), or hard EM, is an efficient way of parameter learning\nfor probabilistic models with hidden variables. Given an observation $y$, it\nsearches for a state of hidden variables $x$ that maximizes $p(x,y \\mid\n\\theta)$ by coordinate ascent on parameters $\\theta$ and $x$. In this paper we\nintroduce VT to PRISM, a logic-based probabilistic modeling system for\ngenerative models. VT improves PRISM in three ways. First VT in PRISM converges\nfaster than EM in PRISM due to the VT's termination condition. Second,\nparameters learned by VT often show good prediction performance compared to\nthose learned by EM. We conducted two parsing experiments with probabilistic\ngrammars while learning parameters by a variety of inference methods, i.e.\\ VT,\nEM, MAP and VB. The result is that VT achieved the best parsing accuracy among\nthem in both experiments. Also we conducted a similar experiment for\nclassification tasks where a hidden variable is not a prediction target unlike\nprobabilistic grammars. We found that in such a case VT does not necessarily\nyield superior performance. Third since VT always deals with a single\nprobability of a single explanation, Viterbi explanation, the exclusiveness\ncondition that is imposed on PRISM programs is no more required if we learn\nparameters by VT.\n  Last but not least we can say that as VT in PRISM is general and applicable\nto any PRISM program, it largely reduces the need for the user to develop a\nspecific VT algorithm for a specific model. Furthermore since VT in PRISM can\nbe used just by setting a PRISM flag appropriately, it makes VT easily\naccessible to (probabilistic) logic programmers. To appear in Theory and\nPractice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 16:22:43 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2013 02:55:17 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Sato", "Taisuke", ""], ["Kubota", "Keiichi", ""]]}, {"id": "1303.5703", "submitter": "Bruce Abramson", "authors": "Bruce Abramson", "title": "ARCO1: An Application of Belief Networks to the Oil Market", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-1-8", "categories": "cs.AI q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief networks are a new, potentially important, class of knowledge-based\nmodels. ARCO1, currently under development at the Atlantic Richfield Company\n(ARCO) and the University of Southern California (USC), is the most advanced\nreported implementation of these models in a financial forecasting setting.\nARCO1's underlying belief network models the variables believed to have an\nimpact on the crude oil market. A pictorial market model-developed on a MAC II-\nfacilitates consensus among the members of the forecasting team. The system\nforecasts crude oil prices via Monte Carlo analyses of the network. Several\ndifferent models of the oil market have been developed; the system's ability to\nbe updated quickly highlights its flexibility.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:29:24 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Abramson", "Bruce", ""]]}, {"id": "1303.5704", "submitter": "John Mark Agosta", "authors": "John Mark Agosta", "title": "\"Conditional Inter-Causally Independent\" Node Distributions, a Property\n  of \"Noisy-Or\" Models", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-9-16", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the interdependence generated between two parent nodes\nwith a common instantiated child node, such as two hypotheses sharing common\nevidence. The relation so generated has been termed \"intercausal.\" It is shown\nby construction that inter-causal independence is possible for binary\ndistributions at one state of evidence. For such \"CICI\" distributions, the two\nmeasures of inter-causal effect, \"multiplicative synergy\" and \"additive\nsynergy\" are equal. The well known \"noisy-or\" model is an example of such a\ndistribution. This introduces novel semantics for the noisy-or, as a model of\nthe degree of conflict among competing hypotheses of a common observation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:29:29 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Agosta", "John Mark", ""]]}, {"id": "1303.5705", "submitter": "Jaume Agust\\'i-Cullell", "authors": "Jaume Agust\\'i-Cullell, Francesc Esteva, Pere Garcia, Lluis Godo,\n  Carles Sierra", "title": "Combining Multiple-Valued Logics in Modular Expert Systems", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-17-25", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way experts manage uncertainty usually changes depending on the task they\nare performing. This fact has lead us to consider the problem of communicating\nmodules (task implementations) in a large and structured knowledge based system\nwhen modules have different uncertainty calculi. In this paper, the analysis of\nthe communication problem is made assuming that (i) each uncertainty calculus\nis an inference mechanism defining an entailment relation, and therefore the\ncommunication is considered to be inference-preserving, and (ii) we restrict\nourselves to the case which the different uncertainty calculi are given by a\nclass of truth functional Multiple-valued Logics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:29:35 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Agust\u00ed-Cullell", "Jaume", ""], ["Esteva", "Francesc", ""], ["Garcia", "Pere", ""], ["Godo", "Lluis", ""], ["Sierra", "Carles", ""]]}, {"id": "1303.5706", "submitter": "Stephane Amarger", "authors": "Stephane Amarger, Didier Dubois, Henri Prade", "title": "Constraint Propagation with Imprecise Conditional Probabilities", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-26-34", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach to reasoning with default rules where the proportion of\nexceptions, or more generally the probability of encountering an exception, can\nbe at least roughly assessed is presented. It is based on local uncertainty\npropagation rules which provide the best bracketing of a conditional\nprobability of interest from the knowledge of the bracketing of some other\nconditional probabilities. A procedure that uses two such propagation rules\nrepeatedly is proposed in order to estimate any simple conditional probability\nof interest from the available knowledge. The iterative procedure, that does\nnot require independence assumptions, looks promising with respect to the\nlinear programming method. Improved bounds for conditional probabilities are\ngiven when independence assumptions hold.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:29:40 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Amarger", "Stephane", ""], ["Dubois", "Didier", ""], ["Prade", "Henri", ""]]}, {"id": "1303.5707", "submitter": "Carlo Berzuini", "authors": "Carlo Berzuini, David J. Spiegelhalter, Riccardo Bellazzi", "title": "Bayesian Networks Aplied to Therapy Monitoring", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-35-43", "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general Bayesian network model for application in a wide class\nof problems of therapy monitoring. We discuss the use of stochastic simulation\nas a computational approach to inference on the proposed class of models. As an\nillustration we present an application to the monitoring of cytotoxic\nchemotherapy in breast cancer.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:29:46 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Berzuini", "Carlo", ""], ["Spiegelhalter", "David J.", ""], ["Bellazzi", "Riccardo", ""]]}, {"id": "1303.5708", "submitter": "Wray L. Buntine", "authors": "Wray L. Buntine", "title": "Some Properties of Plausible Reasoning", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-44-51", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a plausible reasoning system to illustrate some broad\nissues in knowledge representation: dualities between different reasoning\nforms, the difficulty of unifying complementary reasoning styles, and the\napproximate nature of plausible reasoning. These issues have a common\nunderlying theme: there should be an underlying belief calculus of which the\nmany different reasoning forms are special cases, sometimes approximate. The\nsystem presented allows reasoning about defaults, likelihood, necessity and\npossibility in a manner similar to the earlier work of Adams. The system is\nbased on the belief calculus of subjective Bayesian probability which itself is\nbased on a few simple assumptions about how belief should be manipulated.\nApproximations, semantics, consistency and consequence results are presented\nfor the system. While this puts these often discussed plausible reasoning forms\non a probabilistic footing, useful application to practical problems remains an\nissue.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:29:51 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Buntine", "Wray L.", ""]]}, {"id": "1303.5709", "submitter": "Wray L. Buntine", "authors": "Wray L. Buntine", "title": "Theory Refinement on Bayesian Networks", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-52-60", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theory refinement is the task of updating a domain theory in the light of new\ncases, to be done automatically or with some expert assistance. The problem of\ntheory refinement under uncertainty is reviewed here in the context of Bayesian\nstatistics, a theory of belief revision. The problem is reduced to an\nincremental learning task as follows: the learning system is initially primed\nwith a partial theory supplied by a domain expert, and thereafter maintains its\nown internal representation of alternative theories which is able to be\ninterrogated by the domain expert and able to be incrementally refined from\ndata. Algorithms for refinement of Bayesian networks are presented to\nillustrate what is meant by \"partial theory\", \"alternative theory\nrepresentation\", etc. The algorithms are an incremental variant of batch\nlearning algorithms from the literature so can work well in batch and\nincremental mode.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:29:57 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Buntine", "Wray L.", ""]]}, {"id": "1303.5710", "submitter": "Jose E. Cano", "authors": "Jose E. Cano, Serafin Moral, Juan F. Verdegay-Lopez", "title": "Combination of Upper and Lower Probabilities", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-61-68", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider several types of information and methods of\ncombination associated with incomplete probabilistic systems. We discriminate\nbetween 'a priori' and evidential information. The former one is a description\nof the whole population, the latest is a restriction based on observations for\na particular case. Then, we propose different combination methods for each one\nof them. We also consider conditioning as the heterogeneous combination of 'a\npriori' and evidential information. The evidential information is represented\nas a convex set of likelihood functions. These will have an associated\npossibility distribution with behavior according to classical Possibility\nTheory.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:30:02 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Cano", "Jose E.", ""], ["Moral", "Serafin", ""], ["Verdegay-Lopez", "Juan F.", ""]]}, {"id": "1303.5711", "submitter": "Glenn Carroll", "authors": "Glenn Carroll, Eugene Charniak", "title": "A Probabilistic Analysis of Marker-Passing Techniques for\n  Plan-Recognition", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-69-76", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Useless paths are a chronic problem for marker-passing techniques. We use a\nprobabilistic analysis to justify a method for quickly identifying and\nrejecting useless paths. Using the same analysis, we identify key conditions\nand assumptions necessary for marker-passing to perform well.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:30:07 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Carroll", "Glenn", ""], ["Charniak", "Eugene", ""]]}, {"id": "1303.5712", "submitter": "Kuo-Chu Chang", "authors": "Kuo-Chu Chang, Robert Fung", "title": "Symbolic Probabilistic Inference with Continuous Variables", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-77-81", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on Symbolic Probabilistic Inference (SPI) [2, 3] has provided an\nalgorithm for resolving general queries in Bayesian networks. SPI applies the\nconcept of dependency directed backward search to probabilistic inference, and\nis incremental with respect to both queries and observations. Unlike\ntraditional Bayesian network inferencing algorithms, SPI algorithm is goal\ndirected, performing only those calculations that are required to respond to\nqueries. Research to date on SPI applies to Bayesian networks with\ndiscrete-valued variables and does not address variables with continuous\nvalues. In this papers, we extend the SPI algorithm to handle Bayesian networks\nmade up of continuous variables where the relationships between the variables\nare restricted to be ?linear gaussian?. We call this variation of the SPI\nalgorithm, SPI Continuous (SPIC). SPIC modifies the three basic SPI operations:\nmultiplication, summation, and substitution. However, SPIC retains the\nframework of the SPI algorithm, namely building the search tree and recursive\nquery mechanism and therefore retains the goal-directed and incrementality\nfeatures of SPI.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:30:11 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Chang", "Kuo-Chu", ""], ["Fung", "Robert", ""]]}, {"id": "1303.5713", "submitter": "Kuo-Chu Chang", "authors": "Kuo-Chu Chang, Robert Fung", "title": "Symbolic Probabilistic Inference with Evidence Potential", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-82-85", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on the Symbolic Probabilistic Inference (SPI) algorithm[2]\nhas focused attention on the importance of resolving general queries in\nBayesian networks. SPI applies the concept of dependency-directed backward\nsearch to probabilistic inference, and is incremental with respect to both\nqueries and observations. In response to this research we have extended the\nevidence potential algorithm [3] with the same features. We call the extension\nsymbolic evidence potential inference (SEPI). SEPI like SPI can handle generic\nqueries and is incremental with respect to queries and observations. While in\nSPI, operations are done on a search tree constructed from the nodes of the\noriginal network, in SEPI, a clique-tree structure obtained from the evidence\npotential algorithm [3] is the basic framework for recursive query processing.\nIn this paper, we describe the systematic query and caching procedure of SEPI.\nSEPI begins with finding a clique tree from a Bayesian network-the standard\nprocedure of the evidence potential algorithm. With the clique tree, various\nprobability distributions are computed and stored in each clique. This is the\n?pre-processing? step of SEPI. Once this step is done, the query can then be\ncomputed. To process a query, a recursive process similar to the SPI algorithm\nis used. The queries are directed to the root clique and decomposed into\nqueries for the clique's subtrees until a particular query can be answered at\nthe clique at which it is directed. The algorithm and the computation are\nsimple. The SEPI algorithm will be presented in this paper along with several\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:30:16 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Chang", "Kuo-Chu", ""], ["Fung", "Robert", ""]]}, {"id": "1303.5714", "submitter": "Gregory F. Cooper", "authors": "Gregory F. Cooper, Edward H. Herskovits", "title": "A Bayesian Method for Constructing Bayesian Belief Networks from\n  Databases", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-86-94", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian method for constructing Bayesian belief\nnetworks from a database of cases. Potential applications include\ncomputer-assisted hypothesis testing, automated scientific discovery, and\nautomated construction of probabilistic expert systems. Results are presented\nof a preliminary evaluation of an algorithm for constructing a belief network\nfrom a database of cases. We relate the methods in this paper to previous work,\nand we discuss open problems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:30:21 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Cooper", "Gregory F.", ""], ["Herskovits", "Edward H.", ""]]}, {"id": "1303.5715", "submitter": "Bruce D'Ambrosio", "authors": "Bruce D'Ambrosio", "title": "Local Expression Languages for Probabilistic Dependence: a Preliminary\n  Report", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-95-102", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalization of the local expression language used in the\nSymbolic Probabilistic Inference (SPI) approach to inference in belief nets\n[1l, [8]. The local expression language in SPI is the language in which the\ndependence of a node on its antecedents is described. The original language\nrepresented the dependence as a single monolithic conditional probability\ndistribution. The extended language provides a set of operators (*, +, and -)\nwhich can be used to specify methods for combining partial conditional\ndistributions. As one instance of the utility of this extension, we show how\nthis extended language can be used to capture the semantics, representational\nadvantages, and inferential complexity advantages of the \"noisy or\"\nrelationship.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:30:27 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["D'Ambrosio", "Bruce", ""]]}, {"id": "1303.5716", "submitter": "John Fox", "authors": "John Fox, Paul J. Krause", "title": "Symbolic Decision Theory and Autonomous Systems", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-103-110", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to reason under uncertainty and with incomplete information is a\nfundamental requirement of decision support technology. In this paper we argue\nthat the concentration on theoretical techniques for the evaluation and\nselection of decision options has distracted attention from many of the wider\nissues in decision making. Although numerical methods of reasoning under\nuncertainty have strong theoretical foundations, they are representationally\nweak and only deal with a small part of the decision process. Knowledge based\nsystems, on the other hand, offer greater flexibility but have not been\naccompanied by a clear decision theory. We describe here work which is under\nway towards providing a theoretical framework for symbolic decision procedures.\nA central proposal is an extended form of inference which we call\nargumentation; reasoning for and against decision options from generalised\ndomain theories. The approach has been successfully used in several decision\nsupport applications, but it is argued that a comprehensive decision theory\nmust cover autonomous decision making, where the agent can formulate questions\nas well as take decisions. A major theoretical challenge for this theory is to\ncapture the idea of reflection to permit decision agents to reason about their\ngoals, what they believe and why, and what they need to know or do in order to\nachieve their goals.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:30:32 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Fox", "John", ""], ["Krause", "Paul J.", ""]]}, {"id": "1303.5717", "submitter": "B. Fringuelli", "authors": "B. Fringuelli, S. Marcugini, A. Milani, S. Rivoira", "title": "A Reason Maintenace System Dealing with Vague Data", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-111-117", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reason maintenance system which extends an ATMS through Mukaidono's fuzzy\nlogic is described. It supports a problem solver in situations affected by\nincomplete information and vague data, by allowing nonmonotonic inferences and\nthe revision of previous conclusions when contradictions are detected.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:30:37 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Fringuelli", "B.", ""], ["Marcugini", "S.", ""], ["Milani", "A.", ""], ["Rivoira", "S.", ""]]}, {"id": "1303.5718", "submitter": "Dan Geiger", "authors": "Dan Geiger, David Heckerman", "title": "Advances in Probabilistic Reasoning", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-118-126", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discuses multiple Bayesian networks representation paradigms for\nencoding asymmetric independence assertions. We offer three contributions: (1)\nan inference mechanism that makes explicit use of asymmetric independence to\nspeed up computations, (2) a simplified definition of similarity networks and\nextensions of their theory, and (3) a generalized representation scheme that\nencodes more types of asymmetric independence assertions than do similarity\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:30:42 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:56:18 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Geiger", "Dan", ""], ["Heckerman", "David", ""]]}, {"id": "1303.5719", "submitter": "Adam J. Grove", "authors": "Adam J. Grove, Daphne Koller", "title": "Probability Estimation in Face of Irrelevant Information", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-127-134", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider one aspect of the problem of applying decision\ntheory to the design of agents that learn how to make decisions under\nuncertainty. This aspect concerns how an agent can estimate probabilities for\nthe possible states of the world, given that it only makes limited observations\nbefore committing to a decision. We show that the naive application of\nstatistical tools can be improved upon if the agent can determine which of his\nobservations are truly relevant to the estimation problem at hand. We give a\nframework in which such determinations can be made, and define an estimation\nprocedure to use them. Our framework also suggests several extensions, which\nshow how additional knowledge can be used to improve tile estimation procedure\nstill further.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:30:46 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Grove", "Adam J.", ""], ["Koller", "Daphne", ""]]}, {"id": "1303.5720", "submitter": "David Heckerman", "authors": "David Heckerman, Eric J. Horvitz, Blackford Middleton", "title": "An Approximate Nonmyopic Computation for Value of Information", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-135-141", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value-of-information analyses provide a straightforward means for selecting\nthe best next observation to make, and for determining whether it is better to\ngather additional information or to act immediately. Determining the next best\ntest to perform, given a state of uncertainty about the world, requires a\nconsideration of the value of making all possible sequences of observations. In\npractice, decision analysts and expert-system designers have avoided the\nintractability of exact computation of the value of information by relying on a\nmyopic approximation. Myopic analyses are based on the assumption that only one\nadditional test will be performed, even when there is an opportunity to make a\nlarge number of observations. We present a nonmyopic approximation for value of\ninformation that bypasses the traditional myopic analyses by exploiting the\nstatistical properties of large samples.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:30:51 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:55:05 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Heckerman", "David", ""], ["Horvitz", "Eric J.", ""], ["Middleton", "Blackford", ""]]}, {"id": "1303.5721", "submitter": "Max Henrion", "authors": "Max Henrion", "title": "Search-based Methods to Bound Diagnostic Probabilities in Very Large\n  Belief Nets", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-142-150", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since exact probabilistic inference is intractable in general for large\nmultiply connected belief nets, approximate methods are required. A promising\napproach is to use heuristic search among hypotheses (instantiations of the\nnetwork) to find the most probable ones, as in the TopN algorithm. Search is\nbased on the relative probabilities of hypotheses which are efficient to\ncompute. Given upper and lower bounds on the relative probability of partial\nhypotheses, it is possible to obtain bounds on the absolute probabilities of\nhypotheses. Best-first search aimed at reducing the maximum error progressively\nnarrows the bounds as more hypotheses are examined. Here, qualitative\nprobabilistic analysis is employed to obtain bounds on the relative probability\nof partial hypotheses for the BN20 class of networks networks and a\ngeneralization replacing the noisy OR assumption by negative synergy. The\napproach is illustrated by application to a very large belief network, QMR-BN,\nwhich is a reformulation of the Internist-1 system for diagnosis in internal\nmedicine.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:30:56 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Henrion", "Max", ""]]}, {"id": "1303.5722", "submitter": "Eric J. Horvitz", "authors": "Eric J. Horvitz, Geoffrey Rutledge", "title": "Time-Dependent Utility and Action Under Uncertainty", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-151-158", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss representing and reasoning with knowledge about the time-dependent\nutility of an agent's actions. Time-dependent utility plays a crucial role in\nthe interaction between computation and action under bounded resources. We\npresent a semantics for time-dependent utility and describe the use of\ntime-dependent information in decision contexts. We illustrate our discussion\nwith examples of time-pressured reasoning in Protos, a system constructed to\nexplore the ideal control of inference by reasoners with limit abilities.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:31:01 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Horvitz", "Eric J.", ""], ["Rutledge", "Geoffrey", ""]]}, {"id": "1303.5723", "submitter": "Daniel Hunter", "authors": "Daniel Hunter", "title": "Non-monotonic Reasoning and the Reversibility of Belief Change", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-159-164", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to non-monotonic reasoning fail to satisfy a number of\nplausible axioms for belief revision and suffer from conceptual difficulties as\nwell. Recent work on ranked preferential models (RPMs) promises to overcome\nsome of these difficulties. Here we show that RPMs are not adequate to handle\niterated belief change. Specifically, we show that RPMs do not always allow for\nthe reversibility of belief change. This result indicates the need for\nnumerical strengths of belief.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:31:06 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Hunter", "Daniel", ""]]}, {"id": "1303.5724", "submitter": "Yen-Teh Hsia", "authors": "Yen-Teh Hsia", "title": "Belief and Surprise - A Belief-Function Formulation", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-165-173", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We motivate and describe a theory of belief in this paper. This theory is\ndeveloped with the following view of human belief in mind. Consider the belief\nthat an event E will occur (or has occurred or is occurring). An agent either\nentertains this belief or does not entertain this belief (i.e., there is no\n\"grade\" in entertaining the belief). If the agent chooses to exercise \"the will\nto believe\" and entertain this belief, he/she/it is entitled to a degree of\nconfidence c (1 > c > 0) in doing so. Adopting this view of human belief, we\nconjecture that whenever an agent entertains the belief that E will occur with\nc degree of confidence, the agent will be surprised (to the extent c) upon\nrealizing that E did not occur.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:31:10 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Hsia", "Yen-Teh", ""]]}, {"id": "1303.5725", "submitter": "Robert Kennes", "authors": "Robert Kennes", "title": "Evidential Reasoning in a Categorial Perspective: Conjunction and\n  Disjunction of Belief Functions", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-174-181", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The categorial approach to evidential reasoning can be seen as a combination\nof the probability kinematics approach of Richard Jeffrey (1965) and the\nmaximum (cross-) entropy inference approach of E. T. Jaynes (1957). As a\nconsequence of that viewpoint, it is well known that category theory provides\nnatural definitions for logical connectives. In particular, disjunction and\nconjunction are modelled by general categorial constructions known as products\nand coproducts. In this paper, I focus mainly on Dempster-Shafer theory of\nbelief functions for which I introduce a category I call Dempster?s category. I\nprove the existence of and give explicit formulas for conjunction and\ndisjunction in the subcategory of separable belief functions. In Dempster?s\ncategory, the new defined conjunction can be seen as the most cautious\nconjunction of beliefs, and thus no assumption about distinctness (of the\nsources) of beliefs is needed as opposed to Dempster?s rule of combination,\nwhich calls for distinctness (of the sources) of beliefs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:31:15 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Kennes", "Robert", ""]]}, {"id": "1303.5726", "submitter": "Rudolf Kruse", "authors": "Rudolf Kruse, Detlef Nauck, Frank Klawonn", "title": "Reasoning with Mass Distributions", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-182-187", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of movable evidence masses that flow from supersets to subsets as\nspecified by experts represents a suitable framework for reasoning under\nuncertainty. The mass flow is controlled by specialization matrices. New\nevidence is integrated into the frame of discernment by conditioning or\nrevision (Dempster's rule of conditioning), for which special specialization\nmatrices exist. Even some aspects of non-monotonic reasoning can be represented\nby certain specialization matrices.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:31:20 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Kruse", "Rudolf", ""], ["Nauck", "Detlef", ""], ["Klawonn", "Frank", ""]]}, {"id": "1303.5727", "submitter": "Jerome Lang", "authors": "Jerome Lang, Didier Dubois, Henri Prade", "title": "A Logic of Graded Possibility and Certainty Coping with Partial\n  Inconsistency", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-188-196", "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semantics is given to possibilistic logic, a logic that handles weighted\nclassical logic formulae, and where weights are interpreted as lower bounds on\ndegrees of certainty or possibility, in the sense of Zadeh's possibility\ntheory. The proposed semantics is based on fuzzy sets of interpretations. It is\ntolerant to partial inconsistency. Satisfiability is extended from\ninterpretations to fuzzy sets of interpretations, each fuzzy set representing a\npossibility distribution describing what is known about the state of the world.\nA possibilistic knowledge base is then viewed as a set of possibility\ndistributions that satisfy it. The refutation method of automated deduction in\npossibilistic logic, based on previously introduced generalized resolution\nprinciple is proved to be sound and complete with respect to the proposed\nsemantics, including the case of partial inconsistency.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:31:26 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Lang", "Jerome", ""], ["Dubois", "Didier", ""], ["Prade", "Henri", ""]]}, {"id": "1303.5728", "submitter": "Kathryn Blackmond Laskey", "authors": "Kathryn Blackmond Laskey", "title": "Conflict and Surprise: Heuristics for Model Revision", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-197-204", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any probabilistic model of a problem is based on assumptions which, if\nviolated, invalidate the model. Users of probability based decision aids need\nto be alerted when cases arise that are not covered by the aid's model.\nDiagnosis of model failure is also necessary to control dynamic model\nconstruction and revision. This paper presents a set of decision theoretically\nmotivated heuristics for diagnosing situations in which a model is likely to\nprovide an inadequate representation of the process being modeled.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:31:31 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Laskey", "Kathryn Blackmond", ""]]}, {"id": "1303.5729", "submitter": "Paul E. Lehner", "authors": "Paul E. Lehner, Azar Sadigh", "title": "Reasoning under Uncertainty: Some Monte Carlo Results", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-205-211", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A series of monte carlo studies were performed to compare the behavior of\nsome alternative procedures for reasoning under uncertainty. The behavior of\nseveral Bayesian, linear model and default reasoning procedures were examined\nin the context of increasing levels of calibration error. The most interesting\nresult is that Bayesian procedures tended to output more extreme posterior\nbelief values (posterior beliefs near 0.0 or 1.0) than other techniques, but\nthe linear models were relatively less likely to output strong support for an\nerroneous conclusion. Also, accounting for the probabilistic dependencies\nbetween evidence items was important for both Bayesian and linear updating\nprocedures.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:31:35 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Lehner", "Paul E.", ""], ["Sadigh", "Azar", ""]]}, {"id": "1303.5730", "submitter": "Tze-Yun Leong", "authors": "Tze-Yun Leong", "title": "Representation Requirements for Supporting Decision Model Formulation", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-212-219", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines a methodology for analyzing the representational support\nfor knowledge-based decision-modeling in a broad domain. A relevant set of\ninference patterns and knowledge types are identified. By comparing the\nanalysis results to existing representations, some insights are gained into a\ndesign approach for integrating categorical and uncertain knowledge in a\ncontext sensitive manner.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:31:41 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Leong", "Tze-Yun", ""]]}, {"id": "1303.5731", "submitter": "Nathaniel G. Martin", "authors": "Nathaniel G. Martin, James F. Allen", "title": "A Language for Planning with Statistics", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-220-227", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a planner must decide whether it has enough evidence to make a decision\nbased on probability, it faces the sample size problem. Current planners using\nprobabilities need not deal with this problem because they do not generate\ntheir probabilities from observations. This paper presents an event based\nlanguage in which the planner's probabilities are calculated from the binomial\nrandom variable generated by the observed ratio of one type of event to\nanother. Such probabilities are subject to error, so the planner must\nintrospect about their validity. Inferences about the probability of these\nevents can be made using statistics. Inferences about the validity of the\napproximations can be made using interval estimation. Interval estimation\nallows the planner to avoid making choices that are only weakly supported by\nthe planner's evidence.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:31:46 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Martin", "Nathaniel G.", ""], ["Allen", "James F.", ""]]}, {"id": "1303.5732", "submitter": "B\\\"ulent Murtezaoglu", "authors": "B\\\"ulent Murtezao\\u{g}lu, Henry E. Kyburg Jr", "title": "A Modification to Evidential Probability", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-228-231", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the right reference class and the right interval when faced with\nconflicting candidates and no possibility of establishing subset style\ndominance has been a problem for Kyburg's Evidential Probability system.\nVarious methods have been proposed by Loui and Kyburg to solve this problem in\na way that is both intuitively appealing and justifiable within Kyburg's\nframework. The scheme proposed in this paper leads to stronger statistical\nassertions without sacrificing too much of the intuitive appeal of Kyburg's\nlatest proposal.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:31:51 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Murtezao\u011flu", "B\u00fclent", ""], ["Kyburg", "Henry E.", "Jr"]]}, {"id": "1303.5733", "submitter": "Richard E. Neapolitan", "authors": "Richard E. Neapolitan, James Kenevan", "title": "Investigation of Variances in Belief Networks", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-232-241", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The belief network is a well-known graphical structure for representing\nindependences in a joint probability distribution. The methods, which perform\nprobabilistic inference in belief networks, often treat the conditional\nprobabilities which are stored in the network as certain values. However, if\none takes either a subjectivistic or a limiting frequency approach to\nprobability, one can never be certain of probability values. An algorithm\nshould not only be capable of reporting the probabilities of the alternatives\nof remaining nodes when other nodes are instantiated; it should also be capable\nof reporting the uncertainty in these probabilities relative to the uncertainty\nin the probabilities which are stored in the network. In this paper a method\nfor determining the variances in inferred probabilities is obtained under the\nassumption that a posterior distribution on the uncertainty variables can be\napproximated by the prior distribution. It is shown that this assumption is\nplausible if their is a reasonable amount of confidence in the probabilities\nwhich are stored in the network. Furthermore in this paper, a surprising upper\nbound for the prior variances in the probabilities of the alternatives of all\nnodes is obtained in the case where the probability distributions of the\nprobabilities of the alternatives are beta distributions. It is shown that the\nprior variance in the probability at an alternative of a node is bounded above\nby the largest variance in an element of the conditional probability\ndistribution for that node.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:31:56 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Neapolitan", "Richard E.", ""], ["Kenevan", "James", ""]]}, {"id": "1303.5734", "submitter": "Keung-Chi Ng", "authors": "Keung-Chi Ng, Bruce Abramson", "title": "A Sensitivity Analysis of Pathfinder: A Follow-up Study", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-242-248", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At last year?s Uncertainty in AI Conference, we reported the results of a\nsensitivity analysis study of Pathfinder. Our findings were quite\nunexpected-slight variations to Pathfinder?s parameters appeared to lead to\nsubstantial degradations in system performance. A careful look at our first\nanalysis, together with the valuable feedback provided by the participants of\nlast year?s conference, led us to conduct a follow-up study. Our follow-up\ndiffers from our initial study in two ways: (i) the probabilities 0.0 and 1.0\nremained unchanged, and (ii) the variations to the probabilities that are close\nto both ends (0.0 or 1.0) were less than the ones close to the middle (0.5).\nThe results of the follow-up study look more reasonable-slight variations to\nPathfinder?s parameters now have little effect on its performance. Taken\ntogether, these two sets of results suggest a viable extension of a common\ndecision analytic sensitivity analysis to the larger, more complex settings\ngenerally encountered in artificial intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:32:02 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Ng", "Keung-Chi", ""], ["Abramson", "Bruce", ""]]}, {"id": "1303.5735", "submitter": "Raymond T. Ng", "authors": "Raymond T. Ng, V. S. Subrahmanian", "title": "Non-monotonic Negation in Probabilistic Deductive Databases", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-249-256", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the uses and the semantics of non-monotonic negation\nin probabilistic deductive data bases. Based on the stable semantics for\nclassical logic programming, we introduce the notion of stable formula,\nfunctions. We show that stable formula, functions are minimal fixpoints of\noperators associated with probabilistic deductive databases with negation.\nFurthermore, since a. probabilistic deductive database may not necessarily have\na stable formula function, we provide a stable class semantics for such\ndatabases. Finally, we demonstrate that the proposed semantics can handle\ndefault reasoning naturally in the context of probabilistic deduction.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:32:08 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Ng", "Raymond T.", ""], ["Subrahmanian", "V. S.", ""]]}, {"id": "1303.5736", "submitter": "Robert K. Paasch", "authors": "Robert K. Paasch, Alice M. Agogino", "title": "Management of Uncertainty in the Multi-Level Monitoring and Diagnosis of\n  the Time of Flight Scintillation Array", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-257-263", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general architecture for the monitoring and diagnosis of large\nscale sensor-based systems with real time diagnostic constraints. This\narchitecture is multileveled, combining a single monitoring level based on\nstatistical methods with two model based diagnostic levels. At each level,\nsources of uncertainty are identified, and integrated methodologies for\nuncertainty management are developed. The general architecture was applied to\nthe monitoring and diagnosis of a specific nuclear physics detector at Lawrence\nBerkeley National Laboratory that contained approximately 5000 components and\nproduced over 500 channels of output data. The general architecture is\nscalable, and work is ongoing to apply it to detector systems one and two\norders of magnitude more complex.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:32:13 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Paasch", "Robert K.", ""], ["Agogino", "Alice M.", ""]]}, {"id": "1303.5737", "submitter": "Gerhard Paa{\\ss}", "authors": "Gerhard Paass", "title": "Integrating Probabilistic Rules into Neural Networks: A Stochastic EM\n  Learning Algorithm", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-264-270", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EM-algorithm is a general procedure to get maximum likelihood estimates\nif part of the observations on the variables of a network are missing. In this\npaper a stochastic version of the algorithm is adapted to probabilistic neural\nnetworks describing the associative dependency of variables. These networks\nhave a probability distribution, which is a special case of the distribution\ngenerated by probabilistic inference networks. Hence both types of networks can\nbe combined allowing to integrate probabilistic rules as well as unspecified\nassociations in a sound way. The resulting network may have a number of\ninteresting features including cycles of probabilistic rules, hidden\n'unobservable' variables, and uncertain and contradictory evidence.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:32:18 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Paass", "Gerhard", ""]]}, {"id": "1303.5738", "submitter": "David L Poole", "authors": "David L. Poole", "title": "Representing Bayesian Networks within Probabilistic Horn Abduction", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-271-278", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple framework for Horn clause abduction, with\nprobabilities associated with hypotheses. It is shown how this representation\ncan represent any probabilistic knowledge representable in a Bayesian belief\nnetwork. The main contributions are in finding a relationship between logical\nand probabilistic notions of evidential reasoning. This can be used as a basis\nfor a new way to implement Bayesian Networks that allows for approximations to\nthe value of the posterior probabilities, and also points to a way that\nBayesian networks can be extended beyond a propositional language.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:32:22 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Poole", "David L.", ""]]}, {"id": "1303.5739", "submitter": "Gregory M. Provan", "authors": "Gregory M. Provan", "title": "Dynamic Network Updating Techniques For Diagnostic Reasoning", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-279-286", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new probabilistic network construction system, DYNASTY, is proposed for\ndiagnostic reasoning given variables whose probabilities change over time.\nDiagnostic reasoning is formulated as a sequential stochastic process, and is\nmodeled using influence diagrams. Given a set O of observations, DYNASTY\ncreates an influence diagram in order to devise the best action given O.\nSensitivity analyses are conducted to determine if the best network has been\ncreated, given the uncertainty in network parameters and topology. DYNASTY uses\nan equivalence class approach to provide decision thresholds for the\nsensitivity analysis. This equivalence-class approach to diagnostic reasoning\ndifferentiates diagnoses only if the required actions are different. A set of\nnetwork-topology updating algorithms are proposed for dynamically updating the\nnetwork when necessary.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:32:27 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Provan", "Gregory M.", ""]]}, {"id": "1303.5740", "submitter": "Runping Qi", "authors": "Runping Qi, David L. Poole", "title": "High Level Path Planning with Uncertainty", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-287-294", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For high level path planning, environments are usually modeled as distance\ngraphs, and path planning problems are reduced to computing the shortest path\nin distance graphs. One major drawback of this modeling is the inability to\nmodel uncertainties, which are often encountered in practice. In this paper, a\nnew tool, called U-yraph, is proposed for environment modeling. A U-graph is an\nextension of distance graphs with the ability to handle a kind of uncertainty.\nBy modeling an uncertain environment as a U-graph, and a navigation problem as\na Markovian decision process, we can precisely define a new optimality\ncriterion for navigation plans, and more importantly, we can come up with a\ngeneral algorithm for computing optimal plans for navigation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:32:32 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Qi", "Runping", ""], ["Poole", "David L.", ""]]}, {"id": "1303.5741", "submitter": "Arthur Ramer", "authors": "Arthur Ramer", "title": "Formal Model of Uncertainty for Possibilistic Rules", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-295-299", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a universe of discourse X-a domain of possible outcomes-an experiment\nmay consist of selecting one of its elements, subject to the operation of\nchance, or of observing the elements, subject to imprecision. A priori\nuncertainty about the actual result of the experiment may be quantified,\nrepresenting either the likelihood of the choice of :r_X or the degree to which\nany such X would be suitable as a description of the outcome. The former case\ncorresponds to a probability distribution, while the latter gives a possibility\nassignment on X. The study of such assignments and their properties falls\nwithin the purview of possibility theory [DP88, Y80, Z783. It, like probability\ntheory, assigns values between 0 and 1 to express likelihoods of outcomes.\nHere, however, the similarity ends. Possibility theory uses the maximum and\nminimum functions to combine uncertainties, whereas probability theory uses the\nplus and times operations. This leads to very dissimilar theories in terms of\nanalytical framework, even though they share several semantic concepts. One of\nthe shared concepts consists of expressing quantitatively the uncertainty\nassociated with a given distribution. In probability theory its value\ncorresponds to the gain of information that would result from conducting an\nexperiment and ascertaining an actual result. This gain of information can\nequally well be viewed as a decrease in uncertainty about the outcome of an\nexperiment. In this case the standard measure of information, and thus\nuncertainty, is Shannon entropy [AD75, G77]. It enjoys several advantages-it is\ncharacterized uniquely by a few, very natural properties, and it can be\nconveniently used in decision processes. This application is based on the\nprinciple of maximum entropy; it has become a popular method of relating\ndecisions to uncertainty. This paper demonstrates that an equally integrated\ntheory can be built on the foundation of possibility theory. We first show how\nto define measures of in formation and uncertainty for possibility assignments.\nNext we construct an information-based metric on the space of all possibility\ndistributions defined on a given domain. It allows us to capture the notion of\nproximity in information content among the distributions. Lastly, we show that\nall the above constructions can be carried out for continuous\ndistributions-possibility assignments on arbitrary measurable domains. We\nconsider this step very significant-finite domains of discourse are but\napproximations of the real-life infinite domains. If possibility theory is to\nrepresent real world situations, it must handle continuous distributions both\ndirectly and through finite approximations. In the last section we discuss a\nprinciple of maximum uncertainty for possibility distributions. We show how\nsuch a principle could be formalized as an inference rule. We also suggest it\ncould be derived as a consequence of simple assumptions about combining\ninformation. We would like to mention that possibility assignments can be\nviewed as fuzzy sets and that every fuzzy set gives rise to an assignment of\npossibilities. This correspondence has far reaching consequences in logic and\nin control theory. Our treatment here is independent of any special\ninterpretation; in particular we speak of possibility distributions and\npossibility measures, defining them as measurable mappings into the interval\n[0, 1]. Our presentation is intended as a self-contained, albeit terse summary.\nTopics discussed were selected with care, to demonstrate both the completeness\nand a certain elegance of the theory. Proofs are not included; we only offer\nillustrative examples.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:32:37 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Ramer", "Arthur", ""]]}, {"id": "1303.5742", "submitter": "Anand S. Rao", "authors": "Anand S. Rao, Michael P. Georgeff", "title": "Deliberation and its Role in the Formation of Intentions", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-300-307", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deliberation plays an important role in the design of rational agents\nembedded in the real-world. In particular, deliberation leads to the formation\nof intentions, i.e., plans of action that the agent is committed to achieving.\nIn this paper, we present a branching time possible-worlds model for\nrepresenting and reasoning about, beliefs, goals, intentions, time, actions,\nprobabilities, and payoffs. We compare this possible-worlds approach with the\nmore traditional decision tree representation and provide a transformation from\ndecision trees to possible worlds. Finally, we illustrate how an agent can\nperform deliberation using a decision-tree representation and then use a\npossible-worlds model to form and reason about his intentions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:32:42 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Rao", "Anand S.", ""], ["Georgeff", "Michael P.", ""]]}, {"id": "1303.5743", "submitter": "Bhavani Raskutti", "authors": "Bhavani Raskutti, Ingrid Zukerman", "title": "Handling Uncertainty during Plan Recognition in Task-Oriented\n  Consultation Systems", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-308-315", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During interactions with human consultants, people are used to providing\npartial and/or inaccurate information, and still be understood and assisted. We\nattempt to emulate this capability of human consultants; in computer\nconsultation systems. In this paper, we present a mechanism for handling\nuncertainty in plan recognition during task-oriented consultations. The\nuncertainty arises while choosing an appropriate interpretation of a user?s\nstatements among many possible interpretations. Our mechanism handles this\nuncertainty by using probability theory to assess the probabilities of the\ninterpretations, and complements this assessment by taking into account the\ninformation content of the interpretations. The information content of an\ninterpretation is a measure of how well defined an interpretation is in terms\nof the actions to be performed on the basis of the interpretation. This measure\nis used to guide the inference process towards interpretations with a higher\ninformation content. The information content for an interpretation depends on\nthe specificity and the strength of the inferences in it, where the strength of\nan inference depends on the reliability of the information on which the\ninference is based. Our mechanism has been developed for use in task-oriented\nconsultation systems. The domain that we have chosen for exploration is that of\na travel agency.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:32:47 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Raskutti", "Bhavani", ""], ["Zukerman", "Ingrid", ""]]}, {"id": "1303.5744", "submitter": "Enrique H. Ruspini", "authors": "Enrique H. Ruspini", "title": "Truth as Utility: A Conceptual Synthesis", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-316-322", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces conceptual relations that synthesize utilitarian and\nlogical concepts, extending the logics of preference of Rescher. We define\nfirst, in the context of a possible worlds model, constraint-dependent measures\nthat quantify the relative quality of alternative solutions of reasoning\nproblems or the relative desirability of various policies in control, decision,\nand planning problems. We show that these measures may be interpreted as truth\nvalues in a multi valued logic and propose mechanisms for the representation of\ncomplex constraints as combinations of simpler restrictions. These extended\nlogical operations permit also the combination and aggregation of goal-specific\nquality measures into global measures of utility. We identify also relations\nthat represent differential preferences between alternative solutions and\nrelate them to the previously defined desirability measures. Extending\nconventional modal logic formulations, we introduce structures for the\nrepresentation of ignorance about the utility of alternative solutions.\nFinally, we examine relations between these concepts and similarity based\nsemantic models of fuzzy logic.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:32:53 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Ruspini", "Enrique H.", ""]]}, {"id": "1303.5745", "submitter": "Alessandro Saffiotti", "authors": "Alessandro Saffiotti, Elisabeth Umkehrer", "title": "Pulcinella: A General Tool for Propagating Uncertainty in Valuation\n  Networks", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-323-331", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PULCinella and its use in comparing uncertainty theories.\nPULCinella is a general tool for Propagating Uncertainty based on the Local\nComputation technique of Shafer and Shenoy. It may be specialized to different\nuncertainty theories: at the moment, Pulcinella can propagate probabilities,\nbelief functions, Boolean values, and possibilities. Moreover, Pulcinella\nallows the user to easily define his own specializations. To illustrate\nPulcinella, we analyze two examples by using each of the four theories above.\nIn the first one, we mainly focus on intrinsic differences between theories. In\nthe second one, we take a knowledge engineer viewpoint, and check the adequacy\nof each theory to a given problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:32:58 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Saffiotti", "Alessandro", ""], ["Umkehrer", "Elisabeth", ""]]}, {"id": "1303.5746", "submitter": "Sandra Sandri", "authors": "Sandra Sandri", "title": "Structuring Bodies of Evidence", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-332-338", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present two ways of structuring bodies of evidence, which\nallow us to reduce the complexity of the operations usually performed in the\nframework of evidence theory. The first structure just partitions the focal\nelements in a body of evidence by their cardinality. With this structure we are\nable to reduce the complexity on the calculation of the belief functions Bel,\nPl, and Q. The other structure proposed here, the Hierarchical Trees, permits\nus to reduce the complexity of the calculation of Bel, Pl, and Q, as well as of\nthe Dempster's rule of combination in relation to the brute-force algorithm.\nBoth these structures do not require the generation of all the subsets of the\nreference domain.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:33:02 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Sandri", "Sandra", ""]]}, {"id": "1303.5747", "submitter": "Eugene Santos Jr.", "authors": "Eugene Santos Jr", "title": "On the Generation of Alternative Explanations with Implications for\n  Belief Revision", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-339-347", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, the best explanation for a given observation makes no promises on\nhow good it is with respect to other alternative explanations. A major\ndeficiency of message-passing schemes for belief revision in Bayesian networks\nis their inability to generate alternatives beyond the second best. In this\npaper, we present a general approach based on linear constraint systems that\nnaturally generates alternative explanations in an orderly and highly efficient\nmanner. This approach is then applied to cost-based abduction problems as well\nas belief revision in Bayesian net works.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:33:07 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Santos", "Eugene", "Jr"]]}, {"id": "1303.5748", "submitter": "Kerstin Schill", "authors": "Kerstin Schill, Ernst Poppel, Christoph Zetzsche", "title": "Completing Knowledge by Competing Hierarchies", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-348-352", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A control strategy for expert systems is presented which is based on Shafer's\nBelief theory and the combination rule of Dempster. In contrast to well known\nstrategies it is not sequentially and hypotheses-driven, but parallel and self\norganizing, determined by the concept of information gain. The information\ngain, calculated as the maximal difference between the actual evidence\ndistribution in the knowledge base and the potential evidence determines each\nconsultation step. Hierarchically structured knowledge is an important\nrepresentation form and experts even use several hierarchies in parallel for\nconstituting their knowledge. Hence the control strategy is applied to a\nlayered set of distinct hierarchies. Depending on the actual data one of these\nhierarchies is chosen by the control strategy for the next step in the\nreasoning process. Provided the actual data are well matched to the structure\nof one hierarchy, this hierarchy remains selected for a longer consultation\ntime. If no good match can be achieved, a switch from the actual hierarchy to a\ncompeting one will result, very similar to the phenomenon of restructuring in\nproblem solving tasks. Up to now the control strategy is restricted to multi\nhierarchical knowledge bases with disjunct hierarchies. It is implemented in\nthe expert system IBIG (inference by information gain), being presently applied\nto acquired speech disorders (aphasia).\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:33:12 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Schill", "Kerstin", ""], ["Poppel", "Ernst", ""], ["Zetzsche", "Christoph", ""]]}, {"id": "1303.5749", "submitter": "Ross D. Shachter", "authors": "Ross D. Shachter", "title": "A Graph-Based Inference Method for Conditional Independence", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-353-360", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graphoid axioms for conditional independence, originally described by\nDawid [1979], are fundamental to probabilistic reasoning [Pearl, 19881. Such\naxioms provide a mechanism for manipulating conditional independence assertions\nwithout resorting to their numerical definition. This paper explores a\nrepresentation for independence statements using multiple undirected graphs and\nsome simple graphical transformations. The independence statements derivable in\nthis system are equivalent to those obtainable by the graphoid axioms.\nTherefore, this is a purely graphical proof technique for conditional\nindependence.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:33:17 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Shachter", "Ross D.", ""]]}, {"id": "1303.5750", "submitter": "Prakash P. Shenoy", "authors": "Prakash P. Shenoy", "title": "A Fusion Algorithm for Solving Bayesian Decision Problems", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-361-369", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for solving Bayesian decision problems. The\nmethod consists of representing a Bayesian decision problem as a\nvaluation-based system and applying a fusion algorithm for solving it. The\nfusion algorithm is a hybrid of local computational methods for computation of\nmarginals of joint probability distributions and the local computational\nmethods for discrete optimization problems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:33:23 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Shenoy", "Prakash P.", ""]]}, {"id": "1303.5751", "submitter": "Solomon Eyal Shimony", "authors": "Solomon Eyal Shimony", "title": "Algorithms for Irrelevance-Based Partial MAPs", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-370-377", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irrelevance-based partial MAPs are useful constructs for domain-independent\nexplanation using belief networks. We look at two definitions for such partial\nMAPs, and prove important properties that are useful in designing algorithms\nfor computing them effectively. We make use of these properties in modifying\nour standard MAP best-first algorithm, so as to handle irrelevance-based\npartial MAPs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:33:28 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Shimony", "Solomon Eyal", ""]]}, {"id": "1303.5752", "submitter": "Philippe Smets", "authors": "Philippe Smets", "title": "About Updating", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-378-385", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survey of several forms of updating, with a practical illustrative example.\nWe study several updating (conditioning) schemes that emerge naturally from a\ncommon scenarion to provide some insights into their meaning. Updating is a\nsubtle operation and there is no single method, no single 'good' rule. The\nchoice of the appropriate rule must always be given due consideration. Planchet\n(1989) presents a mathematical survey of many rules. We focus on the practical\nmeaning of these rules. After summarizing the several rules for conditioning,\nwe present an illustrative example in which the various forms of conditioning\ncan be explained.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:33:33 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Smets", "Philippe", ""]]}, {"id": "1303.5753", "submitter": "Paul Snow", "authors": "Paul Snow", "title": "Compressed Constraints in Probabilistic Logic and Their Revision", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-386-391", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In probabilistic logic entailments, even moderate size problems can yield\nlinear constraint systems with so many variables that exact methods are\nimpractical. This difficulty can be remedied in many cases of interest by\nintroducing a three valued logic (true, false, and \"don't care\"). The\nthree-valued approach allows the construction of \"compressed\" constraint\nsystems which have the same solution sets as their two-valued counterparts, but\nwhich may involve dramatically fewer variables. Techniques to calculate point\nestimates for the posterior probabilities of entailed sentences are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:33:38 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Snow", "Paul", ""]]}, {"id": "1303.5754", "submitter": "Peter L. Spirtes", "authors": "Peter L. Spirtes", "title": "Detecting Causal Relations in the Presence of Unmeasured Variables", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-392-397", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of latent variables can greatly complicate inferences about\ncausal relations between measured variables from statistical data. In many\ncases, the presence of latent variables makes it impossible to determine for\ntwo measured variables A and B, whether A causes B, B causes A, or there is\nsome common cause. In this paper I present several theorems that state\nconditions under which it is possible to reliably infer the causal relation\nbetween two measured variables, regardless of whether latent variables are\nacting or not.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:33:43 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Spirtes", "Peter L.", ""]]}, {"id": "1303.5755", "submitter": "Deborah L. Thurston", "authors": "Deborah L. Thurston, Yun Qi Tian", "title": "A Method for Integrating Utility Analysis into an Expert System for\n  Design Evaluation", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-398-405", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mechanical design, there is often unavoidable uncertainty in estimates of\ndesign performance. Evaluation of design alternatives requires consideration of\nthe impact of this uncertainty. Expert heuristics embody assumptions regarding\nthe designer's attitude towards risk and uncertainty that might be reasonable\nin most cases but inaccurate in others. We present a technique to allow\ndesigners to incorporate their own unique attitude towards uncertainty as\nopposed to those assumed by the domain expert's rules. The general approach is\nto eliminate aspects of heuristic rules which directly or indirectly include\nassumptions regarding the user's attitude towards risk, and replace them with\nexplicit, user-specified probabilistic multi attribute utility and probability\ndistribution functions. We illustrate the method in a system for material\nselection for automobile bumpers.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:33:48 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Thurston", "Deborah L.", ""], ["Tian", "Yun Qi", ""]]}, {"id": "1303.5756", "submitter": "Wilson X. Wen", "authors": "Wilson X. Wen", "title": "From Relational Databases to Belief Networks", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-406-413", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between belief networks and relational databases is\nexamined. Based on this analysis, a method to construct belief networks\nautomatically from statistical relational data is proposed. A comparison\nbetween our method and other methods shows that our method has several\nadvantages when generalization or prediction is deeded.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:33:53 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Wen", "Wilson X.", ""]]}, {"id": "1303.5757", "submitter": "Nic Wilson", "authors": "Nic Wilson", "title": "A Monte-Carlo Algorithm for Dempster-Shafer Belief", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-414-417", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A very computationally-efficient Monte-Carlo algorithm for the calculation of\nDempster-Shafer belief is described. If Bel is the combination using Dempster's\nRule of belief functions Bel, ..., Bel,7, then, for subset b of the frame C),\nBel(b) can be calculated in time linear in 1(31 and m (given that the weight of\nconflict is bounded). The algorithm can also be used to improve the complexity\nof the Shenoy-Shafer algorithms on Markov trees, and be generalised to\ncalculate Dempster-Shafer Belief over other logics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:33:58 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Wilson", "Nic", ""]]}, {"id": "1303.5758", "submitter": "Michael S. K. M. Wong", "authors": "Michael S. K. M. Wong, Y. Y. Yao, P. Lingras", "title": "Compatibility of Quantitative and Qualitative Representations of Belief", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-418-424", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compatibility of quantitative and qualitative representations of beliefs\nwas studied extensively in probability theory. It is only recently that this\nimportant topic is considered in the context of belief functions. In this\npaper, the compatibility of various quantitative belief measures and\nqualitative belief structures is investigated. Four classes of belief measures\nconsidered are: the probability function, the monotonic belief function,\nShafer's belief function, and Smets' generalized belief function. The analysis\nof their individual compatibility with different belief structures not only\nprovides a sound b<msis for these quantitative measures, but also alleviates\nsome of the difficulties in the acquisition and interpretation of numeric\nbelief numbers. It is shown that the structure of qualitative probability is\ncompatible with monotonic belief functions. Moreover, a belief structure\nslightly weaker than that of qualitative belief is compatible with Smets'\ngeneralized belief functions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:34:02 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Wong", "Michael S. K. M.", ""], ["Yao", "Y. Y.", ""], ["Lingras", "P.", ""]]}, {"id": "1303.5759", "submitter": "Hong Xu", "authors": "Hong Xu", "title": "An Efficient Implementation of Belief Function Propagation", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-425-432", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local computation technique (Shafer et al. 1987, Shafer and Shenoy 1988,\nShenoy and Shafer 1986) is used for propagating belief functions in so called a\nMarkov Tree. In this paper, we describe an efficient implementation of belief\nfunction propagation on the basis of the local computation technique. The\npresented method avoids all the redundant computations in the propagation\nprocess, and so makes the computational complexity decrease with respect to\nother existing implementations (Hsia and Shenoy 1989, Zarley et al. 1988). We\nalso give a combined algorithm for both propagation and re-propagation which\nmakes the re-propagation process more efficient when one or more of the prior\nbelief functions is changed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:34:07 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Xu", "Hong", ""]]}, {"id": "1303.5760", "submitter": "Ronald R. Yager", "authors": "Ronald R. Yager", "title": "A Non-Numeric Approach to Multi-Criteria/Multi-Expert Aggregation Based\n  on Approximate Reasoning", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-433-437", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a technique that can be used for the fusion of multiple sources\nof information as well as for the evaluation and selection of alternatives\nunder multi-criteria. Three important properties contribute to the uniqueness\nof the technique introduced. The first is the ability to do all necessary\noperations and aggregations with information that is of a nonnumeric linguistic\nnature. This facility greatly reduces the burden on the providers of\ninformation, the experts. A second characterizing feature is the ability\nassign, again linguistically, differing importance to the criteria or in the\ncase of information fusion to the individual sources of information. A third\nsignificant feature of the approach is its ability to be used as method to find\na consensus of the opinion of multiple experts on the issue of concern. The\ntechniques used in this approach are base on ideas developed from the theory of\napproximate reasoning. We illustrate the approach with a problem of project\nselection.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:34:12 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Yager", "Ronald R.", ""]]}, {"id": "1303.5761", "submitter": "Henry E. Kyburg Jr.", "authors": "Henry E. Kyburg Jr", "title": "Why Do We Need Foundations for Modelling Uncertainties?", "comments": "Appears in Proceedings of the Seventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1991)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1991-PG-438-442", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surely we want solid foundations. What kind of castle can we build on sand?\nWhat is the point of devoting effort to balconies and minarets, if the\nfoundation may be so weak as to allow the structure to collapse of its own\nweight? We want our foundations set on bedrock, designed to last for\ngenerations. Who would want an architect who cannot certify the soundness of\nthe foundations of his buildings?\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:34:17 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Kyburg", "Henry E.", "Jr"]]}, {"id": "1303.5887", "submitter": "Hector Zenil", "authors": "Hector Zenil", "title": "A Behavioural Foundation for Natural Computing and a Programmability\n  Test", "comments": "37 pages, 4 figures. Based on an invited Talk at the Symposium on\n  Natural/Unconventional Computing and its Philosophical Significance, Alan\n  Turing World Congress 2012, Birmingham, UK.\n  http://link.springer.com/article/10.1007/s13347-012-0095-2 Ref. glitch fixed\n  in 2nd. version; Philosophy & Technology (special issue on History and\n  Philosophy of Computing), Springer, 2013", "journal-ref": null, "doi": "10.1007/s13347-012-0095-2", "report-no": null, "categories": "cs.IT cs.AI cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does it mean to claim that a physical or natural system computes? One\nanswer, endorsed here, is that computing is about programming a system to\nbehave in different ways. This paper offers an account of what it means for a\nphysical system to compute based on this notion. It proposes a behavioural\ncharacterisation of computing in terms of a measure of programmability, which\nreflects a system's ability to react to external stimuli. The proposed measure\nof programmability is useful for classifying computers in terms of the apparent\nalgorithmic complexity of their evolution in time. I make some specific\nproposals in this connection and discuss this approach in the context of other\nbehavioural approaches, notably Turing's test of machine intelligence. I also\nanticipate possible objections and consider the applicability of these\nproposals to the task of relating abstract computation to nature-like\ncomputation.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2013 21:44:08 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2013 12:02:27 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Zenil", "Hector", ""]]}, {"id": "1303.5919", "submitter": "Akhil Jabbar Meerja", "authors": "M.Akhil Jabbar, B L Deekshatulu, Priti Chandra", "title": "Heart Disease Prediction System using Associative Classification and\n  Genetic Algorithm", "comments": "International Conference on Emerging Trends in Electrical,\n  Electronics and Communication Technologies-ICECIT, 2012", "journal-ref": "Vol no1 pp 183-192, Elsevier Dec 2012", "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associative classification is a recent and rewarding technique which\nintegrates association rule mining and classification to a model for prediction\nand achieves maximum accuracy. Associative classifiers are especially fit to\napplications where maximum accuracy is desired to a model for prediction. There\nare many domains such as medical where the maximum accuracy of the model is\ndesired. Heart disease is a single largest cause of death in developed\ncountries and one of the main contributors to disease burden in developing\ncountries. Mortality data from the registrar general of India shows that heart\ndisease are a major cause of death in India, and in Andhra Pradesh coronary\nheart disease cause about 30%of deaths in rural areas. Hence there is a need to\ndevelop a decision support system for predicting heart disease of a patient. In\nthis paper we propose efficient associative classification algorithm using\ngenetic approach for heart disease prediction. The main motivation for using\ngenetic algorithm in the discovery of high level prediction rules is that the\ndiscovered rules are highly comprehensible, having high predictive accuracy and\nof high interestingness values. Experimental Results show that most of the\nclassifier rules help in the best prediction of heart disease which even helps\ndoctors in their diagnosis decisions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 07:18:50 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Jabbar", "M. Akhil", ""], ["Deekshatulu", "B L", ""], ["Chandra", "Priti", ""]]}, {"id": "1303.5929", "submitter": "Sourish Dasgupta", "authors": "Sourish Dasgupta, Ankur Padia, Kushal Shah, Rupali KaPatel, Prasenjit\n  Majumder", "title": "DLOLIS-A: Description Logic based Text Ontology Learning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology Learning has been the subject of intensive study for the past\ndecade. Researchers in this field have been motivated by the possibility of\nautomatically building a knowledge base on top of text documents so as to\nsupport reasoning based knowledge extraction. While most works in this field\nhave been primarily statistical (known as light-weight Ontology Learning) not\nmuch attempt has been made in axiomatic Ontology Learning (called heavy-weight\nOntology Learning) from Natural Language text documents. Heavy-weight Ontology\nLearning supports more precise formal logic-based reasoning when compared to\nstatistical ontology learning. In this paper we have proposed a sound Ontology\nLearning tool DLOL_(IS-A) that maps English language IS-A sentences into their\nequivalent Description Logic (DL) expressions in order to automatically\ngenerate a consistent pair of T-box and A-box thereby forming both regular\n(definitional form) and generalized (axiomatic form) DL ontology. The current\nscope of the paper is strictly limited to IS-A sentences that exclude the\npossible structures of: (i) implicative IS-A sentences, and (ii) \"Wh\" IS-A\nquestions. Other linguistic nuances that arise out of pragmatics and epistemic\nof IS-A sentences are beyond the scope of this present work. We have adopted\nGold Standard based Ontology Learning evaluation on chosen IS-A rich Wikipedia\ndocuments.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 08:39:18 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Dasgupta", "Sourish", ""], ["Padia", "Ankur", ""], ["Shah", "Kushal", ""], ["KaPatel", "Rupali", ""], ["Majumder", "Prasenjit", ""]]}, {"id": "1303.6145", "submitter": "Manuel Schmitt", "authors": "Manuel Schmitt and Rolf Wanka", "title": "Particles Prefer Walking Along the Axes: Experimental Insights into the\n  Behavior of a Particle Swarm", "comments": "Full version of poster on Genetic and Evolutionary Computation\n  Conference (GECCO) 13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle swarm optimization (PSO) is a widely used nature-inspired\nmeta-heuristic for solving continuous optimization problems. However, when\nrunning the PSO algorithm, one encounters the phenomenon of so-called\nstagnation, that means in our context, the whole swarm starts to converge to a\nsolution that is not (even a local) optimum. The goal of this work is to point\nout possible reasons why the swarm stagnates at these non-optimal points. To\nachieve our results, we use the newly defined potential of a swarm. The total\npotential has a portion for every dimension of the search space, and it drops\nwhen the swarm approaches the point of convergence. As it turns out\nexperimentally, the swarm is very likely to come sometimes into \"unbalanced\"\nstates, i. e., almost all potential belongs to one axis. Therefore, the swarm\nbecomes blind for improvements still possible in any other direction. Finally,\nwe show how in the light of the potential and these observations, a slightly\nadapted PSO rebalances the potential and therefore increases the quality of the\nsolution.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 14:39:27 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 15:17:07 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Schmitt", "Manuel", ""], ["Wanka", "Rolf", ""]]}, {"id": "1303.6932", "submitter": "Saleem Abdullah", "authors": "Muhammad Aslam, Saleem Abdullah and Kifayat ullah", "title": "Bipolar Fuzzy Soft sets and its applications in decision making problem", "comments": null, "journal-ref": null, "doi": "10.3233/IFS-131031", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we combine the concept of a bipolar fuzzy set and a soft\nset. We introduce the notion of bipolar fuzzy soft set and study fundamental\nproperties. We study basic operations on bipolar fuzzy soft set. We define\nexdended union, intersection of two bipolar fuzzy soft set. We also give an\napplication of bipolar fuzzy soft set into decision making problem. We give a\ngeneral algorithm to solve decision making problems by using bipolar fuzzy soft\nset.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2013 19:26:43 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Aslam", "Muhammad", ""], ["Abdullah", "Saleem", ""], ["ullah", "Kifayat", ""]]}, {"id": "1303.7032", "submitter": "Zhe Yao", "authors": "Zhe Yao, Vincent Gripon and Michael G. Rabbat", "title": "A Massively Parallel Associative Memory Based on Sparse Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associative memories store content in such a way that the content can be\nlater retrieved by presenting the memory with a small portion of the content,\nrather than presenting the memory with an address as in more traditional\nmemories. Associative memories are used as building blocks for algorithms\nwithin database engines, anomaly detection systems, compression algorithms, and\nface recognition systems. A classical example of an associative memory is the\nHopfield neural network. Recently, Gripon and Berrou have introduced an\nalternative construction which builds on ideas from the theory of error\ncorrecting codes and which greatly outperforms the Hopfield network in\ncapacity, diversity, and efficiency. In this paper we implement a variation of\nthe Gripon-Berrou associative memory on a general purpose graphical processing\nunit (GPU). The work of Gripon and Berrou proposes two retrieval rules,\nsum-of-sum and sum-of-max. The sum-of-sum rule uses only matrix-vector\nmultiplication and is easily implemented on the GPU. The sum-of-max rule is\nmuch less straightforward to implement because it involves non-linear\noperations. However, the sum-of-max rule gives significantly better retrieval\nerror rates. We propose a hybrid rule tailored for implementation on a GPU\nwhich achieves a 880-fold speedup without sacrificing any accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 03:49:57 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2013 14:29:21 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Yao", "Zhe", ""], ["Gripon", "Vincent", ""], ["Rabbat", "Michael G.", ""]]}, {"id": "1303.7077", "submitter": "Oleg Verbitsky", "authors": "Christoph Berkholz and Oleg Verbitsky", "title": "On the speed of constraint propagation and the time complexity of arc\n  consistency testing", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing arc consistency on two relational structures is one of the most\npopular heuristics for the constraint satisfaction problem. We aim at\ndetermining the time complexity of arc consistency testing. The input\nstructures $G$ and $H$ can be supposed to be connected colored graphs, as the\ngeneral problem reduces to this particular case. We first observe the upper\nbound $O(e(G)v(H)+v(G)e(H))$, which implies the bound $O(e(G)e(H))$ in terms of\nthe number of edges and the bound $O((v(G)+v(H))^3)$ in terms of the number of\nvertices. We then show that both bounds are tight up to a constant factor as\nlong as an arc consistency algorithm is based on constraint propagation (like\nany algorithm currently known).\n  Our argument for the lower bounds is based on examples of slow constraint\npropagation. We measure the speed of constraint propagation observed on a pair\n$G,H$ by the size of a proof, in a natural combinatorial proof system, that\nSpoiler wins the existential 2-pebble game on $G,H$. The proof size is bounded\nfrom below by the game length $D(G,H)$, and a crucial ingredient of our\nanalysis is the existence of $G,H$ with $D(G,H)=\\Omega(v(G)v(H))$. We find one\nsuch example among old benchmark instances for the arc consistency problem and\nalso suggest a new, different construction.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 09:39:53 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Berkholz", "Christoph", ""], ["Verbitsky", "Oleg", ""]]}, {"id": "1303.7085", "submitter": "Othman Benammar", "authors": "Othman Benammar, Hicham Elasri, Abderrahim Sekkaki", "title": "Semantic Matching of Security Policies to Support Security Experts", "comments": "SECURWARE 2012 : The Sixth International Conference on Emerging\n  Security Information, Systems and Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Management of security policies has become increasingly difficult given the\nnumber of domains to manage, taken into consideration their extent and their\ncomplexity. Security experts has to deal with a variety of frameworks and\nspecification languages used in different domains that may belong to any Cloud\nComputing or Distributed Systems. This wealth of frameworks and languages make\nthe management task and the interpretation of the security policies so\ndifficult. Each approach provides its own conflict management method or tool,\nthe security expert will be forced to manage all these tools, which makes the\nfield maintenance and time consuming expensive. In order to hide this\ncomplexity and to facilitate some security experts tasks and automate the\nothers, we propose a security policies aligning based on ontologies process;\nthis process enables to detect and resolve security policies conflicts and to\nsupport security experts in managing tasks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 10:40:23 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Benammar", "Othman", ""], ["Elasri", "Hicham", ""], ["Sekkaki", "Abderrahim", ""]]}, {"id": "1303.7137", "submitter": "Maksims Fiosins", "authors": "A. Andronov and M. Fioshin", "title": "Discrete Optimization of Statistical Sample Sizes in Simulation by Using\n  the Hierarchical Bootstrap Method", "comments": "9 pages", "journal-ref": "proceedings of the 6th Tartu Conference, 1999", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bootstrap method application in simulation supposes that value of random\nvariables are not generated during the simulation process but extracted from\navailable sample populations. In the case of Hierarchical Bootstrap the\nfunction of interest is calculated recurrently using the calculation tree. In\nthe present paper we consider the optimization of sample sizes in each vertex\nof the calculation tree. The dynamic programming method is used for this aim.\nProposed method allows to decrease a variance of system characteristic\nestimators.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 14:48:44 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Andronov", "A.", ""], ["Fioshin", "M.", ""]]}, {"id": "1303.7200", "submitter": "Chrisantha Fernando Dr", "authors": "Chrisantha Fernando", "title": "Design for a Darwinian Brain: Part 1. Philosophy and Neuroscience", "comments": "Darwinian Neurodynamics. Submitted as a two part paper to Living\n  Machines 2013 Natural History Museum, London", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Physical symbol systems are needed for open-ended cognition. A good way to\nunderstand physical symbol systems is by comparison of thought to chemistry.\nBoth have systematicity, productivity and compositionality. The state of the\nart in cognitive architectures for open-ended cognition is critically assessed.\nI conclude that a cognitive architecture that evolves symbol structures in the\nbrain is a promising candidate to explain open-ended cognition. Part 2 of the\npaper presents such a cognitive architecture.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 18:45:52 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Fernando", "Chrisantha", ""]]}, {"id": "1303.7201", "submitter": "Chrisantha Fernando Dr", "authors": "Chrisantha Fernando, Vera Vasas", "title": "Design for a Darwinian Brain: Part 2. Cognitive Architecture", "comments": "Submitted as Part 2 to Living Machines 2013, Natural History Museum,\n  London. Code available on github as it is being developed to implement the\n  cognitive architecture above, here...\n  https://github.com/ctf20/DarwinianNeurodynamics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The accumulation of adaptations in an open-ended manner during lifetime\nlearning is a holy grail in reinforcement learning, intrinsic motivation,\nartificial curiosity, and developmental robotics. We present a specification\nfor a cognitive architecture that is capable of specifying an unlimited range\nof behaviors. We then give examples of how it can stochastically explore an\ninteresting space of adjacent possible behaviors. There are two main novelties;\nthe first is a proper definition of the fitness of self-generated games such\nthat interesting games are expected to evolve. The second is a modular and\nevolvable behavior language that has systematicity, productivity, and\ncompositionality, i.e. it is a physical symbol system. A part of the\narchitecture has already been implemented on a humanoid robot.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 18:47:32 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Fernando", "Chrisantha", ""], ["Vasas", "Vera", ""]]}, {"id": "1303.7327", "submitter": "EPTCS", "authors": "Carlos Areces (FaMAF - Universidad Nacional de C\\'ordoba, CONICET),\n  Guillaume Hoffmann (FaMAF - Universidad Nacional de C\\'ordoba), Ezequiel Orbe\n  (FaMAF - Universidad Nacional de C\\'ordoba, CONICET)", "title": "Symmetries in Modal Logics", "comments": "In Proceedings LSFA 2012, arXiv:1303.7136", "journal-ref": "EPTCS 113, 2013, pp. 27-44", "doi": "10.4204/EPTCS.113.6", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the notion of symmetries of propositional formulas in\nconjunctive normal form to modal formulas. Our framework uses the coinductive\nmodels and, hence, the results apply to a wide class of modal logics including,\nfor example, hybrid logics. Our main result shows that the symmetries of a\nmodal formula preserve entailment.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 09:01:44 GMT"}], "update_date": "2013-04-01", "authors_parsed": [["Areces", "Carlos", "", "FaMAF - Universidad Nacional de C\u00f3rdoba, CONICET"], ["Hoffmann", "Guillaume", "", "FaMAF - Universidad Nacional de C\u00f3rdoba"], ["Orbe", "Ezequiel", "", "FaMAF - Universidad Nacional de C\u00f3rdoba, CONICET"]]}, {"id": "1303.7335", "submitter": "EPTCS", "authors": "Ana Cristina Rocha Oliveira (Universidade de Brasilia), Mauricio\n  Ayala-Rinc\\'on (Universidade de Brasilia)", "title": "Formalizing the Confluence of Orthogonal Rewriting Systems", "comments": "In Proceedings LSFA 2012, arXiv:1303.7136", "journal-ref": "EPTCS 113, 2013, pp. 145-152", "doi": "10.4204/EPTCS.113.14", "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonality is a discipline of programming that in a syntactic manner\nguarantees determinism of functional specifications. Essentially, orthogonality\navoids, on the one side, the inherent ambiguity of non determinism, prohibiting\nthe existence of different rules that specify the same function and that may\napply simultaneously (non-ambiguity), and, on the other side, it eliminates the\npossibility of occurrence of repetitions of variables in the left-hand side of\nthese rules (left linearity). In the theory of term rewriting systems (TRSs)\ndeterminism is captured by the well-known property of confluence, that\nbasically states that whenever different computations or simplifications from a\nterm are possible, the computed answers should coincide. Although the proofs\nare technically elaborated, confluence is well-known to be a consequence of\northogonality. Thus, orthogonality is an important mathematical discipline\nintrinsic to the specification of recursive functions that is naturally applied\nin functional programming and specification. Starting from a formalization of\nthe theory of TRSs in the proof assistant PVS, this work describes how\nconfluence of orthogonal TRSs has been formalized, based on axiomatizations of\nproperties of rules, positions and substitutions involved in parallel steps of\nreduction, in this proof assistant. Proofs for some similar but restricted\nproperties such as the property of confluence of non-ambiguous and (left and\nright) linear TRSs have been fully formalized.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 09:02:42 GMT"}], "update_date": "2013-04-01", "authors_parsed": [["Oliveira", "Ana Cristina Rocha", "", "Universidade de Brasilia"], ["Ayala-Rinc\u00f3n", "Mauricio", "", "Universidade de Brasilia"]]}, {"id": "1303.7430", "submitter": "Giorgio Stefanoni", "authors": "Giorgio Stefanoni, Boris Motik, Ian Horrocks", "title": "Introducing Nominals to the Combined Query Answering Approaches for EL", "comments": "Extended version of a paper to appear on AAAI-13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So-called combined approaches answer a conjunctive query over a description\nlogic ontology in three steps: first, they materialise certain consequences of\nthe ontology and the data; second, they evaluate the query over the data; and\nthird, they filter the result of the second phase to eliminate unsound answers.\nSuch approaches were developed for various members of the DL-Lite and the EL\nfamilies of languages, but none of them can handle ontologies containing\nnominals. In our work, we bridge this gap and present a combined query\nanswering approach for ELHO---a logic that contains all features of the OWL 2\nEL standard apart from transitive roles and complex role inclusions. This\nextension is nontrivial because nominals require equality reasoning, which\nintroduces complexity into the first and the third step. Our empirical\nevaluation suggests that our technique is suitable for practical application,\nand so it provides a practical basis for conjunctive query answering in a large\nfragment of OWL 2 EL.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 16:07:12 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2013 17:08:32 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Stefanoni", "Giorgio", ""], ["Motik", "Boris", ""], ["Horrocks", "Ian", ""]]}, {"id": "1303.7445", "submitter": "Saad Khan", "authors": "Saad Ahmad Khan, Ladislau Boloni", "title": "Agent-based modeling of a price information trading business", "comments": "Extended version of the paper published at Computer and Information\n  Sciences, Proc. of ISCIS-26, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an agent-based simulation of a fictional (but feasible)\ninformation trading business. The Gas Price Information Trader (GPIT) buys\ninformation about real-time gas prices in a metropolitan area from drivers and\nresells the information to drivers who need to refuel their vehicles.\n  Our simulation uses real world geographic data, lifestyle-dependent driving\npatterns and vehicle models to create an agent-based model of the drivers. We\nuse real world statistics of gas price fluctuation to create scenarios of\ntemporal and spatial distribution of gas prices. The price of the information\nis determined on a case-by-case basis through a simple negotiation model. The\ntrader and the customers are adapting their negotiation strategies based on\ntheir historical profits.\n  We are interested in the general properties of the emerging information\nmarket: the amount of realizable profit and its distribution between the trader\nand customers, the business strategies necessary to keep the market operational\n(such as promotional deals), the price elasticity of demand and the impact of\npricing strategies on the profit.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 17:40:04 GMT"}], "update_date": "2013-04-01", "authors_parsed": [["Khan", "Saad Ahmad", ""], ["Boloni", "Ladislau", ""]]}]