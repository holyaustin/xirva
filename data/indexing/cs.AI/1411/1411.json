[{"id": "1411.0149", "submitter": "Aleksandrs Slivkins", "authors": "Ittai Abraham, Omar Alonso, Vasilis Kandylas, Rajesh Patel, Steven\n  Shelford, Aleksandrs Slivkins", "title": "How Many Workers to Ask? Adaptive Exploration for Collecting High\n  Quality Labels", "comments": "SIGIR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has been part of the IR toolbox as a cheap and fast mechanism\nto obtain labels for system development and evaluation. Successful deployment\nof crowdsourcing at scale involves adjusting many variables, a very important\none being the number of workers needed per human intelligence task (HIT). We\nconsider the crowdsourcing task of learning the answer to simple\nmultiple-choice HITs, which are representative of many relevance experiments.\nIn order to provide statistically significant results, one often needs to ask\nmultiple workers to answer the same HIT. A stopping rule is an algorithm that,\ngiven a HIT, decides for any given set of worker answers if the system should\nstop and output an answer or iterate and ask one more worker. Knowing the\nhistoric performance of a worker in the form of a quality score can be\nbeneficial in such a scenario. In this paper we investigate how to devise\nbetter stopping rules given such quality scores. We also suggest adaptive\nexploration as a promising approach for scalable and automatic creation of\nground truth. We conduct a data analysis on an industrial crowdsourcing\nplatform, and use the observations from this analysis to design new stopping\nrules that use the workers' quality scores in a non-trivial manner. We then\nperform a simulation based on a real-world workload, showing that our algorithm\nperforms better than the more naive approaches.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 18:28:49 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 01:52:19 GMT"}, {"version": "v3", "created": "Thu, 19 May 2016 19:11:46 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Abraham", "Ittai", ""], ["Alonso", "Omar", ""], ["Kandylas", "Vasilis", ""], ["Patel", "Rajesh", ""], ["Shelford", "Steven", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1411.0156", "submitter": "Subbarao Kambhampati", "authors": "William Cushing, J. Benton, Patrick Eyerich, Subbarao Kambhampati", "title": "Surrogate Search As a Way to Combat Harmful Effects of Ill-behaved\n  Evaluation Functions", "comments": "arXiv admin note: substantial text overlap with arXiv:1103.3687", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several researchers have found that cost-based satisficing search\nwith A* often runs into problems. Although some \"work arounds\" have been\nproposed to ameliorate the problem, there has been little concerted effort to\npinpoint its origin. In this paper, we argue that the origins of this problem\ncan be traced back to the fact that most planners that try to optimize cost\nalso use cost-based evaluation functions (i.e., f(n) is a cost estimate). We\nshow that cost-based evaluation functions become ill-behaved whenever there is\na wide variance in action costs; something that is all too common in planning\ndomains. The general solution to this malady is what we call a surrogatesearch,\nwhere a surrogate evaluation function that doesn't directly track the cost\nobjective, and is resistant to cost-variance, is used. We will discuss some\ncompelling choices for surrogate evaluation functions that are based on size\nrather that cost. Of particular practical interest is a cost-sensitive version\nof size-based evaluation function -- where the heuristic estimates the size of\ncheap paths, as it provides attractive quality vs. speed tradeoffs\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 19:04:17 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Cushing", "William", ""], ["Benton", "J.", ""], ["Eyerich", "Patrick", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1411.0264", "submitter": "Igor Razgon", "authors": "Igor Razgon", "title": "On the read-once property of branching programs and CNFs of bounded\n  treewidth", "comments": "Significantly simplified proof of the main combinatorial lemma.\n  AROSRN replaced back by NROBP due to their equivalence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we prove a space lower bound of $n^{\\Omega(k)}$ for\nnon-deterministic (syntactic) read-once branching programs ({\\sc nrobp}s) on\nfunctions expressible as {\\sc cnf}s with treewidth at most $k$ of their primal\ngraphs. This lower bound rules out the possibility of fixed-parameter space\ncomplexity of {\\sc nrobp}s parameterized by $k$.\n  We use lower bound for {\\sc nrobp}s to obtain a quasi-polynomial separation\nbetween Free Binary Decision Diagrams and Decision Decomposable Negation Normal\nForms, essentially matching the existing upper bound introduced by Beame et al.\nand thus proving the tightness of the latter.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 14:51:29 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 11:49:31 GMT"}, {"version": "v3", "created": "Sun, 26 Jul 2015 10:25:59 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Razgon", "Igor", ""]]}, {"id": "1411.0359", "submitter": "Carleton Coffrin", "authors": "Carleton Coffrin, Dan Gordon, and Paul Scott", "title": "NESTA, The NICTA Energy System Test Case Archive", "comments": "This archive is discontinued", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the power systems research community has seen an explosion of\nwork applying operations research techniques to challenging power network\noptimization problems. Regardless of the application under consideration, all\nof these works rely on power system test cases for evaluation and validation.\nHowever, many of the well established power system test cases were developed as\nfar back as the 1960s with the aim of testing AC power flow algorithms. It is\nunclear if these power flow test cases are suitable for power system\noptimization studies. This report surveys all of the publicly available AC\ntransmission system test cases, to the best of our knowledge, and assess their\nsuitability for optimization tasks. It finds that many of the traditional test\ncases are missing key network operation constraints, such as line thermal\nlimits and generator capability curves. To incorporate these missing\nconstraints, data driven models are developed from a variety of publicly\navailable data sources. The resulting extended test cases form a compressive\narchive, NESTA, for the evaluation and validation of power system optimization\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 04:16:51 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 08:13:59 GMT"}, {"version": "v3", "created": "Mon, 11 May 2015 07:34:21 GMT"}, {"version": "v4", "created": "Fri, 24 Jun 2016 21:48:26 GMT"}, {"version": "v5", "created": "Thu, 11 Aug 2016 04:24:02 GMT"}, {"version": "v6", "created": "Tue, 3 Sep 2019 02:49:19 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Coffrin", "Carleton", ""], ["Gordon", "Dan", ""], ["Scott", "Paul", ""]]}, {"id": "1411.0406", "submitter": "Arjun Bhardwaj", "authors": "Arjun Bhardwaj and Sangeetha", "title": "GC-SROIQ(C) : Expressive Constraint Modelling and Grounded\n  Circumscription for SROIQ", "comments": "For an improved formulation of the problem, which addresses critical\n  shortcomings of this paper, please refer to the following : Extending SROIQ\n  with Constraint Networks and Grounded Circumscription, arXiv:1508.00116", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developments in semantic web technologies have promoted ontological encoding\nof knowledge from diverse domains. However, modelling many practical domains\nrequires more expressive representations schemes than what the standard\ndescription logics(DLs) support. We extend the DL SROIQ with constraint\nnetworks and grounded circumscription. Applications of constraint modelling\ninclude embedding ontologies with temporal or spatial information, while\ngrounded circumscription allows defeasible inference and closed world\nreasoning. This paper overcomes restrictions on existing constraint modelling\napproaches by introducing expressive constructs. Grounded circumscription\nallows concept and role minimization and is decidable for DL. We provide a\ngeneral and intuitive algorithm for the framework of grounded circumscription\nthat can be applied to a whole range of logics. We present the resulting logic:\nGC-SROIQ(C), and describe a tableau decision procedure for it.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 10:05:29 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 07:46:47 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2015 08:45:52 GMT"}, {"version": "v4", "created": "Mon, 3 Apr 2017 18:04:45 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Bhardwaj", "Arjun", ""], ["Sangeetha", "", ""]]}, {"id": "1411.0440", "submitter": "Joseph Corneli", "authors": "Joseph Corneli, Anna Jordanous, Christian Guckelsberger, Alison Pease,\n  Simon Colton", "title": "Modelling serendipity in a computational context", "comments": "68pp, submitted to New Generation Computing special issue on New\n  Directions in Computational Creativity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term serendipity describes a creative process that develops, in context,\nwith the active participation of a creative agent, but not entirely within that\nagent's control. While a system cannot be made to perform serendipitously on\ndemand, we argue that its $\\mathit{serendipity\\ potential}$ can be increased by\nmeans of a suitable system architecture and other design choices. We distil a\nunified description of serendipitous occurrences from historical theorisations\nof serendipity and creativity. This takes the form of a framework with six\nphases: $\\mathit{perception}$, $\\mathit{attention}$, $\\mathit{interest}$,\n$\\mathit{explanation}$, $\\mathit{bridge}$, and $\\mathit{valuation}$. We then\nuse this framework to organise a survey of literature in cognitive science,\nphilosophy, and computing, which yields practical definitions of the six\nphases, along with heuristics for implementation. We use the resulting model to\nevaluate the serendipity potential of four existing systems developed by\nothers, and two systems previously developed by two of the authors. Most\nexisting research that considers serendipity in a computing context deals with\nserendipity as a service; here we relate theories of serendipity to the\ndevelopment of autonomous systems and computational creativity practice. We\nargue that serendipity is not teleologically blind, and outline representative\ndirections for future applications of our model. We conclude that it is\nfeasible to equip computational systems with the potential for serendipity, and\nthat this could be beneficial in varied computational creativity/AI\napplications, particularly those designed to operate responsively in real-world\ncontexts.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 11:50:19 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 11:23:44 GMT"}, {"version": "v3", "created": "Sun, 14 Feb 2016 17:47:29 GMT"}, {"version": "v4", "created": "Wed, 27 Jul 2016 13:19:32 GMT"}, {"version": "v5", "created": "Tue, 16 May 2017 11:56:12 GMT"}, {"version": "v6", "created": "Thu, 6 Dec 2018 16:12:42 GMT"}, {"version": "v7", "created": "Fri, 30 Aug 2019 09:47:39 GMT"}, {"version": "v8", "created": "Sun, 19 Apr 2020 19:58:37 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Corneli", "Joseph", ""], ["Jordanous", "Anna", ""], ["Guckelsberger", "Christian", ""], ["Pease", "Alison", ""], ["Colton", "Simon", ""]]}, {"id": "1411.0541", "submitter": "Baharan Mirzasoleiman", "authors": "Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause", "title": "Distributed Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale machine learning problems--clustering, non-parametric\nlearning, kernel machines, etc.--require selecting a small yet representative\nsubset from a large dataset. Such problems can often be reduced to maximizing a\nsubmodular set function subject to various constraints. Classical approaches to\nsubmodular optimization require centralized access to the full dataset, which\nis impractical for truly large-scale problems. In this paper, we consider the\nproblem of submodular function maximization in a distributed fashion. We\ndevelop a simple, two-stage protocol GreeDi, that is easily implemented using\nMapReduce style computations. We theoretically analyze our approach, and show\nthat under certain natural conditions, performance close to the centralized\napproach can be achieved. We begin with monotone submodular maximization\nsubject to a cardinality constraint, and then extend this approach to obtain\napproximation guarantees for (not necessarily monotone) submodular maximization\nsubject to more general constraints including matroid or knapsack constraints.\nIn our extensive experiments, we demonstrate the effectiveness of our approach\non several applications, including sparse Gaussian process inference and\nexemplar based clustering on tens of millions of examples using Hadoop.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 16:03:05 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 16:32:35 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Mirzasoleiman", "Baharan", ""], ["Karbasi", "Amin", ""], ["Sarkar", "Rik", ""], ["Krause", "Andreas", ""]]}, {"id": "1411.0659", "submitter": "Dmitry Chistikov", "authors": "Dmitry Chistikov, Rayna Dimitrova, Rupak Majumdar", "title": "Approximate Counting in SMT and Value Estimation for Probabilistic\n  Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  #SMT, or model counting for logical theories, is a well-known hard problem\nthat generalizes such tasks as counting the number of satisfying assignments to\na Boolean formula and computing the volume of a polytope. In the realm of\nsatisfiability modulo theories (SMT) there is a growing need for model counting\nsolvers, coming from several application domains (quantitative information\nflow, static analysis of probabilistic programs). In this paper, we show a\nreduction from an approximate version of #SMT to SMT.\n  We focus on the theories of integer arithmetic and linear real arithmetic. We\npropose model counting algorithms that provide approximate solutions with\nformal bounds on the approximation error. They run in polynomial time and make\na polynomial number of queries to the SMT solver for the underlying theory,\nexploiting \"for free\" the sophisticated heuristics implemented within modern\nSMT solvers. We have implemented the algorithms and used them to solve the\nvalue problem for a model of loop-free probabilistic programs with\nnondeterminism.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 20:59:19 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 15:44:52 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Chistikov", "Dmitry", ""], ["Dimitrova", "Rayna", ""], ["Majumdar", "Rupak", ""]]}, {"id": "1411.0895", "submitter": "Liang Lu", "authors": "Liang Lu and Steve Renals", "title": "Tied Probabilistic Linear Discriminant Analysis for Speech Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2014.2313410", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic models using probabilistic linear discriminant analysis (PLDA)\ncapture the correlations within feature vectors using subspaces which do not\nvastly expand the model. This allows high dimensional and correlated feature\nspaces to be used, without requiring the estimation of multiple high dimension\ncovariance matrices. In this letter we extend the recently presented PLDA\nmixture model for speech recognition through a tied PLDA approach, which is\nbetter able to control the model size to avoid overfitting. We carried out\nexperiments using the Switchboard corpus, with both mel frequency cepstral\ncoefficient features and bottleneck feature derived from a deep neural network.\nReductions in word error rate were obtained by using tied PLDA, compared with\nthe PLDA mixture model, subspace Gaussian mixture models, and deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 13:11:06 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Lu", "Liang", ""], ["Renals", "Steve", ""]]}, {"id": "1411.0967", "submitter": "Raka Jovanovic", "authors": "Raka Jovanovic, Milan Tuba, Stefan Voss", "title": "A Multi-Heuristic Approach for Solving the Pre-Marshalling Problem", "comments": null, "journal-ref": null, "doi": "10.1007/s10100-015-0410-y", "report-no": null, "categories": "cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the number of reshuffling operations at maritime container\nterminals incorporates the Pre-Marshalling Problem (PMP) as an important\nproblem. Based on an analysis of existing solution approaches we develop new\nheuristics utilizing specific properties of problem instances of the PMP. We\nshow that the heuristic performance is highly dependent on these properties. We\nintroduce a new method that exploits a greedy heuristic of four stages, where\nfor each of these stages several different heuristics may be applied. Instead\nof using randomization to improve the performance of the heuristic, we\nrepetitively generate a number of solutions by using a combination of different\nheuristics for each stage. In doing so, only a small number of solutions is\ngenerated for which we intend that they do not have undesirable properties,\ncontrary to the case when simple randomization is used. Our experiments show\nthat such a deterministic algorithm significantly outperforms the original\nnondeterministic method when the quality of found solutions is observed, with a\nmuch lower number of generated solutions.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 08:24:24 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Jovanovic", "Raka", ""], ["Tuba", "Milan", ""], ["Voss", "Stefan", ""]]}, {"id": "1411.1080", "submitter": "Raka Jovanovic", "authors": "Raka Jovanovic, Abdelkader Bousselham, Stefan Voss", "title": "A Heuristic Method for Solving the Problem of Partitioning Graphs with\n  Supply and Demand", "comments": null, "journal-ref": null, "doi": "10.1007/s10479-015-1930-5", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a greedy algorithm for solving the problem of the\nmaximum partitioning of graphs with supply and demand (MPGSD). The goal of the\nmethod is to solve the MPGSD for large graphs in a reasonable time limit. This\nis done by using a two stage greedy algorithm, with two corresponding types of\nheuristics. The solutions acquired in this way are improved by applying a\ncomputationally inexpensive, hill climbing like, greedy correction procedure.\nIn our numeric experiments we analyze different heuristic functions for each\nstage of the greedy algorithm, and show that their performance is highly\ndependent on the properties of the specific instance. Our tests show that by\nexploring a relatively small number of solutions generated by combining\ndifferent heuristic functions, and applying the proposed correction procedure\nwe can find solutions within only a few percent of the optimal ones.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 08:29:59 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Jovanovic", "Raka", ""], ["Bousselham", "Abdelkader", ""], ["Voss", "Stefan", ""]]}, {"id": "1411.1112", "submitter": "Yu Zhang", "authors": "Yu Zhang and Subbarao Kambhampati", "title": "Learning of Agent Capability Models with Applications in Multi-agent\n  Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important challenge for a set of agents to achieve more efficient\ncollaboration is for these agents to maintain proper models of each other. An\nimportant aspect of these models of other agents is that they are often partial\nand incomplete. Thus far, there are two common representations of agent models:\nMDP based and action based, which are both based on action modeling. In many\napplications, agent models may not have been given, and hence must be learnt.\nWhile it may seem convenient to use either MDP based or action based models for\nlearning, in this paper, we introduce a new representation based on capability\nmodels, which has several unique advantages. First, we show that learning\ncapability models can be performed efficiently online via Bayesian learning,\nand the learning process is robust to high degrees of incompleteness in plan\nexecution traces (e.g., with only start and end states). While high degrees of\nincompleteness in plan execution traces presents learning challenges for MDP\nbased and action based models, capability models can still learn to {\\em\nabstract} useful information out of these traces. As a result, capability\nmodels are useful in applications in which such incompleteness is common, e.g.,\nrobot learning human model from observations and interactions. Furthermore,\nwhen used in multi-agent planning (with each agent modeled separately),\ncapability models provide flexible abstraction of actions. The limitation,\nhowever, is that the synthesized plan is incomplete and abstract.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 23:54:07 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Zhang", "Yu", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1411.1170", "submitter": "Ong Sing Goh", "authors": "Ong Sing Goh, Lance Fung", "title": "An Intelligent Personal Robot Assistant", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error", "journal-ref": null, "doi": null, "report-no": "TJ211.G63 2004", "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent development in developing humanoid robot poses new challenges to\nhuman-machine interaction communication. A major challenge is to develop robots\nthat can behave like and interact with human in the most natural way possible.\nThis paper proposes a system to develop a robot that can receive command, and\ntalk to people in natural language. In addition, the robot can also be\n\"trained\" to become an expert in sepcific areas to provide expert advice to\nhuman-beings. Most important of all, the robot can display emotions through\nfacial expression, speech and gesture so that the interaction process will\nbecome more comprehensive and compelling.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 07:24:59 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 01:13:31 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Goh", "Ong Sing", ""], ["Fung", "Lance", ""]]}, {"id": "1411.1373", "submitter": "Bill Hibbard", "authors": "Bill Hibbard", "title": "Ethical Artificial Intelligence", "comments": "minor edit: remove page break between Figure 10.2 and its caption", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This book-length article combines several peer reviewed papers and new\nmaterial to analyze the issues of ethical artificial intelligence (AI). The\nbehavior of future AI systems can be described by mathematical equations, which\nare adapted to analyze possible unintended AI behaviors and ways that AI\ndesigns can avoid them. This article makes the case for utility-maximizing\nagents and for avoiding infinite sets in agent definitions. It shows how to\navoid agent self-delusion using model-based utility functions and how to avoid\nagents that corrupt their reward generators (sometimes called \"perverse\ninstantiation\") using utility functions that evaluate outcomes at one point in\ntime from the perspective of humans at a different point in time. It argues\nthat agents can avoid unintended instrumental actions (sometimes called \"basic\nAI drives\" or \"instrumental goals\") by accurately learning human values. This\narticle defines a self-modeling agent framework and shows how it can avoid\nproblems of resource limits, being predicted by other agents, and inconsistency\nbetween the agent's utility function and its definition (one version of this\nproblem is sometimes called \"motivated value selection\"). This article also\ndiscusses how future AI will differ from current AI, the politics of AI, and\nthe ultimate use of AI to help understand the nature of the universe and our\nplace in it.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 19:40:02 GMT"}, {"version": "v2", "created": "Wed, 12 Nov 2014 19:11:41 GMT"}, {"version": "v3", "created": "Thu, 20 Nov 2014 18:37:22 GMT"}, {"version": "v4", "created": "Thu, 4 Dec 2014 10:22:11 GMT"}, {"version": "v5", "created": "Wed, 24 Dec 2014 18:45:16 GMT"}, {"version": "v6", "created": "Mon, 19 Jan 2015 13:15:45 GMT"}, {"version": "v7", "created": "Wed, 4 Feb 2015 11:49:39 GMT"}, {"version": "v8", "created": "Thu, 5 Mar 2015 17:49:32 GMT"}, {"version": "v9", "created": "Tue, 17 Nov 2015 20:54:38 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Hibbard", "Bill", ""]]}, {"id": "1411.1497", "submitter": "Xiaoyu Chen", "authors": "Xiaoyu Chen, Dongming Wang", "title": "The Spaces of Data, Information, and Knowledge", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the data space $D$ of any given data set $X$ and explain how\nfunctions and relations are defined over $D$. From $D$ and for a specific\ndomain $\\Delta$ we construct the information space $I$ of $X$ by interpreting\nvariables, functions, and explicit relations over $D$ in $\\Delta$ and by\nincluding other relations that $D$ implies under the interpretation in\n$\\Delta$. Then from $I$ we build up the knowledge space $K$ of $X$ as the\nproduct of two spaces $K_T$ and $K_P$, where $K_T$ is obtained from $I$ by\nusing the induction principle to generalize propositional relations to\nquantified relations, the deduction principle to generate new relations, and\nstandard mechanisms to validate relations and $K_P$ is the space of\nspecifications of methods with operational instructions which are valid in\n$K_T$. Through our construction of the three topological spaces the following\nkey observation is made clear: the retrieval of information from the given data\nset for $\\Delta$ consists essentially in mining domain objects and relations,\nand the discovery of knowledge from the retrieved information consists\nessentially in applying the induction and deduction principles to generate\npropositions, synthesizing and modeling the information to generate\nspecifications of methods with operational instructions, and validating the\npropositions and specifications. Based on this observation, efficient\napproaches may be designed to discover profound knowledge automatically from\nsimple data, as demonstrated by the result of our study in the case of\ngeometry.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 04:50:45 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Chen", "Xiaoyu", ""], ["Wang", "Dongming", ""]]}, {"id": "1411.1507", "submitter": "Daisuke Ishii", "authors": "Daisuke Ishii, Kazuki Yoshizoe, Toyotaro Suzumura", "title": "Scalable Parallel Numerical CSP Solver", "comments": "The final publication is available at Springer", "journal-ref": null, "doi": "10.1007/978-3-319-10428-7_30", "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel solver for numerical constraint satisfaction problems\n(NCSPs) that can scale on a number of cores. Our proposed method runs worker\nsolvers on the available cores and simultaneously the workers cooperate for the\nsearch space distribution and balancing. In the experiments, we attained up to\n119-fold speedup using 256 cores of a parallel computer.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 06:33:17 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Ishii", "Daisuke", ""], ["Yoshizoe", "Kazuki", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1411.1629", "submitter": "Ernest Davis", "authors": "Ernest Davis", "title": "The Limitations of Standardized Science Tests as Benchmarks for\n  Artificial Intelligence Research: Position Paper", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this position paper, I argue that standardized tests for elementary\nscience such as SAT or Regents tests are not very good benchmarks for measuring\nthe progress of artificial intelligence systems in understanding basic science.\nThe primary problem is that these tests are designed to test aspects of\nknowledge and ability that are challenging for people; the aspects that are\nchallenging for AI systems are very different. In particular, standardized\ntests do not test knowledge that is obvious for people; none of this knowledge\ncan be assumed in AI systems. Individual standardized tests also have specific\nfeatures that are not necessarily appropriate for an AI benchmark. I analyze\nthe Physics subject SAT in some detail and the New York State Regents Science\ntest more briefly. I also argue that the apparent advantages offered by using\nstandardized tests are mostly either minor or illusory. The one major real\nadvantage is that the significance is easily explained to the public; but I\nargue that even this is a somewhat mixed blessing. I conclude by arguing that,\nfirst, more appropriate collections of exam style problems could be assembled,\nand second, that there are better kinds of benchmarks than exam-style problems.\nIn an appendix I present a collection of sample exam-style problems that test\nkinds of knowledge missing from the standardized tests.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 14:44:12 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2015 20:17:31 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Davis", "Ernest", ""]]}, {"id": "1411.1752", "submitter": "Adarsh Prasad", "authors": "Adarsh Prasad, Stefanie Jegelka and Dhruv Batra", "title": "Submodular meets Structured: Finding Diverse Subsets in\n  Exponentially-Large Structured Item Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the high level of ambiguity faced in domains such as Computer\nVision or Natural Language processing, robust prediction methods often search\nfor a diverse set of high-quality candidate solutions or proposals. In\nstructured prediction problems, this becomes a daunting task, as the solution\nspace (image labelings, sentence parses, etc.) is exponentially large. We study\ngreedy algorithms for finding a diverse subset of solutions in\nstructured-output spaces by drawing new connections between submodular\nfunctions over combinatorial item sets and High-Order Potentials (HOPs) studied\nfor graphical models. Specifically, we show via examples that when marginal\ngains of submodular diversity functions allow structured representations, this\nenables efficient (sub-linear time) approximate maximization by reducing the\ngreedy augmentation step to inference in a factor graph with appropriately\nconstructed HOPs. We discuss benefits, tradeoffs, and show that our\nconstructions lead to significantly better proposals.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 20:07:37 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Prasad", "Adarsh", ""], ["Jegelka", "Stefanie", ""], ["Batra", "Dhruv", ""]]}, {"id": "1411.1784", "submitter": "Mehdi Mirza", "authors": "Mehdi Mirza, Simon Osindero", "title": "Conditional Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Nets [8] were recently introduced as a novel way to\ntrain generative models. In this work we introduce the conditional version of\ngenerative adversarial nets, which can be constructed by simply feeding the\ndata, y, we wish to condition on to both the generator and discriminator. We\nshow that this model can generate MNIST digits conditioned on class labels. We\nalso illustrate how this model could be used to learn a multi-modal model, and\nprovide preliminary examples of an application to image tagging in which we\ndemonstrate how this approach can generate descriptive tags which are not part\nof training labels.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 22:33:22 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Mirza", "Mehdi", ""], ["Osindero", "Simon", ""]]}, {"id": "1411.1953", "submitter": "Leroy Cronin Prof", "authors": "Juan Manuel Parrilla Gutierrez, Trevor Hinkley, James Taylor, Kliment\n  Yanev, Leroy Cronin", "title": "Hardware and Software manual for Evolution of Oil Droplets in a\n  Chemo-Robotic Platform", "comments": "42 pages, 25 figures, list of printed parts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This manual outlines a fully automated liquid handling robot to enable\nphysically-embodied evolution within a chemical oil-droplet system. The robot\nis based upon the REPRAP3D printer system and makes the droplets by mixing\nchemicals and then placing them in a petri dish after which they are recorded\nusing a camera and the behaviour of the droplets analysed using image\nrecognition software. This manual accompanies the open access publication\npublished in Nature Communications DOI: 10.1038/ncomms6571.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 15:41:34 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Gutierrez", "Juan Manuel Parrilla", ""], ["Hinkley", "Trevor", ""], ["Taylor", "James", ""], ["Yanev", "Kliment", ""], ["Cronin", "Leroy", ""]]}, {"id": "1411.1999", "submitter": "Hossam   Ishkewy", "authors": "Hossam Ishkewy, Hany Harb and Hassan Farahat", "title": "Azhary: An Arabic Lexical Ontology", "comments": "appears in International Journal of Web & Semantic Technology\n  (IJWesT) Vol.5, No.4, October 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arabic language is the most spoken languages in the Semitic languages group,\nand one of the most common languages in the world spoken by more than 422\nmillion. It is also of paramount importance to Muslims, it is a sacred language\nof the Islamic Holly Book (Quran) and prayer (and other acts of worship) in\nIslam is performed only by mastering some of Arabic words. Arabic is also a\nmajor ritual language of a number of Christian churches in the Arab world and\nit is also used in writing several intellectual and religious Jewish books in\nthe Middle Ages. Despite this, there is no semantic Arabic lexicon which\nresearchers can depend on. In this paper we introduce Azhary as a lexical\nontology for the Arabic language. It groups Arabic words into sets of synonyms\ncalled synsets, and records a number of relationships between words such as\nsynonym, antonym, hypernym, hyponym, meronym, holonym and association\nrelations. The ontology contains 26,195 words organized in 13,328 synsets. It\nhas been developed and contrasted against AWN which is the most common\navailable Arabic lexical ontology.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 18:23:00 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Ishkewy", "Hossam", ""], ["Harb", "Hany", ""], ["Farahat", "Hassan", ""]]}, {"id": "1411.2186", "submitter": "Lianli Gao", "authors": "Lianli Gao and Michael Bruenig and Jane Hunter", "title": "Estimating Fire Weather Indices via Semantic Reasoning over Wireless\n  Sensor Network Data Streams", "comments": "20pages, 12 figures", "journal-ref": "Estimating Fire Weather Indices via Semantic Reasoning of Wireless\n  Sensor Network Streams, International Journal of Web and Semantic Technology,\n  Vol.5, pp.1-20 (2014)", "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wildfires are frequent, devastating events in Australia that regularly cause\nsignificant loss of life and widespread property damage. Fire weather indices\nare a widely-adopted method for measuring fire danger and they play a\nsignificant role in issuing bushfire warnings and in anticipating demand for\nbushfire management resources. Existing systems that calculate fire weather\nindices are limited due to low spatial and temporal resolution. Localized\nwireless sensor networks, on the other hand, gather continuous sensor data\nmeasuring variables such as air temperature, relative humidity, rainfall and\nwind speed at high resolutions. However, using wireless sensor networks to\nestimate fire weather indices is a challenge due to data quality issues, lack\nof standard data formats and lack of agreement on thresholds and methods for\ncalculating fire weather indices. Within the scope of this paper, we propose a\nstandardized approach to calculating Fire Weather Indices (a.k.a. fire danger\nratings) and overcome a number of the challenges by applying Semantic Web\nTechnologies to the processing of data streams from a wireless sensor network\ndeployed in the Springbrook region of South East Queensland. This paper\ndescribes the underlying ontologies, the semantic reasoning and the Semantic\nFire Weather Index (SFWI) system that we have developed to enable domain\nexperts to specify and adapt rules for calculating Fire Weather Indices. We\nalso describe the Web-based mapping interface that we have developed, that\nenables users to improve their understanding of how fire weather indices vary\nover time within a particular region.Finally, we discuss our evaluation results\nthat indicate that the proposed system outperforms state-of-the-art techniques\nin terms of accuracy, precision and query performance.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2014 02:37:52 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Gao", "Lianli", ""], ["Bruenig", "Michael", ""], ["Hunter", "Jane", ""]]}, {"id": "1411.2328", "submitter": "Xun Wang", "authors": "Xun Wang", "title": "Modeling Word Relatedness in Latent Dirichlet Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard LDA model suffers the problem that the topic assignment of each word\nis independent and word correlation hence is neglected. To address this\nproblem, in this paper, we propose a model called Word Related Latent Dirichlet\nAllocation (WR-LDA) by incorporating word correlation into LDA topic models.\nThis leads to new capabilities that standard LDA model does not have such as\nestimating infrequently occurring words or multi-language topic modeling.\nExperimental results demonstrate the effectiveness of our model compared with\nstandard LDA.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 05:24:41 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Wang", "Xun", ""]]}, {"id": "1411.2374", "submitter": "Aur\\'elien Bellet", "authors": "Kuan Liu and Aur\\'elien Bellet and Fei Sha", "title": "Similarity Learning for High-Dimensional Sparse Data", "comments": "14 pages. Proceedings of the 18th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2015). Matlab code:\n  https://github.com/bellet/HDSL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good measure of similarity between data points is crucial to many tasks in\nmachine learning. Similarity and metric learning methods learn such measures\nautomatically from data, but they do not scale well respect to the\ndimensionality of the data. In this paper, we propose a method that can learn\nefficiently similarity measure from high-dimensional sparse data. The core idea\nis to parameterize the similarity measure as a convex combination of rank-one\nmatrices with specific sparsity structures. The parameters are then optimized\nwith an approximate Frank-Wolfe procedure to maximally satisfy relative\nsimilarity constraints on the training data. Our algorithm greedily\nincorporates one pair of features at a time into the similarity measure,\nproviding an efficient way to control the number of active features and thus\nreduce overfitting. It enjoys very appealing convergence guarantees and its\ntime and memory complexity depends on the sparsity of the data instead of the\ndimension of the feature space. Our experiments on real-world high-dimensional\ndatasets demonstrate its potential for classification, dimensionality reduction\nand data exploration.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 10:40:47 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 13:45:00 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 16:53:40 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Kuan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Sha", "Fei", ""]]}, {"id": "1411.2499", "submitter": "Radhakrishnan Delhibabu", "authors": "Radhakrishnan Delhibabu", "title": "Comparative Study of View Update Algorithms in Rational Choice Theory", "comments": "http://link.springer.com/article/10.1007/s10489-014-0580-7. arXiv\n  admin note: substantial text overlap with arXiv:1407.3512, arXiv:1301.5154", "journal-ref": null, "doi": "10.1007/s10489-014-0580-7", "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics of belief and knowledge is one of the major components of any\nautonomous system that should be able to incorporate new pieces of information.\nWe show that knowledge base dynamics has interesting connection with kernel\nchange via hitting set and abduction. The approach extends and integrates\nstandard techniques for efficient query answering and integrity checking. The\ngeneration of hitting set is carried out through a hyper tableaux calculus and\nmagic set that is focused on the goal of minimality. Many different view update\nalgorithms have been proposed in the literature to address this problem. The\npresent paper provides a comparative study of view update algorithms in\nrational approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 16:43:24 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Delhibabu", "Radhakrishnan", ""]]}, {"id": "1411.2516", "submitter": "Giorgio Stefanoni", "authors": "Giorgio Stefanoni and Boris Motik", "title": "Answering Conjunctive Queries over $\\mathcal{EL}$ Knowledge Bases with\n  Transitive and Reflexive Roles", "comments": "Extended version of a paper to appear on AAAI-15. In this version of\n  the report, we fixed a few typos; all the results are unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering conjunctive queries (CQs) over $\\mathcal{EL}$ knowledge bases (KBs)\nwith complex role inclusions is PSPACE-hard and in PSPACE in certain cases;\nhowever, if complex role inclusions are restricted to role transitivity, the\ntight upper complexity bound has so far been unknown. Furthermore, the existing\nalgorithms cannot handle reflexive roles, and they are not practicable.\nFinally, the problem is tractable for acyclic CQs and $\\mathcal{ELH}$, and\nNP-complete for unrestricted CQs and $\\mathcal{ELHO}$ KBs. In this paper we\ncomplete the complexity landscape of CQ answering for several important cases.\nIn particular, we present a practicable NP algorithm for answering CQs over\n$\\mathcal{ELHO}^s$ KBs---a logic containing all of OWL 2 EL, but with complex\nrole inclusions restricted to role transitivity. Our preliminary evaluation\nsuggests that the algorithm can be suitable for practical use. Moreover, we\nshow that, even for a restricted class of so-called arborescent acyclic\nqueries, CQ answering over $\\mathcal{EL}$ KBs becomes NP-hard in the presence\nof either transitive or reflexive roles. Finally, we show that answering\narborescent CQs over $\\mathcal{ELHO}$ KBs is tractable, whereas answering\nacyclic CQs is NP-hard.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 17:50:06 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 15:25:06 GMT"}, {"version": "v3", "created": "Wed, 13 May 2015 15:30:39 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Stefanoni", "Giorgio", ""], ["Motik", "Boris", ""]]}, {"id": "1411.2636", "submitter": "Philip Dawid", "authors": "A. P. Dawid, R. Murtas and M. Musio", "title": "Bounding the Probability of Causation in Mediation Analysis", "comments": "9 pages, 1 figure, 3 tables", "journal-ref": "In Topics on Methodological and Applied Statistical Inference,\n  edited by T. Di Battista, E. Moreno and W. Racugno. Springer (2016), 75-84", "doi": null, "report-no": null, "categories": "math.ST cs.AI stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given empirical evidence for the dependence of an outcome variable on an\nexposure variable, we can typically only provide bounds for the \"probability of\ncausation\" in the case of an individual who has developed the outcome after\nbeing exposed. We show how these bounds can be adapted or improved if further\ninformation becomes available. In addition to reviewing existing work on this\ntopic, we provide a new analysis for the case where a mediating variable can be\nobserved. In particular we show how the probability of causation can be bounded\nwhen there is no direct effect and no confounding.\n  Keywords: Causal inference, Mediation Analysis, Probability of Causation\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 21:48:56 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dawid", "A. P.", ""], ["Murtas", "R.", ""], ["Musio", "M.", ""]]}, {"id": "1411.2679", "submitter": "Jiwei Li", "authors": "Jiwei Li, Alan Ritter and Dan Jurafsky", "title": "Inferring User Preferences by Probabilistic Logical Reasoning over\n  Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for inferring the latent attitudes or preferences of\nusers by performing probabilistic first-order logical reasoning over the social\nnetwork graph. Our method answers questions about Twitter users like {\\em Does\nthis user like sushi?} or {\\em Is this user a New York Knicks fan?} by building\na probabilistic model that reasons over user attributes (the user's location or\ngender) and the social network (the user's friends and spouse), via inferences\nlike homophily (I am more likely to like sushi if spouse or friends like sushi,\nI am more likely to like the Knicks if I live in New York). The algorithm uses\ndistant supervision, semi-supervised data harvesting and vector space models to\nextract user attributes (e.g. spouse, education, location) and preferences\n(likes and dislikes) from text. The extracted propositions are then fed into a\nprobabilistic reasoner (we investigate both Markov Logic and Probabilistic Soft\nLogic). Our experiments show that probabilistic logical reasoning significantly\nimproves the performance on attribute and relation extraction, and also\nachieves an F-score of 0.791 at predicting a users likes or dislikes,\nsignificantly better than two strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 01:53:21 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Li", "Jiwei", ""], ["Ritter", "Alan", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1411.2800", "submitter": "Federico Cerutti", "authors": "Federico Cerutti, Ilias Tachmazidis, Mauro Vallati, Sotirios Batsakis,\n  Massimiliano Giacomin, Grigoris Antoniou", "title": "Exploiting Parallelism for Hard Problems in Abstract Argumentation:\n  Technical Report", "comments": "Technical report of an accepted AAAI-2015 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract argumentation framework (\\AFname) is a unifying framework able to\nencompass a variety of nonmonotonic reasoning approaches, logic programming and\ncomputational argumentation. Yet, efficient approaches for most of the decision\nand enumeration problems associated to \\AFname s are missing, thus potentially\nlimiting the efficacy of argumentation-based approaches in real domains. In\nthis paper, we present an algorithm for enumerating the preferred extensions of\nabstract argumentation frameworks which exploits parallel computation. To this\npurpose, the SCC-recursive semantics definition schema is adopted, where\nextensions are defined at the level of specific sub-frameworks. The algorithm\nshows significant performance improvements in large frameworks, in terms of\nnumber of solutions found and speedup.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 13:33:33 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 14:31:12 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Cerutti", "Federico", ""], ["Tachmazidis", "Ilias", ""], ["Vallati", "Mauro", ""], ["Batsakis", "Sotirios", ""], ["Giacomin", "Massimiliano", ""], ["Antoniou", "Grigoris", ""]]}, {"id": "1411.2842", "submitter": "Martin Ziegler", "authors": "Matthias Englert and Sandra Siebert and Martin Ziegler", "title": "Logical Limitations to Machine Ethics with Consequences to Lethal\n  Autonomous Weapons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lethal Autonomous Weapons promise to revolutionize warfare -- and raise a\nmultitude of ethical and legal questions. It has thus been suggested to program\nvalues and principles of conduct (such as the Geneva Conventions) into the\nmachines' control, thereby rendering them both physically and morally superior\nto human combatants.\n  We employ mathematical logic and theoretical computer science to explore\nfundamental limitations to the moral behaviour of intelligent machines in a\nseries of \"Gedankenexperiments\": Refining and sharpening variants of the\nTrolley Problem leads us to construct an (admittedly artificial but) fully\ndeterministic situation where a robot is presented with two choices: one\nmorally clearly preferable over the other -- yet, based on the undecidability\nof the Halting problem, it provably cannot decide algorithmically which one.\nOur considerations have surprising implications to the question of\nresponsibility and liability for an autonomous system's actions and lead to\nspecific technical recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 15:05:01 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Englert", "Matthias", ""], ["Siebert", "Sandra", ""], ["Ziegler", "Martin", ""]]}, {"id": "1411.3197", "submitter": "Karamjit Singh", "authors": "Karamjit Singh, Puneet Agarwal, Gautam Shroff", "title": "Warranty Cost Estimation Using Bayesian Network", "comments": "Selected for publication in Poster Proceedings of \"Industrial\n  Conference on Data Mining (ICDM 2014)\"\n  http://www.data-mining-forum.de/files/Program%202014%20DIN%20Lang.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All multi-component product manufacturing companies face the problem of\nwarranty cost estimation. Failure rate analysis of components plays a key role\nin this problem. Data source used for failure rate analysis has traditionally\nbeen past failure data of components. However, failure rate analysis can be\nimproved by means of fusion of additional information, such as symptoms\nobserved during after-sale service of the product, geographical information\n(hilly or plains areas), and information from tele-diagnostic analytics. In\nthis paper, we propose an approach, which learns dependency between\npart-failures and symptoms gleaned from such diverse sources of information, to\npredict expected number of failures with better accuracy. We also indicate how\nthe optimum warranty period can be computed. We demonstrate, through empirical\nresults, that our method can improve the warranty cost estimates significantly.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 08:23:55 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Singh", "Karamjit", ""], ["Agarwal", "Puneet", ""], ["Shroff", "Gautam", ""]]}, {"id": "1411.3320", "submitter": "Luis Ortiz", "authors": "Luis E. Ortiz", "title": "On Sparse Discretization for Graphical Games", "comments": "30 pages. Original research note drafted in Dec. 2002 and posted\n  online Spring'03 (http://www.cis.upenn.\n  edu/~mkearns/teaching/cgt/revised_approx_bnd.pdf) as part of a course on\n  computational game theory taught by Prof. Michael Kearns at the University of\n  Pennsylvania; First major revision sent to WINE'10; Current version sent to\n  JAIR on April 25, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper concerns discretization schemes for representing and\ncomputing approximate Nash equilibria, with emphasis on graphical games, but\nbriefly touching on normal-form and poly-matrix games. The main technical\ncontribution is a representation theorem that informally states that to account\nfor every exact Nash equilibrium using a nearby approximate Nash equilibrium on\na grid over mixed strategies, a uniform discretization size linear on the\ninverse of the approximation quality and natural game-representation parameters\nsuffices. For graphical games, under natural conditions, the discretization is\nlogarithmic in the game-representation size, a substantial improvement over the\nlinear dependency previously required. The paper has five other objectives: (1)\ngiven the venue, to highlight the important, but often ignored, role that work\non constraint networks in AI has in simplifying the derivation and analysis of\nalgorithms for computing approximate Nash equilibria; (2) to summarize the\nstate-of-the-art on computing approximate Nash equilibria, with emphasis on\nrelevance to graphical games; (3) to help clarify the distinction between\nsparse-discretization and sparse-support techniques; (4) to illustrate and\nadvocate for the deliberate mathematical simplicity of the formal proof of the\nrepresentation theorem; and (5) to list and discuss important open problems,\nemphasizing graphical-game generalizations, which the AI community is most\nsuitable to solve.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 20:59:07 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Ortiz", "Luis E.", ""]]}, {"id": "1411.3346", "submitter": "Olegs Verhodubs", "authors": "Olegs Verhodubs", "title": "Membership Function Assignment for Elements of Single OWL Ontology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops the idea of membership function assignment for OWL (Web\nOntology Language) ontology elements in order to subsequently generate fuzzy\nrules from this ontology. The task of membership function assignment for OWL\nontology elements had already been partially described, but this concerned the\ncase, when several OWL ontologies of the same domain were available, and they\nwere merged into a single ontology. The purpose of this paper is to present the\nway of membership function assignment for OWL ontology elements in the case,\nwhen there is the only one available ontology. Fuzzy rules, generated from the\nOWL ontology, are necessary for supplement of the SWES (Semantic Web Expert\nSystem) knowledge base. SWES is an expert system, which will be able to extract\nknowledge from OWL ontologies, found in the Web, and will serve as a universal\nexpert for the user.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 21:13:08 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Verhodubs", "Olegs", ""]]}, {"id": "1411.3622", "submitter": "Robert Piro", "authors": "Boris Motik, Yavor Nenov, Robert Piro, Ian Horrocks", "title": "Handling owl:sameAs via Rewriting", "comments": "This is the technical report supporting the AAAI 2015 Conference\n  submission with the same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rewriting is widely used to optimise owl:sameAs reasoning in materialisation\nbased OWL 2 RL systems. We investigate issues related to both the correctness\nand efficiency of rewriting, and present an algorithm that guarantees\ncorrectness, improves efficiency, and can be effectively parallelised. Our\nevaluation shows that our approach can reduce reasoning times on practical data\nsets by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 17:30:03 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Motik", "Boris", ""], ["Nenov", "Yavor", ""], ["Piro", "Robert", ""], ["Horrocks", "Ian", ""]]}, {"id": "1411.3675", "submitter": "Linhong Zhu", "authors": "Linhong Zhu, Dong Guo, Junming Yin, Greg Ver Steeg, Aram Galstyan", "title": "Scalable Link Prediction in Dynamic Networks via Non-Negative Matrix\n  Factorization", "comments": "Technical report for paper \"Scalable Temporal Latent Space Inference\n  for Link Prediction in Dynamic Social Networks\" that appears in IEEE\n  Transactions on Knowledge and Data Engineering 2016", "journal-ref": null, "doi": "10.1109/TKDE.2016.2591009", "report-no": null, "categories": "cs.SI cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable temporal latent space model for link prediction in\ndynamic social networks, where the goal is to predict links over time based on\na sequence of previous graph snapshots. The model assumes that each user lies\nin an unobserved latent space and interactions are more likely to form between\nsimilar users in the latent space representation. In addition, the model allows\neach user to gradually move its position in the latent space as the network\nstructure evolves over time. We present a global optimization algorithm to\neffectively infer the temporal latent space, with a quadratic convergence rate.\nTwo alternative optimization algorithms with local and incremental updates are\nalso proposed, allowing the model to scale to larger networks without\ncompromising prediction accuracy. Empirically, we demonstrate that our model,\nwhen evaluated on a number of real-world dynamic networks, significantly\noutperforms existing approaches for temporal link prediction in terms of both\nscalability and predictive power.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 19:36:54 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 00:03:01 GMT"}, {"version": "v3", "created": "Sat, 23 Jul 2016 22:37:22 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Zhu", "Linhong", ""], ["Guo", "Dong", ""], ["Yin", "Junming", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1411.3792", "submitter": "EPTCS", "authors": "Natalia Garanina (A.P. Ershov Institute of Informatics Systems,\n  Novosibirsk, Russia), Eugene Bodin (A.P. Ershov Institute of Informatics\n  Systems, Novosibirsk, Russia), Elena Sidorova (A.P. Ershov Institute of\n  Informatics Systems, Novosibirsk, Russia)", "title": "An Approach to Model Checking of Multi-agent Data Analysis", "comments": "In Proceedings MOD* 2014, arXiv:1411.3453", "journal-ref": "EPTCS 168, 2014, pp. 32-44", "doi": "10.4204/EPTCS.168.3", "report-no": null, "categories": "cs.AI cs.MA cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an approach to verification of a multi-agent data analysis\nalgorithm. We base correct simulation of the multi-agent system by a finite\ninteger model. For verification we use model checking tool SPIN. Protocols of\nagents are written in Promela language and properties of the multi-agent data\nanalysis system are expressed in logic LTL. We run several experiments with\nSPIN and the model.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 04:39:04 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Garanina", "Natalia", "", "A.P. Ershov Institute of Informatics Systems,\n  Novosibirsk, Russia"], ["Bodin", "Eugene", "", "A.P. Ershov Institute of Informatics\n  Systems, Novosibirsk, Russia"], ["Sidorova", "Elena", "", "A.P. Ershov Institute of\n  Informatics Systems, Novosibirsk, Russia"]]}, {"id": "1411.3806", "submitter": "Sandhya Bansal", "authors": "Sandhya Bansal, V. Katiyar", "title": "Integrating Fuzzy and Ant Colony System for Fuzzy Vehicle Routing\n  Problem with Time Windows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper fuzzy VRPTW with an uncertain travel time is considered.\nCredibility theory is used to model the problem and specifies a preference\nindex at which it is desired that the travel times to reach the customers fall\ninto their time windows. We propose the integration of fuzzy and ant colony\nsystem based evolutionary algorithm to solve the problem while preserving the\nconstraints. Computational results for certain benchmark problems having short\nand long time horizons are presented to show the effectiveness of the\nalgorithm. Comparison between different preferences indexes have been obtained\nto help the user in making suitable decisions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 06:11:48 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Bansal", "Sandhya", ""], ["Katiyar", "V.", ""]]}, {"id": "1411.3880", "submitter": "Martin Chmel\\'ik", "authors": "Krishnendu Chatterjee, Martin Chmel\\'ik, Raghav Gupta, Ayush Kanodia", "title": "Optimal Cost Almost-sure Reachability in POMDPs", "comments": "Full Version of Optimal Cost Almost-sure Reachability in POMDPs, AAAI\n  2015. arXiv admin note: text overlap with arXiv:1207.4166 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider partially observable Markov decision processes (POMDPs) with a\nset of target states and every transition is associated with an integer cost.\nThe optimization objective we study asks to minimize the expected total cost\ntill the target set is reached, while ensuring that the target set is reached\nalmost-surely (with probability 1). We show that for integer costs\napproximating the optimal cost is undecidable. For positive costs, our results\nare as follows: (i) we establish matching lower and upper bounds for the\noptimal cost and the bound is double exponential; (ii) we show that the problem\nof approximating the optimal cost is decidable and present approximation\nalgorithms developing on the existing algorithms for POMDPs with finite-horizon\nobjectives. While the worst-case running time of our algorithm is double\nexponential, we also present efficient stopping criteria for the algorithm and\nshow experimentally that it performs well in many examples of interest.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 12:13:45 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Chmel\u00edk", "Martin", ""], ["Gupta", "Raghav", ""], ["Kanodia", "Ayush", ""]]}, {"id": "1411.3895", "submitter": "Ismael Rodr\\'iguez-Fdez M.Sc", "authors": "I. Rodr\\'iguez-Fdez, M. Mucientes, A. Bugar\\'in", "title": "Learning Fuzzy Controllers in Mobile Robotics with Embedded\n  Preprocessing", "comments": null, "journal-ref": "Applied Soft Computing, vol 26, pp. 123-142, 2015", "doi": "10.1016/j.asoc.2014.09.021", "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic design of controllers for mobile robots usually requires two\nstages. In the first stage,sensorial data are preprocessed or transformed into\nhigh level and meaningful values of variables whichare usually defined from\nexpert knowledge. In the second stage, a machine learning technique is applied\ntoobtain a controller that maps these high level variables to the control\ncommands that are actually sent tothe robot. This paper describes an algorithm\nthat is able to embed the preprocessing stage into the learningstage in order\nto get controllers directly starting from sensorial raw data with no expert\nknowledgeinvolved. Due to the high dimensionality of the sensorial data, this\napproach uses Quantified Fuzzy Rules(QFRs), that are able to transform\nlow-level input variables into high-level input variables, reducingthe\ndimensionality through summarization. The proposed learning algorithm, called\nIterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic\nprogramming. IQFRL is able to learn rules with differentstructures, and can\nmanage linguistic variables with multiple granularities. The algorithm has been\ntestedwith the implementation of the wall-following behavior both in several\nrealistic simulated environmentswith different complexity and on a Pioneer 3-AT\nrobot in two real environments. Results have beencompared with several\nwell-known learning algorithms combined with different data\npreprocessingtechniques, showing that IQFRL exhibits a better and statistically\nsignificant performance. Moreover,three real world applications for which IQFRL\nplays a central role are also presented: path and objecttracking with static\nand moving obstacles avoidance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 13:11:32 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Rodr\u00edguez-Fdez", "I.", ""], ["Mucientes", "M.", ""], ["Bugar\u00edn", "A.", ""]]}, {"id": "1411.4000", "submitter": "Fei Sha", "authors": "Zhiyun Lu and Avner May and Kuan Liu and Alireza Bagheri Garakani and\n  Dong Guo and Aur\\'elien Bellet and Linxi Fan and Michael Collins and Brian\n  Kingsbury and Michael Picheny and Fei Sha", "title": "How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of kernel methods has often been a major barrier\nfor applying them to large-scale learning problems. We argue that this barrier\ncan be effectively overcome. In particular, we develop methods to scale up\nkernel models to successfully tackle large-scale learning problems that are so\nfar only approachable by deep learning architectures. Based on the seminal work\nby Rahimi and Recht on approximating kernel functions with features derived\nfrom random projections, we advance the state-of-the-art by proposing methods\nthat can efficiently train models with hundreds of millions of parameters, and\nlearn optimal representations from multiple kernels. We conduct extensive\nempirical studies on problems from image recognition and automatic speech\nrecognition, and show that the performance of our kernel models matches that of\nwell-engineered deep neural nets (DNNs). To the best of our knowledge, this is\nthe first time that a direct comparison between these two methods on\nlarge-scale problems is reported. Our kernel methods have several appealing\nproperties: training with convex optimization, cost for training a single model\ncomparable to DNNs, and significantly reduced total cost due to fewer\nhyperparameters to tune for model selection. Our contrastive study between\nthese two very different but equally competitive models sheds light on\nfundamental questions such as how to learn good representations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 18:24:20 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 10:31:35 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Lu", "Zhiyun", ""], ["May", "Avner", ""], ["Liu", "Kuan", ""], ["Garakani", "Alireza Bagheri", ""], ["Guo", "Dong", ""], ["Bellet", "Aur\u00e9lien", ""], ["Fan", "Linxi", ""], ["Collins", "Michael", ""], ["Kingsbury", "Brian", ""], ["Picheny", "Michael", ""], ["Sha", "Fei", ""]]}, {"id": "1411.4023", "submitter": "Umair Z Ahmed", "authors": "Umair Z. Ahmed, Krishnendu Chatterjee, Sumit Gulwani", "title": "Automatic Generation of Alternative Starting Positions for Simple\n  Traditional Board Games", "comments": "A conference version of the paper will appear in AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple board games, like Tic-Tac-Toe and CONNECT-4, play an important role\nnot only in the development of mathematical and logical skills, but also in the\nemotional and social development. In this paper, we address the problem of\ngenerating targeted starting positions for such games. This can facilitate new\napproaches for bringing novice players to mastery, and also leads to discovery\nof interesting game variants. We present an approach that generates starting\nstates of varying hardness levels for player~$1$ in a two-player board game,\ngiven rules of the board game, the desired number of steps required for\nplayer~$1$ to win, and the expertise levels of the two players. Our approach\nleverages symbolic methods and iterative simulation to efficiently search the\nextremely large state space. We present experimental results that include\ndiscovery of states of varying hardness levels for several simple grid-based\nboard games. The presence of such states for standard game variants like $4\n\\times 4$ Tic-Tac-Toe opens up new games to be played that have never been\nplayed as the default start state is heavily biased.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 19:43:12 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Ahmed", "Umair Z.", ""], ["Chatterjee", "Krishnendu", ""], ["Gulwani", "Sumit", ""]]}, {"id": "1411.4109", "submitter": "Glenn Hofford", "authors": "Glenn R. Hofford", "title": "Resolution of Difficult Pronouns Using the ROSS Method", "comments": "106 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new natural language understanding method for disambiguation of difficult\npronouns is described. Difficult pronouns are those pronouns for which a level\nof world or domain knowledge is needed in order to perform anaphoral or other\ntypes of resolution. Resolution of difficult pronouns may in some cases require\na prior step involving the application of inference to a situation that is\nrepresented by the natural language text. A general method is described: it\nperforms entity resolution and pronoun resolution. An extension to the general\npronoun resolution method performs inference as an embedded commonsense\nreasoning method. The general method and the embedded method utilize features\nof the ROSS representational scheme; in particular the methods use ROSS\nontology classes and the ROSS situation model. The overall method is a working\nsolution that solves the following Winograd schemas: a) trophy and suitcase, b)\nperson lifts person, c) person pays detective, and d) councilmen and\ndemonstrators.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 03:43:01 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Hofford", "Glenn R.", ""]]}, {"id": "1411.4156", "submitter": "Peter Patel-Schneider", "authors": "Peter F. Patel-Schneider", "title": "Using Description Logics for RDF Constraint Checking and Closed-World\n  Recognition", "comments": "Extended version of a paper of the same name that will appear in\n  AAAI-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDF and Description Logics work in an open-world setting where absence of\ninformation is not information about absence. Nevertheless, Description Logic\naxioms can be interpreted in a closed-world setting and in this setting they\ncan be used for both constraint checking and closed-world recognition against\ninformation sources. When the information sources are expressed in well-behaved\nRDF or RDFS (i.e., RDF graphs interpreted in the RDF or RDFS semantics) this\nconstraint checking and closed-world recognition is simple to describe. Further\nthis constraint checking can be implemented as SPARQL querying and thus\neffectively performed.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 15:33:38 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 21:09:56 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Patel-Schneider", "Peter F.", ""]]}, {"id": "1411.4192", "submitter": "Glenn Hofford", "authors": "Glenn R. Hofford", "title": "Introduction to ROSS: A New Representational Scheme", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ROSS (\"Representation, Ontology, Structure, Star\") is introduced as a new\nmethod for knowledge representation that emphasizes representational constructs\nfor physical structure. The ROSS representational scheme includes a language\ncalled \"Star\" for the specification of ontology classes. The ROSS method also\nincludes a formal scheme called the \"instance model\". Instance models are used\nin the area of natural language meaning representation to represent situations.\nThis paper provides both the rationale and the philosophical background for the\nROSS method.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 22:31:05 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Hofford", "Glenn R.", ""]]}, {"id": "1411.4194", "submitter": "Glenn Hofford", "authors": "Glenn R. Hofford", "title": "ROSS User's Guide and Reference Manual (Version 1.0)", "comments": "128 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ROSS method is a new approach in the area of knowledge representation\nthat is useful for many artificial intelligence and natural language\nunderstanding representation and reasoning tasks. (ROSS stands for\n\"Representation\", \"Ontology\", \"Structure\", \"Star\" language). ROSS is a physical\nsymbol-based representational scheme. ROSS provides a complex model for the\ndeclarative representation of physical structure and for the representation of\nprocesses and causality. From the metaphysical perspective, the ROSS view of\nexternal reality involves a 4D model, wherein discrete single-time-point\nunit-sized locations with states are the basis for all objects, processes and\naspects that can be modeled. ROSS includes a language called \"Star\" for the\nspecification of ontology classes. The ROSS method also includes a formal\nscheme called the \"instance model\". Instance models are used in the area of\nnatural language meaning representation to represent situations. This document\nis an in-depth specification of the ROSS method.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 22:47:35 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Hofford", "Glenn R.", ""]]}, {"id": "1411.4342", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, Larry\n  Wasserman, James M. Robins", "title": "Influence Functions for Machine Learning: Nonparametric Estimators for\n  Entropies, Divergences and Mutual Informations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze estimators for statistical functionals of one or more\ndistributions under nonparametric assumptions. Our estimators are based on the\ntheory of influence functions, which appear in the semiparametric statistics\nliterature. We show that estimators based either on data-splitting or a\nleave-one-out technique enjoy fast rates of convergence and other favorable\ntheoretical properties. We apply this framework to derive estimators for\nseveral popular information theoretic quantities, and via empirical evaluation,\nshow the advantage of this approach over existing estimators.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 02:04:57 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 18:47:41 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 23:29:07 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Krishnamurthy", "Akshay", ""], ["Poczos", "Barnabas", ""], ["Wasserman", "Larry", ""], ["Robins", "James M.", ""]]}, {"id": "1411.4379", "submitter": "Md Lisul Islam", "authors": "Md. Lisul Islam, Novia Nurain, Swakkhar Shatabda and M Sohel Rahman", "title": "FGPGA: An Efficient Genetic Approach for Producing Feasible Graph\n  Partitions", "comments": "Accepted in the 1st International Conference on Networking Systems\n  and Security 2015 (NSysS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph partitioning, a well studied problem of parallel computing has many\napplications in diversified fields such as distributed computing, social\nnetwork analysis, data mining and many other domains. In this paper, we\nintroduce FGPGA, an efficient genetic approach for producing feasible graph\npartitions. Our method takes into account the heterogeneity and capacity\nconstraints of the partitions to ensure balanced partitioning. Such approach\nhas various applications in mobile cloud computing that include feasible\ndeployment of software applications on the more resourceful infrastructure in\nthe cloud instead of mobile hand set. Our proposed approach is light weight and\nhence suitable for use in cloud architecture. We ensure feasibility of the\npartitions generated by not allowing over-sized partitions to be generated\nduring the initialization and search. Our proposed method tested on standard\nbenchmark datasets significantly outperforms the state-of-the-art methods in\nterms of quality of partitions and feasibility of the solutions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 06:51:50 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Islam", "Md. Lisul", ""], ["Nurain", "Novia", ""], ["Shatabda", "Swakkhar", ""], ["Rahman", "M Sohel", ""]]}, {"id": "1411.4516", "submitter": "Marco Montali", "authors": "Diego Calvanese and Giorgio Delzanno and Marco Montali", "title": "Verification of Relational Multiagent Systems with Data Types (Extended\n  Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the extension of relational multiagent systems (RMASs), where agents\nmanipulate full-fledged relational databases, with data types and facets\nequipped with domain-specific, rigid relations (such as total orders).\nSpecifically, we focus on design-time verification of RMASs against rich\nfirst-order temporal properties expressed in a variant of first-order\nmu-calculus with quantification across states. We build on previous\ndecidability results under the \"state-bounded\" assumption, i.e., in each single\nstate only a bounded number of data objects is stored in the agent databases,\nwhile unboundedly many can be encountered over time. We recast this condition,\nshowing decidability in presence of dense, linear orders, and facets defined on\ntop of them. Our approach is based on the construction of a finite-state, sound\nand complete abstraction of the original system, in which dense linear orders\nare reformulated as non-rigid relations working on the active domain of the\nsystem only. We also show undecidability when including a data type equipped\nwith the successor relation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 15:49:35 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Calvanese", "Diego", ""], ["Delzanno", "Giorgio", ""], ["Montali", "Marco", ""]]}, {"id": "1411.4616", "submitter": "Antoni Lig\\k{e}za", "authors": "Antoni Lig\\k{e}za", "title": "A Note on Systematic Conflict Generation in CA-EN-type Causal Structures", "comments": "This report is available form LAAS - Toulouse, France, from 1996.\n  Report No.: 96317 http://www.laas.fr/pulman/pulman-isens/web/app.php/", "journal-ref": null, "doi": null, "report-no": "LAAS Report No. 96317, 22 pp. (1996)", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is aimed at providing a very first, more \"global\", systematic\npoint of view with respect to possible conflict generation in CA-EN-like causal\nstructures. For simplicity, only the outermost level of graphs is taken into\naccount. Localization of the \"conflict area\", diagnostic preferences, and bases\nfor systematic conflict generation are considered. A notion of {\\em Potential\nConflict Structure} ({\\em PCS}) constituting a basic tool for identification of\npossible conflicts is proposed and its use is discussed.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 20:07:45 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Lig\u0119za", "Antoni", ""]]}, {"id": "1411.4618", "submitter": "Christopher Burges", "authors": "Christopher J.C. Burges, Erin Renshaw, and Andrzej Pastusiak", "title": "Relations World: A Possibilistic Graphical Model", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the idea of using a \"possibilistic graphical model\" as the basis\nfor a world model that drives a dialog system. As a first step we have\ndeveloped a system that uses text-based dialog to derive a model of the user's\nfamily relations. The system leverages its world model to infer relational\ntriples, to learn to recover from upstream coreference resolution errors and\nambiguities, and to learn context-dependent paraphrase models. We also explore\nsome theoretical aspects of the underlying graphical model.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 20:15:00 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Burges", "Christopher J. C.", ""], ["Renshaw", "Erin", ""], ["Pastusiak", "Andrzej", ""]]}, {"id": "1411.4619", "submitter": "Alexandros A. Voudouris", "authors": "Ioannis Caragiannis, George A. Krimpas, Alexandros A. Voudouris", "title": "Aggregating partial rankings with applications to peer grading in\n  massive online open courses", "comments": "16 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the potential of using ordinal peer grading for the evaluation\nof students in massive online open courses (MOOCs). According to such grading\nschemes, each student receives a few assignments (by other students) which she\nhas to rank. Then, a global ranking (possibly translated into numerical scores)\nis produced by combining the individual ones. This is a novel application area\nfor social choice concepts and methods where the important problem to be solved\nis as follows: how should the assignments be distributed so that the collected\nindividual rankings can be easily merged into a global one that is as close as\npossible to the ranking that represents the relative performance of the\nstudents in the assignment? Our main theoretical result suggests that using\nvery simple ways to distribute the assignments so that each student has to rank\nonly $k$ of them, a Borda-like aggregation method can recover a $1-O(1/k)$\nfraction of the true ranking when each student correctly ranks the assignments\nshe receives. Experimental results strengthen our analysis further and also\ndemonstrate that the same method is extremely robust even when students have\nimperfect capabilities as graders. We believe that our results provide strong\nevidence that ordinal peer grading can be a highly effective and scalable\nsolution for evaluation in MOOCs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 20:16:38 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Caragiannis", "Ioannis", ""], ["Krimpas", "George A.", ""], ["Voudouris", "Alexandros A.", ""]]}, {"id": "1411.4702", "submitter": "Michael Ferrier", "authors": "Michael R. Ferrier", "title": "Toward a Universal Cortical Algorithm: Examining Hierarchical Temporal\n  Memory in Light of Frontal Cortical Function", "comments": "105 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of evidence points toward the existence of a common algorithm\nunderlying the processing of information throughout the cerebral cortex.\nSeveral hypothesized features of this cortical algorithm are reviewed,\nincluding sparse distributed representation, Bayesian inference, hierarchical\norganization composed of alternating template matching and pooling layers,\ntemporal slowness and predictive coding. Hierarchical Temporal Memory (HTM) is\na family of learning algorithms and corresponding theories of cortical function\nthat embodies these principles. HTM has previously been applied mainly to\nperceptual tasks typical of posterior cortex. In order to evaluate HTM as a\ncandidate model of cortical function, it is necessary also to investigate its\ncompatibility with the requirements of frontal cortical function. To this end,\na variety of models of frontal cortical function are reviewed and integrated,\nto arrive at the hypothesis that frontal functions including attention, working\nmemory and action selection depend largely upon the same basic algorithms as do\nposterior functions, with the notable additions of a mechanism for the active\nmaintenance of representations and of multiple cortico-striato-thalamo-cortical\nloops that allow communication between regions of frontal cortex to be gated in\nan adaptive manner. Computational models of this system are reviewed. Finally,\nthere is a discussion of how HTM can contribute to the understanding of frontal\ncortical function, and of what the requirements of frontal cortical function\nmean for the future development of HTM.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 00:38:30 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Ferrier", "Michael R.", ""]]}, {"id": "1411.4823", "submitter": "Claudia Schon", "authors": "Ulrich Furbach and Claudia Schon and Frieder Stolzenburg", "title": "Automated Reasoning in Deontic Logic", "comments": null, "journal-ref": "In M. Narasimha Murty, Xiangjian He, Raghavendra Rao Chillarige,\n  and Paul Weng, editors, Proc. of MIWAI 2014: Multi-disciplinary International\n  Workshop on Artificial Intelligence, LNAI 8875, pp. 57-68, Bangalore, India,\n  2014. Springer", "doi": "10.1007/978-3-319-13365-2_6", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deontic logic is a very well researched branch of mathematical logic and\nphilosophy. Various kinds of deontic logics are discussed for different\napplication domains like argumentation theory, legal reasoning, and acts in\nmulti-agent systems. In this paper, we show how standard deontic logic can be\nstepwise transformed into description logic and DL- clauses, such that it can\nbe processed by Hyper, a high performance theorem prover which uses a\nhypertableau calculus. Two use cases, one from multi-agent research and one\nfrom the development of normative system are investigated.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 12:27:01 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Furbach", "Ulrich", ""], ["Schon", "Claudia", ""], ["Stolzenburg", "Frieder", ""]]}, {"id": "1411.4825", "submitter": "Claudia Schon", "authors": "Ulrich Furbach and Claudia Schon and Frieder Stolzenburg", "title": "Cognitive Systems and Question Answering", "comments": null, "journal-ref": "Industrie 4.0 Management, 31(1):29-32, 2015", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper briefly characterizes the field of cognitive computing. As an\nexemplification, the field of natural language question answering is introduced\ntogether with its specific challenges. A possibility to master these challenges\nis illustrated by a detailed presentation of the LogAnswer system, which is a\nsuccessful representative of the field of natural language question answering.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 12:37:00 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Furbach", "Ulrich", ""], ["Schon", "Claudia", ""], ["Stolzenburg", "Frieder", ""]]}, {"id": "1411.4925", "submitter": "Alejandro Ramos Soto", "authors": "A. Ramos-Soto and A. Bugar\\'in and S. Barro and J. Taboada", "title": "Linguistic Descriptions for Automatic Generation of Textual Short-Term\n  Weather Forecasts on Real Prediction Data", "comments": "13 pages, 20 figures, IEEE Transactions on Fuzzy Systems, 2014", "journal-ref": null, "doi": "10.1109/TFUZZ.2014.2328011", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper an application which automatically generates textual\nshort-term weather forecasts for every municipality in Galicia (NW Spain),\nusing the real data provided by the Galician Meteorology Agency (MeteoGalicia).\nThis solution combines in an innovative way computing with perceptions\ntechniques and strategies for linguistic description of data together with a\nnatural language generation (NLG) system. The application, named GALiWeather,\nextracts relevant information from weather forecast input data and encodes it\ninto intermediate descriptions using linguistic variables and temporal\nreferences. These descriptions are later translated into natural language texts\nby the natural language generation system. The obtained forecast results have\nbeen thoroughly validated by an expert meteorologist from MeteoGalicia using a\nquality assessment methodology which covers two key dimensions of a text: the\naccuracy of its content and the correctness of its form. Following this\nvalidation GALiWeather will be released as a real service offering custom\nforecasts for a wide public.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 17:35:59 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Ramos-Soto", "A.", ""], ["Bugar\u00edn", "A.", ""], ["Barro", "S.", ""], ["Taboada", "J.", ""]]}, {"id": "1411.5007", "submitter": "Kevin Waugh", "authors": "Kevin Waugh and J. Andrew Bagnell", "title": "A Unified View of Large-scale Zero-sum Equilibrium Computation", "comments": "AAAI Workshop on Computer Poker and Imperfect Information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of computing approximate Nash equilibria in large zero-sum\nextensive-form games has received a tremendous amount of attention due mainly\nto the Annual Computer Poker Competition. Immediately after its inception, two\ncompeting and seemingly different approaches emerged---one an application of\nno-regret online learning, the other a sophisticated gradient method applied to\na convex-concave saddle-point formulation. Since then, both approaches have\ngrown in relative isolation with advancements on one side not effecting the\nother. In this paper, we rectify this by dissecting and, in a sense, unify the\ntwo views.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 20:43:39 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Waugh", "Kevin", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1411.5220", "submitter": "Heng Zhang", "authors": "Heng Zhang, Yan Zhang, Jia-Huai You", "title": "Existential Rule Languages with Finite Chase: Complexity and\n  Expressiveness", "comments": "Extended version of a paper to appear on AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite chase, or alternatively chase termination, is an important condition\nto ensure the decidability of existential rule languages. In the past few\nyears, a number of rule languages with finite chase have been studied. In this\nwork, we propose a novel approach for classifying the rule languages with\nfinite chase. Using this approach, a family of decidable rule languages, which\nextend the existing languages with the finite chase property, are naturally\ndefined. We then study the complexity of these languages. Although all of them\nare tractable for data complexity, we show that their combined complexity can\nbe arbitrarily high. Furthermore, we prove that all the rule languages with\nfinite chase that extend the weakly acyclic language are of the same\nexpressiveness as the weakly acyclic one, while rule languages with higher\ncombined complexity are in general more succinct than those with lower combined\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 13:37:22 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 05:36:49 GMT"}, {"version": "v3", "created": "Thu, 8 Jan 2015 22:53:11 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Zhang", "Heng", ""], ["Zhang", "Yan", ""], ["You", "Jia-Huai", ""]]}, {"id": "1411.5224", "submitter": "Dorian Aur", "authors": "Dorian Aur", "title": "Can we build a conscious machine?", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": "10.13140/2.1.2286.5608", "report-no": null, "categories": "cs.ET cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The underlying physiological mechanisms of generating conscious states are\nstill unknown. To make progress on the problem of consciousness, we will need\nto experimentally design a system that evolves in a similar way our brains do.\nRecent experimental data show that the multiscale nature of the evolving human\nbrain can be implemented by reprogramming human cells. A hybrid system can be\ndesigned to include an evolving brain equipped with digital computers that\nmaintain homeostasis and provide the right amount of nutrients and oxygen for\nthe brain growth. Shaping the structure of the evolving brain will be\nprogressively achieved by controlling spatial organization of various types of\ncells. Following a specific program, the evolving brain can be trained using\nsubstitutional reality to learn and experience live scenes. We already know\nfrom neuroelectrodynamics that meaningful information in the brain is\nelectrically (wirelessly) read out and written fast in neurons and synapses at\nthe molecular (protein) level during the generation of action potentials and\nsynaptic activities. Since with training, meaningful information accumulates\nand is electrically integrated in the brain, one can predict, that this gradual\nprocess of training will trigger a tipping point for conscious experience to\nemerge in the hybrid system.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 23:12:25 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Aur", "Dorian", ""]]}, {"id": "1411.5268", "submitter": "Alex James Dr", "authors": "Swathikiran Sudhakarana, Alex Pappachen James", "title": "Sparse distributed localized gradient fused features of objects", "comments": "Pages 13", "journal-ref": "Pattern Recognition, Available online 31 October 2014", "doi": "10.1016/j.patcog.2014.10.002", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The sparse, hierarchical, and modular processing of natural signals is\nrelated to the ability of humans to recognize objects with high accuracy. In\nthis study, we report a sparse feature processing and encoding method, which\nimproved the recognition performance of an automated object recognition system.\nRandomly distributed localized gradient enhanced features were selected before\nemploying aggregate functions for representation, where we used a modular and\nhierarchical approach to detect the object features. These object features were\ncombined with a minimum distance classifier, thereby obtaining object\nrecognition system accuracies of 93% using the Amsterdam library of object\nimages (ALOI) database, 92% using the Columbia object image library (COIL)-100\ndatabase, and 69% using the PASCAL visual object challenge 2007 database. The\nobject recognition performance was shown to be robust to variations in noise,\nobject scaling, and object shifts. Finally, a comparison with eight existing\nobject recognition methods indicated that our new method improved the\nrecognition accuracy by 10% with ALOI, 8% with the COIL-100 database, and 10%\nwith the PASCAL visual object challenge 2007 database.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 15:57:02 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Sudhakarana", "Swathikiran", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1411.5313", "submitter": "Mark Kaminski", "authors": "Ana Armas Romero, Mark Kaminski, Bernardo Cuenca Grau, Ian Horrocks", "title": "Ontology Module Extraction via Datalog Reasoning", "comments": "13 pages. To appear in AAAI-15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Module extraction - the task of computing a (preferably small) fragment M of\nan ontology T that preserves entailments over a signature S - has found many\napplications in recent years. Extracting modules of minimal size is, however,\ncomputationally hard, and often algorithmically infeasible. Thus, practical\ntechniques are based on approximations, where M provably captures the relevant\nentailments, but is not guaranteed to be minimal. Existing approximations,\nhowever, ensure that M preserves all second-order entailments of T w.r.t. S,\nwhich is stronger than is required in many applications, and may lead to large\nmodules in practice. In this paper we propose a novel approach in which module\nextraction is reduced to a reasoning problem in datalog. Our approach not only\ngeneralises existing approximations in an elegant way, but it can also be\ntailored to preserve only specific kinds of entailments, which allows us to\nextract significantly smaller modules. An evaluation on widely-used ontologies\nhas shown very encouraging results.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 18:43:28 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 17:57:49 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Romero", "Ana Armas", ""], ["Kaminski", "Mark", ""], ["Grau", "Bernardo Cuenca", ""], ["Horrocks", "Ian", ""]]}, {"id": "1411.5326", "submitter": "Joel Veness", "authors": "Joel Veness, Marc G. Bellemare, Marcus Hutter, Alvin Chua, Guillaume\n  Desjardins", "title": "Compress and Control", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new information-theoretic policy evaluation technique\nfor reinforcement learning. This technique converts any compression or density\nmodel into a corresponding estimate of value. Under appropriate stationarity\nand ergodicity conditions, we show that the use of a sufficiently powerful\nmodel gives rise to a consistent value function estimator. We also study the\nbehavior of this technique when applied to various Atari 2600 video games,\nwhere the use of suboptimal modeling techniques is unavoidable. We consider\nthree fundamentally different models, all too limited to perfectly model the\ndynamics of the system. Remarkably, we find that our technique provides\nsufficiently accurate value estimates for effective on-policy control. We\nconclude with a suggestive study highlighting the potential of our technique to\nscale to large problems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 19:32:45 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Veness", "Joel", ""], ["Bellemare", "Marc G.", ""], ["Hutter", "Marcus", ""], ["Chua", "Alvin", ""], ["Desjardins", "Guillaume", ""]]}, {"id": "1411.5328", "submitter": "Robinson Piramuthu Robinson Piramuthu", "authors": "Bolei Zhou, Vignesh Jagadeesh, Robinson Piramuthu", "title": "ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image\n  Collections", "comments": "9 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering visual knowledge from weakly labeled data is crucial to scale up\ncomputer vision recognition system, since it is expensive to obtain fully\nlabeled data for a large number of concept categories. In this paper, we\npropose ConceptLearner, which is a scalable approach to discover visual\nconcepts from weakly labeled image collections. Thousands of visual concept\ndetectors are learned automatically, without human in the loop for additional\nannotation. We show that these learned detectors could be applied to recognize\nconcepts at image-level and to detect concepts at image region-level\naccurately. Under domain-specific supervision, we further evaluate the learned\nconcepts for scene recognition on SUN database and for object detection on\nPascal VOC 2007. ConceptLearner shows promising performance compared to fully\nsupervised and weakly supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 19:35:39 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Zhou", "Bolei", ""], ["Jagadeesh", "Vignesh", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1411.5410", "submitter": "Rehan Abdul Aziz", "authors": "Rehan Abdul Aziz, Geoffrey Chu, Christian Muise, Peter Stuckey", "title": "Stable Model Counting and Its Application in Probabilistic Logic\n  Programming", "comments": "Accepted in AAAI, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model counting is the problem of computing the number of models that satisfy\na given propositional theory. It has recently been applied to solving inference\ntasks in probabilistic logic programming, where the goal is to compute the\nprobability of given queries being true provided a set of mutually independent\nrandom variables, a model (a logic program) and some evidence. The core of\nsolving this inference task involves translating the logic program to a\npropositional theory and using a model counter. In this paper, we show that for\nsome problems that involve inductive definitions like reachability in a graph,\nthe translation of logic programs to SAT can be expensive for the purpose of\nsolving inference tasks. For such problems, direct implementation of stable\nmodel semantics allows for more efficient solving. We present two\nimplementation techniques, based on unfounded set detection, that extend a\npropositional model counter to a stable model counter. Our experiments show\nthat for particular problems, our approach can outperform a state-of-the-art\nprobabilistic logic programming solver by several orders of magnitude in terms\nof running time and space requirements, and can solve instances of\nsignificantly larger sizes on which the current solver runs out of time or\nmemory.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 00:54:45 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Aziz", "Rehan Abdul", ""], ["Chu", "Geoffrey", ""], ["Muise", "Christian", ""], ["Stuckey", "Peter", ""]]}, {"id": "1411.5416", "submitter": "Marius Silaghi", "authors": "Marius C. Silaghi and Roussi Roussev", "title": "Recommending the Most Encompassing Opposing and Endorsing Arguments in\n  Debates", "comments": "10 pages. This report was reviewed by a committee within Florida Tech\n  during April 2014, and had been written in Summer 2013 by summarizing a set\n  of emails exchanged during Spring 2013, concerning the DirectDemocracyP2P.net\n  system", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arguments are essential objects in DirectDemocracyP2P, where they can occur\nboth in association with signatures for petitions, or in association with other\ndebated decisions, such as bug sorting by importance. The arguments of a signer\non a given issue are grouped into one single justification, are classified by\nthe type of signature (e.g., supporting or opposing), and can be subject to\nvarious types of threading.\n  Given the available inputs, the two addressed problems are: (i) how to\nrecommend the best justification, of a given type, to a new voter, (ii) how to\nrecommend a compact list of justifications subsuming the majority of known\narguments for (or against) an issue.\n  We investigate solutions based on weighted bipartite graphs.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 01:29:19 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Silaghi", "Marius C.", ""], ["Roussev", "Roussi", ""]]}, {"id": "1411.5635", "submitter": "Claudia Schulz", "authors": "Claudia Schulz and Francesca Toni", "title": "Justifying Answer Sets using Argumentation", "comments": "This article has been accepted for publication in Theory and Practice\n  of Logic Programming", "journal-ref": "Theory and Practice of Logic Programming 16 (2016) 59-110", "doi": "10.1017/S1471068414000702", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An answer set is a plain set of literals which has no further structure that\nwould explain why certain literals are part of it and why others are not. We\nshow how argumentation theory can help to explain why a literal is or is not\ncontained in a given answer set by defining two justification methods, both of\nwhich make use of the correspondence between answer sets of a logic program and\nstable extensions of the Assumption-Based Argumentation (ABA) framework\nconstructed from the same logic program. Attack Trees justify a literal in\nargumentation-theoretic terms, i.e. using arguments and attacks between them,\nwhereas ABA-Based Answer Set Justifications express the same justification\nstructure in logic programming terms, that is using literals and their\nrelationships. Interestingly, an ABA-Based Answer Set Justification corresponds\nto an admissible fragment of the answer set in question, and an Attack Tree\ncorresponds to an admissible fragment of the stable extension corresponding to\nthis answer set.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 18:37:12 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 14:52:20 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Schulz", "Claudia", ""], ["Toni", "Francesca", ""]]}, {"id": "1411.5654", "submitter": "Xinlei Chen", "authors": "Xinlei Chen and C. Lawrence Zitnick", "title": "Learning a Recurrent Visual Representation for Image Caption Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the bi-directional mapping between images and their\nsentence-based descriptions. We propose learning this mapping using a recurrent\nneural network. Unlike previous approaches that map both sentences and images\nto a common embedding, we enable the generation of novel sentences given an\nimage. Using the same model, we can also reconstruct the visual features\nassociated with an image given its visual description. We use a novel recurrent\nvisual memory that automatically learns to remember long-term visual concepts\nto aid in both sentence generation and visual feature reconstruction. We\nevaluate our approach on several tasks. These include sentence generation,\nsentence retrieval and image retrieval. State-of-the-art results are shown for\nthe task of generating novel image descriptions. When compared to human\ngenerated captions, our automatically generated captions are preferred by\nhumans over $19.8\\%$ of the time. Results are better than or comparable to\nstate-of-the-art results on the image and sentence retrieval tasks for methods\nusing similar visual features.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 19:50:27 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Chen", "Xinlei", ""], ["Zitnick", "C. Lawrence", ""]]}, {"id": "1411.5878", "submitter": "Ming-Ming Cheng Prof.", "authors": "Ali Borji, Ming-Ming Cheng, Qibin Hou, Huaizu Jiang, Jia Li", "title": "Salient Object Detection: A Survey", "comments": null, "journal-ref": "Computational Visual Media, 5(2):117-150, 2019", "doi": "10.1007/s41095-019-0149-9", "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and segmenting salient objects from natural scenes, often referred\nto as salient object detection, has attracted great interest in computer\nvision. While many models have been proposed and several applications have\nemerged, a deep understanding of achievements and issues remains lacking. We\naim to provide a comprehensive review of recent progress in salient object\ndetection and situate this field among other closely related areas such as\ngeneric scene segmentation, object proposal generation, and saliency for\nfixation prediction. Covering 228 publications, we survey i) roots, key\nconcepts, and tasks, ii) core techniques and main modeling trends, and iii)\ndatasets and evaluation metrics for salient object detection. We also discuss\nopen problems such as evaluation metrics and dataset bias in model performance,\nand suggest future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 22:41:24 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 17:17:03 GMT"}, {"version": "v3", "created": "Wed, 16 Aug 2017 21:45:20 GMT"}, {"version": "v4", "created": "Mon, 4 Sep 2017 02:51:33 GMT"}, {"version": "v5", "created": "Thu, 6 Sep 2018 19:31:29 GMT"}, {"version": "v6", "created": "Mon, 1 Jul 2019 11:13:07 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Borji", "Ali", ""], ["Cheng", "Ming-Ming", ""], ["Hou", "Qibin", ""], ["Jiang", "Huaizu", ""], ["Li", "Jia", ""]]}, {"id": "1411.5899", "submitter": "Fulton Wang", "authors": "Fulton Wang, Cynthia Rudin", "title": "Falling Rule Lists", "comments": "Accepted at AISTATS 2015. Contains number of rules mined, running\n  times. in Proceedings of AISTATS 2015. JMLR: W&CP 38", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Falling rule lists are classification models consisting of an ordered list of\nif-then rules, where (i) the order of rules determines which example should be\nclassified by each rule, and (ii) the estimated probability of success\ndecreases monotonically down the list. These kinds of rule lists are inspired\nby healthcare applications where patients would be stratified into risk sets\nand the highest at-risk patients should be considered first. We provide a\nBayesian framework for learning falling rule lists that does not rely on\ntraditional greedy decision tree learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 15:01:56 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 04:41:38 GMT"}, {"version": "v3", "created": "Sun, 1 Feb 2015 06:46:47 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Wang", "Fulton", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1411.6279", "submitter": "Bryan Renne", "authors": "Bryan Renne and Joshua Sack and Audrey Yap", "title": "Logics of Temporal-Epistemic Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Dynamic Epistemic Temporal Logic, a framework for reasoning about\noperations on multi-agent Kripke models that contain a designated temporal\nrelation. These operations are natural extensions of the well-known \"action\nmodels\" from Dynamic Epistemic Logic. Our \"temporal action models\" may be used\nto define a number of informational actions that can modify the \"objective\"\ntemporal structure of a model along with the agents' basic and higher-order\nknowledge and beliefs about this structure, including their beliefs about the\ntime. In essence, this approach provides one way to extend the domain of action\nmodel-style operations from atemporal Kripke models to temporal Kripke models\nin a manner that allows actions to control the flow of time. We present a\nnumber of examples to illustrate the subtleties involved in interpreting the\neffects of our extended action models on temporal Kripke models. We also study\npreservation of important epistemic-temporal properties of temporal Kripke\nmodels under temporal action model-induced operations, provide complete\naxiomatizations for two theories of temporal action models, and connect our\napproach with previous work on time in Dynamic Epistemic Logic.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 18:33:28 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Renne", "Bryan", ""], ["Sack", "Joshua", ""], ["Yap", "Audrey", ""]]}, {"id": "1411.6300", "submitter": "Do L Paul Minh", "authors": "Do Le Paul Minh", "title": "Discrete Bayesian Networks: The Exact Posterior Marginal Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In a Bayesian network, we wish to evaluate the marginal probability of a\nquery variable, which may be conditioned on the observed values of some\nevidence variables. Here we first present our \"border algorithm,\" which\nconverts a BN into a directed chain. For the polytrees, we then present in\ndetails, with some modifications and within the border algorithm framework, the\n\"revised polytree algorithm\" by Peot & Shachter (1991). Finally, we present our\n\"parentless polytree method,\" which, coupled with the border algorithm,\nconverts any Bayesian network into a polytree, rendering the complexity of our\ninferences independent of the size of network, and linear with the number of\nits evidence and query variables. All quantities in this paper have\nprobabilistic interpretations.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 21:19:44 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Minh", "Do Le Paul", ""]]}, {"id": "1411.6307", "submitter": "Nematollah Kayhan Batmanghelich", "authors": "Nematollah Kayhan Batmanghelich, Gerald Quon, Alex Kulesza, Manolis\n  Kellis, Polina Golland, Luke Bornn", "title": "Diversifying Sparsity Using Variational Determinantal Point Processes", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel diverse feature selection method based on determinantal\npoint processes (DPPs). Our model enables one to flexibly define diversity\nbased on the covariance of features (similar to orthogonal matching pursuit) or\nalternatively based on side information. We introduce our approach in the\ncontext of Bayesian sparse regression, employing a DPP as a variational\napproximation to the true spike and slab posterior distribution. We\nsubsequently show how this variational DPP approximation generalizes and\nextends mean-field approximation, and can be learned efficiently by exploiting\nthe fast sampling properties of DPPs. Our motivating application comes from\nbioinformatics, where we aim to identify a diverse set of genes whose\nexpression profiles predict a tumor type where the diversity is defined with\nrespect to a gene-gene interaction network. We also explore an application in\nspatial statistics. In both cases, we demonstrate that the proposed method\nyields significantly more diverse feature sets than classic sparse methods,\nwithout compromising accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 22:11:34 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Batmanghelich", "Nematollah Kayhan", ""], ["Quon", "Gerald", ""], ["Kulesza", "Alex", ""], ["Kellis", "Manolis", ""], ["Golland", "Polina", ""], ["Bornn", "Luke", ""]]}, {"id": "1411.6314", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry\n  Wasserman", "title": "On the High-dimensional Power of Linear-time Kernel Two-Sample Testing\n  under Mean-difference Alternatives", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric two sample testing deals with the question of consistently\ndeciding if two distributions are different, given samples from both, without\nmaking any parametric assumptions about the form of the distributions. The\ncurrent literature is split into two kinds of tests - those which are\nconsistent without any assumptions about how the distributions may differ\n(\\textit{general} alternatives), and those which are designed to specifically\ntest easier alternatives, like a difference in means (\\textit{mean-shift}\nalternatives).\n  The main contribution of this paper is to explicitly characterize the power\nof a popular nonparametric two sample test, designed for general alternatives,\nunder a mean-shift alternative in the high-dimensional setting. Specifically,\nwe explicitly derive the power of the linear-time Maximum Mean Discrepancy\nstatistic using the Gaussian kernel, where the dimension and sample size can\nboth tend to infinity at any rate, and the two distributions differ in their\nmeans. As a corollary, we find that if the signal-to-noise ratio is held\nconstant, then the test's power goes to one if the number of samples increases\nfaster than the dimension increases. This is the first explicit power\nderivation for a general nonparametric test in the high-dimensional setting,\nand also the first analysis of how tests designed for general alternatives\nperform when faced with easier ones.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 23:32:02 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Reddi", "Sashank J.", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1411.6593", "submitter": "David Tolpin", "authors": "David Tolpin, Oded Betzalel, Ariel Felner, Solomon Eyal Shimony", "title": "Rational Deployment of Multiple Heuristics in IDA*", "comments": "7 pages, 6 tables, 20 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in metareasoning for search has shown its usefulness in\nimproving numerous search algorithms. This paper applies rational metareasoning\nto IDA* when several admissible heuristics are available. The obvious basic\napproach of taking the maximum of the heuristics is improved upon by lazy\nevaluation of the heuristics, resulting in a variant known as Lazy IDA*. We\nintroduce a rational version of lazy IDA* that decides whether to compute the\nmore expensive heuristics or to bypass it, based on a myopic expected regret\nestimate. Empirical evaluation in several domains supports the theoretical\nresults, and shows that rational lazy IDA* is a state-of-the-art heuristic\ncombination method.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 20:04:20 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Tolpin", "David", ""], ["Betzalel", "Oded", ""], ["Felner", "Ariel", ""], ["Shimony", "Solomon Eyal", ""]]}, {"id": "1411.6651", "submitter": "Amir Arsalan Soltani", "authors": "Amir Arsalan Soltani", "title": "A Greedy, Flexible Algorithm to Learn an Optimal Bayesian Network\n  Structure", "comments": "This was my Advanced Machine Learning's course project in Spring 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report paper we first present a report of the Advanced Machine\nLearning Course Project on the provided data set and then present a novel\nheuristic algorithm for exact Bayesian network (BN) structure discovery that\nuses decomposable scoring functions. Our algorithm follows a different approach\nto solve the problem of BN structure discovery than the previously used methods\nsuch as Dynamic Programming (DP) and Branch and Bound to reduce the search\nspace and find the global optima space for the problem. The algorithm we\npropose has some degree of flexibility that can make it more or less greedy.\nThe more the algorithm is set to be greedy, the more the speed of the algorithm\nwill be, and the less optimal the final structure. Our algorithm runs in a much\nless time than the previously known methods and guarantees to have an\noptimality of close to 99%. Therefore, it sacrifices less than one percent of\nscore of an optimal structure in order to gain a much lower running time and\nmake the algorithm feasible for large data sets (we may note that we never used\nany toolbox except for result validation)\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 21:27:37 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Soltani", "Amir Arsalan", ""]]}, {"id": "1411.6721", "submitter": "Marc Solanas Tarre", "authors": "Marc Solanas, Julio Hernandez-Castro, Debojyoti Dutta", "title": "Detecting fraudulent activity in a cloud using privacy-friendly data\n  aggregates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  More users and companies make use of cloud services every day. They all\nexpect a perfect performance and any issue to remain transparent to them. This\nlast statement is very challenging to perform. A user's activities in our cloud\ncan affect the overall performance of our servers, having an impact on other\nresources. We can consider these kind of activities as fraudulent. They can be\neither illegal activities, such as launching a DDoS attack or just activities\nwhich are undesired by the cloud provider, such as Bitcoin mining, which uses\nsubstantial power, reduces the life of the hardware and can possibly slow down\nother user's activities. This article discusses a method to detect such\nactivities by using non-intrusive, privacy-friendly data: billing data. We use\nOpenStack as an example with data provided by Telemetry, the component in\ncharge of measuring resource usage for billing purposes. Results will be shown\nproving the efficiency of this method and ways to improve it will be provided\nas well as its advantages and disadvantages.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 03:56:43 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Solanas", "Marc", ""], ["Hernandez-Castro", "Julio", ""], ["Dutta", "Debojyoti", ""]]}, {"id": "1411.6754", "submitter": "Xiaosong Hu", "authors": "Xiaosong Hu, Wen Zhu, Qing Li", "title": "HCRS: A hybrid clothes recommender system based on user ratings and\n  product features", "comments": "ICMECG '13 Proceedings of the 2013 International Conference on\n  Management of e-Commerce and e-Government Pages 270-274", "journal-ref": null, "doi": "10.1109/ICMeCG.2013.60", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, online clothes-selling business has become popular and extremely\nattractive because of its convenience and cheap-and-fine price. Good examples\nof these successful Web sites include Yintai.com, Vancl.com and\nShop.vipshop.com which provide thousands of clothes for online shoppers. The\nchallenge for online shoppers lies on how to find a good product from lots of\noptions. In this article, we propose a collaborative clothes recommender for\neasy shopping. One of the unique features of this system is the ability to\nrecommend clothes in terms of both user ratings and clothing attributes.\nExperiments in our simulation environment show that the proposed recommender\ncan better satisfy the needs of users.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 07:55:07 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Hu", "Xiaosong", ""], ["Zhu", "Wen", ""], ["Li", "Qing", ""]]}, {"id": "1411.6768", "submitter": "Yuriy Parzhin", "authors": "Yuri Parzhin", "title": "Hypotheses of neural code and the information model of the\n  neuron-detector", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of neural code solving. On the basis of the\nformulated hypotheses the information model of a neuron-detector is suggested,\nthe detector being one of the basic elements of an artificial neural network\n(ANN). The paper subjects the connectionist paradigm of ANN building to\ncriticism and suggests a new presentation paradigm for ANN building and\nneuroelements (NE) learning. The adequacy of the suggested model is proved by\nthe fact that is does not contradict the modern propositions of neuropsychology\nand neurophysiology.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 08:52:14 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Parzhin", "Yuri", ""]]}, {"id": "1411.6794", "submitter": "Mart\\'in Pereira-Fari\\~na", "authors": "M. Pereira-Fari\\~na", "title": "Some Reflections on the Set-based and the Conditional-based\n  Interpretations of Statements in Syllogistic Reasoning", "comments": "16 pages, Artificial Intelligence", "journal-ref": "Archives for the Philosophy and History of Soft Computing 1, 2014,\n  pp. 1-16", "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two interpretations about syllogistic statements are described in this paper.\nOne is the so-called set-based interpretation, which assumes that quantified\nstatements and syllogisms talk about quantity-relationships between sets. The\nother one, the so-called conditional interpretation, assumes that quantified\npropositions talk about conditional propositions and how strong are the links\nbetween the antecedent and the consequent. Both interpretations are compared\nattending to three different questions (existential import, singular statements\nand non-proportional quantifiers) from the point of view of their impact on the\nfurther development of this type of reasoning.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 10:27:38 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Pereira-Fari\u00f1a", "M.", ""]]}, {"id": "1411.6998", "submitter": "Diego Arenas", "authors": "Diego Arenas (IFSTTAR/COSYS/ESTAS, LAMIH), Remy Chevirer\n  (IFSTTAR/COSYS/ESTAS), Said Hanafi (LAMIH), Joaquin Rodriguez\n  (IFSTTAR/COSYS/ESTAS)", "title": "Solving the Periodic Timetabling Problem using a Genetic Algorithm", "comments": "XVIII Congreso Panamericano de Ingenier\\'ia de Transito, Transporte y\n  Logistica (PANAM 2014), Jun 2014, Santander, Spain.\n  http://www.panam2014.unican.es", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In railway operations, a timetable is established to determine the departure\nand arrival times for the trains or other rolling stock at the different\nstations or relevant points inside the rail network or a subset of this\nnetwork. The elaboration of this timetable is done to respond to the commercial\nrequirements for both passenger and freight traffic, but also it must respect a\nset of security and capacity constraints associated with the railway network,\nrolling stock and legislation. Combining these requirements and constraints, as\nwell as the important number of trains and schedules to plan, makes the\npreparation of a feasible timetable a complex and time-consuming process, that\nnormally takes several months to be completed. This article addresses the\nproblem of generating periodic timetables, which means that the involved trains\noperate in a recurrent pattern. For instance, the trains belonging to the same\ntrain line, depart from some station every 15 minutes or one hour. To tackle\nthe problem, we present a constraint-based model suitable for this kind of\nproblem. Then, we propose a genetic algorithm, allowing a rapid generation of\nfeasible periodic timetables. Finally, two case studies are presented, the\nfirst, describing a sub-set of the Netherlands rail network, and the second a\nlarge portion of the Nord-pas-de-Calais regional rail network, both of them are\nthen solved using our algorithm and the results are presented and discussed.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 15:24:25 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Arenas", "Diego", "", "IFSTTAR/COSYS/ESTAS, LAMIH"], ["Chevirer", "Remy", "", "IFSTTAR/COSYS/ESTAS"], ["Hanafi", "Said", "", "LAMIH"], ["Rodriguez", "Joaquin", "", "IFSTTAR/COSYS/ESTAS"]]}, {"id": "1411.7014", "submitter": "Guy Van den Broeck", "authors": "Guy Van den Broeck, Karthika Mohan, Arthur Choi, Judea Pearl", "title": "Efficient Algorithms for Bayesian Network Parameter Learning from\n  Incomplete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient family of algorithms to learn the parameters of a\nBayesian network from incomplete data. In contrast to textbook approaches such\nas EM and the gradient method, our approach is non-iterative, yields closed\nform parameter estimates, and eliminates the need for inference in a Bayesian\nnetwork. Our approach provides consistent parameter estimates for missing data\nproblems that are MCAR, MAR, and in some cases, MNAR. Empirically, our approach\nis orders of magnitude faster than EM (as our approach requires no inference).\nGiven sufficient data, we learn parameters that can be orders of magnitude more\naccurate.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 20:39:51 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Broeck", "Guy Van den", ""], ["Mohan", "Karthika", ""], ["Choi", "Arthur", ""], ["Pearl", "Judea", ""]]}, {"id": "1411.7149", "submitter": "Mart\\'in Pereira-Fari\\~na", "authors": "M. Pereira-Fari\\~na, Juan C. Vidal, F. D\\'iaz-Hermida, A. Bugar\\'in", "title": "A Fuzzy Syllogistic Reasoning Schema for Generalized Quantifiers", "comments": "22 pages, 6 figures, journal paper", "journal-ref": "(2014) Fuzzy Sets and Systems 234, 79-96", "doi": "10.1016/j.fss.2013.02.007", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new approximate syllogistic reasoning schema is described\nthat expands some of the approaches expounded in the literature into two ways:\n(i) a number of different types of quantifiers (logical, absolute,\nproportional, comparative and exception) taken from Theory of Generalized\nQuantifiers and similarity quantifiers, taken from statistics, are considered\nand (ii) any number of premises can be taken into account within the reasoning\nprocess. Furthermore, a systematic reasoning procedure to solve the syllogism\nis also proposed, interpreting it as an equivalent mathematical optimization\nproblem, where the premises constitute the constraints of the searching space\nfor the quantifier in the conclusion.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 09:26:14 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Pereira-Fari\u00f1a", "M.", ""], ["Vidal", "Juan C.", ""], ["D\u00edaz-Hermida", "F.", ""], ["Bugar\u00edn", "A.", ""]]}, {"id": "1411.7441", "submitter": "Stefano Ermon", "authors": "Stefano Ermon, Ronan Le Bras, Santosh K. Suram, John M. Gregoire,\n  Carla Gomes, Bart Selman, Robert B. van Dover", "title": "Pattern Decomposition with Complex Combinatorial Constraints:\n  Application to Materials Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying important components or factors in large amounts of noisy data is\na key problem in machine learning and data mining. Motivated by a pattern\ndecomposition problem in materials discovery, aimed at discovering new\nmaterials for renewable energy, e.g. for fuel and solar cells, we introduce\nCombiFD, a framework for factor based pattern decomposition that allows the\nincorporation of a-priori knowledge as constraints, including complex\ncombinatorial constraints. In addition, we propose a new pattern decomposition\nalgorithm, called AMIQO, based on solving a sequence of (mixed-integer)\nquadratic programs. Our approach considerably outperforms the state of the art\non the materials discovery problem, scaling to larger datasets and recovering\nmore precise and physically meaningful decompositions. We also show the\neffectiveness of our approach for enforcing background knowledge on other\napplication domains.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 02:31:41 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Ermon", "Stefano", ""], ["Bras", "Ronan Le", ""], ["Suram", "Santosh K.", ""], ["Gregoire", "John M.", ""], ["Gomes", "Carla", ""], ["Selman", "Bart", ""], ["van Dover", "Robert B.", ""]]}, {"id": "1411.7480", "submitter": "Christopher Rosin", "authors": "Christopher D. Rosin", "title": "Unweighted Stochastic Local Search can be Effective for Random CSP\n  Benchmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ULSA, a novel stochastic local search algorithm for random binary\nconstraint satisfaction problems (CSP). ULSA is many times faster than the\nprior state of the art on a widely-studied suite of random CSP benchmarks.\nUnlike the best previous methods for these benchmarks, ULSA is a simple\nunweighted method that does not require dynamic adaptation of weights or\npenalties. ULSA obtains new record best solutions satisfying 99 of 100\nvariables in the challenging frb100-40 benchmark instance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 06:41:22 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Rosin", "Christopher D.", ""]]}, {"id": "1411.7525", "submitter": "Mart\\'in Pereira-Fari\\~na", "authors": "M. Pereira-Fari\\~na, F. D\\'iaz-Hermida, A. Bugar\\'in", "title": "On the analysis of set-based fuzzy quantified reasoning using classical\n  syllogistics", "comments": "19 pages, 4 figures", "journal-ref": "\"Fuzzy Sets and Systems\", vol. 214(1), 83-94", "doi": "10.1016/j.fss.2012.03.015", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syllogism is a type of deductive reasoning involving quantified statements.\nThe syllogistic reasoning scheme in the classical Aristotelian framework\ninvolves three crisp term sets and four linguistic quantifiers, for which the\nmain support is the linguistic properties of the quantifiers. A number of fuzzy\napproaches for defining an approximate syllogism have been proposed for which\nthe main support is cardinality calculus. In this paper we analyze fuzzy\nsyllogistic models previously described by Zadeh and Dubois et al. and compare\ntheir behavior with that of the classical Aristotelian framework to check which\nof the 24 classical valid syllogistic reasoning patterns or moods are\nparticular crisp cases of these fuzzy approaches. This allows us to assess to\nwhat extent these approaches can be considered as either plausible extensions\nof the classical crisp syllogism or a basis for a general approach to the\nproblem of approximate syllogism.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 10:12:21 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Pereira-Fari\u00f1a", "M.", ""], ["D\u00edaz-Hermida", "F.", ""], ["Bugar\u00edn", "A.", ""]]}, {"id": "1411.7806", "submitter": "Martin Hole\\v{n}a", "authors": "Luk\\'a\\v{s} Bajer, Martin Hole\\v{n}a", "title": "Two Gaussian Approaches to Black-Box Optomization", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Outline of several strategies for using Gaussian processes as surrogate\nmodels for the covariance matrix adaptation evolution strategy (CMA-ES).\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 10:39:24 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Bajer", "Luk\u00e1\u0161", ""], ["Hole\u0148a", "Martin", ""]]}, {"id": "1411.7812", "submitter": "Piotr Faliszewski", "authors": "Jiehua Chen, Piotr Faliszewski, Rolf Niedermeier, Nimrod Talmon", "title": "Elections with Few Voters: Candidate Control Can Be Easy", "comments": "56 pages, short version presented at AAAI-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of candidate control in elections with\nfew voters, that is, we consider the parameterized complexity of candidate\ncontrol in elections with respect to the number of voters as a parameter. We\nconsider both the standard scenario of adding and deleting candidates, where\none asks whether a given candidate can become a winner (or, in the destructive\ncase, can be precluded from winning) by adding or deleting few candidates, as\nwell as a combinatorial scenario where adding/deleting a candidate\nautomatically means adding or deleting a whole group of candidates. Considering\nseveral fundamental voting rules, our results show that the parameterized\ncomplexity of candidate control, with the number of voters as the parameter, is\nmuch more varied than in the setting with many voters.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 11:09:04 GMT"}, {"version": "v2", "created": "Sat, 18 Mar 2017 11:32:11 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Chen", "Jiehua", ""], ["Faliszewski", "Piotr", ""], ["Niedermeier", "Rolf", ""], ["Talmon", "Nimrod", ""]]}, {"id": "1411.7920", "submitter": "Samuel Rodriques", "authors": "Samuel G. Rodriques", "title": "Probability Theory without Bayes' Rule", "comments": "12 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the Kolmogorov theory of probability, Bayes' rule allows one to\nperform statistical inference by relating conditional probabilities to\nunconditional probabilities. As we show here, however, there is a continuous\nset of alternative inference rules that yield the same results, and that may\nhave computational or practical advantages for certain problems. We formulate\ngeneralized axioms for probability theory, according to which the reverse\nconditional probability distribution P(B|A) is not specified by the forward\nconditional probability distribution P(A|B) and the marginals P(A) and P(B).\nThus, in order to perform statistical inference, one must specify an additional\n\"inference axiom,\" which relates P(B|A) to P(A|B), P(A), and P(B). We show that\nwhen Bayes' rule is chosen as the inference axiom, the axioms are equivalent to\nthe classical Kolmogorov axioms. We then derive consistency conditions on the\ninference axiom, and thereby characterize the set of all possible rules for\ninference. The set of \"first-order\" inference axioms, defined as the set of\naxioms in which P(B|A) depends on the first power of P(A|B), is found to be a\n1-simplex, with Bayes' rule at one of the extreme points. The other extreme\npoint, the \"inversion rule,\" is studied in depth.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 15:59:44 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 01:20:30 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Rodriques", "Samuel G.", ""]]}, {"id": "1411.7974", "submitter": "Kevin Waugh", "authors": "Kevin Waugh and Dustin Morrill and J. Andrew Bagnell and Michael\n  Bowling", "title": "Solving Games with Functional Regret Estimation", "comments": "AAAI Conference on Artificial Intelligence 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel online learning method for minimizing regret in large\nextensive-form games. The approach learns a function approximator online to\nestimate the regret for choosing a particular action. A no-regret algorithm\nuses these estimates in place of the true regrets to define a sequence of\npolicies.\n  We prove the approach sound by providing a bound relating the quality of the\nfunction approximation and regret of the algorithm. A corollary being that the\nmethod is guaranteed to converge to a Nash equilibrium in self-play so long as\nthe regrets are ultimately realizable by the function approximator. Our\ntechnique can be understood as a principled generalization of existing work on\nabstraction in large games; in our work, both the abstraction as well as the\nequilibrium are learned during self-play. We demonstrate empirically the method\nachieves higher quality strategies than state-of-the-art abstraction techniques\ngiven the same resources.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 18:45:50 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2014 23:45:22 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Waugh", "Kevin", ""], ["Morrill", "Dustin", ""], ["Bagnell", "J. Andrew", ""], ["Bowling", "Michael", ""]]}]