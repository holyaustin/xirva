[{"id": "1909.00031", "submitter": "Toby Jia-Jun Li", "authors": "Toby Jia-Jun Li, Marissa Radensky, Justin Jia, Kirielle Singarajah,\n  Tom M. Mitchell, Brad A. Myers", "title": "Interactive Task and Concept Learning from Natural Language Instructions\n  and GUI Demonstrations", "comments": "The AAAI-20 Workshop on Intelligent Process Automation (IPA-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language programming is a promising approach to enable end users to\ninstruct new tasks for intelligent agents. However, our formative study found\nthat end users would often use unclear, ambiguous or vague concepts when\nnaturally instructing tasks in natural language, especially when specifying\nconditionals. Existing systems have limited support for letting the user teach\nagents new concepts or explaining unclear concepts. In this paper, we describe\na new multi-modal domain-independent approach that combines natural language\nprogramming and programming-by-demonstration to allow users to first naturally\ndescribe tasks and associated conditions at a high level, and then collaborate\nwith the agent to recursively resolve any ambiguities or vagueness through\nconversations and demonstrations. Users can also define new procedures and\nconcepts by demonstrating and referring to contents within GUIs of existing\nmobile apps. We demonstrate this approach in PUMICE, an end-user programmable\nagent that implements this approach. A lab study with 10 users showed its\nusability.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 18:35:01 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 00:57:53 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Toby Jia-Jun", ""], ["Radensky", "Marissa", ""], ["Jia", "Justin", ""], ["Singarajah", "Kirielle", ""], ["Mitchell", "Tom M.", ""], ["Myers", "Brad A.", ""]]}, {"id": "1909.00105", "submitter": "Bodhisattwa Prasad Majumder", "authors": "Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, Julian McAuley", "title": "Generating Personalized Recipes from Historical User Preferences", "comments": "Accepted in EMNLP 2019. Data and codes are available at\n  https://github.com/majumderb/recipe-personalization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to recipe generation are unable to create recipes for\nusers with culinary preferences but incomplete knowledge of ingredients in\nspecific dishes. We propose a new task of personalized recipe generation to\nhelp these users: expanding a name and incomplete ingredient details into\ncomplete natural-text instructions aligned with the user's historical\npreferences. We attend on technique- and recipe-level representations of a\nuser's previously consumed recipes, fusing these 'user-aware' representations\nin an attention fusion layer to control recipe text generation. Experiments on\na new dataset of 180K recipes and 700K interactions show our model's ability to\ngenerate plausible and personalized recipes compared to non-personalized\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 01:50:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Majumder", "Bodhisattwa Prasad", ""], ["Li", "Shuyang", ""], ["Ni", "Jianmo", ""], ["McAuley", "Julian", ""]]}, {"id": "1909.00126", "submitter": "Tao Li", "authors": "Tao Li, Vivek Gupta, Maitrey Mehta, Vivek Srikumar", "title": "A Logic-Driven Framework for Consistency of Neural Models", "comments": "Accepted in EMNLP 2019; Extra footnote after camera ready; Addressing\n  R-fuzzy and S-fuzzy logic + extra acknowledgement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While neural models show remarkable accuracy on individual predictions, their\ninternal beliefs can be inconsistent across examples. In this paper, we\nformalize such inconsistency as a generalization of prediction error. We\npropose a learning framework for constraining models using logic rules to\nregularize them away from inconsistency. Our framework can leverage both\nlabeled and unlabeled examples and is directly compatible with off-the-shelf\nlearning schemes without model redesign. We instantiate our framework on\nnatural language inference, where experiments show that enforcing invariants\nstated in logic can help make the predictions of neural models both accurate\nand consistent.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 04:38:06 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 01:17:57 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 00:20:14 GMT"}, {"version": "v4", "created": "Fri, 13 Sep 2019 03:52:50 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Li", "Tao", ""], ["Gupta", "Vivek", ""], ["Mehta", "Maitrey", ""], ["Srikumar", "Vivek", ""]]}, {"id": "1909.00131", "submitter": "Prathyusha Jwalapuram", "authors": "Prathyusha Jwalapuram, Shafiq Joty, Irina Temnikova and Preslav Nakov", "title": "Evaluating Pronominal Anaphora in Machine Translation: An Evaluation\n  Measure and a Test Suite", "comments": "Accepted at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing neural revolution in machine translation has made it easier to\nmodel larger contexts beyond the sentence-level, which can potentially help\nresolve some discourse-level ambiguities such as pronominal anaphora, thus\nenabling better translations. Unfortunately, even when the resulting\nimprovements are seen as substantial by humans, they remain virtually unnoticed\nby traditional automatic evaluation measures like BLEU, as only a few words end\nup being affected. Thus, specialized evaluation measures are needed. With this\naim in mind, we contribute an extensive, targeted dataset that can be used as a\ntest suite for pronoun translation, covering multiple source languages and\ndifferent pronoun errors drawn from real system translations, for English. We\nfurther propose an evaluation measure to differentiate good and bad pronoun\ntranslations. We also conduct a user study to report correlations with human\njudgments.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 05:28:51 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Jwalapuram", "Prathyusha", ""], ["Joty", "Shafiq", ""], ["Temnikova", "Irina", ""], ["Nakov", "Preslav", ""]]}, {"id": "1909.00141", "submitter": "Deren Lei", "authors": "Siyao Li, Deren Lei, Pengda Qin, William Yang Wang", "title": "Deep Reinforcement Learning with Distributional Semantic Rewards for\n  Abstractive Summarization", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) has been a commonly-used strategy for the\nabstractive summarization task to address both the exposure bias and\nnon-differentiable task issues. However, the conventional reward Rouge-L simply\nlooks for exact n-grams matches between candidates and annotated references,\nwhich inevitably makes the generated sentences repetitive and incoherent. In\nthis paper, instead of Rouge-L, we explore the practicability of utilizing the\ndistributional semantics to measure the matching degrees. With distributional\nsemantics, sentence-level evaluation can be obtained, and semantically-correct\nphrases can also be generated without being limited to the surface form of the\nreference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets\nshow that our proposed distributional semantics reward (DSR) has distinct\nsuperiority in capturing the lexical and compositional diversity of natural\nlanguage.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 06:13:33 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 23:30:26 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Li", "Siyao", ""], ["Lei", "Deren", ""], ["Qin", "Pengda", ""], ["Wang", "William Yang", ""]]}, {"id": "1909.00160", "submitter": "Soumya Sharma", "authors": "Soumya Sharma, Bishal Santra, Abhik Jana, T.Y.S.S. Santosh, Niloy\n  Ganguly and Pawan Goyal", "title": "Incorporating Domain Knowledge into Medical NLI using Knowledge Graphs", "comments": "EMNLP 2019 accepted short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, biomedical version of embeddings obtained from language models such\nas BioELMo have shown state-of-the-art results for the textual inference task\nin the medical domain. In this paper, we explore how to incorporate structured\ndomain knowledge, available in the form of a knowledge graph (UMLS), for the\nMedical NLI task. Specifically, we experiment with fusing embeddings obtained\nfrom knowledge graph with the state-of-the-art approaches for NLI task (ESIM\nmodel). We also experiment with fusing the domain-specific sentiment\ninformation for the task. Experiments conducted on MedNLI dataset clearly show\nthat this strategy improves the baseline BioELMo architecture for the Medical\nNLI task.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 07:41:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Sharma", "Soumya", ""], ["Santra", "Bishal", ""], ["Jana", "Abhik", ""], ["Santosh", "T. Y. S. S.", ""], ["Ganguly", "Niloy", ""], ["Goyal", "Pawan", ""]]}, {"id": "1909.00193", "submitter": "Dimitri Ognibene", "authors": "Francesca Bianco and Dimitri Ognibene", "title": "Functional advantages of an adaptive Theory of Mind for robotics: a\n  review of current architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Great advancements have been achieved in the field of robotics, however, main\nchallenges remain, including building robots with an adaptive Theory of Mind\n(ToM). In the present paper, seven current robotic architectures for\nhuman-robot interactions were described as well as four main functional\nadvantages of equipping robots with an adaptive ToM. The aim of the present\npaper was to determine in which way and how often ToM features are integrated\nin the architectures analyzed, and if they provide robots with the associated\nfunctional advantages. Our assessment shows that different methods are used to\nimplement ToM features in robotic architectures. Furthermore, while a ToM for\nfalse-belief understanding and tracking is often built in social robotic\narchitectures, a ToM for proactivity, active perception and learning is less\ncommon. Nonetheless, progresses towards better adaptive ToM features in robots\nare warranted to provide them with full access to the advantages of having a\nToM resembling that of humans.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 10:59:58 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bianco", "Francesca", ""], ["Ognibene", "Dimitri", ""]]}, {"id": "1909.00197", "submitter": "Dimitri Ognibene", "authors": "Francesca Bianco and Dimitri Ognibene", "title": "Transferring Adaptive Theory of Mind to social robots: insights from\n  developmental psychology to robotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent advancement in the social robotic field, important\nlimitations restrain its progress and delay the application of robots in\neveryday scenarios. In the present paper, we propose to develop computational\nmodels inspired by our knowledge of human infants' social adaptive abilities.\nWe believe this may provide solutions at an architectural level to overcome the\nlimits of current systems. Specifically, we present the functional advantages\nthat adaptive Theory of Mind (ToM) systems would support in robotics (i.e.,\nmentalizing for belief understanding, proactivity and preparation, active\nperception and learning) and contextualize them in practical applications. We\nreview current computational models mainly based on the simulation and\nteleological theories, and robotic implementations to identify the limitations\nof ToM functions in current robotic architectures and suggest a possible future\ndevelopmental pathway. Finally, we propose future studies to create innovative\ncomputational models integrating the properties of the simulation and\nteleological approaches for an improved adaptive ToM ability in robots with the\naim of enhancing human-robot interactions and permitting the application of\nrobots in unexplored environments, such as disasters and construction sites. To\nachieve this goal, we suggest directing future research towards the modern\ncross-talk between the fields of robotics and developmental psychology.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 11:19:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bianco", "Francesca", ""], ["Ognibene", "Dimitri", ""]]}, {"id": "1909.00215", "submitter": "Yi-Ting Yeh", "authors": "Yi-Ting Yeh, Yun-Nung Chen", "title": "QAInfomax: Learning Robust Question Answering System by Mutual\n  Information Maximization", "comments": "EMNLP 2019 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard accuracy metrics indicate that modern reading comprehension systems\nhave achieved strong performance in many question answering datasets. However,\nthe extent these systems truly understand language remains unknown, and\nexisting systems are not good at distinguishing distractor sentences, which\nlook related but do not actually answer the question. To address this problem,\nwe propose QAInfomax as a regularizer in reading comprehension systems by\nmaximizing mutual information among passages, a question, and its answer.\nQAInfomax helps regularize the model to not simply learn the superficial\ncorrelation for answering questions. The experiments show that our proposed\nQAInfomax achieves the state-of-the-art performance on the benchmark\nAdversarial-SQuAD dataset.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 13:50:28 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Yeh", "Yi-Ting", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "1909.00216", "submitter": "Francesco Giannini", "authors": "Francesco Giannini and Marco Maggini", "title": "Conditions for Unnecessary Logical Constraints in Kernel Machines", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30484-3_49", "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main property of support vector machines consists in the fact that only a\nsmall portion of the training data is significant to determine the maximum\nmargin separating hyperplane in the feature space, the so called support\nvectors. In a similar way, in the general scheme of learning from constraints,\nwhere possibly several constraints are considered, some of them may turn out to\nbe unnecessary with respect to the learning optimization, even if they are\nactive for a given optimal solution. In this paper we extend the definition of\nsupport vector to support constraint and we provide some criteria to determine\nwhich constraints can be removed from the learning problem still yielding the\nsame optimal solutions. In particular, we discuss the case of logical\nconstraints expressed by Lukasiewicz logic, where both inferential and\nalgebraic arguments can be considered. Some theoretical results that\ncharacterize the concept of unnecessary constraint are proved and explained by\nmeans of examples.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 14:00:04 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 13:38:07 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Giannini", "Francesco", ""], ["Maggini", "Marco", ""]]}, {"id": "1909.00230", "submitter": "Cong Fu", "authors": "Cong Fu, Tong Chen, Meng Qu, Woojeong Jin, Xiang Ren", "title": "Collaborative Policy Learning for Open Knowledge Graph Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a surge of interests in interpretable graph\nreasoning methods. However, these models often suffer from limited performance\nwhen working on sparse and incomplete graphs, due to the lack of evidential\npaths that can reach target entities. Here we study open knowledge graph\nreasoning---a task that aims to reason for missing facts over a graph augmented\nby a background text corpus. A key challenge of the task is to filter out\n\"irrelevant\" facts extracted from corpus, in order to maintain an effective\nsearch space during path inference. We propose a novel reinforcement learning\nframework to train two collaborative agents jointly, i.e., a multi-hop graph\nreasoner and a fact extractor. The fact extraction agent generates fact triples\nfrom corpora to enrich the graph on the fly; while the reasoning agent provides\nfeedback to the fact extractor and guides it towards promoting facts that are\nhelpful for the interpretable reasoning. Experiments on two public datasets\ndemonstrate the effectiveness of the proposed approach. Source code and\ndatasets used in this paper can be downloaded at\nhttps://github.com/shanzhenren/CPL\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 15:46:05 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Fu", "Cong", ""], ["Chen", "Tong", ""], ["Qu", "Meng", ""], ["Jin", "Woojeong", ""], ["Ren", "Xiang", ""]]}, {"id": "1909.00277", "submitter": "Lifu Huang", "authors": "Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi", "title": "Cosmos QA: Machine Reading Comprehension with Contextual Commonsense\n  Reasoning", "comments": "EMNLP'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding narratives requires reading between the lines, which in turn,\nrequires interpreting the likely causes and effects of events, even when they\nare not mentioned explicitly. In this paper, we introduce Cosmos QA, a\nlarge-scale dataset of 35,600 problems that require commonsense-based reading\ncomprehension, formulated as multiple-choice questions. In stark contrast to\nmost existing reading comprehension datasets where the questions focus on\nfactual and literal understanding of the context paragraph, our dataset focuses\non reading between the lines over a diverse collection of people's everyday\nnarratives, asking such questions as \"what might be the possible reason of\n...?\", or \"what would have happened if ...\" that require reasoning beyond the\nexact text spans in the context. To establish baseline performances on Cosmos\nQA, we experiment with several state-of-the-art neural architectures for\nreading comprehension, and also propose a new architecture that improves over\nthe competitive baselines. Experimental results demonstrate a significant gap\nbetween machine (68.4%) and human performance (94%), pointing to avenues for\nfuture research on commonsense machine comprehension. Dataset, code and\nleaderboard is publicly available at https://wilburone.github.io/cosmos.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 19:55:44 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 21:16:16 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Huang", "Lifu", ""], ["Bras", "Ronan Le", ""], ["Bhagavatula", "Chandra", ""], ["Choi", "Yejin", ""]]}, {"id": "1909.00279", "submitter": "Zhichao Yang", "authors": "Zhichao Yang, Pengshan Cai, Yansong Feng, Fei Li, Weijiang Feng, Elena\n  Suet-Ying Chiu, Hong Yu", "title": "Generating Classical Chinese Poems from Vernacular Chinese", "comments": "Published in EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Classical Chinese poetry is a jewel in the treasure house of Chinese culture.\nPrevious poem generation models only allow users to employ keywords to\ninterfere the meaning of generated poems, leaving the dominion of generation to\nthe model. In this paper, we propose a novel task of generating classical\nChinese poems from vernacular, which allows users to have more control over the\nsemantic of generated poems. We adapt the approach of unsupervised machine\ntranslation (UMT) to our task. We use segmentation-based padding and\nreinforcement learning to address under-translation and over-translation\nrespectively. According to experiments, our approach significantly improve the\nperplexity and BLEU compared with typical UMT models. Furthermore, we explored\nguidelines on how to write the input vernacular to generate better poems. Human\nevaluation showed our approach can generate high-quality poems which are\ncomparable to amateur poems.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 20:07:25 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Yang", "Zhichao", ""], ["Cai", "Pengshan", ""], ["Feng", "Yansong", ""], ["Li", "Fei", ""], ["Feng", "Weijiang", ""], ["Chiu", "Elena Suet-Ying", ""], ["Yu", "Hong", ""]]}, {"id": "1909.00295", "submitter": "Bryan (Ning) Xia", "authors": "Bryan (Ning) Xia, Yuan Gong, Yizhe Zhang, Christian Poellabauer", "title": "Second-order Non-local Attention Networks for Person Re-identification", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent efforts have shown promising results for person re-identification by\ndesigning part-based architectures to allow a neural network to learn\ndiscriminative representations from semantically coherent parts. Some efforts\nuse soft attention to reallocate distant outliers to their most similar parts,\nwhile others adjust part granularity to incorporate more distant positions for\nlearning the relationships. Others seek to generalize part-based methods by\nintroducing a dropout mechanism on consecutive regions of the feature map to\nenhance distant region relationships. However, only few prior efforts model the\ndistant or non-local positions of the feature map directly for the person re-ID\ntask. In this paper, we propose a novel attention mechanism to directly model\nlong-range relationships via second-order feature statistics. When combined\nwith a generalized DropBlock module, our method performs equally to or better\nthan state-of-the-art results for mainstream person re-identification datasets,\nincluding Market1501, CUHK03, and DukeMTMC-reID.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 22:50:42 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Bryan", "", "", "Ning"], ["Xia", "", ""], ["Gong", "Yuan", ""], ["Zhang", "Yizhe", ""], ["Poellabauer", "Christian", ""]]}, {"id": "1909.00360", "submitter": "Nicholas Cummins Dr", "authors": "Vidhyasaharan Sethu, Emily Mower Provost, Julien Epps, Carlos Busso,\n  Nicholas Cummins, Shrikanth Narayanan", "title": "The Ambiguous World of Emotion Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence and machine learning systems have demonstrated huge\nimprovements and human-level parity in a range of activities, including speech\nrecognition, face recognition and speaker verification. However, these diverse\ntasks share a key commonality that is not true in affective computing: the\nground truth information that is inferred can be unambiguously represented.\nThis observation provides some hints as to why affective computing, despite\nhaving attracted the attention of researchers for years, may not still be\nconsidered a mature field of research. A key reason for this is the lack of a\ncommon mathematical framework to describe all the relevant elements of emotion\nrepresentations. This paper proposes the AMBiguous Emotion Representation\n(AMBER) framework to address this deficiency. AMBER is a unified framework that\nexplicitly describes categorical, numerical and ordinal representations of\nemotions, including time varying representations. In addition to explaining the\ncore elements of AMBER, the paper also discusses how some of the commonly\nemployed emotion representation schemes can be viewed through the AMBER\nframework, and concludes with a discussion of how the proposed framework can be\nused to reason about current and future affective computing systems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 09:05:58 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Sethu", "Vidhyasaharan", ""], ["Provost", "Emily Mower", ""], ["Epps", "Julien", ""], ["Busso", "Carlos", ""], ["Cummins", "Nicholas", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "1909.00372", "submitter": "Zitao Liu", "authors": "Zhiwei Wang, Xiaoqin Feng, Jiliang Tang, Gale Yan Huang, Zitao Liu", "title": "Deep Knowledge Tracing with Side Information", "comments": "The 20th International Conference on Artificial Intelligence in\n  Education(AIED), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring student knowledge states or skill acquisition levels known as\nknowledge tracing, is a fundamental part of intelligent tutoring systems.\nDespite its inherent challenges, recent deep neural networks based knowledge\ntracing models have achieved great success, which is largely from models'\nability to learn sequential dependencies of questions in student exercise data.\nHowever, in addition to sequential information, questions inherently exhibit\nside relations, which can enrich our understandings about student knowledge\nstates and has great potentials to advance knowledge tracing. Thus, in this\npaper, we exploit side relations to improve knowledge tracing and design a\nnovel framework DTKS. The experimental results on real education data validate\nthe effectiveness of the proposed framework and demonstrate the importance of\nside information in knowledge tracing.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 10:23:21 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Zhiwei", ""], ["Feng", "Xiaoqin", ""], ["Tang", "Jiliang", ""], ["Huang", "Gale Yan", ""], ["Liu", "Zitao", ""]]}, {"id": "1909.00393", "submitter": "Matan Orbach", "authors": "Matan Orbach, Yonatan Bilu, Ariel Gera, Yoav Kantor, Lena Dankin,\n  Tamar Lavee, Lili Kotlerman, Shachar Mirkin, Michal Jacovi, Ranit Aharonov\n  and Noam Slonim", "title": "A Dataset of General-Purpose Rebuttal", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Natural Language Understanding, the task of response generation is usually\nfocused on responses to short texts, such as tweets or a turn in a dialog. Here\nwe present a novel task of producing a critical response to a long\nargumentative text, and suggest a method based on general rebuttal arguments to\naddress it. We do this in the context of the recently-suggested task of\nlistening comprehension over argumentative content: given a speech on some\nspecified topic, and a list of relevant arguments, the goal is to determine\nwhich of the arguments appear in the speech. The general rebuttals we describe\nhere (written in English) overcome the need for topic-specific arguments to be\nprovided, by proving to be applicable for a large set of topics. This allows\ncreating responses beyond the scope of topics for which specific arguments are\navailable. All data collected during this work is freely available for\nresearch.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 13:24:35 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Orbach", "Matan", ""], ["Bilu", "Yonatan", ""], ["Gera", "Ariel", ""], ["Kantor", "Yoav", ""], ["Dankin", "Lena", ""], ["Lavee", "Tamar", ""], ["Kotlerman", "Lili", ""], ["Mirkin", "Shachar", ""], ["Jacovi", "Michal", ""], ["Aharonov", "Ranit", ""], ["Slonim", "Noam", ""]]}, {"id": "1909.00395", "submitter": "Wade Genders", "authors": "Wade Genders, Saiedeh Razavi", "title": "An Open-Source Framework for Adaptive Traffic Signal Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sub-optimal control policies in transportation systems negatively impact\nmobility, the environment and human health. Developing optimal transportation\ncontrol systems at the appropriate scale can be difficult as cities'\ntransportation systems can be large, complex and stochastic. Intersection\ntraffic signal controllers are an important element of modern transportation\ninfrastructure where sub-optimal control policies can incur high costs to many\nusers. Many adaptive traffic signal controllers have been proposed by the\ncommunity but research is lacking regarding their relative performance\ndifference - which adaptive traffic signal controller is best remains an open\nquestion. This research contributes a framework for developing and evaluating\ndifferent adaptive traffic signal controller models in simulation - both\nlearning and non-learning - and demonstrates its capabilities. The framework is\nused to first, investigate the performance variance of the modelled adaptive\ntraffic signal controllers with respect to their hyperparameters and second,\nanalyze the performance differences between controllers with optimal\nhyperparameters. The proposed framework contains implementations of some of the\nmost popular adaptive traffic signal controllers from the literature;\nWebster's, Max-pressure and Self-Organizing Traffic Lights, along with deep\nQ-network and deep deterministic policy gradient reinforcement learning\ncontrollers. This framework will aid researchers by accelerating their work\nfrom a common starting point, allowing them to generate results faster with\nless effort. All framework source code is available at\nhttps://github.com/docwza/sumolights.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 13:26:49 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Genders", "Wade", ""], ["Razavi", "Saiedeh", ""]]}, {"id": "1909.00413", "submitter": "Hong-Ning Dai Prof.", "authors": "Hong-Ning Dai and Hao Wang and Guangquan Xu and Jiafu Wan and Muhammad\n  Imran", "title": "Big Data Analytics for Manufacturing Internet of Things: Opportunities,\n  Challenges and Enabling Technologies", "comments": "14 pages, 6 figures, 3 tables", "journal-ref": "Enterprise Information Systems, 2019", "doi": "10.1080/17517575.2019.1633689", "report-no": null, "categories": "cs.CY cs.AI cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent advances in information and communication technology (ICT) have\npromoted the evolution of conventional computer-aided manufacturing industry to\nsmart data-driven manufacturing. Data analytics in massive manufacturing data\ncan extract huge business values while can also result in research challenges\ndue to the heterogeneous data types, enormous volume and real-time velocity of\nmanufacturing data. This paper provides an overview on big data analytics in\nmanufacturing Internet of Things (MIoT). This paper first starts with a\ndiscussion on necessities and challenges of big data analytics in manufacturing\ndata of MIoT. Then, the enabling technologies of big data analytics of\nmanufacturing data are surveyed and discussed. Moreover, this paper also\noutlines the future directions in this promising area.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 14:53:16 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Dai", "Hong-Ning", ""], ["Wang", "Hao", ""], ["Xu", "Guangquan", ""], ["Wan", "Jiafu", ""], ["Imran", "Muhammad", ""]]}, {"id": "1909.00442", "submitter": "Alexander Dockhorn", "authors": "Alexander Dockhorn, Simon M. Lucas, Vanessa Volz, Ivan Bravi, Raluca\n  D. Gaina, Diego Perez-Liebana", "title": "Learning Local Forward Models on Unforgiving Games", "comments": "4 pages, 3 figures, 3 tables, accepted at IEEE COG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper examines learning approaches for forward models based on local\ncell transition functions. We provide a formal definition of local forward\nmodels for which we propose two basic learning approaches. Our analysis is\nbased on the game Sokoban, where a wrong action can lead to an unsolvable game\nstate. Therefore, an accurate prediction of an action's resulting state is\nnecessary to avoid this scenario.\n  In contrast to learning the complete state transition function, local forward\nmodels allow extracting multiple training examples from a single state\ntransition. In this way, the Hash Set model, as well as the Decision Tree\nmodel, quickly learn to predict upcoming state transitions of both the training\nand the test set. Applying the model using a statistical forward planner showed\nthat the best models can be used to satisfying degree even in cases in which\nthe test levels have not yet been seen.\n  Our evaluation includes an analysis of various local neighbourhood patterns\nand sizes to test the learners' capabilities in case too few or too many\nattributes are extracted, of which the latter has shown do degrade the\nperformance of the model learner.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 18:25:14 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Dockhorn", "Alexander", ""], ["Lucas", "Simon M.", ""], ["Volz", "Vanessa", ""], ["Bravi", "Ivan", ""], ["Gaina", "Raluca D.", ""], ["Perez-Liebana", "Diego", ""]]}, {"id": "1909.00505", "submitter": "Joseph Davison", "authors": "Joshua Feldman, Joe Davison, Alexander M. Rush", "title": "Commonsense Knowledge Mining from Pretrained Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring commonsense knowledge is a key challenge in natural language\nprocessing, but due to the sparsity of training data, previous work has shown\nthat supervised methods for commonsense knowledge mining underperform when\nevaluated on novel data. In this work, we develop a method for generating\ncommonsense knowledge using a large, pre-trained bidirectional language model.\nBy transforming relational triples into masked sentences, we can use this model\nto rank a triple's validity by the estimated pointwise mutual information\nbetween the two entities. Since we do not update the weights of the\nbidirectional model, our approach is not biased by the coverage of any one\ncommonsense knowledge base. Though this method performs worse on a test set\nthan models explicitly trained on a corresponding training set, it outperforms\nthese methods when mining commonsense knowledge from new sources, suggesting\nthat unsupervised techniques may generalize better than current supervised\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 01:41:00 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Feldman", "Joshua", ""], ["Davison", "Joe", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1909.00513", "submitter": "Shengyu Zhu", "authors": "Zhitang Chen, Shengyu Zhu, Yue Liu, Tim Tse", "title": "Causal Discovery by Kernel Intrinsic Invariance Measure", "comments": "9 pages, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning based on causality, instead of association has been considered as a\nkey ingredient towards real machine intelligence. However, it is a challenging\ntask to infer causal relationship/structure among variables. In recent years,\nan Independent Mechanism (IM) principle was proposed, stating that the\nmechanism generating the cause and the one mapping the cause to the effect are\nindependent. As the conjecture, it is argued that in the causal direction, the\nconditional distributions instantiated at different value of the conditioning\nvariable have less variation than the anti-causal direction. Existing\nstate-of-the-arts simply compare the variance of the RKHS mean embedding norms\nof these conditional distributions. In this paper, we prove that this\nnorm-based approach sacrifices important information of the original\nconditional distributions. We propose a Kernel Intrinsic Invariance Measure\n(KIIM) to capture higher order statistics corresponding to the shapes of the\ndensity functions. We show our algorithm can be reduced to an\neigen-decomposition task on a kernel matrix measuring intrinsic\ndeviance/invariance. Causal directions can then be inferred by comparing the\nKIIM scores of two hypothetic directions. Experiments on synthetic and real\ndata are conducted to show the advantages of our methods over existing\nsolutions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 01:56:47 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Chen", "Zhitang", ""], ["Zhu", "Shengyu", ""], ["Liu", "Yue", ""], ["Tse", "Tim", ""]]}, {"id": "1909.00519", "submitter": "Mojtaba Nayyeri", "authors": "Mojtaba Nayyeri, Chengjin Xu, Yadollah Yaghoobzadeh, Hamed Shariat\n  Yazdi, Jens Lehmann", "title": "Toward Understanding The Effect Of Loss function On Then Performance Of\n  Knowledge Graph Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) represent world's facts in structured forms. KG\ncompletion exploits the existing facts in a KG to discover new ones.\nTranslation-based embedding model (TransE) is a prominent formulation to do KG\ncompletion. Despite the efficiency of TransE in memory and time, it suffers\nfrom several limitations in encoding relation patterns such as symmetric,\nreflexive etc. To resolve this problem, most of the attempts have circled\naround the revision of the score function of TransE i.e., proposing a more\ncomplicated score function such as Trans(A, D, G, H, R, etc) to mitigate the\nlimitations. In this paper, we tackle this problem from a different\nperspective. We show that existing theories corresponding to the limitations of\nTransE are inaccurate because they ignore the effect of loss function.\nAccordingly, we pose theoretical investigations of the main limitations of\nTransE in the light of loss function. To the best of our knowledge, this has\nnot been investigated so far comprehensively. We show that by a proper\nselection of the loss function for training the TransE model, the main\nlimitations of the model are mitigated. This is explained by setting\nupper-bound for the scores of positive samples, showing the region of truth\n(i.e., the region that a triple is considered positive by the model). Our\ntheoretical proofs with experimental results fill the gap between the\ncapability of translation-based class of embedding models and the loss\nfunction. The theories emphasise the importance of the selection of the loss\nfunctions for training the models. Our experimental evaluations on different\nloss functions used for training the models justify our theoretical proofs and\nconfirm the importance of the loss functions on the performance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 03:10:14 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 08:58:05 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Nayyeri", "Mojtaba", ""], ["Xu", "Chengjin", ""], ["Yaghoobzadeh", "Yadollah", ""], ["Yazdi", "Hamed Shariat", ""], ["Lehmann", "Jens", ""]]}, {"id": "1909.00621", "submitter": "Sarah Alice Gaggl", "authors": "Sarah A. Gaggl and Thomas Linsbichler and Marco Maratea and Stefan\n  Woltran", "title": "Design and Results of the Second International Competition on\n  Computational Models of Argumentation", "comments": "submitted to Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argumentation is a major topic in the study of Artificial Intelligence. Since\nthe first edition in 2015, advancements in solving (abstract) argumentation\nframeworks are assessed in competition events, similar to other closely related\nproblem solving technologies. In this paper, we report about the design and\nresults of the Second International Competition on Computational Models of\nArgumentation, which has been jointly organized by TU Dresden (Germany), TU\nWien (Austria), and the University of Genova (Italy), in affiliation with the\n2017 International Workshop on Theory and Applications of Formal Argumentation.\nThis second edition maintains some of the design choices made in the first\nevent, e.g. the I/O formats, the basic reasoning problems, and the organization\ninto tasks and tracks. At the same time, it introduces significant novelties,\ne.g. three additional prominent semantics, and an instance selection stage for\nclassifying instances according to their empirical hardness.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 09:23:48 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Gaggl", "Sarah A.", ""], ["Linsbichler", "Thomas", ""], ["Maratea", "Marco", ""], ["Woltran", "Stefan", ""]]}, {"id": "1909.00672", "submitter": "Linfeng Li", "authors": "Linfeng Li, Peng Wang, Yao Wang, Jinpeng Jiang, Buzhou Tang, Jun Yan,\n  Shenghui Wang, Yuting Liu", "title": "A Method to Learn Embedding of a Probabilistic Medical Knowledge Graph:\n  Algorithm Development", "comments": null, "journal-ref": "JMIR Med Inform 2020;8(5):e17645", "doi": "10.2196/17645", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an algorithm named as PrTransH to learn embedding vectors\nfrom real world EMR data based medical knowledge. The unique challenge in\nembedding medical knowledge graph from real world EMR data is that the\nuncertainty of knowledge triplets blurs the border between \"correct triplet\"\nand \"wrong triplet\", changing the fundamental assumption of many existing\nalgorithms. To address the challenge, some enhancements are made to existing\nTransH algorithm, including: 1) involve probability of medical knowledge\ntriplet into training objective; 2) replace the margin-based ranking loss with\nunified loss calculation considering both valid and corrupted triplets; 3)\naugment training data set with medical background knowledge. Verifications on\nreal world EMR data based medical knowledge graph prove that PrTransH\noutperforms TransH in link prediction task. To the best of our survey, this\npaper is the first one to learn and verify knowledge embedding on probabilistic\nknowledge graphs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 11:28:35 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 18:06:34 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Linfeng", ""], ["Wang", "Peng", ""], ["Wang", "Yao", ""], ["Jiang", "Jinpeng", ""], ["Tang", "Buzhou", ""], ["Yan", "Jun", ""], ["Wang", "Shenghui", ""], ["Liu", "Yuting", ""]]}, {"id": "1909.00690", "submitter": "Maria Maleshkova", "authors": "Sebastian R. Bader and Maria Maleshkova", "title": "The Semantic Asset Administration Shell", "comments": "15 pages, pre-print of Semantics 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disruptive potential of the upcoming digital transformations for the\nindustrial manufacturing domain have led to several reference frameworks and\nnumerous standardization approaches. On the other hand, the Semantic Web\ncommunity has made significant contributions in the field, for instance on data\nand service description, integration of heterogeneous sources and devices, and\nAI techniques in distributed systems. These two streams of work are, however,\nmostly unrelated and only briefly regard each others requirements, practices\nand terminology. We contribute to closing this gap by providing the Semantic\nAsset Administration Shell, an RDF-based representation of the Industrie 4.0\nComponent. We provide an ontology for the latest data model specification,\ncreated a RML mapping, supply resources to validate the RDF entities and\nintroduce basic reasoning on the Asset Administration Shell data model.\nFurthermore, we discuss the different assumptions and presentation patterns,\nand analyze the implications of a semantic representation on the original data.\nWe evaluate the thereby created overheads, and conclude that the semantic\nlifting is manageable, also for restricted or embedded devices, and therefore\nmeets the needs of Industrie 4.0 scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 12:38:58 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bader", "Sebastian R.", ""], ["Maleshkova", "Maria", ""]]}, {"id": "1909.00754", "submitter": "Liliang Ren", "authors": "Liliang Ren, Jianmo Ni and Julian McAuley", "title": "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence\n  Generation", "comments": "The 2019 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2019); Updated empirical results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to dialogue state tracking rely on pre-defined ontologies\nconsisting of a set of all possible slot types and values. Though such\napproaches exhibit promising performance on single-domain benchmarks, they\nsuffer from computational complexity that increases proportionally to the\nnumber of pre-defined slots that need tracking. This issue becomes more severe\nwhen it comes to multi-domain dialogues which include larger numbers of slots.\nIn this paper, we investigate how to approach DST using a generation framework\nwithout the pre-defined ontology list. Given each turn of user utterance and\nsystem response, we directly generate a sequence of belief states by applying a\nhierarchical encoder-decoder structure. In this way, the computational\ncomplexity of our model will be a constant regardless of the number of\npre-defined slots. Experiments on both the multi-domain and the single domain\ndialogue state tracking dataset show that our model not only scales easily with\nthe increasing number of pre-defined domains and slots but also reaches the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 15:00:08 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 04:25:31 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Ren", "Liliang", ""], ["Ni", "Jianmo", ""], ["McAuley", "Julian", ""]]}, {"id": "1909.00769", "submitter": "Umair Z. Ahmed", "authors": "Umair Z. Ahmed, Renuka Sindhgatta, Nisheeth Srivastava, Amey Karkare", "title": "Targeted Example Generation for Compilation Errors", "comments": "To appear in 34th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TEGCER, an automated feedback tool for novice programmers. TEGCER\nuses supervised classification to match compilation errors in new code\nsubmissions with relevant pre-existing errors, submitted by other students\nbefore. The dense neural network used to perform this classification task is\ntrained on 15000+ error-repair code examples. The proposed model yields a test\nset classification Pred@3 accuracy of 97.7% across 212 error category labels.\nUsing this model as its base, TEGCER presents students with the closest\nrelevant examples of solutions for their specific error on demand.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 15:36:06 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 10:58:29 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Ahmed", "Umair Z.", ""], ["Sindhgatta", "Renuka", ""], ["Srivastava", "Nisheeth", ""], ["Karkare", "Amey", ""]]}, {"id": "1909.00792", "submitter": "Emilie Wirbel", "authors": "Thibault Buhet, Emilie Wirbel, Xavier Perrotton", "title": "Conditional Vehicle Trajectories Prediction in CARLA Urban Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning is becoming more and more successful for autonomous\ndriving. End-to-end (raw signal to command) performs well on relatively simple\ntasks (lane keeping and navigation). Mid-to-mid (environment abstraction to\nmid-level trajectory representation) or direct perception (raw signal to\nperformance) approaches strive to handle more complex, real life environment\nand tasks (e.g. complex intersection). In this work, we show that complex urban\nsituations can be handled with raw signal input and mid-level representation.\nWe build a hybrid end-to-mid approach predicting trajectories for neighbor\nvehicles and for the ego vehicle with a conditional navigation goal. We propose\nan original architecture inspired from social pooling LSTM taking low and mid\nlevel data as input and producing trajectories as polynomials of time. We\nintroduce a label augmentation mechanism to get the level of generalization\nthat is required to control a vehicle. The performance is evaluated on CARLA\n0.8 benchmark, showing significant improvements over previously published state\nof the art.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 16:41:24 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Buhet", "Thibault", ""], ["Wirbel", "Emilie", ""], ["Perrotton", "Xavier", ""]]}, {"id": "1909.00885", "submitter": "Khen Elimelech", "authors": "Khen Elimelech, Vadim Indelman", "title": "Simplified Decision Making in the Belief Space using Belief\n  Sparsification", "comments": "Submitted to the International Journal of Robotics Research (IJRR),\n  December 2018; conditionally accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.IT cs.RO cs.SY eess.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a new approach for the efficient solution of\nautonomous decision and planning problems, with a special focus on decision\nmaking under uncertainty and belief space planning (BSP) in high-dimensional\nstate spaces. Usually, to solve the decision problem, we identify the optimal\naction, according to some objective function. We claim that we can sometimes\ngenerate and solve an analogous yet simplified decision problem, which can be\nsolved more efficiently; a wise simplification method can lead to the same\naction selection, or one for which the maximal loss can be guaranteed.\nFurthermore, such simplification is separated from the state inference, and\ndoes not compromise its accuracy, as the selected action would finally be\napplied on the original state. First, we present the concept for general\ndecision problems, and provide a theoretical framework for a coherent\nformulation of the approach. We then practically apply these ideas to BSP\nproblems, which can be simplified by considering a sparse approximation of the\ninitial (Gaussian) belief. The scalable belief sparsification algorithm we\nprovide is able to yield solutions which are guaranteed to be consistent with\nthe original problem. We demonstrate the benefits of the approach in the\nsolution of a highly realistic active-SLAM problem, and manage to significantly\nreduce computation time, with practically no loss in the quality of solution.\nThis work is conceptual and fundamental, and holds numerous possible\nextensions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 23:00:59 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 07:30:30 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 19:01:30 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Elimelech", "Khen", ""], ["Indelman", "Vadim", ""]]}, {"id": "1909.00895", "submitter": "Boyi Liu", "authors": "Boyi Liu, Lujia Wang, Ming Liu and Cheng-Zhong Xu", "title": "Federated Imitation Learning: A Privacy Considered Imitation Learning\n  Framework for Cloud Robotic Systems with Heterogeneous Sensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are capable of learning a new behavior by observing others perform the\nskill. Robots can also implement this by imitation learning. Furthermore, if\nwith external guidance, humans will master the new behavior more efficiently.\nSo how can robots implement this? To address the issue, we present Federated\nImitation Learning (FIL) in the paper. Firstly, a knowledge fusion algorithm\ndeployed on the cloud for fusing knowledge from local robots is presented.\nThen, effective transfer learning methods in FIL are introduced. With FIL, a\nrobot is capable of utilizing knowledge from other robots to increase its\nimitation learning. FIL considers information privacy and data heterogeneity\nwhen robots share knowledge. It is suitable to be deployed in cloud robotic\nsystems. Finally, we conduct experiments of a simplified self-driving task for\nrobots (cars). The experimental results demonstrate that FIL is capable of\nincreasing imitation learning of local robots in cloud robotic systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 00:21:43 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 08:18:19 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Liu", "Boyi", ""], ["Wang", "Lujia", ""], ["Liu", "Ming", ""], ["Xu", "Cheng-Zhong", ""]]}, {"id": "1909.00923", "submitter": "Shengluan Hou", "authors": "Ruqian Lu and Shengluan Hou and Chuanqing Wang and Yu Huang and\n  Chaoqun Fei and Songmao Zhang", "title": "Attributed Rhetorical Structure Grammar for Domain Text Summarization", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach of automatic text summarization which\ncombines domain oriented text analysis (DoTA) and rhetorical structure theory\n(RST) in a grammar form: the attributed rhetorical structure grammar (ARSG),\nwhere the non-terminal symbols are domain keywords, called domain relations,\nwhile the rhetorical relations serve as attributes. We developed machine\nlearning algorithms for learning such a grammar from a corpus of sample domain\ntexts, as well as parsing algorithms for the learned grammar, together with\nadjustable text summarization algorithms for generating domain specific\nsummaries. Our practical experiments have shown that with support of domain\nknowledge the drawback of missing very large training data set can be\neffectively compensated. We have also shown that the knowledge based approach\nmay be made more powerful by introducing grammar parsing and RST as inference\nengine. For checking the feasibility of model transfer, we introduced a\ntechnique for mapping a grammar from one domain to others with acceptable cost.\nWe have also made a comprehensive comparison of our approach with some others.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 02:31:47 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Lu", "Ruqian", ""], ["Hou", "Shengluan", ""], ["Wang", "Chuanqing", ""], ["Huang", "Yu", ""], ["Fei", "Chaoqun", ""], ["Zhang", "Songmao", ""]]}, {"id": "1909.00934", "submitter": "Yuxiang Zhu", "authors": "Yuxiang Zhu, Minxue Pan, Yu Pei and Tian Zhang", "title": "A Bug or a Suggestion? An Automatic Way to Label Issues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more users and developers are using Issue Tracking Systems (ITSs) to\nreport issues, including bugs, feature requests, enhancement suggestions, etc.\nDifferent information, however, is gathered from users when issues are reported\non different ITSs, which presents considerable challenges for issue\nclassification tools to work effectively across the ITSs. Besides, bugs often\ntake higher priority when it comes to classifying the issues, while existing\napproaches to issue classification seldom focus on distinguishing bugs and the\nother non-bug issues, leading to suboptimal accuracy in bug identification.\n  In this paper, we propose a deep learning-based approach to automatically\nidentify bug-reporting issues across various ITSs. The approach implements the\nk-NN algorithm to detect and correct misclassifications in data extracted from\nthe ITSs, and trains an attention-based bi-directional long short-term memory\n(ABLSTM) network using a dataset of over 1.2 million labelled issues to\nidentify bug reports. Experimental evaluation shows that our approach achieved\nan F-measure of 85.6\\% in distinguishing bugs and other issues, significantly\noutperforming the other benchmark and state-of-the-art approaches examined in\nthe experiment.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 03:23:27 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhu", "Yuxiang", ""], ["Pan", "Minxue", ""], ["Pei", "Yu", ""], ["Zhang", "Tian", ""]]}, {"id": "1909.00982", "submitter": "Arpita Biswas", "authors": "Arpita Biswas, Siddharth Barman, Amit Deshpande, Amit Sharma", "title": "Quantifying Infra-Marginality and Its Trade-off with Group Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In critical decision-making scenarios, optimizing accuracy can lead to a\nbiased classifier, hence past work recommends enforcing group-based fairness\nmetrics in addition to maximizing accuracy. However, doing so exposes the\nclassifier to another kind of bias called infra-marginality. This refers to\nindividual-level bias where some individuals/subgroups can be worse off than\nunder simply optimizing for accuracy. For instance, a classifier implementing\nrace-based parity may significantly disadvantage women of the advantaged race.\nTo quantify this bias, we propose a general notion of $\\eta$-infra-marginality\nthat can be used to evaluate the extent of this bias. We prove theoretically\nthat, unlike other fairness metrics, infra-marginality does not have a\ntrade-off with accuracy: high accuracy directly leads to low infra-marginality.\nThis observation is confirmed through empirical analysis on multiple simulated\nand real-world datasets. Further, we find that maximizing group fairness often\nincreases infra-marginality, suggesting the consideration of both group-level\nfairness and individual-level infra-marginality. However, measuring\ninfra-marginality requires knowledge of the true distribution of\nindividual-level outcomes correctly and explicitly. We propose a practical\nmethod to measure infra-marginality, and a simple algorithm to maximize\ngroup-wise accuracy and avoid infra-marginality.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 07:24:35 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Biswas", "Arpita", ""], ["Barman", "Siddharth", ""], ["Deshpande", "Amit", ""], ["Sharma", "Amit", ""]]}, {"id": "1909.00991", "submitter": "Joel Robertson", "authors": "Joel Robertson", "title": "Modelling Bushfire Evacuation Behaviours", "comments": "84 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bushfires pose a significant threat to Australia's regional areas. To\nminimise risk and increase resilience, communities need robust evacuation\nstrategies that account for people's likely behaviour both before and during a\nbushfire. Agent-based modelling (ABM) offers a practical way to simulate a\nrange of bushfire evacuation scenarios. However, the ABM should reflect the\ndiversity of possible human responses in a given community. The\nBelief-Desire-Intention (BDI) cognitive model captures behaviour in a compact\nrepresentation that is understandable by domain experts. Within a BDI-ABM\nsimulation, individual BDI agents can be assigned profiles that determine their\nlikely behaviour. Over a population of agents their collective behaviour will\ncharacterise the community response. These profiles are drawn from existing\nhuman behaviour research and consultation with emergency services personnel and\ncapture the expected behaviours of identified groups in the population, both\nprior to and during an evacuation. A realistic representation of each community\ncan then be formed, and evacuation scenarios within the simulation can be used\nto explore the possible impact of population structure on outcomes. It is hoped\nthat this will give an improved understanding of the risks associated with\nevacuation, and lead to tailored evacuation plans for each community to help\nthem prepare for and respond to bushfire.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 08:07:27 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Robertson", "Joel", ""]]}, {"id": "1909.00997", "submitter": "Pritha Ganguly", "authors": "Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra and Pratyush Kumar", "title": "PlotQA: Reasoning over Scientific Plots", "comments": "This is an extension of our previous arxiv paper \"Data Interpretation\n  over Plots\" and it is to be presented at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing synthetic datasets (FigureQA, DVQA) for reasoning over plots do not\ncontain variability in data labels, real-valued data, or complex reasoning\nquestions. Consequently, proposed models for these datasets do not fully\naddress the challenge of reasoning over plots. In particular, they assume that\nthe answer comes either from a small fixed size vocabulary or from a bounding\nbox within the image. However, in practice, this is an unrealistic assumption\nbecause many questions require reasoning and thus have real-valued answers\nwhich appear neither in a small fixed size vocabulary nor in the image. In this\nwork, we aim to bridge this gap between existing datasets and real-world plots.\nSpecifically, we propose PlotQA with 28.9 million question-answer pairs over\n224,377 plots on data from real-world sources and questions based on\ncrowd-sourced question templates. Further, 80.76% of the out-of-vocabulary\n(OOV) questions in PlotQA have answers that are not in a fixed vocabulary.\nAnalysis of existing models on PlotQA reveals that they cannot deal with OOV\nquestions: their overall accuracy on our dataset is in single digits. This is\nnot surprising given that these models were not designed for such questions. As\na step towards a more holistic model which can address fixed vocabulary as well\nas OOV questions, we propose a hybrid approach: Specific questions are answered\nby choosing the answer from a fixed vocabulary or by extracting it from a\npredicted bounding box in the plot, while other questions are answered with a\ntable question-answering engine which is fed with a structured table generated\nby detecting visual elements from the image. On the existing DVQA dataset, our\nmodel has an accuracy of 58%, significantly improving on the highest reported\naccuracy of 46%. On PlotQA, our model has an accuracy of 22.52%, which is\nsignificantly better than state of the art models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 08:23:51 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 08:45:36 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2020 06:56:30 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Methani", "Nitesh", ""], ["Ganguly", "Pritha", ""], ["Khapra", "Mitesh M.", ""], ["Kumar", "Pratyush", ""]]}, {"id": "1909.01063", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang, Zhen Meng, Hai Zhao", "title": "A Smart Sliding Chinese Pinyin Input Method Editor on Touchscreen", "comments": "There are some insufficient explanations that may confuse readers. We\n  will continue the research, but it will take a lot of time. After discussing\n  with co-authors, we decide to withdraw this version from ArXiv, instead of\n  replacement. We may re-upload a new version of this work in the future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a smart sliding Chinese pinyin Input Method Editor (IME)\nfor touchscreen devices which allows user finger sliding from one key to\nanother on the touchscreen instead of tapping keys one by one, while the target\nChinese character sequence will be predicted during the sliding process to help\nuser input Chinese characters efficiently. Moreover, the layout of the virtual\nkeyboard of our IME adapts to user sliding for more efficient inputting. The\nlayout adaption process is utilized with Recurrent Neural Networks (RNN) and\ndeep reinforcement learning. The pinyin-to-character converter is implemented\nwith a sequence-to-sequence (Seq2Seq) model to predict the target Chinese\nsequence. A sliding simulator is built to automatically produce sliding samples\nfor model training and virtual keyboard test. The key advantage of our proposed\nIME is that nearly all its built-in tactics can be optimized automatically with\ndeep learning algorithms only following user behavior. Empirical studies verify\nthe effectiveness of the proposed model and show a better user input\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 11:04:19 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 02:10:38 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Meng", "Zhen", ""], ["Zhao", "Hai", ""]]}, {"id": "1909.01128", "submitter": "Tomi Janhunen", "authors": "Tomi Janhunen, Michael Sioutis", "title": "Allen's Interval Algebra Makes the Difference", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allen's Interval Algebra constitutes a framework for reasoning about temporal\ninformation in a qualitative manner. In particular, it uses intervals, i.e.,\npairs of endpoints, on the timeline to represent entities corresponding to\nactions, events, or tasks, and binary relations such as precedes and overlaps\nto encode the possible configurations between those entities. Allen's calculus\nhas found its way in many academic and industrial applications that involve,\nmost commonly, planning and scheduling, temporal databases, and healthcare. In\nthis paper, we present a novel encoding of Interval Algebra using answer-set\nprogramming (ASP) extended by difference constraints, i.e., the fragment\nabbreviated as ASP(DL), and demonstrate its performance via a preliminary\nexperimental evaluation. Although our ASP encoding is presented in the case of\nAllen's calculus for the sake of clarity, we suggest that analogous encodings\ncan be devised for other point-based calculi, too.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 12:56:15 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Janhunen", "Tomi", ""], ["Sioutis", "Michael", ""]]}, {"id": "1909.01136", "submitter": "Binbin Xu", "authors": "Binbin Xu and C\\'edric Gil-Jardin\\'e and Frantz Thiessard and Eric\n  Tellier and Marta Avalos and Emmanuel Lagarde", "title": "Pre-training A Neural Language Model Improves The Sample Efficiency of\n  an Emergency Room Classification Model", "comments": "Version of the published manuscript", "journal-ref": "The 33rd Florida Artificial Intelligence Research Society\n  Conference, 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To build a French national electronic injury surveillance system based on\nemergency room visits, we aim to develop a coding system to classify their\ncauses from clinical notes in free-text. Supervised learning techniques have\nshown good results in this area but require a large amount of expert annotated\ndataset which is time consuming and costly to obtain. We hypothesize that the\nNatural Language Processing Transformer model incorporating a generative\nself-supervised pre-training step can significantly reduce the required number\nof annotated samples for supervised fine-tuning. In this preliminary study, we\ntest our hypothesis in the simplified problem of predicting whether a visit is\nthe consequence of a traumatic event or not from free-text clinical notes.\nUsing fully re-trained GPT-2 models (without OpenAI pre-trained weights), we\nassess the gain of applying a self-supervised pre-training phase with unlabeled\nnotes prior to the supervised learning task. Results show that the number of\ndata required to achieve a ginve level of performance (AUC>0.95) was reduced by\na factor of 10 when applying pre-training. Namely, for 16 times more data, the\nfully-supervised model achieved an improvement <1% in AUC. To conclude, it is\npossible to adapt a multi-purpose neural language model such as the GPT-2 to\ncreate a powerful tool for classification of free-text notes with only a small\nnumber of labeled samples.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 17:25:06 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 16:16:29 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 14:47:26 GMT"}, {"version": "v4", "created": "Thu, 24 Oct 2019 12:19:14 GMT"}, {"version": "v5", "created": "Wed, 7 Apr 2021 09:33:10 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Xu", "Binbin", ""], ["Gil-Jardin\u00e9", "C\u00e9dric", ""], ["Thiessard", "Frantz", ""], ["Tellier", "Eric", ""], ["Avalos", "Marta", ""], ["Lagarde", "Emmanuel", ""]]}, {"id": "1909.01161", "submitter": "Yaqi Xie", "authors": "Yaqi Xie, Ziwei Xu, Mohan S. Kankanhalli, Kuldeep S. Meel, Harold Soh", "title": "Embedding Symbolic Knowledge into Deep Networks", "comments": "*Equal contribution; Accepted at conference Neural Information\n  Processing Systems (NeurIPS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim to leverage prior symbolic knowledge to improve the\nperformance of deep models. We propose a graph embedding network that projects\npropositional formulae (and assignments) onto a manifold via an augmented Graph\nConvolutional Network (GCN). To generate semantically-faithful embeddings, we\ndevelop techniques to recognize node heterogeneity, and semantic regularization\nthat incorporate structural constraints into the embedding. Experiments show\nthat our approach improves the performance of models trained to perform\nentailment checking and visual relation prediction. Interestingly, we observe a\nconnection between the tractability of the propositional theory representation\nand the ease of embedding. Future exploration of this connection may elucidate\nthe relationship between knowledge compilation and vector representation\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 13:23:25 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 12:38:40 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 10:26:34 GMT"}, {"version": "v4", "created": "Tue, 29 Oct 2019 10:53:02 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Xie", "Yaqi", ""], ["Xu", "Ziwei", ""], ["Kankanhalli", "Mohan S.", ""], ["Meel", "Kuldeep S.", ""], ["Soh", "Harold", ""]]}, {"id": "1909.01251", "submitter": "Guy W Cole", "authors": "Guy W. Cole and Sinead A. Williamson", "title": "Avoiding Resentment Via Monotonic Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers that achieve demographic balance by explicitly using protected\nattributes such as race or gender are often politically or culturally\ncontroversial due to their lack of individual fairness, i.e. individuals with\nsimilar qualifications will receive different outcomes. Individually and group\nfair decision criteria can produce counter-intuitive results, e.g. that the\noptimal constrained boundary may reject intuitively better candidates due to\ndemographic imbalance in similar candidates. Both approaches can be seen as\nintroducing individual resentment, where some individuals would have received a\nbetter outcome if they either belonged to a different demographic class and had\nthe same qualifications, or if they remained in the same class but had\nobjectively worse qualifications (e.g. lower test scores). We show that both\nforms of resentment can be avoided by using monotonically constrained machine\nlearning models to create individually fair, demographically balanced\nclassifiers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 15:28:16 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Cole", "Guy W.", ""], ["Williamson", "Sinead A.", ""]]}, {"id": "1909.01326", "submitter": "Emily Sheng", "authors": "Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng", "title": "The Woman Worked as a Babysitter: On Biases in Language Generation", "comments": "EMNLP 2019 short paper (5 pages); Updated references and examples,\n  changed figure 2 & 3 order, fixed grammar, results unmodified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a systematic study of biases in natural language generation (NLG)\nby analyzing text generated from prompts that contain mentions of different\ndemographic groups. In this work, we introduce the notion of the regard towards\na demographic, use the varying levels of regard towards different demographics\nas a defining metric for bias in NLG, and analyze the extent to which sentiment\nscores are a relevant proxy metric for regard. To this end, we collect\nstrategically-generated text from language models and manually annotate the\ntext with both sentiment and regard scores. Additionally, we build an automatic\nregard classifier through transfer learning, so that we can analyze biases in\nunseen text. Together, these methods reveal the extent of the biased nature of\nlanguage model generations. Our analysis provides a study of biases in NLG,\nbias metrics and correlated human judgments, and empirical evidence on the\nusefulness of our annotated dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 17:50:44 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 18:55:16 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Sheng", "Emily", ""], ["Chang", "Kai-Wei", ""], ["Natarajan", "Premkumar", ""], ["Peng", "Nanyun", ""]]}, {"id": "1909.01331", "submitter": "Suzan Ece Ada", "authors": "Suzan Ece Ada and Emre Ugur and H. Levent Akin", "title": "Generalization in Transfer Learning", "comments": "23 pages, 36 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents trained with deep reinforcement learning algorithms are capable of\nperforming highly complex tasks including locomotion in continuous\nenvironments. We investigate transferring the learning acquired in one task to\na set of previously unseen tasks. Generalization and overfitting in deep\nreinforcement learning are not commonly addressed in current transfer learning\nresearch. Conducting a comparative analysis without an intermediate\nregularization step results in underperforming benchmarks and inaccurate\nalgorithm comparisons due to rudimentary assessments. In this study, we propose\nregularization techniques in deep reinforcement learning for continuous control\nthrough the application of sample elimination, early stopping and maximum\nentropy regularized adversarial learning. First, the importance of the\ninclusion of training iteration number to the hyperparameters in deep transfer\nreinforcement learning will be discussed. Because source task performance is\nnot indicative of the generalization capacity of the algorithm, we start by\nacknowledging the training iteration number as a hyperparameter. In line with\nthis, we introduce an additional step of resorting to earlier snapshots of\npolicy parameters to prevent overfitting to the source task. Then, to generate\nrobust policies, we discard the samples that lead to overfitting via a method\nwe call strict clipping. Furthermore, we increase the generalization capacity\nin widely used transfer learning benchmarks by using maximum entropy\nregularization, different critic methods, and curriculum learning in an\nadversarial setup. Subsequently, we propose maximum entropy adversarial\nreinforcement learning to increase the domain randomization. Finally, we\nevaluate the robustness of these methods on simulated robots in target\nenvironments where the morphology of the robot, gravity, and tangential\nfriction coefficient of the environment are altered.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 17:57:07 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 07:48:49 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ada", "Suzan Ece", ""], ["Ugur", "Emre", ""], ["Akin", "H. Levent", ""]]}, {"id": "1909.01362", "submitter": "Jonathan Chang", "authors": "Jonathan P. Chang and Cristian Danescu-Niculescu-Mizil", "title": "Trouble on the Horizon: Forecasting the Derailment of Online\n  Conversations as they Develop", "comments": "To appear in Proceedings of EMNLP 2019. Data and code to be released\n  as part of the Cornell Conversational Analysis Toolkit (convokit.cornell.edu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online discussions often derail into toxic exchanges between participants.\nRecent efforts mostly focused on detecting antisocial behavior after the fact,\nby analyzing single comments in isolation. To provide more timely notice to\nhuman moderators, a system needs to preemptively detect that a conversation is\nheading towards derailment before it actually turns toxic. This means modeling\nderailment as an emerging property of a conversation rather than as an isolated\nutterance-level event.\n  Forecasting emerging conversational properties, however, poses several\ninherent modeling challenges. First, since conversations are dynamic, a\nforecasting model needs to capture the flow of the discussion, rather than\nproperties of individual comments. Second, real conversations have an unknown\nhorizon: they can end or derail at any time; thus a practical forecasting model\nneeds to assess the risk in an online fashion, as the conversation develops. In\nthis work we introduce a conversational forecasting model that learns an\nunsupervised representation of conversational dynamics and exploits it to\npredict future derailment as the conversation develops. By applying this model\nto two new diverse datasets of online conversations with labels for antisocial\nevents, we show that it outperforms state-of-the-art systems at forecasting\nderailment.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 18:00:05 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Chang", "Jonathan P.", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1909.01387", "submitter": "Tom Paine", "authors": "Tom Le Paine, Caglar Gulcehre, Bobak Shahriari, Misha Denil, Matt\n  Hoffman, Hubert Soyer, Richard Tanburn, Steven Kapturowski, Neil Rabinowitz,\n  Duncan Williams, Gabriel Barth-Maron, Ziyu Wang, Nando de Freitas, Worlds\n  Team", "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces R2D3, an agent that makes efficient use of\ndemonstrations to solve hard exploration problems in partially observable\nenvironments with highly variable initial conditions. We also introduce a suite\nof eight tasks that combine these three properties, and show that R2D3 can\nsolve several of the tasks where other state of the art methods (both with and\nwithout demonstrations) fail to see even a single successful trajectory after\ntens of billions of steps of exploration.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 18:20:48 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Paine", "Tom Le", ""], ["Gulcehre", "Caglar", ""], ["Shahriari", "Bobak", ""], ["Denil", "Misha", ""], ["Hoffman", "Matt", ""], ["Soyer", "Hubert", ""], ["Tanburn", "Richard", ""], ["Kapturowski", "Steven", ""], ["Rabinowitz", "Neil", ""], ["Williams", "Duncan", ""], ["Barth-Maron", "Gabriel", ""], ["Wang", "Ziyu", ""], ["de Freitas", "Nando", ""], ["Team", "Worlds", ""]]}, {"id": "1909.01388", "submitter": "Weiyan Shi", "authors": "Weiyan Shi, Kun Qian, Xuewei Wang and Zhou Yu", "title": "How to Build User Simulators to Train RL-based Dialog Systems", "comments": "Long Paper Accepted by EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User simulators are essential for training reinforcement learning (RL) based\ndialog models. The performance of the simulator directly impacts the RL policy.\nHowever, building a good user simulator that models real user behaviors is\nchallenging. We propose a method of standardizing user simulator building that\ncan be used by the community to compare dialog system quality using the same\nset of user simulators fairly. We present implementations of six user\nsimulators trained with different dialog planning and generation methods. We\nthen calculate a set of automatic metrics to evaluate the quality of these\nsimulators both directly and indirectly. We also ask human users to assess the\nsimulators directly and indirectly by rating the simulated dialogs and\ninteracting with the trained systems. This paper presents a comprehensive\nevaluation framework for user simulator study and provides a better\nunderstanding of the pros and cons of different user simulators, as well as\ntheir impacts on the trained systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 18:22:24 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Shi", "Weiyan", ""], ["Qian", "Kun", ""], ["Wang", "Xuewei", ""], ["Yu", "Zhou", ""]]}, {"id": "1909.01421", "submitter": "Niek Tax", "authors": "Niek Tax", "title": "Mining Insights from Weakly-Structured Event Data", "comments": "PhD thesis successfully defended and doctoral degree obtained at\n  Eindhoven University of Technology on 19th of June 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis focuses on process mining on event data where such a normative\nspecification is absent and, as a result, the event data is less structured.\nThe thesis puts special emphasis on one application domain that fits this\ndescription: the analysis of smart home data where sequences of daily\nactivities are recorded. In this thesis we propose a set of techniques to\nanalyze such data, which can be grouped into two categories of techniques. The\nfirst category of methods focuses on preprocessing event logs in order to\nenable process discovery techniques to extract insights into unstructured event\ndata. In this category we have developed the following techniques: - An\nunsupervised approach to refine event labels based on the time at which the\nevent took place, allowing for example to distinguish recorded eating events\ninto breakfast, lunch, and dinner. - An approach to detect and filter from\nevent logs so-called chaotic activities, which are activities that cause\nprocess discovery methods to overgeneralize. - A supervised approach to\nabstract low-level events into more high-level events, where we show that there\nexist situations where process discovery approaches overgeneralize on the\nlow-level event data but are able to find precise models on the high-level\nevent data. The second category focuses on mining local process models, i.e.,\ncollections of process model patterns that each describe some frequent pattern,\nin contrast to the single global process model that is obtained with existing\nprocess discovery techniques. Several techniques are introduced in the area of\nlocal process model mining, including a basic method, fast but approximate\nheuristic methods, and constraint-based techniques.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 19:43:37 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Tax", "Niek", ""]]}, {"id": "1909.01432", "submitter": "Kai Zhou", "authors": "Kai Zhou, Tomasz P. Michalak, and Yevgeniy Vorobeychik", "title": "Adversarial Robustness of Similarity-Based Link Prediction", "comments": "ICDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction is one of the fundamental problems in social network\nanalysis. A common set of techniques for link prediction rely on similarity\nmetrics which use the topology of the observed subnetwork to quantify the\nlikelihood of unobserved links. Recently, similarity metrics for link\nprediction have been shown to be vulnerable to attacks whereby observations\nabout the network are adversarially modified to hide target links. We propose a\nnovel approach for increasing robustness of similarity-based link prediction by\nendowing the analyst with a restricted set of reliable queries which accurately\nmeasure the existence of queried links. The analyst aims to robustly predict a\ncollection of possible links by optimally allocating the reliable queries. We\nformalize the analyst problem as a Bayesian Stackelberg game in which they\nfirst choose the reliable queries, followed by an adversary who deletes a\nsubset of links among the remaining (unreliable) queries by the analyst. The\nanalyst in our model is uncertain about the particular target link the\nadversary attempts to hide, whereas the adversary has full information about\nthe analyst and the network. Focusing on similarity metrics using only local\ninformation, we show that the problem is NP-Hard for both players, and devise\ntwo principled and efficient approaches for solving it approximately. Extensive\nexperiments with real and synthetic networks demonstrate the effectiveness of\nour approach.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 20:20:45 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Zhou", "Kai", ""], ["Michalak", "Tomasz P.", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1909.01500", "submitter": "Adam Stooke", "authors": "Adam Stooke and Pieter Abbeel", "title": "rlpyt: A Research Code Base for Deep Reinforcement Learning in PyTorch", "comments": "v2: Updated learning curves for SAC and TD3, improved by\n  bootstrapping value-function when trajectory ends due to time limit, and\n  switching to newer SAC version, now referenced", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the recent advent of deep reinforcement learning for game play and\nsimulated robotic control, a multitude of new algorithms have flourished. Most\nare model-free algorithms which can be categorized into three families: deep\nQ-learning, policy gradients, and Q-value policy gradients. These have\ndeveloped along separate lines of research, such that few, if any, code bases\nincorporate all three kinds. Yet these algorithms share a great depth of common\ndeep reinforcement learning machinery. We are pleased to share rlpyt, which\nimplements all three algorithm families on top of a shared, optimized\ninfrastructure, in a single repository. It contains modular implementations of\nmany common deep RL algorithms in Python using PyTorch, a leading deep learning\nlibrary. rlpyt is designed as a high-throughput code base for small- to\nmedium-scale research in deep RL. This white paper summarizes its features,\nalgorithms implemented, and relation to prior work, and concludes with detailed\nimplementation and usage notes. rlpyt is available at\nhttps://github.com/astooke/rlpyt.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 23:57:13 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 06:22:31 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Stooke", "Adam", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1909.01504", "submitter": "Arun Verma Mr.", "authors": "Arun Verma, Manjesh K. Hanawal, Arun Rajkumar, Raman Sankaran", "title": "Censored Semi-Bandits: A Framework for Resource Allocation with Censored\n  Feedback", "comments": "Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study censored Semi-Bandits, a novel variant of the\nsemi-bandits problem. The learner is assumed to have a fixed amount of\nresources, which it allocates to the arms at each time step. The loss observed\nfrom an arm is random and depends on the amount of resources allocated to it.\nMore specifically, the loss equals zero if the allocation for the arm exceeds a\nconstant (but unknown)threshold that can be dependent on the arm. Our goal is\nto learn a feasible allocation that minimizes the expected loss. The problem is\nchallenging because the loss distribution and threshold value of each arm are\nunknown. We study this novel setting by establishing its `equivalence' to\nMultiple-Play Multi-Armed Bandits(MP-MAB) and Combinatorial Semi-Bandits.\nExploiting these equivalences, we derive optimal algorithms for our setting\nusing existing algorithms for MP-MABand Combinatorial Semi-Bandits. Experiments\non synthetically generated data validate performance guarantees of the proposed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 00:25:31 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 23:54:18 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 07:56:28 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Verma", "Arun", ""], ["Hanawal", "Manjesh K.", ""], ["Rajkumar", "Arun", ""], ["Sankaran", "Raman", ""]]}, {"id": "1909.01561", "submitter": "Dileep George", "authors": "Dileep George", "title": "What can the brain teach us about building artificial intelligence?", "comments": null, "journal-ref": "Behavioral and Brain Sciences, volume 40, 2017", "doi": "10.1017/S0140525X17000140", "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the preprint of an invited commentary on Lake et al's\nBehavioral and Brain Sciences article titled \"Building machines that learn and\nthink like people\". Lake et al's paper offers a timely critique on the recent\naccomplishments in artificial intelligence from the vantage point of human\nintelligence, and provides insightful suggestions about research directions for\nbuilding more human-like intelligence. Since we agree with most of the points\nraised in that paper, we will offer a few points that are complementary.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 05:49:59 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["George", "Dileep", ""]]}, {"id": "1909.01567", "submitter": "Masashi Shimbo", "authors": "Katsuhiko Hayashi and Masashi Shimbo", "title": "A Non-commutative Bilinear Model for Answering Path Queries in Knowledge\n  Graphs", "comments": "Accepted for EMNLP-IJCNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilinear diagonal models for knowledge graph embedding (KGE), such as\nDistMult and ComplEx, balance expressiveness and computational efficiency by\nrepresenting relations as diagonal matrices. Although they perform well in\npredicting atomic relations, composite relations (relation paths) cannot be\nmodeled naturally by the product of relation matrices, as the product of\ndiagonal matrices is commutative and hence invariant with the order of\nrelations. In this paper, we propose a new bilinear KGE model, called\nBlockHolE, based on block circulant matrices. In BlockHolE, relation matrices\ncan be non-commutative, allowing composite relations to be modeled by matrix\nproduct. The model is parameterized in a way that covers a spectrum ranging\nfrom diagonal to full relation matrices. A fast computation technique is\ndeveloped on the basis of the duality of the Fourier transform of circulant\nmatrices.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 06:26:05 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Hayashi", "Katsuhiko", ""], ["Shimbo", "Masashi", ""]]}, {"id": "1909.01575", "submitter": "Jacob Rafati", "authors": "Jacob Rafati and David C. Noelle", "title": "Learning sparse representations in reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) algorithms allow artificial agents to improve\ntheir selection of actions to increase rewarding experiences in their\nenvironments. Temporal Difference (TD) Learning -- a model-free RL method -- is\na leading account of the midbrain dopamine system and the basal ganglia in\nreinforcement learning. These algorithms typically learn a mapping from the\nagent's current sensed state to a selected action (known as a policy function)\nvia learning a value function (expected future rewards). TD Learning methods\nhave been very successful on a broad range of control tasks, but learning can\nbecome intractably slow as the state space of the environment grows. This has\nmotivated methods that learn internal representations of the agent's state,\neffectively reducing the size of the state space and restructuring state\nrepresentations in order to support generalization. However, TD Learning\ncoupled with an artificial neural network, as a function approximator, has been\nshown to fail to learn some fairly simple control tasks, challenging this\nexplanation of reward-based learning. We hypothesize that such failures do not\narise in the brain because of the ubiquitous presence of lateral inhibition in\nthe cortex, producing sparse distributed internal representations that support\nthe learning of expected future reward. The sparse conjunctive representations\ncan avoid catastrophic interference while still supporting generalization. We\nprovide support for this conjecture through computational simulations,\ndemonstrating the benefits of learned sparse representations for three\nproblematic classic control tasks: Puddle-world, Mountain-car, and Acrobot.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 06:58:32 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Rafati", "Jacob", ""], ["Noelle", "David C.", ""]]}, {"id": "1909.01602", "submitter": "Andrea Giovanni Nuzzolese", "authors": "Paolo Ciancarini and Andrea Giovanni Nuzzolese and Valentina Presutti\n  and Daniel Russo", "title": "SQuAP-Ont: an Ontology of Software Quality Relational Factors from\n  Financial Systems", "comments": null, "journal-ref": null, "doi": "10.3233/SW-200372", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quality, architecture, and process are considered the keystones of software\nengineering. ISO defines them in three separate standards. However, their\ninteraction has been scarcely studied, so far. The SQuAP model (Software\nQuality, Architecture, Process) describes twenty-eight main factors that impact\non software quality in banking systems, and each factor is described as a\nrelation among some characteristics from the three ISO standards. Hence, SQuAP\nmakes such relations emerge rigorously, although informally. In this paper, we\npresent SQuAP-Ont, an OWL ontology designed by following a well-established\nmethodology based on the re-use of Ontology Design Patterns (i.e. ODPs).\nSQuAP-Ont formalises the relations emerging from SQuAP to represent and reason\nvia Linked Data about software engineering in a three-dimensional model\nconsisting of quality, architecture, and process ISO characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 07:57:54 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Ciancarini", "Paolo", ""], ["Nuzzolese", "Andrea Giovanni", ""], ["Presutti", "Valentina", ""], ["Russo", "Daniel", ""]]}, {"id": "1909.01610", "submitter": "Jacopo Staiano", "authors": "Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano", "title": "Answers Unite! Unsupervised Metrics for Reinforced Summarization Models", "comments": "Accepted at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstractive summarization approaches based on Reinforcement Learning (RL)\nhave recently been proposed to overcome classical likelihood maximization. RL\nenables to consider complex, possibly non-differentiable, metrics that globally\nassess the quality and relevance of the generated outputs. ROUGE, the most used\nsummarization metric, is known to suffer from bias towards lexical similarity\nas well as from suboptimal accounting for fluency and readability of the\ngenerated abstracts. We thus explore and propose alternative evaluation\nmeasures: the reported human-evaluation analysis shows that the proposed\nmetrics, based on Question Answering, favorably compares to ROUGE -- with the\nadditional property of not requiring reference summaries. Training a RL-based\nmodel on these metrics leads to improvements (both in terms of human or\nautomated metrics) over current approaches that use ROUGE as a reward.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 08:20:31 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Scialom", "Thomas", ""], ["Lamprier", "Sylvain", ""], ["Piwowarski", "Benjamin", ""], ["Staiano", "Jacopo", ""]]}, {"id": "1909.01645", "submitter": "Antonio Lieto", "authors": "Antonio Lieto", "title": "Heterogeneous Proxytypes Extended: Integrating Theory-like\n  Representations and Mechanisms with Prototypes and Exemplars", "comments": "10 pages, 1 figure, BICA Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces an extension of the proposal according to which\nconceptual representations in cognitive agents should be intended as\nheterogeneous proxytypes. The main contribution of this paper is in that it\ndetails how to reconcile, under a heterogeneous representational perspective,\ndifferent theories of typicality about conceptual representation and reasoning.\nIn particular, it provides a novel theoretical hypothesis - as well as a novel\ncategorization algorithm called DELTA - showing how to integrate the\nrepresentational and reasoning assumptions of the theory-theory of concepts\nwith the those ascribed to the prototype and exemplars-based theories.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 09:30:54 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Lieto", "Antonio", ""]]}, {"id": "1909.01646", "submitter": "Leonard Adolphs", "authors": "Leonard Adolphs and Thomas Hofmann", "title": "LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Reinforcement Learning (RL) approaches lead to significant achievements\nin a variety of areas in recent history, natural language tasks remained mostly\nunaffected, due to the compositional and combinatorial nature that makes them\nnotoriously hard to optimize. With the emerging field of Text-Based Games\n(TBGs), researchers try to bridge this gap. Inspired by the success of RL\nalgorithms on Atari games, the idea is to develop new methods in a restricted\ngame world and then gradually move to more complex environments. Previous work\nin the area of TBGs has mainly focused on solving individual games. We,\nhowever, consider the task of designing an agent that not just succeeds in a\nsingle game, but performs well across a whole family of games, sharing the same\ntheme. In this work, we present our deep RL agent--LeDeepChef--that shows\ngeneralization capabilities to never-before-seen games of the same family with\ndifferent environments and task descriptions. The agent participated in\nMicrosoft Research's \"First TextWorld Problems: A Language and Reinforcement\nLearning Challenge\" and outperformed all but one competitor on the final test\nset. The games from the challenge all share the same theme, namely cooking in a\nmodern house environment, but differ significantly in the arrangement of the\nrooms, the presented objects, and the specific goal (recipe to cook). To build\nan agent that achieves high scores across a whole family of games, we use an\nactor-critic framework and prune the action-space by using ideas from\nhierarchical reinforcement learning and a specialized module trained on a\nrecipe database.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 09:30:55 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Adolphs", "Leonard", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1909.01747", "submitter": "EPTCS", "authors": "Isabela Dr\\u{a}mnesc (Department of Computer Science West University\n  Timisoara, Romania), Tudor Jebelean (Research Institute for Symbolic\n  Computation, Johannes Kepler University, Linz, Austria)", "title": "Proof-Based Synthesis of Sorting Algorithms Using Multisets in Theorema", "comments": "In Proceedings FROM 2019, arXiv:1909.00584", "journal-ref": "EPTCS 303, 2019, pp. 76-91", "doi": "10.4204/EPTCS.303.6", "report-no": null, "categories": "cs.LO cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using multisets, we develop novel techniques for mechanizing the proofs of\nthe synthesis conjectures for list-sorting algorithms, and we demonstrate them\nin the Theorema system. We use the classical principle of extracting the\nalgorithm as a set of rewrite rules based on the witnesses found in the proof\nof the synthesis conjecture produced from the specification of the desired\nfunction (input and output conditions). The proofs are in natural style, using\nstandard rules, but most importantly domain specific inference rules and\nstrategies. In particular the use of multisets allows us to develop powerful\nstrategies for the synthesis of arbitrarily structured recursive algorithms by\ngeneral Noetherian induction, as well as for the automatic generation of the\nspecifications of all necessary auxiliary functions (insert, merge, split),\nwhose synthesis is performed using the same method.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 12:49:23 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Dr\u0103mnesc", "Isabela", "", "Department of Computer Science West University\n  Timisoara, Romania"], ["Jebelean", "Tudor", "", "Research Institute for Symbolic\n  Computation, Johannes Kepler University, Linz, Austria"]]}, {"id": "1909.01759", "submitter": "Manel Mart\\'inez-Ram\\'on", "authors": "Nestor Pereira, Miguel Angel Hombrados Herrera, Vanesssa\n  G\\'omez-Verdejo, Andrea A. Mammoli, Manel Mart\\'inez-Ram\\'on", "title": "Data Selection for Short Term load forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power load forecast with Machine Learning is a fairly mature application of\nartificial intelligence and it is indispensable in operation, control and\nplanning. Data selection techniqies have been hardly used in this application.\nHowever, the use of such techniques could be beneficial provided the assumption\nthat the data is identically distributed is clearly not true in load\nforecasting, but it is cyclostationary. In this work we present a fully\nautomatic methodology to determine what are the most adequate data to train a\npredictor which is based on a full Bayesian probabilistic model. We assess the\nperformance of the method with experiments based on real publicly available\ndata recorded from several years in the United States of America.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 21:51:48 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 19:48:59 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Pereira", "Nestor", ""], ["Herrera", "Miguel Angel Hombrados", ""], ["G\u00f3mez-Verdejo", "Vanesssa", ""], ["Mammoli", "Andrea A.", ""], ["Mart\u00ednez-Ram\u00f3n", "Manel", ""]]}, {"id": "1909.01779", "submitter": "Matthia Sabatelli", "authors": "Matthia Sabatelli, Gilles Louppe, Pierre Geurts, Marco A. Wiering", "title": "Approximating two value functions instead of one: towards characterizing\n  a new family of Deep Reinforcement Learning algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper makes one step forward towards characterizing a new family of\n\\textit{model-free} Deep Reinforcement Learning (DRL) algorithms. The aim of\nthese algorithms is to jointly learn an approximation of the state-value\nfunction ($V$), alongside an approximation of the state-action value function\n($Q$). Our analysis starts with a thorough study of the Deep Quality-Value\nLearning (DQV) algorithm, a DRL algorithm which has been shown to outperform\npopular techniques such as Deep-Q-Learning (DQN) and Double-Deep-Q-Learning\n(DDQN) \\cite{sabatelli2018deep}. Intending to investigate why DQV's learning\ndynamics allow this algorithm to perform so well, we formulate a set of\nresearch questions which help us characterize a new family of DRL algorithms.\nAmong our results, we present some specific cases in which DQV's performance\ncan get harmed and introduce a novel \\textit{off-policy} DRL algorithm, called\nDQV-Max, which can outperform DQV. We then study the behavior of the $V$ and\n$Q$ functions that are learned by DQV and DQV-Max and show that both algorithms\nmight perform so well on several DRL test-beds because they are less prone to\nsuffer from the overestimation bias of the $Q$ function.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 10:29:54 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 09:06:37 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Sabatelli", "Matthia", ""], ["Louppe", "Gilles", ""], ["Geurts", "Pierre", ""], ["Wiering", "Marco A.", ""]]}, {"id": "1909.01786", "submitter": "Andrea Formisano", "authors": "Agostino Dovier and Andrea Formisano and Flavio Vella", "title": "GPU-based parallelism for ASP-solving", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) has become, the paradigm of choice in the field\nof logic programming and non-monotonic reasoning. Thanks to the availability of\nefficient solvers, ASP has been successfully employed in a large number of\napplication domains. The term GPU-computing indicates a recent programming\nparadigm aimed at enabling the use of modern parallel Graphical Processing\nUnits (GPUs) for general purpose computing. In this paper we describe an\napproach to ASP-solving that exploits GPU parallelism. The design of a\nGPU-based solver poses various challenges due to the peculiarities of GPUs'\nsoftware and hardware architectures and to the intrinsic nature of the\nsatisfiability problem.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 13:28:37 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Dovier", "Agostino", ""], ["Formisano", "Andrea", ""], ["Vella", "Flavio", ""]]}, {"id": "1909.01788", "submitter": "Mikhail Prokopenko", "authors": "Mikhail Prokopenko and Peter Wang", "title": "Fractals2019: Combinatorial Optimisation with Dynamic Constraint\n  Annealing", "comments": "12 pages, 1 figure, RoboCup-2019, champion paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractals2019 started as a new experimental entry in the RoboCup Soccer 2D\nSimulation League, based on Gliders2d code base, and advanced to become a\nRoboCup-2019 champion. We employ combinatorial optimisation methods, within the\nframework of Guided Self-Organisation, with the search guided by local\nconstraints. We present examples of several tactical tasks based on the\nGliders2d code (version v2), including the search for an optimal assignment of\nheterogeneous player types, as well as blocking behaviours, offside trap, and\nattacking formations. We propose a new method, Dynamic Constraint Annealing,\nfor solving dynamic constraint satisfaction problems, and apply it to optimise\nthermodynamic potential of collective behaviours, under dynamically induced\nconstraints.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 13:31:24 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 03:08:06 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Prokopenko", "Mikhail", ""], ["Wang", "Peter", ""]]}, {"id": "1909.01789", "submitter": "Shuyan Wang", "authors": "Shuyan Wang", "title": "Unifying Causal Models with Trek Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific contexts, different investigators experiment with or\nobserve different variables with data from a domain in which the distinct\nvariable sets might well be related. This sort of fragmentation sometimes\noccurs in molecular biology, whether in studies of RNA expression or studies of\nprotein interaction, and it is common in the social sciences. Models are built\non the diverse data sets, but combining them can provide a more unified account\nof the causal processes in the domain. On the other hand, this problem is made\nchallenging by the fact that a variable in one data set may influence variables\nin another although neither data set contains all of the variables involved.\nSeveral authors have proposed using conditional independence properties of\nfragmentary (marginal) data collections to form unified causal explanations\nwhen it is assumed that the data have a common causal explanation but cannot be\nmerged to form a unified dataset. These methods typically return a large number\nof alternative causal models. The first part of the thesis shows that marginal\ndatasets contain extra information that can be used to reduce the number of\npossible models, in some cases yielding a unique model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 20:42:18 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Wang", "Shuyan", ""]]}, {"id": "1909.01794", "submitter": "Albert Harm Schrotenboer", "authors": "Albert H. Schrotenboer, Susanne Wruck, Iris F. A. Vis, Kees Jan\n  Roodbergen", "title": "Integration of returns and decomposition of customer orders in\n  e-commerce warehouses", "comments": "Authors' preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In picker-to-parts warehouses, order picking is a cost- and labor-intensive\noperation that must be designed efficiently. It comprises the construction of\norder batches and the associated order picker routes, and the assignment and\nsequencing of those batches to multiple order pickers. The ever-increasing\ncompetitiveness among e-commerce companies has made the joint optimization of\nthis order picking process inevitable. Inspired by the large number of product\nreturns and the many but small-sized customer orders, we address a new\nintegrated order picking process problem. We integrate the restocking of\nreturned products into regular order picking routes and we allow for the\ndecomposition of customer orders so that multiple batches may contain products\nfrom the same customer order. We thereby generalize the existing models on\norder picking processing. We provide Mixed Integer Programming (MIP)\nformulations and a tailored adaptive large neighborhood search heuristic that,\namongst others, exploits these MIPs. We propose a new set of practically-sized\nbenchmark instances, consisting of up to 5547 to be picked products and 2491 to\nbe restocked products. On those large-scale instances, we show that integrating\nthe restocking of returned products into regular order picker routes results in\ncost-savings of 10 to 15%. Allowing for the decomposition of the customer\norders' products results in cost savings of up to 44% compared to not allowing\nthis. Finally, we show that on average cost-savings of 17.4% can be obtained by\nusing our ALNS instead of heuristics typically used in practice.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 11:12:26 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Schrotenboer", "Albert H.", ""], ["Wruck", "Susanne", ""], ["Vis", "Iris F. A.", ""], ["Roodbergen", "Kees Jan", ""]]}, {"id": "1909.01801", "submitter": "Paul Kantor", "authors": "Paul B. Kantor", "title": "Soft Triangles for Expert Aggregation", "comments": "10 pp. 5 figures. 1 Table. Research Technical Report. This is really\n  about elicitation -- the MSC class is not a very good fit This revision\n  corrects and error in Eq 14, and adds one missing citation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of eliciting expert assessments of an uncertain\nparameter. The context is risk control, where there are, in fact, three\nuncertain parameters to be estimates. Two of these are probabilities, requiring\nthe that the experts be guided in the concept of \"uncertainty about\nuncertainty.\" We propose a novel formulation for expert estimates, which relies\non the range and the median, rather than the variance and the mean. We discuss\nthe process of elicitation, and provide precise formulas for these new\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 18:33:40 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 16:27:40 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kantor", "Paul B.", ""]]}, {"id": "1909.01860", "submitter": "Shiv Ram Dubey", "authors": "Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey, Snehasis Mukherjee", "title": "Visual Question Answering using Deep Learning: A Survey and Performance\n  Analysis", "comments": "Accepted in Fifth IAPR International Conference on Computer Vision\n  and Image Processing (CVIP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Visual Question Answering (VQA) task combines challenges for processing\ndata with both Visual and Linguistic processing, to answer basic `common sense'\nquestions about given images. Given an image and a question in natural\nlanguage, the VQA system tries to find the correct answer to it using visual\nelements of the image and inference gathered from textual questions. In this\nsurvey, we cover and discuss the recent datasets released in the VQA domain\ndealing with various types of question-formats and robustness of the\nmachine-learning models. Next, we discuss about new deep learning models that\nhave shown promising results over the VQA datasets. At the end, we present and\ndiscuss some of the results computed by us over the vanilla VQA model, Stacked\nAttention Network and the VQA Challenge 2017 winner model. We also provide the\ndetailed analysis along with the challenges and future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 07:03:03 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 01:11:29 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Srivastava", "Yash", ""], ["Murali", "Vaishnav", ""], ["Dubey", "Shiv Ram", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1909.01871", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen and Hal Daum\\'e III", "title": "Help, Anna! Visual Navigation with Natural Multimodal Assistance via\n  Retrospective Curiosity-Encouraging Imitation Learning", "comments": "In EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile agents that can leverage help from humans can potentially accomplish\nmore complex tasks than they could entirely on their own. We develop \"Help,\nAnna!\" (HANNA), an interactive photo-realistic simulator in which an agent\nfulfills object-finding tasks by requesting and interpreting natural\nlanguage-and-vision assistance. An agent solving tasks in a HANNA environment\ncan leverage simulated human assistants, called ANNA (Automatic Natural\nNavigation Assistants), which, upon request, provide natural language and\nvisual instructions to direct the agent towards the goals. To address the HANNA\nproblem, we develop a memory-augmented neural agent that hierarchically models\nmultiple levels of decision-making, and an imitation learning algorithm that\nteaches the agent to avoid repeating past mistakes while simultaneously\npredicting its own chances of making future progress. Empirically, our approach\nis able to ask for help more effectively than competitive baselines and, thus,\nattains higher task success rate on both previously seen and previously unseen\nenvironments. We publicly release code and data at\nhttps://github.com/khanhptnk/hanna . A video demo is available at\nhttps://youtu.be/18P94aaaLKg .\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 15:20:01 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 16:16:31 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 03:30:13 GMT"}, {"version": "v4", "created": "Tue, 8 Oct 2019 16:08:40 GMT"}, {"version": "v5", "created": "Mon, 21 Oct 2019 07:12:17 GMT"}, {"version": "v6", "created": "Fri, 22 Nov 2019 16:11:17 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Nguyen", "Khanh", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "1909.01891", "submitter": "Carole Sudre", "authors": "Carole H. Sudre, Beatriz Gomez Anson, Silvia Ingala, Chris D. Lane,\n  Daniel Jimenez, Lukas Haider, Thomas Varsavsky, Ryutaro Tanno, Lorna Smith,\n  S\\'ebastien Ourselin, Rolf H. J\\\"ager, M. Jorge Cardoso", "title": "Let's agree to disagree: learning highly debatable multirater labelling", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and differentiation of small pathological objects may greatly\nvary among human raters due to differences in training, expertise and their\nconsistency over time. In a radiological setting, objects commonly have high\nwithin-class appearance variability whilst sharing certain characteristics\nacross different classes, making their distinction even more difficult. As an\nexample, markers of cerebral small vessel disease, such as enlarged\nperivascular spaces (EPVS) and lacunes, can be very varied in their appearance\nwhile exhibiting high inter-class similarity, making this task highly\nchallenging for human raters. In this work, we investigate joint models of\nindividual rater behaviour and multirater consensus in a deep learning setting,\nand apply it to a brain lesion object-detection task. Results show that jointly\nmodelling both individual and consensus estimates leads to significant\nimprovements in performance when compared to directly predicting consensus\nlabels, while also allowing the characterization of human-rater consistency.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 15:40:14 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Sudre", "Carole H.", ""], ["Anson", "Beatriz Gomez", ""], ["Ingala", "Silvia", ""], ["Lane", "Chris D.", ""], ["Jimenez", "Daniel", ""], ["Haider", "Lukas", ""], ["Varsavsky", "Thomas", ""], ["Tanno", "Ryutaro", ""], ["Smith", "Lorna", ""], ["Ourselin", "S\u00e9bastien", ""], ["J\u00e4ger", "Rolf H.", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1909.01940", "submitter": "Eduardo Pooch", "authors": "Eduardo H. P. Pooch, Pedro L. Ballester, Rodrigo C. Barros", "title": "Can we trust deep learning models diagnosis? The impact of domain shift\n  in chest radiograph classification", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning models become more widespread, their ability to handle\nunseen data and generalize for any scenario is yet to be challenged. In medical\nimaging, there is a high heterogeneity of distributions among images based on\nthe equipment that generates them and their parametrization. This heterogeneity\ntriggers a common issue in machine learning called domain shift, which\nrepresents the difference between the training data distribution and the\ndistribution of where a model is employed. A high domain shift tends to\nimplicate in a poor generalization performance from the models. In this work,\nwe evaluate the extent of domain shift on four of the largest datasets of chest\nradiographs. We show how training and testing with different datasets (e.g.,\ntraining in ChestX-ray14 and testing in CheXpert) drastically affects model\nperformance, posing a big question over the reliability of deep learning models\ntrained on public datasets. We also show that models trained on CheXpert and\nMIMIC-CXR generalize better to other datasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:03:55 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 22:50:10 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Pooch", "Eduardo H. P.", ""], ["Ballester", "Pedro L.", ""], ["Barros", "Rodrigo C.", ""]]}, {"id": "1909.01958", "submitter": "Peter Clark", "authors": "Peter Clark, Oren Etzioni, Daniel Khashabi, Tushar Khot, Bhavana Dalvi\n  Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord,\n  Niket Tandon, Sumithra Bhakthavatsalam, Dirk Groeneveld, Michal Guerquin,\n  Michael Schmitz", "title": "From 'F' to 'A' on the N.Y. Regents Science Exams: An Overview of the\n  Aristo Project", "comments": "AI Magazine 41 (4) Winter 2020. New analysis sections added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AI has achieved remarkable mastery over games such as Chess, Go, and Poker,\nand even Jeopardy, but the rich variety of standardized exams has remained a\nlandmark challenge. Even in 2016, the best AI system achieved merely 59.3% on\nan 8th Grade science exam challenge. This paper reports unprecedented success\non the Grade 8 New York Regents Science Exam, where for the first time a system\nscores more than 90% on the exam's non-diagram, multiple choice (NDMC)\nquestions. In addition, our Aristo system, building upon the success of recent\nlanguage models, exceeded 83% on the corresponding Grade 12 Science Exam NDMC\nquestions. The results, on unseen test questions, are robust across different\ntest years and different variations of this kind of test. They demonstrate that\nmodern NLP methods can result in mastery on this task. While not a full\nsolution to general question-answering (the questions are multiple choice, and\nthe domain is restricted to 8th Grade science), it represents a significant\nmilestone for the field.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 17:33:42 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 20:51:37 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 00:42:53 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Clark", "Peter", ""], ["Etzioni", "Oren", ""], ["Khashabi", "Daniel", ""], ["Khot", "Tushar", ""], ["Mishra", "Bhavana Dalvi", ""], ["Richardson", "Kyle", ""], ["Sabharwal", "Ashish", ""], ["Schoenick", "Carissa", ""], ["Tafjord", "Oyvind", ""], ["Tandon", "Niket", ""], ["Bhakthavatsalam", "Sumithra", ""], ["Groeneveld", "Dirk", ""], ["Guerquin", "Michal", ""], ["Schmitz", "Michael", ""]]}, {"id": "1909.02027", "submitter": "Stefan Larson", "authors": "Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke,\n  Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A.\n  Laurenzano, Lingjia Tang, Jason Mars", "title": "An Evaluation Dataset for Intent Classification and Out-of-Scope\n  Prediction", "comments": "Accepted to EMNLP-IJCNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented dialog systems need to know when a query falls outside their\nrange of supported intents, but current text classification corpora only define\nlabel sets that cover every example. We introduce a new dataset that includes\nqueries that are out-of-scope---i.e., queries that do not fall into any of the\nsystem's supported intents. This poses a new challenge because models cannot\nassume that every query at inference time belongs to a system-supported intent\nclass. Our dataset also covers 150 intent classes over 10 domains, capturing\nthe breadth that a production task-oriented agent must handle. We evaluate a\nrange of benchmark classifiers on our dataset along with several different\nout-of-scope identification schemes. We find that while the classifiers perform\nwell on in-scope intent classification, they struggle to identify out-of-scope\nqueries. Our dataset and evaluation fill an important gap in the field,\noffering a way of more rigorously and realistically benchmarking text\nclassification in task-driven dialog systems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 18:04:56 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Larson", "Stefan", ""], ["Mahendran", "Anish", ""], ["Peper", "Joseph J.", ""], ["Clarke", "Christopher", ""], ["Lee", "Andrew", ""], ["Hill", "Parker", ""], ["Kummerfeld", "Jonathan K.", ""], ["Leach", "Kevin", ""], ["Laurenzano", "Michael A.", ""], ["Tang", "Lingjia", ""], ["Mars", "Jason", ""]]}, {"id": "1909.02059", "submitter": "Eva Sharma", "authors": "Eva Sharma, Luyang Huang, Zhe Hu and Lu Wang", "title": "An Entity-Driven Framework for Abstractive Summarization", "comments": "Proceedings of the 2019 Empirical Methods in Natural Language\n  Processing Conference and 9th International Joint Conference on Natural\n  Language Processing (EMNLP-IJCNLP-2019) (19 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstractive summarization systems aim to produce more coherent and concise\nsummaries than their extractive counterparts. Popular neural models have\nachieved impressive results for single-document summarization, yet their\noutputs are often incoherent and unfaithful to the input. In this paper, we\nintroduce SENECA, a novel System for ENtity-drivEn Coherent Abstractive\nsummarization framework that leverages entity information to generate\ninformative and coherent abstracts. Our framework takes a two-step approach:\n(1) an entity-aware content selection module first identifies salient sentences\nfrom the input, then (2) an abstract generation module conducts cross-sentence\ninformation compression and abstraction to generate the final summary, which is\ntrained with rewards to promote coherence, conciseness, and clarity. The two\ncomponents are further connected using reinforcement learning. Automatic\nevaluation shows that our model significantly outperforms previous\nstate-of-the-art on ROUGE and our proposed coherence measures on New York Times\nand CNN/Daily Mail datasets. Human judges further rate our system summaries as\nmore informative and coherent than those by popular summarization models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 19:07:29 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Sharma", "Eva", ""], ["Huang", "Luyang", ""], ["Hu", "Zhe", ""], ["Wang", "Lu", ""]]}, {"id": "1909.02128", "submitter": "Yuchen Lu", "authors": "Philip Paquette, Yuchen Lu, Steven Bocco, Max O. Smith, Satya\n  Ortiz-Gagne, Jonathan K. Kummerfeld, Satinder Singh, Joelle Pineau, Aaron\n  Courville", "title": "No Press Diplomacy: Modeling Multi-Agent Gameplay", "comments": "Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diplomacy is a seven-player non-stochastic, non-cooperative game, where\nagents acquire resources through a mix of teamwork and betrayal. Reliance on\ntrust and coordination makes Diplomacy the first non-cooperative multi-agent\nbenchmark for complex sequential social dilemmas in a rich environment. In this\nwork, we focus on training an agent that learns to play the No Press version of\nDiplomacy where there is no dedicated communication channel between players. We\npresent DipNet, a neural-network-based policy model for No Press Diplomacy. The\nmodel was trained on a new dataset of more than 150,000 human games. Our model\nis trained by supervised learning (SL) from expert trajectories, which is then\nused to initialize a reinforcement learning (RL) agent trained through\nself-play. Both the SL and RL agents demonstrate state-of-the-art No Press\nperformance by beating popular rule-based bots.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 21:48:04 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 19:32:29 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Paquette", "Philip", ""], ["Lu", "Yuchen", ""], ["Bocco", "Steven", ""], ["Smith", "Max O.", ""], ["Ortiz-Gagne", "Satya", ""], ["Kummerfeld", "Jonathan K.", ""], ["Singh", "Satinder", ""], ["Pineau", "Joelle", ""], ["Courville", "Aaron", ""]]}, {"id": "1909.02151", "submitter": "Bill Yuchen Lin", "authors": "Bill Yuchen Lin, Xinyue Chen, Jamin Chen, Xiang Ren", "title": "KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning", "comments": "11 pages, 4 figures, in Proc. of EMNLP-IJCNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense reasoning aims to empower machines with the human ability to make\npresumptions about ordinary situations in our daily life. In this paper, we\npropose a textual inference framework for answering commonsense questions,\nwhich effectively utilizes external, structured commonsense knowledge graphs to\nperform explainable inferences. The framework first grounds a question-answer\npair from the semantic space to the knowledge-based symbolic space as a schema\ngraph, a related sub-graph of external knowledge graphs. It represents schema\ngraphs with a novel knowledge-aware graph network module named KagNet, and\nfinally scores answers with graph representations. Our model is based on graph\nconvolutional networks and LSTMs, with a hierarchical path-based attention\nmechanism. The intermediate attention scores make it transparent and\ninterpretable, which thus produce trustworthy inferences. Using ConceptNet as\nthe only external resource for Bert-based models, we achieved state-of-the-art\nperformance on the CommonsenseQA, a large-scale dataset for commonsense\nreasoning.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 23:37:25 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Lin", "Bill Yuchen", ""], ["Chen", "Xinyue", ""], ["Chen", "Jamin", ""], ["Ren", "Xiang", ""]]}, {"id": "1909.02164", "submitter": "Wenhu Chen", "authors": "Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang,\n  Shiyang Li, Xiyou Zhou and William Yang Wang", "title": "TabFact: A Large-scale Dataset for Table-based Fact Verification", "comments": "Accepted to ICLR 2020, 17 pages, 15 figures. Main paper has 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of verifying whether a textual hypothesis holds based on the\ngiven evidence, also known as fact verification, plays an important role in the\nstudy of natural language understanding and semantic representation. However,\nexisting studies are mainly restricted to dealing with unstructured evidence\n(e.g., natural language sentences and documents, news, etc), while verification\nunder structured evidence, such as tables, graphs, and databases, remains\nunder-explored. This paper specifically aims to study the fact verification\ngiven semi-structured data as evidence. To this end, we construct a large-scale\ndataset called TabFact with 16k Wikipedia tables as the evidence for 118k\nhuman-annotated natural language statements, which are labeled as either\nENTAILED or REFUTED. TabFact is challenging since it involves both soft\nlinguistic reasoning and hard symbolic reasoning. To address these reasoning\nchallenges, we design two different models: Table-BERT and Latent Program\nAlgorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language\nmodel to encode the linearized tables and statements into continuous vectors\nfor verification. LPA parses statements into programs and executes them against\nthe tables to obtain the returned binary value for verification. Both methods\nachieve similar accuracy but still lag far behind human performance. We also\nperform a comprehensive analysis to demonstrate great future opportunities. The\ndata and code of the dataset are provided in\n\\url{https://github.com/wenhuchen/Table-Fact-Checking}.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 00:25:17 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 01:59:41 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 05:58:51 GMT"}, {"version": "v4", "created": "Tue, 31 Dec 2019 17:16:32 GMT"}, {"version": "v5", "created": "Sun, 14 Jun 2020 19:14:22 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Chen", "Wenhu", ""], ["Wang", "Hongmin", ""], ["Chen", "Jianshu", ""], ["Zhang", "Yunkai", ""], ["Wang", "Hong", ""], ["Li", "Shiyang", ""], ["Zhou", "Xiyou", ""], ["Wang", "William Yang", ""]]}, {"id": "1909.02195", "submitter": "Matthew Guzdial", "authors": "Shukan Shah, Matthew Guzdial, Mark O. Riedl", "title": "Automated Let's Play Commentary", "comments": "5 pages, 2 figures", "journal-ref": "Proceedings of the 2019 Experimental AI in Games Workshop", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let's Plays of video games represent a relatively unexplored area for\nexperimental AI in games. In this short paper, we discuss an approach to\ngenerate automated commentary for Let's Play videos, drawing on convolutional\ndeep neural networks. We focus on Let's Plays of the popular game Minecraft. We\ncompare our approach and a prior approach and demonstrate the generation of\nautomated, artificial commentary.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 03:30:26 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 20:40:28 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Shah", "Shukan", ""], ["Guzdial", "Matthew", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1909.02199", "submitter": "Alessandro Fontana", "authors": "Alessandro Fontana", "title": "Towards a general model for psychopathology", "comments": "40 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DSM-1 was published in 1952, contains 128 diagnostic categories,\ndescribed in 132 pages. The DSM-5 appeared in 2013, contains 541 diagnostic\ncategories, described in 947 pages. The field of psychology is characterised by\na steady proliferation of diagnostic models and subcategories, that seems to be\ninspired by the principle of \"divide and inflate\". This approach is in contrast\nwith experimental evidence, which suggests on one hand that traumas of various\nkind are often present in the anamnesis of patients and, on the other, that the\ngene variants implicated are shared across a wide range of diagnoses. In this\nwork I propose a holistic approach, built with tools borrowed from the field of\nArtificial Intelligence. My model is based on two pillars. The first one is\ntrauma, which represents the attack to the mind, is psychological in nature and\nhas its origin in the environment. The second pillar is dissociation, which\nrepresents the mind defence in both physiological and pathological conditions,\nand incorporates all other defence mechanisms. Damages to dissociation can be\nconsidered as another category of attacks, that are neurobiological in nature\nand can be of genetic or environmental origin. They include, among other\nfactors, synaptic over-pruning, abuse of drugs and inflammation. These factors\nconcur to weaken the defence, represented by the neural networks that implement\nthe dissociation mechanism in the brain. The model is subsequently used to\ninterpret five mental conditions: PTSD, complex PTSD, dissociative identity\ndisorder, schizophrenia and bipolar disorder. Ideally, this is a first step\ntowards building a model that aims to explain a wider range of\npsychopathological affections with a single theoretical framework. The last\npart is dedicated to sketching a new psychotherapy for psychological trauma.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 03:38:03 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Fontana", "Alessandro", ""]]}, {"id": "1909.02261", "submitter": "Olivier Goudet Dr", "authors": "Olivier Goudet, B\\'eatrice Duval, Jin-Kao Hao", "title": "Population-based Gradient Descent Weight Learning for Graph Coloring\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph coloring involves assigning colors to the vertices of a graph such that\ntwo vertices linked by an edge receive different colors. Graph coloring\nproblems are general models that are very useful to formulate many relevant\napplications and, however, are computationally difficult. In this work, a\ngeneral population-based weight learning framework for solving graph coloring\nproblems is presented. Unlike existing methods for graph coloring that are\nspecific to the considered problem, the presented work targets a generic\nobjective by introducing a unified method that can be applied to different\ngraph coloring problems. This work distinguishes itself by its solving approach\nthat formulates the search of a solution as a continuous weight tensor\noptimization problem and takes advantage of a gradient descent method computed\nin parallel on graphics processing units. The proposed approach is also\ncharacterized by its general global loss function that can easily be adapted to\ndifferent graph coloring problems. The usefulness of the proposed approach is\ndemonstrated by applying it to solve two typical graph coloring problems and\nperforming large computational studies on popular benchmarks. Improved\nbest-known results (new upper bounds) are reported for several large graphs.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 08:41:11 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 17:19:12 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 08:05:02 GMT"}, {"version": "v4", "created": "Mon, 26 Oct 2020 14:13:42 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Goudet", "Olivier", ""], ["Duval", "B\u00e9atrice", ""], ["Hao", "Jin-Kao", ""]]}, {"id": "1909.02291", "submitter": "Yu Chen", "authors": "Yu Chen and Yingfeng Chen and Zhipeng Hu and Tianpei Yang and Changjie\n  Fan and Yang Yu and Jianye Hao", "title": "Learning Action-Transferable Policy with Action Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning (TL) is a promising way to improve the sample efficiency of\nreinforcement learning. However, how to efficiently transfer knowledge across\ntasks with different state-action spaces is investigated at an early stage.\nMost previous studies only addressed the inconsistency across different state\nspaces by learning a common feature space, without considering that similar\nactions in different action spaces of related tasks share similar semantics. In\nthis paper, we propose a method to learning action embeddings by leveraging\nthis idea, and a framework that learns both state embeddings and action\nembeddings to transfer policy across tasks with different state and action\nspaces. Our experimental results on various tasks show that the proposed method\ncan not only learn informative action embeddings but accelerate policy\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 10:02:29 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 06:19:08 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 12:24:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Yu", ""], ["Chen", "Yingfeng", ""], ["Hu", "Zhipeng", ""], ["Yang", "Tianpei", ""], ["Fan", "Changjie", ""], ["Yu", "Yang", ""], ["Hao", "Jianye", ""]]}, {"id": "1909.02309", "submitter": "Dakuo Wang", "authors": "Dakuo Wang, Justin D. Weisz, Michael Muller, Parikshit Ram, Werner\n  Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, Alexander Gray", "title": "Human-AI Collaboration in Data Science: Exploring Data Scientists'\n  Perceptions of Automated AI", "comments": null, "journal-ref": null, "doi": "10.1145/3359313", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid advancement of artificial intelligence (AI) is changing our lives\nin many ways. One application domain is data science. New techniques in\nautomating the creation of AI, known as AutoAI or AutoML, aim to automate the\nwork practices of data scientists. AutoAI systems are capable of autonomously\ningesting and pre-processing data, engineering new features, and creating and\nscoring models based on a target objectives (e.g. accuracy or run-time\nefficiency). Though not yet widely adopted, we are interested in understanding\nhow AutoAI will impact the practice of data science. We conducted interviews\nwith 20 data scientists who work at a large, multinational technology company\nand practice data science in various business settings. Our goal is to\nunderstand their current work practices and how these practices might change\nwith AutoAI. Reactions were mixed: while informants expressed concerns about\nthe trend of automating their jobs, they also strongly felt it was inevitable.\nDespite these concerns, they remained optimistic about their future job\nsecurity due to a view that the future of data science work will be a\ncollaboration between humans and AI systems, in which both automation and human\nexpertise are indispensable.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 10:39:37 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Wang", "Dakuo", ""], ["Weisz", "Justin D.", ""], ["Muller", "Michael", ""], ["Ram", "Parikshit", ""], ["Geyer", "Werner", ""], ["Dugan", "Casey", ""], ["Tausczik", "Yla", ""], ["Samulowitz", "Horst", ""], ["Gray", "Alexander", ""]]}, {"id": "1909.02314", "submitter": "Javier \\'Alvez", "authors": "Javier \\'Alvez and Itziar Gonzalez-Dios and German Rigau", "title": "Commonsense Reasoning Using WordNet and SUMO: a Detailed Analysis", "comments": "9 pages, 2 figures, 2 tables; 10th Global WordNet Conference - GWC\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a detailed analysis of a sample of large benchmark of commonsense\nreasoning problems that has been automatically obtained from WordNet, SUMO and\ntheir mapping. The objective is to provide a better assessment of the quality\nof both the benchmark and the involved knowledge resources for advanced\ncommonsense reasoning tasks. By means of this analysis, we are able to detect\nsome knowledge misalignments, mapping errors and lack of knowledge and\nresources. Our final objective is the extraction of some guidelines towards a\nbetter exploitation of this commonsense knowledge framework by the improvement\nof the included resources.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 10:54:03 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 16:46:34 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["\u00c1lvez", "Javier", ""], ["Gonzalez-Dios", "Itziar", ""], ["Rigau", "German", ""]]}, {"id": "1909.02333", "submitter": "Tin Lai", "authors": "Tin Lai, Weiming Zhi, Fabio Ramos", "title": "Occ-Traj120: Occupancy Maps with Associated Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Trajectory modelling had been the principal research area for understanding\nand anticipating human behaviour. Predicting the dynamic path by observing the\nagent and its surrounding environment are essential for applications such as\nautonomous driving and indoor navigation suggestions. However, despite the\nnumerous researches that had been presented, most available dataset does not\ncontains any information on environmental factors---such as the occupancy\nrepresentation of the map---which arguably plays a significant role on how an\nagent chooses its trajectory.\n  We present a trajectory dataset with the corresponding occupancy\nrepresentations of different local-maps. The dataset contains more than 120\nlocally-structured maps with occupancy representation and more than 110K\ntrajectories in total. Each map has few hundred corresponding simulated\ntrajectories that navigate from a spatial location of a room to another point.\nThe dataset is freely available online.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 11:40:26 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 05:51:16 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lai", "Tin", ""], ["Zhi", "Weiming", ""], ["Ramos", "Fabio", ""]]}, {"id": "1909.02393", "submitter": "Wil van der Aalst", "authors": "Anja F. Syring and Niek Tax and Wil M.P. van der Aalst", "title": "Evaluating Conformance Measures in Process Mining using Conformance\n  Propositions (Extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining sheds new light on the relationship between process models and\nreal-life processes. Process discovery can be used to learn process models from\nevent logs. Conformance checking is concerned with quantifying the quality of a\nbusiness process model in relation to event data that was logged during the\nexecution of the business process. There exist different categories of\nconformance measures. Recall, also called fitness, is concerned with\nquantifying how much of the behavior that was observed in the event log fits\nthe process model. Precision is concerned with quantifying how much behavior a\nprocess model allows for that was never observed in the event log.\nGeneralization is concerned with quantifying how well a process model\ngeneralizes to behavior that is possible in the business process but was never\nobserved in the event log. Many recall, precision, and generalization measures\nhave been developed throughout the years, but they are often defined in an\nad-hoc manner without formally defining the desired properties up front. To\naddress these problems, we formulate 21 conformance propositions and we use\nthese propositions to evaluate current and existing conformance measures. The\ngoal is to trigger a discussion by clearly formulating the challenges and\nrequirements (rather than proposing new measures). Additionally, this paper\nserves as an overview of the conformance checking measures that are available\nin the process mining area.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 19:04:18 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Syring", "Anja F.", ""], ["Tax", "Niek", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1909.02553", "submitter": "Nathan Kallus", "authors": "Yichun Hu, Nathan Kallus, Xiaojie Mao", "title": "Smooth Contextual Bandits: Bridging the Parametric and\n  Non-differentiable Regret Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a nonparametric contextual bandit problem where the expected reward\nfunctions belong to a H\\\"older class with smoothness parameter $\\beta$. We show\nhow this interpolates between two extremes that were previously studied in\nisolation: non-differentiable bandits ($\\beta\\leq1$), where rate-optimal regret\nis achieved by running separate non-contextual bandits in different context\nregions, and parametric-response bandits (satisfying $\\beta=\\infty$), where\nrate-optimal regret can be achieved with minimal or no exploration due to\ninfinite extrapolatability. We develop a novel algorithm that carefully adjusts\nto all smoothness settings and we prove its regret is rate-optimal by\nestablishing matching upper and lower bounds, recovering the existing results\nat the two extremes. In this sense, our work bridges the gap between the\nexisting literature on parametric and non-differentiable contextual bandit\nproblems and between bandit algorithms that exclusively use global or local\ninformation, shedding light on the crucial interplay of complexity and regret\nin contextual bandits.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 17:51:14 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 13:12:08 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 14:36:21 GMT"}, {"version": "v4", "created": "Fri, 11 Sep 2020 12:55:27 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Hu", "Yichun", ""], ["Kallus", "Nathan", ""], ["Mao", "Xiaojie", ""]]}, {"id": "1909.02564", "submitter": "Jarom\\'ir Janisch", "authors": "Jarom\\'ir Janisch, Tom\\'a\\v{s} Pevn\\'y and Viliam Lis\\'y", "title": "Classification with Costly Features as a Sequential Decision-Making\n  Problem", "comments": null, "journal-ref": "Machine Learning (2020): 1-29", "doi": "10.1007/s10994-020-05874-8", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on a specific classification problem, where the information\nabout a sample is not readily available, but has to be acquired for a cost, and\nthere is a per-sample budget. Inspired by real-world use-cases, we analyze\naverage and hard variations of a directly specified budget. We postulate the\nproblem in its explicit formulation and then convert it into an equivalent MDP,\nthat can be solved with deep reinforcement learning. Also, we evaluate a\nreal-world inspired setting with sparse training dataset with missing features.\nThe presented method performs robustly well in all settings across several\ndistinct datasets, outperforming other prior-art algorithms. The method is\nflexible, as showcased with all mentioned modifications and can be improved\nwith any domain independent advancement in RL.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 14:46:40 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Janisch", "Jarom\u00edr", ""], ["Pevn\u00fd", "Tom\u00e1\u0161", ""], ["Lis\u00fd", "Viliam", ""]]}, {"id": "1909.02583", "submitter": "Xian Yeow Lee", "authors": "Xian Yeow Lee, Sambit Ghadai, Kai Liang Tan, Chinmay Hegde, Soumik\n  Sarkar", "title": "Spatiotemporally Constrained Action Space Attacks on Deep Reinforcement\n  Learning Agents", "comments": "Version 2 with supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness of Deep Reinforcement Learning (DRL) algorithms towards\nadversarial attacks in real world applications such as those deployed in\ncyber-physical systems (CPS) are of increasing concern. Numerous studies have\ninvestigated the mechanisms of attacks on the RL agent's state space.\nNonetheless, attacks on the RL agent's action space (AS) (corresponding to\nactuators in engineering systems) are equally perverse; such attacks are\nrelatively less studied in the ML literature. In this work, we first frame the\nproblem as an optimization problem of minimizing the cumulative reward of an RL\nagent with decoupled constraints as the budget of attack. We propose a\nwhite-box Myopic Action Space (MAS) attack algorithm that distributes the\nattacks across the action space dimensions. Next, we reformulate the\noptimization problem above with the same objective function, but with a\ntemporally coupled constraint on the attack budget to take into account the\napproximated dynamics of the agent. This leads to the white-box Look-ahead\nAction Space (LAS) attack algorithm that distributes the attacks across the\naction and temporal dimensions. Our results shows that using the same amount of\nresources, the LAS attack deteriorates the agent's performance significantly\nmore than the MAS attack. This reveals the possibility that with limited\nresource, an adversary can utilize the agent's dynamics to malevolently craft\nattacks that causes the agent to fail. Additionally, we leverage these attack\nstrategies as a possible tool to gain insights on the potential vulnerabilities\nof DRL agents.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 18:04:04 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 03:36:57 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Lee", "Xian Yeow", ""], ["Ghadai", "Sambit", ""], ["Tan", "Kai Liang", ""], ["Hegde", "Chinmay", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1909.02789", "submitter": "Martin Golumbic", "authors": "Boi Faltings and Martin Charles Golumbic", "title": "An Effective Upperbound on Treewidth Using Partial Fill-in of Separators", "comments": "Unpublished Research Report: January 22, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.AI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partitioning a graph using graph separators, and particularly clique\nseparators, are well-known techniques to decompose a graph into smaller units\nwhich can be treated independently. It was previously known that the treewidth\nwas bounded above by the sum of the size of the separator plus the treewidth of\ndisjoint components, and this was obtained by the heuristic of filling in all\nedges of the separator making it into a clique.\n  In this paper, we present a new, tighter upper bound on the treewidth of a\ngraph obtained by only partially filling in the edges of a separator. In\nparticular, the method completes just those pairs of separator vertices that\nare adjacent to a common component, and indicates a more effective heuristic\nthan filling in the entire separator. We discuss the relevance of this result\nfor combinatorial algorithms and give an example of how the tighter bound can\nbe exploited in the domain of constraint satisfaction problems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 09:23:27 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Faltings", "Boi", ""], ["Golumbic", "Martin Charles", ""]]}, {"id": "1909.02790", "submitter": "Weixun Wang", "authors": "Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing\n  Hu, Yingfeng Chen, Changjie Fan, Yang Gao", "title": "From Few to More: Large-scale Dynamic Multiagent Curriculum Learning", "comments": "Accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of efforts have been devoted to investigating how agents can learn\neffectively and achieve coordination in multiagent systems. However, it is\nstill challenging in large-scale multiagent settings due to the complex\ndynamics between the environment and agents and the explosion of state-action\nspace. In this paper, we design a novel Dynamic Multiagent Curriculum Learning\n(DyMA-CL) to solve large-scale problems by starting from learning on a\nmultiagent scenario with a small size and progressively increasing the number\nof agents. We propose three transfer mechanisms across curricula to accelerate\nthe learning process. Moreover, due to the fact that the state dimension varies\nacross curricula,, and existing network structures cannot be applied in such a\ntransfer setting since their network input sizes are fixed. Therefore, we\ndesign a novel network structure called Dynamic Agent-number Network (DyAN) to\nhandle the dynamic size of the network input. Experimental results show that\nDyMA-CL using DyAN greatly improves the performance of large-scale multiagent\nlearning compared with state-of-the-art deep reinforcement learning approaches.\nWe also investigate the influence of three transfer mechanisms across curricula\nthrough extensive simulations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 09:26:05 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 02:48:33 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Wang", "Weixun", ""], ["Yang", "Tianpei", ""], ["Liu", "Yong", ""], ["Hao", "Jianye", ""], ["Hao", "Xiaotian", ""], ["Hu", "Yujing", ""], ["Chen", "Yingfeng", ""], ["Fan", "Changjie", ""], ["Gao", "Yang", ""]]}, {"id": "1909.02809", "submitter": "Gerasimos Spanakis", "authors": "Tobias Bauer and Emre Devrim and Misha Glazunov and William Lopez\n  Jaramillo and Balaganesh Mohan and Gerasimos Spanakis", "title": "#MeTooMaastricht: Building a chatbot to assist survivors of sexual\n  harassment", "comments": "19 pages, accepted at SoGood2019 workshop (ECMLPKDD2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent social movement of #MeToo, we are building a chatbot\nto assist survivors of sexual harassment cases (designed for the city of\nMaastricht but can easily be extended). The motivation behind this work is\ntwofold: properly assist survivors of such events by directing them to\nappropriate institutions that can offer them help and increase the incident\ndocumentation so as to gather more data about harassment cases which are\ncurrently under reported. We break down the problem into three data\nscience/machine learning components: harassment type identification (treated as\na classification problem), spatio-temporal information extraction (treated as\nNamed Entity Recognition problem) and dialogue with the users (treated as a\nslot-filling based chatbot). We are able to achieve a success rate of more than\n98% for the identification of a harassment-or-not case and around 80% for the\nspecific type harassment identification. Locations and dates are identified\nwith more than 90% accuracy and time occurrences prove more challenging with\nalmost 80%. Finally, initial validation of the chatbot shows great potential\nfor the further development and deployment of such a beneficial for the whole\nsociety tool.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 10:36:33 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Bauer", "Tobias", ""], ["Devrim", "Emre", ""], ["Glazunov", "Misha", ""], ["Jaramillo", "William Lopez", ""], ["Mohan", "Balaganesh", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "1909.02810", "submitter": "Henry Prakken", "authors": "Alejandro J. Garcia, Henry Prakken, Guillermo R. Simari", "title": "A Comparative Study of Some Central Notions of ASPIC+ and DeLP", "comments": "To appear in Theory and Practice of Logic Programming (TPLP). In the\n  second uploaded version a small typo was corrected in Example 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper formally compares some central notions from two well-known\nformalisms for rule-based argumentation, DeLP and ASPIC+. The comparisons\nespecially focus on intuitive adequacy and inter-translatability, consistency,\nand closure properties. As for differences in the definitions of arguments and\nattack, it turns out that DeLP's definitions are intuitively appealing but that\nthey may not fully comply with Caminada and Amgoud's rationality postulates of\nstrict closure and indirect consistency. For some special cases, the DeLP\ndefinitions are shown to fare better than ASPIC+. Next, it is argued that there\nare reasons to consider a variant of DeLP with grounded semantics, since in\nsome examples its current notion of warrant arguably has counterintuitive\nconsequences and may lead to sets of warranted arguments that are not\nadmissible. Finally, under some minimality and consistency assumptions on\nASPIC+ arguments, a one-to-many correspondence between ASPIC+ arguments and\nDeLP arguments is identified in such a way that if the DeLP warranting\nprocedure is changed to grounded semantics, then DeLP notion of warrant and\nASPIC+'s notion of justification are equivalent. This result is proven for\nthree alternative definitions of attack.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 10:37:48 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 06:43:01 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Garcia", "Alejandro J.", ""], ["Prakken", "Henry", ""], ["Simari", "Guillermo R.", ""]]}, {"id": "1909.02930", "submitter": "Michael Cochez", "authors": "Ruijie Wang, Meng Wang, Jun Liu, Michael Cochez, Stefan Decker", "title": "Structured Query Construction via Knowledge Graph Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to facilitate the accesses of general users to knowledge graphs, an\nincreasing effort is being exerted to construct graph-structured queries of\ngiven natural language questions. At the core of the construction is to deduce\nthe structure of the target query and determine the vertices/edges which\nconstitute the query. Existing query construction methods rely on question\nunderstanding and conventional graph-based algorithms which lead to inefficient\nand degraded performances facing complex natural language questions over\nknowledge graphs with large scales. In this paper, we focus on this problem and\npropose a novel framework standing on recent knowledge graph embedding\ntechniques. Our framework first encodes the underlying knowledge graph into a\nlow-dimensional embedding space by leveraging generalized local knowledge\ngraphs. Given a natural language question, the learned embedding\nrepresentations of the knowledge graph are utilized to compute the query\nstructure and assemble vertices/edges into the target query. Extensive\nexperiments were conducted on the benchmark dataset, and the results\ndemonstrate that our framework outperforms state-of-the-art baseline models\nregarding effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 14:29:00 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Wang", "Ruijie", ""], ["Wang", "Meng", ""], ["Liu", "Jun", ""], ["Cochez", "Michael", ""], ["Decker", "Stefan", ""]]}, {"id": "1909.02940", "submitter": "Vaneet Aggarwal", "authors": "Mridul Agarwal and Vaneet Aggarwal", "title": "Reinforcement Learning for Joint Optimization of Multiple Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT cs.IT cs.MA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) algorithms such as DQN owe their success to\nMarkov Decision Processes, and the fact that maximizing the sum of rewards\nallows using backward induction and reduce to the Bellman optimality equation.\nHowever, many real-world problems require optimization of an objective that is\nnon-linear in cumulative rewards for which dynamic programming cannot be\napplied directly. For example, in a resource allocation problem, one of the\nobjectives is to maximize long-term fairness among the users. We notice that\nwhen the function of the sum of rewards is considered, the problem loses its\nMarkov nature. This paper addresses and formalizes the problem of optimizing a\nnon-linear function of the long term average of rewards. We propose model-based\nand model-free algorithms to learn the policy, where the model-based policy is\nshown to achieve a regret of $\\Tilde{O}\\left(KDSA\\sqrt{\\frac{A}{T}}\\right)$ for\n$K$ users. Further, using the fairness in cellular base-station scheduling, and\nqueueing system scheduling as examples, the proposed algorithm is shown to\nsignificantly outperform the conventional RL approaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 14:48:07 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 20:42:51 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 05:10:01 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Agarwal", "Mridul", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1909.02964", "submitter": "Roxana R\\u{a}dulescu", "authors": "Roxana R\\u{a}dulescu, Patrick Mannion, Diederik M. Roijers, Ann Now\\'e", "title": "Multi-Objective Multi-Agent Decision Making: A Utility-based Analysis\n  and Survey", "comments": "Under review since 15 May 2019", "journal-ref": null, "doi": "10.1007/s10458-019-09433-x", "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of multi-agent system (MAS) implementations aim to optimise\nagents' policies with respect to a single objective, despite the fact that many\nreal-world problem domains are inherently multi-objective in nature.\nMulti-objective multi-agent systems (MOMAS) explicitly consider the possible\ntrade-offs between conflicting objective functions. We argue that, in MOMAS,\nsuch compromises should be analysed on the basis of the utility that these\ncompromises have for the users of a system. As is standard in multi-objective\noptimisation, we model the user utility using utility functions that map value\nor return vectors to scalar values. This approach naturally leads to two\ndifferent optimisation criteria: expected scalarised returns (ESR) and\nscalarised expected returns (SER). We develop a new taxonomy which classifies\nmulti-objective multi-agent decision making settings, on the basis of the\nreward structures, and which and how utility functions are applied. This allows\nus to offer a structured view of the field, to clearly delineate the current\nstate-of-the-art in multi-objective multi-agent decision making approaches and\nto identify promising directions for future research. Starting from the\nexecution phase, in which the selected policies are applied and the utility for\nthe users is attained, we analyse which solution concepts apply to the\ndifferent settings in our taxonomy. Furthermore, we define and discuss these\nsolution concepts under both ESR and SER optimisation criteria. We conclude\nwith a summary of our main findings and a discussion of many promising future\nresearch directions in multi-objective multi-agent systems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 15:09:31 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["R\u0103dulescu", "Roxana", ""], ["Mannion", "Patrick", ""], ["Roijers", "Diederik M.", ""], ["Now\u00e9", "Ann", ""]]}, {"id": "1909.02965", "submitter": "Simon Keizer", "authors": "Simon Keizer, Ond\\v{r}ej Du\\v{s}ek, Xingkun Liu, Verena Rieser", "title": "User Evaluation of a Multi-dimensional Statistical Dialogue System", "comments": "SIGdial 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first complete spoken dialogue system driven by a\nmulti-dimensional statistical dialogue manager. This framework has been shown\nto substantially reduce data needs by leveraging domain-independent dimensions,\nsuch as social obligations or feedback, which (as we show) can be transferred\nbetween domains. In this paper, we conduct a user study and show that the\nperformance of a multi-dimensional system, which can be adapted from a source\ndomain, is equivalent to that of a one-dimensional baseline, which can only be\ntrained from scratch.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 15:10:37 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Keizer", "Simon", ""], ["Du\u0161ek", "Ond\u0159ej", ""], ["Liu", "Xingkun", ""], ["Rieser", "Verena", ""]]}, {"id": "1909.02982", "submitter": "Theo Jaunet", "authors": "Theo Jaunet, Romain Vuillemot and Christian Wolf", "title": "DRLViz: Understanding Decisions and Memory in Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DRLViz, a visual analytics interface to interpret the internal\nmemory of an agent (e.g. a robot) trained using deep reinforcement learning.\nThis memory is composed of large temporal vectors updated when the agent moves\nin an environment and is not trivial to understand due to the number of\ndimensions, dependencies to past vectors, spatial/temporal correlations, and\nco-correlation between dimensions. It is often referred to as a black box as\nonly inputs (images) and outputs (actions) are intelligible for humans. Using\nDRLViz, experts are assisted to interpret decisions using memory reduction\ninteractions, and to investigate the role of parts of the memory when errors\nhave been made (e.g. wrong direction). We report on DRLViz applied in the\ncontext of video games simulators (ViZDoom) for a navigation scenario with item\ngathering tasks. We also report on experts evaluation using DRLViz, and\napplicability of DRLViz to other scenarios and navigation problems beyond\nsimulation games, as well as its contribution to black box models\ninterpretability and explainability in the field of visual analytics.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 15:56:39 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 16:07:35 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Jaunet", "Theo", ""], ["Vuillemot", "Romain", ""], ["Wolf", "Christian", ""]]}, {"id": "1909.03012", "submitter": "Amit Dhurandhar", "authors": "Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar,\n  Michael Hind, Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss,\n  Aleksandra Mojsilovi\\'c, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra,\n  John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh,\n  Kush R. Varshney, Dennis Wei and Yunfeng Zhang", "title": "One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI\n  Explainability Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As artificial intelligence and machine learning algorithms make further\ninroads into society, calls are increasing from multiple stakeholders for these\nalgorithms to explain their outputs. At the same time, these stakeholders,\nwhether they be affected citizens, government regulators, domain experts, or\nsystem developers, present different requirements for explanations. Toward\naddressing these needs, we introduce AI Explainability 360\n(http://aix360.mybluemix.net/), an open-source software toolkit featuring eight\ndiverse and state-of-the-art explainability methods and two evaluation metrics.\nEqually important, we provide a taxonomy to help entities requiring\nexplanations to navigate the space of explanation methods, not only those in\nthe toolkit but also in the broader literature on explainability. For data\nscientists and other users of the toolkit, we have implemented an extensible\nsoftware architecture that organizes methods according to their place in the AI\nmodeling pipeline. We also discuss enhancements to bring research innovations\ncloser to consumers of explanations, ranging from simplified, more accessible\nversions of algorithms, to tutorials and an interactive web demo to introduce\nAI explainability to different audiences and application domains. Together, our\ntoolkit and taxonomy can help identify gaps where more explainability methods\nare needed and provide a platform to incorporate them as they are developed.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:53:01 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 15:08:57 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Arya", "Vijay", ""], ["Bellamy", "Rachel K. E.", ""], ["Chen", "Pin-Yu", ""], ["Dhurandhar", "Amit", ""], ["Hind", "Michael", ""], ["Hoffman", "Samuel C.", ""], ["Houde", "Stephanie", ""], ["Liao", "Q. Vera", ""], ["Luss", "Ronny", ""], ["Mojsilovi\u0107", "Aleksandra", ""], ["Mourad", "Sami", ""], ["Pedemonte", "Pablo", ""], ["Raghavendra", "Ramya", ""], ["Richards", "John", ""], ["Sattigeri", "Prasanna", ""], ["Shanmugam", "Karthikeyan", ""], ["Singh", "Moninder", ""], ["Varshney", "Kush R.", ""], ["Wei", "Dennis", ""], ["Zhang", "Yunfeng", ""]]}, {"id": "1909.03019", "submitter": "Valentin Robu PhD", "authors": "Xingyu Zhao, Matt Osborne, Jenny Lantair, Valentin Robu, David Flynn,\n  Xiaowei Huang, Michael Fisher, Fabio Papacchini, Angelo Ferrando", "title": "Towards Integrating Formal Verification of Autonomous Robots with\n  Battery Prognostics and Health Management", "comments": null, "journal-ref": "Proceedings of 17th International Conference on Software\n  Engineering and Formal Methods (SEFM 2019), Oslo, Norway (September 2019)", "doi": "10.1007/978-3-030-30446-1_6", "report-no": null, "categories": "cs.AI cs.RO cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The battery is a key component of autonomous robots. Its performance limits\nthe robot's safety and reliability. Unlike liquid-fuel, a battery, as a\nchemical device, exhibits complicated features, including (i) capacity fade\nover successive recharges and (ii) increasing discharge rate as the state of\ncharge (SOC) goes down for a given power demand. Existing formal verification\nstudies of autonomous robots, when considering energy constraints, formalise\nthe energy component in a generic manner such that the battery features are\noverlooked. In this paper, we model an unmanned aerial vehicle (UAV) inspection\nmission on a wind farm and via probabilistic model checking in PRISM show (i)\nhow the battery features may affect the verification results significantly in\npractical cases; and (ii) how the battery features, together with dynamic\nenvironments and battery safety strategies, jointly affect the verification\nresults. Potential solutions to explicitly integrate battery prognostics and\nhealth management (PHM) with formal verification of autonomous robots are also\ndiscussed to motivate future work.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 13:39:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhao", "Xingyu", ""], ["Osborne", "Matt", ""], ["Lantair", "Jenny", ""], ["Robu", "Valentin", ""], ["Flynn", "David", ""], ["Huang", "Xiaowei", ""], ["Fisher", "Michael", ""], ["Papacchini", "Fabio", ""], ["Ferrando", "Angelo", ""]]}, {"id": "1909.03026", "submitter": "Jonas Traub", "authors": "Jonas Traub, Jorge-Arnulfo Quian\\'e-Ruiz, Zoi Kaoudi, Volker Markl\n  (Technische Universit\\\"at Berlin, German Research Center for Artificial\n  Intelligence (DFKI))", "title": "Agora: A Unified Asset Ecosystem Going Beyond Marketplaces and Cloud\n  Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data, algorithms, and compute/storage infrastructure are key assets that\ndrive data science and artificial intelligence applications. As providing all\nthese assets requires a huge investment, data science and artificial\nintelligence technologies are currently dominated by a small number of\nproviders who can afford these investments. This leads to lock-in effects and\nhinders features that require a flexible exchange of assets among users. In\nthis vision paper, we present Agora, a unified asset ecosystem. The Agora\nsystem provides the technical infrastructure that allows for offering and using\ndata and algorithms, as well as physical infrastructure components. Agora is\ndesigned as an open ecosystem of asset marketplaces and provides to a broad\naudience not only data but the entire data value chain (including computational\nresources and human expertise). Agora (i) leverages a fine-grained exchange of\nassets, (ii) allows for combining assets to novel applications, and (iii)\nflexibly executes such applications on available resources. As a result, Agora\novercomes lock-in effects and removes entry barriers for new asset providers.\nIn contrast to existing data management systems, Agora operates in a heavily\ndecentralized and dynamic environment: Data, algorithms, and even compute\nresources are dynamically created, modified, and removed by different\nstakeholders. Agora presents novel research directions for the data management\ncommunity as a whole: It requires to combine our traditional expertise in\nscalable data processing and management with infrastructure provisioning as\nwell as economic and application aspects of data, algorithms, and\ninfrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 17:22:28 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 14:35:22 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 07:23:17 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Traub", "Jonas", "", "Technische Universit\u00e4t Berlin, German Research Center for Artificial\n  Intelligence"], ["Quian\u00e9-Ruiz", "Jorge-Arnulfo", "", "Technische Universit\u00e4t Berlin, German Research Center for Artificial\n  Intelligence"], ["Kaoudi", "Zoi", "", "Technische Universit\u00e4t Berlin, German Research Center for Artificial\n  Intelligence"], ["Markl", "Volker", "", "Technische Universit\u00e4t Berlin, German Research Center for Artificial\n  Intelligence"]]}, {"id": "1909.03054", "submitter": "Giuseppe Vizzari", "authors": "Luca Crociani, Giuseppe Vizzari, and Stefania Bandini", "title": "Calibrating Wayfinding Decisions in Pedestrian Simulation Models: The\n  Entropy Map", "comments": "pre-print of paper presented at the The 16th International Conference\n  on Modeling Decisions for Artificial Intelligence, Milan, Italy September 4 -\n  6, 2019. arXiv admin note: substantial text overlap with arXiv:1610.07901", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents entropy maps, an approach to describing and visualising\nuncertainty among alternative potential movement intentions in pedestrian\nsimulation models. In particular, entropy maps show the instantaneous level of\nrandomness in decisions of a pedestrian agent situated in a specific point of\nthe simulated environment with an heatmap approach. Experimental results\nhighlighting the relevance of this tool supporting modelers are provided and\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 11:10:34 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Crociani", "Luca", ""], ["Vizzari", "Giuseppe", ""], ["Bandini", "Stefania", ""]]}, {"id": "1909.03094", "submitter": "Michael Green", "authors": "Michael Cerny Green, Ahmed Khalifa, Gabriella A.B. Barros, Tiago\n  Machado and Julian Togelius", "title": "Automatic Critical Mechanic Discovery Using Playtraces in Video Games", "comments": "15 pages, 4 figures, 2 tables, 1 algorithm, 1 equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method of automatic critical mechanic discovery for video\ngames using a combination of game description parsing and playtrace\ninformation. This method is applied to several games within the General Video\nGame Artificial Intelligence (GVG-AI) framework. In a user study,\nhuman-identified mechanics are compared against system-identified critical\nmechanics to verify alignment between humans and the system. The results of the\nstudy demonstrate that the new method is able to match humans with higher\nconsistency than baseline. Our system is further validated by comparing MCTS\nagents augmented with critical mechanics and vanilla MCTS agents on $4$ games\nfrom GVG-AI. Our new playtrace method shows a significant performance\nimprovement over the baseline for all 4 tested games. The proposed method also\nshows either matched or improved performance over the old method, demonstrating\nthat playtrace information is responsible for more complete critical mechanic\ndiscovery.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 19:12:45 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 23:15:48 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 12:36:34 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Green", "Michael Cerny", ""], ["Khalifa", "Ahmed", ""], ["Barros", "Gabriella A. B.", ""], ["Machado", "Tiago", ""], ["Togelius", "Julian", ""]]}, {"id": "1909.03099", "submitter": "Sathyanarayanan Aakur", "authors": "Sathyanarayanan N. Aakur and Sudeep Sarkar", "title": "Abductive Reasoning as Self-Supervision for Common Sense Question\n  Answering", "comments": "8 Pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering has seen significant advances in recent times, especially\nwith the introduction of increasingly bigger transformer-based models\npre-trained on massive amounts of data. While achieving impressive results on\nmany benchmarks, their performances appear to be proportional to the amount of\ntraining data available in the target domain. In this work, we explore the\nability of current question-answering models to generalize - to both other\ndomains as well as with restricted training data. We find that large amounts of\ntraining data are necessary, both for pre-training as well as fine-tuning to a\ntask, for the models to perform well on the designated task. We introduce a\nnovel abductive reasoning approach based on Grenander's Pattern Theory\nframework to provide self-supervised domain adaptation cues or \"pseudo-labels,\"\nwhich can be used instead of expensive human annotations. The proposed\nself-supervised training regimen allows for effective domain adaptation without\nlosing performance compared to fully supervised baselines. Extensive\nexperiments on two publicly available benchmarks show the efficacy of the\nproposed approach. We show that neural networks models trained using\nself-labeled data can retain up to $75\\%$ of the performance of models trained\non large amounts of human-annotated training data.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 19:39:37 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 02:50:17 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Aakur", "Sathyanarayanan N.", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "1909.03162", "submitter": "Palash Dey", "authors": "Aditya Anand and Palash Dey", "title": "Distance Restricted Manipulation in Voting", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of {\\em Distance Restricted Manipulation}, where\ncolluding manipulator(s) need to compute if there exist votes which make their\npreferred alternative win the election when their knowledge about the others'\nvotes is a little inaccurate. We use the Kendall-Tau distance to model the\nmanipulators' confidence in the non-manipulators' votes. To this end, we study\nthis problem in two settings - one where the manipulators need to compute a\nmanipulating vote that succeeds irrespective of perturbations in others' votes\n({\\em Distance Restricted Strong Manipulation}), and the second where the\nmanipulators need to compute a manipulating vote that succeeds for at least one\npossible vote profile of the others ({\\em Distance Restricted Weak\nManipulation}). We show that {\\em Distance Restricted Strong Manipulation}\nadmits polynomial-time algorithms for every scoring rule, maximin, Bucklin, and\nsimplified Bucklin voting rules for a single manipulator, and for the\n$k$-approval rule for any number of manipulators, but becomes intractable for\nthe Copeland$^\\alpha$ voting rule for every $\\alpha\\in[0,1]$ even for a single\nmanipulator. In contrast, {\\em Distance Restricted Weak Manipulation} is\nintractable for almost all the common voting rules, with the exception of the\nplurality rule. For a constant number of alternatives, we show that both the\nproblems are polynomial-time solvable for every anonymous and efficient voting\nrule.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 01:29:32 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 11:19:49 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Anand", "Aditya", ""], ["Dey", "Palash", ""]]}, {"id": "1909.03166", "submitter": "Vivek Gupta", "authors": "Vivek Gupta, Pegah Nokhiz, Chitradeep Dutta Roy, Suresh\n  Venkatasubramanian", "title": "Equalizing Recourse across Groups", "comments": "13 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rise in machine learning-assisted decision-making has led to concerns\nabout the fairness of the decisions and techniques to mitigate problems of\ndiscrimination. If a negative decision is made about an individual (denying a\nloan, rejecting an application for housing, and so on) justice dictates that we\nbe able to ask how we might change circumstances to get a favorable decision\nthe next time. Moreover, the ability to change circumstances (a better\neducation, improved credentials) should not be limited to only those with\naccess to expensive resources. In other words, \\emph{recourse} for negative\ndecisions should be considered a desirable value that can be equalized across\n(demographically defined) groups. This paper describes how to build models that\nmake accurate predictions while still ensuring that the penalties for a\nnegative outcome do not disadvantage different groups disproportionately. We\nmeasure recourse as the distance of an individual from the decision boundary of\na classifier. We then introduce a regularized objective to minimize the\ndifference in recourse across groups. We explore linear settings and further\nextend recourse to non-linear settings as well as model-agnostic settings where\nthe exact distance from boundary cannot be calculated. Our results show that we\ncan successfully decrease the unfairness in recourse while maintaining\nclassifier performance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 01:50:06 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Gupta", "Vivek", ""], ["Nokhiz", "Pegah", ""], ["Roy", "Chitradeep Dutta", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1909.03193", "submitter": "Liang Yao", "authors": "Liang Yao, Chengsheng Mao, Yuan Luo", "title": "KG-BERT: BERT for Knowledge Graph Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs are important resources for many artificial intelligence\ntasks but often suffer from incompleteness. In this work, we propose to use\npre-trained language models for knowledge graph completion. We treat triples in\nknowledge graphs as textual sequences and propose a novel framework named\nKnowledge Graph Bidirectional Encoder Representations from Transformer\n(KG-BERT) to model these triples. Our method takes entity and relation\ndescriptions of a triple as input and computes scoring function of the triple\nwith the KG-BERT language model. Experimental results on multiple benchmark\nknowledge graphs show that our method can achieve state-of-the-art performance\nin triple classification, link prediction and relation prediction tasks.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 06:09:25 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 06:03:30 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Yao", "Liang", ""], ["Mao", "Chengsheng", ""], ["Luo", "Yuan", ""]]}, {"id": "1909.03198", "submitter": "Wenjie Shi", "authors": "Wenjie Shi, Shiji Song and Cheng Wu", "title": "Soft Policy Gradient Method for Maximum Entropy Deep Reinforcement\n  Learning", "comments": "to be published in Proceedings of the Twenty-Eighth International\n  Joint Conference on Artificial Intelligence (IJCAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum entropy deep reinforcement learning (RL) methods have been\ndemonstrated on a range of challenging continuous tasks. However, existing\nmethods either suffer from severe instability when training on large off-policy\ndata or cannot scale to tasks with very high state and action dimensionality\nsuch as 3D humanoid locomotion. Besides, the optimality of desired Boltzmann\npolicy set for non-optimal soft value function is not persuasive enough. In\nthis paper, we first derive soft policy gradient based on entropy regularized\nexpected reward objective for RL with continuous actions. Then, we present an\noff-policy actor-critic, model-free maximum entropy deep RL algorithm called\ndeep soft policy gradient (DSPG) by combining soft policy gradient with soft\nBellman equation. To ensure stable learning while eliminating the need of two\nseparate critics for soft value functions, we leverage double sampling approach\nto making the soft Bellman equation tractable. The experimental results\ndemonstrate that our method outperforms in performance over off-policy prior\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 06:53:01 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Shi", "Wenjie", ""], ["Song", "Shiji", ""], ["Wu", "Cheng", ""]]}, {"id": "1909.03204", "submitter": "Wenjie Shi", "authors": "Wenjie Shi, Shiji Song, Cheng Wu and C. L. Philip Chen", "title": "Multi Pseudo Q-learning Based Deterministic Policy Gradient for Tracking\n  Control of Autonomous Underwater Vehicles", "comments": "IEEE Transactions on Neural Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates trajectory tracking problem for a class of\nunderactuated autonomous underwater vehicles (AUVs) with unknown dynamics and\nconstrained inputs. Different from existing policy gradient methods which\nemploy single actor-critic but cannot realize satisfactory tracking control\naccuracy and stable learning, our proposed algorithm can achieve high-level\ntracking control accuracy of AUVs and stable learning by applying a hybrid\nactors-critics architecture, where multiple actors and critics are trained to\nlearn a deterministic policy and action-value function, respectively.\nSpecifically, for the critics, the expected absolute Bellman error based\nupdating rule is used to choose the worst critic to be updated in each time\nstep. Subsequently, to calculate the loss function with more accurate target\nvalue for the chosen critic, Pseudo Q-learning, which uses sub-greedy policy to\nreplace the greedy policy in Q-learning, is developed for continuous action\nspaces, and Multi Pseudo Q-learning (MPQ) is proposed to reduce the\noverestimation of action-value function and to stabilize the learning. As for\nthe actors, deterministic policy gradient is applied to update the weights, and\nthe final learned policy is defined as the average of all actors to avoid large\nbut bad updates. Moreover, the stability analysis of the learning is given\nqualitatively. The effectiveness and generality of the proposed MPQ-based\nDeterministic Policy Gradient (MPQ-DPG) algorithm are verified by the\napplication on AUV with two different reference trajectories. And the results\ndemonstrate high-level tracking control accuracy and stable learning of\nMPQ-DPG. Besides, the results also validate that increasing the number of the\nactors and critics will further improve the performance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 07:18:41 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Shi", "Wenjie", ""], ["Song", "Shiji", ""], ["Wu", "Cheng", ""], ["Chen", "C. L. Philip", ""]]}, {"id": "1909.03209", "submitter": "Ying Wei", "authors": "Ying Wei, Peilin Zhao, Huaxiu Yao, Junzhou Huang", "title": "Transferable Neural Processes for Hyperparameter Optimization", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated machine learning aims to automate the whole process of machine\nlearning, including model configuration. In this paper, we focus on automated\nhyperparameter optimization (HPO) based on sequential model-based optimization\n(SMBO). Though conventional SMBO algorithms work well when abundant HPO trials\nare available, they are far from satisfactory in practical applications where a\ntrial on a huge dataset may be so costly that an optimal hyperparameter\nconfiguration is expected to return in as few trials as possible. Observing\nthat human experts draw on their expertise in a machine learning model by\ntrying configurations that once performed well on other datasets, we are\ninspired to speed up HPO by transferring knowledge from historical HPO trials\non other datasets. We propose an end-to-end and efficient HPO algorithm named\nas Transfer Neural Processes (TNP), which achieves transfer learning by\nincorporating trials on other datasets, initializing the model with\nwell-generalized parameters, and learning an initial set of hyperparameters to\nevaluate. Experiments on extensive OpenML datasets and three computer vision\ndatasets show that the proposed model can achieve state-of-the-art performance\nin at least one order of magnitude less trials.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 08:10:08 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 04:41:24 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Wei", "Ying", ""], ["Zhao", "Peilin", ""], ["Yao", "Huaxiu", ""], ["Huang", "Junzhou", ""]]}, {"id": "1909.03212", "submitter": "Praneet Dutta", "authors": "Praneet Dutta, Man Kit (Joe) Cheuk, Jonathan S Kim, Massimo Mascaro", "title": "AutoML for Contextual Bandits", "comments": "To be presented at the REVEAL Workshop at the ACM RecSys Conference\n  Copenhagen'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Contextual Bandits is one of the widely popular techniques used in\napplications such as personalization, recommendation systems, mobile health,\ncausal marketing etc . As a dynamic approach, it can be more efficient than\nstandard A/B testing in minimizing regret. We propose an end to end automated\nmeta-learning pipeline to approximate the optimal Q function for contextual\nbandits problems. We see that our model is able to perform much better than\nrandom exploration, being more regret efficient and able to converge with a\nlimited number of samples, while remaining very general and easy to use due to\nthe meta-learning approach. We used a linearly annealed e-greedy exploration\npolicy to define the exploration vs exploitation schedule. We tested the system\non a synthetic environment to characterize it fully and we evaluated it on some\nopen source datasets to benchmark against prior work. We see that our model\noutperforms or performs comparatively to other models while requiring no tuning\nnor feature engineering.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 08:18:03 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Dutta", "Praneet", "", "Joe"], ["Kit", "Man", "", "Joe"], ["Cheuk", "", ""], ["Kim", "Jonathan S", ""], ["Mascaro", "Massimo", ""]]}, {"id": "1909.03245", "submitter": "Wenjie Shi", "authors": "Wenjie Shi, Shiji Song, Hui Wu, Ya-Chu Hsu, Cheng Wu, Gao Huang", "title": "Regularized Anderson Acceleration for Off-Policy Deep Reinforcement\n  Learning", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free deep reinforcement learning (RL) algorithms have been widely used\nfor a range of complex control tasks. However, slow convergence and sample\ninefficiency remain challenging problems in RL, especially when handling\ncontinuous and high-dimensional state spaces. To tackle this problem, we\npropose a general acceleration method for model-free, off-policy deep RL\nalgorithms by drawing the idea underlying regularized Anderson acceleration\n(RAA), which is an effective approach to accelerating the solving of fixed\npoint problems with perturbations. Specifically, we first explain how policy\niteration can be applied directly with Anderson acceleration. Then we extend\nRAA to the case of deep RL by introducing a regularization term to control the\nimpact of perturbation induced by function approximation errors. We further\npropose two strategies, i.e., progressive update and adaptive restart, to\nenhance the performance. The effectiveness of our method is evaluated on a\nvariety of benchmark tasks, including Atari 2600 and MuJoCo. Experimental\nresults show that our approach substantially improves both the learning speed\nand final performance of state-of-the-art deep RL algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 11:18:32 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 01:11:40 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Shi", "Wenjie", ""], ["Song", "Shiji", ""], ["Wu", "Hui", ""], ["Hsu", "Ya-Chu", ""], ["Wu", "Cheng", ""], ["Huang", "Gao", ""]]}, {"id": "1909.03261", "submitter": "Riccardo Volpato", "authors": "Riccardo Volpato and Guangyan Song", "title": "Active learning to optimise time-expensive algorithm selection", "comments": "11 pages, 3 figures, 3 tables and 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hard optimisation problems such as Boolean Satisfiability typically have long\nsolving times and can usually be solved by many algorithms, although the\nperformance can vary widely in practice. Research has shown that no single\nalgorithm outperforms all the others; thus, it is crucial to select the best\nalgorithm for a given problem. Supervised machine learning models can\naccurately predict which solver is best for a given problem, but they require\nfirst to run every solver in the portfolio for all examples available to create\nlabelled data. As this approach cannot scale, we developed an active learning\nframework that addresses this problem by constructing an optimal training set,\nso that the learner can achieve higher or equal performances with less training\ndata. Our work proves that active learning is beneficial for algorithm\nselection techniques and provides practical guidance to incorporate into\nexisting systems.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 12:33:31 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Volpato", "Riccardo", ""], ["Song", "Guangyan", ""]]}, {"id": "1909.03276", "submitter": "Weiyu Cheng", "authors": "Weiyu Cheng, Yanyan Shen, Linpeng Huang", "title": "Adaptive Factorization Network: Learning Adaptive-Order Feature\n  Interactions", "comments": "Accepted by AAAI'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various factorization-based methods have been proposed to leverage\nsecond-order, or higher-order cross features for boosting the performance of\npredictive models. They generally enumerate all the cross features under a\npredefined maximum order, and then identify useful feature interactions through\nmodel training, which suffer from two drawbacks. First, they have to make a\ntrade-off between the expressiveness of higher-order cross features and the\ncomputational cost, resulting in suboptimal predictions. Second, enumerating\nall the cross features, including irrelevant ones, may introduce noisy feature\ncombinations that degrade model performance. In this work, we propose the\nAdaptive Factorization Network (AFN), a new model that learns arbitrary-order\ncross features adaptively from data. The core of AFN is a logarithmic\ntransformation layer to convert the power of each feature in a feature\ncombination into the coefficient to be learned. The experimental results on\nfour real datasets demonstrate the superior predictive performance of AFN\nagainst the start-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 14:30:43 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 02:05:51 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Cheng", "Weiyu", ""], ["Shen", "Yanyan", ""], ["Huang", "Linpeng", ""]]}, {"id": "1909.03278", "submitter": "Wonsup Shin", "authors": "Wonsup Shin, Seok-Jun Bu, and Sung-Bae Cho", "title": "Automatic Financial Trading Agent for Low-risk Portfolio Management\n  using Deep Reinforcement Learning", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autonomous trading agent is one of the most actively studied areas of\nartificial intelligence to solve the capital market portfolio management\nproblem. The two primary goals of the portfolio management problem are\nmaximizing profit and restrainting risk. However, most approaches to this\nproblem solely take account of maximizing returns. Therefore, this paper\nproposes a deep reinforcement learning based trading agent that can manage the\nportfolio considering not only profit maximization but also risk restraint. We\nalso propose a new target policy to allow the trading agent to learn to prefer\nlow-risk actions. The new target policy can be reflected in the update by\nadjusting the greediness for the optimal action through the hyper parameter.\nThe proposed trading agent verifies the performance through the data of the\ncryptocurrency market. The Cryptocurrency market is the best test-ground for\ntesting our trading agents because of the huge amount of data accumulated every\nminute and the market volatility is extremely large. As a experimental result,\nduring the test period, our agents achieved a return of 1800% and provided the\nleast risky investment strategy among the existing methods. And, another\nexperiment shows that the agent can maintain robust generalized performance\neven if market volatility is large or training period is short.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 14:45:36 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Shin", "Wonsup", ""], ["Bu", "Seok-Jun", ""], ["Cho", "Sung-Bae", ""]]}, {"id": "1909.03287", "submitter": "Davide Bacciu", "authors": "Davide Bacciu and Luigi Di Sotto", "title": "A Non-Negative Factorization approach to node pooling in Graph\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper discusses a pooling mechanism to induce subsampling in graph\nstructured data and introduces it as a component of a graph convolutional\nneural network. The pooling mechanism builds on the Non-Negative Matrix\nFactorization (NMF) of a matrix representing node adjacency and node similarity\nas adaptively obtained through the vertices embedding learned by the model.\nSuch mechanism is applied to obtain an incrementally coarser graph where nodes\nare adaptively pooled into communities based on the outcomes of the\nnon-negative factorization. The empirical analysis on graph classification\nbenchmarks shows how such coarsening process yields significant improvements in\nthe predictive performance of the model with respect to its non-pooled\ncounterpart.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 15:27:49 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Bacciu", "Davide", ""], ["Di Sotto", "Luigi", ""]]}, {"id": "1909.03290", "submitter": "Lav Varshney", "authors": "Lav R. Varshney, Nitish Shirish Keskar, Richard Socher", "title": "Pretrained AI Models: Performativity, Mobility, and Change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paradigm of pretrained deep learning models has recently emerged in\nartificial intelligence practice, allowing deployment in numerous societal\nsettings with limited computational resources, but also embedding biases and\nenabling unintended negative uses. In this paper, we treat pretrained models as\nobjects of study and discuss the ethical impacts of their sociological\nposition. We discuss how pretrained models are developed and compared under the\ncommon task framework, but that this may make self-regulation inadequate.\nFurther how pretrained models may have a performative effect on society that\nexacerbates biases. We then discuss how pretrained models move through actor\nnetworks as a kind of computationally immutable mobile, but that users also act\nas agents of technological change by reinterpreting them via fine-tuning and\ntransfer. We further discuss how users may use pretrained models in malicious\nways, drawing a novel connection between the responsible innovation and\nuser-centered innovation literatures. We close by discussing how this\nsociological understanding of pretrained models can inform AI governance\nframeworks for fairness, accountability, and transparency.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 15:40:22 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Varshney", "Lav R.", ""], ["Keskar", "Nitish Shirish", ""], ["Socher", "Richard", ""]]}, {"id": "1909.03313", "submitter": "Wen Wu", "authors": "Wen Wu, Nan Cheng, Ning Zhang, Peng Yang, Weihua Zhuang, Xuemin\n  (Sherman) Shen", "title": "Fast mmwave Beam Alignment via Correlated Bandit Learning", "comments": "Accepted by IEEE Transactions on Wireless Communications. In this\n  article, we propose a learning-based fast beam alignment algorithm to reduce\n  beam alignment latency", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beam alignment (BA) is to ensure the transmitter and receiver beams are\naccurately aligned to establish a reliable communication link in\nmillimeter-wave (mmwave) systems. Existing BA methods search the entire beam\nspace to identify the optimal transmit-receive beam pair, which incurs\nsignificant BA latency on the order of seconds in the worst case. In this\npaper, we develop a learning algorithm to reduce BA latency, namely\nHierarchical Beam Alignment (HBA) algorithm. We first formulate the BA problem\nas a stochastic multi-armed bandit problem with the objective to maximize the\ncumulative received signal strength within a certain period. The proposed\nalgorithm takes advantage of the correlation structure among beams such that\nthe information from nearby beams is extracted to identify the optimal beam,\ninstead of searching the entire beam space. Furthermore, the prior knowledge on\nthe channel fluctuation is incorporated in the proposed algorithm to further\naccelerate the BA process. Theoretical analysis indicates that the proposed\nalgorithm is asymptotically optimal. Extensive simulation results demonstrate\nthat the proposed algorithm can identify the optimal beam with a high\nprobability and reduce the BA latency from hundreds of milliseconds to a few\nmilliseconds in the multipath channel, as compared to the existing BA method in\nIEEE 802.11ad.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 18:12:07 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wu", "Wen", "", "Sherman"], ["Cheng", "Nan", "", "Sherman"], ["Zhang", "Ning", "", "Sherman"], ["Yang", "Peng", "", "Sherman"], ["Zhuang", "Weihua", "", "Sherman"], ["Xuemin", "", "", "Sherman"], ["Shen", "", ""]]}, {"id": "1909.03315", "submitter": "Martin Andrews", "authors": "Martin Andrews, Sam Witteveen", "title": "Relationships from Entity Stream", "comments": "Accepted paper for the ViGIL workshop at NIPS 2017. (4 pages +\n  references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational reasoning is a central component of intelligent behavior, but has\nproven difficult for neural networks to learn. The Relation Network (RN) module\nwas recently proposed by DeepMind to solve such problems, and demonstrated\nstate-of-the-art results on a number of datasets. However, the RN module scales\nquadratically in the size of the input, since it calculates relationship\nfactors between every patch in the visual field, including those that do not\ncorrespond to entities. In this paper, we describe an architecture that enables\nrelationships to be determined from a stream of entities obtained by an\nattention mechanism over the input field. The model is trained end-to-end, and\ndemonstrates equivalent performance with greater interpretability while\nrequiring only a fraction of the model parameters of the original RN module.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 18:24:57 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Andrews", "Martin", ""], ["Witteveen", "Sam", ""]]}, {"id": "1909.03317", "submitter": "Sam Davidson", "authors": "Sam Davidson, Dian Yu, Zhou Yu", "title": "Dependency Parsing for Spoken Dialog Systems", "comments": "To be presented at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependency parsing of conversational input can play an important role in\nlanguage understanding for dialog systems by identifying the relationships\nbetween entities extracted from user utterances. Additionally, effective\ndependency parsing can elucidate differences in language structure and usage\nfor discourse analysis of human-human versus human-machine dialogs. However,\nmodels trained on datasets based on news articles and web data do not perform\nwell on spoken human-machine dialog, and currently available annotation schemes\ndo not adapt well to dialog data. Therefore, we propose the Spoken Conversation\nUniversal Dependencies (SCUD) annotation scheme that extends the Universal\nDependencies (UD) (Nivre et al., 2016) guidelines to spoken human-machine\ndialogs. We also provide ConvBank, a conversation dataset between humans and an\nopen-domain conversational dialog system with SCUD annotation. Finally, to\ndemonstrate the utility of the dataset, we train a dependency parser on the\nConvBank dataset. We demonstrate that by pre-training a dependency parser on a\nset of larger public datasets and fine-tuning on ConvBank data, we achieved the\nbest result, 85.05% unlabeled and 77.82% labeled attachment accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 18:32:28 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Davidson", "Sam", ""], ["Yu", "Dian", ""], ["Yu", "Zhou", ""]]}, {"id": "1909.03329", "submitter": "Fan-Keng Sun", "authors": "Fan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee", "title": "LAMOL: LAnguage MOdeling for Lifelong Language Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most research on lifelong learning applies to images or games, but not\nlanguage. We present LAMOL, a simple yet effective method for lifelong language\nlearning (LLL) based on language modeling. LAMOL replays pseudo-samples of\nprevious tasks while requiring no extra memory or model capacity. Specifically,\nLAMOL is a language model that simultaneously learns to solve the tasks and\ngenerate training samples. When the model is trained for a new task, it\ngenerates pseudo-samples of previous tasks for training alongside data for the\nnew task. The results show that LAMOL prevents catastrophic forgetting without\nany sign of intransigence and can perform five very different language tasks\nsequentially with only one model. Overall, LAMOL outperforms previous methods\nby a considerable margin and is only 2-3% worse than multitasking, which is\nusually considered the LLL upper bound. The source code is available at\nhttps://github.com/jojotenya/LAMOL.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 20:17:34 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 04:36:52 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Sun", "Fan-Keng", ""], ["Ho", "Cheng-Hao", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "1909.03331", "submitter": "Georgios Papagiannis", "authors": "Georgios Papagiannis, Sotiris Moschoyiannis", "title": "Deep Reinforcement Learning for Control of Probabilistic Boolean\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Boolean Networks (PBNs) were introduced as a computational\nmodel for the study of complex dynamical systems, such as Gene Regulatory\nNetworks (GRNs). Controllability in this context is the process of making\nstrategic interventions to the state of a network in order to drive it towards\nsome other state that exhibits favourable biological properties. In this paper\nwe study the ability of a Double Deep Q-Network with Prioritized Experience\nReplay in learning control strategies within a finite number of time steps that\ndrive a PBN towards a target state, typically an attractor. The control method\nis model-free and does not require knowledge of the network's underlying\ndynamics, making it suitable for applications where inference of such dynamics\nis intractable. We present extensive experiment results on two synthetic PBNs\nand the PBN model constructed directly from gene-expression data of a study on\nmetastatic-melanoma.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 20:24:41 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 11:03:38 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 19:14:06 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2020 12:38:16 GMT"}, {"version": "v5", "created": "Mon, 7 Sep 2020 16:05:20 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Papagiannis", "Georgios", ""], ["Moschoyiannis", "Sotiris", ""]]}, {"id": "1909.03350", "submitter": "Faez Ahmed", "authors": "Saba Ahmadi, Faez Ahmed, John P. Dickerson, Mark Fuge and Samir\n  Khuller", "title": "An Algorithm for Multi-Attribute Diverse Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite b-matching, where agents on one side of a market are matched to one\nor more agents or items on the other, is a classical model that is used in\nmyriad application areas such as healthcare, advertising, education, and\ngeneral resource allocation. Traditionally, the primary goal of such models is\nto maximize a linear function of the constituent matches (e.g., linear social\nwelfare maximization) subject to some constraints. Recent work has studied a\nnew goal of balancing whole-match diversity and economic efficiency, where the\nobjective is instead a monotone submodular function over the matching. Basic\nversions of this problem are solvable in polynomial time. In this work, we\nprove that the problem of simultaneously maximizing diversity along several\nfeatures (e.g., country of citizenship, gender, skills) is NP-hard. To address\nthis problem, we develop the first combinatorial algorithm that constructs\nprovably-optimal diverse b-matchings in pseudo-polynomial time. We also provide\na Mixed-Integer Quadratic formulation for the same problem and show that our\nmethod guarantees optimal solutions and takes less computation time for a\nreviewer assignment application.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 23:29:47 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 06:54:09 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2020 05:17:03 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Ahmadi", "Saba", ""], ["Ahmed", "Faez", ""], ["Dickerson", "John P.", ""], ["Fuge", "Mark", ""], ["Khuller", "Samir", ""]]}, {"id": "1909.03373", "submitter": "Dong Li", "authors": "Dong Li, Bo Ouyang, Duanpo Wu, Yaonan Wang", "title": "Artificial intelligence empowered multi-AGVs in manufacturing systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AGVs are driverless robotic vehicles that picks up and delivers materials.\nHow to improve the efficiency while preventing deadlocks is the core issue in\ndesigning AGV systems. In this paper, we propose an approach to tackle this\nproblem.The proposed approach includes a traditional AGV scheduling algorithm,\nwhich aims at solving deadlock problems, and an artificial neural network based\ncomponent, which predict future tasks of the AGV system, and make decisions on\nwhether to send an AGV to the predicted starting location of the upcoming\ntask,so as to save the time of waiting for an AGV to go to there first when the\nupcoming task is created. Simulation results show that the proposed method\nsignificantly improves the efficiency as against traditional method, up to 20%\nto 30%.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 02:41:19 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Li", "Dong", ""], ["Ouyang", "Bo", ""], ["Wu", "Duanpo", ""], ["Wang", "Yaonan", ""]]}, {"id": "1909.03404", "submitter": "Martin Atzmueller", "authors": "Martin Atzmueller and Cicek G\\\"uven and Dietmar Seipel", "title": "Towards Generating Explanations for ASP-Based Link Analysis using\n  Declarative Program Transformations", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explication and the generation of explanations are prominent topics in\nartificial intelligence and data science, in order to make methods and systems\nmore transparent and understandable for humans. This paper investigates the\nproblem of link analysis, specifically link prediction and anomalous link\ndiscovery in social networks using the declarative method of Answer set\nprogramming (ASP). Applying ASP for link prediction provides a powerful\ndeclarative approach, e.g., for incorporating domain knowledge for explicative\nprediction. In this context, we propose a novel method for generating\nexplanations - as offline justifications - using declarative program\ntransformations. The method itself is purely based on syntactic transformations\nof declarative programs, e.g., in an ASP formalism, using rule instrumentation.\nWe demonstrate the efficacy of the proposed approach, exemplifying it in an\napplication on link analysis in social networks, also including domain\nknowledge.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 08:51:19 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Atzmueller", "Martin", ""], ["G\u00fcven", "Cicek", ""], ["Seipel", "Dietmar", ""]]}, {"id": "1909.03409", "submitter": "Bin Guo", "authors": "Bin Guo, Hao Wang, Yasan Ding, Wei Wu, Shaoyang Hao, Yueqi Sun, Zhiwen\n  Yu", "title": "Conditional Text Generation for Harmonious Human-Machine Interaction", "comments": "Accepted by the ACM Transactions on Intelligent Systems and\n  Technology (TIST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the development of deep learning, text generation\ntechnology has undergone great changes and provided many kinds of services for\nhuman beings, such as restaurant reservation and daily communication. The\nautomatically generated text is becoming more and more fluent so researchers\nbegin to consider more anthropomorphic text generation technology, that is the\nconditional text generation, including emotional text generation, personalized\ntext generation, and so on. Conditional Text Generation (CTG) has thus become a\nresearch hotspot. As a promising research field, we find that many efforts have\nbeen paid to exploring it. Therefore, we aim to give a comprehensive review of\nthe new research trends of CTG. We first summary several key techniques and\nillustrate the technical evolution route in the field of neural text\ngeneration, based on the concept model of CTG. We further make an investigation\nof existing CTG fields and propose several general learning models for CTG.\nFinally, we discuss the open issues and promising research directions of CTG.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 09:31:20 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 15:38:26 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Guo", "Bin", ""], ["Wang", "Hao", ""], ["Ding", "Yasan", ""], ["Wu", "Wei", ""], ["Hao", "Shaoyang", ""], ["Sun", "Yueqi", ""], ["Yu", "Zhiwen", ""]]}, {"id": "1909.03418", "submitter": "Asaf Shabtai", "authors": "Gil Fidel, Ron Bitton, Asaf Shabtai", "title": "When Explainability Meets Adversarial Learning: Detecting Adversarial\n  Examples using SHAP Signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep neural networks (DNNs) are highly effective in solving\nmany complex real-world problems. However, these models are vulnerable to\nadversarial perturbation attacks, and despite the plethora of research in this\ndomain, to this day, adversaries still have the upper hand in the cat and mouse\ngame of adversarial example generation methods vs. detection and prevention\nmethods. In this research, we present a novel detection method that uses\nShapley Additive Explanations (SHAP) values computed for the internal layers of\na DNN classifier to discriminate between normal and adversarial inputs. We\nevaluate our method by building an extensive dataset of adversarial examples\nover the popular CIFAR-10 and MNIST datasets, and training a neural\nnetwork-based detector to distinguish between normal and adversarial inputs. We\nevaluate our detector against adversarial examples generated by diverse\nstate-of-the-art attacks and demonstrate its high detection accuracy and strong\ngeneralization ability to adversarial inputs generated with different attack\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 10:00:44 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Fidel", "Gil", ""], ["Bitton", "Ron", ""], ["Shabtai", "Asaf", ""]]}, {"id": "1909.03428", "submitter": "Martin Atzmueller", "authors": "Spyroula Masiala and Willem Huijbers and Martin Atzmueller", "title": "Feature-Set-Engineering for Detecting Freezing of Gait in Parkinson's\n  Disease using Deep Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Freezing of gait (FoG) is a common gait disability in Parkinson's disease,\nthat usually appears in its advanced stage. Freeze episodes are associated with\nfalls, injuries, and psychological consequences, negatively affecting the\npatients' quality of life. For detecting FoG episodes automatically, a highly\naccurate detection method is necessary. This paper presents an approach for\ndetecting FoG episodes utilizing a deep recurrent neural network (RNN) on\n3D-accelerometer measurements. We investigate suitable features and feature\ncombinations extracted from the sensors' time series data. Specifically, for\ndetecting FoG episodes, we apply a deep RNN with Long Short-Term Memory cells.\nIn our experiments, we perform both user dependent and user independent\nexperiments, to detect freeze episodes. Our experimental results show that the\nfrequency domain features extracted from the trunk sensor are the most\ninformative feature group in the subject independent method, achieving an\naverage AUC score of 93%, Specificity of 90% and Sensitivity of 81%. Moreover,\nfrequency and statistical features of all the sensors are identified as the\nbest single input for the subject dependent method, achieving an average AUC\nscore of 97%, Specificity of 96% and Sensitivity of 87%. Overall, in a\ncomparison to state-of-the-art approaches from literature as baseline methods,\nour proposed approach outperforms these significantly.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 11:02:50 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Masiala", "Spyroula", ""], ["Huijbers", "Willem", ""], ["Atzmueller", "Martin", ""]]}, {"id": "1909.03452", "submitter": "Tin Lai", "authors": "Tin Lai, Philippe Morere, Fabio Ramos, Gilad Francis", "title": "Bayesian Local Sampling-based Planning", "comments": "Accepted in IEEE Robotics and Automation Letters (RA-L)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling-based planning is the predominant paradigm for motion planning in\nrobotics. Most sampling-based planners use a global random sampling scheme to\nguarantee probabilistic completeness. However, most schemes are often\ninefficient as the samples drawn from the global proposal distribution, and do\nnot exploit relevant local structures. Local sampling-based motion planners, on\nthe other hand, take sequential decisions of random walks to samples valid\ntrajectories in configuration space. However, current approaches do not adapt\ntheir strategies according to the success and failures of past samples.\n  In this work, we introduce a local sampling-based motion planner with a\nBayesian learning scheme for modelling an adaptive sampling proposal\ndistribution. The proposal distribution is sequentially updated based on\nprevious samples, consequently shaping it according to local obstacles and\nconstraints in the configuration space. Thus, through learning from past\nobserved outcomes, we maximise the likelihood of sampling in regions that have\na higher probability to form trajectories within narrow passages. We provide\nthe formulation of a sample-efficient distribution, along with theoretical\nfoundation of sequentially updating this distribution. We demonstrate\nexperimentally that by using a Bayesian proposal distribution, a solution is\nfound faster, requiring fewer samples, and without any noticeable performance\noverhead.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 12:52:15 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 13:38:24 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Lai", "Tin", ""], ["Morere", "Philippe", ""], ["Ramos", "Fabio", ""], ["Francis", "Gilad", ""]]}, {"id": "1909.03466", "submitter": "Muhammad Usman Khalid", "authors": "Muhammad Usman Khalid and Jie Yu", "title": "Multi-Modal Three-Stream Network for Action Recognition", "comments": "Presented in IEEE ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition in video is an active yet challenging research topic\ndue to high variation and complexity of data. In this paper, a novel video\nbased action recognition framework utilizing complementary cues is proposed to\nhandle this complex problem. Inspired by the successful two stream networks for\naction classification, additional pose features are studied and fused to\nenhance understanding of human action in a more abstract and semantic way.\nTowards practices, not only ground truth poses but also noisy estimated poses\nare incorporated in the framework with our proposed pre-processing module. The\nwhole framework and each cue are evaluated on varied benchmarking datasets as\nJHMDB, sub-JHMDB and Penn Action. Our results outperform state-of-the-art\nperformance on these datasets and show the strength of complementary cues.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 13:40:16 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Khalid", "Muhammad Usman", ""], ["Yu", "Jie", ""]]}, {"id": "1909.03467", "submitter": "Qi Zhang", "authors": "Qi Zhang, Tao Du, Changzheng Tian", "title": "Self-driving scale car trained by Deep reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-driving based on deep reinforcement learning, as the most important\napplication of artificial intelligence, has become a popular topic. Most of the\ncurrent self-driving methods focus on how to directly learn end-to-end\nself-driving control strategy from the raw sensory data. Essentially, this\ncontrol strategy can be considered as a mapping between images and driving\nbehavior, which usually faces a problem of low generalization ability. To\nimprove the generalization ability for the driving behavior, the reinforcement\nlearning method requires extrinsic reward from the real environment, which may\ndamage the car. In order to obtain a good generalization ability in safety, a\nvirtual simulation environment that can be constructed different driving scene\nis designed by Unity. A theoretical model is established and analyzed in the\nvirtual simulation environment, and it is trained by double Deep Q-network.\nThen, the trained model is migrated to a scale car in real world. This process\nis also called a sim2real method. The sim2real training method efficiently\nhandle the these two problems. The simulations and experiments are carried out\nto evaluate the performance and effectiveness of the proposed algorithm.\nFinally, it is demonstrated that the scale car in real world obtain the\ncapability for autonomous driving.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 13:41:42 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 15:53:53 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 12:48:39 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Zhang", "Qi", ""], ["Du", "Tao", ""], ["Tian", "Changzheng", ""]]}, {"id": "1909.03470", "submitter": "Moshe BenBassat Professor", "authors": "Moshe BenBassat", "title": "Disease Labeling via Machine Learning is NOT quite the same as Medical\n  Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key step in medical diagnosis is giving the patient a universally\nrecognized label (e.g. Appendicitis) which essentially assigns the patient to a\nclass(es) of patients with similar body failures. However, two patients having\nthe same disease label(s) with high probability may still have differences in\ntheir feature manifestation patterns implying differences in the required\ntreatments. Additionally, in many cases, the labels of the primary diagnoses\nleave some findings unexplained. Medical diagnosis is only partially about\nprobability calculations for label X or Y. Diagnosis is not complete until the\npatient overall situation is clinically understood to the level that enables\nthe best therapeutic decisions. Most machine learning models are data centric\nmodels, and evidence so far suggest they can reach expert level performance in\nthe disease labeling phase. Nonetheless, like any other mathematical technique,\nthey have their limitations and applicability scope. Primarily, data centric\nalgorithms are knowledge blind and lack anatomy and physiology knowledge that\nphysicians leverage to achieve complete diagnosis. This article advocates to\ncomplement them with intelligence to overcome their inherent limitations as\nknowledge blind algorithms. Machines can learn many things from data, but data\nis not the only source that machines can learn from. Historic patient data only\ntells us what the possible manifestations of a certain body failure are.\nAnatomy and physiology knowledge tell us how the body works and fails. Both are\nneeded for complete diagnosis. The proposed Double Deep Learning approach,\nalong with the initiative for Medical Wikipedia for Smart Machines, leads to AI\ndiagnostic support solutions for complete diagnosis beyond the limited data\nonly labeling solutions we see today. AI for medicine will forever be limited\nuntil their intelligence also integrates anatomy and physiology.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 13:54:00 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["BenBassat", "Moshe", ""]]}, {"id": "1909.03480", "submitter": "Prithviraj Ammanabrolu", "authors": "Prithviraj Ammanabrolu, Ethan Tien, Wesley Cheung, Zhaochen Luo,\n  William Ma, Lara J. Martin, Mark O. Riedl", "title": "Story Realization: Expanding Plot Events into Sentences", "comments": "In proceedings of AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based approaches to automated story plot generation attempt to\nlearn how to generate novel plots from a corpus of natural language plot\nsummaries. Prior work has shown that a semantic abstraction of sentences called\nevents improves neural plot generation and and allows one to decompose the\nproblem into: (1) the generation of a sequence of events (event-to-event) and\n(2) the transformation of these events into natural language sentences\n(event-to-sentence). However, typical neural language generation approaches to\nevent-to-sentence can ignore the event details and produce\ngrammatically-correct but semantically-unrelated sentences. We present an\nensemble-based model that generates natural language guided by events.We\nprovide results---including a human subjects study---for a full end-to-end\nautomated story generation system showing that our method generates more\ncoherent and plausible stories than baseline approaches.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 15:09:32 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 18:32:23 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Ammanabrolu", "Prithviraj", ""], ["Tien", "Ethan", ""], ["Cheung", "Wesley", ""], ["Luo", "Zhaochen", ""], ["Ma", "William", ""], ["Martin", "Lara J.", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1909.03486", "submitter": "Dakuo Wang", "authors": "Yaoli Mao, Dakuo Wang, Michael Muller, Kush R. Varshney, Ioana\n  Baldini, Casey Dugan, AleksandraMojsilovi\\'c", "title": "How Data Scientists Work Together With Domain Experts in Scientific\n  Collaborations: To Find The Right Answer Or To Ask The Right Question?", "comments": null, "journal-ref": null, "doi": "10.1145/3361118", "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been an increasing trend in which data scientists\nand domain experts work together to tackle complex scientific questions.\nHowever, such collaborations often face challenges. In this paper, we aim to\ndecipher this collaboration complexity through a semi-structured interview\nstudy with 22 interviewees from teams of bio-medical scientists collaborating\nwith data scientists. In the analysis, we adopt the Olsons' four-dimensions\nframework proposed in Distance Matters to code interview transcripts. Our\nfindings suggest that besides the glitches in the collaboration readiness,\ntechnology readiness, and coupling of work dimensions, the tensions that exist\nin the common ground building process influence the collaboration outcomes, and\nthen persist in the actual collaboration process. In contrast to prior works'\ngeneral account of building a high level of common ground, the breakdowns of\ncontent common ground together with the strengthen of process common ground in\nthis process is more beneficial for scientific discovery. We discuss why that\nis and what the design suggestions are, and conclude the paper with future\ndirections and limitations.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 15:48:35 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Mao", "Yaoli", ""], ["Wang", "Dakuo", ""], ["Muller", "Michael", ""], ["Varshney", "Kush R.", ""], ["Baldini", "Ioana", ""], ["Dugan", "Casey", ""], ["AleksandraMojsilovi\u0107", "", ""]]}, {"id": "1909.03500", "submitter": "Zhining Liu", "authors": "Zhining Liu, Wei Cao, Zhifeng Gao, Jiang Bian, Hechang Chen, Yi Chang,\n  Tie-Yan Liu", "title": "Self-paced Ensemble for Highly Imbalanced Massive Data Classification", "comments": "IEEE 36th International Conference on Data Engineering (ICDE 2020)", "journal-ref": "2020 IEEE 36th International Conference on Data Engineering\n  (ICDE). IEEE, 2020: 841-852", "doi": "10.1109/ICDE48307.2020.00078", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications reveal difficulties in learning classifiers from\nimbalanced data. The rising big data era has been witnessing more\nclassification tasks with large-scale but extremely imbalance and low-quality\ndatasets. Most of existing learning methods suffer from poor performance or low\ncomputation efficiency under such a scenario. To tackle this problem, we\nconduct deep investigations into the nature of class imbalance, which reveals\nthat not only the disproportion between classes, but also other difficulties\nembedded in the nature of data, especially, noises and class overlapping,\nprevent us from learning effective classifiers. Taking those factors into\nconsideration, we propose a novel framework for imbalance classification that\naims to generate a strong ensemble by self-paced harmonizing data hardness via\nunder-sampling. Extensive experiments have shown that this new framework, while\nbeing very computationally efficient, can lead to robust performance even under\nhighly overlapping classes and extremely skewed distribution. Note that, our\nmethods can be easily adapted to most of existing learning methods (e.g., C4.5,\nSVM, GBDT and Neural Network) to boost their performance on imbalanced data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 16:32:47 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 10:39:57 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2020 13:49:17 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Liu", "Zhining", ""], ["Cao", "Wei", ""], ["Gao", "Zhifeng", ""], ["Bian", "Jiang", ""], ["Chen", "Hechang", ""], ["Chang", "Yi", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1909.03527", "submitter": "Danish Contractor", "authors": "Danish Contractor and Krunal Shah and Aditi Partap and Mausam and\n  Parag Singla", "title": "Large Scale Question Answering using Tourism Data", "comments": "20 pages with supplementary notes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the novel task of answering entity-seeking recommendation\nquestions using a collection of reviews that describe candidate answer\nentities. We harvest a QA dataset that contains 47,124 paragraph-sized real\nuser questions from travelers seeking recommendations for hotels, attractions\nand restaurants. Each question can have thousands of candidate answers to\nchoose from and each candidate is associated with a collection of unstructured\nreviews. This dataset is especially challenging because commonly used neural\narchitectures for reasoning and QA are prohibitively expensive for a task of\nthis scale. As a solution, we design a scalable cluster-select-rerank approach.\nIt first clusters text for each entity to identify exemplar sentences\ndescribing an entity. It then uses a scalable neural information retrieval (IR)\nmodule to select a set of potential entities from the large candidate set. A\nreranker uses a deeper attention-based architecture to pick the best answers\nfrom the selected entities. This strategy performs better than a pure IR or a\npure attention-based reasoning approach yielding nearly 25% relative\nimprovement in Accuracy@3 over both approaches.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 18:35:03 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 17:17:28 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Contractor", "Danish", ""], ["Shah", "Krunal", ""], ["Partap", "Aditi", ""], ["Mausam", "", ""], ["Singla", "Parag", ""]]}, {"id": "1909.03539", "submitter": "Peng Liao", "authors": "Peng Liao, Kristjan Greenewald, Predrag Klasnja, Susan Murphy", "title": "Personalized HeartSteps: A Reinforcement Learning Algorithm for\n  Optimizing Physical Activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent evolution of mobile health technologies, health scientists\nare increasingly interested in developing just-in-time adaptive interventions\n(JITAIs), typically delivered via notification on mobile device and designed to\nhelp the user prevent negative health outcomes and promote the adoption and\nmaintenance of healthy behaviors. A JITAI involves a sequence of decision rules\n(i.e., treatment policy) that takes the user's current context as input and\nspecifies whether and what type of an intervention should be provided at the\nmoment. In this paper, we develop a Reinforcement Learning (RL) algorithm that\ncontinuously learns and improves the treatment policy embedded in the JITAI as\nthe data is being collected from the user. This work is motivated by our\ncollaboration on designing the RL algorithm in HeartSteps V2 based on data from\nHeartSteps V1. HeartSteps is a physical activity mobile health application. The\nRL algorithm developed in this paper is being used in HeartSteps V2 to decide,\nfive times per day, whether to deliver a context-tailored activity suggestion.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 20:12:30 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liao", "Peng", ""], ["Greenewald", "Kristjan", ""], ["Klasnja", "Predrag", ""], ["Murphy", "Susan", ""]]}, {"id": "1909.03553", "submitter": "Peter Clark", "authors": "Oyvind Tafjord, Matt Gardner, Kevin Lin, Peter Clark", "title": "QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions", "comments": "EMNLP'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first open-domain dataset, called QuaRTz, for reasoning\nabout textual qualitative relationships. QuaRTz contains general qualitative\nstatements, e.g., \"A sunscreen with a higher SPF protects the skin longer.\",\ntwinned with 3864 crowdsourced situated questions, e.g., \"Billy is wearing\nsunscreen with a lower SPF than Lucy. Who will be best protected from the\nsun?\", plus annotations of the properties being compared. Unlike previous\ndatasets, the general knowledge is textual and not tied to a fixed set of\nrelationships, and tests a system's ability to comprehend and apply textual\nqualitative knowledge in a novel setting. We find state-of-the-art results are\nsubstantially (20%) below human performance, presenting an open challenge to\nthe NLP community.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 22:05:19 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Tafjord", "Oyvind", ""], ["Gardner", "Matt", ""], ["Lin", "Kevin", ""], ["Clark", "Peter", ""]]}, {"id": "1909.03560", "submitter": "Anthony Rhodes", "authors": "Anthony D. Rhodes", "title": "Evolving Order and Chaos: Comparing Particle Swarm Optimization and\n  Genetic Algorithms for Global Coordination of Cellular Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply two evolutionary search algorithms: Particle Swarm Optimization\n(PSO) and Genetic Algorithms (GAs) to the design of Cellular Automata (CA) that\ncan perform computational tasks requiring global coordination. In particular,\nwe compare search efficiency for PSO and GAs applied to both the density\nclassification problem and to the novel generation of 'chaotic' CA. Our work\nfurthermore introduces a new variant of PSO, the Binary Global-Local PSO\n(BGL-PSO).\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 22:57:10 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Rhodes", "Anthony D.", ""]]}, {"id": "1909.03567", "submitter": "Andi Peng", "authors": "Andi Peng, Besmira Nushi, Emre Kiciman, Kori Inkpen, Siddharth Suri,\n  Ece Kamar", "title": "What You See Is What You Get? The Impact of Representation Criteria on\n  Human Bias in Hiring", "comments": "This paper has been accepted for publication at HCOMP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although systematic biases in decision-making are widely documented, the ways\nin which they emerge from different sources is less understood. We present a\ncontrolled experimental platform to study gender bias in hiring by decoupling\nthe effect of world distribution (the gender breakdown of candidates in a\nspecific profession) from bias in human decision-making. We explore the\neffectiveness of \\textit{representation criteria}, fixed proportional display\nof candidates, as an intervention strategy for mitigation of gender bias by\nconducting experiments measuring human decision-makers' rankings for who they\nwould recommend as potential hires. Experiments across professions with varying\ngender proportions show that balancing gender representation in candidate\nslates can correct biases for some professions where the world distribution is\nskewed, although doing so has no impact on other professions where human\npersistent preferences are at play. We show that the gender of the\ndecision-maker, complexity of the decision-making task and over- and\nunder-representation of genders in the candidate slate can all impact the final\ndecision. By decoupling sources of bias, we can better isolate strategies for\nbias mitigation in human-in-the-loop systems.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 23:52:23 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Peng", "Andi", ""], ["Nushi", "Besmira", ""], ["Kiciman", "Emre", ""], ["Inkpen", "Kori", ""], ["Suri", "Siddharth", ""], ["Kamar", "Ece", ""]]}, {"id": "1909.03613", "submitter": "Aakanksha Rana", "authors": "S. M. Iman Zolanvari, Susana Ruano, Aakanksha Rana, Alan Cummins,\n  Rogerio Eduardo da Silva, Morteza Rahbar, Aljosa Smolic", "title": "DublinCity: Annotated LiDAR Point Cloud and its Applications", "comments": "Accepted to the 30th British Machine Vision Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding of full-scale 3D models of an urban area remains a\nchallenging task. While advanced computer vision techniques offer\ncost-effective approaches to analyse 3D urban elements, a precise and densely\nlabelled dataset is quintessential. The paper presents the first-ever labelled\ndataset for a highly dense Aerial Laser Scanning (ALS) point cloud at\ncity-scale. This work introduces a novel benchmark dataset that includes a\nmanually annotated point cloud for over 260 million laser scanning points into\n100'000 (approx.) assets from Dublin LiDAR point cloud [12] in 2015. Objects\nare labelled into 13 classes using hierarchical levels of detail from large\n(i.e., building, vegetation and ground) to refined (i.e., window, door and\ntree) elements. To validate the performance of our dataset, two different\napplications are showcased. Firstly, the labelled point cloud is employed for\ntraining Convolutional Neural Networks (CNNs) to classify urban elements. The\ndataset is tested on the well-known state-of-the-art CNNs (i.e., PointNet,\nPointNet++ and So-Net). Secondly, the complete ALS dataset is applied as\ndetailed ground truth for city-scale image-based 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 13:47:31 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zolanvari", "S. M. Iman", ""], ["Ruano", "Susana", ""], ["Rana", "Aakanksha", ""], ["Cummins", "Alan", ""], ["da Silva", "Rogerio Eduardo", ""], ["Rahbar", "Morteza", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1909.03616", "submitter": "Ryuta Arisaka", "authors": "Ryuta Arisaka, Makoto Hagiwara, Takayuki Ito", "title": "Formulating Manipulable Argumentation with Intra-/Inter-Agent\n  Preferences", "comments": "No major change except for some stylistic change", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From marketing to politics, exploitation of incomplete information through\nselective communication of arguments is ubiquitous. In this work, we focus on\ndevelopment of an argumentation-theoretic model for manipulable multi-agent\nargumentation, where each agent may transmit deceptive information to others\nfor tactical motives. In particular, we study characterisation of epistemic\nstates, and their roles in deception/honesty detection and (mis)trust-building.\nTo this end, we propose the use of intra-agent preferences to handle\ndeception/honesty detection and inter-agent preferences to determine which\nagent(s) to believe in more. We show how deception/honesty in an argumentation\nof an agent, if detected, would alter the agent's perceived trustworthiness,\nand how that may affect their judgement as to which arguments should be\nacceptable.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 03:29:11 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 03:45:12 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Arisaka", "Ryuta", ""], ["Hagiwara", "Makoto", ""], ["Ito", "Takayuki", ""]]}, {"id": "1909.03638", "submitter": "Hyungseok Song", "authors": "Hyungseok Song, Hyeryung Jang, Hai H. Tran, Se-eun Yoon, Kyunghwan\n  Son, Donggyu Yun, Hyoju Chung, Yung Yi", "title": "Solving Continual Combinatorial Selection via Deep Reinforcement\n  Learning", "comments": "Accepted to IJCAI 2019,14 pages,8 figures", "journal-ref": "Proceedings of the Twenty-Eighth International Joint Conference\n  Artificial Intelligence, {IJCAI-19} (2019), 3467--3474", "doi": "10.24963/ijcai.2019/481", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Markov Decision Process (MDP) of selecting a subset of items\nat each step, termed the Select-MDP (S-MDP). The large state and action spaces\nof S-MDPs make them intractable to solve with typical reinforcement learning\n(RL) algorithms especially when the number of items is huge. In this paper, we\npresent a deep RL algorithm to solve this issue by adopting the following key\nideas. First, we convert the original S-MDP into an Iterative Select-MDP\n(IS-MDP), which is equivalent to the S-MDP in terms of optimal actions. IS-MDP\ndecomposes a joint action of selecting K items simultaneously into K iterative\nselections resulting in the decrease of actions at the expense of an\nexponential increase of states. Second, we overcome this state space explo-sion\nby exploiting a special symmetry in IS-MDPs with novel weight shared\nQ-networks, which prov-ably maintain sufficient expressive power. Various\nexperiments demonstrate that our approach works well even when the item space\nis large and that it scales to environments with item spaces different from\nthose used in training.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 05:45:11 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Song", "Hyungseok", ""], ["Jang", "Hyeryung", ""], ["Tran", "Hai H.", ""], ["Yoon", "Se-eun", ""], ["Son", "Kyunghwan", ""], ["Yun", "Donggyu", ""], ["Chung", "Hyoju", ""], ["Yi", "Yung", ""]]}, {"id": "1909.03669", "submitter": "Yongcheng Liu", "authors": "Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming Xiang,\n  Chunhong Pan", "title": "DensePoint: Learning Densely Contextual Representation for Efficient\n  Point Cloud Processing", "comments": "Accepted to ICCV 2019. 15 pages, 8 figures, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud processing is very challenging, as the diverse shapes formed by\nirregular points are often indistinguishable. A thorough grasp of the elusive\nshape requires sufficiently contextual semantic information, yet few works\ndevote to this. Here we propose DensePoint, a general architecture to learn\ndensely contextual representation for point cloud processing. Technically, it\nextends regular grid CNN to irregular point configuration by generalizing a\nconvolution operator, which holds the permutation invariance of points, and\nachieves efficient inductive learning of local patterns. Architecturally, it\nfinds inspiration from dense connection mode, to repeatedly aggregate\nmulti-level and multi-scale semantics in a deep hierarchy. As a result, densely\ncontextual information along with rich semantics, can be acquired by DensePoint\nin an organic manner, making it highly effective. Extensive experiments on\nchallenging benchmarks across four tasks, as well as thorough model analysis,\nverify DensePoint achieves the state of the arts.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 07:18:30 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Yongcheng", ""], ["Fan", "Bin", ""], ["Meng", "Gaofeng", ""], ["Lu", "Jiwen", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1909.03681", "submitter": "Firuz Kamalov", "authors": "Firuz Kamalov and Ho Hon Leung", "title": "Outlier Detection in High Dimensional Data", "comments": null, "journal-ref": "Journal of Information & Knowledge Management (2020)", "doi": "10.1142/S0219649220400134", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data poses unique challenges in outlier detection process.\nMost of the existing algorithms fail to properly address the issues stemming\nfrom a large number of features. In particular, outlier detection algorithms\nperform poorly on data set of small size with a large number of features. In\nthis paper, we propose a novel outlier detection algorithm based on principal\ncomponent analysis and kernel density estimation. The proposed method is\ndesigned to address the challenges of dealing with high-dimensional data by\nprojecting the original data onto a smaller space and using the innate\nstructure of the data to calculate anomaly scores for each data point.\nNumerical experiments on synthetic and real-life data show that our method\nperforms well on high-dimensional data. In particular, the proposed method\noutperforms the benchmark methods as measured by the $F_1$-score. Our method\nalso produces better-than-average execution times compared to the benchmark\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 07:43:47 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kamalov", "Firuz", ""], ["Leung", "Ho Hon", ""]]}, {"id": "1909.03712", "submitter": "Zhao Kang", "authors": "Xiaofan Bo and Zhao Kang and Zhitong Zhao and Yuanzhang Su and Wenyu\n  Chen", "title": "Latent Multi-view Semi-Supervised Classification", "comments": "ACML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To explore underlying complementary information from multiple views, in this\npaper, we propose a novel Latent Multi-view Semi-Supervised Classification\n(LMSSC) method. Unlike most existing multi-view semi-supervised classification\nmethods that learn the graph using original features, our method seeks an\nunderlying latent representation and performs graph learning and label\npropagation based on the learned latent representation. With the\ncomplementarity of multiple views, the latent representation could depict the\ndata more comprehensively than every single view individually, accordingly\nmaking the graph more accurate and robust as well. Finally, LMSSC integrates\nlatent representation learning, graph construction, and label propagation into\na unified framework, which makes each subtask optimized. Experimental results\non real-world benchmark datasets validate the effectiveness of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 09:18:39 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Bo", "Xiaofan", ""], ["Kang", "Zhao", ""], ["Zhao", "Zhitong", ""], ["Su", "Yuanzhang", ""], ["Chen", "Wenyu", ""]]}, {"id": "1909.03739", "submitter": "Guy Tennenholtz", "authors": "Guy Tennenholtz, Shie Mannor, Uri Shalit", "title": "Off-Policy Evaluation in Partially Observable Environments", "comments": "Accepted to AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the problem of batch off-policy evaluation for\nReinforcement Learning in partially observable environments. Off-policy\nevaluation under partial observability is inherently prone to bias, with risk\nof arbitrarily large errors. We define the problem of off-policy evaluation for\nPartially Observable Markov Decision Processes (POMDPs) and establish what we\nbelieve is the first off-policy evaluation result for POMDPs. In addition, we\nformulate a model in which observed and unobserved variables are decoupled into\ntwo dynamic processes, called a Decoupled POMDP. We show how off-policy\nevaluation can be performed under this new model, mitigating estimation errors\ninherent to general POMDPs. We demonstrate the pitfalls of off-policy\nevaluation in POMDPs using a well-known off-policy method, Importance Sampling,\nand compare it with our result on synthetic medical data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 10:13:09 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 08:51:04 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 07:10:15 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Tennenholtz", "Guy", ""], ["Mannor", "Shie", ""], ["Shalit", "Uri", ""]]}, {"id": "1909.03745", "submitter": "Wanjun Zhong", "authors": "Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou,\n  Jiahai Wang, Jian Yin", "title": "Reasoning Over Semantic-Level Graph for Fact Checking", "comments": "9pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fact checking is a challenging task because verifying the truthfulness of a\nclaim requires reasoning about multiple retrievable evidence. In this work, we\npresent a method suitable for reasoning about the semantic-level structure of\nevidence. Unlike most previous works, which typically represent evidence\nsentences with either string concatenation or fusing the features of isolated\nevidence sentences, our approach operates on rich semantic structures of\nevidence obtained by semantic role labeling. We propose two mechanisms to\nexploit the structure of evidence while leveraging the advances of pre-trained\nmodels like BERT, GPT or XLNet. Specifically, using XLNet as the backbone, we\nfirst utilize the graph structure to re-define the relative distances of words,\nwith the intuition that semantically related words should have short distances.\nThen, we adopt graph convolutional network and graph attention network to\npropagate and aggregate information from neighboring nodes on the graph. We\nevaluate our system on FEVER, a benchmark dataset for fact checking, and find\nthat rich structural information is helpful and both our graph-based mechanisms\nimprove the accuracy. Our model is the state-of-the-art system in terms of both\nofficial evaluation metrics, namely claim verification accuracy and FEVER\nscore.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 10:34:09 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 07:33:15 GMT"}, {"version": "v3", "created": "Sat, 25 Apr 2020 06:48:06 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zhong", "Wanjun", ""], ["Xu", "Jingjing", ""], ["Tang", "Duyu", ""], ["Xu", "Zenan", ""], ["Duan", "Nan", ""], ["Zhou", "Ming", ""], ["Wang", "Jiahai", ""], ["Yin", "Jian", ""]]}, {"id": "1909.03759", "submitter": "Danish Contractor", "authors": "Nikhil Verma and Abhishek Sharma and Dhiraj Madan and Danish\n  Contractor and Harshit Kumar and Sachindra Joshi", "title": "Neural Conversational QA: Learning to Reason v.s. Exploiting Patterns", "comments": "Accepted at EMNLP 2020. NOTE: An older version of this paper\n  presented a model called 'UrcaNet'. Please view the v1 version of this paper\n  on arxiv for details on that model. This version does not contain UrcaNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Conversational QA tasks like ShARC require systems to answer questions\nbased on the contents of a given passage. On studying recent state-of-the-art\nmodels on the ShARCQA task, we found indications that the models learn spurious\nclues/patterns in the dataset. Furthermore, we show that a heuristic-based\nprogram designed to exploit these patterns can have performance comparable to\nthat of the neural models. In this paper we share our findings about four types\nof patterns found in the ShARC corpus and describe how neural models exploit\nthem. Motivated by the aforementioned findings, we create and share a modified\ndataset that has fewer spurious patterns, consequently allowing models to learn\nbetter.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 11:05:15 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 07:43:53 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Verma", "Nikhil", ""], ["Sharma", "Abhishek", ""], ["Madan", "Dhiraj", ""], ["Contractor", "Danish", ""], ["Kumar", "Harshit", ""], ["Joshi", "Sachindra", ""]]}, {"id": "1909.03772", "submitter": "Nicolai Anton Lynnerup", "authors": "Nicolai A. Lynnerup, Laura Nolling, Rasmus Hasle, John Hallam", "title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning\n  Algorithms on Real-World Robots", "comments": "Appears in Proceedings of the Third Conference on Robot Learning\n  (CoRL 2019). Companion source code at\n  https://github.com/dti-research/SenseActExperiments/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As reinforcement learning (RL) achieves more success in solving complex\ntasks, more care is needed to ensure that RL research is reproducible and that\nalgorithms herein can be compared easily and fairly with minimal bias. RL\nresults are, however, notoriously hard to reproduce due to the algorithms'\nintrinsic variance, the environments' stochasticity, and numerous (potentially\nunreported) hyper-parameters. In this work we investigate the many issues\nleading to irreproducible research and how to manage those. We further show how\nto utilise a rigorous and standardised evaluation approach for easing the\nprocess of documentation, evaluation and fair comparison of different\nalgorithms, where we emphasise the importance of choosing the right measurement\nmetrics and conducting proper statistics on the results, for unbiased reporting\nof the results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 11:33:09 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 07:42:00 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Lynnerup", "Nicolai A.", ""], ["Nolling", "Laura", ""], ["Hasle", "Rasmus", ""], ["Hallam", "John", ""]]}, {"id": "1909.03794", "submitter": "Zhiwei Lin", "authors": "Lianbo Ma, Peng Sun, Zhiwei Lin, Hui Wang", "title": "Composing Knowledge Graph Embeddings via Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning knowledge graph embedding from an existing knowledge graph is very\nimportant to knowledge graph completion. For a fact $(h,r,t)$ with the head\nentity $h$ having a relation $r$ with the tail entity $t$, the current\napproaches aim to learn low dimensional representations\n$(\\mathbf{h},\\mathbf{r},\\mathbf{t})$, each of which corresponds to the elements\nin $(h, r, t)$, respectively. As $(\\mathbf{h},\\mathbf{r},\\mathbf{t})$ is\nlearned from the existing facts within a knowledge graph, these representations\ncan not be used to detect unknown facts (if the entities or relations never\noccur in the knowledge graph).\n  This paper proposes a new approach called TransW, aiming to go beyond the\ncurrent work by composing knowledge graph embeddings using word embeddings.\nGiven the fact that an entity or a relation contains one or more words (quite\noften), it is sensible to learn a mapping function from word embedding spaces\nto knowledge embedding spaces, which shows how entities are constructed using\nhuman words. More importantly, composing knowledge embeddings using word\nembeddings makes it possible to deal with the emerging new facts (either new\nentities or relations). Experimental results using three public datasets show\nthe consistency and outperformance of the proposed TransW.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 12:22:28 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Ma", "Lianbo", ""], ["Sun", "Peng", ""], ["Lin", "Zhiwei", ""], ["Wang", "Hui", ""]]}, {"id": "1909.03798", "submitter": "Feng Chen", "authors": "Xin Su, Shangqi Guo and Feng Chen", "title": "Subjectivity Learning Theory towards Artificial General Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of artificial general intelligence (AGI) was a long-term\ngoal of AI research aiming to deal with the complex data in the real world and\nmake reasonable judgments in various cases like a human. However, the current\nAI creations, referred to as \"Narrow AI\", are limited to a specific problem.\nThe constraints come from two basic assumptions of data, which are independent\nand identical distributed samples and single-valued mapping between inputs and\noutputs. We completely break these constraints and develop the subjectivity\nlearning theory for general intelligence. We assign the mathematical meaning\nfor the philosophical concept of subjectivity and build the data representation\nof general intelligence. Under the subjectivity representation, then the global\nrisk is constructed as the new learning goal. We prove that subjectivity\nlearning holds a lower risk bound than traditional machine learning. Moreover,\nwe propose the principle of empirical global risk minimization (EGRM) as the\nsubjectivity learning process in practice, establish the condition of\nconsistency, and present triple variables for controlling the total risk bound.\nThe subjectivity learning is a novel learning theory for unconstrained real\ndata and provides a path to develop AGI.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 12:29:32 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 03:20:11 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Su", "Xin", ""], ["Guo", "Shangqi", ""], ["Chen", "Feng", ""]]}, {"id": "1909.03812", "submitter": "Anastasia Ingacheva", "authors": "Alexander Sheshkus, Anastasia Ingacheva, Vladimir Arlazarov, Dmitry\n  Nikolaev", "title": "HoughNet: neural network architecture for vanishing points detection", "comments": "6 pages, 6 figures, 2 tables, 28 references, conference", "journal-ref": "15th International Conference on Document Analysis and Recognition\n  (ICDAR 2019)", "doi": "10.1109/ICDAR.2019.00140", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel neural network architecture based on Fast\nHough Transform layer. The layer of this type allows our neural network to\naccumulate features from linear areas across the entire image instead of local\nareas. We demonstrate its potential by solving the problem of vanishing points\ndetection in the images of documents. Such problem occurs when dealing with\ncamera shots of the documents in uncontrolled conditions. In this case, the\ndocument image can suffer several specific distortions including projective\ntransform. To train our model, we use MIDV-500 dataset and provide testing\nresults. The strong generalization ability of the suggested method is proven\nwith its applying to a completely different ICDAR 2011 dewarping contest. In\npreviously published papers considering these dataset authors measured the\nquality of vanishing point detection by counting correctly recognized words\nwith open OCR engine Tesseract. To compare with them, we reproduce this\nexperiment and show that our method outperforms the state-of-the-art result.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 12:45:19 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 07:41:38 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Sheshkus", "Alexander", ""], ["Ingacheva", "Anastasia", ""], ["Arlazarov", "Vladimir", ""], ["Nikolaev", "Dmitry", ""]]}, {"id": "1909.03814", "submitter": "Dmytro Pukhkaiev", "authors": "Dmytro Pukhkaiev, Uwe A{\\ss}mann", "title": "Parameter Tuning for Self-optimizing Software at Scale", "comments": "To appear in Workshop on Model Selection and Parameter Tuning in\n  Recommender Systems (MoST-Rec'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiency of self-optimizing systems is heavily dependent on their\noptimization strategies, e.g., choosing exact or approximate solver. A choice\nof such a strategy, in turn, is influenced by numerous factors, such as\nre-optimization time, size of the problem, optimality constraints, etc. Exact\nsolvers are domain-independent and can guarantee optimality but suffer from\nscaling, while approximate solvers offer a \"good-enough\" solution in exchange\nfor a lack of generality and parameter-dependence. In this paper we discuss the\ntrade-offs between exact and approximate optimizers for solving a quality-based\nsoftware selection and hardware mapping problem from the scalability\nperspective. We show that even a simple heuristic can compete with thoroughly\ndeveloped exact solvers under condition of an effective parameter tuning.\nMoreover, we discuss robustness of the obtained algorithm's configuration. Last\nbut not least, we present a software product line for parameter tuning, which\ncomprise the main features of this process and can serve as a platform for\nfurther research in the area of parameter tuning.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 12:50:24 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Pukhkaiev", "Dmytro", ""], ["A\u00dfmann", "Uwe", ""]]}, {"id": "1909.03820", "submitter": "Steffen Van Bergerem", "authors": "Steffen van Bergerem", "title": "Learning Concepts Definable in First-Order Logic with Counting", "comments": null, "journal-ref": "34th Annual ACM/IEEE Symposium on Logic in Computer Science, LICS\n  2019, Vancouver, BC, Canada, June 24-27, 2019", "doi": "10.1109/LICS.2019.8785811", "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study classification problems over relational background structures for\nhypotheses that are defined using logics with counting. The aim of this paper\nis to find learning algorithms running in time sublinear in the size of the\nbackground structure. We show that hypotheses defined by FOCN(P)-formulas over\nstructures of polylogarithmic degree can be learned in sublinear time.\nFurthermore, we prove that for structures of unbounded degree there is no\nsublinear learning algorithm for first-order formulas.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 12:57:29 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["van Bergerem", "Steffen", ""]]}, {"id": "1909.03821", "submitter": "Takuma Ebisu", "authors": "Takuma Ebisu, Ryutaro Ichise", "title": "Combination of Unified Embedding Model and Observed Features for\n  Knowledge Graph Completion", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs are useful for many artificial intelligence tasks but often\nhave missing data. Hence, a method for completing knowledge graphs is required.\nExisting approaches include embedding models, the Path Ranking Algorithm, and\nrule evaluation models. However, these approaches have limitations. For\nexample, all the information is mixed and difficult to interpret in embedding\nmodels, and traditional rule evaluation models are basically slow. In this\npaper, we provide an integrated view of various approaches and combine them to\ncompensate for their limitations. We first unify state-of-the-art embedding\nmodels, such as ComplEx and TorusE, reinterpreting them as a variant of\ntranslation-based models. Then, we show that these models utilize paths for\nlink prediction and propose a method for evaluating rules based on this idea.\nFinally, we combine an embedding model and observed feature models to predict\nmissing triples. This is possible because all of these models utilize paths. We\nalso conduct experiments, including link prediction tasks, with standard\ndatasets to evaluate our method and framework. The experiments show that our\nmethod can evaluate rules faster than traditional methods and that our\nframework outperforms state-of-the-art models in terms of link prediction.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 12:58:16 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 10:00:41 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Ebisu", "Takuma", ""], ["Ichise", "Ryutaro", ""]]}, {"id": "1909.03834", "submitter": "Dongsheng Ruan", "authors": "Dongsheng Ruan and Jun Wen and Nenggan Zheng and Min Zheng", "title": "Linear Context Transform Block", "comments": "AAAI-2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Squeeze-and-Excitation (SE) block presents a channel attention mechanism for\nmodeling global context via explicitly capturing dependencies across channels.\nHowever, we are still far from understanding how the SE block works. In this\nwork, we first revisit the SE block, and then present a detailed empirical\nstudy of the relationship between global context and attention distribution,\nbased on which we propose a simple yet effective module, called Linear Context\nTransform (LCT) block. We divide all channels into different groups and\nnormalize the globally aggregated context features within each channel group,\nreducing the disturbance from irrelevant channels. Through linear transform of\nthe normalized context features, we model global context for each channel\nindependently. The LCT block is extremely lightweight and easy to be plugged\ninto different backbone models while with negligible parameters and\ncomputational burden increase. Extensive experiments show that the LCT block\noutperforms the SE block in image classification task on the ImageNet and\nobject detection/segmentation on the COCO dataset with different backbone\nmodels. Moreover, LCT yields consistent performance gains over existing\nstate-of-the-art detection architectures, e.g., 1.5$\\sim$1.7% AP$^{bbox}$ and\n1.0$\\sim$1.2% AP$^{mask}$ improvements on the COCO benchmark, irrespective of\ndifferent baseline models of varied capacities. We hope our simple yet\neffective approach will shed some light on future research of attention-based\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 12:31:28 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 10:57:33 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Ruan", "Dongsheng", ""], ["Wen", "Jun", ""], ["Zheng", "Nenggan", ""], ["Zheng", "Min", ""]]}, {"id": "1909.03862", "submitter": "Yinhe Zheng Dr.", "authors": "Yinhe Zheng, Guanyi Chen, Minlie Huang", "title": "Out-of-domain Detection for Natural Language Understanding in Dialog\n  Systems", "comments": "Accepted by TALSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Understanding (NLU) is a vital component of dialogue\nsystems, and its ability to detect Out-of-Domain (OOD) inputs is critical in\npractical applications, since the acceptance of the OOD input that is\nunsupported by the current system may lead to catastrophic failure. However,\nmost existing OOD detection methods rely heavily on manually labeled OOD\nsamples and cannot take full advantage of unlabeled data. This limits the\nfeasibility of these models in practical applications.\n  In this paper, we propose a novel model to generate high-quality pseudo OOD\nsamples that are akin to IN-Domain (IND) input utterances, and thereby improves\nthe performance of OOD detection. To this end, an autoencoder is trained to map\nan input utterance into a latent code. and the codes of IND and OOD samples are\ntrained to be indistinguishable by utilizing a generative adversarial network.\nTo provide more supervision signals, an auxiliary classifier is introduced to\nregularize the generated OOD samples to have indistinguishable intent labels.\nExperiments show that these pseudo OOD samples generated by our model can be\nused to effectively improve OOD detection in NLU. Besides, we also demonstrate\nthat the effectiveness of these pseudo OOD data can be further improved by\nefficiently utilizing unlabeled data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 13:49:10 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 02:50:03 GMT"}, {"version": "v3", "created": "Sun, 22 Mar 2020 01:58:50 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Zheng", "Yinhe", ""], ["Chen", "Guanyi", ""], ["Huang", "Minlie", ""]]}, {"id": "1909.03868", "submitter": "Florian K\\\"opf", "authors": "Florian K\\\"opf, Alexander Nitsch, Michael Flad and S\\\"oren Hohmann", "title": "Partner Approximating Learners (PAL): Simulation-Accelerated Learning\n  with Explicit Partner Modeling in Multi-Agent Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed cooperative-competitive control scenarios such as human-machine\ninteraction with individual goals of the interacting partners are very\nchallenging for reinforcement learning agents. In order to contribute towards\nintuitive human-machine collaboration, we focus on problems in the continuous\nstate and control domain where no explicit communication is considered and the\nagents do not know the others' goals or control laws but only sense their\ncontrol inputs retrospectively. Our proposed framework combines a learned\npartner model based on online data with a reinforcement learning agent that is\ntrained in a simulated environment including the partner model. Thus, we\novercome drawbacks of independent learners and, in addition, benefit from a\nreduced amount of real world data required for reinforcement learning which is\nvital in the human-machine context. We finally analyze an example that\ndemonstrates the merits of our proposed framework which learns fast due to the\nsimulated environment and adapts to the continuously changing partner due to\nthe partner approximation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 13:58:15 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 09:41:21 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 17:17:32 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["K\u00f6pf", "Florian", ""], ["Nitsch", "Alexander", ""], ["Flad", "Michael", ""], ["Hohmann", "S\u00f6ren", ""]]}, {"id": "1909.03881", "submitter": "Sahil Garg", "authors": "Sahil Garg, Aram Galstyan, Greg Ver Steeg, Guillermo Cecchi", "title": "Nearly-Unsupervised Hashcode Representations for Relation Extraction", "comments": "Proceedings of EMNLP-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, kernelized locality sensitive hashcodes have been successfully\nemployed as representations of natural language text, especially showing high\nrelevance to biomedical relation extraction tasks. In this paper, we propose to\noptimize the hashcode representations in a nearly unsupervised manner, in which\nwe only use data points, but not their class labels, for learning. The\noptimized hashcode representations are then fed to a supervised classifier\nfollowing the prior work. This nearly unsupervised approach allows fine-grained\noptimization of each hash function, which is particularly suitable for building\nhashcode representations generalizing from a training set to a test set. We\nempirically evaluate the proposed approach for biomedical relation extraction\ntasks, obtaining significant accuracy improvements w.r.t. state-of-the-art\nsupervised and semi-supervised approaches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 14:20:05 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Garg", "Sahil", ""], ["Galstyan", "Aram", ""], ["Steeg", "Greg Ver", ""], ["Cecchi", "Guillermo", ""]]}, {"id": "1909.03889", "submitter": "Guangcan Liu", "authors": "Guangcan Liu, Wayne Zhang", "title": "Recovery of Future Data via Convolution Nuclear Norm Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of time series forecasting (TSF) from the\nperspective of compressed sensing. First of all, we convert TSF into a more\ninclusive problem called tensor completion with arbitrary sampling (TCAS),\nwhich is to restore a tensor from a subset of its entries sampled in an\narbitrary manner. While it is known that, in the framework of Tucker\nlow-rankness, it is theoretically impossible to identify the target tensor\nbased on some arbitrarily selected entries, in this work we shall show that\nTCAS is indeed tackleable in the light of a new concept called convolutional\nlow-rankness, which is a generalization of the well-known Fourier sparsity.\nThen we introduce a convex program termed Convolution Nuclear Norm Minimization\n(CNNM), and we prove that CNNM succeeds in solving TCAS as long as a sampling\ncondition--which depends on the convolution rank of the target tensor--is\nobeyed. Experiments on univariate time series, images and videos show\nencouraging results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 07:52:22 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 08:00:23 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 08:04:08 GMT"}, {"version": "v4", "created": "Fri, 28 May 2021 08:12:17 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Liu", "Guangcan", ""], ["Zhang", "Wayne", ""]]}, {"id": "1909.03906", "submitter": "Kristopher De Asis", "authors": "Kristopher De Asis, Alan Chan, Silviu Pitis, Richard S. Sutton, Daniel\n  Graves", "title": "Fixed-Horizon Temporal Difference Methods for Stable Reinforcement\n  Learning", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore fixed-horizon temporal difference (TD) methods, reinforcement\nlearning algorithms for a new kind of value function that predicts the sum of\nrewards over a $\\textit{fixed}$ number of future time steps. To learn the value\nfunction for horizon $h$, these algorithms bootstrap from the value function\nfor horizon $h-1$, or some shorter horizon. Because no value function\nbootstraps from itself, fixed-horizon methods are immune to the stability\nproblems that plague other off-policy TD methods using function approximation\n(also known as \"the deadly triad\"). Although fixed-horizon methods require the\nstorage of additional value functions, this gives the agent additional\npredictive power, while the added complexity can be substantially reduced via\nparallel updates, shared weights, and $n$-step bootstrapping. We show how to\nuse fixed-horizon value functions to solve reinforcement learning problems\ncompetitively with methods such as Q-learning that learn conventional value\nfunctions. We also prove convergence of fixed-horizon temporal difference\nmethods with linear and general function approximation. Taken together, our\nresults establish fixed-horizon TD methods as a viable new way of avoiding the\nstability problems of the deadly triad.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 14:57:42 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 04:54:49 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["De Asis", "Kristopher", ""], ["Chan", "Alan", ""], ["Pitis", "Silviu", ""], ["Sutton", "Richard S.", ""], ["Graves", "Daniel", ""]]}, {"id": "1909.03922", "submitter": "Dongyeop Kang", "authors": "Dongyeop Kang, Anusha Balakrishnan, Pararth Shah, Paul Crook, Y-Lan\n  Boureau, Jason Weston", "title": "Recommendation as a Communication Game: Self-Supervised Bot-Play for\n  Goal-oriented Dialogue", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional recommendation systems produce static rather than interactive\nrecommendations invariant to a user's specific requests, clarifications, or\ncurrent mood, and can suffer from the cold-start problem if their tastes are\nunknown. These issues can be alleviated by treating recommendation as an\ninteractive dialogue task instead, where an expert recommender can sequentially\nask about someone's preferences, react to their requests, and recommend more\nappropriate items. In this work, we collect a goal-driven recommendation\ndialogue dataset (GoRecDial), which consists of 9,125 dialogue games and 81,260\nconversation turns between pairs of human workers recommending movies to each\nother. The task is specifically designed as a cooperative game between two\nplayers working towards a quantifiable common goal. We leverage the dataset to\ndevelop an end-to-end dialogue system that can simultaneously converse and\nrecommend. Models are first trained to imitate the behavior of human players\nwithout considering the task goal itself (supervised training). We then\nfinetune our models on simulated bot-bot conversations between two paired\npre-trained models (bot-play), in order to achieve the dialogue goal. Our\nexperiments show that models finetuned with bot-play learn improved dialogue\nstrategies, reach the dialogue goal more often when paired with a human, and\nare rated as more consistent by humans compared to models trained without\nbot-play. The dataset and code are publicly available through the ParlAI\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 15:19:56 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Kang", "Dongyeop", ""], ["Balakrishnan", "Anusha", ""], ["Shah", "Pararth", ""], ["Crook", "Paul", ""], ["Boureau", "Y-Lan", ""], ["Weston", "Jason", ""]]}, {"id": "1909.03934", "submitter": "Jan Karwowski", "authors": "Jan Karwowski and Jacek Ma\\'ndziuk", "title": "Double-oracle sampling method for Stackelberg Equilibrium approximation\n  in general-sum extensive-form games", "comments": null, "journal-ref": null, "doi": "10.1609/aaai.v34i02.5578", "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a new method for approximating Strong Stackelberg\nEquilibrium in general-sum sequential games with imperfect information and\nperfect recall. The proposed approach is generic as it does not rely on any\nspecific properties of a particular game model. The method is based on\niterative interleaving of the two following phases: (1) guided Monte Carlo Tree\nSearch sampling of the Follower's strategy space and (2) building the Leader's\nbehavior strategy tree for which the sampled Follower's strategy is an optimal\nresponse. The above solution scheme is evaluated with respect to expected\nLeader's utility and time requirements on three sets of interception games with\nvariable characteristics, played on graphs. A comparison with three\nstate-of-the-art MILP/LP-based methods shows that in vast majority of test\ncases proposed simulation-based approach leads to optimal Leader's strategies,\nwhile excelling the competitive methods in terms of better time scalability and\nlower memory requirements.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 15:34:04 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Karwowski", "Jan", ""], ["Ma\u0144dziuk", "Jacek", ""]]}, {"id": "1909.03983", "submitter": "Debarpita Santra", "authors": "Debarpita Santra, S. K. Basu, J. K. Mondal, Subrata Goswami", "title": "Lattice-Based Fuzzy Medical Expert System for Low Back Pain Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low Back Pain (LBP) is a common medical condition that deprives many\nindividuals worldwide of their normal routine activities. In the absence of\nexternal biomarkers, diagnosis of LBP is quite challenging. It requires dealing\nwith several clinical variables, which have no precisely quantified values.\nAiming at the development of a fuzzy medical expert system for LBP management,\nthis research proposes an attractive lattice-based knowledge representation\nscheme for handling imprecision in knowledge, offering a suitable design\nmethodology for a fuzzy knowledge base and a fuzzy inference system. The fuzzy\nknowledge base is constructed in modular fashion, with each module capturing\ninterrelated medical knowledge about the relevant clinical history, clinical\nexaminations and laboratory investigation results. This approach in design\nensures optimality, consistency and preciseness in the knowledge base and\nscalability. The fuzzy inference system, which uses the Mamdani method, adopts\nthe triangular membership function for fuzzification and the Centroid of Area\ntechnique for defuzzification. A prototype of this system has been built using\nthe knowledge extracted from the domain expert physicians. The inference of the\nsystem against a few available patient records at the ESI Hospital, Sealdah has\nbeen checked. It was found to be acceptable by the verifying medical experts.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 16:44:51 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Santra", "Debarpita", ""], ["Basu", "S. K.", ""], ["Mondal", "J. K.", ""], ["Goswami", "Subrata", ""]]}, {"id": "1909.03984", "submitter": "Alberto Maria Metelli", "authors": "Alberto Maria Metelli, Guglielmo Manneschi, Marcello Restelli", "title": "Policy Space Identification in Configurable Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of identifying the policy space of a learning agent,\nhaving access to a set of demonstrations generated by its optimal policy. We\nintroduce an approach based on statistical testing to identify the set of\npolicy parameters the agent can control, within a larger parametric policy\nspace. After presenting two identification rules (combinatorial and\nsimplified), applicable under different assumptions on the policy space, we\nprovide a probabilistic analysis of the simplified one in the case of linear\npolicies belonging to the exponential family. To improve the performance of our\nidentification rules, we frame the problem in the recently introduced framework\nof the Configurable Markov Decision Processes, exploiting the opportunity of\nconfiguring the environment to induce the agent revealing which parameters it\ncan control. Finally, we provide an empirical evaluation, on both discrete and\ncontinuous domains, to prove the effectiveness of our identification rules.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 16:47:27 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Metelli", "Alberto Maria", ""], ["Manneschi", "Guglielmo", ""], ["Restelli", "Marcello", ""]]}, {"id": "1909.03987", "submitter": "Debarpita Santra", "authors": "Debarpita Santra, Jyotsna Kumar Mandal, Swapan Kumar Basu, Subrata\n  Goswami", "title": "Addressing Design Issues in Medical Expert System for Low Back Pain\n  Management: Knowledge Representation, Inference Mechanism, and Conflict\n  Resolution Using Bayesian Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aiming at developing a medical expert system for low back pain management,\nthe paper proposes an efficient knowledge representation scheme using frame\ndata structures, and also derives a reliable resolution logic through Bayesian\nNetwork. When a patient comes to the intended expert system for diagnosis, the\nproposed inference engine outputs a number of probable diseases in sorted\norder, with each disease being associated with a numeric measure to indicate\nits possibility of occurrence. When two or more diseases in the list have the\nsame or closer possibility of occurrence, Bayesian Network is used for conflict\nresolution. The proposed scheme has been validated with cases of empirically\nselected thirty patients. Considering the expected value 0.75 as level of\nacceptance, the proposed system offers the diagnostic inference with the\nstandard deviation of 0.029. The computational value of Chi-Squared test has\nbeen obtained as 11.08 with 12 degree of freedom, implying that the derived\nresults from the designed system conform the homogeneity with the expected\noutcomes. Prior to any clinical investigations on the selected low back pain\npatients, the accuracy level (average) of 73.89% has been achieved by the\nproposed system, which is quite close to the expected clinical accuracy level\nof 75%.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 16:55:30 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Santra", "Debarpita", ""], ["Mandal", "Jyotsna Kumar", ""], ["Basu", "Swapan Kumar", ""], ["Goswami", "Subrata", ""]]}, {"id": "1909.04019", "submitter": "Yang Li", "authors": "Yang Li and Jos\\'e M. F. Moura", "title": "Forecaster: A Graph Transformer for Forecasting Spatial and\n  Time-Dependent Data", "comments": null, "journal-ref": "in European Conference on Artificial Intelligence (ECAI), pp. 1293\n  - 1300, 2020", "doi": "10.3233/FAIA200231", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial and time-dependent data is of interest in many applications. This\ntask is difficult due to its complex spatial dependency, long-range temporal\ndependency, data non-stationarity, and data heterogeneity. To address these\nchallenges, we propose Forecaster, a graph Transformer architecture.\nSpecifically, we start by learning the structure of the graph that\nparsimoniously represents the spatial dependency between the data at different\nlocations. Based on the topology of the graph, we sparsify the Transformer to\naccount for the strength of spatial dependency, long-range temporal dependency,\ndata non-stationarity, and data heterogeneity. We evaluate Forecaster in the\nproblem of forecasting taxi ride-hailing demand and show that our proposed\narchitecture significantly outperforms the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 17:58:43 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 20:41:59 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 12:19:38 GMT"}, {"version": "v4", "created": "Tue, 14 Jan 2020 19:53:12 GMT"}, {"version": "v5", "created": "Fri, 21 Feb 2020 00:00:14 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Li", "Yang", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "1909.04060", "submitter": "Alireza Vafaei Sadr", "authors": "Alireza Vafaei Sadr, Bruce A. Bassett and Martin Kunz", "title": "A Flexible Framework for Anomaly Detection via Dimensionality Reduction", "comments": "6 pages", "journal-ref": "Proceeding, 6th International Conference on Soft Computing &\n  Machine Intelligence (ISCMI), Johannesburg, South Africa, 2019, pp. 106-110", "doi": "10.1109/ISCMI47871.2019.9004400", "report-no": null, "categories": "cs.LG astro-ph.IM cs.AI stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is challenging, especially for large datasets in high\ndimensions. Here we explore a general anomaly detection framework based on\ndimensionality reduction and unsupervised clustering. We release DRAMA, a\ngeneral python package that implements the general framework with a wide range\nof built-in options. We test DRAMA on a wide variety of simulated and real\ndatasets, in up to 3000 dimensions, and find it robust and highly competitive\nwith commonly-used anomaly detection algorithms, especially in high dimensions.\nThe flexibility of the DRAMA framework allows for significant optimization once\nsome examples of anomalies are available, making it ideal for online anomaly\ndetection, active learning and highly unbalanced datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:00:12 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Sadr", "Alireza Vafaei", ""], ["Bassett", "Bruce A.", ""], ["Kunz", "Martin", ""]]}, {"id": "1909.04063", "submitter": "Thomas Barrett Dr", "authors": "Thomas D. Barrett, William R. Clements, Jakob N. Foerster, A. I.\n  Lvovsky", "title": "Exploratory Combinatorial Optimization with Reinforcement Learning", "comments": "In Proceedings of the 34th National Conference on Artificial\n  Intelligence, AAAI 2020", "journal-ref": "Proceedings of Thirty-fourth AAAI conference on artificial\n  intelligence, 3243-3250 (2020)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world problems can be reduced to combinatorial optimization on a\ngraph, where the subset or ordering of vertices that maximize some objective\nfunction must be found. With such tasks often NP-hard and analytically\nintractable, reinforcement learning (RL) has shown promise as a framework with\nwhich efficient heuristic methods to tackle these problems can be learned.\nPrevious works construct the solution subset incrementally, adding one element\nat a time, however, the irreversible nature of this approach prevents the agent\nfrom revising its earlier decisions, which may be necessary given the\ncomplexity of the optimization task. We instead propose that the agent should\nseek to continuously improve the solution by learning to explore at test time.\nOur approach of exploratory combinatorial optimization (ECO-DQN) is, in\nprinciple, applicable to any combinatorial problem that can be defined on a\ngraph. Experimentally, we show our method to produce state-of-the-art RL\nperformance on the Maximum Cut problem. Moreover, because ECO-DQN can start\nfrom any arbitrary configuration, it can be combined with other search methods\nto further improve performance, which we demonstrate using a simple random\nsearch.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:00:24 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 13:38:56 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Barrett", "Thomas D.", ""], ["Clements", "William R.", ""], ["Foerster", "Jakob N.", ""], ["Lvovsky", "A. I.", ""]]}, {"id": "1909.04068", "submitter": "Pratyush Maini", "authors": "Pratyush Maini, Eric Wong and J. Zico Kolter", "title": "Adversarial Robustness Against the Union of Multiple Perturbation Models", "comments": "ICML 2020 Final Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the susceptibility of deep learning systems to adversarial attacks,\nthere has been a great deal of work in developing (both empirically and\ncertifiably) robust classifiers. While most work has defended against a single\ntype of attack, recent work has looked at defending against multiple\nperturbation models using simple aggregations of multiple attacks. However,\nthese methods can be difficult to tune, and can easily result in imbalanced\ndegrees of robustness to individual perturbation models, resulting in a\nsub-optimal worst-case loss over the union. In this work, we develop a natural\ngeneralization of the standard PGD-based procedure to incorporate multiple\nperturbation models into a single attack, by taking the worst-case over all\nsteepest descent directions. This approach has the advantage of directly\nconverging upon a trade-off between different perturbation models which\nminimizes the worst-case performance over the union. With this approach, we are\nable to train standard architectures which are simultaneously robust against\n$\\ell_\\infty$, $\\ell_2$, and $\\ell_1$ attacks, outperforming past approaches on\nthe MNIST and CIFAR10 datasets and achieving adversarial accuracy of 47.0%\nagainst the union of ($\\ell_\\infty$, $\\ell_2$, $\\ell_1$) perturbations with\nradius = (0.03, 0.5, 12) on the latter, improving upon previous approaches\nwhich achieve 40.6% accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:02:09 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 21:44:04 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Maini", "Pratyush", ""], ["Wong", "Eric", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1909.04076", "submitter": "Lianhui Qin", "authors": "Lianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula,\n  Elizabeth Clark and Yejin Choi", "title": "Counterfactual Story Reasoning and Generation", "comments": "Accepted to EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual reasoning requires predicting how alternative events, contrary\nto what actually happened, might have resulted in different outcomes. Despite\nbeing considered a necessary component of AI-complete systems, few resources\nhave been developed for evaluating counterfactual reasoning in narratives.\n  In this paper, we propose Counterfactual Story Rewriting: given an original\nstory and an intervening counterfactual event, the task is to minimally revise\nthe story to make it compatible with the given counterfactual event. Solving\nthis task will require deep understanding of causal narrative chains and\ncounterfactual invariance, and integration of such story reasoning capabilities\ninto conditional language generation models.\n  We present TimeTravel, a new dataset of 29,849 counterfactual rewritings,\neach with the original story, a counterfactual event, and human-generated\nrevision of the original story compatible with the counterfactual event.\nAdditionally, we include 80,115 counterfactual \"branches\" without a rewritten\nstoryline to support future work on semi- or un-supervised approaches to\ncounterfactual story rewriting.\n  Finally, we evaluate the counterfactual rewriting capacities of several\ncompetitive baselines based on pretrained language models, and assess whether\ncommon overlap and model-based automatic metrics for text generation correlate\nwell with human scores for counterfactual rewriting.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:08:35 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 06:19:50 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Qin", "Lianhui", ""], ["Bosselut", "Antoine", ""], ["Holtzman", "Ari", ""], ["Bhagavatula", "Chandra", ""], ["Clark", "Elizabeth", ""], ["Choi", "Yejin", ""]]}, {"id": "1909.04078", "submitter": "Riccardo La Grassa", "authors": "Riccardo La Grassa, Ignazio Gallo, Alessandro Calefati, Dimitri\n  Ognibene", "title": "A Classification Methodology based on Subspace Graphs Learning", "comments": "8 pages, Dicta Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a design methodology for one-class classifiers\nusing an ensemble-of-classifiers approach. The objective is to select the best\nstructures created during the training phase using an ensemble of spanning\ntrees. It takes the best classifier, partitioning the area near a pattern into\n$\\gamma^{\\gamma-2}$ sub-spaces and combining all possible spanning trees that\ncan be created starting from $\\gamma$ nodes. The proposed method leverages on a\nsupervised classification methodology and the concept of minimum distance. We\nevaluate our approach on well-known benchmark datasets and results obtained\ndemonstrate that it achieves comparable and, in many cases, state-of-the-art\nresults. Moreover, it obtains good performance even with unbalanced datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:10:06 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["La Grassa", "Riccardo", ""], ["Gallo", "Ignazio", ""], ["Calefati", "Alessandro", ""], ["Ognibene", "Dimitri", ""]]}, {"id": "1909.04104", "submitter": "Zengming Shen", "authors": "Zengming Shen, Yifan Chen, S.Kevin Zhou, Bogdan Georgescu, Xuqi Liu,\n  Thomas S. Huang", "title": "Towards Learning a Self-inverse Network for Bidirectional Image-to-image\n  Translation", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The one-to-one mapping is necessary for many bidirectional image-to-image\ntranslation applications, such as MRI image synthesis as MRI images are unique\nto the patient. State-of-the-art approaches for image synthesis from domain X\nto domain Y learn a convolutional neural network that meticulously maps between\nthe domains. A different network is typically implemented to map along the\nopposite direction, from Y to X. In this paper, we explore the possibility of\nonly wielding one network for bi-directional image synthesis. In other words,\nsuch an autonomous learning network implements a self-inverse function. A\nself-inverse network shares several distinct advantages: only one network\ninstead of two, better generalization and more restricted parameter space. Most\nimportantly, a self-inverse function guarantees a one-to-one mapping, a\nproperty that cannot be guaranteed by earlier approaches that are not\nself-inverse. The experiments on three datasets show that, compared with the\nbaseline approaches that use two separate models for the image synthesis along\ntwo directions, our self-inverse network achieves better synthesis results in\nterms of standard metrics. Finally, our sensitivity analysis confirms the\nfeasibility of learning a self-inverse function for the bidirectional image\ntranslation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:56:30 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 20:46:02 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Shen", "Zengming", ""], ["Chen", "Yifan", ""], ["Zhou", "S. Kevin", ""], ["Georgescu", "Bogdan", ""], ["Liu", "Xuqi", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1909.04110", "submitter": "Zengming Shen", "authors": "Zengming Shen, S.Kevin Zhou, Yifan Chen, Bogdan Georgescu, Xuqi Liu,\n  Thomas S. Huang", "title": "One-to-one Mapping for Unpaired Image-to-image Translation", "comments": "Accepted by WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently image-to-image translation has attracted significant interests in\nthe literature, starting from the successful use of the generative adversarial\nnetwork (GAN), to the introduction of cyclic constraint, to extensions to\nmultiple domains. However, in existing approaches, there is no guarantee that\nthe mapping between two image domains is unique or one-to-one. Here we propose\na self-inverse network learning approach for unpaired image-to-image\ntranslation. Building on top of CycleGAN, we learn a self-inverse function by\nsimply augmenting the training samples by swapping inputs and outputs during\ntraining and with separated cycle consistency loss for each mapping direction.\nThe outcome of such learning is a proven one-to-one mapping function. Our\nextensive experiments on a variety of datasets, including cross-modal medical\nimage synthesis, object transfiguration, and semantic labeling, consistently\ndemonstrate clear improvement over the CycleGAN method both qualitatively and\nquantitatively. Especially our proposed method reaches the state-of-the-art\nresult on the cityscapes benchmark dataset for the label to photo unpaired\ndirectional image translation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:10:05 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 15:41:37 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 14:26:52 GMT"}, {"version": "v4", "created": "Mon, 16 Sep 2019 20:52:09 GMT"}, {"version": "v5", "created": "Sat, 12 Oct 2019 07:35:28 GMT"}, {"version": "v6", "created": "Wed, 15 Jan 2020 03:13:18 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Shen", "Zengming", ""], ["Zhou", "S. Kevin", ""], ["Chen", "Yifan", ""], ["Georgescu", "Bogdan", ""], ["Liu", "Xuqi", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1909.04115", "submitter": "Pierluca D'Oro", "authors": "Pierluca D'Oro, Alberto Maria Metelli, Andrea Tirinzoni, Matteo\n  Papini, Marcello Restelli", "title": "Gradient-Aware Model-based Policy Search", "comments": null, "journal-ref": null, "doi": "10.1609/aaai.v34i04.5791", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional model-based reinforcement learning approaches learn a model of\nthe environment dynamics without explicitly considering how it will be used by\nthe agent. In the presence of misspecified model classes, this can lead to poor\nestimates, as some relevant available information is ignored. In this paper, we\nintroduce a novel model-based policy search approach that exploits the\nknowledge of the current agent policy to learn an approximate transition model,\nfocusing on the portions of the environment that are most relevant for policy\nimprovement. We leverage a weighting scheme, derived from the minimization of\nthe error on the model-based policy gradient estimator, in order to define a\nsuitable objective function that is optimized for learning the approximate\ntransition model. Then, we integrate this procedure into a batch policy\nimprovement algorithm, named Gradient-Aware Model-based Policy Search (GAMPS),\nwhich iteratively learns a transition model and uses it, together with the\ncollected trajectories, to compute the new policy parameters. Finally, we\nempirically validate GAMPS on benchmark domains analyzing and discussing its\nproperties.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:26:27 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 22:17:37 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["D'Oro", "Pierluca", ""], ["Metelli", "Alberto Maria", ""], ["Tirinzoni", "Andrea", ""], ["Papini", "Matteo", ""], ["Restelli", "Marcello", ""]]}, {"id": "1909.04117", "submitter": "Sandro Rama Fiorini", "authors": "Sandro Rama Fiorini, Wallas Sousa dos Santos, Rodrigo Costa Mesquita,\n  Guilherme Ferreira Lima, Marcio F. Moreno", "title": "General Fragment Model for Information Artifacts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of semantic descriptions in data intensive domains require a\nsystematic model for linking semantic descriptions with their manifestations in\nfragments of heterogeneous information and data objects. Such information\nheterogeneity requires a fragment model that is general enough to support the\nspecification of anchors from conceptual models to multiple types of\ninformation artifacts. While diverse proposals of anchoring models exist in the\nliterature, they are usually focused in audiovisual information. We propose a\ngeneralized fragment model that can be instantiated to different kinds of\ninformation artifacts. Our objective is to systematize the way in which\nfragments and anchors can be described in conceptual models, without committing\nto a specific vocabulary.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:29:17 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Fiorini", "Sandro Rama", ""], ["Santos", "Wallas Sousa dos", ""], ["Mesquita", "Rodrigo Costa", ""], ["Lima", "Guilherme Ferreira", ""], ["Moreno", "Marcio F.", ""]]}, {"id": "1909.04120", "submitter": "Michael Glass", "authors": "Michael Glass, Alfio Gliozzo, Rishav Chakravarti, Anthony Ferritto,\n  Lin Pan, G P Shrivatsa Bhargav, Dinesh Garg, Avirup Sil", "title": "Span Selection Pre-training for Question Answering", "comments": "Accepted at ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BERT (Bidirectional Encoder Representations from Transformers) and related\npre-trained Transformers have provided large gains across many language\nunderstanding tasks, achieving a new state-of-the-art (SOTA). BERT is\npre-trained on two auxiliary tasks: Masked Language Model and Next Sentence\nPrediction. In this paper we introduce a new pre-training task inspired by\nreading comprehension to better align the pre-training from memorization to\nunderstanding. Span Selection Pre-Training (SSPT) poses cloze-like training\ninstances, but rather than draw the answer from the model's parameters, it is\nselected from a relevant passage. We find significant and consistent\nimprovements over both BERT-BASE and BERT-LARGE on multiple reading\ncomprehension (MRC) datasets. Specifically, our proposed model has strong\nempirical evidence as it obtains SOTA results on Natural Questions, a new\nbenchmark MRC dataset, outperforming BERT-LARGE by 3 F1 points on short answer\nprediction. We also show significant impact in HotpotQA, improving answer\nprediction F1 by 4 points and supporting fact prediction F1 by 1 point and\noutperforming the previous best system. Moreover, we show that our pre-training\napproach is particularly effective when training data is limited, improving the\nlearning curve by a large amount.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:32:31 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 18:18:04 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Glass", "Michael", ""], ["Gliozzo", "Alfio", ""], ["Chakravarti", "Rishav", ""], ["Ferritto", "Anthony", ""], ["Pan", "Lin", ""], ["Bhargav", "G P Shrivatsa", ""], ["Garg", "Dinesh", ""], ["Sil", "Avirup", ""]]}, {"id": "1909.04121", "submitter": "Andrey Kurenkov", "authors": "Andrey Kurenkov, Ajay Mandlekar, Roberto Martin-Martin, Silvio\n  Savarese, Animesh Garg", "title": "AC-Teach: A Bayesian Actor-Critic Method for Policy Learning with an\n  Ensemble of Suboptimal Teachers", "comments": "Conference on Robot Learning (CoRL) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exploration mechanism used by a Deep Reinforcement Learning (RL) agent\nplays a key role in determining its sample efficiency. Thus, improving over\nrandom exploration is crucial to solve long-horizon tasks with sparse rewards.\nWe propose to leverage an ensemble of partial solutions as teachers that guide\nthe agent's exploration with action suggestions throughout training. While the\nsetup of learning with teachers has been previously studied, our proposed\napproach - Actor-Critic with Teacher Ensembles (AC-Teach) - is the first to\nwork with an ensemble of suboptimal teachers that may solve only part of the\nproblem or contradict other each other, forming a unified algorithmic solution\nthat is compatible with a broad range of teacher ensembles. AC-Teach leverages\na probabilistic representation of the expected outcome of the teachers' and\nstudent's actions to direct exploration, reduce dithering, and adapt to the\ndynamically changing quality of the learner. We evaluate a variant of AC-Teach\nthat guides the learning of a Bayesian DDPG agent on three tasks - path\nfollowing, robotic pick and place, and robotic cube sweeping using a hook - and\nshow that it improves largely on sampling efficiency over a set of baselines,\nboth for our target scenario of unconstrained suboptimal teachers and for\neasier setups with optimal or single teachers. Additional results and videos at\nhttps://sites.google.com/view/acteach/home.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:38:25 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 09:14:07 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2019 00:15:26 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kurenkov", "Andrey", ""], ["Mandlekar", "Ajay", ""], ["Martin-Martin", "Roberto", ""], ["Savarese", "Silvio", ""], ["Garg", "Animesh", ""]]}, {"id": "1909.04126", "submitter": "Ang Li", "authors": "Ang Li, Jiayi Guo, Huanrui Yang, Flora D. Salim, Yiran Chen", "title": "DeepObfuscator: Obfuscating Intermediate Representations with\n  Privacy-Preserving Adversarial Learning on Smartphones", "comments": "This paper is to be published in IoTDI'21", "journal-ref": null, "doi": "10.1145/3450268.3453519", "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been widely applied in many computer vision applications,\nwith remarkable success. However, running deep learning models on mobile\ndevices is generally challenging due to the limitation of computing resources.\nA popular alternative is to use cloud services to run deep learning models to\nprocess raw data. This, however, imposes privacy risks. Some prior arts\nproposed sending the features extracted from raw data to the cloud.\nUnfortunately, these extracted features can still be exploited by attackers to\nrecover raw images and to infer embedded private attributes. In this paper, we\npropose an adversarial training framework, DeepObfuscator, which prevents the\nusage of the features for reconstruction of the raw images and inference of\nprivate attributes. This is done while retaining useful information for the\nintended cloud service. DeepObfuscator includes a learnable obfuscator that is\ndesigned to hide privacy-related sensitive information from the features by\nperforming our proposed adversarial training algorithm. The proposed algorithm\nis designed by simulating the game between an attacker who makes efforts to\nreconstruct raw image and infer private attributes from the extracted features\nand a defender who aims to protect user privacy. By deploying the trained\nobfuscator on the smartphone, features can be locally extracted and then sent\nto the cloud. Our experiments on CelebA and LFW datasets show that the quality\nof the reconstructed images from the obfuscated features of the raw image is\ndramatically decreased from 0.9458 to 0.3175 in terms of multi-scale structural\nsimilarity. The person in the reconstructed image, hence, becomes hardly to be\nre-identified. The classification accuracy of the inferred private attributes\nthat can be achieved by the attacker is significantly reduced to a\nrandom-guessing level.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:57:01 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 02:46:26 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Li", "Ang", ""], ["Guo", "Jiayi", ""], ["Yang", "Huanrui", ""], ["Salim", "Flora D.", ""], ["Chen", "Yiran", ""]]}, {"id": "1909.04134", "submitter": "Rahul Ramesh", "authors": "Arjun Manoharan, Rahul Ramesh, and Balaraman Ravindran", "title": "Option Encoder: A Framework for Discovering a Policy Basis in\n  Reinforcement Learning", "comments": "ECML-PKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Option discovery and skill acquisition frameworks are integral to the\nfunctioning of a Hierarchically organized Reinforcement learning agent.\nHowever, such techniques often yield a large number of options or skills, which\ncan potentially be represented succinctly by filtering out any redundant\ninformation. Such a reduction can reduce the required computation while also\nimproving the performance on a target task. In order to compress an array of\noption policies, we attempt to find a policy basis that accurately captures the\nset of all options. In this work, we propose Option Encoder, an auto-encoder\nbased framework with intelligently constrained weights, that helps discover a\ncollection of basis policies. The policy basis can be used as a proxy for the\noriginal set of skills in a suitable hierarchically organized framework. We\ndemonstrate the efficacy of our method on a collection of grid-worlds and on\nthe high-dimensional Fetch-Reach robotic manipulation task by evaluating the\nobtained policy basis on a set of downstream tasks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 20:10:12 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 05:50:15 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 08:12:02 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Manoharan", "Arjun", ""], ["Ramesh", "Rahul", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1909.04170", "submitter": "Giacomo Spigler", "authors": "Giacomo Spigler", "title": "Meta-learnt priors slow down catastrophic forgetting in neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current training regimes for deep learning usually involve exposure to a\nsingle task / dataset at a time. Here we start from the observation that in\nthis context the trained model is not given any knowledge of anything outside\nits (single-task) training distribution, and has thus no way to learn\nparameters (i.e., feature detectors or policies) that could be helpful to solve\nother tasks, and to limit future interference with the acquired knowledge, and\nthus catastrophic forgetting. Here we show that catastrophic forgetting can be\nmitigated in a meta-learning context, by exposing a neural network to multiple\ntasks in a sequential manner during training. Finally, we present SeqFOMAML, a\nmeta-learning algorithm that implements these principles, and we evaluate it on\nsequential learning problems composed by Omniglot and MiniImageNet\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 21:46:19 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 16:39:32 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Spigler", "Giacomo", ""]]}, {"id": "1909.04171", "submitter": "Joshua Bertram", "authors": "Joshua R. Bertram and Peng Wei", "title": "An Efficient Algorithm for Multiple-Pursuer-Multiple-Evader\n  Pursuit/Evasion Game", "comments": "submitted to ACC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for pursuit/evasion that is highly efficient and and\nscales to large teams of aircraft. The underlying algorithm is an efficient\nalgorithm for solving Markov Decision Processes (MDPs) that supports fully\ncontinuous state spaces. We demonstrate the algorithm in a team pursuit/evasion\nsetting in a 3D environment using a pseudo-6DOF model and study performance by\nvarying sizes of team members. We show that as the number of aircraft in the\nsimulation grows, computational performance remains efficient and is suitable\nfor real-time systems. We also define probability-to-win and survivability\nmetrics that describe the teams' performance over multiple trials, and show\nthat the algorithm performs consistently. We provide numerical results showing\ncontrol inputs for a typical 1v1 encounter and provide videos for 1v1, 2v2,\n3v3, 4v4, and 10v10 contests to demonstrate the ability of the algorithm to\nadapt seamlessly to complex environments.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 21:46:31 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Bertram", "Joshua R.", ""], ["Wei", "Peng", ""]]}, {"id": "1909.04224", "submitter": "Liheng Chen", "authors": "Liheng Chen, Hongyi Guo, Yali Du, Fei Fang, Haifeng Zhang, Yaoming\n  Zhu, Ming Zhou, Weinan Zhang, Qing Wang, Yong Yu", "title": "Signal Instructed Coordination in Cooperative Multi-agent Reinforcement\n  Learning", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world problems, a team of agents need to collaborate to maximize\nthe common reward. Although existing works formulate this problem into a\ncentralized learning with decentralized execution framework, which avoids the\nnon-stationary problem in training, their decentralized execution paradigm\nlimits the agents' capability to coordinate. Inspired by the concept of\ncorrelated equilibrium, we propose to introduce a coordination signal to\naddress this limitation, and theoretically show that following mild conditions,\ndecentralized agents with the coordination signal can coordinate their\nindividual policies as manipulated by a centralized controller. The idea of\nintroducing coordination signal is to encapsulate coordinated strategies into\nthe signals, and use the signals to instruct the collaboration in decentralized\nexecution. To encourage agents to learn to exploit the coordination signal, we\npropose Signal Instructed Coordination (SIC), a novel coordination module that\ncan be integrated with most existing MARL frameworks. SIC casts a common signal\nsampled from a pre-defined distribution to all agents, and introduces an\ninformation-theoretic regularization to facilitate the consistency between the\nobserved signal and agents' policies. Our experiments show that SIC\nconsistently improves performance over well-recognized MARL models in both\nmatrix games and a predator-prey game with high-dimensional strategy space.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 01:28:25 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 08:30:30 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Chen", "Liheng", ""], ["Guo", "Hongyi", ""], ["Du", "Yali", ""], ["Fang", "Fei", ""], ["Zhang", "Haifeng", ""], ["Zhu", "Yaoming", ""], ["Zhou", "Ming", ""], ["Zhang", "Weinan", ""], ["Wang", "Qing", ""], ["Yu", "Yong", ""]]}, {"id": "1909.04239", "submitter": "Yitong Meng", "authors": "Yitong Meng, Xinyan Dai, Xiao Yan, James Cheng, Weiwen Liu, Benben\n  Liao, Jun Guo, Guangyong Chen", "title": "PMD: An Optimal Transportation-based User Distance for Recommender\n  Systems", "comments": "This paper is accepted by European Conference on Information\n  Retrieval (ECIR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering, a widely-used recommendation technique, predicts a\nuser's preference by aggregating the ratings from similar users. As a result,\nthese measures cannot fully utilize the rating information and are not suitable\nfor real world sparse data. To solve these issues, we propose a novel user\ndistance measure named Preference Mover's Distance (PMD) which makes full use\nof all ratings made by each user. Our proposed PMD can properly measure the\ndistance between a pair of users even if they have no co-rated items. We show\nthat this measure can be cast as an instance of the Earth Mover's Distance, a\nwell-studied transportation problem for which several highly efficient solvers\nhave been developed. Experimental results show that PMD can help achieve\nsuperior recommendation accuracy than state-of-the-art methods, especially when\ntraining data is very sparse.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 02:06:57 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 07:05:41 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Meng", "Yitong", ""], ["Dai", "Xinyan", ""], ["Yan", "Xiao", ""], ["Cheng", "James", ""], ["Liu", "Weiwen", ""], ["Liao", "Benben", ""], ["Guo", "Jun", ""], ["Chen", "Guangyong", ""]]}, {"id": "1909.04251", "submitter": "Jing Qian", "authors": "Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth Belding, William Yang\n  Wang", "title": "A Benchmark Dataset for Learning to Intervene in Online Hate Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Countering online hate speech is a critical yet challenging task, but one\nwhich can be aided by the use of Natural Language Processing (NLP) techniques.\nPrevious research has primarily focused on the development of NLP methods to\nautomatically and effectively detect online hate speech while disregarding\nfurther action needed to calm and discourage individuals from using hate speech\nin the future. In addition, most existing hate speech datasets treat each post\nas an isolated instance, ignoring the conversational context. In this paper, we\npropose a novel task of generative hate speech intervention, where the goal is\nto automatically generate responses to intervene during online conversations\nthat contain hate speech. As a part of this work, we introduce two\nfully-labeled large-scale hate speech intervention datasets collected from Gab\nand Reddit. These datasets provide conversation segments, hate speech labels,\nas well as intervention responses written by Mechanical Turk Workers. In this\npaper, we also analyze the datasets to understand the common intervention\nstrategies and explore the performance of common automatic response generation\nmethods on these new datasets to provide a benchmark for future research.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 03:00:58 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Qian", "Jing", ""], ["Bethke", "Anna", ""], ["Liu", "Yinyin", ""], ["Belding", "Elizabeth", ""], ["Wang", "William Yang", ""]]}, {"id": "1909.04256", "submitter": "Zhe Xu", "authors": "Zhe Xu and Ufuk Topcu", "title": "Transfer of Temporal Logic Formulas in Reinforcement Learning", "comments": "IJCAI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring high-level knowledge from a source task to a target task is an\neffective way to expedite reinforcement learning (RL). For example,\npropositional logic and first-order logic have been used as representations of\nsuch knowledge. We study the transfer of knowledge between tasks in which the\ntiming of the events matters. We call such tasks temporal tasks. We concretize\nsimilarity between temporal tasks through a notion of logical transferability,\nand develop a transfer learning approach between different yet similar temporal\ntasks. We first propose an inference technique to extract metric interval\ntemporal logic (MITL) formulas in sequential disjunctive normal form from\nlabeled trajectories collected in RL of the two tasks. If logical\ntransferability is identified through this inference, we construct a timed\nautomaton for each sequential conjunctive subformula of the inferred MITL\nformulas from both tasks. We perform RL on the extended state which includes\nthe locations and clock valuations of the timed automata for the source task.\nWe then establish mappings between the corresponding components (clocks,\nlocations, etc.) of the timed automata from the two tasks, and transfer the\nextended Q-functions based on the established mappings. Finally, we perform RL\non the extended state for the target task, starting with the transferred\nextended Q-functions. Our results in two case studies show, depending on how\nsimilar the source task and the target task are, that the sampling efficiency\nfor the target task can be improved by up to one order of magnitude by\nperforming RL in the extended state space, and further improved by up to\nanother order of magnitude using the transferred extended Q-functions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 03:11:05 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Xu", "Zhe", ""], ["Topcu", "Ufuk", ""]]}, {"id": "1909.04303", "submitter": "Deng Cai", "authors": "Deng Cai and Wai Lam", "title": "Core Semantic First: A Top-down Approach for AMR Parsing", "comments": "EMNLP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel scheme for parsing a piece of text into its Abstract\nMeaning Representation (AMR): Graph Spanning based Parsing (GSP). One novel\ncharacteristic of GSP is that it constructs a parse graph incrementally in a\ntop-down fashion. Starting from the root, at each step, a new node and its\nconnections to existing nodes will be jointly predicted. The output graph spans\nthe nodes by the distance to the root, following the intuition of first\ngrasping the main ideas then digging into more details. The \\textit{core\nsemantic first} principle emphasizes capturing the main ideas of a sentence,\nwhich is of great interest. We evaluate our model on the latest AMR sembank and\nachieve the state-of-the-art performance in the sense that no heuristic graph\nre-categorization is adopted. More importantly, the experiments show that our\nparser is especially good at obtaining the core semantics.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 05:51:12 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 07:02:03 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Cai", "Deng", ""], ["Lam", "Wai", ""]]}, {"id": "1909.04307", "submitter": "Thommen George Karimpanal", "authors": "Thommen George Karimpanal, Santu Rana, Sunil Gupta, Truyen Tran and\n  Svetha Venkatesh", "title": "Learning Transferable Domain Priors for Safe Exploration in\n  Reinforcement Learning", "comments": "IJCNN, 2020 (To appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior access to domain knowledge could significantly improve the performance\nof a reinforcement learning agent. In particular, it could help agents avoid\npotentially catastrophic exploratory actions, which would otherwise have to be\nexperienced during learning. In this work, we identify consistently undesirable\nactions in a set of previously learned tasks, and use pseudo-rewards associated\nwith them to learn a prior policy. In addition to enabling safer exploratory\nbehaviors in subsequent tasks in the domain, we show that these priors are\ntransferable to similar environments, and can be learned off-policy and in\nparallel with the learning of other tasks in the domain. We compare our\napproach to established, state-of-the-art algorithms in both discrete as well\nas continuous environments, and demonstrate that it exhibits a safer\nexploratory behavior while learning to perform arbitrary tasks in the domain.\nWe also present a theoretical analysis to support these results, and briefly\ndiscuss the implications and some alternative formulations of this approach,\nwhich could also be useful in certain scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 06:03:52 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 13:09:56 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 00:02:49 GMT"}, {"version": "v4", "created": "Mon, 10 Feb 2020 23:46:32 GMT"}, {"version": "v5", "created": "Sun, 13 Sep 2020 05:15:00 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Karimpanal", "Thommen George", ""], ["Rana", "Santu", ""], ["Gupta", "Sunil", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1909.04309", "submitter": "Loic Pauleve", "authors": "St\\'ephanie Chevalier (BioInfo - LRI), Christine Froidevaux (BioInfo -\n  LRI), Lo\\\"ic Paulev\\'e (LaBRI), Andrei Zinovyev", "title": "Synthesis of Boolean Networks from Biological Dynamical Constraints\n  using Answer-Set Programming", "comments": null, "journal-ref": "31st International Conference on Tools with Artificial\n  Intelligence, 2019, Portland, Oregon, United States", "doi": null, "report-no": null, "categories": "cs.AI cs.LO q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean networks model finite discrete dynamical systems with complex\nbehaviours. The state of each component is determined by a Boolean function of\nthe state of (a subset of) the components of the network. This paper addresses\nthe synthesis of these Boolean functions from constraints on their domain and\nemerging dynamical properties of the resulting network. The dynamical\nproperties relate to the existence and absence of trajectories between\npartially observed configurations, and to the stable behaviours (fixpoints and\ncyclic attractors). The synthesis is expressed as a Boolean satisfiability\nproblem relying on Answer-Set Programming with a parametrized complexity, and\nleads to a complete non-redundant characterization of the set of solutions.\nConsidered constraints are particularly suited to address the synthesis of\nmodels of cellular differentiation processes, as illustrated on a case study.\nThe scalability of the approach is demonstrated on random networks with\nscale-free structures up to 100 to 1,000 nodes depending on the type of\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 06:14:23 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 09:18:11 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Chevalier", "St\u00e9phanie", "", "BioInfo - LRI"], ["Froidevaux", "Christine", "", "BioInfo -\n  LRI"], ["Paulev\u00e9", "Lo\u00efc", "", "LaBRI"], ["Zinovyev", "Andrei", ""]]}, {"id": "1909.04319", "submitter": "Hiroyuki Kido", "authors": "Hiroyuki Kido and Beishui Liao", "title": "A Bayesian Approach to Direct and Inverse Abstract Argumentation\n  Problems", "comments": "This paper was submitted to the journal of Artificial Intelligence\n  (AIJ) and rejected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a fundamental mechanism of how to detect a conflict\nbetween arguments given sentiments regarding acceptability of the arguments. We\nintroduce a concept of the inverse problem of the abstract argumentation to\ntackle the problem. Given noisy sets of acceptable arguments, it aims to find\nattack relations explaining the sets well in terms of acceptability semantics.\nIt is the inverse of the direct problem corresponding to the traditional\nproblem of the abstract argumentation that focuses on finding sets of\nacceptable arguments in terms of the semantics given an attack relation between\nthe arguments. We give a probabilistic model handling both of the problems in a\nway that is faithful to the acceptability semantics. From a theoretical point\nof view, we show that a solution to both the direct and inverse problems is a\nspecial case of the probabilistic inference on the model. We discuss that the\nmodel provides a natural extension of the semantics to cope with uncertain\nattack relations distributed probabilistically. From en empirical point of\nview, we argue that it reasonably predicts individuals sentiments regarding\nacceptability of arguments. This paper contributes to lay the foundation for\nmaking acceptability semantics data-driven and to provide a way to tackle the\nknowledge acquisition bottleneck.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 06:37:12 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 17:51:56 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Kido", "Hiroyuki", ""], ["Liao", "Beishui", ""]]}, {"id": "1909.04368", "submitter": "Ciprian Paduraru", "authors": "Ciprian Paduraru and Miruna Paduraru", "title": "Automatic difficulty management and testing in games using a framework\n  based on behavior trees and genetic algorithms", "comments": "Accepted for publication in the IEEE Proceedings of The 24\n  International Conference on Engineering of Complex Computer Systems (ICECCS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diversity of agent behaviors is an important topic for the quality of\nvideo games and virtual environments in general. Offering the most compelling\nexperience for users with different skills is a difficult task, and usually\nneeds important manual human effort for tuning existing code. This can get even\nharder when dealing with adaptive difficulty systems. Our paper's main purpose\nis to create a framework that can automatically create behaviors for game\nagents of different difficulty classes and enough diversity. In parallel with\nthis, a second purpose is to create more automated tests for showing defects in\nthe source code or possible logic exploits with less human effort.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 09:36:00 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Paduraru", "Ciprian", ""], ["Paduraru", "Miruna", ""]]}, {"id": "1909.04385", "submitter": "Aditya Ganeshan Master", "authors": "Aditya Ganeshan, B.S. Vivek, R. Venkatesh Babu", "title": "FDA: Feature Disruptive Attack", "comments": "Accepted in ICCV;19. Code Available at\n  https://github.com/BardOfCodes/fda", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though Deep Neural Networks (DNN) show excellent performance across various\ncomputer vision tasks, several works show their vulnerability to adversarial\nsamples, i.e., image samples with imperceptible noise engineered to manipulate\nthe network's prediction. Adversarial sample generation methods range from\nsimple to complex optimization techniques. Majority of these methods generate\nadversaries through optimization objectives that are tied to the pre-softmax or\nsoftmax output of the network. In this work we, (i) show the drawbacks of such\nattacks, (ii) propose two new evaluation metrics: Old Label New Rank (OLNR) and\nNew Label Old Rank (NLOR) in order to quantify the extent of damage made by an\nattack, and (iii) propose a new adversarial attack FDA: Feature Disruptive\nAttack, to address the drawbacks of existing attacks. FDA works by generating\nimage perturbation that disrupt features at each layer of the network and\ncauses deep-features to be highly corrupt. This allows FDA adversaries to\nseverely reduce the performance of deep networks. We experimentally validate\nthat FDA generates stronger adversaries than other state-of-the-art methods for\nimage classification, even in the presence of various defense measures. More\nimportantly, we show that FDA disrupts feature-representation based tasks even\nwithout access to the task-specific network or methodology. Code available at:\nhttps://github.com/BardOfCodes/fda\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 10:09:38 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Ganeshan", "Aditya", ""], ["Vivek", "B. S.", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1909.04386", "submitter": "Xavier Ferrer Aran", "authors": "Xavier Ferrer Aran, Jose M. Such, Natalia Criado", "title": "Attesting Biases and Discrimination using Language Semantics", "comments": "Author's copy of the manuscript accepted in the Responsible\n  Artificial Intelligence Agents workshop of the International Conference on\n  Autonomous Agents and Multiagent Systems (AAMAS'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI agents are increasingly deployed and used to make automated decisions that\naffect our lives on a daily basis. It is imperative to ensure that these\nsystems embed ethical principles and respect human values. We focus on how we\ncan attest to whether AI agents treat users fairly without discriminating\nagainst particular individuals or groups through biases in language. In\nparticular, we discuss human unconscious biases, how they are embedded in\nlanguage, and how AI systems inherit those biases by learning from and\nprocessing human language. Then, we outline a roadmap for future research to\nbetter understand and attest problematic AI biases derived from language.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 10:12:01 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Aran", "Xavier Ferrer", ""], ["Such", "Jose M.", ""], ["Criado", "Natalia", ""]]}, {"id": "1909.04405", "submitter": "Damien Pellier", "authors": "D. H\\\"oller, G. Behnke, P. Bercher, S. Biundo, H. Fiorino, D. Pellier,\n  R. Alford", "title": "Hierarchical Planning in the IPC", "comments": null, "journal-ref": "Workshop on the International Planning Competition (ICAPS), 2019", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last year, the amount of research in hierarchical planning has\nincreased, leading to significant improvements in the performance of planners.\nHowever, the research is diverging and planners are somewhat hard to compare\nagainst each other. This is mostly caused by the fact that there is no standard\nset of benchmark domains, nor even a common description language for\nhierarchical planning problems. As a consequence, the available planners\nsupport a widely varying set of features and (almost) none of them can solve\n(or even parse) any problem developed for another planner. With this paper, we\npropose to create a new track for the IPC in which hierarchical planners will\ncompete. This competition will result in a standardised description language,\nbroader support for core features of that language among planners, a set of\nbenchmark problems, a means to fairly and objectively compare HTN planners, and\nfor new challenges for planners.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 11:02:56 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["H\u00f6ller", "D.", ""], ["Behnke", "G.", ""], ["Bercher", "P.", ""], ["Biundo", "S.", ""], ["Fiorino", "H.", ""], ["Pellier", "D.", ""], ["Alford", "R.", ""]]}, {"id": "1909.04430", "submitter": "Tim Taylor", "authors": "Norman Packard, Mark A. Bedau, Alastair Channon, Takashi Ikegami,\n  Steen Rasmussen, Kenneth O. Stanley, Tim Taylor", "title": "An Overview of Open-Ended Evolution: Editorial Introduction to the\n  Open-Ended Evolution II Special Issue", "comments": "This article is published in the Artificial Life journal\n  (https://www.mitpressjournals.org/loi/artl) and is copyright (c) 2019\n  Massachusetts Institute of Technology. It it posted on arXiv.org after the\n  publication embargo period in accordance with MIT Press Journals' author\n  posting guidelines\n  (https://www.mitpressjournals.org/for_authors#authorposting)", "journal-ref": "Artificial Life, 25(2), pp. 93-103, 2019", "doi": "10.1162/artl_a_00291", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nature's spectacular inventiveness, reflected in the enormous diversity of\nform and function displayed by the biosphere, is a feature of life that\ndistinguishes living most strongly from nonliving. It is, therefore, not\nsurprising that this aspect of life should become a central focus of artificial\nlife. We have known since Darwin that the diversity is produced dynamically,\nthrough the process of evolution; this has led life's creative productivity to\nbe called Open-Ended Evolution (OEE) in the field. This article introduces the\nsecond of two special issues on current research in OEE and provides an\noverview of the contents of both special issues. Most of the work was presented\nat a workshop on open-ended evolution that was held as a part of the 2018\nConference on Artificial Life in Tokyo, and much of it had antecedents in two\nprevious workshops on open-ended evolution at artificial life conferences in\nCancun and York. We present a simplified categorization of OEE and summarize\nprogress in the field as represented by the articles in this special issue.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 12:20:19 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Packard", "Norman", ""], ["Bedau", "Mark A.", ""], ["Channon", "Alastair", ""], ["Ikegami", "Takashi", ""], ["Rasmussen", "Steen", ""], ["Stanley", "Kenneth O.", ""], ["Taylor", "Tim", ""]]}, {"id": "1909.04436", "submitter": "Martin Shepperd", "authors": "Martin Shepperd, Yuchen Guo, Ning Li, Mahir Arzoky, Andrea Capiluppi,\n  Steve Counsell, Giuseppe Destefanis, Stephen Swift, Allan Tucker, and Leila\n  Yousefi", "title": "The Prevalence of Errors in Machine Learning Experiments", "comments": "20th International Conference on Intelligent Data Engineering and\n  Automated Learning (IDEAL), 14--16 November 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Context: Conducting experiments is central to research machine learning\nresearch to benchmark, evaluate and compare learning algorithms. Consequently\nit is important we conduct reliable, trustworthy experiments. Objective: We\ninvestigate the incidence of errors in a sample of machine learning experiments\nin the domain of software defect prediction. Our focus is simple arithmetical\nand statistical errors. Method: We analyse 49 papers describing 2456 individual\nexperimental results from a previously undertaken systematic review comparing\nsupervised and unsupervised defect prediction classifiers. We extract the\nconfusion matrices and test for relevant constraints, e.g., the marginal\nprobabilities must sum to one. We also check for multiple statistical\nsignificance testing errors. Results: We find that a total of 22 out of 49\npapers contain demonstrable errors. Of these 7 were statistical and 16 related\nto confusion matrix inconsistency (one paper contained both classes of error).\nConclusions: Whilst some errors may be of a relatively trivial nature, e.g.,\ntranscription errors their presence does not engender confidence. We strongly\nurge researchers to follow open science principles so errors can be more easily\nbe detected and corrected, thus as a community reduce this worryingly high\nerror rate with our computational experiments.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 12:32:00 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Shepperd", "Martin", ""], ["Guo", "Yuchen", ""], ["Li", "Ning", ""], ["Arzoky", "Mahir", ""], ["Capiluppi", "Andrea", ""], ["Counsell", "Steve", ""], ["Destefanis", "Giuseppe", ""], ["Swift", "Stephen", ""], ["Tucker", "Allan", ""], ["Yousefi", "Leila", ""]]}, {"id": "1909.04455", "submitter": "Chun Yuan Yuan", "authors": "Chunyuan Yuan, Wei Zhou, Qianwen Ma, Shangwen Lv, Jizhong Han, Songlin\n  Hu", "title": "Learning review representations from user and product level information\n  for spam detection", "comments": "6 pages. Accepted as IEEE ICDM 2019, Short Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion spam has become a widespread problem in social media, where hired\nspammers write deceptive reviews to promote or demote products to mislead the\nconsumers for profit or fame. Existing works mainly focus on manually designing\ndiscrete textual or behavior features, which cannot capture complex semantics\nof reviews. Although recent works apply deep learning methods to learn\nreview-level semantic features, their models ignore the impact of the\nuser-level and product-level information on learning review semantics and the\ninherent user-review-product relationship information. In this paper, we\npropose a Hierarchical Fusion Attention Network (HFAN) to automatically learn\nthe semantics of reviews from the user and product level. Specifically, we\ndesign a multi-attention unit to extract user(product)-related review\ninformation. Then, we use orthogonal decomposition and fusion attention to\nlearn a user, review, and product representation from the review information.\nFinally, we take the review as a relation between user and product entity and\napply TransH to jointly encode this relationship into review representation.\nExperimental results obtained more than 10\\% absolute precision improvement\nover the state-of-the-art performances on four real-world datasets, which show\nthe effectiveness and versatility of the model.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 13:01:27 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Yuan", "Chunyuan", ""], ["Zhou", "Wei", ""], ["Ma", "Qianwen", ""], ["Lv", "Shangwen", ""], ["Han", "Jizhong", ""], ["Hu", "Songlin", ""]]}, {"id": "1909.04485", "submitter": "Jose M. Alvarez", "authors": "Shuang Gao and Xin Liu and Lung-Sheng Chien and William Zhang and Jose\n  M. Alvarez", "title": "VACL: Variance-Aware Cross-Layer Regularization for Pruning Deep\n  Residual Networks", "comments": "ICCV Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving weight sparsity is a common strategy for producing light-weight\ndeep neural networks. However, pruning models with residual learning is more\nchallenging. In this paper, we introduce Variance-Aware Cross-Layer (VACL), a\nnovel approach to address this problem. VACL consists of two parts, a\nCross-Layer grouping and a Variance Aware regularization. In Cross-Layer\ngrouping the $i^{th}$ filters of layers connected by skip-connections are\ngrouped into one regularization group. Then, the Variance-Aware regularization\nterm takes into account both the first and second-order statistics of the\nconnected layers to constrain the variance within a group. Our approach can\neffectively improve the structural sparsity of residual models. For CIFAR10,\nthe proposed method reduces a ResNet model by up to 79.5% with no accuracy drop\nand reduces a ResNeXt model by up to 82% with less than 1% accuracy drop. For\nImageNet, it yields a pruned ratio of up to 63.3% with less than 1% top-5\naccuracy drop. Our experimental results show that the proposed approach\nsignificantly outperforms other state-of-the-art methods in terms of overall\nmodel size and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 13:59:04 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Gao", "Shuang", ""], ["Liu", "Xin", ""], ["Chien", "Lung-Sheng", ""], ["Zhang", "William", ""], ["Alvarez", "Jose M.", ""]]}, {"id": "1909.04492", "submitter": "Jurriaan van Diggelen", "authors": "J. van Diggelen, J.S. Barnhoorn, M.M.M. Peeters, W. van Staal, M.L.\n  Stolk, B. van der Vecht, J. van der Waa, J.M. Schraagen", "title": "Pluggable Social Artificial Intelligence for Enabling Human-Agent\n  Teaming", "comments": "presented at NATO HFM symposium on Human Autonomy Teaming,\n  Portsmouth, October 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As intelligent systems are increasingly capable of performing their tasks\nwithout the need for continuous human input, direction, or supervision, new\nhuman-machine interaction concepts are needed. A promising approach to this end\nis human-agent teaming, which envisions a novel interaction form where humans\nand machines behave as equal team partners. This paper presents an overview of\nthe current state of the art in human-agent teaming, including the analysis of\nhuman-agent teams on five dimensions; a framework describing important teaming\nfunctionalities; a technical architecture, called SAIL, supporting social\nhuman-agent teaming through the modular implementation of the human-agent\nteaming functionalities; a technical implementation of the architecture; and a\nproof-of-concept prototype created with the framework and architecture. We\nconclude this paper with a reflection on where we stand and a glance into the\nfuture showing the way forward.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 14:03:41 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 09:21:17 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["van Diggelen", "J.", ""], ["Barnhoorn", "J. S.", ""], ["Peeters", "M. M. M.", ""], ["van Staal", "W.", ""], ["Stolk", "M. L.", ""], ["van der Vecht", "B.", ""], ["van der Waa", "J.", ""], ["Schraagen", "J. M.", ""]]}, {"id": "1909.04493", "submitter": "Ningyu Zhang", "authors": "Qianghuai Jia, Ningyu Zhang, Nengwei Hua", "title": "Context-aware Deep Model for Entity Recommendation in Search Engine at\n  Alibaba", "comments": "CIKM2019 International Workshop on Entity Retrieval. arXiv admin\n  note: text overlap with arXiv:1511.08996 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity recommendation, providing search users with an improved experience via\nassisting them in finding related entities for a given query, has become an\nindispensable feature of today's search engines. Existing studies typically\nonly consider the queries with explicit entities. They usually fail to handle\ncomplex queries that without entities, such as \"what food is good for cold\nweather\", because their models could not infer the underlying meaning of the\ninput text. In this work, we believe that contexts convey valuable evidence\nthat could facilitate the semantic modeling of queries, and take them into\nconsideration for entity recommendation. In order to better model the semantics\nof queries and entities, we learn the representation of queries and entities\njointly with attentive deep neural networks. We evaluate our approach using\nlarge-scale, real-world search logs from a widely used commercial Chinese\nsearch engine. Our system has been deployed in ShenMa Search Engine and you can\nfetch it in UC Browser of Alibaba. Results from online A/B test suggest that\nthe impression efficiency of click-through rate increased by 5.1% and page view\nincreased by 5.5%.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 07:47:20 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Jia", "Qianghuai", ""], ["Zhang", "Ningyu", ""], ["Hua", "Nengwei", ""]]}, {"id": "1909.04538", "submitter": "H{\\aa}kon Hukkel{\\aa}s", "authors": "H{\\aa}kon Hukkel{\\aa}s, Rudolf Mester and Frank Lindseth", "title": "DeepPrivacy: A Generative Adversarial Network for Face Anonymization", "comments": "Accepted to ISVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel architecture which is able to automatically anonymize\nfaces in images while retaining the original data distribution. We ensure total\nanonymization of all faces in an image by generating images exclusively on\nprivacy-safe information. Our model is based on a conditional generative\nadversarial network, generating images considering the original pose and image\nbackground. The conditional information enables us to generate highly realistic\nfaces with a seamless transition between the generated face and the existing\nbackground. Furthermore, we introduce a diverse dataset of human faces,\nincluding unconventional poses, occluded faces, and a vast variability in\nbackgrounds. Finally, we present experimental results reflecting the capability\nof our model to anonymize images while preserving the data distribution, making\nthe data suitable for further training of deep learning models. As far as we\nknow, no other solution has been proposed that guarantees the anonymization of\nfaces while generating realistic images.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 14:52:24 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Hukkel\u00e5s", "H\u00e5kon", ""], ["Mester", "Rudolf", ""], ["Lindseth", "Frank", ""]]}, {"id": "1909.04559", "submitter": "Frederik Mallmann-Trenn", "authors": "Nancy Lynch and Frederik Mallmann-Trenn", "title": "Learning Hierarchically Structured Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of how concepts that have structure get represented in\nthe brain. Specifically, we introduce a model for hierarchically structured\nconcepts and we show how a biologically plausible neural network can recognize\nthese concepts, and how it can learn them in the first place. Our main goal is\nto introduce a general framework for these tasks and prove formally how both\n(recognition and learning) can be achieved.\n  We show that both tasks can be accomplished even in presence of noise. For\nlearning, we analyze Oja's rule formally, a well-known biologically-plausible\nrule for adjusting the weights of synapses. We complement the learning results\nwith lower bounds asserting that, in order to recognize concepts of a certain\nhierarchical depth, neural networks must have a corresponding number of layers.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 15:11:38 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 20:07:22 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 11:10:16 GMT"}, {"version": "v4", "created": "Sun, 17 Jan 2021 08:23:09 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Lynch", "Nancy", ""], ["Mallmann-Trenn", "Frederik", ""]]}, {"id": "1909.04572", "submitter": "Venkateswararao Cherukuri", "authors": "Venkateswararao Cherukuri, Tiantong Guo, Steve. J. Schiff, Vishal\n  Monga", "title": "Deep MR Brain Image Super-Resolution Using Spatio-Structural Priors", "comments": "Accepted to IEEE transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2942510", "report-no": null, "categories": "eess.IV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution Magnetic Resonance (MR) images are desired for accurate\ndiagnostics. In practice, image resolution is restricted by factors like\nhardware and processing constraints. Recently, deep learning methods have been\nshown to produce compelling state-of-the-art results for image\nenhancement/super-resolution. Paying particular attention to desired\nhi-resolution MR image structure, we propose a new regularized network that\nexploits image priors, namely a low-rank structure and a sharpness prior to\nenhance deep MR image super-resolution (SR). Our contributions are then\nincorporating these priors in an analytically tractable fashion \\color{black}\nas well as towards a novel prior guided network architecture that accomplishes\nthe super-resolution task. This is particularly challenging for the low rank\nprior since the rank is not a differentiable function of the image matrix(and\nhence the network parameters), an issue we address by pursuing differentiable\napproximations of the rank. Sharpness is emphasized by the variance of the\nLaplacian which we show can be implemented by a fixed feedback layer at the\noutput of the network. As a key extension, we modify the fixed feedback\n(Laplacian) layer by learning a new set of training data driven filters that\nare optimized for enhanced sharpness. Experiments performed on publicly\navailable MR brain image databases and comparisons against existing\nstate-of-the-art methods show that the proposed prior guided network offers\nsignificant practical gains in terms of improved SNR/image quality measures.\nBecause our priors are on output images, the proposed method is versatile and\ncan be combined with a wide variety of existing network architectures to\nfurther enhance their performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 15:33:18 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Cherukuri", "Venkateswararao", ""], ["Guo", "Tiantong", ""], ["Schiff", "Steve. J.", ""], ["Monga", "Vishal", ""]]}, {"id": "1909.04607", "submitter": "Vivek Veeriah", "authors": "Vivek Veeriah, Matteo Hessel, Zhongwen Xu, Richard Lewis, Janarthanan\n  Rajendran, Junhyuk Oh, Hado van Hasselt, David Silver, Satinder Singh", "title": "Discovery of Useful Questions as Auxiliary Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arguably, intelligent agents ought to be able to discover their own questions\nso that in learning answers for them they learn unanticipated useful knowledge\nand skills; this departs from the focus in much of machine learning on agents\nlearning answers to externally defined questions. We present a novel method for\na reinforcement learning (RL) agent to discover questions formulated as general\nvalue functions or GVFs, a fairly rich form of knowledge representation.\nSpecifically, our method uses non-myopic meta-gradients to learn GVF-questions\nsuch that learning answers to them, as an auxiliary task, induces useful\nrepresentations for the main task faced by the RL agent. We demonstrate that\nauxiliary tasks based on the discovered GVFs are sufficient, on their own, to\nbuild representations that support main task learning, and that they do so\nbetter than popular hand-designed auxiliary tasks from the literature.\nFurthermore, we show, in the context of Atari 2600 videogames, how such\nauxiliary tasks, meta-learned alongside the main task, can improve the data\nefficiency of an actor-critic agent.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 16:30:54 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Veeriah", "Vivek", ""], ["Hessel", "Matteo", ""], ["Xu", "Zhongwen", ""], ["Lewis", "Richard", ""], ["Rajendran", "Janarthanan", ""], ["Oh", "Junhyuk", ""], ["van Hasselt", "Hado", ""], ["Silver", "David", ""], ["Singh", "Satinder", ""]]}, {"id": "1909.04625", "submitter": "Ethan Wilcox", "authors": "Aixiu An, Peng Qian, Ethan Wilcox, and Roger Levy", "title": "Representation of Constituents in Neural Language Models: Coordination\n  Phrase as a Case Study", "comments": "To appear at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural language models have achieved state-of-the-art performances on many\nNLP tasks, and recently have been shown to learn a number of\nhierarchically-sensitive syntactic dependencies between individual words.\nHowever, equally important for language processing is the ability to combine\nwords into phrasal constituents, and use constituent-level features to drive\ndownstream expectations. Here we investigate neural models' ability to\nrepresent constituent-level features, using coordinated noun phrases as a case\nstudy. We assess whether different neural language models trained on English\nand French represent phrase-level number and gender features, and use those\nfeatures to drive downstream expectations. Our results suggest that models use\na linear combination of NP constituent number to drive CoordNP/verb number\nagreement. This behavior is highly regular and even sensitive to local\nsyntactic context, however it differs crucially from observed human behavior.\nModels have less success with gender agreement. Models trained on large corpora\nperform best, and there is no obvious advantage for models trained using\nexplicit syntactic supervision.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 17:02:15 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["An", "Aixiu", ""], ["Qian", "Peng", ""], ["Wilcox", "Ethan", ""], ["Levy", "Roger", ""]]}, {"id": "1909.04630", "submitter": "Aravind Rajeswaran", "authors": "Aravind Rajeswaran, Chelsea Finn, Sham Kakade, Sergey Levine", "title": "Meta-Learning with Implicit Gradients", "comments": "NeurIPS 2019. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core capability of intelligent systems is the ability to quickly learn new\ntasks by drawing on prior experience. Gradient (or optimization) based\nmeta-learning has recently emerged as an effective approach for few-shot\nlearning. In this formulation, meta-parameters are learned in the outer loop,\nwhile task-specific models are learned in the inner-loop, by using only a small\namount of data from the current task. A key challenge in scaling these\napproaches is the need to differentiate through the inner loop learning\nprocess, which can impose considerable computational and memory burdens. By\ndrawing upon implicit differentiation, we develop the implicit MAML algorithm,\nwhich depends only on the solution to the inner level optimization and not the\npath taken by the inner loop optimizer. This effectively decouples the\nmeta-gradient computation from the choice of inner loop optimizer. As a result,\nour approach is agnostic to the choice of inner loop optimizer and can\ngracefully handle many gradient steps without vanishing gradients or memory\nconstraints. Theoretically, we prove that implicit MAML can compute accurate\nmeta-gradients with a memory footprint that is, up to small constant factors,\nno more than that which is required to compute a single inner loop gradient and\nat no overall increase in the total computational cost. Experimentally, we show\nthat these benefits of implicit MAML translate into empirical gains on few-shot\nimage recognition benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 17:14:14 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Rajeswaran", "Aravind", ""], ["Finn", "Chelsea", ""], ["Kakade", "Sham", ""], ["Levine", "Sergey", ""]]}, {"id": "1909.04696", "submitter": "Arijit Ray", "authors": "Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee, Giedrius Burachas", "title": "Sunny and Dark Outside?! Improving Answer Consistency in VQA through\n  Entailed Question Generation", "comments": "2019 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While models for Visual Question Answering (VQA) have steadily improved over\nthe years, interacting with one quickly reveals that these models lack\nconsistency. For instance, if a model answers \"red\" to \"What color is the\nballoon?\", it might answer \"no\" if asked, \"Is the balloon red?\". These\nresponses violate simple notions of entailment and raise questions about how\neffectively VQA models ground language. In this work, we introduce a dataset,\nConVQA, and metrics that enable quantitative evaluation of consistency in VQA.\nFor a given observable fact in an image (e.g. the balloon's color), we generate\na set of logically consistent question-answer (QA) pairs (e.g. Is the balloon\nred?) and also collect a human-annotated set of common-sense based consistent\nQA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we\npropose a consistency-improving data augmentation module, a Consistency Teacher\nModule (CTM). CTM automatically generates entailed (or similar-intent)\nquestions for a source QA pair and fine-tunes the VQA model if the VQA's answer\nto the entailed question is consistent with the source QA pair. We demonstrate\nthat our CTM-based training improves the consistency of VQA models on the\nConVQA datasets and is a strong baseline for further research.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 18:18:45 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Ray", "Arijit", ""], ["Sikka", "Karan", ""], ["Divakaran", "Ajay", ""], ["Lee", "Stefan", ""], ["Burachas", "Giedrius", ""]]}, {"id": "1909.04719", "submitter": "Haifeng Qian", "authors": "Haifeng Qian", "title": "Neural Belief Reasoner", "comments": null, "journal-ref": "International Joint Conference on Artificial Intelligence (IJCAI),\n  2020", "doi": "10.24963/ijcai.2020/590", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new generative model called neural belief reasoner\n(NBR). It differs from previous models in that it specifies a belief function\nrather than a probability distribution. Its implementation consists of neural\nnetworks, fuzzy-set operations and belief-function operations, and\nquery-answering, sample-generation and training algorithms are presented. This\npaper studies NBR in two tasks. The first is a synthetic unsupervised-learning\ntask, which demonstrates NBR's ability to perform multi-hop reasoning,\nreasoning with uncertainty and reasoning about conflicting information. The\nsecond is supervised learning: a robust MNIST classifier for 4 and 9, which is\nthe most challenging pair of digits. This classifier needs no adversarial\ntraining, and it substantially exceeds the state of the art in adversarial\nrobustness as measured by the L2 metric, while at the same time maintains 99.1%\naccuracy on natural images.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 19:56:11 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 03:40:04 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 18:12:49 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Qian", "Haifeng", ""]]}, {"id": "1909.04723", "submitter": "Navdeep Kaur", "authors": "Navdeep Kaur and Gautam Kunapuli and Saket Joshi and Kristian Kersting\n  and Sriraam Natarajan", "title": "Neural Networks for Relational Data", "comments": "15 pages, 2 figures. To appear in the proceedings of 29th\n  International Conference on Inductive Logic Programming (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While deep networks have been enormously successful over the last decade,\nthey rely on flat-feature vector representations, which makes them unsuitable\nfor richly structured domains such as those arising in applications like social\nnetwork analysis. Such domains rely on relational representations to capture\ncomplex relationships between entities and their attributes. Thus, we consider\nthe problem of learning neural networks for relational data. We distinguish\nourselves from current approaches that rely on expert hand-coded rules by\nlearning relational random-walk-based features to capture local structural\ninteractions and the resulting network architecture. We further exploit\nparameter tying of the network weights of the resulting relational neural\nnetwork, where instances of the same type share parameters. Our experimental\nresults across several standard relational data sets demonstrate the\neffectiveness of the proposed approach over multiple neural net baselines as\nwell as state-of-the-art statistical relational models.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 17:11:05 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 17:15:26 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 17:26:00 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Kaur", "Navdeep", ""], ["Kunapuli", "Gautam", ""], ["Joshi", "Saket", ""], ["Kersting", "Kristian", ""], ["Natarajan", "Sriraam", ""]]}, {"id": "1909.04739", "submitter": "Niket Tandon", "authors": "Niket Tandon and Bhavana Dalvi Mishra and Keisuke Sakaguchi and\n  Antoine Bosselut and Peter Clark", "title": "WIQA: A dataset for \"What if...\" reasoning over procedural text", "comments": "Accepted at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce WIQA, the first large-scale dataset of \"What if...\" questions\nover procedural text. WIQA contains three parts: a collection of paragraphs\neach describing a process, e.g., beach erosion; a set of crowdsourced influence\ngraphs for each paragraph, describing how one change affects another; and a\nlarge (40k) collection of \"What if...?\" multiple-choice questions derived from\nthe graphs. For example, given a paragraph about beach erosion, would stormy\nweather result in more or less erosion (or have no effect)? The task is to\nanswer the questions, given their associated paragraph. WIQA contains three\nkinds of questions: perturbations to steps mentioned in the paragraph; external\n(out-of-paragraph) perturbations requiring commonsense knowledge; and\nirrelevant (no effect) perturbations. We find that state-of-the-art models\nachieve 73.8% accuracy, well below the human performance of 96.3%. We analyze\nthe challenges, in particular tracking chains of influences, and present the\ndataset as an open challenge to the community.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 20:37:39 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Tandon", "Niket", ""], ["Mishra", "Bhavana Dalvi", ""], ["Sakaguchi", "Keisuke", ""], ["Bosselut", "Antoine", ""], ["Clark", "Peter", ""]]}, {"id": "1909.04745", "submitter": "Niket Tandon", "authors": "Bhavana Dalvi Mishra and Niket Tandon and Antoine Bosselut and Wen-tau\n  Yih and Peter Clark", "title": "Everything Happens for a Reason: Discovering the Purpose of Actions in\n  Procedural Text", "comments": "Accepted to EMNLP 2019 as a long paper. This revision fixed a typo in\n  an author name in references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to better comprehend procedural text, e.g., a paragraph about\nphotosynthesis, by not only predicting what happens, but why some actions need\nto happen before others. Our approach builds on a prior process comprehension\nframework for predicting actions' effects, to also identify subsequent steps\nthat those effects enable. We present our new model (XPAD) that biases effect\npredictions towards those that (1) explain more of the actions in the paragraph\nand (2) are more plausible with respect to background knowledge. We also extend\nan existing benchmark dataset for procedural text comprehension, ProPara, by\nadding the new task of explaining actions by predicting their dependencies. We\nfind that XPAD significantly outperforms prior systems on this task, while\nmaintaining the performance on the original task in ProPara. The dataset is\navailable at http://data.allenai.org/propara\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 20:46:56 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 23:55:04 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Mishra", "Bhavana Dalvi", ""], ["Tandon", "Niket", ""], ["Bosselut", "Antoine", ""], ["Yih", "Wen-tau", ""], ["Clark", "Peter", ""]]}, {"id": "1909.04760", "submitter": "Venktesh Pandey", "authors": "Venktesh Pandey, Evana Wang, and Stephen D. Boyles", "title": "Deep Reinforcement Learning Algorithm for Dynamic Pricing of Express\n  Lanes with Multiple Access Locations", "comments": null, "journal-ref": null, "doi": "10.1016/j.trc.2020.102715", "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a deep reinforcement learning (Deep-RL) framework for\ndynamic pricing on managed lanes with multiple access locations and\nheterogeneity in travelers' value of time, origin, and destination. This\nframework relaxes assumptions in the literature by considering multiple origins\nand destinations, multiple access locations to the managed lane, en route\ndiversion of travelers, partial observability of the sensor readings, and\nstochastic demand and observations. The problem is formulated as a partially\nobservable Markov decision process (POMDP) and policy gradient methods are used\nto determine tolls as a function of real-time observations. Tolls are modeled\nas continuous and stochastic variables, and are determined using a feedforward\nneural network. The method is compared against a feedback control method used\nfor dynamic pricing. We show that Deep-RL is effective in learning toll\npolicies for maximizing revenue, minimizing total system travel time, and other\njoint weighted objectives, when tested on real-world transportation networks.\nThe Deep-RL toll policies outperform the feedback control heuristic for the\nrevenue maximization objective by generating revenues up to 9.5% higher than\nthe heuristic and for the objective minimizing total system travel time (TSTT)\nby generating TSTT up to 10.4% lower than the heuristic. We also propose reward\nshaping methods for the POMDP to overcome the undesired behavior of toll\npolicies, like the jam-and-harvest behavior of revenue-maximizing policies.\nAdditionally, we test transferability of the algorithm trained on one set of\ninputs for new input distributions and offer recommendations on real-time\nimplementations of Deep-RL algorithms. The source code for our experiments is\navailable online at https://github.com/venktesh22/ExpressLanes_Deep-RL\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 21:20:59 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Pandey", "Venktesh", ""], ["Wang", "Evana", ""], ["Boyles", "Stephen D.", ""]]}, {"id": "1909.04787", "submitter": "Bohan Wu", "authors": "Bohan Wu, Iretiayo Akinola, Jacob Varley, Peter Allen", "title": "MAT: Multi-Fingered Adaptive Tactile Grasping via Deep Reinforcement\n  Learning", "comments": "Accepted at 3rd Conference on Robot Learning (CoRL 2019). Oral\n  Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vision-based grasping systems typically adopt an open-loop execution of a\nplanned grasp. This policy can fail due to many reasons, including ubiquitous\ncalibration error. Recovery from a failed grasp is further complicated by\nvisual occlusion, as the hand is usually occluding the vision sensor as it\nattempts another open-loop regrasp. This work presents MAT, a tactile\nclosed-loop method capable of realizing grasps provided by a coarse initial\npositioning of the hand above an object. Our algorithm is a deep reinforcement\nlearning (RL) policy optimized through the clipped surrogate objective within a\nmaximum entropy RL framework to balance exploitation and exploration. The\nmethod utilizes tactile and proprioceptive information to act through both fine\nfinger motions and larger regrasp movements to execute stable grasps. A novel\ncurriculum of action motion magnitude makes learning more tractable and helps\nturn common failure cases into successes. Careful selection of features that\nexhibit small sim-to-real gaps enables this tactile grasping policy, trained\npurely in simulation, to transfer well to real world environments without the\nneed for additional learning. Experimentally, this methodology improves over a\nvision-only grasp success rate substantially on a multi-fingered robot hand.\nWhen this methodology is used to realize grasps from coarse initial positions\nprovided by a vision-only planner, the system is made dramatically more robust\nto calibration errors in the camera-robot transform.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 23:02:04 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 00:04:26 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Wu", "Bohan", ""], ["Akinola", "Iretiayo", ""], ["Varley", "Jacob", ""], ["Allen", "Peter", ""]]}, {"id": "1909.04840", "submitter": "Yang Yang", "authors": "Yang Yang, Hengyue Liang and Changhyun Choi", "title": "A Deep Learning Approach to Grasping the Invisible", "comments": "Accepted to the IEEE Robotics and Automation Letters (RA-L). Project\n  page: https://sites.google.com/umn.edu/grasping-invisible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an emerging problem named \"grasping the invisible\" in robotic\nmanipulation, in which a robot is tasked to grasp an initially invisible target\nobject via a sequence of pushing and grasping actions. In this problem, pushes\nare needed to search for the target and rearrange cluttered objects around it\nto enable effective grasps. We propose to solve the problem by formulating a\ndeep learning approach in a critic-policy format. The target-oriented motion\ncritic, which maps both visual observations and target information to the\nexpected future rewards of pushing and grasping motion primitives, is learned\nvia deep Q-learning. We divide the problem into two subtasks, and two policies\nare proposed to tackle each of them, by combining the critic predictions and\nrelevant domain knowledge. A Bayesian-based policy accounting for past action\nexperience performs pushing to search for the target; once the target is found,\na classifier-based policy coordinates target-oriented pushing and grasping to\ngrasp the target in clutter. The motion critic and the classifier are trained\nin a self-supervised manner through robot-environment interactions. Our system\nachieves a 93% and 87% task success rate on each of the two subtasks in\nsimulation and an 85% task success rate in real robot experiments on the whole\nproblem, which outperforms several baselines by large margins. Supplementary\nmaterial is available at https://sites.google.com/umn.edu/grasping-invisible.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 03:28:55 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 15:43:44 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Yang", "Yang", ""], ["Liang", "Hengyue", ""], ["Choi", "Changhyun", ""]]}, {"id": "1909.04849", "submitter": "Sewon Min", "authors": "Sewon Min, Danqi Chen, Hannaneh Hajishirzi, Luke Zettlemoyer", "title": "A Discrete Hard EM Approach for Weakly Supervised Question Answering", "comments": "Published as a conference paper at EMNLP 2019 (long). Code available\n  at https://github.com/shmsw25/qa-hard-em", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many question answering (QA) tasks only provide weak supervision for how the\nanswer should be computed. For example, TriviaQA answers are entities that can\nbe mentioned multiple times in supporting documents, while DROP answers can be\ncomputed by deriving many different equations from numbers in the reference\ntext. In this paper, we show it is possible to convert such tasks into discrete\nlatent variable learning problems with a precomputed, task-specific set of\npossible \"solutions\" (e.g. different mentions or equations) that contains one\ncorrect option. We then develop a hard EM learning scheme that computes\ngradients relative to the most likely solution at each update. Despite its\nsimplicity, we show that this approach significantly outperforms previous\nmethods on six QA tasks, including absolute gains of 2--10%, and achieves the\nstate-of-the-art on five of them. Using hard updates instead of maximizing\nmarginal likelihood is key to these results as it encourages the model to find\nthe one correct answer, which we show through detailed qualitative analysis.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 04:47:36 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Min", "Sewon", ""], ["Chen", "Danqi", ""], ["Hajishirzi", "Hannaneh", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1909.04866", "submitter": "Dylan Campbell", "authors": "Stephen Gould, Richard Hartley and Dylan Campbell", "title": "Deep Declarative Networks: A New Hope", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a new class of end-to-end learnable models wherein data processing\nnodes (or network layers) are defined in terms of desired behavior rather than\nan explicit forward function. Specifically, the forward function is implicitly\ndefined as the solution to a mathematical optimization problem. Consistent with\nnomenclature in the programming languages community, we name these models deep\ndeclarative networks. Importantly, we show that the class of deep declarative\nnetworks subsumes current deep learning models. Moreover, invoking the implicit\nfunction theorem, we show how gradients can be back-propagated through many\ndeclaratively defined data processing nodes thereby enabling end-to-end\nlearning. We show how these declarative processing nodes can be implemented in\nthe popular PyTorch deep learning software library allowing declarative and\nimperative nodes to co-exist within the same network. We also provide numerous\ninsights and illustrative examples of declarative nodes and demonstrate their\napplication for image and point cloud classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 06:19:25 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 03:56:39 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Gould", "Stephen", ""], ["Hartley", "Richard", ""], ["Campbell", "Dylan", ""]]}, {"id": "1909.04998", "submitter": "Zeynep G\\\"ozen Saribatur", "authors": "Thomas Eiter, Zeynep G. Saribatur, Peter Sch\\\"uller", "title": "Abstraction for Zooming-In to Unsolvability Reasons of Grid-Cell\n  Problems", "comments": "Presented at the IJCAI 2019 Workshop on Explainable Artificial\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are capable of abstracting away irrelevant details when studying\nproblems. This is especially noticeable for problems over grid-cells, as humans\nare able to disregard certain parts of the grid and focus on the key elements\nimportant for the problem. Recently, the notion of abstraction has been\nintroduced for Answer Set Programming (ASP), a knowledge representation and\nreasoning paradigm widely used in problem solving, with the potential to\nunderstand the key elements of a program that play a role in finding a\nsolution. The present paper takes this further and empowers abstraction to deal\nwith structural aspects, and in particular with hierarchical abstraction over\nthe domain. We focus on obtaining the reasons for unsolvability of problems on\ngrids, and show the possibility to automatically achieve human-like\nabstractions that distinguish only the relevant part of the grid. A user study\non abstract explanations confirms the similarity of the focus points in machine\nvs. human explanations and reaffirms the challenge of employing abstraction to\nobtain machine explanations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 12:17:09 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Eiter", "Thomas", ""], ["Saribatur", "Zeynep G.", ""], ["Sch\u00fcller", "Peter", ""]]}, {"id": "1909.05017", "submitter": "Artit Wangperawong", "authors": "Kettip Kriangchaivech and Artit Wangperawong", "title": "Question Generation by Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A machine learning model was developed to automatically generate questions\nfrom Wikipedia passages using transformers, an attention-based model eschewing\nthe paradigm of existing recurrent neural networks (RNNs). The model was\ntrained on the inverted Stanford Question Answering Dataset (SQuAD), which is a\nreading comprehension dataset consisting of 100,000+ questions posed by\ncrowdworkers on a set of Wikipedia articles. After training, the question\ngeneration model is able to generate simple questions relevant to unseen\npassages and answers containing an average of 8 words per question. The word\nerror rate (WER) was used as a metric to compare the similarity between SQuAD\nquestions and the model-generated questions. Although the high average WER\nsuggests that the questions generated differ from the original SQuAD questions,\nthe questions generated are mostly grammatically correct and plausible in their\nown right.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:48:53 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 20:02:20 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kriangchaivech", "Kettip", ""], ["Wangperawong", "Artit", ""]]}, {"id": "1909.05035", "submitter": "Andreas Orthey", "authors": "Andreas Orthey, Benjamin Fr\\'esz, Marc Toussaint", "title": "Motion Planning Explorer: Visualizing Local Minima using a Local-Minima\n  Tree", "comments": "Submitted to Robotics and Automation Letters (RA-L) / International\n  Conference on Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion planning problems often have many local minima. Those minima are\nimportant to visualize to let a user guide, prevent or predict motions. Towards\nthis goal, we develop the motion planning explorer, an algorithm to let users\ninteractively explore a tree of local-minima. Following ideas from Morse\ntheory, we define local minima as paths invariant under minimization of a cost\nfunctional. The local-minima are grouped into a local-minima tree using\nlower-dimensional projections specified by a user. The user can then\ninteractively explore the local-minima tree, thereby visualizing the problem\nstructure and guide or prevent motions. We show the motion planning explorer to\nfaithfully capture local minima in four realistic scenarios, both for holonomic\nand certain non-holonomic robots.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 13:18:49 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 17:44:19 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Orthey", "Andreas", ""], ["Fr\u00e9sz", "Benjamin", ""], ["Toussaint", "Marc", ""]]}, {"id": "1909.05044", "submitter": "Jonas Schouterden", "authors": "Jonas Schouterden, Jesse Davis, Hendrik Blockeel", "title": "LazyBum: Decision tree learning using lazy propositionalization", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": "10.1007/978-3-030-49210-6_9", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propositionalization is the process of summarizing relational data into a\ntabular (attribute-value) format. The resulting table can next be used by any\npropositional learner. This approach makes it possible to apply a wide variety\nof learning methods to relational data. However, the transformation from\nrelational to propositional format is generally not lossless: different\nrelational structures may be mapped onto the same feature vector. At the same\ntime, features may be introduced that are not needed for the learning task at\nhand. In general, it is hard to define a feature space that contains all and\nonly those features that are needed for the learning task. This paper presents\nLazyBum, a system that can be considered a lazy version of the recently\nproposed OneBM method for propositionalization. LazyBum interleaves OneBM's\nfeature construction method with a decision tree learner. This learner both\nuses and guides the propositionalization process. It indicates when and where\nto look for new features. This approach is similar to what has elsewhere been\ncalled dynamic propositionalization. In an experimental comparison with the\noriginal OneBM and with two other recently proposed propositionalization\nmethods (nFOIL and MODL, which respectively perform dynamic and static\npropositionalization), LazyBum achieves a comparable accuracy with a lower\nexecution time on most of the datasets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 13:33:27 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Schouterden", "Jonas", ""], ["Davis", "Jesse", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1909.05100", "submitter": "Chen Cai", "authors": "Chen Cai", "title": "Group Representation Theory for Knowledge Graph Embedding", "comments": "Paper withdrawn due to company policy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding has recently become a popular way to model\nrelations and infer missing links. In this paper, we present a group\ntheoretical perspective of knowledge graph embedding, connecting previous\nmethods with different group actions. Furthermore, by utilizing Schur's lemma\nfrom group representation theory, we show that the state of the art embedding\nmethod RotatE can model relations from any finite Abelian group.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 14:55:19 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 16:24:46 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Cai", "Chen", ""]]}, {"id": "1909.05106", "submitter": "Bastian Alt", "authors": "Bastian Alt, Adrian \\v{S}o\\v{s}i\\'c, Heinz Koeppl", "title": "Correlation Priors for Reinforcement Learning", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many decision-making problems naturally exhibit pronounced structures\ninherited from the characteristics of the underlying environment. In a Markov\ndecision process model, for example, two distinct states can have inherently\nrelated semantics or encode resembling physical state configurations. This\noften implies locally correlated transition dynamics among the states. In order\nto complete a certain task in such environments, the operating agent usually\nneeds to execute a series of temporally and spatially correlated actions.\nThough there exists a variety of approaches to capture these correlations in\ncontinuous state-action domains, a principled solution for discrete\nenvironments is missing. In this work, we present a Bayesian learning framework\nbased on P\\'olya-Gamma augmentation that enables an analogous reasoning in such\ncases. We demonstrate the framework on a number of common decision-making\nrelated problems, such as imitation learning, subgoal extraction, system\nidentification and Bayesian reinforcement learning. By explicitly modeling the\nunderlying correlation structures of these problems, the proposed approach\nyields superior predictive performance compared to correlation-agnostic models,\neven when trained on data sets that are an order of magnitude smaller in size.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 15:01:31 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 11:01:39 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Alt", "Bastian", ""], ["\u0160o\u0161i\u0107", "Adrian", ""], ["Koeppl", "Heinz", ""]]}, {"id": "1909.05160", "submitter": "Muneeb Imtiaz Ahmad", "authors": "Muneeb Imtiaz Ahmad, Jasmin Bernotat, Katrin Lohan, Friederike Eyssel", "title": "Trust and Cognitive Load During Human-Robot Interaction", "comments": "10 Pages, 5 figures, AAAI Symposium on Artificial Intelligence for\n  Human-Robot Interaction, 7th-9th November, 2019", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/06", "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an exploratory study to understand the relationship\nbetween a humans' cognitive load, trust, and anthropomorphism during\nhuman-robot interaction. To understand the relationship, we created a\n\\say{Matching the Pair} game that participants could play collaboratively with\none of two robot types, Husky or Pepper. The goal was to understand if humans\nwould trust the robot as a teammate while being in the game-playing situation\nthat demanded a high level of cognitive load. Using a humanoid vs. a technical\nrobot, we also investigated the impact of physical anthropomorphism and we\nfurthermore tested the impact of robot error rate on subsequent judgments and\nbehavior. Our results showed that there was an inversely proportional\nrelationship between trust and cognitive load, suggesting that as the amount of\ncognitive load increased in the participants, their ratings of trust decreased.\nWe also found a triple interaction impact between robot-type, error-rate and\nparticipant's ratings of trust. We found that participants perceived Pepper to\nbe more trustworthy in comparison with the Husky robot after playing the game\nwith both robots under high error-rate condition. On the contrary, Husky was\nperceived as more trustworthy than Pepper when it was depicted as featuring a\nlow error-rate. Our results are interesting and call further investigation of\nthe impact of physical anthropomorphism in combination with variable\nerror-rates of the robot.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 15:56:22 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Ahmad", "Muneeb Imtiaz", ""], ["Bernotat", "Jasmin", ""], ["Lohan", "Katrin", ""], ["Eyssel", "Friederike", ""]]}, {"id": "1909.05167", "submitter": "Kacper Sokol", "authors": "Kacper Sokol, Raul Santos-Rodriguez, Peter Flach", "title": "FAT Forensics: A Python Toolbox for Algorithmic Fairness, Accountability\n  and Transparency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms can take important decisions, sometimes legally\nbinding, about our everyday life. In most cases, however, these systems and\ndecisions are neither regulated nor certified. Given the potential harm that\nthese algorithms can cause, qualities such as fairness, accountability and\ntransparency of predictive systems are of paramount importance. Recent\nliterature suggested voluntary self-reporting on these aspects of predictive\nsystems -- e.g., data sheets for data sets -- but their scope is often limited\nto a single component of a machine learning pipeline, and producing them\nrequires manual labour. To resolve this impasse and ensure high-quality, fair,\ntransparent and reliable machine learning systems, we developed an open source\ntoolbox that can inspect selected fairness, accountability and transparency\naspects of these systems to automatically and objectively report them back to\ntheir engineers and users. We describe design, scope and usage examples of this\nPython toolbox in this paper. The toolbox provides functionality for inspecting\nfairness, accountability and transparency of all aspects of the machine\nlearning process: data (and their features), models and predictions. It is\navailable to the public under the BSD 3-Clause open source licence.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 16:11:44 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Sokol", "Kacper", ""], ["Santos-Rodriguez", "Raul", ""], ["Flach", "Peter", ""]]}, {"id": "1909.05190", "submitter": "Kuo Liao", "authors": "Xiao Ding, Kuo Liao, Ting Liu, Zhongyang Li, Junwen Duan", "title": "Event Representation Learning Enhanced with External Commonsense\n  Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work has proposed effective methods to learn event representations that\ncan capture syntactic and semantic information over text corpus, demonstrating\ntheir effectiveness for downstream tasks such as script event prediction. On\nthe other hand, events extracted from raw texts lacks of commonsense knowledge,\nsuch as the intents and emotions of the event participants, which are useful\nfor distinguishing event pairs when there are only subtle differences in their\nsurface realizations. To address this issue, this paper proposes to leverage\nexternal commonsense knowledge about the intent and sentiment of the event.\nExperiments on three event-related tasks, i.e., event similarity, script event\nprediction and stock market prediction, show that our model obtains much better\nevent embeddings for the tasks, achieving 78% improvements on hard similarity\ntask, yielding more precise inferences on subsequent events under given\ncontexts, and better accuracies in predicting the volatilities of the stock\nmarket.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 03:00:39 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 08:51:02 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ding", "Xiao", ""], ["Liao", "Kuo", ""], ["Liu", "Ting", ""], ["Li", "Zhongyang", ""], ["Duan", "Junwen", ""]]}, {"id": "1909.05232", "submitter": "Yilun Zhou", "authors": "Yilun Zhou, Derrik E. Asher, Nicholas R. Waytowich, Julie A. Shah", "title": "On Memory Mechanism in Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning (MARL) extends (single-agent)\nreinforcement learning (RL) by introducing additional agents and (potentially)\npartial observability of the environment. Consequently, algorithms for solving\nMARL problems incorporate various extensions beyond traditional RL methods,\nsuch as a learned communication protocol between cooperative agents that\nenables exchange of private information or adaptive modeling of opponents in\ncompetitive settings. One popular algorithmic construct is a memory mechanism\nsuch that an agent's decisions can depend not only upon the current state but\nalso upon the history of observed states and actions. In this paper, we study\nhow a memory mechanism can be useful in environments with different properties,\nsuch as observability, internality and presence of a communication channel.\nUsing both prior work and new experiments, we show that a memory mechanism is\nhelpful when learning agents need to model other agents and/or when\ncommunication is constrained in some way; however we must to be cautious of\nagents achieving effective memoryfulness through other means.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 17:42:14 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Zhou", "Yilun", ""], ["Asher", "Derrik E.", ""], ["Waytowich", "Nicholas R.", ""], ["Shah", "Julie A.", ""]]}, {"id": "1909.05236", "submitter": "Thiago D. Sim\\~ao", "authors": "Thiago D. Sim\\~ao, Romain Laroche, R\\'emi Tachet des Combes", "title": "Safe Policy Improvement with an Estimated Baseline Policy", "comments": "Published at AAMAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous work has shown the unreliability of existing algorithms in the batch\nReinforcement Learning setting, and proposed the theoretically-grounded Safe\nPolicy Improvement with Baseline Bootstrapping (SPIBB) fix: reproduce the\nbaseline policy in the uncertain state-action pairs, in order to control the\nvariance on the trained policy performance. However, in many real-world\napplications such as dialogue systems, pharmaceutical tests or crop management,\ndata is collected under human supervision and the baseline remains unknown. In\nthis paper, we apply SPIBB algorithms with a baseline estimate built from the\ndata. We formally show safe policy improvement guarantees over the true\nbaseline even without direct access to it. Our empirical experiments on finite\nand continuous states tasks support the theoretical findings. It shows little\nloss of performance in comparison with SPIBB when the baseline policy is given,\nand more importantly, drastically and significantly outperforms competing\nalgorithms both in safe policy improvement, and in average performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 17:48:00 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 20:25:07 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sim\u00e3o", "Thiago D.", ""], ["Laroche", "Romain", ""], ["Combes", "R\u00e9mi Tachet des", ""]]}, {"id": "1909.05329", "submitter": "Yaqi Xie", "authors": "Yaqi Xie, Indu P Bodala, Desmond C. Ong, David Hsu, Harold Soh", "title": "Robot Capability and Intention in Trust-based Decisions across Tasks", "comments": null, "journal-ref": "ACM/IEEE Conference on Human Robot Interaction (HRI), 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present results from a human-subject study designed to\nexplore two facets of human mental models of robots---inferred capability and\nintention---and their relationship to overall trust and eventual decisions. In\nparticular, we examine delegation situations characterized by uncertainty, and\nexplore how inferred capability and intention are applied across different\ntasks. We develop an online survey where human participants decide whether to\ndelegate control to a simulated UAV agent. Our study shows that human\nestimations of robot capability and intent correlate strongly with overall\nself-reported trust. However, overall trust is not independently sufficient to\ndetermine whether a human will decide to trust (delegate) a given task to a\nrobot. Instead, our study reveals that estimations of robot intention,\ncapability, and overall trust are integrated when deciding to delegate. From a\nbroader perspective, these results suggest that calibrating overall trust alone\nis insufficient; to make correct decisions, humans need (and use) multi-faceted\nmental models when collaborating with robots across multiple contexts.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 10:42:24 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Xie", "Yaqi", ""], ["Bodala", "Indu P", ""], ["Ong", "Desmond C.", ""], ["Hsu", "David", ""], ["Soh", "Harold", ""]]}, {"id": "1909.05355", "submitter": "Preksha Nema I", "authors": "Preksha Nema, Akash Kumar Mohankumar, Mitesh M. Khapra, Balaji Vasan\n  Srinivasan, Balaraman Ravindran", "title": "Let's Ask Again: Refine Network for Automatic Question Generation", "comments": "accepted in EMNLP 2019 in Main Conference, (10 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we focus on the task of Automatic Question Generation (AQG)\nwhere given a passage and an answer the task is to generate the corresponding\nquestion. It is desired that the generated question should be (i) grammatically\ncorrect (ii) answerable from the passage and (iii) specific to the given\nanswer. An analysis of existing AQG models shows that they produce questions\nwhich do not adhere to one or more of {the above-mentioned qualities}. In\nparticular, the generated questions look like an incomplete draft of the\ndesired question with a clear scope for refinement. {To alleviate this\nshortcoming}, we propose a method which tries to mimic the human process of\ngenerating questions by first creating an initial draft and then refining it.\nMore specifically, we propose Refine Network (RefNet) which contains two\ndecoders. The second decoder uses a dual attention network which pays attention\nto both (i) the original passage and (ii) the question (initial draft)\ngenerated by the first decoder. In effect, it refines the question generated by\nthe first decoder, thereby making it more correct and complete. We evaluate\nRefNet on three datasets, \\textit{viz.}, SQuAD, HOTPOT-QA, and DROP, and show\nthat it outperforms existing state-of-the-art methods by 7-16\\% on all of these\ndatasets. Lastly, we show that we can improve the quality of the second decoder\non specific metrics, such as, fluency and answerability by explicitly rewarding\nrevisions that improve on the corresponding metric during training. The code\nhas been made publicly available\n\\footnote{https://github.com/PrekshaNema25/RefNet-QG}\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 04:03:26 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Nema", "Preksha", ""], ["Mohankumar", "Akash Kumar", ""], ["Khapra", "Mitesh M.", ""], ["Srinivasan", "Balaji Vasan", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1909.05356", "submitter": "Alankar Jain", "authors": "Alankar Jain, Bhargavi Paranjape, Zachary C. Lipton", "title": "Entity Projection via Machine Translation for Cross-Lingual NER", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although over 100 languages are supported by strong off-the-shelf machine\ntranslation systems, only a subset of them possess large annotated corpora for\nnamed entity recognition. Motivated by this fact, we leverage machine\ntranslation to improve annotation-projection approaches to cross-lingual named\nentity recognition. We propose a system that improves over prior\nentity-projection methods by: (a) leveraging machine translation systems twice:\nfirst for translating sentences and subsequently for translating entities; (b)\nmatching entities based on orthographic and phonetic similarity; and (c)\nidentifying matches based on distributional statistics derived from the\ndataset. Our approach improves upon current state-of-the-art methods for\ncross-lingual named entity recognition on 5 diverse languages by an average of\n4.1 points. Further, our method achieves state-of-the-art F_1 scores for\nArmenian, outperforming even a monolingual model trained on Armenian source\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 17:40:21 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 06:44:24 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Jain", "Alankar", ""], ["Paranjape", "Bhargavi", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "1909.05358", "submitter": "Chinnadhurai Sankar", "authors": "Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind\n  Neelakantan, Daniel Duckworth, Semih Yavuz, Ben Goodrich, Amit Dubey, Andy\n  Cedilnik, Kyu-Young Kim", "title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset", "comments": "To appear at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant barrier to progress in data-driven approaches to building\ndialog systems is the lack of high quality, goal-oriented conversational data.\nTo help satisfy this elementary requirement, we introduce the initial release\nof the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising\nsix domains. Two procedures were used to create this collection, each with\nunique advantages. The first involves a two-person, spoken \"Wizard of Oz\" (WOz)\napproach in which trained agents and crowdsourced workers interact to complete\nthe task while the second is \"self-dialog\" in which crowdsourced workers write\nthe entire dialog themselves. We do not restrict the workers to detailed\nscripts or to a small knowledge base and hence we observe that our dataset\ncontains more realistic and diverse conversations in comparison to existing\ndatasets. We offer several baseline models including state of the art neural\nseq2seq architectures with benchmark performance as well as qualitative human\nevaluations. Dialogs are labeled with API calls and arguments, a simple and\ncost effective approach which avoids the requirement of complex annotation\nschema. The layer of abstraction between the dialog model and the service\nprovider API allows for a given model to interact with multiple services that\nprovide similar functionally. Finally, the dataset will evoke interest in\nwritten vs. spoken language, discourse patterns, error handling and other\nlinguistic phenomena related to dialog system research, development and design.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 22:18:39 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Byrne", "Bill", ""], ["Krishnamoorthi", "Karthik", ""], ["Sankar", "Chinnadhurai", ""], ["Neelakantan", "Arvind", ""], ["Duckworth", "Daniel", ""], ["Yavuz", "Semih", ""], ["Goodrich", "Ben", ""], ["Dubey", "Amit", ""], ["Cedilnik", "Andy", ""], ["Kim", "Kyu-Young", ""]]}, {"id": "1909.05359", "submitter": "Vitor Beires Nogueira", "authors": "Paulo Quaresma, Vitor Beires Nogueira, Kashyap Raiyani, Roy Bayot, and\n  Teresa Gon\\c{c}alves", "title": "From Textual Information Sources to Linked Data in the Agatha Project", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic reasoning about textual information is a challenging task in modern\nNatural Language Processing (NLP) systems. In this work we describe our\nproposal for representing and reasoning about Portuguese documents by means of\nLinked Data like ontologies and thesauri. Our approach resorts to a specialized\npipeline of natural language processing (part-of-speech tagger, named entity\nrecognition, semantic role labeling) to populate an ontology for the domain of\ncriminal investigations. The provided architecture and ontology are language\nindependent. Although some of the NLP modules are language dependent, they can\nbe built using adequate AI methodologies.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 08:27:37 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Quaresma", "Paulo", ""], ["Nogueira", "Vitor Beires", ""], ["Raiyani", "Kashyap", ""], ["Bayot", "Roy", ""], ["Gon\u00e7alves", "Teresa", ""]]}, {"id": "1909.05360", "submitter": "Rujun Han", "authors": "Rujun Han, Qiang Ning, Nanyun Peng", "title": "Joint Event and Temporal Relation Extraction with Shared Representations\n  and Structured Prediction", "comments": "Published at EMNLP'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a joint event and temporal relation extraction model with shared\nrepresentation learning and structured prediction. The proposed method has two\nadvantages over existing work. First, it improves event representation by\nallowing the event and relation modules to share the same contextualized\nembeddings and neural representation learner. Second, it avoids error\npropagation in the conventional pipeline systems by leveraging structured\ninference and learning methods to assign both the event labels and the temporal\nrelation labels jointly. Experiments show that the proposed method can improve\nboth event extraction and temporal relation extraction over state-of-the-art\nsystems, with the end-to-end F1 improved by 10% and 6.8% on two benchmark\ndatasets respectively.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 18:00:43 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 22:23:33 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Han", "Rujun", ""], ["Ning", "Qiang", ""], ["Peng", "Nanyun", ""]]}, {"id": "1909.05361", "submitter": "Xiang Gao", "authors": "Xiang Gao, Yizhe Zhang, Sungjin Lee, Michel Galley, Chris Brockett,\n  Jianfeng Gao, Bill Dolan", "title": "Structuring Latent Spaces for Stylized Response Generation", "comments": "accepted to appear at EMNLP 2019 (long)", "journal-ref": "EMNLP 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating responses in a targeted style is a useful yet challenging task,\nespecially in the absence of parallel data. With limited data, existing methods\ntend to generate responses that are either less stylized or less\ncontext-relevant. We propose StyleFusion, which bridges conversation modeling\nand non-parallel style transfer by sharing a structured latent space. This\nstructure allows the system to generate stylized relevant responses by sampling\nin the neighborhood of the conversation model prediction, and continuously\ncontrol the style level. We demonstrate this method using dialogues from Reddit\ndata and two sets of sentences with distinct styles (arXiv and Sherlock Holmes\nnovels). Automatic and human evaluation show that, without sacrificing\nappropriateness, the system generates responses of the targeted style and\noutperforms competitive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 18:11:58 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Gao", "Xiang", ""], ["Zhang", "Yizhe", ""], ["Lee", "Sungjin", ""], ["Galley", "Michel", ""], ["Brockett", "Chris", ""], ["Gao", "Jianfeng", ""], ["Dolan", "Bill", ""]]}, {"id": "1909.05363", "submitter": "Armins Stepanjans", "authors": "Armins Stepanjans and Andr\\'e Freitas", "title": "Identifying and Explaining Discriminative Attributes", "comments": "EMNLP-IJCNLP 2019, source code available at\n  https://github.com/ab-10/hawk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying what is at the center of the meaning of a word and what\ndiscriminates it from other words is a fundamental natural language inference\ntask. This paper describes an explicit word vector representation model (WVM)\nto support the identification of discriminative attributes. A core contribution\nof the paper is a quantitative and qualitative comparative analysis of\ndifferent types of data sources and Knowledge Bases in the construction of\nexplainable and explicit WVMs: (i) knowledge graphs built from dictionary\ndefinitions, (ii) entity-attribute-relationships graphs derived from images and\n(iii) commonsense knowledge graphs. Using a detailed quantitative and\nqualitative analysis, we demonstrate that these data sources have complementary\nsemantic aspects, supporting the creation of explicit semantic vector spaces.\nThe explicit vector spaces are evaluated using the task of discriminative\nattribute identification, showing comparable performance to the\nstate-of-the-art systems in the task (F1-score = 0.69), while delivering full\nmodel transparency and explainability.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 01:13:41 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Stepanjans", "Armins", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "1909.05364", "submitter": "Wu Xing", "authors": "Xing Wu, Dongjun Wei, Liangjun Zang, Jizhong Han and Songlin Hu", "title": "TransSent: Towards Generation of Structured Sentences with Discourse\n  Marker", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured sentences are important expressions in human writings and\ndialogues. Previous works on neural text generation fused semantic and\nstructural information by encoding the entire sentence into a mixed hidden\nrepresentation. However, when a generated sentence becomes complicated, the\nstructure is difficult to be properly maintained. To alleviate this problem, we\nexplicitly separate the modeling process of semantic and structural\ninformation. Intuitively, humans generate structured sentences by directly\nconnecting discourses with discourse markers (such as and, but, etc.).\nTherefore, we propose a task that mimics this process, called discourse\ntransfer. This task represents a structured sentence as (head discourse,\ndiscourse marker, tail discourse), and aims at tail discourse generation based\non head discourse and discourse marker. We also propose a corresponding model\ncalled TransSent, which interprets the relationship between two discourses as a\ntranslation1 from the head discourse to the tail discourse in the embedding\nspace. We experiment TransSent not only in discourse transfer task but also in\nfree text generation and dialogue generation tasks. Automatic and human\nevaluation results show that TransSent can generate structured sentences with\nhigh quality, and has certain scalability in different tasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 14:03:35 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 17:05:52 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 10:44:23 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Wu", "Xing", ""], ["Wei", "Dongjun", ""], ["Zang", "Liangjun", ""], ["Han", "Jizhong", ""], ["Hu", "Songlin", ""]]}, {"id": "1909.05365", "submitter": "Mingyang Zhou", "authors": "Mingyang Zhou, Josh Arnold, Zhou Yu", "title": "Building Task-Oriented Visual Dialog Systems Through Alternative\n  Optimization Between Dialog Policy and Language Generation", "comments": "updated with acknowledgement and minor typo fixes on tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) is an effective approach to learn an optimal\ndialog policy for task-oriented visual dialog systems. A common practice is to\napply RL on a neural sequence-to-sequence (seq2seq) framework with the action\nspace being the output vocabulary in the decoder. However, it is difficult to\ndesign a reward function that can achieve a balance between learning an\neffective policy and generating a natural dialog response. This paper proposes\na novel framework that alternatively trains a RL policy for image guessing and\na supervised seq2seq model to improve dialog generation quality. We evaluate\nour framework on the GuessWhich task and the framework achieves the\nstate-of-the-art performance in both task completion and dialog quality.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 01:28:34 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 19:45:17 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Zhou", "Mingyang", ""], ["Arnold", "Josh", ""], ["Yu", "Zhou", ""]]}, {"id": "1909.05378", "submitter": "Tao Yu", "authors": "Tao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria\n  Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\n  Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang,\n  Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter S Lasecki,\n  Dragomir Radev", "title": "CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain\n  Natural Language Interfaces to Databases", "comments": "Accepted to EMNLP 2019, long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CoSQL, a corpus for building cross-domain, general-purpose\ndatabase (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+\nannotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k\ndialogues querying 200 complex DBs spanning 138 domains. Each dialogue\nsimulates a real-world DB query scenario with a crowd worker as a user\nexploring the DB and a SQL expert retrieving answers with SQL, clarifying\nambiguous questions, or otherwise informing of unanswerable questions. When\nuser questions are answerable by SQL, the expert describes the SQL and\nexecution results to the user, hence maintaining a natural interaction flow.\nCoSQL introduces new challenges compared to existing task-oriented dialogue\ndatasets:(1) the dialogue states are grounded in SQL, a domain-independent\nexecutable representation, instead of domain-specific slot-value pairs, and (2)\nbecause testing is done on unseen databases, success requires generalizing to\nnew domains. CoSQL includes three tasks: SQL-grounded dialogue state tracking,\nresponse generation from query results, and user dialogue act prediction. We\nevaluate a set of strong baselines for each task and show that CoSQL presents\nsignificant challenges for future research. The dataset, baselines, and\nleaderboard will be released at https://yale-lily.github.io/cosql.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 21:15:47 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Yu", "Tao", ""], ["Zhang", "Rui", ""], ["Er", "He Yang", ""], ["Li", "Suyi", ""], ["Xue", "Eric", ""], ["Pang", "Bo", ""], ["Lin", "Xi Victoria", ""], ["Tan", "Yi Chern", ""], ["Shi", "Tianze", ""], ["Li", "Zihan", ""], ["Jiang", "Youxuan", ""], ["Yasunaga", "Michihiro", ""], ["Shim", "Sungrok", ""], ["Chen", "Tao", ""], ["Fabbri", "Alexander", ""], ["Li", "Zifan", ""], ["Chen", "Luyao", ""], ["Zhang", "Yuwen", ""], ["Dixit", "Shreya", ""], ["Zhang", "Vincent", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""], ["Lasecki", "Walter S", ""], ["Radev", "Dragomir", ""]]}, {"id": "1909.05393", "submitter": "Richard Jiang", "authors": "Tiancheng Xia, Richard Jiang, YongQing Fu and Nanlin Jin", "title": "Automated Blood Cell Detection and Counting via Deep Learning for\n  Microfluidic Point-of-Care Medical Devices", "comments": null, "journal-ref": "Proceeding of 2019 3rd International Conference on Artificial\n  Intelligence Applications and Technologies (AIAAT 2019)", "doi": "10.1088/1757-899X/646/1/012048", "report-no": null, "categories": "cs.CV cs.AI cs.ET cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated in-vitro cell detection and counting have been a key theme for\nartificial and intelligent biological analysis such as biopsy, drug analysis\nand decease diagnosis. Along with the rapid development of microfluidics and\nlab-on-chip technologies, in-vitro live cell analysis has been one of the\ncritical tasks for both research and industry communities. However, it is a\ngreat challenge to obtain and then predict the precise information of live\ncells from numerous microscopic videos and images. In this paper, we\ninvestigated in-vitro detection of white blood cells using deep neural\nnetworks, and discussed how state-of-the-art machine learning techniques could\nfulfil the needs of medical diagnosis. The approach we used in this study was\nbased on Faster Region-based Convolutional Neural Networks (Faster RCNNs), and\na transfer learning process was applied to apply this technique to the\nmicroscopic detection of blood cells. Our experimental results demonstrated\nthat fast and efficient analysis of blood cells via automated microscopic\nimaging can achieve much better accuracy and faster speed than the\nconventionally applied methods, implying a promising future of this technology\nto be applied to the microfluidic point-of-care medical devices.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 22:14:03 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Xia", "Tiancheng", ""], ["Jiang", "Richard", ""], ["Fu", "YongQing", ""], ["Jin", "Nanlin", ""]]}, {"id": "1909.05398", "submitter": "Matthew Hausknecht", "authors": "Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre C\\^ot\\'e,\n  Xingdi Yuan", "title": "Interactive Fiction Games: A Colossal Adventure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hallmark of human intelligence is the ability to understand and communicate\nwith language. Interactive Fiction games are fully text-based simulation\nenvironments where a player issues text commands to effect change in the\nenvironment and progress through the story. We argue that IF games are an\nexcellent testbed for studying language-based autonomous agents. In particular,\nIF games combine challenges of combinatorial action spaces, language\nunderstanding, and commonsense reasoning. To facilitate rapid development of\nlanguage-based agents, we introduce Jericho, a learning environment for\nman-made IF games and conduct a comprehensive study of text-agents across a\nrich set of games, highlighting directions in which agents can improve.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 22:41:00 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 18:59:52 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 19:36:33 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Hausknecht", "Matthew", ""], ["Ammanabrolu", "Prithviraj", ""], ["C\u00f4t\u00e9", "Marc-Alexandre", ""], ["Yuan", "Xingdi", ""]]}, {"id": "1909.05415", "submitter": "Samaneh Hoseini", "authors": "Samaneh Hosseini Semnani, Anton de Ruiter, Hugh Liu", "title": "Force-based Algorithm for Motion Planning of Large Agent Teams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a distributed, efficient, scalable and real-time motion\nplanning algorithm for a large group of agents moving in 2 or 3-dimensional\nspaces. This algorithm enables autonomous agents to generate individual\ntrajectories independently with only the relative position information of\nneighboring agents. Each agent applies a force-based control that contains two\nmain terms: collision avoidance and navigational feedback. The first term keeps\ntwo agents separate with a certain distance, while the second term attracts\neach agent toward its goal location. Compared with existing collision-avoidance\nalgorithms, the proposed force-based motion planning (FMP) algorithm is able to\nfind collision-free motions with lower transition time, free from velocity\nstate information of neighbouring agents. It leads to less computational\noverhead. The performance of proposed FMP is examined over several dense and\ncomplex 2D and 3D benchmark simulation scenarios, with results outperforming\nexisting methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 14:43:45 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Semnani", "Samaneh Hosseini", ""], ["de Ruiter", "Anton", ""], ["Liu", "Hugh", ""]]}, {"id": "1909.05416", "submitter": "Rebekka Burkholz", "authors": "Rebekka Burkholz, John Quackenbush", "title": "Cascade Size Distributions: Why They Matter and How to Compute Them\n  Efficiently", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascade models are central to understanding, predicting, and controlling\nepidemic spreading and information propagation. Related optimization, including\ninfluence maximization, model parameter inference, or the development of\nvaccination strategies, relies heavily on sampling from a model. This is either\ninefficient or inaccurate. As alternative, we present an efficient message\npassing algorithm that computes the probability distribution of the cascade\nsize for the Independent Cascade Model on weighted directed networks and\ngeneralizations. Our approach is exact on trees but can be applied to any\nnetwork topology. It approximates locally tree-like networks well, scales to\nlarge networks, and can lead to surprisingly good performance on more dense\nnetworks, as we also exemplify on real world data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 15:02:03 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 14:43:08 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Burkholz", "Rebekka", ""], ["Quackenbush", "John", ""]]}, {"id": "1909.05460", "submitter": "Vishnu Suresh Lokhande", "authors": "Vishnu Suresh Lokhande, Shaofei Wang, Maneesh Singh, Julian Yarkony", "title": "Accelerating Column Generation via Flexible Dual Optimal Inequalities\n  with Application to Entity Resolution", "comments": "Accepted at AAAI20. Version update. Update the link to project page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new optimization approach to Entity Resolution.\nTraditional approaches tackle entity resolution with hierarchical clustering,\nwhich does not benefit from a formal optimization formulation. In contrast, we\nmodel entity resolution as correlation-clustering, which we treat as a weighted\nset-packing problem and write as an integer linear program (ILP). In this case\nsources in the input data correspond to elements and entities in output data\ncorrespond to sets/clusters. We tackle optimization of weighted set packing by\nrelaxing integrality in our ILP formulation. The set of potential sets/clusters\ncan not be explicitly enumerated, thus motivating optimization via column\ngeneration. In addition to the novel formulation, we also introduce new dual\noptimal inequalities (DOI), that we call flexible dual optimal inequalities,\nwhich tightly lower-bound dual variables during optimization and accelerate\ncolumn generation. We apply our formulation to entity resolution (also called\nde-duplication of records), and achieve state-of-the-art accuracy on two\npopular benchmark datasets. The project page is available at the following url,\nhttps://github.com/lokhande-vishnu/EntityResolution\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 05:33:06 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 23:19:34 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 07:39:48 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Lokhande", "Vishnu Suresh", ""], ["Wang", "Shaofei", ""], ["Singh", "Maneesh", ""], ["Yarkony", "Julian", ""]]}, {"id": "1909.05477", "submitter": "Dexter Scobee", "authors": "Dexter R.R. Scobee and S. Shankar Sastry", "title": "Maximum Likelihood Constraint Inference for Inverse Reinforcement\n  Learning", "comments": "Published as a conference paper at the International Conference on\n  Learning Representations (ICLR), 2020 (at\n  https://openreview.net/forum?id=BJliakStvH )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most approaches to the problem of Inverse Reinforcement Learning (IRL)\nfocus on estimating a reward function that best explains an expert agent's\npolicy or demonstrated behavior on a control task, it is often the case that\nsuch behavior is more succinctly represented by a simple reward combined with a\nset of hard constraints. In this setting, the agent is attempting to maximize\ncumulative rewards subject to these given constraints on their behavior. We\nreformulate the problem of IRL on Markov Decision Processes (MDPs) such that,\ngiven a nominal model of the environment and a nominal reward function, we seek\nto estimate state, action, and feature constraints in the environment that\nmotivate an agent's behavior. Our approach is based on the Maximum Entropy IRL\nframework, which allows us to reason about the likelihood of an expert agent's\ndemonstrations given our knowledge of an MDP. Using our method, we can infer\nwhich constraints can be added to the MDP to most increase the likelihood of\nobserving these demonstrations. We present an algorithm which iteratively\ninfers the Maximum Likelihood Constraint to best explain observed behavior, and\nwe evaluate its efficacy using both simulated behavior and recorded data of\nhumans navigating around an obstacle.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:38:46 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 00:13:16 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Scobee", "Dexter R. R.", ""], ["Sastry", "S. Shankar", ""]]}, {"id": "1909.05479", "submitter": "Vishnu Suresh Lokhande", "authors": "Vishnu Suresh Lokhande, Songwong Tasneeyapant, Abhay Venkatesh, Sathya\n  N. Ravi and Vikas Singh", "title": "Generating Accurate Pseudo-labels in Semi-Supervised Learning and\n  Avoiding Overconfident Predictions via Hermite Polynomial Activations", "comments": "Accepted at 2020 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rectified Linear Units (ReLUs) are among the most widely used activation\nfunction in a broad variety of tasks in vision. Recent theoretical results\nsuggest that despite their excellent practical performance, in various cases, a\nsubstitution with basis expansions (e.g., polynomials) can yield significant\nbenefits from both the optimization and generalization perspective.\nUnfortunately, the existing results remain limited to networks with a couple of\nlayers, and the practical viability of these results is not yet known.\nMotivated by some of these results, we explore the use of Hermite polynomial\nexpansions as a substitute for ReLUs in deep networks. While our experiments\nwith supervised learning do not provide a clear verdict, we find that this\nstrategy offers considerable benefits in semi-supervised learning (SSL) /\ntransductive learning settings. We carefully develop this idea and show how the\nuse of Hermite polynomials based activations can yield improvements in\npseudo-label accuracies and sizable financial savings (due to concurrent\nruntime benefits). Further, we show via theoretical analysis, that the networks\n(with Hermite activations) offer robustness to noise and other attractive\nmathematical properties.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:42:08 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 06:01:54 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Lokhande", "Vishnu Suresh", ""], ["Tasneeyapant", "Songwong", ""], ["Venkatesh", "Abhay", ""], ["Ravi", "Sathya N.", ""], ["Singh", "Vikas", ""]]}, {"id": "1909.05508", "submitter": "Giuseppe Paolo Mr", "authors": "Giuseppe Paolo and Alban Laflaqui\\`ere and Alexandre Coninx and\n  Stephane Doncieux", "title": "Unsupervised Learning and Exploration of Reachable Outcome Space", "comments": "Published at IEEE International Conference on Robotics and Automation\n  (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing Reinforcement Learning in sparse rewards settings, with very\nlittle prior knowledge, is a challenging problem since there is no signal to\nproperly guide the learning process. In such situations, a good search strategy\nis fundamental. At the same time, not having to adapt the algorithm to every\nsingle problem is very desirable. Here we introduce TAXONS, a Task Agnostic\neXploration of Outcome spaces through Novelty and Surprise algorithm. Based on\na population-based divergent-search approach, it learns a set of diverse\npolicies directly from high-dimensional observations, without any task-specific\ninformation. TAXONS builds a repertoire of policies while training an\nautoencoder on the high-dimensional observation of the final state of the\nsystem to build a low-dimensional outcome space. The learned outcome space,\ncombined with the reconstruction error, is used to drive the search for new\npolicies. Results show that TAXONS can find a diverse set of controllers,\ncovering a good part of the ground-truth outcome space, while having no\ninformation about such space.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:47:44 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 12:34:35 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 18:03:22 GMT"}, {"version": "v4", "created": "Mon, 4 May 2020 09:20:08 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Paolo", "Giuseppe", ""], ["Laflaqui\u00e8re", "Alban", ""], ["Coninx", "Alexandre", ""], ["Doncieux", "Stephane", ""]]}, {"id": "1909.05527", "submitter": "J\\\"org Martin", "authors": "J\\\"org Martin, Clemens Elster", "title": "Inspecting adversarial examples using the Fisher information", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are slight perturbations that are designed to fool\nartificial neural networks when fed as an input. In this work the usability of\nthe Fisher information for the detection of such adversarial attacks is\nstudied. We discuss various quantities whose computation scales well with the\nnetwork size, study their behavior on adversarial examples and show how they\ncan highlight the importance of single input neurons, thereby providing a\nvisual tool for further analyzing (un-)reasonable behavior of a neural network.\nThe potential of our methods is demonstrated by applications to the MNIST,\nCIFAR10 and Fruits-360 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 09:26:30 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Martin", "J\u00f6rg", ""], ["Elster", "Clemens", ""]]}, {"id": "1909.05528", "submitter": "Youzhi Tian", "authors": "Weixin Liang, Youzhi Tian, Chengcai Chen, Zhou Yu", "title": "MOSS: End-to-End Dialog System Framework with Modular Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major bottleneck in training end-to-end task-oriented dialog system is the\nlack of data. To utilize limited training data more efficiently, we propose\nModular Supervision Network (MOSS), an encoder-decoder training framework that\ncould incorporate supervision from various intermediate dialog system modules\nincluding natural language understanding, dialog state tracking, dialog policy\nlearning, and natural language generation. With only 60% of the training data,\nMOSS-all (i.e., MOSS with supervision from all four dialog modules) outperforms\nstate-of-the-art models on CamRest676. Moreover, introducing modular\nsupervision has even bigger benefits when the dialog task has a more complex\ndialog state and action space. With only 40% of the training data, MOSS-all\noutperforms the state-of-the-art model on a complex laptop network\ntroubleshooting dataset, LaptopNetwork, that we introduced. LaptopNetwork\nconsists of conversations between real customers and customer service agents in\nChinese. Moreover, MOSS framework can accommodate dialogs that have supervision\nfrom different dialog modules at both the framework level and model level.\nTherefore, MOSS is extremely flexible to update in a real-world deployment.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 09:27:37 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Liang", "Weixin", ""], ["Tian", "Youzhi", ""], ["Chen", "Chengcai", ""], ["Yu", "Zhou", ""]]}, {"id": "1909.05546", "submitter": "Blai Bonet", "authors": "Blai Bonet, Hector Geffner", "title": "Learning First-Order Symbolic Representations for Planning from the\n  Structure of the State Space", "comments": "Proc. ECAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main obstacles for developing flexible AI systems is the split\nbetween data-based learners and model-based solvers. Solvers such as classical\nplanners are very flexible and can deal with a variety of problem instances and\ngoals but require first-order symbolic models. Data-based learners, on the\nother hand, are robust but do not produce such representations. In this work we\naddress this split by showing how the first-order symbolic representations that\nare used by planners can be learned from non-symbolic inputs that encode the\nstructure of the state space. The representation learning problem is formulated\nas the problem of inferring planning instances over a common but unknown\nfirst-order domain that account for the structure of the observed state space.\nThis means to infer a complete first-order representation (i.e. general action\nschemas, relational symbols, and objects) that explains the observed state\nspace structures. The inference problem is cast as a two-level combinatorial\nsearch where the outer level searches for values of a small set of\nhyperparameters and the inner level, solved via SAT, searches for a first-order\nsymbolic model. The framework is shown to produce general and correct\nfirst-order representations for standard problems like Gripper, Blocksworld,\nand Hanoi from input graphs that encode the flat state-space structure of a\nsingle instance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 10:13:08 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 23:11:44 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 15:45:42 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Bonet", "Blai", ""], ["Geffner", "Hector", ""]]}, {"id": "1909.05557", "submitter": "Yutian Chen", "authors": "Yutian Chen, Abram L. Friesen, Feryal Behbahani, Arnaud Doucet, David\n  Budden, Matthew W. Hoffman, Nando de Freitas", "title": "Modular Meta-Learning with Shrinkage", "comments": "Accepted by NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world problems, including multi-speaker text-to-speech synthesis,\ncan greatly benefit from the ability to meta-learn large models with only a few\ntask-specific components. Updating only these task-specific modules then allows\nthe model to be adapted to low-data tasks for as many steps as necessary\nwithout risking overfitting. Unfortunately, existing meta-learning methods\neither do not scale to long adaptation or else rely on handcrafted\ntask-specific architectures. Here, we propose a meta-learning approach that\nobviates the need for this often sub-optimal hand-selection. In particular, we\ndevelop general techniques based on Bayesian shrinkage to automatically\ndiscover and learn both task-specific and general reusable modules.\nEmpirically, we demonstrate that our method discovers a small set of meaningful\ntask-specific modules and outperforms existing meta-learning approaches in\ndomains like few-shot text-to-speech that have little task data and long\nadaptation horizons. We also show that existing meta-learning methods including\nMAML, iMAML, and Reptile emerge as special cases of our method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 10:40:13 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 11:08:35 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 20:37:23 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2020 16:45:20 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Chen", "Yutian", ""], ["Friesen", "Abram L.", ""], ["Behbahani", "Feryal", ""], ["Doucet", "Arnaud", ""], ["Budden", "David", ""], ["Hoffman", "Matthew W.", ""], ["de Freitas", "Nando", ""]]}, {"id": "1909.05608", "submitter": "Daniel Korat", "authors": "Oren Pereg, Daniel Korat, Moshe Wasserblat, Jonathan Mamou, Ido Dagan", "title": "ABSApp: A Portable Weakly-Supervised Aspect-Based Sentiment Extraction\n  System", "comments": "6 pages, demo paper at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ABSApp, a portable system for weakly-supervised aspect-based\nsentiment extraction. The system is interpretable and user friendly and does\nnot require labeled training data, hence can be rapidly and cost-effectively\nused across different domains in applied setups. The system flow includes three\nstages: First, it generates domain-specific aspect and opinion lexicons based\non an unlabeled dataset; second, it enables the user to view and edit those\nlexicons (weak supervision); and finally, it enables the user to select an\nunlabeled target dataset from the same domain, classify it, and generate an\naspect-based sentiment report. ABSApp has been successfully used in a number of\nreal-life use cases, among them movie review analysis and convention impact\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 12:50:34 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Pereg", "Oren", ""], ["Korat", "Daniel", ""], ["Wasserblat", "Moshe", ""], ["Mamou", "Jonathan", ""], ["Dagan", "Ido", ""]]}, {"id": "1909.05620", "submitter": "Wan-Yi Lin", "authors": "Govind Rathore and Wan-Yi Lin and Ji Eun Kim", "title": "DeepBbox: Accelerating Precise Ground Truth Generation for Autonomous\n  Driving Datasets", "comments": "accepted by ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving requires various computer vision algorithms, such as\nobject detection and tracking.Precisely-labeled datasets (i.e., objects are\nfully contained in bounding boxes with only a few extra pixels) are preferred\nfor training such algorithms, so that the algorithms can detect exact locations\nof the objects. However, it is very time-consuming and hence expensive to\ngenerate precise labels for image sequences at scale. In this paper, we propose\nDeepBbox, an algorithm that corrects loose object labels into right bounding\nboxes to reduce human annotation efforts. We use Cityscapes dataset to show\nannotation efficiency and accuracy improvement using DeepBbox. Experimental\nresults show that, with DeepBbox,we can increase the number of object edges\nthat are labeled automatically (within 1\\% error) by 50% to reduce manual\nannotation time.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 20:04:35 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Rathore", "Govind", ""], ["Lin", "Wan-Yi", ""], ["Kim", "Ji Eun", ""]]}, {"id": "1909.05628", "submitter": "Theophanes Raptis Mr", "authors": "T. E. Raptis", "title": "Hidden Structure in the Solutions Set of the N Queens Problem", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some preliminary results are reported on the equivalence of any n-queens\nproblem with the roots of a Boolean valued quadratic form via a generic\ndimensional reduction scheme. It is then proven that the solutions set is\nencoded in the entries of a special matrix. Further examination reveals a\ndirect association with pointwise Boolean fractal operators applied on certain\ninteger sequences associated with this matrix suggesting the presence of an\nunderlying special geometry of the solutions set.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 17:16:37 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Raptis", "T. E.", ""]]}, {"id": "1909.05663", "submitter": "Riccardo La Grassa", "authors": "Ignazio Gallo, Shah Nawaz, Alessandro Calefati, Riccardo La Grassa,\n  Nicola Landro", "title": "Picture What you Read", "comments": "7 pages, Dicta2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visualization refers to our ability to create an image in our head based on\nthe text we read or the words we hear. It is one of the many skills that makes\nreading comprehension possible. Convolutional Neural Networks (CNN) are an\nexcellent tool for recognizing and classifying text documents. In addition, it\ncan generate images conditioned on natural language. In this work, we utilize\nCNNs capabilities to generate realistic images representative of the text\nillustrating the semantic concept. We conducted various experiments to\nhighlight the capacity of the proposed model to generate representative images\nof the text descriptions used as input to the proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 11:26:35 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Gallo", "Ignazio", ""], ["Nawaz", "Shah", ""], ["Calefati", "Alessandro", ""], ["La Grassa", "Riccardo", ""], ["Landro", "Nicola", ""]]}, {"id": "1909.05664", "submitter": "Aly Magassouba", "authors": "Aly Magassouba and Komei Sugiura and Hisashi Kawai", "title": "Multimodal Attention Branch Network for Perspective-Free Sentence\n  Generation", "comments": "10 pages, 4 figures. Accepted for CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the automatic sentence generation of fetching\ninstructions for domestic service robots. Typical fetching commands such as\n\"bring me the yellow toy from the upper part of the white shelf\" includes\nreferring expressions, i.e., \"from the white upper part of the white shelf\". To\nsolve this task, we propose a multimodal attention branch network (Multi-ABN)\nwhich generates natural sentences in an end-to-end manner. Multi-ABN uses\nmultiple images of the same fixed scene to generate sentences that are not tied\nto a particular viewpoint. This approach combines a linguistic attention branch\nmechanism with several attention branch mechanisms. We evaluated our approach,\nwhich outperforms the state-of-the-art method on a standard metrics. Our method\nalso allows us to visualize the alignment between the linguistic input and the\nvisual features.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 01:10:24 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Magassouba", "Aly", ""], ["Sugiura", "Komei", ""], ["Kawai", "Hisashi", ""]]}, {"id": "1909.05665", "submitter": "Sangjae Bae", "authors": "Sangjae Bae, Dhruv Saxena, Alireza Nakhaei, Chiho Choi, Kikuo\n  Fujimura, and Scott Moura", "title": "Cooperation-Aware Lane Change Maneuver in Dense Traffic based on Model\n  Predictive Control with Recurrent Neural Network", "comments": "Submitted to 2020 American Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a real-time lane change control framework of autonomous\ndriving in dense traffic, which exploits cooperative behaviors of other\ndrivers. This paper focuses on heavy traffic where vehicles cannot change lanes\nwithout cooperating with other drivers. In this case, classical robust controls\nmay not apply since there is no safe area to merge to without interacting with\nthe other drivers. That said, modeling complex and interactive human behaviors\nis highly non-trivial from the perspective of control engineers. We propose a\nmathematical control framework based on Model Predictive Control (MPC)\nencompassing a state-of-the-art Recurrent Neural network (RNN) architecture. In\nparticular, RNN predicts interactive motions of other drivers in response to\npotential actions of the autonomous vehicle, which are then systematically\nevaluated in safety constraints. We also propose a real-time heuristic\nalgorithm to find locally optimal control inputs. Finally, quantitative and\nqualitative analysis on simulation studies are presented to illustrate the\nbenefits of the proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:01:36 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 06:34:25 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Bae", "Sangjae", ""], ["Saxena", "Dhruv", ""], ["Nakhaei", "Alireza", ""], ["Choi", "Chiho", ""], ["Fujimura", "Kikuo", ""], ["Moura", "Scott", ""]]}, {"id": "1909.05666", "submitter": "Yumeng Zhang", "authors": "Yumeng Zhang, Li Chen, Yufeng Liu, Junhai Yong, Wen Zheng", "title": "Adaptive Wasserstein Hourglass for Weakly Supervised Hand Pose\n  Estimation from Monocular RGB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insufficient labeled training datasets is one of the bottlenecks of 3D hand\npose estimation from monocular RGB images. Synthetic datasets have a large\nnumber of images with precise annotations, but the obvious difference with\nreal-world datasets impacts the generalization. Little work has been done to\nbridge the gap between two domains over their wide difference. In this paper,\nwe propose a domain adaptation method called Adaptive Wasserstein Hourglass (AW\nHourglass) for weakly-supervised 3D hand pose estimation, which aims to\ndistinguish the difference and explore the common characteristics (e.g. hand\nstructure) of synthetic and real-world datasets. Learning the common\ncharacteristics helps the network focus on pose-related information. The\nsimilarity of the characteristics makes it easier to enforce domain-invariant\nconstraints. During training, based on the relation between these common\ncharacteristics and 3D pose learned from fully-annotated synthetic datasets, it\nis beneficial for the network to restore the 3D pose of weakly labeled\nreal-world datasets with the aid of 2D annotations and depth images. While in\ntesting, the network predicts the 3D pose with the input of RGB.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 01:26:19 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Zhang", "Yumeng", ""], ["Chen", "Li", ""], ["Liu", "Yufeng", ""], ["Yong", "Junhai", ""], ["Zheng", "Wen", ""]]}, {"id": "1909.05675", "submitter": "Mostafa Elhoushi", "authors": "Mostafa Elhoushi, Ye Henry Tian, Zihao Chen, Farhan Shafiq, Joey Yiwei\n  Li", "title": "Accelerating Training using Tensor Decomposition", "comments": null, "journal-ref": "AAAI 2020 Artificial Intelligence of Things Workshop", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decomposition is one of the well-known approaches to reduce the\nlatency time and number of parameters of a pre-trained model. However, in this\npaper, we propose an approach to use tensor decomposition to reduce training\ntime of training a model from scratch. In our approach, we train the model from\nscratch (i.e., randomly initialized weights) with its original architecture for\na small number of epochs, then the model is decomposed, and then continue\ntraining the decomposed model till the end. There is an optional step in our\napproach to convert the decomposed architecture back to the original\narchitecture. We present results of using this approach on both CIFAR10 and\nImagenet datasets, and show that there can be upto 2x speed up in training time\nwith accuracy drop of upto 1.5% only, and in other cases no accuracy drop. This\ntraining acceleration approach is independent of hardware and is expected to\nhave similar speed ups on both CPU and GPU platforms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 21:15:46 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Elhoushi", "Mostafa", ""], ["Tian", "Ye Henry", ""], ["Chen", "Zihao", ""], ["Shafiq", "Farhan", ""], ["Li", "Joey Yiwei", ""]]}, {"id": "1909.05682", "submitter": "Jin Cao", "authors": "Huseyin Uzunalioglu, Jin Cao, Chitra Phadke, Gerald Lehmann, Ahmet\n  Akyamac, Ran He, Jeongran Lee, and Maria Able", "title": "Augmented Data Science: Towards Industrialization and Democratization of\n  Data Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversion of raw data into insights and knowledge requires substantial\namounts of effort from data scientists. Despite breathtaking advances in\nMachine Learning (ML) and Artificial Intelligence (AI), data scientists still\nspend the majority of their effort in understanding and then preparing the raw\ndata for ML/AI. The effort is often manual and ad hoc, and requires some level\nof domain knowledge. The complexity of the effort increases dramatically when\ndata diversity, both in form and context, increases. In this paper, we\nintroduce our solution, Augmented Data Science (ADS), towards addressing this\n\"human bottleneck\" in creating value from diverse datasets. ADS is a\ndata-driven approach and relies on statistics and ML to extract insights from\nany data set in a domain-agnostic way to facilitate the data science process.\nKey features of ADS are the replacement of rudimentary data exploration and\nprocessing steps with automation and the augmentation of data scientist\njudgment with automatically-generated insights. We present building blocks of\nour end-to-end solution and provide a case study to exemplify its capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 14:00:03 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Uzunalioglu", "Huseyin", ""], ["Cao", "Jin", ""], ["Phadke", "Chitra", ""], ["Lehmann", "Gerald", ""], ["Akyamac", "Ahmet", ""], ["He", "Ran", ""], ["Lee", "Jeongran", ""], ["Able", "Maria", ""]]}, {"id": "1909.05690", "submitter": "Kaili Wang", "authors": "Kaili Wang, Jose Oramas, Tinne Tuytelaars", "title": "In Defense of LSTMs for Addressing Multiple Instance Learning Problems", "comments": "accepted in ACCV 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTMs have a proven track record in analyzing sequential data. But what about\nunordered instance bags, as found under a Multiple Instance Learning (MIL)\nsetting? While not often used for this, we show LSTMs excell under this setting\ntoo. In addition, we show thatLSTMs are capable of indirectly capturing\ninstance-level information us-ing only bag-level annotations. Thus, they can be\nused to learn instance-level models in a weakly supervised manner. Our\nempirical evaluation on both simplified (MNIST) and realistic (Lookbook and\nHistopathology) datasets shows that LSTMs are competitive with or even surpass\nstate-of-the-art methods specially designed for handling specific MIL problems.\nMoreover, we show that their performance on instance-level prediction is close\nto that of fully-supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 09:14:08 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 09:19:30 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 20:56:13 GMT"}, {"version": "v4", "created": "Fri, 18 Sep 2020 09:46:33 GMT"}, {"version": "v5", "created": "Thu, 14 Jan 2021 09:56:52 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wang", "Kaili", ""], ["Oramas", "Jose", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1909.05693", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Zizhou Jia, Hui Chen, Leida Li, Guiguang Ding, Kurt\n  Keutzer", "title": "PDANet: Polarity-consistent Deep Attention Network for Fine-grained\n  Visual Emotion Regression", "comments": "Accepted by ACM Multimedia 2019", "journal-ref": null, "doi": "10.1145/3343031.3351062", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods on visual emotion analysis mainly focus on coarse-grained\nemotion classification, i.e. assigning an image with a dominant discrete\nemotion category. However, these methods cannot well reflect the complexity and\nsubtlety of emotions. In this paper, we study the fine-grained regression\nproblem of visual emotions based on convolutional neural networks (CNNs).\nSpecifically, we develop a Polarity-consistent Deep Attention Network (PDANet),\na novel network architecture that integrates attention into a CNN with an\nemotion polarity constraint. First, we propose to incorporate both spatial and\nchannel-wise attentions into a CNN for visual emotion regression, which jointly\nconsiders the local spatial connectivity patterns along each channel and the\ninterdependency between different channels. Second, we design a novel\nregression loss, i.e. polarity-consistent regression (PCR) loss, based on the\nweakly supervised emotion polarity to guide the attention generation. By\noptimizing the PCR loss, PDANet can generate a polarity preserved attention map\nand thus improve the emotion regression performance. Extensive experiments are\nconducted on the IAPS, NAPS, and EMOTIC datasets, and the results demonstrate\nthat the proposed PDANet outperforms the state-of-the-art approaches by a large\nmargin for fine-grained visual emotion regression. Our source code is released\nat: https://github.com/ZizhouJia/PDANet.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 05:16:36 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Zhao", "Sicheng", ""], ["Jia", "Zizhou", ""], ["Chen", "Hui", ""], ["Li", "Leida", ""], ["Ding", "Guiguang", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1909.05780", "submitter": "Yasumasa Onoe", "authors": "Yasumasa Onoe, Greg Durrett", "title": "Fine-Grained Entity Typing for Domain Independent Entity Linking", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural entity linking models are very powerful, but run the risk of\noverfitting to the domain they are trained in. For this problem, a domain is\ncharacterized not just by genre of text but even by factors as specific as the\nparticular distribution of entities, as neural models tend to overfit by\nmemorizing properties of frequent entities in a dataset. We tackle the problem\nof building robust entity linking models that generalize effectively and do not\nrely on labeled entity linking data with a specific entity distribution. Rather\nthan predicting entities directly, our approach models fine-grained entity\nproperties, which can help disambiguate between even closely related entities.\nWe derive a large inventory of types (tens of thousands) from Wikipedia\ncategories, and use hyperlinked mentions in Wikipedia to distantly label data\nand train an entity typing model. At test time, we classify a mention with this\ntyping model and use soft type predictions to link the mention to the most\nsimilar candidate entity. We evaluate our entity linking system on the\nCoNLL-YAGO dataset (Hoffart et al., 2011) and show that our approach\noutperforms prior domain-independent entity linking systems. We also test our\napproach in a harder setting derived from the WikilinksNED dataset (Eshel et\nal., 2017) where all the mention-entity pairs are unseen during test time.\nResults indicate that our approach generalizes better than a state-of-the-art\nneural model on the dataset.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 16:29:24 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 17:50:10 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Onoe", "Yasumasa", ""], ["Durrett", "Greg", ""]]}, {"id": "1909.05784", "submitter": "Ce Ju", "authors": "Dashan Gao, Ce Ju, Xiguang Wei, Yang Liu, Tianjian Chen and Qiang Yang", "title": "HHHFL: Hierarchical Heterogeneous Horizontal Federated Learning for\n  Electroencephalography", "comments": "5 pages, 6 figures, Accepted for International Workshop on Federated\n  Machine Learning for User Privacy and Data Confidentiality in Conjunction\n  with IJCAI 2019 (FL-IJCAI'2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) classification techniques have been widely\nstudied for human behavior and emotion recognition tasks. But it is still a\nchallenging issue since the data may vary from subject to subject, may change\nover time for the same subject, and maybe heterogeneous. Recent years,\nincreasing privacy-preserving demands poses new challenges to this task. The\ndata heterogeneity, as well as the privacy constraint of the EEG data, is not\nconcerned in previous studies. To fill this gap, in this paper, we propose a\nheterogeneous federated learning approach to train machine learning models over\nheterogeneous EEG data, while preserving the data privacy of each party. To\nverify the effectiveness of our approach, we conduct experiments on a\nreal-world EEG dataset, consisting of heterogeneous data collected from diverse\ndevices. Our approach achieves consistent performance improvement on every\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 06:29:23 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 11:34:17 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 03:40:35 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Gao", "Dashan", ""], ["Ju", "Ce", ""], ["Wei", "Xiguang", ""], ["Liu", "Yang", ""], ["Chen", "Tianjian", ""], ["Yang", "Qiang", ""]]}, {"id": "1909.05803", "submitter": "Yichen Jiang", "authors": "Yichen Jiang, Mohit Bansal", "title": "Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning", "comments": "11 pages (EMNLP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop QA requires a model to connect multiple pieces of evidence\nscattered in a long context to answer the question. The recently proposed\nHotpotQA (Yang et al., 2018) dataset is comprised of questions embodying four\ndifferent multi-hop reasoning paradigms (two bridge entity setups, checking\nmultiple properties, and comparing two entities), making it challenging for a\nsingle neural network to handle all four. In this work, we present an\ninterpretable, controller-based Self-Assembling Neural Modular Network (Hu et\nal., 2017, 2018) for multi-hop reasoning, where we design four novel modules\n(Find, Relocate, Compare, NoOp) to perform unique types of language reasoning.\nBased on a question, our layout controller RNN dynamically infers a series of\nreasoning modules to construct the entire network. Empirically, we show that\nour dynamic, multi-hop modular network achieves significant improvements over\nthe static, single-hop baseline (on both regular and adversarial evaluation).\nWe further demonstrate the interpretability of our model via three analyses.\nFirst, the controller can softly decompose the multi-hop question into multiple\nsingle-hop sub-questions to promote compositional reasoning behavior of the\nmain network. Second, the controller can predict layouts that conform to the\nlayouts designed by human experts. Finally, the intermediate module can infer\nthe entity that connects two distantly-located supporting facts by addressing\nthe sub-question from the controller.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 17:00:45 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 21:11:19 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Jiang", "Yichen", ""], ["Bansal", "Mohit", ""]]}, {"id": "1909.05815", "submitter": "Bowen Jing", "authors": "Bowen Jing and William Yin", "title": "Modeling Sensorimotor Coordination as Multi-Agent Reinforcement Learning\n  with Differentiable Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning has shown promise on a variety of\ncooperative tasks as a consequence of recent developments in differentiable\ninter-agent communication. However, most architectures are limited to pools of\nhomogeneous agents, limiting their applicability. Here we propose a modular\nframework for learning complex tasks in which a traditional monolithic agent is\nframed as a collection of cooperating heterogeneous agents. We apply this\napproach to model sensorimotor coordination in the neocortex as a multi-agent\nreinforcement learning problem. Our results demonstrate proof-of-concept of the\nproposed architecture and open new avenues for learning complex tasks and for\nunderstanding functional localization in the brain and future intelligent\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 17:20:15 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Jing", "Bowen", ""], ["Yin", "William", ""]]}, {"id": "1909.05829", "submitter": "Suraj Nair", "authors": "Suraj Nair, Chelsea Finn", "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks\n  via Visual Subgoal Generation", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video prediction models combined with planning algorithms have shown promise\nin enabling robots to learn to perform many vision-based tasks through only\nself-supervision, reaching novel goals in cluttered scenes with unseen objects.\nHowever, due to the compounding uncertainty in long horizon video prediction\nand poor scalability of sampling-based planning optimizers, one significant\nlimitation of these approaches is the ability to plan over long horizons to\nreach distant goals. To that end, we propose a framework for subgoal generation\nand planning, hierarchical visual foresight (HVF), which generates subgoal\nimages conditioned on a goal image, and uses them for planning. The subgoal\nimages are directly optimized to decompose the task into easy to plan segments,\nand as a result, we observe that the method naturally identifies semantically\nmeaningful states as subgoals. Across three out of four simulated vision-based\nmanipulation tasks, we find that our method achieves nearly a 200% performance\nimprovement over planning without subgoals and model-free RL approaches.\nFurther, our experiments illustrate that our approach extends to real,\ncluttered visual scenes. Project page:\nhttps://sites.google.com/stanford.edu/hvf\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 17:36:45 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Nair", "Suraj", ""], ["Finn", "Chelsea", ""]]}, {"id": "1909.05830", "submitter": "Jeffrey Li", "authors": "Jeffrey Li, Mikhail Khodak, Sebastian Caldas, Ameet Talwalkar", "title": "Differentially Private Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter-transfer is a well-known and versatile approach for meta-learning,\nwith applications including few-shot learning, federated learning, and\nreinforcement learning. However, parameter-transfer algorithms often require\nsharing models that have been trained on the samples from specific tasks, thus\nleaving the task-owners susceptible to breaches of privacy. We conduct the\nfirst formal study of privacy in this setting and formalize the notion of\ntask-global differential privacy as a practical relaxation of more commonly\nstudied threat models. We then propose a new differentially private algorithm\nfor gradient-based parameter transfer that not only satisfies this privacy\nrequirement but also retains provable transfer learning guarantees in convex\nsettings. Empirically, we apply our analysis to the problems of federated\nlearning with personalization and few-shot classification, showing that\nallowing the relaxation to task-global privacy from the more commonly studied\nnotion of local privacy leads to dramatically increased performance in\nrecurrent neural language modeling and image classification.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 17:37:08 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 17:08:10 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Li", "Jeffrey", ""], ["Khodak", "Mikhail", ""], ["Caldas", "Sebastian", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1909.05863", "submitter": "Ethan Perez", "authors": "Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe\n  Kiela, Kyunghyun Cho", "title": "Finding Generalizable Evidence by Learning to Convince Q&A Models", "comments": "EMNLP 2019. Code available at https://github.com/ethanjperez/convince", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a system that finds the strongest supporting evidence for a given\nanswer to a question, using passage-based question-answering (QA) as a testbed.\nWe train evidence agents to select the passage sentences that most convince a\npretrained QA model of a given answer, if the QA model received those sentences\ninstead of the full passage. Rather than finding evidence that convinces one\nmodel alone, we find that agents select evidence that generalizes; agent-chosen\nevidence increases the plausibility of the supported answer, as judged by other\nQA models and humans. Given its general nature, this approach improves QA in a\nrobust manner: using agent-selected evidence (i) humans can correctly answer\nquestions with only ~20% of the full passage and (ii) QA models can generalize\nto longer passages and harder questions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 18:00:00 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Perez", "Ethan", ""], ["Karamcheti", "Siddharth", ""], ["Fergus", "Rob", ""], ["Weston", "Jason", ""], ["Kiela", "Douwe", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1909.05912", "submitter": "Zhe Xu", "authors": "Zhe Xu, Ivan Gavran, Yousef Ahmad, Rupak Majumdar, Daniel Neider, Ufuk\n  Topcu and Bo Wu", "title": "Joint Inference of Reward Machines and Policies for Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating high-level knowledge is an effective way to expedite\nreinforcement learning (RL), especially for complex tasks with sparse rewards.\nWe investigate an RL problem where the high-level knowledge is in the form of\nreward machines, i.e., a type of Mealy machine that encodes the reward\nfunctions. We focus on a setting in which this knowledge is a priori not\navailable to the learning agent. We develop an iterative algorithm that\nperforms joint inference of reward machines and policies for RL (more\nspecifically, q-learning). In each iteration, the algorithm maintains a\nhypothesis reward machine and a sample of RL episodes. It derives q-functions\nfrom the current hypothesis reward machine, and performs RL to update the\nq-functions. While performing RL, the algorithm updates the sample by adding RL\nepisodes along which the obtained rewards are inconsistent with the rewards\nbased on the current hypothesis reward machine. In the next iteration, the\nalgorithm infers a new hypothesis reward machine from the updated sample. Based\non an equivalence relationship we defined between states of reward machines, we\ntransfer the q-functions between the hypothesis reward machines in consecutive\niterations. We prove that the proposed algorithm converges almost surely to an\noptimal policy in the limit if a minimal reward machine can be inferred and the\nmaximal length of each RL episode is sufficiently long. The experiments show\nthat learning high-level knowledge in the form of reward machines can lead to\nfast convergence to optimal policies in RL, while standard RL methods such as\nq-learning and hierarchical RL methods fail to converge to optimal policies\nafter a substantial number of training steps in many tasks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 19:09:13 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Xu", "Zhe", ""], ["Gavran", "Ivan", ""], ["Ahmad", "Yousef", ""], ["Majumdar", "Rupak", ""], ["Neider", "Daniel", ""], ["Topcu", "Ufuk", ""], ["Wu", "Bo", ""]]}, {"id": "1909.05950", "submitter": "Felix Leibfried", "authors": "Felix Leibfried and Jordi Grau-Moya", "title": "Mutual-Information Regularization in Markov Decision Processes and\n  Actor-Critic Learning", "comments": "Proceedings of the 3rd Conference on Robot Learning (CoRL), Osaka,\n  Japan, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cumulative entropy regularization introduces a regulatory signal to the\nreinforcement learning (RL) problem that encourages policies with high-entropy\nactions, which is equivalent to enforcing small deviations from a uniform\nreference marginal policy. This has been shown to improve exploration and\nrobustness, and it tackles the value overestimation problem. It also leads to a\nsignificant performance increase in tabular and high-dimensional settings, as\ndemonstrated via algorithms such as soft Q-learning (SQL) and soft actor-critic\n(SAC). Cumulative entropy regularization has been extended to optimize over the\nreference marginal policy instead of keeping it fixed, yielding a\nregularization that minimizes the mutual information between states and\nactions. While this has been initially proposed for Markov Decision Processes\n(MDPs) in tabular settings, it was recently shown that a similar principle\nleads to significant improvements over vanilla SQL in RL for high-dimensional\ndomains with discrete actions and function approximators.\n  Here, we follow the motivation of mutual-information regularization from an\ninference perspective and theoretically analyze the corresponding Bellman\noperator. Inspired by this Bellman operator, we devise a novel\nmutual-information regularized actor-critic learning (MIRACLE) algorithm for\ncontinuous action spaces that optimizes over the reference marginal policy. We\nempirically validate MIRACLE in the Mujoco robotics simulator, where we\ndemonstrate that it can compete with contemporary RL methods. Most notably, it\ncan improve over the model-free state-of-the-art SAC algorithm which implicitly\nassumes a fixed reference policy.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 16:43:25 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Leibfried", "Felix", ""], ["Grau-Moya", "Jordi", ""]]}, {"id": "1909.05964", "submitter": "Ashish Tiwari", "authors": "Sumit Gulwani and Kunal Pathak and Arjun Radhakrishna and Ashish\n  Tiwari and Abhishek Udupa", "title": "Quantitative Programming by Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming-by-Example (PBE) systems synthesize an intended program in some\n(relatively constrained) domain-specific language from a small number of\ninput-output examples provided by the user. In this paper, we motivate and\ndefine the problem of quantitative PBE (qPBE) that relates to synthesizing an\nintended program over an underlying (real world) programming language that also\nminimizes a given quantitative cost function. We present a modular approach for\nsolving qPBE that consists of three phases: intent disambiguation, global\nsearch, and local search. On two concrete objectives, namely program\nperformance and size, our qPBE procedure achieves $1.53 X$ and $1.26 X$\nimprovement respectively over the baseline FlashFill PBE system, averaged over\n$701$ benchmarks. Our detailed experiments validate the design of our procedure\nand show the value of combining global and local search for qPBE.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 21:55:00 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Gulwani", "Sumit", ""], ["Pathak", "Kunal", ""], ["Radhakrishna", "Arjun", ""], ["Tiwari", "Ashish", ""], ["Udupa", "Abhishek", ""]]}, {"id": "1909.06008", "submitter": "Zhao Kang", "authors": "Zhao Kang and Zipeng Guo and Shudong Huang and Siying Wang and Wenyu\n  Chen and Yuanzhang Su and Zenglin Xu", "title": "Multiple Partitions Aligned Clustering", "comments": "IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view clustering is an important yet challenging task due to the\ndifficulty of integrating the information from multiple representations. Most\nexisting multi-view clustering methods explore the heterogeneous information in\nthe space where the data points lie. Such common practice may cause significant\ninformation loss because of unavoidable noise or inconsistency among views.\nSince different views admit the same cluster structure, the natural space\nshould be all partitions. Orthogonal to existing techniques, in this paper, we\npropose to leverage the multi-view information by fusing partitions.\nSpecifically, we align each partition to form a consensus cluster indicator\nmatrix through a distinct rotation matrix. Moreover, a weight is assigned for\neach view to account for the clustering capacity differences of views. Finally,\nthe basic partitions, weights, and consensus clustering are jointly learned in\na unified framework. We demonstrate the effectiveness of our approach on\nseveral real datasets, where significant improvement is found over other\nstate-of-the-art multi-view clustering methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 02:45:13 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Kang", "Zhao", ""], ["Guo", "Zipeng", ""], ["Huang", "Shudong", ""], ["Wang", "Siying", ""], ["Chen", "Wenyu", ""], ["Su", "Yuanzhang", ""], ["Xu", "Zenglin", ""]]}, {"id": "1909.06017", "submitter": "George Leu", "authors": "George Leu and Jiangjun Tang", "title": "On educating machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine education is an emerging research field that focuses on the problem\nwhich is inverse to machine learning. To date, the literature on educating\nmachines is still in its infancy. A fairly low number of methodology and method\npapers are scattered throughout various formal and informal publication\navenues, mainly because the field is not yet well coalesced (with no well\nestablished discussion forums or investigation pathways), but also due to the\nbreadth of its potential ramifications and research directions. In this study\nwe bring together the existing literature and organise the discussion into a\nsmall number of research directions (out of many) which are to date\nsufficiently explored to form a minimal critical mass that can push the machine\neducation concept further towards a standalone research field status.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 03:39:53 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Leu", "George", ""], ["Tang", "Jiangjun", ""]]}, {"id": "1909.06019", "submitter": "Michael Katehakis", "authors": "Wesley Cowan, Michael N. Katehakis, Daniel Pirutinsky", "title": "Reinforcement Learning: a Comparison of UCB Versus Alternative Adaptive\n  Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the basic version of Reinforcement Learning (RL)\nthat involves computing optimal data driven (adaptive) policies for Markovian\ndecision process with unknown transition probabilities. We provide a brief\nsurvey of the state of the art of the area and we compare the performance of\nthe classic UCB policy of \\cc{bkmdp97} with a new policy developed herein which\nwe call MDP-Deterministic Minimum Empirical Divergence (MDP-DMED), and a method\nbased on Posterior sampling (MDP-PS).\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 03:43:19 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Cowan", "Wesley", ""], ["Katehakis", "Michael N.", ""], ["Pirutinsky", "Daniel", ""]]}, {"id": "1909.06030", "submitter": "Xiaoyang Huang", "authors": "Xiaoyang Huang and Jiancheng Yang and Linguo Li and Haoran Deng and\n  Bingbing Ni and Yi Xu", "title": "Evaluating and Boosting Uncertainty Quantification in Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergence of artificial intelligence techniques in biomedical applications\nurges the researchers to pay more attention on the uncertainty quantification\n(UQ) in machine-assisted medical decision making. For classification tasks,\nprior studies on UQ are difficult to compare with each other, due to the lack\nof a unified quantitative evaluation metric. Considering that well-performing\nUQ models ought to know when the classification models act incorrectly, we\ndesign a new evaluation metric, area under Confidence-Classification\nCharacteristic curves (AUCCC), to quantitatively evaluate the performance of\nthe UQ models. AUCCC is threshold-free, robust to perturbation, and insensitive\nto the classification performance. We evaluate several UQ methods (e.g., max\nsoftmax output) with AUCCC to validate its effectiveness. Furthermore, a simple\nscheme, named Uncertainty Distillation (UDist), is developed to boost the UQ\nperformance, where a confidence model is distilling the confidence estimated by\ndeep ensembles. The proposed method is easy to implement; it consistently\noutperforms strong baselines on natural and medical image datasets in our\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 04:37:39 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 09:34:49 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Huang", "Xiaoyang", ""], ["Yang", "Jiancheng", ""], ["Li", "Linguo", ""], ["Deng", "Haoran", ""], ["Ni", "Bingbing", ""], ["Xu", "Yi", ""]]}, {"id": "1909.06034", "submitter": "Tamir Blum", "authors": "Tamir Blum, William Jones and Kazuya Yoshida", "title": "Deep Learned Path Planning via Randomized Reward-Linked-Goals and\n  Potential Space Applications", "comments": "8 pages, 3 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space exploration missions have seen use of increasingly sophisticated\nrobotic systems with ever more autonomy. Deep learning promises to take this\neven a step further, and has applications for high-level tasks, like path\nplanning, as well as low-level tasks, like motion control, which are critical\ncomponents for mission efficiency and success. Using deep reinforcement\nend-to-end learning with randomized reward function parameters during training,\nwe teach a simulated 8 degree-of-freedom quadruped ant-like robot to travel\nanywhere within a perimeter, conducting path plan and motion control on a\nsingle neural network, without any system model or prior knowledge of the\nterrain or environment. Our approach also allows for user specified waypoints,\nwhich could translate well to either fully autonomous or\nsemi-autonomous/teleoperated space applications that encounter delay times. We\ntrained the agent using randomly generated waypoints linked to the reward\nfunction and passed waypoint coordinates as inputs to the neural network. Such\napplications show promise on a variety of space exploration robots, including\nhigh speed rovers for fast locomotion and legged cave robots for rough terrain.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 04:58:52 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Blum", "Tamir", ""], ["Jones", "William", ""], ["Yoshida", "Kazuya", ""]]}, {"id": "1909.06044", "submitter": "Haochen Liu", "authors": "Haochen Liu, Tyler Derr, Zitao Liu and Jiliang Tang", "title": "Say What I Want: Towards the Dark Side of Neural Dialogue Models", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural dialogue models have been widely adopted in various chatbot\napplications because of their good performance in simulating and generalizing\nhuman conversations. However, there exists a dark side of these models -- due\nto the vulnerability of neural networks, a neural dialogue model can be\nmanipulated by users to say what they want, which brings in concerns about the\nsecurity of practical chatbot services. In this work, we investigate whether we\ncan craft inputs that lead a well-trained black-box neural dialogue model to\ngenerate targeted outputs. We formulate this as a reinforcement learning (RL)\nproblem and train a Reverse Dialogue Generator which efficiently finds such\ninputs for targeted outputs. Experiments conducted on a representative neural\ndialogue model show that our proposed model is able to discover such desired\ninputs in a considerable portion of cases. Overall, our work reveals this\nweakness of neural dialogue models and may prompt further researches of\ndeveloping corresponding solutions to avoid it.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 05:50:50 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 16:12:10 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 00:43:28 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Liu", "Haochen", ""], ["Derr", "Tyler", ""], ["Liu", "Zitao", ""], ["Tang", "Jiliang", ""]]}, {"id": "1909.06092", "submitter": "Goran Glava\\v{s}", "authors": "Anne Lauscher, Goran Glava\\v{s}, Simone Paolo Ponzetto, Ivan Vuli\\'c", "title": "A General Framework for Implicit and Explicit Debiasing of\n  Distributional Word Vector Spaces", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Distributional word vectors have recently been shown to encode many of the\nhuman biases, most notably gender and racial biases, and models for attenuating\nsuch biases have consequently been proposed. However, existing models and\nstudies (1) operate on under-specified and mutually differing bias definitions,\n(2) are tailored for a particular bias (e.g., gender bias) and (3) have been\nevaluated inconsistently and non-rigorously. In this work, we introduce a\ngeneral framework for debiasing word embeddings. We operationalize the\ndefinition of a bias by discerning two types of bias specification: explicit\nand implicit. We then propose three debiasing models that operate on explicit\nor implicit bias specifications and that can be composed towards more robust\ndebiasing. Finally, we devise a full-fledged evaluation framework in which we\ncouple existing bias metrics with newly proposed ones. Experimental findings\nacross three embedding methods suggest that the proposed debiasing models are\nrobust and widely applicable: they often completely remove the bias both\nimplicitly and explicitly without degradation of semantic information encoded\nin any of the input distributional spaces. Moreover, we successfully transfer\ndebiasing models, by means of cross-lingual embedding spaces, and remove or\nattenuate biases in distributional word vector spaces of languages that lack\nreadily available bias specifications.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 08:57:14 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 17:22:06 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Lauscher", "Anne", ""], ["Glava\u0161", "Goran", ""], ["Ponzetto", "Simone Paolo", ""], ["Vuli\u0107", "Ivan", ""]]}, {"id": "1909.06133", "submitter": "Andrea Barraza", "authors": "Andrea Barraza-Urbina and Mathieu d'Aquin", "title": "Towards Sharing Task Environments to Support Reproducible Evaluations of\n  Interactive Recommender Systems", "comments": "Included in the Offline Evaluation for Recommender Systems Workshop\n  (REVEAL'19), collocated with ACM RecSys 2019. REVEAL'19, September 20th,\n  2019, Copenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond sharing datasets or simulations, we believe the Recommender Systems\n(RS) community should share Task Environments. In this work, we propose a\nhigh-level logical architecture that will help to reason about the core\ncomponents of a RS Task Environment, identify the differences between\nEnvironments, datasets and simulations; and most importantly, understand what\nneeds to be shared about Environments to achieve reproducible experiments. The\nwork presents itself as valuable initial groundwork, open to discussion and\nextensions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 10:52:30 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 09:57:55 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Barraza-Urbina", "Andrea", ""], ["d'Aquin", "Mathieu", ""]]}, {"id": "1909.06174", "submitter": "Christian Dondrup", "authors": "Christian Dondrup, Ioannis Papaioannou, Oliver Lemon", "title": "Petri Net Machines for Human-Agent Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/02", "categories": "cs.AI cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smart speakers and robots become ever more prevalent in our daily lives.\nThese agents are able to execute a wide range of tasks and actions and,\ntherefore, need systems to control their execution. Current state-of-the-art\nsuch as (deep) reinforcement learning, however, requires vast amounts of data\nfor training which is often hard to come by when interacting with humans. To\novercome this issue, most systems still rely on Finite State Machines. We\nintroduce Petri Net Machines which present a formal definition for state\nmachines based on Petri Nets that are able to execute concurrent actions\nreliably, execute and interleave several plans at the same time, and provide an\neasy to use modelling language. We show their workings based on the example of\nHuman-Robot Interaction in a shopping mall.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 12:31:39 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Dondrup", "Christian", ""], ["Papaioannou", "Ioannis", ""], ["Lemon", "Oliver", ""]]}, {"id": "1909.06200", "submitter": "Mengdi Zhu", "authors": "Mengdi Zhu, Zhiwei Yu, Xiaojun Wan", "title": "A Neural Approach to Irony Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ironies can not only express stronger emotions but also show a sense of\nhumor. With the development of social media, ironies are widely used in public.\nAlthough many prior research studies have been conducted in irony detection,\nfew studies focus on irony generation. The main challenges for irony generation\nare the lack of large-scale irony dataset and difficulties in modeling the\nironic pattern. In this work, we first systematically define irony generation\nbased on style transfer task. To address the lack of data, we make use of\ntwitter and build a large-scale dataset. We also design a combination of\nrewards for reinforcement learning to control the generation of ironic\nsentences. Experimental results demonstrate the effectiveness of our model in\nterms of irony accuracy, sentiment preservation, and content preservation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 13:05:27 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 03:56:02 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zhu", "Mengdi", ""], ["Yu", "Zhiwei", ""], ["Wan", "Xiaojun", ""]]}, {"id": "1909.06219", "submitter": "Simon Gottschalk", "authors": "Simon Gottschalk and Elena Demidova", "title": "HapPenIng: Happen, Predict, Infer -- Event Series Completion in a\n  Knowledge Graph", "comments": "ISWC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event series, such as the Wimbledon Championships and the US presidential\nelections, represent important happenings in key societal areas including\nsports, culture and politics. However, semantic reference sources, such as\nWikidata, DBpedia and EventKG knowledge graphs, provide only an incomplete\nevent series representation. In this paper we target the problem of event\nseries completion in a knowledge graph. We address two tasks: 1) prediction of\nsub-event relations, and 2) inference of real-world events that happened as a\npart of event series and are missing in the knowledge graph. To address these\nproblems, our proposed supervised HapPenIng approach leverages structural\nfeatures of event series. HapPenIng does not require any external knowledge -\nthe characteristics making it unique in the context of event inference. Our\nexperimental evaluation demonstrates that HapPenIng outperforms the baselines\nby 44 and 52 percentage points in terms of precision for the sub-event\nprediction and the inference tasks, correspondingly.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 16:28:56 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Gottschalk", "Simon", ""], ["Demidova", "Elena", ""]]}, {"id": "1909.06273", "submitter": "Martin Andrews", "authors": "Martin Andrews, Yew Ken Chia, Sam Witteveen", "title": "Scene Graph Parsing by Attention Graph", "comments": "Accepted paper for the ViGIL workshop at NeurIPS 2018. (4 pages +\n  references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graph representations, which form a graph of visual object nodes\ntogether with their attributes and relations, have proved useful across a\nvariety of vision and language applications. Recent work in the area has used\nNatural Language Processing dependency tree methods to automatically build\nscene graphs.\n  In this work, we present an 'Attention Graph' mechanism that can be trained\nend-to-end, and produces a scene graph structure that can be lifted directly\nfrom the top layer of a standard Transformer model.\n  The scene graphs generated by our model achieve an F-score similarity of\n52.21% to ground-truth graphs on the evaluation set using the SPICE metric,\nsurpassing the best previous approaches by 2.5%.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 14:54:37 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Andrews", "Martin", ""], ["Chia", "Yew Ken", ""], ["Witteveen", "Sam", ""]]}, {"id": "1909.06283", "submitter": "Prithviraj Ammanabrolu", "authors": "Prithviraj Ammanabrolu, William Broniec, Alex Mueller, Jeremy Paul,\n  Mark O. Riedl", "title": "Toward Automated Quest Generation in Text-Adventure Games", "comments": "In Proceedings of the International Conference on Computational\n  Creativity (ICCC-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive fictions, or text-adventures, are games in which a player\ninteracts with a world entirely through textual descriptions and text actions.\nText-adventure games are typically structured as puzzles or quests wherein the\nplayer must execute certain actions in a certain order to succeed. In this\npaper, we consider the problem of procedurally generating a quest, defined as a\nseries of actions required to progress towards a goal, in a text-adventure\ngame. Quest generation in text environments is challenging because they must be\nsemantically coherent. We present and evaluate two quest generation techniques:\n(1) a Markov model, and (2) a neural generative model. We specifically look at\ngenerating quests about cooking and train our models on recipe data. We\nevaluate our techniques with human participant studies looking at perceived\ncreativity and coherence.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 15:10:06 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 23:16:40 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 00:36:59 GMT"}, {"version": "v4", "created": "Wed, 19 Aug 2020 17:37:02 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Ammanabrolu", "Prithviraj", ""], ["Broniec", "William", ""], ["Mueller", "Alex", ""], ["Paul", "Jeremy", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1909.06293", "submitter": "Lucas Cassano", "authors": "Lucas Cassano and Ali H. Sayed", "title": "ISL: A novel approach for deep exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we explore an alternative approach to address deep\nexploration and we introduce the ISL algorithm, which is efficient at\nperforming deep exploration. Similarly to maximum entropy RL, we derive the\nalgorithm by augmenting the traditional RL objective with a novel\nregularization term. A distinctive feature of our approach is that, as opposed\nto other works that tackle the problem of deep exploration, in our derivation\nboth the learning equations and the exploration-exploitation strategy are\nderived in tandem as the solution to a well-posed optimization problem whose\nminimization leads to the optimal value function. Empirically we show that our\nmethod exhibits state of the art performance on a range of challenging\ndeep-exploration benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 15:28:09 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 21:15:14 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 10:12:19 GMT"}, {"version": "v4", "created": "Fri, 5 Jun 2020 15:10:00 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Cassano", "Lucas", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1909.06342", "submitter": "Umang Bhatt", "authors": "Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly,\n  Yunhan Jia, Joydeep Ghosh, Ruchir Puri, Jos\\'e M. F. Moura, Peter Eckersley", "title": "Explainable Machine Learning in Deployment", "comments": "ACM Conference on Fairness, Accountability, and Transparency 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable machine learning offers the potential to provide stakeholders\nwith insights into model behavior by using various methods such as feature\nimportance scores, counterfactual explanations, or influential training data.\nYet there is little understanding of how organizations use these methods in\npractice. This study explores how organizations view and use explainability for\nstakeholder consumption. We find that, currently, the majority of deployments\nare not for end users affected by the model but rather for machine learning\nengineers, who use explainability to debug the model itself. There is thus a\ngap between explainability in practice and the goal of transparency, since\nexplanations primarily serve internal stakeholders rather than external ones.\nOur study synthesizes the limitations of current explainability techniques that\nhamper their use for end users. To facilitate end user interaction, we develop\na framework for establishing clear goals for explainability. We end by\ndiscussing concerns raised regarding explainability.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 17:35:53 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 18:30:09 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 17:31:01 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 13:53:00 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Bhatt", "Umang", ""], ["Xiang", "Alice", ""], ["Sharma", "Shubham", ""], ["Weller", "Adrian", ""], ["Taly", "Ankur", ""], ["Jia", "Yunhan", ""], ["Ghosh", "Joydeep", ""], ["Puri", "Ruchir", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Eckersley", "Peter", ""]]}, {"id": "1909.06349", "submitter": "Vincent Chen", "authors": "Vincent S. Chen and Sen Wu and Zhenzhen Weng and Alexander Ratner and\n  Christopher R\\'e", "title": "Slice-based Learning: A Programming Model for Residual Learning in\n  Critical Data Slices", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world machine learning applications, data subsets correspond to\nespecially critical outcomes: vulnerable cyclist detections are safety-critical\nin an autonomous driving task, and \"question\" sentences might be important to a\ndialogue agent's language understanding for product purposes. While machine\nlearning models can achieve high quality performance on coarse-grained metrics\nlike F1-score and overall accuracy, they may underperform on critical\nsubsets---we define these as slices, the key abstraction in our approach. To\naddress slice-level performance, practitioners often train separate \"expert\"\nmodels on slice subsets or use multi-task hard parameter sharing. We propose\nSlice-based Learning, a new programming model in which the slicing function\n(SF), a programming interface, specifies critical data subsets for which the\nmodel should commit additional capacity. Any model can leverage SFs to learn\nslice expert representations, which are combined with an attention mechanism to\nmake slice-aware predictions. We show that our approach maintains a\nparameter-efficient representation while improving over baselines by up to 19.0\nF1 on slices and 4.6 F1 overall on datasets spanning language understanding\n(e.g. SuperGLUE), computer vision, and production-scale industrial systems.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 17:49:20 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 06:56:36 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Vincent S.", ""], ["Wu", "Sen", ""], ["Weng", "Zhenzhen", ""], ["Ratner", "Alexander", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1909.06356", "submitter": "Shiyue Zhang", "authors": "Shiyue Zhang, Mohit Bansal", "title": "Addressing Semantic Drift in Question Generation for Semi-Supervised\n  Question Answering", "comments": "15 pages (EMNLP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based Question Generation (QG) aims at generating natural and relevant\nquestions that can be answered by a given answer in some context. Existing QG\nmodels suffer from a \"semantic drift\" problem, i.e., the semantics of the\nmodel-generated question drifts away from the given context and answer. In this\npaper, we first propose two semantics-enhanced rewards obtained from downstream\nquestion paraphrasing and question answering tasks to regularize the QG model\nto generate semantically valid questions. Second, since the traditional\nevaluation metrics (e.g., BLEU) often fall short in evaluating the quality of\ngenerated questions, we propose a QA-based evaluation method which measures the\nQG model's ability to mimic human annotators in generating QA training data.\nExperiments show that our method achieves the new state-of-the-art performance\nw.r.t. traditional metrics, and also performs best on our QA-based evaluation\nmetrics. Further, we investigate how to use our QG model to augment QA datasets\nand enable semi-supervised QA. We propose two ways to generate synthetic QA\npairs: generate new questions from existing articles or collect QA pairs from\nnew articles. We also propose two empirically effective strategies, a data\nfilter and mixing mini-batch training, to properly use the QG-generated data\nfor QA. Experiments show that our method improves over both BiDAF and BERT QA\nbaselines, even without introducing new articles.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 17:59:03 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Zhang", "Shiyue", ""], ["Bansal", "Mohit", ""]]}, {"id": "1909.06414", "submitter": "Yilun Zhou", "authors": "Yilun Zhou, Julie A. Shah, Steven Schockaert", "title": "Learning Household Task Knowledge from WikiHow Descriptions", "comments": "IJCAI 2019 Workshop on Semantic Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense procedural knowledge is important for AI agents and robots that\noperate in a human environment. While previous attempts at constructing\nprocedural knowledge are mostly rule- and template-based, recent advances in\ndeep learning provide the possibility of acquiring such knowledge directly from\nnatural language sources. As a first step in this direction, we propose a model\nto learn embeddings for tasks, as well as the individual steps that need to be\ntaken to solve them, based on WikiHow articles. We learn these embeddings such\nthat they are predictive of both step relevance and step ordering. We also\nexperiment with the use of integer programming for inferring consistent global\nstep orderings from noisy pairwise predictions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 19:16:53 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zhou", "Yilun", ""], ["Shah", "Julie A.", ""], ["Schockaert", "Steven", ""]]}, {"id": "1909.06415", "submitter": "Jason Gregory", "authors": "Jason M. Gregory, Christopher Reardon, Kevin Lee, Geoffrey White, Ki\n  Ng, Caitlyn Sims", "title": "Enabling Intuitive Human-Robot Teaming Using Augmented Reality and\n  Gesture Control", "comments": "Proceedings of the Artificial Intelligence for Human-Robot\n  Interaction AAAI Symposium Series (AI-HRI 2019)", "journal-ref": "Proceedings of the Artificial Intelligence for Human-Robot\n  Interaction AAAI Symposium Series (AI-HRI 2019), Arlington, Virginia USA,\n  November 2019", "doi": null, "report-no": "AI-HRI/2019/31", "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-robot teaming offers great potential because of the opportunities to\ncombine strengths of heterogeneous agents. However, one of the critical\nchallenges in realizing an effective human-robot team is efficient information\nexchange - both from the human to the robot as well as from the robot to the\nhuman. In this work, we present and analyze an augmented reality-enabled,\ngesture-based system that supports intuitive human-robot teaming through\nimproved information exchange. Our proposed system requires no external\ninstrumentation aside from human-wearable devices and shows promise of\nreal-world applicability for service-oriented missions. Additionally, we\npresent preliminary results from a pilot study with human participants, and\nhighlight lessons learned and open research questions that may help direct\nfuture development, fielding, and experimentation of autonomous HRI systems.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 19:18:52 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gregory", "Jason M.", ""], ["Reardon", "Christopher", ""], ["Lee", "Kevin", ""], ["White", "Geoffrey", ""], ["Ng", "Ki", ""], ["Sims", "Caitlyn", ""]]}, {"id": "1909.06427", "submitter": "Richard Freedman", "authors": "Richard G. Freedman, Yi Ren Fung, Roman Ganchin, Shlomo Zilberstein", "title": "Responsive Planning and Recognition for Closed-Loop Interaction", "comments": "Accepted for presentation at the AAAI 2019 Fall Symposium Series, in\n  the symposium for Artificial Intelligence and Human-Robot Interaction for\n  Service Robots in Human Environments", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/24", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many intelligent systems currently interact with others using at least one of\nfixed communication inputs or preset responses, resulting in rigid interaction\nexperiences and extensive efforts developing a variety of scenarios for the\nsystem. Fixed inputs limit the natural behavior of the user in order to\neffectively communicate, and preset responses prevent the system from adapting\nto the current situation unless it was specifically implemented. Closed-loop\ninteraction instead focuses on dynamic responses that account for what the user\nis currently doing based on interpretations of their perceived activity. Agents\nemploying closed-loop interaction can also monitor their interactions to ensure\nthat the user responds as expected. We introduce a closed-loop interactive\nagent framework that integrates planning and recognition to predict what the\nuser is trying to accomplish and autonomously decide on actions to take in\nresponse to these predictions. Based on a recent demonstration of such an\nassistive interactive agent in a turn-based simulated game, we also discuss new\nresearch challenges that are not present in the areas of artificial\nintelligence planning or recognition alone.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 20:11:16 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Freedman", "Richard G.", ""], ["Fung", "Yi Ren", ""], ["Ganchin", "Roman", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1909.06480", "submitter": "Sarah Al-Hussaini", "authors": "Sarah Al-Hussaini, Jason M. Gregory, Shaurya Shriyam, Satyandra K.\n  Gupta", "title": "An Alert-Generation Framework for Improving Resiliency in\n  Human-Supervised, Multi-Agent Teams", "comments": null, "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/22", "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-supervision in multi-agent teams is a critical requirement to ensure\nthat the decision-maker's risk preferences are utilized to assign tasks to\nrobots. In stressful complex missions that pose risk to human health and life,\nsuch as humanitarian-assistance and disaster-relief missions, human mistakes or\ndelays in tasking robots can adversely affect the mission. To assist human\ndecision making in such missions, we present an alert-generation framework\ncapable of detecting various modes of potential failure or performance\ndegradation. We demonstrate that our framework, based on state machine\nsimulation and formal methods, offers probabilistic modeling to estimate the\nlikelihood of unfavorable events. We introduce smart simulation that offers a\ncomputationally-efficient way of detecting low-probability situations compared\nto standard Monte-Carlo simulations. Moreover, for certain class of problems,\nour inference-based method can provide guarantees on correctly detecting task\nfailures.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 22:39:08 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Al-Hussaini", "Sarah", ""], ["Gregory", "Jason M.", ""], ["Shriyam", "Shaurya", ""], ["Gupta", "Satyandra K.", ""]]}, {"id": "1909.06493", "submitter": "William Koch", "authors": "William Koch", "title": "Flight Controller Synthesis Via Deep Reinforcement Learning", "comments": "206 pages, PhD Dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traditional control methods are inadequate in many deployment settings\ninvolving control of Cyber-Physical Systems (CPS). In such settings, CPS\ncontrollers must operate and respond to unpredictable interactions, conditions,\nor failure modes. Dealing with such unpredictability requires the use of\nexecutive and cognitive control functions that allow for planning and\nreasoning. Motivated by the sport of drone racing, this dissertation addresses\nthese concerns for state-of-the-art flight control by investigating the use of\ndeep neural networks to bring essential elements of higher-level cognition for\nconstructing low level flight controllers.\n  This thesis reports on the development and release of an open source, full\nsolution stack for building neuro-flight controllers. This stack consists of\nthe methodology for constructing a multicopter digital twin for synthesize the\nflight controller unique to a specific aircraft, a tuning framework for\nimplementing training environments (GymFC), and a firmware for the world's\nfirst neural network supported flight controller (Neuroflight). GymFC's novel\napproach fuses together the digital twinning paradigm for flight control\ntraining to provide seamless transfer to hardware. Additionally, this thesis\nexamines alternative reward system functions as well as changes to the software\nenvironment to bridge the gap between the simulation and real world deployment\nenvironments.\n  Work summarized in this thesis demonstrates that reinforcement learning is\nable to be leveraged for training neural network controllers capable, not only\nof maintaining stable flight, but also precision aerobatic maneuvers in real\nworld settings. As such, this work provides a foundation for developing the\nnext generation of flight control systems.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 00:35:21 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Koch", "William", ""]]}, {"id": "1909.06510", "submitter": "Jason Wilson", "authors": "Jason R. Wilson, Seongsik Kim, Ulyana Kurylo, Joseph Cummings, Eshan\n  Tarneja", "title": "Developing Computational Models of Social Assistance to Guide Socially\n  Assistive Robots", "comments": "5 pages, AI-HRI, work in progress", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/23", "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there are many examples in which robots provide social assistance, a\nlack of theory on how the robots should decide how to assist impedes progress\nin realizing these technologies. To address this deficiency, we propose a pair\nof computational models to guide a robot as it provides social assistance. The\nmodel of social autonomy helps a robot select an appropriate assistance that\nwill help with the task at hand while also maintaining the autonomy of the\nperson being assisted. The model of social alliance describes how a to\ndetermine whether the robot and the person being assisted are cooperatively\nworking towards the same goal. Each of these models are rooted in social\nreasoning between people, and we describe here our ongoing work to adapt this\nsocial reasoning to human-robot interactions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 02:37:40 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Wilson", "Jason R.", ""], ["Kim", "Seongsik", ""], ["Kurylo", "Ulyana", ""], ["Cummings", "Joseph", ""], ["Tarneja", "Eshan", ""]]}, {"id": "1909.06529", "submitter": "Rishi Shah", "authors": "Rishi Shah, Yuqian Jiang, Haresh Karnan, Gilberto Briscoe-Martinez,\n  Dominick Mulder, Ryan Gupta, Rachel Schlossman, Marika Murphy, Justin W.\n  Hart, Luis Sentis, Peter Stone", "title": "Solving Service Robot Tasks: UT Austin Villa@Home 2019 Team Report", "comments": null, "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/26", "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RoboCup@Home is an international robotics competition based on domestic tasks\nrequiring autonomous capabilities pertaining to a large variety of AI\ntechnologies. Research challenges are motivated by these tasks both at the\nlevel of individual technologies and the integration of subsystems into a fully\nfunctional, robustly autonomous system. We describe the progress made by the UT\nAustin Villa 2019 RoboCup@Home team which represents a significant step forward\nin AI-based HRI due to the breadth of tasks accomplished within a unified\nsystem. Presented are the competition tasks, component technologies they rely\non, our initial approaches both to the components and their integration, and\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 04:39:14 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Shah", "Rishi", ""], ["Jiang", "Yuqian", ""], ["Karnan", "Haresh", ""], ["Briscoe-Martinez", "Gilberto", ""], ["Mulder", "Dominick", ""], ["Gupta", "Ryan", ""], ["Schlossman", "Rachel", ""], ["Murphy", "Marika", ""], ["Hart", "Justin W.", ""], ["Sentis", "Luis", ""], ["Stone", "Peter", ""]]}, {"id": "1909.06533", "submitter": "Jeffrey Krichmar", "authors": "Jinwei Xing, Xinyun Zou, Jeffrey L. Krichmar", "title": "Neuromodulated Patience for Robot and Self-Driving Vehicle Navigation", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robots and self-driving vehicles face a number of challenges when navigating\nthrough real environments. Successful navigation in dynamic environments\nrequires prioritizing subtasks and monitoring resources. Animals are under\nsimilar constraints. It has been shown that the neuromodulator serotonin\nregulates impulsiveness and patience in animals. In the present paper, we take\ninspiration from the serotonergic system and apply it to the task of robot\nnavigation. In a set of outdoor experiments, we show how changing the level of\npatience can affect the amount of time the robot will spend searching for a\ndesired location. To navigate GPS compromised environments, we introduce a deep\nreinforcement learning paradigm in which the robot learns to follow sidewalks.\nThis may further regulate a tradeoff between a smooth long route and a rough\nshorter route. Using patience as a parameter may be beneficial for autonomous\nsystems under time pressure.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 04:57:26 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Xing", "Jinwei", ""], ["Zou", "Xinyun", ""], ["Krichmar", "Jeffrey L.", ""]]}, {"id": "1909.06537", "submitter": "Mashrur Rashik", "authors": "Mashrur Rashik, Md. Musfiqur Rahman, Md. Mamun-or-Rashid, Md. Mosaddek\n  Khan", "title": "Speeding Up Distributed Pseudo-tree Optimization Procedure with Cross\n  Edge Consistency to Solve DCOPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Pseudo-tree Optimization Procedure (DPOP) is a well-known message\npassing algorithm that has been used to provide optimal solutions of\nDistributed Constraint Optimization Problems (DCOPs) -- a framework that is\ndesigned to optimize constraints in cooperative multi-agent systems. The\ntraditional DCOP formulation does not consider those constraints that must be\nsatisfied (also known as hard constraints), rather it concentrates only on soft\nconstraints. However, the presence of both types of constraints are observed in\na number of applications, such as Distributed Radio Link Frequency Assignment\nand Distributed Event Scheduling, etc. Although the combination of these types\nof constraints is recently incorporated in DPOP to solve DCOPs, scalability\nremains an issue for them as finding an optimal solution is NP-hard.\nAdditionally, in DPOP, the agents are arranged as a DFS pseudo-tree. Recently\nit has been observed that the constructed pseudo-trees in this way often come\nto be chain-like and greatly impair the algorithm's performance. To address\nthese issues, we develop an algorithm that speeds up the DPOP algorithm by\nreducing the size of the messages exchanged and increasing parallelism in the\npseudo tree. Our empirical evidence suggests that our approach outperforms the\nstate-of-the-art algorithms by a significant margin.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 05:38:50 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Rashik", "Mashrur", ""], ["Rahman", "Md. Musfiqur", ""], ["Mamun-or-Rashid", "Md.", ""], ["Khan", "Md. Mosaddek", ""]]}, {"id": "1909.06673", "submitter": "Petr Savick\\'y", "authors": "Petr Ku\\v{c}era, Petr Savick\\'y", "title": "Propagation complete encodings of smooth DNNF theories", "comments": "29 pages, correction of the example in Section 2.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate conjunctive normal form (CNF) encodings of a function\nrepresented with a smooth decomposable negation normal form (DNNF). Several\nencodings of DNNFs and decision diagrams were considered by (Abio et al. 2016).\nThe authors differentiate between encodings which implement consistency or\ndomain consistency from encodings which implement unit refutation completeness\nor propagation completeness (in both cases implements means by unit\npropagation). The difference is that in the former case we do not care about\nproperties of the encoding with respect to the auxiliary variables while in the\nlatter case we treat all variables (the input ones and the auxiliary ones) in\nthe same way. The latter case is useful if a DNNF is a part of a problem\ncontaining also other constraints and a SAT solver is used to test\nsatisfiability. The currently known encodings of smooth DNNF theories implement\ndomain consistency. Building on this and the result of (Abio et al. 2016) on an\nencoding of decision diagrams which implements propagation completeness, we\npresent a new encoding of a smooth DNNF which implements propagation\ncompleteness. This closes the gap left open in the literature on encodings of\nDNNFs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 20:43:41 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 10:40:55 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Ku\u010dera", "Petr", ""], ["Savick\u00fd", "Petr", ""]]}, {"id": "1909.06674", "submitter": "Edward Raff", "authors": "Edward Raff", "title": "A Step Toward Quantifying Independently Reproducible Machine Learning\n  Research", "comments": "to appear in Proc. Neural Information Processing Systems (NeurIPS),\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What makes a paper independently reproducible? Debates on reproducibility\ncenter around intuition or assumptions but lack empirical results. Our field\nfocuses on releasing code, which is important, but is not sufficient for\ndetermining reproducibility. We take the first step toward a quantifiable\nanswer by manually attempting to implement 255 papers published from 1984 until\n2017, recording features of each paper, and performing statistical analysis of\nthe results. For each paper, we did not look at the authors code, if released,\nin order to prevent bias toward discrepancies between code and paper.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 20:53:16 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Raff", "Edward", ""]]}, {"id": "1909.06675", "submitter": "James Boerkoel Jr.", "authors": "Seth Isaacson and Gretchen Rice and James C. Boerkoel Jr", "title": "MAD-TN: A Tool for Measuring Fluency in Human-Robot Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/03", "categories": "cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluency is an important metric in Human-Robot Interaction (HRI) that\ndescribes the coordination with which humans and robots collaborate on a task.\nFluency is inherently linked to the timing of the task, making temporal\nconstraint networks a promising way to model and measure fluency. We show that\nthe Multi-Agent Daisy Temporal Network (MAD-TN) formulation, which expands on\nan existing concept of daisy-structured networks, is both an effective model of\nhuman-robot collaboration and a natural way to measure a number of existing\nfluency metrics. The MAD-TN model highlights new metrics that we hypothesize\nwill strongly correlate with human teammates' perception of fluency.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 21:00:48 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 16:41:40 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 00:43:09 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Isaacson", "Seth", ""], ["Rice", "Gretchen", ""], ["Boerkoel", "James C.", "Jr"]]}, {"id": "1909.06682", "submitter": "Alexander Clegg Ph.D.", "authors": "Alexander Clegg, Zackory Erickson, Patrick Grady, Greg Turk, Charles\n  C. Kemp, and C. Karen Liu", "title": "Learning to Collaborate from Simulation for Robot-Assisted Dressing", "comments": "8 pages, 8 figures, 3 tables; simulation to reality experiment added\n  to evaluation; authors added; modified: title, abstract, conclusion,\n  references; figure added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigated the application of haptic feedback control and deep\nreinforcement learning (DRL) to robot-assisted dressing. Our method uses DRL to\nsimultaneously train human and robot control policies as separate neural\nnetworks using physics simulations. In addition, we modeled variations in human\nimpairments relevant to dressing, including unilateral muscle weakness,\ninvoluntary arm motion, and limited range of motion. Our approach resulted in\ncontrol policies that successfully collaborate in a variety of simulated\ndressing tasks involving a hospital gown and a T-shirt. In addition, our\napproach resulted in policies trained in simulation that enabled a real PR2\nrobot to dress the arm of a humanoid robot with a hospital gown. We found that\ntraining policies for specific impairments dramatically improved performance;\nthat controller execution speed could be scaled after training to reduce the\nrobot's speed without steep reductions in performance; that curriculum learning\ncould be used to lower applied forces; and that multi-modal sensing, including\na simulated capacitive sensor, improved performance.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 21:25:38 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 20:10:03 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Clegg", "Alexander", ""], ["Erickson", "Zackory", ""], ["Grady", "Patrick", ""], ["Turk", "Greg", ""], ["Kemp", "Charles C.", ""], ["Liu", "C. Karen", ""]]}, {"id": "1909.06710", "submitter": "Dhruv Mauria Saxena", "authors": "Dhruv Mauria Saxena, Sangjae Bae, Alireza Nakhaei, Kikuo Fujimura,\n  Maxim Likhachev", "title": "Driving in Dense Traffic with Model-Free Reinforcement Learning", "comments": "Proceedings of the IEEE International Conference on Robotics and\n  Automation (ICRA), 2020. Updated Github repository links", "journal-ref": null, "doi": "10.1109/ICRA40945.2020.9197132", "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional planning and control methods could fail to find a feasible\ntrajectory for an autonomous vehicle to execute amongst dense traffic on roads.\nThis is because the obstacle-free volume in spacetime is very small in these\nscenarios for the vehicle to drive through. However, that does not mean the\ntask is infeasible since human drivers are known to be able to drive amongst\ndense traffic by leveraging the cooperativeness of other drivers to open a gap.\nThe traditional methods fail to take into account the fact that the actions\ntaken by an agent affect the behaviour of other vehicles on the road. In this\nwork, we rely on the ability of deep reinforcement learning to implicitly model\nsuch interactions and learn a continuous control policy over the action space\nof an autonomous vehicle. The application we consider requires our agent to\nnegotiate and open a gap in the road in order to successfully merge or change\nlanes. Our policy learns to repeatedly probe into the target road lane while\ntrying to find a safe spot to move in to. We compare against two\nmodel-predictive control-based algorithms and show that our policy outperforms\nthem in simulation.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 01:59:10 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 16:50:59 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Saxena", "Dhruv Mauria", ""], ["Bae", "Sangjae", ""], ["Nakhaei", "Alireza", ""], ["Fujimura", "Kikuo", ""], ["Likhachev", "Maxim", ""]]}, {"id": "1909.06749", "submitter": "Mary Ellen Foster", "authors": "Mary Ellen Foster (1), Bart Craenen (1), Amol Deshmukh (1), Oliver\n  Lemon (2), Emanuele Bastianelli (2), Christian Dondrup (2), Ioannis\n  Papaioannou (2), Andrea Vanzo (2), Jean-Marc Odobez (3), Olivier Can\\'evet\n  (3), Yuanzhouhan Cao (3), Weipeng He (3), Angel Mart\\'inez-Gonz\\'alez (3),\n  Petr Motlicek (3), R\\'emy Siegfried (3), Rachid Alami (4), Kathleen\n  Belhassein (4), Guilhem Buisan (4), Aur\\'elie Clodic (4), Amandine Mayima\n  (4), Yoan Sallami (4), Guillaume Sarthou (4), Phani-Teja Singamaneni (4),\n  Jules Waldhart (4), Alexandre Mazel (5), Maxime Caniot (5), Marketta\n  Niemel\\\"a (6), P\\\"aivi Heikkil\\\"a (6), Hanna Lammi (6), Antti Tammela (6)\n  ((1) University of Glasgow, (2) Heriot-Watt University, (3) Idiap Research\n  Institute, (4) LAAS-CNRS, (5) SoftBank Robotics Europe, (6) VTT Technical\n  Research Centre of Finland)", "title": "MuMMER: Socially Intelligent Human-Robot Interaction in Public Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/14", "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the EU-funded MuMMER project, we have developed a social robot designed to\ninteract naturally and flexibly with users in public spaces such as a shopping\nmall. We present the latest version of the robot system developed during the\nproject. This system encompasses audio-visual sensing, social signal\nprocessing, conversational interaction, perspective taking, geometric\nreasoning, and motion planning. It successfully combines all these components\nin an overarching framework using the Robot Operating System (ROS) and has been\ndeployed to a shopping mall in Finland interacting with customers. In this\npaper, we describe the system components, their interplay, and the resulting\nrobot behaviours and scenarios provided at the shopping mall.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 06:54:59 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Foster", "Mary Ellen", ""], ["Craenen", "Bart", ""], ["Deshmukh", "Amol", ""], ["Lemon", "Oliver", ""], ["Bastianelli", "Emanuele", ""], ["Dondrup", "Christian", ""], ["Papaioannou", "Ioannis", ""], ["Vanzo", "Andrea", ""], ["Odobez", "Jean-Marc", ""], ["Can\u00e9vet", "Olivier", ""], ["Cao", "Yuanzhouhan", ""], ["He", "Weipeng", ""], ["Mart\u00ednez-Gonz\u00e1lez", "Angel", ""], ["Motlicek", "Petr", ""], ["Siegfried", "R\u00e9my", ""], ["Alami", "Rachid", ""], ["Belhassein", "Kathleen", ""], ["Buisan", "Guilhem", ""], ["Clodic", "Aur\u00e9lie", ""], ["Mayima", "Amandine", ""], ["Sallami", "Yoan", ""], ["Sarthou", "Guillaume", ""], ["Singamaneni", "Phani-Teja", ""], ["Waldhart", "Jules", ""], ["Mazel", "Alexandre", ""], ["Caniot", "Maxime", ""], ["Niemel\u00e4", "Marketta", ""], ["Heikkil\u00e4", "P\u00e4ivi", ""], ["Lammi", "Hanna", ""], ["Tammela", "Antti", ""]]}, {"id": "1909.06904", "submitter": "Steve DiPaola", "authors": "Vanessa Utz and Steve DiPaola", "title": "Using an AI creativity system to explore how aesthetic experiences are\n  processed along the brains perceptual neural pathways", "comments": null, "journal-ref": "Cognitive Systems Research, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased sophistication of AI techniques, the application of these\nsystems has been expanding to ever newer fields. Increasingly, these systems\nare being used in modeling of human aesthetics and creativity, e.g. how humans\ncreate artworks and design products. Our lab has developed one such AI\ncreativity deep learning system that can be used to create artworks in the form\nof images and videos. In this paper, we describe this system and its use in\nstudying the human visual system and the formation of aesthetic experiences.\nSpecifically, we show how time-based AI created media can be used to explore\nthe nature of the dual-pathway neuro-architecture of the human visual system\nand how this relates to higher cognitive judgments such as aesthetic\nexperiences that rely on these divergent information streams. We propose a\ntheoretical framework for how the movement within percepts such as video clips,\ncauses the engagement of reflexive attention and a subsequent focus on visual\ninformation that are primarily processed via the dorsal stream, thereby\nmodulating aesthetic experiences that rely on information relayed via the\nventral stream. We outline our recent study in support of our proposed\nframework, which serves as the first study that investigates the relationship\nbetween the two visual streams and aesthetic experiences.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 22:49:41 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Utz", "Vanessa", ""], ["DiPaola", "Steve", ""]]}, {"id": "1909.06907", "submitter": "Arjun Akula", "authors": "Arjun R. Akula, Changsong Liu, Sari Saba-Sadiya, Hongjing Lu, Sinisa\n  Todorovic, Joyce Y. Chai, Song-Chun Zhu", "title": "X-ToM: Explaining with Theory-of-Mind for Gaining Justified Human Trust", "comments": "A short version of this was presented at CVPR 2019 Workshop on\n  Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new explainable AI (XAI) framework aimed at increasing justified\nhuman trust and reliance in the AI machine through explanations. We pose\nexplanation as an iterative communication process, i.e. dialog, between the\nmachine and human user. More concretely, the machine generates sequence of\nexplanations in a dialog which takes into account three important aspects at\neach dialog turn: (a) human's intention (or curiosity); (b) human's\nunderstanding of the machine; and (c) machine's understanding of the human\nuser. To do this, we use Theory of Mind (ToM) which helps us in explicitly\nmodeling human's intention, machine's mind as inferred by the human as well as\nhuman's mind as inferred by the machine. In other words, these explicit mental\nrepresentations in ToM are incorporated to learn an optimal explanation policy\nthat takes into account human's perception and beliefs. Furthermore, we also\nshow that ToM facilitates in quantitatively measuring justified human trust in\nthe machine by comparing all the three mental representations.\n  We applied our framework to three visual recognition tasks, namely, image\nclassification, action recognition, and human body pose estimation. We argue\nthat our ToM based explanations are practical and more natural for both expert\nand non-expert users to understand the internal workings of complex machine\nlearning models. To the best of our knowledge, this is the first work to derive\nexplanations using ToM. Extensive human study experiments verify our\nhypotheses, showing that the proposed explanations significantly outperform the\nstate-of-the-art XAI methods in terms of all the standard quantitative and\nqualitative XAI evaluation metrics including human trust, reliance, and\nexplanation satisfaction.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 23:24:32 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Akula", "Arjun R.", ""], ["Liu", "Changsong", ""], ["Saba-Sadiya", "Sari", ""], ["Lu", "Hongjing", ""], ["Todorovic", "Sinisa", ""], ["Chai", "Joyce Y.", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1909.06927", "submitter": "Ece Calikus", "authors": "Ece Calikus, Slawomir Nowaczyk, Anita Sant'Anna and Onur Dikmen", "title": "No Free Lunch But A Cheaper Supper: A General Framework for Streaming\n  Anomaly Detection", "comments": "Submitted to Expert Systems with Applications", "journal-ref": null, "doi": "10.1016/j.eswa.2020.113453", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been increased research interest in detecting\nanomalies in temporal streaming data. A variety of algorithms have been\ndeveloped in the data mining community, which can be divided into two\ncategories (i.e., general and ad hoc). In most cases, general approaches assume\nthe one-size-fits-all solution model where a single anomaly detector can detect\nall anomalies in any domain. To date, there exists no single general method\nthat has been shown to outperform the others across different anomaly types,\nuse cases and datasets. In this paper, we propose SAFARI, a general framework\nformulated by abstracting and unifying the fundamental tasks in streaming\nanomaly detection, which provides a flexible and extensible anomaly detection\nprocedure to overcome the limitations of one-size-fits-all solutions. SAFARI\nhelps to facilitate more elaborate algorithm comparisons by allowing us to\nisolate the effects of shared and unique characteristics of different\nalgorithms on detection performance. Using SAFARI, we have implemented various\nanomaly detectors and identified a research gap that motivates us to propose a\nnovel learning strategy in this work. We conducted an extensive evaluation\nstudy of 20 detectors that are composed using SAFARI and compared their\nperformances using real-world benchmark datasets with different properties. The\nresults indicate that there is no single superior detector that works well for\nevery case, proving our hypothesis that \"there is no free lunch\" in the\nstreaming anomaly detection world. Finally, we discuss the benefits and\ndrawbacks of each method in-depth and draw a set of conclusions to guide future\nusers of SAFARI.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 01:40:02 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 12:38:15 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 17:06:44 GMT"}, {"version": "v4", "created": "Sat, 18 Apr 2020 16:46:44 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Calikus", "Ece", ""], ["Nowaczyk", "Slawomir", ""], ["Sant'Anna", "Anita", ""], ["Dikmen", "Onur", ""]]}, {"id": "1909.06965", "submitter": "Nikos Ar\\'echiga PhD", "authors": "Nikos Arechiga, Jonathan DeCastro, Soonho Kong, Karen Leung", "title": "Better AI through Logical Scaffolding", "comments": "CAV Workshop on Formal Methods for ML-enabled Autonomous Systems 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the concept of logical scaffolds, which can be used to improve\nthe quality of software that relies on AI components. We explain how some of\nthe existing ideas on runtime monitors for perception systems can be seen as a\nspecific instance of logical scaffolds. Furthermore, we describe how logical\nscaffolds may be useful for improving AI programs beyond perception systems, to\ninclude general prediction systems and agent behavior models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 05:41:25 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Arechiga", "Nikos", ""], ["DeCastro", "Jonathan", ""], ["Kong", "Soonho", ""], ["Leung", "Karen", ""]]}, {"id": "1909.06983", "submitter": "Fang Liu", "authors": "Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin", "title": "A Self-Attentional Neural Architecture for Code Completion with\n  Multi-Task Learning", "comments": "Accepted by International Conference on Program Comprehension (ICPC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code completion, one of the most useful features in the Integrated\nDevelopment Environments (IDEs), can accelerate software development by\nsuggesting the libraries, APIs, and method names in real-time. Recent studies\nhave shown that statistical language models can improve the performance of code\ncompletion tools through learning from large-scale software repositories.\nHowever, these models suffer from three major drawbacks: a) The hierarchical\nstructural information of the programs is not fully utilized in the program's\nrepresentation; b) In programs, the semantic relationships can be very long.\nExisting recurrent neural networks based language models are not sufficient to\nmodel the long-term dependency. c) Existing approaches perform a specific task\nin one model, which leads to the underuse of the information from related\ntasks. To address these challenges, in this paper, we propose a\nself-attentional neural architecture for code completion with multi-task\nlearning. To utilize the hierarchical structural information of the programs,\nwe present a novel method that considers the path from the predicting node to\nthe root node. To capture the long-term dependency in the input programs, we\nadopt a self-attentional architecture based network as the base language model.\nTo enable the knowledge sharing between related tasks, we creatively propose a\nMulti-Task Learning (MTL) framework to learn two related tasks in code\ncompletion jointly. Experiments on three real-world datasets demonstrate the\neffectiveness of our model when compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 04:41:26 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 04:59:43 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 07:42:09 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Liu", "Fang", ""], ["Li", "Ge", ""], ["Wei", "Bolin", ""], ["Xia", "Xin", ""], ["Fu", "Zhiyi", ""], ["Jin", "Zhi", ""]]}, {"id": "1909.07036", "submitter": "Keehang Kwon", "authors": "Keehang Kwon", "title": "Towards Distributed Logic Programming based on Computability Logic", "comments": "We discuss an agent programming model with query/knowledgebase\n  duality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  {\\em Computability logic} (CoL) is a powerful computational model which views\ncomputational problems as games played by a machine and its environment. It\nuses formulas to represent computational problems. In this paper, we show that\nCoL naturally supports multiagent programming models with distributed control.\nTo be specific, we discuss a web-based implemention of a distributed logic\nprogramming model based on CoL (CL1 to be exact).\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 07:33:22 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kwon", "Keehang", ""]]}, {"id": "1909.07063", "submitter": "Marc Dymetman", "authors": "Tetiana Parshakova and Jean-Marc Andreoli and Marc Dymetman", "title": "Global Autoregressive Models for Data-Efficient Sequence Learning", "comments": "To appear in CONLL (The SIGNLL Conference on Computational Natural\n  Language Learning) Hong Kong, Nov. 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard autoregressive seq2seq models are easily trained by max-likelihood,\nbut tend to show poor results under small-data conditions. We introduce a class\nof seq2seq models, GAMs (Global Autoregressive Models), which combine an\nautoregressive component with a log-linear component, allowing the use of\nglobal \\textit{a priori} features to compensate for lack of data. We train\nthese models in two steps. In the first step, we obtain an \\emph{unnormalized}\nGAM that maximizes the likelihood of the data, but is improper for fast\ninference or evaluation. In the second step, we use this GAM to train (by\ndistillation) a second autoregressive model that approximates the\n\\emph{normalized} distribution associated with the GAM, and can be used for\nfast inference and evaluation. Our experiments focus on language modelling\nunder synthetic conditions and show a strong perplexity reduction of using the\nsecond autoregressive model over the standard one.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 08:46:30 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 19:40:01 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Parshakova", "Tetiana", ""], ["Andreoli", "Jean-Marc", ""], ["Dymetman", "Marc", ""]]}, {"id": "1909.07082", "submitter": "Udo Schlegel", "authors": "Udo Schlegel, Hiba Arnout, Mennatallah El-Assady, Daniela Oelke,\n  Daniel A. Keim", "title": "Towards a Rigorous Evaluation of XAI Methods on Time Series", "comments": "5 Pages 1 Figure 1 Table 1 Page Reference - 2019 ICCV Workshop on\n  Interpreting and Explaining Visual Artificial Intelligence Models", "journal-ref": "2019 ICCV Workshop on Interpreting and Explaining Visual\n  Artificial Intelligence Models", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable Artificial Intelligence (XAI) methods are typically deployed to\nexplain and debug black-box machine learning models. However, most proposed XAI\nmethods are black-boxes themselves and designed for images. Thus, they rely on\nvisual interpretability to evaluate and prove explanations. In this work, we\napply XAI methods previously used in the image and text-domain on time series.\nWe present a methodology to test and evaluate various XAI methods on time\nseries by introducing new verification techniques to incorporate the temporal\ndimension. We further conduct preliminary experiments to assess the quality of\nselected XAI method explanations with various verification methods on a range\nof datasets and inspecting quality metrics on it. We demonstrate that in our\ninitial experiments, SHAP works robust for all models, but others like\nDeepLIFT, LRP, and Saliency Maps work better with specific architectures.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 09:26:00 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 14:24:01 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Schlegel", "Udo", ""], ["Arnout", "Hiba", ""], ["El-Assady", "Mennatallah", ""], ["Oelke", "Daniela", ""], ["Keim", "Daniel A.", ""]]}, {"id": "1909.07095", "submitter": "Cristina Cornelio PhD", "authors": "Cristina Cornelio and Veronika Thost", "title": "RuDaS: Synthetic Datasets for Rule Learning and Evaluation Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logical rules are a popular knowledge representation language in many\ndomains, representing background knowledge and encoding information that can be\nderived from given facts in a compact form. However, rule formulation is a\ncomplex process that requires deep domain expertise,and is further challenged\nby today's often large, heterogeneous, and incomplete knowledge graphs. Several\napproaches for learning rules automatically, given a set of input example\nfacts,have been proposed over time, including, more recently, neural systems.\nYet, the area is missing adequate datasets and evaluation approaches: existing\ndatasets often resemble toy examples that neither cover the various kinds of\ndependencies between rules nor allow for testing scalability. We present a tool\nfor generating different kinds of datasets and for evaluating rule learning\nsystems, including new performance measures.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 09:56:06 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 14:20:34 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Cornelio", "Cristina", ""], ["Thost", "Veronika", ""]]}, {"id": "1909.07116", "submitter": "Dattaraj Rao", "authors": "Dattaraj Rao", "title": "Leveraging human Domain Knowledge to model an empirical Reward function\n  for a Reinforcement Learning problem", "comments": "4 pages, 3 figures, code shared on Google colab", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Reinforcement Learning (RL) problems depend on an exhaustive\nsimulation environment that models real-world physics of the problem and trains\nthe RL agent by observing this environment. In this paper, we present a novel\napproach to creating an environment by modeling the reward function based on\nempirical rules extracted from human domain knowledge of the system under\nstudy. Using this empirical rewards function, we will build an environment and\ntrain the agent. We will first create an environment that emulates the effect\nof setting cabin temperature through thermostat. This is typically done in RL\nproblems by creating an exhaustive model of the system with detailed\nthermodynamic study. Instead, we propose an empirical approach to model the\nreward function based on human domain knowledge. We will document some rules of\nthumb that we usually exercise as humans while setting thermostat temperature\nand try and model these into our reward function. This modeling of empirical\nhuman domain rules into a reward function for RL is the unique aspect of this\npaper. This is a continuous action space problem and using deep deterministic\npolicy gradient (DDPG) method, we will solve for maximizing the reward\nfunction. We will create a policy network that predicts optimal temperature\nsetpoint given external temperature and humidity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 10:57:26 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Rao", "Dattaraj", ""]]}, {"id": "1909.07156", "submitter": "Masanari Kimura", "authors": "Masanari Kimura, Masayuki Tanaka", "title": "New Perspective of Interpretability of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known as black-box models. In other words, it\nis difficult to interpret the internal state of the model. Improving the\ninterpretability of DNNs is one of the hot research topics. However, at\npresent, the definition of interpretability for DNNs is vague, and the question\nof what is a highly explanatory model is still controversial. To address this\nissue, we provide the definition of the human predictability of the model, as a\npart of the interpretability of the DNNs. The human predictability proposed in\nthis paper is defined by easiness to predict the change of the inference when\nperturbating the model of the DNNs. In addition, we introduce one example of\nhigh human-predictable DNNs. We discuss that our definition will help to the\nresearch of the interpretability of the DNNs considering various types of\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 07:24:20 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kimura", "Masanari", ""], ["Tanaka", "Masayuki", ""]]}, {"id": "1909.07157", "submitter": "Xianlong Zeng", "authors": "Xianlong Zeng, Soheil Moosavinasab, En-Ju D Lin, Simon Lin, Razvan\n  Bunescu, Chang Liu", "title": "Distributed representation of patients and its use for medical cost\n  prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient representation of patients is very important in the healthcare\ndomain and can help with many tasks such as medical risk prediction. Many\nexisting methods, such as diagnostic Cost Groups (DCG), rely on expert\nknowledge to build patient representation from medical data, which is resource\nconsuming and non-scalable. Unsupervised machine learning algorithms are a good\nchoice for automating the representation learning process. However, there is\nvery little research focusing on onpatient-level representation learning\ndirectly from medical claims. In this paper, weproposed a novel patient vector\nlearning architecture that learns high quality,fixed-length patient\nrepresentation from claims data. We conducted several experiments to test the\nquality of our learned representation, and the empirical results show that our\nlearned patient vectors are superior to vectors learned through other methods\nincluding a popular commercial model. Lastly, we provide potential clinical\ninterpretation for using our representation on predictive tasks, as\ninterpretability is vital in the healthcare domain\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 13:37:46 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zeng", "Xianlong", ""], ["Moosavinasab", "Soheil", ""], ["Lin", "En-Ju D", ""], ["Lin", "Simon", ""], ["Bunescu", "Razvan", ""], ["Liu", "Chang", ""]]}, {"id": "1909.07208", "submitter": "Emna Rejaibi", "authors": "Emna Rejaibi, Ali Komaty, Fabrice Meriaudeau, Said Agrebi, and Alice\n  Othmani", "title": "MFCC-based Recurrent Neural Network for Automatic Clinical Depression\n  Recognition and Assessment from Speech", "comments": "14 pages, 7 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Clinical depression or Major Depressive Disorder (MDD) is a common and\nserious medical illness. In this paper, a deep recurrent neural network-based\nframework is presented to detect depression and to predict its severity level\nfrom speech. Low-level and high-level audio features are extracted from audio\nrecordings to predict the 24 scores of the Patient Health Questionnaire and the\nbinary class of depression diagnosis. To overcome the problem of the small size\nof Speech Depression Recognition (SDR) datasets, expanding training labels and\ntransferred features are considered. The proposed approach outperforms the\nstate-of-art approaches on the DAIC-WOZ database with an overall accuracy of\n76.27% and a root mean square error of 0.4 in assessing depression, while a\nroot mean square error of 0.168 is achieved in predicting the depression\nseverity levels. The proposed framework has several advantages (fastness,\nnon-invasiveness, and non-intrusion), which makes it convenient for real-time\napplications. The performances of the proposed approach are evaluated under a\nmulti-modal and a multi-features experiments. MFCC based high-level features\nhold relevant information related to depression. Yet, adding visual action\nunits and different other acoustic features further boosts the classification\nresults by 20% and 10% to reach an accuracy of 95.6% and 86%, respectively.\nConsidering visual-facial modality needs to be carefully studied as it sparks\npatient privacy concerns while adding more acoustic features increases the\ncomputation time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 14:03:01 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 13:09:24 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Rejaibi", "Emna", ""], ["Komaty", "Ali", ""], ["Meriaudeau", "Fabrice", ""], ["Agrebi", "Said", ""], ["Othmani", "Alice", ""]]}, {"id": "1909.07245", "submitter": "Alun Preece", "authors": "Alun Preece", "title": "BMVC 2019: Workshop on Interpretable and Explainable Machine Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable\nMachine Vision, Cardiff, UK, September 12, 2019.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 14:44:19 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Preece", "Alun", ""]]}, {"id": "1909.07268", "submitter": "Jessica Rivera Villicana", "authors": "Jessica Rivera-Villicana, Fabio Zambetta, James Harland, Marsha Berry", "title": "Exploring Apprenticeship Learning for Player Modelling in Interactive\n  Narratives", "comments": "Extended Abstracts of the 2019 Annual Symposium on Computer-Human\n  Interaction in Play (CHI Play)", "journal-ref": null, "doi": "10.1145/3341215.3356314", "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an early Apprenticeship Learning approach to mimic\nthe behaviour of different players in a short adaption of the interactive\nfiction Anchorhead. Our motivation is the need to understand and simulate\nplayer behaviour to create systems to aid the design and personalisation of\nInteractive Narratives (INs). INs are partially observable for the players and\ntheir goals are dynamic as a result. We used Receding Horizon IRL (RHIRL) to\nlearn players' goals in the form of reward functions, and derive policies to\nimitate their behaviour. Our preliminary results suggest that RHIRL is able to\nlearn action sequences to complete a game, and provided insights towards\ngenerating behaviour more similar to specific players.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 15:15:57 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Rivera-Villicana", "Jessica", ""], ["Zambetta", "Fabio", ""], ["Harland", "James", ""], ["Berry", "Marsha", ""]]}, {"id": "1909.07299", "submitter": "Alper Kamil Bozkurt", "authors": "Alper Kamil Bozkurt, Yu Wang, Michael M. Zavlanos, Miroslav Pajic", "title": "Control Synthesis from Linear Temporal Logic Specifications using\n  Model-Free Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a reinforcement learning (RL) framework to synthesize a control\npolicy from a given linear temporal logic (LTL) specification in an unknown\nstochastic environment that can be modeled as a Markov Decision Process (MDP).\nSpecifically, we learn a policy that maximizes the probability of satisfying\nthe LTL formula without learning the transition probabilities. We introduce a\nnovel rewarding and path-dependent discounting mechanism based on the LTL\nformula such that (i) an optimal policy maximizing the total discounted reward\neffectively maximizes the probabilities of satisfying LTL objectives, and (ii)\na model-free RL algorithm using these rewards and discount factors is\nguaranteed to converge to such policy. Finally, we illustrate the applicability\nof our RL-based synthesis approach on two motion planning case studies.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 15:56:32 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 05:13:27 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Bozkurt", "Alper Kamil", ""], ["Wang", "Yu", ""], ["Zavlanos", "Michael M.", ""], ["Pajic", "Miroslav", ""]]}, {"id": "1909.07310", "submitter": "Ole-Christoffer Granmo", "authors": "Saeed Rahimi Gorji, Ole-Christoffer Granmo, Adrian Phoulady, Morten\n  Goodwin", "title": "A Tsetlin Machine with Multigranular Clauses", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced Tsetlin Machine (TM) has provided competitive pattern\nrecognition accuracy in several benchmarks, however, requires a 3-dimensional\nhyperparameter search. In this paper, we introduce the Multigranular Tsetlin\nMachine (MTM). The MTM eliminates the specificity hyperparameter, used by the\nTM to control the granularity of the conjunctive clauses that it produces for\nrecognizing patterns. Instead of using a fixed global specificity, we encode\nvarying specificity as part of the clauses, rendering the clauses\nmultigranular. This makes it easier to configure the TM because the\ndimensionality of the hyperparameter search space is reduced to only two\ndimensions. Indeed, it turns out that there is significantly less\nhyperparameter tuning involved in applying the MTM to new problems. Further, we\ndemonstrate empirically that the MTM provides similar performance to what is\nachieved with a finely specificity-optimized TM, by comparing their performance\non both synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 16:09:34 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gorji", "Saeed Rahimi", ""], ["Granmo", "Ole-Christoffer", ""], ["Phoulady", "Adrian", ""], ["Goodwin", "Morten", ""]]}, {"id": "1909.07328", "submitter": "Nuri Cingillioglu", "authors": "Nuri Cingillioglu and Alessandra Russo", "title": "Learning Invariants through Soft Unification", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human reasoning involves recognising common underlying principles across many\nexamples. The by-products of such reasoning are invariants that capture\npatterns such as \"if someone went somewhere then they are there\", expressed\nusing variables \"someone\" and \"somewhere\" instead of mentioning specific people\nor places. Humans learn what variables are and how to use them at a young age.\nThis paper explores whether machines can also learn and use variables solely\nfrom examples without requiring human pre-engineering. We propose Unification\nNetworks, an end-to-end differentiable neural network approach capable of\nlifting examples into invariants and using those invariants to solve a given\ntask. The core characteristic of our architecture is soft unification between\nexamples that enables the network to generalise parts of the input into\nvariables, thereby learning invariants. We evaluate our approach on five\ndatasets to demonstrate that learning invariants captures patterns in the data\nand can improve performance over baselines.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 16:48:52 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 10:24:14 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Cingillioglu", "Nuri", ""], ["Russo", "Alessandra", ""]]}, {"id": "1909.07370", "submitter": "Awais Ashfaq", "authors": "Awais Ashfaq, Slawomir Nowaczyk", "title": "Machine learning in healthcare -- a system's perspective", "comments": "ACM SIGKDD Workshop on Epidemiology meets Data Mining and Knowledge\n  Discovery (epiDAMIK), August 2019 Anchorage, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A consequence of the fragmented and siloed healthcare landscape is that\npatient care (and data) is split along multitude of different facilities and\ncomputer systems and enabling interoperability between these systems is hard.\nThe lack interoperability not only hinders continuity of care and burdens\nproviders, but also hinders effective application of Machine Learning (ML)\nalgorithms. Thus, most current ML algorithms, designed to understand patient\ncare and facilitate clinical decision-support, are trained on limited datasets.\nThis approach is analogous to the Newtonian paradigm of Reductionism in which a\nsystem is broken down into elementary components and a description of the whole\nis formed by understanding those components individually. A key limitation of\nthe reductionist approach is that it ignores the component-component\ninteractions and dynamics within the system which are often of prime\nsignificance in understanding the overall behaviour of complex adaptive systems\n(CAS). Healthcare is a CAS.\n  Though the application of ML on health data have shown incremental\nimprovements for clinical decision support, ML has a much a broader potential\nto restructure care delivery as a whole and maximize care value. However, this\nML potential remains largely untapped: primarily due to functional limitations\nof Electronic Health Records (EHR) and the inability to see the healthcare\nsystem as a whole. This viewpoint (i) articulates the healthcare as a complex\nsystem which has a biological and an organizational perspective, (ii) motivates\nwith examples, the need of a system's approach when addressing healthcare\nchallenges via ML and, (iii) emphasizes to unleash EHR functionality - while\nduly respecting all ethical and legal concerns - to reap full benefits of ML.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 06:13:53 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 21:50:27 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Ashfaq", "Awais", ""], ["Nowaczyk", "Slawomir", ""]]}, {"id": "1909.07373", "submitter": "Zac Wellmer", "authors": "Zac Wellmer, James Kwok", "title": "Policy Prediction Network: Model-Free Behavior Policy with Model-Based\n  Learning in Continuous Action Space", "comments": "Published at ECML PKDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel deep reinforcement learning architecture that was\ninspired by previous tree structured architectures which were only useable in\ndiscrete action spaces. Policy Prediction Network offers a way to improve\nsample complexity and performance on continuous control problems in exchange\nfor extra computation at training time but at no cost in computation at rollout\ntime. Our approach integrates a mix between model-free and model-based\nreinforcement learning. Policy Prediction Network is the first to introduce\nimplicit model-based learning to Policy Gradient algorithms for continuous\naction space and is made possible via the empirically justified clipping\nscheme. Our experiments are focused on the MuJoCo environments so that they can\nbe compared with similar work done in this area.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 12:39:02 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Wellmer", "Zac", ""], ["Kwok", "James", ""]]}, {"id": "1909.07375", "submitter": "Keehang Kwon", "authors": "Keehang Kwon", "title": "Extending and Automating Basic Probability Theory with Propositional\n  Computability Logic", "comments": "4 pages. Some errors were fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.LO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical probability theory is formulated using sets. In this paper, we\nextend classical probability theory with propositional computability logic.\nUnlike other formalisms, computability logic is built on the notion of\nevents/games, which is central to probability theory.\n  The probability theory based on CoL is therefore useful for {\\it automating}\nuncertainty reasoning. We describe some basic properties of this new\nprobability theory. We also discuss a novel isomorphism between the set\noperations and computability logic operations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 04:51:19 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 13:05:44 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 06:09:01 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kwon", "Keehang", ""]]}, {"id": "1909.07445", "submitter": "David Cerezo S\\'anchez", "authors": "David Cerezo S\\'anchez", "title": "Truthful and Faithful Monetary Policy for a Stablecoin Conducted by a\n  Decentralised, Encrypted Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.GT cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Holy Grail of a decentralised stablecoin is achieved on rigorous\nmathematical frameworks, obtaining multiple advantageous proofs: stability,\nconvergence, truthfulness, faithfulness, and malicious-security. These\nproperties could only be attained by the novel and interdisciplinary\ncombination of previously unrelated fields: model predictive control, deep\nlearning, alternating direction method of multipliers (consensus-ADMM),\nmechanism design, secure multi-party computation, and zero-knowledge proofs.\nFor the first time, this paper proves:\n  - the feasibility of decentralising the central bank while securely\npreserving its independence in a decentralised computation setting\n  - the benefits for price stability of combining mechanism design, provable\nsecurity, and control theory, unlike the heuristics of previous stablecoins\n  - the implementation of complex monetary policies on a stablecoin, equivalent\nto the ones used by central banks and beyond the current fixed rules of\ncryptocurrencies that hinder their price stability\n  - methods to circumvent the impossibilities of Guaranteed Output Delivery\n(G.O.D.) and fairness: standing on truthfulness and faithfulness, we reach\nG.O.D. and fairness under the assumption of rational parties\n  As a corollary, a decentralised artificial intelligence is able to conduct\nthe monetary policy of a stablecoin, minimising human intervention.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 19:28:26 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["S\u00e1nchez", "David Cerezo", ""]]}, {"id": "1909.07481", "submitter": "Shenhao Wang", "authors": "Shenhao Wang, Baichuan Mo, Jinhua Zhao", "title": "Deep Neural Networks for Choice Analysis: Architectural Design with\n  Alternative-Specific Utility Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI econ.GN q-fin.EC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas deep neural network (DNN) is increasingly applied to choice analysis,\nit is challenging to reconcile domain-specific behavioral knowledge with\ngeneric-purpose DNN, to improve DNN's interpretability and predictive power,\nand to identify effective regularization methods for specific tasks. This study\ndesigns a particular DNN architecture with alternative-specific utility\nfunctions (ASU-DNN) by using prior behavioral knowledge. Unlike a fully\nconnected DNN (F-DNN), which computes the utility value of an alternative k by\nusing the attributes of all the alternatives, ASU-DNN computes it by using only\nk's own attributes. Theoretically, ASU-DNN can dramatically reduce the\nestimation error of F-DNN because of its lighter architecture and sparser\nconnectivity. Empirically, ASU-DNN has 2-3% higher prediction accuracy than\nF-DNN over the whole hyperparameter space in a private dataset that we\ncollected in Singapore and a public dataset in R mlogit package. The\nalternative-specific connectivity constraint, as a domain-knowledge-based\nregularization method, is more effective than the most popular generic-purpose\nexplicit and implicit regularization methods and architectural hyperparameters.\nASU-DNN is also more interpretable because it provides a more regular\nsubstitution pattern of travel mode choices than F-DNN does. The comparison\nbetween ASU-DNN and F-DNN can also aid in testing the behavioral knowledge. Our\nresults reveal that individuals are more likely to compute utility by using an\nalternative's own attributes, supporting the long-standing practice in choice\nmodeling. Overall, this study demonstrates that prior behavioral knowledge\ncould be used to guide the architecture design of DNN, to function as an\neffective domain-knowledge-based regularization method, and to improve both the\ninterpretability and predictive power of DNN in choice analysis.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 21:01:23 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 22:38:28 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Shenhao", ""], ["Mo", "Baichuan", ""], ["Zhao", "Jinhua", ""]]}, {"id": "1909.07483", "submitter": "Benjamin Beyret", "authors": "Benjamin Beyret, Jos\\'e Hern\\'andez-Orallo, Lucy Cheke, Marta Halina,\n  Murray Shanahan, Matthew Crosby", "title": "The Animal-AI Environment: Training and Testing Animal-Like Artificial\n  Cognition", "comments": "14 pages, 34 figures (update: reduce images size)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in artificial intelligence have been strongly driven by the\nuse of game environments for training and evaluating agents. Games are often\naccessible and versatile, with well-defined state-transitions and goals\nallowing for intensive training and experimentation. However, agents trained in\na particular environment are usually tested on the same or slightly varied\ndistributions, and solutions do not necessarily imply any understanding. If we\nwant AI systems that can model and understand their environment, we need\nenvironments that explicitly test for this. Inspired by the extensive\nliterature on animal cognition, we present an environment that keeps all the\npositive elements of standard gaming environments, but is explicitly designed\nfor the testing of animal-like artificial cognition.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 10:14:12 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 09:07:46 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Beyret", "Benjamin", ""], ["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""], ["Cheke", "Lucy", ""], ["Halina", "Marta", ""], ["Shanahan", "Murray", ""], ["Crosby", "Matthew", ""]]}, {"id": "1909.07490", "submitter": "Rayan Mosli", "authors": "Rayan Mosli, Matthew Wright, Bo Yuan and Yin Pan", "title": "They Might NOT Be Giants: Crafting Black-Box Adversarial Examples with\n  Fewer Queries Using Particle Swarm Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models have been found to be susceptible to adversarial\nexamples that are often indistinguishable from the original inputs. These\nadversarial examples are created by applying adversarial perturbations to input\nsamples, which would cause them to be misclassified by the target models.\nAttacks that search and apply the perturbations to create adversarial examples\nare performed in both white-box and black-box settings, depending on the\ninformation available to the attacker about the target. For black-box attacks,\nthe only capability available to the attacker is the ability to query the\ntarget with specially crafted inputs and observing the labels returned by the\nmodel. Current black-box attacks either have low success rates, requires a high\nnumber of queries, or produce adversarial examples that are easily\ndistinguishable from their sources. In this paper, we present AdversarialPSO, a\nblack-box attack that uses fewer queries to create adversarial examples with\nhigh success rates. AdversarialPSO is based on the evolutionary search\nalgorithm Particle Swarm Optimization, a populationbased gradient-free\noptimization algorithm. It is flexible in balancing the number of queries\nsubmitted to the target vs the quality of imperceptible adversarial examples.\nThe attack has been evaluated using the image classification benchmark datasets\nCIFAR-10, MNIST, and Imagenet, achieving success rates of 99.6%, 96.3%, and\n82.0%, respectively, while submitting substantially fewer queries than the\nstate-of-the-art. We also present a black-box method for isolating salient\nfeatures used by models when making classifications. This method, called Swarms\nwith Individual Search Spaces or SWISS, creates adversarial examples by finding\nand modifying the most important features in the input.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 21:24:19 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Mosli", "Rayan", ""], ["Wright", "Matthew", ""], ["Yuan", "Bo", ""], ["Pan", "Yin", ""]]}, {"id": "1909.07528", "submitter": "Bowen Baker", "authors": "Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell,\n  Bob McGrew, Igor Mordatch", "title": "Emergent Tool Use From Multi-Agent Autocurricula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through multi-agent competition, the simple objective of hide-and-seek, and\nstandard reinforcement learning algorithms at scale, we find that agents create\na self-supervised autocurriculum inducing multiple distinct rounds of emergent\nstrategy, many of which require sophisticated tool use and coordination. We\nfind clear evidence of six emergent phases in agent strategy in our\nenvironment, each of which creates a new pressure for the opposing team to\nadapt; for instance, agents learn to build multi-object shelters using moveable\nboxes which in turn leads to agents discovering that they can overcome\nobstacles using ramps. We further provide evidence that multi-agent competition\nmay scale better with increasing environment complexity and leads to behavior\nthat centers around far more human-relevant skills than other self-supervised\nreinforcement learning methods such as intrinsic motivation. Finally, we\npropose transfer and fine-tuning as a way to quantitatively evaluate targeted\ncapabilities, and we compare hide-and-seek agents to both intrinsic motivation\nand random initialization baselines in a suite of domain-specific intelligence\ntests.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 00:17:02 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 00:56:50 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Baker", "Bowen", ""], ["Kanitscheider", "Ingmar", ""], ["Markov", "Todor", ""], ["Wu", "Yi", ""], ["Powell", "Glenn", ""], ["McGrew", "Bob", ""], ["Mordatch", "Igor", ""]]}, {"id": "1909.07543", "submitter": "Bogdan Mazoure", "authors": "Thang Doan, Bogdan Mazoure, Moloud Abdar, Audrey Durand, Joelle\n  Pineau, R Devon Hjelm", "title": "Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous control tasks in reinforcement learning are important because they\nprovide an important framework for learning in high-dimensional state spaces\nwith deceptive rewards, where the agent can easily become trapped into\nsuboptimal solutions. One way to avoid local optima is to use a population of\nagents to ensure coverage of the policy space, yet learning a population with\nthe \"best\" coverage is still an open problem. In this work, we present a novel\napproach to population-based RL in continuous control that leverages properties\nof normalizing flows to perform attractive and repulsive operations between\ncurrent members of the population and previously observed policies. Empirical\nresults on the MuJoCo suite demonstrate a high performance gain for our\nalgorithm compared to prior work, including Soft-Actor Critic (SAC).\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 01:28:20 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 16:29:36 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 13:51:16 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Doan", "Thang", ""], ["Mazoure", "Bogdan", ""], ["Abdar", "Moloud", ""], ["Durand", "Audrey", ""], ["Pineau", "Joelle", ""], ["Hjelm", "R Devon", ""]]}, {"id": "1909.07547", "submitter": "Abdelrhman Saleh", "authors": "Abdelrhman Saleh, Natasha Jaques, Asma Ghandeharioun, Judy Hanwen\n  Shen, Rosalind Picard", "title": "Hierarchical Reinforcement Learning for Open-Domain Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain dialog generation is a challenging problem; maximum likelihood\ntraining can lead to repetitive outputs, models have difficulty tracking\nlong-term conversational goals, and training on standard movie or online\ndatasets may lead to the generation of inappropriate, biased, or offensive\ntext. Reinforcement Learning (RL) is a powerful framework that could\npotentially address these issues, for example by allowing a dialog model to\noptimize for reducing toxicity and repetitiveness. However, previous approaches\nwhich apply RL to open-domain dialog generation do so at the word level, making\nit difficult for the model to learn proper credit assignment for long-term\nconversational rewards. In this paper, we propose a novel approach to\nhierarchical reinforcement learning, VHRL, which uses policy gradients to tune\nthe utterance-level embedding of a variational sequence model. This\nhierarchical approach provides greater flexibility for learning long-term,\nconversational rewards. We use self-play and RL to optimize for a set of\nhuman-centered conversation metrics, and show that our approach provides\nsignificant improvements -- in terms of both human evaluation and automatic\nmetrics -- over state-of-the-art dialog models, including Transformers.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 01:57:18 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 14:25:28 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2019 21:23:04 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Saleh", "Abdelrhman", ""], ["Jaques", "Natasha", ""], ["Ghandeharioun", "Asma", ""], ["Shen", "Judy Hanwen", ""], ["Picard", "Rosalind", ""]]}, {"id": "1909.07557", "submitter": "Mingyu Xiao", "authors": "Sen Huang and Mingyu Xiao", "title": "Object Reachability via Swaps under Strict and Weak Preferences", "comments": "This version is to appear in Autonomous Agents and Multi-Agent\n  Systems", "journal-ref": "A previous version presented at AAAI 2019 with the title 'Object\n  Reachability via Swaps along a Line'", "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\textsc{Housing Market} problem is a widely studied resource allocation\nproblem. In this problem, each agent can only receive a single object and has\npreferences over all objects. Starting from an initial endowment, we want to\nreach a certain assignment via a sequence of rational trades. We first consider\nwhether an object is reachable for a given agent under a social network, where\na trade between two agents is allowed if they are neighbors in the network and\nno participant has a deficit from the trade. Assume that the preferences of the\nagents are strict (no tie among objects is allowed). This problem is\npolynomially solvable in a star-network and NP-complete in a tree-network. It\nis left as a challenging open problem whether the problem is polynomially\nsolvable when the network is a path. We answer this open problem positively by\ngiving a polynomial-time algorithm. Then we show that when the preferences of\nthe agents are weak (ties among objects are allowed), the problem becomes\nNP-hard even when the network is a path. In addition, we consider the\ncomputational complexity of finding different optimal assignments for the\nproblem under the network being a path or a star.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 02:36:41 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 08:40:52 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Huang", "Sen", ""], ["Xiao", "Mingyu", ""]]}, {"id": "1909.07572", "submitter": "Hongtao Wu", "authors": "Hongtao Wu, Deven Misra, and Gregory S. Chirikjian", "title": "Is That a Chair? Imagining Affordances Using Simulations of an\n  Articulated Human Body", "comments": "7 pages, 6 figures. Accepted to ICRA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robots to exhibit a high level of intelligence in the real world, they\nmust be able to assess objects for which they have no prior knowledge.\nTherefore, it is crucial for robots to perceive object affordances by reasoning\nabout physical interactions with the object. In this paper, we propose a novel\nmethod to provide robots with an ability to imagine object affordances using\nphysical simulations. The class of chair is chosen here as an initial category\nof objects to illustrate a more general paradigm. In our method, the robot\n\"imagines\" the affordance of an arbitrarily oriented object as a chair by\nsimulating a physical sitting interaction between an articulated human body and\nthe object. This object affordance reasoning is used as a cue for object\nclassification (chair vs non-chair). Moreover, if an object is classified as a\nchair, the affordance reasoning can also predict the upright pose of the object\nwhich allows the sitting interaction to take place. We call this type of poses\nthe functional pose. We demonstrate our method in chair classification on\nsynthetic 3D CAD models. Although our method uses only 30 models for training,\nit outperforms appearance-based deep learning methods, which require a large\namount of training data, when the upright orientation is not assumed to be\nknown a priori. In addition, we showcase that the functional pose predictions\nof our method align well with human judgments on both synthetic models and real\nobjects scanned by a depth camera.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 03:36:32 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 21:25:42 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Wu", "Hongtao", ""], ["Misra", "Deven", ""], ["Chirikjian", "Gregory S.", ""]]}, {"id": "1909.07583", "submitter": "Yuhong Guo", "authors": "Yaser Alwattar and Yuhong Guo", "title": "Inverse Visual Question Answering with Multi-Level Attentions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep multi-level attention model to address\ninverse visual question answering. The proposed model generates regional visual\nand semantic features at the object level and then enhances them with the\nanswer cue by using attention mechanisms. Two levels of multiple attentions are\nemployed in the model, including the dual attention at the partial question\nencoding step and the dynamic attention at the next question word generation\nstep. We evaluate the proposed model on the VQA V1 dataset. It demonstrates\nstate-of-the-art performance in terms of multiple commonly used metrics.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 04:41:12 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 00:13:21 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Alwattar", "Yaser", ""], ["Guo", "Yuhong", ""]]}, {"id": "1909.07592", "submitter": "Hongtu Zhou", "authors": "Hongtu Zhou, Xinneng Yang, Enwei Zhang, Junqiao Zhao, Lewen Cai, Chen\n  Ye, Yan Wu", "title": "Real-time Multi-target Path Prediction and Planning for Autonomous\n  Driving aided by FCN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time multi-target path planning is a key issue in the field of\nautonomous driving. Although multiple paths can be generated in real-time with\npolynomial curves, the generated paths are not flexible enough to deal with\ncomplex road scenes such as S-shaped road and unstructured scenes such as\nparking lots. Search and sampling-based methods, such as A* and RRT and their\nderived methods, are flexible in generating paths for these complex road\nenvironments. However, the existing algorithms require significant time to plan\nto multiple targets, which greatly limits their application in autonomous\ndriving. In this paper, a real-time path planning method for multi-targets is\nproposed. We train a fully convolutional neural network (FCN) to predict a path\nregion for the target at first. By taking the predicted path region as soft\nconstraints, the A* algorithm is then applied to search the exact path to the\ntarget. Experiments show that FCN can make multiple predictions in a very short\ntime (50 times in 40ms), and the predicted path region effectively restrict the\nsearching space for the following A* search. Therefore, the A* can search much\nfaster so that the multi-target path planning can be achieved in real-time (3\ntargets in less than 100ms).\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 04:59:07 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Zhou", "Hongtu", ""], ["Yang", "Xinneng", ""], ["Zhang", "Enwei", ""], ["Zhao", "Junqiao", ""], ["Cai", "Lewen", ""], ["Ye", "Chen", ""], ["Wu", "Yan", ""]]}, {"id": "1909.07646", "submitter": "EPTCS", "authors": "Bart Bogaerts (Vrije Universiteit Brussel), Esra Erdem (Sabanci\n  University), Paul Fodor (Stony Brook University), Andrea Formisano\n  (Universit\\`a di Perugia), Giovambattista Ianni (University of Calabria),\n  Daniela Inclezan (Miami University), German Vidal (Universitat Politecnica de\n  Valencia), Alicia Villanueva (Universitat Politecnica de Valencia), Marina De\n  Vos (University of Bath), Fangkai Yang (NVIDIA Corporation)", "title": "Proceedings 35th International Conference on Logic Programming\n  (Technical Communications)", "comments": null, "journal-ref": "EPTCS 306, 2019", "doi": "10.4204/EPTCS.306", "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the first conference held in Marseille in 1982, ICLP has been the\npremier international event for presenting research in logic programming.\nContributions are sought in all areas of logic programming, including but not\nrestricted to:\n  Foundations: Semantics, Formalisms, Nonmonotonic reasoning, Knowledge\nrepresentation.\n  Languages: Concurrency, Objects, Coordination, Mobility, Higher Order, Types,\nModes, Assertions, Modules, Meta-programming, Logic-based domain-specific\nlanguages, Programming Techniques.\n  Declarative programming: Declarative program development, Analysis, Type and\nmode inference, Partial evaluation, Abstract interpretation, Transformation,\nValidation, Verification, Debugging, Profiling, Testing, Execution\nvisualization\n  Implementation: Virtual machines, Compilation, Memory management,\nParallel/distributed execution, Constraint handling rules, Tabling, Foreign\ninterfaces, User interfaces.\n  Related Paradigms and Synergies: Inductive and Co-inductive Logic\nProgramming, Constraint Logic Programming, Answer Set Programming, Interaction\nwith SAT, SMT and CSP solvers, Logic programming techniques for type inference\nand theorem proving, Argumentation, Probabilistic Logic Programming, Relations\nto object-oriented and Functional programming.\n  Applications: Databases, Big Data, Data integration and federation, Software\nengineering, Natural language processing, Web and Semantic Web, Agents,\nArtificial intelligence, Computational life sciences, Education, Cybersecurity,\nand Robotics.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 08:33:12 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Bogaerts", "Bart", "", "Vrije Universiteit Brussel"], ["Erdem", "Esra", "", "Sabanci\n  University"], ["Fodor", "Paul", "", "Stony Brook University"], ["Formisano", "Andrea", "", "Universit\u00e0 di Perugia"], ["Ianni", "Giovambattista", "", "University of Calabria"], ["Inclezan", "Daniela", "", "Miami University"], ["Vidal", "German", "", "Universitat Politecnica de\n  Valencia"], ["Villanueva", "Alicia", "", "Universitat Politecnica de Valencia"], ["De Vos", "Marina", "", "University of Bath"], ["Yang", "Fangkai", "", "NVIDIA Corporation"]]}, {"id": "1909.07670", "submitter": "Tomoharu Iwata", "authors": "Tomoharu Iwata and Takuma Otsuka", "title": "Efficient Transfer Bayesian Optimization with Auxiliary Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient transfer Bayesian optimization method, which finds\nthe maximum of an expensive-to-evaluate black-box function by using data on\nrelated optimization tasks. Our method uses auxiliary information that\nrepresents the task characteristics to effectively transfer knowledge for\nestimating a distribution over target functions. In particular, we use a\nGaussian process, in which the mean and covariance functions are modeled with\nneural networks that simultaneously take both the auxiliary information and\nfeature vectors as input. With a neural network mean function, we can estimate\nthe target function even without evaluations. By using the neural network\ncovariance function, we can extract nonlinear correlation among feature vectors\nthat are shared across related tasks. Our Gaussian process-based formulation\nnot only enables an analytic calculation of the posterior distribution but also\nswiftly adapts the target function to observations. Our method is also\nadvantageous because the computational costs scale linearly with the number of\nsource tasks. Through experiments using a synthetic dataset and datasets for\nfinding the optimal pedestrian traffic regulations and optimal machine learning\nalgorithms, we demonstrate that our method identifies the optimal points with\nfewer target function evaluations than existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 09:28:42 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Iwata", "Tomoharu", ""], ["Otsuka", "Takuma", ""]]}, {"id": "1909.07739", "submitter": "Jifan Yu", "authors": "Jifan Yu, Chenyu Wang, Gan Luo, Lei Hou, Juanzi Li, Jie Tang, Zhiyuan\n  Liu", "title": "Course Concept Expansion in MOOCs with External Knowledge and\n  Interactive Game", "comments": null, "journal-ref": null, "doi": "10.18653/v1/P19-1421", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As Massive Open Online Courses (MOOCs) become increasingly popular, it is\npromising to automatically provide extracurricular knowledge for MOOC users.\nSuffering from semantic drifts and lack of knowledge guidance, existing methods\ncan not effectively expand course concepts in complex MOOC environments. In\nthis paper, we first build a novel boundary during searching for new concepts\nvia external knowledge base and then utilize heterogeneous features to verify\nthe high-quality results. In addition, to involve human efforts in our model,\nwe design an interactive optimization mechanism based on a game. Our\nexperiments on the four datasets from Coursera and XuetangX show that the\nproposed method achieves significant improvements(+0.19 by MAP) over existing\nmethods. The source code and datasets have been published.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 12:07:22 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Yu", "Jifan", ""], ["Wang", "Chenyu", ""], ["Luo", "Gan", ""], ["Hou", "Lei", ""], ["Li", "Juanzi", ""], ["Tang", "Jie", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1909.07750", "submitter": "Raghu Rajan", "authors": "Raghu Rajan, Jessica Lizeth Borja Diaz, Suresh Guttikonda, Fabio\n  Ferreira, Andr\\'e Biedenkapp, Jan Ole von Hartz and Frank Hutter", "title": "MDP Playground: A Design and Debug Testbed for Reinforcement Learning", "comments": "NeurIPS 2021 Data and Benchmark Track submission (with slight\n  formatting differences, most notably citation style)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present \\emph{MDP Playground}, an efficient testbed for Reinforcement\nLearning (RL) agents with \\textit{orthogonal} dimensions that can be controlled\nindependently to challenge agents in different ways and obtain varying degrees\nof hardness in generated environments. We consider and allow control over a\nwide variety of dimensions, including \\textit{delayed rewards},\n\\textit{rewardable sequences}, \\textit{density of rewards},\n\\textit{stochasticity}, \\textit{image representations}, \\textit{irrelevant\nfeatures}, \\textit{time unit}, \\textit{action range} and more. We define a\nparameterised collection of fast-to-run toy environments in \\textit{OpenAI Gym}\nby varying these dimensions and propose to use these for the initial design and\ndevelopment of agents. We also provide wrappers that inject these dimensions\ninto complex environments from \\textit{Atari} and \\textit{Mujoco} to allow for\nevaluating agent robustness. We further provide various example use-cases and\ninstructions on how to use \\textit{MDP Playground} to design and debug agents.\nWe believe that \\textit{MDP Playground} is a valuable testbed for researchers\ndesigning new, adaptive and intelligent RL agents and those wanting to unit\ntest their agents.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 12:41:20 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 12:46:17 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 13:06:35 GMT"}, {"version": "v4", "created": "Fri, 25 Jun 2021 12:38:37 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Rajan", "Raghu", ""], ["Diaz", "Jessica Lizeth Borja", ""], ["Guttikonda", "Suresh", ""], ["Ferreira", "Fabio", ""], ["Biedenkapp", "Andr\u00e9", ""], ["von Hartz", "Jan Ole", ""], ["Hutter", "Frank", ""]]}, {"id": "1909.07771", "submitter": "Thomas Erber Professor", "authors": "T. Erber and M. J. Frank", "title": "Arrow, Hausdorff, and Ambiguities in the Choice of Preferred States in\n  Complex Systems", "comments": "8 pages, 3 figures", "journal-ref": "Journal of Interdisciplinary Mathematics 22 (2019) 129-137\n  (2019)pp 129-137", "doi": null, "report-no": null, "categories": "econ.TH cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arrow's `impossibility' theorem asserts that there are no satisfactory\nmethods of aggregating individual preferences into collective preferences in\nmany complex situations. This result has ramifications in economics, politics,\ni.e., the theory of voting, and the structure of tournaments. By identifying\nthe objects of choice with mathematical sets, and preferences with Hausdorff\nmeasures of the distances between sets, it is possible to extend Arrow's\narguments from a sociological to a mathematical setting. One consequence is\nthat notions of reversibility can be expressed in terms of the relative\nconfigurations of patterns of sets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 21:50:09 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Erber", "T.", ""], ["Frank", "M. J.", ""]]}, {"id": "1909.07775", "submitter": "Junhua Liu", "authors": "Junhua Liu, Kristin L. Wood, Kwan Hui Lim", "title": "Strategic and Crowd-Aware Itinerary Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a rapidly growing demand for itinerary planning in tourism but this\ntask remains complex and difficult, especially when considering the need to\noptimize for queuing time and crowd levels for multiple users. This difficulty\nis further complicated by the large amount of parameters involved, i.e.,\nattraction popularity, queuing time, walking time, operating hours, etc. Many\nrecent works propose solutions based on the single-person perspective, but\notherwise do not address real-world problems resulting from natural crowd\nbehavior, such as the Selfish Routing problem, which describes the consequence\nof ineffective network and sub-optimal social outcome by leaving agents to\ndecide freely. In this work, we propose the Strategic and Crowd-Aware Itinerary\nRecommendation (SCAIR) algorithm which optimizes social welfare in real-world\nsituations. We formulate the strategy of route recommendation as Markov chains\nwhich enables our simulations to be carried out in poly-time. We then evaluate\nour proposed algorithm against various competitive and realistic baselines\nusing a theme park dataset. Our simulation results highlight the existence of\nthe Selfish Routing problem and show that SCAIR outperforms the baselines in\nhandling this issue.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 11:09:47 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 12:52:32 GMT"}, {"version": "v3", "created": "Sun, 26 Apr 2020 17:00:56 GMT"}, {"version": "v4", "created": "Tue, 9 Jun 2020 14:55:22 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Liu", "Junhua", ""], ["Wood", "Kristin L.", ""], ["Lim", "Kwan Hui", ""]]}, {"id": "1909.07788", "submitter": "Yubin Zang", "authors": "Yubin Zang, Minghua Chen, Sigang Yang and Hongwei Chen", "title": "Electro-optical Neural Networks based on Time-stretch Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel architecture of electro-optical neural networks based\non the time-stretch method is proposed and numerically simulated. By stretching\ntime-domain ultrashort pulses, multiplications of large scale weight matrices\nand vectors can be implemented on light and multiple-layer of feedforward\nneural network operations can be easily implemented with fiber loops. Via\nsimulation, the performance of a three-layer electro-optical neural network is\ntested by the handwriting digit recognition task and the accuracy reaches 88%\nunder considerable noise.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 02:17:26 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Zang", "Yubin", ""], ["Chen", "Minghua", ""], ["Yang", "Sigang", ""], ["Chen", "Hongwei", ""]]}, {"id": "1909.07816", "submitter": "Jiawei Gao", "authors": "Jiawei Gao", "title": "The Computational Complexity of Fire Emblem Series and similar Tactical\n  Role-Playing Games", "comments": "Updated the abstract and the first paragraph of \"main results\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fire Emblem (FE) is a popular turn-based tactical role-playing game (TRPG)\nseries on the Nintendo gaming consoles. This paper studies the computational\ncomplexity of a simplified version of FE (only floor tiles and wall tiles, the\nHP and other attributes of characters are constants at most 8, the movement\ndistance per character each turn is fixed to 6 tiles), and proves that: 1.\nSimplified FE is PSPACE-complete (Thus actual FE is at least as hard). 2.\nPoly-round FE is NP-complete, even when the map is cycle-free, without healing\nunits, and the weapon durability is a small constant. Poly-round FE is to\ndecide whether the player can win the game in a certain number of rounds that\nis polynomial to the map size. A map is called cycle-free if its corresponding\nplanar graph is cycle-free. These hardness results also hold for other similar\nTRPG series, such as Final Fantasy Tactics, Tactics Ogre and Disgaea.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 05:30:30 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 05:09:31 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Gao", "Jiawei", ""]]}, {"id": "1909.07876", "submitter": "Matthew Wilson", "authors": "Matthew Wilson, Tucker Hermans", "title": "Learning to Manipulate Object Collections Using Grounded State\n  Representations", "comments": "Accepted to Conference on Robot Learning 2019 (Oral); Video results:\n  https://bit.ly/2khSKUs; v3: fix abstract and appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method for sim-to-real robot learning which exploits simulator\nstate information in a way that scales to many objects. We first train a pair\nof encoder networks to capture multi-object state information in a latent\nspace. One of these encoders is a CNN, which enables our system to operate on\nRGB images in the real world; the other is a graph neural network (GNN) state\nencoder, which directly consumes a set of raw object poses and enables more\naccurate reward calculation and value estimation. Once trained, we use these\nencoders in a reinforcement learning algorithm to train image-based policies\nthat can manipulate many objects. We evaluate our method on the task of pushing\na collection of objects to desired tabletop regions. Compared to methods which\nrely only on images or use fixed-length state encodings, our method achieves\nhigher success rates, performs well in the real world without fine tuning, and\ngeneralizes to different numbers and types of objects not seen during training.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 15:06:25 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 21:39:08 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 21:54:44 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Wilson", "Matthew", ""], ["Hermans", "Tucker", ""]]}, {"id": "1909.07893", "submitter": "Rocsildes Canoy", "authors": "Rocsildes Canoy and Tias Guns", "title": "Vehicle routing by learning from historical solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to investigate a decision support system for\nvehicle routing, where the routing engine learns from the subjective decisions\nthat human planners have made in the past, rather than optimizing a\ndistance-based objective criterion. This is an alternative to the practice of\nformulating a custom VRP for every company with its own routing requirements.\nInstead, we assume the presence of past vehicle routing solutions over similar\nsets of customers, and learn to make similar choices. The approach is based on\nthe concept of learning a first-order Markov model, which corresponds to a\nprobabilistic transition matrix, rather than a deterministic distance matrix.\nThis nevertheless allows us to use existing arc routing VRP software in\ncreating the actual route plans. For the learning, we explore different schemes\nto construct the probabilistic transition matrix. Our results on a use-case\nwith a small transportation company show that our method is able to generate\nresults that are close to the manually created solutions, without needing to\ncharacterize all constraints and sub-objectives explicitly. Even in the case of\nchanges in the client sets, our method is able to find solutions that are\ncloser to the actual route plans than when using distances, and hence,\nsolutions that would require fewer manual changes to transform into the actual\nroute plan.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 15:36:10 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Canoy", "Rocsildes", ""], ["Guns", "Tias", ""]]}, {"id": "1909.07930", "submitter": "Piero Molino", "authors": "Piero Molino, Yaroslav Dudin, Sai Sumanth Miryala", "title": "Ludwig: a type-based declarative deep learning toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present Ludwig, a flexible, extensible and easy to use\ntoolbox which allows users to train deep learning models and use them for\nobtaining predictions without writing code. Ludwig implements a novel approach\nto deep learning model building based on two main abstractions: data types and\ndeclarative configuration files. The data type abstraction allows for easier\ncode and sub-model reuse, and the standardized interfaces imposed by this\nabstraction allow for encapsulation and make the code easy to extend.\nDeclarative model definition configuration files enable inexperienced users to\nobtain effective models and increase the productivity of expert users.\nAlongside these two innovations, Ludwig introduces a general modularized deep\nlearning architecture called Encoder-Combiner-Decoder that can be instantiated\nto perform a vast amount of machine learning tasks. These innovations make it\npossible for engineers, scientists from other fields and, in general, a much\nbroader audience to adopt deep learning models for their tasks, concretely\nhelping in its democratization.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 16:54:29 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Molino", "Piero", ""], ["Dudin", "Yaroslav", ""], ["Miryala", "Sai Sumanth", ""]]}, {"id": "1909.08052", "submitter": "Martin Ross", "authors": "Martin K. Ross, Frank Broz and Lynne Baillie", "title": "Towards an Adaptive Robot for Sports and Rehabilitation Coaching", "comments": "AI-HRI 2019", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/08", "categories": "cs.HC cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The work presented in this paper aims to explore how, and to what extent, an\nadaptive robotic coach has the potential to provide extra motivation to adhere\nto long-term rehabilitation and help fill the coaching gap which occurs during\nrepetitive solo practice in high performance sport. Adapting the behavior of a\nsocial robot to a specific user, using reinforcement learning (RL), could be a\nway of increasing adherence to an exercise routine in both domains. The\nrequirements gathering phase is underway and is presented in this paper along\nwith the rationale of using RL in this context.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 14:51:05 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Ross", "Martin K.", ""], ["Broz", "Frank", ""], ["Baillie", "Lynne", ""]]}, {"id": "1909.08068", "submitter": "F. Richard Yu", "authors": "F. Richard Yu", "title": "From the Internet of Information to the Internet of Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of the Internet of information, we have gone through layering,\ncross-layer, and cross-system design paradigms. Recently, the ``curse of\nmodeling\" and ``curse of dimensionality\" of the cross-system design paradigm\nhave resulted in the popularity of using artificial intelligence (AI) to\noptimize the Internet of information. However, many significant research\nchallenges remain to be addressed for the AI approach, including the lack of\nhigh-quality training data due to privacy and resources constraints in this\ndata-driven approach. To address these challenges, we need to take a look at\nhumans' cooperation in a larger time scale. To facilitate cooperation in modern\nhistory, we have built three major technologies: ``grid of transportation\",\n``grid of energy\", and ``the Internet of information\". In this paper, we argue\nthat the next cooperation paradigm could be the ``Internet of intelligence\n(Intelligence-Net)\", where intelligence can be easily obtained like energy and\ninformation, enabled by the recent advances in blockchain technology. We\npresent some recent advances in these areas, and discuss some open issues and\nchallenges that need to be addressed in the future.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 22:50:10 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Yu", "F. Richard", ""]]}, {"id": "1909.08128", "submitter": "Luke Merrick", "authors": "Luke Merrick and Ankur Taly", "title": "The Explanation Game: Explaining Machine Learning Models Using Shapley\n  Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of techniques have been proposed to explain a machine learning\nmodel's prediction by attributing it to the corresponding input features.\nPopular among these are techniques that apply the Shapley value method from\ncooperative game theory. While existing papers focus on the axiomatic\nmotivation of Shapley values, and efficient techniques for computing them, they\noffer little justification for the game formulations used, and do not address\nthe uncertainty implicit in their methods' outputs. For instance, the popular\nSHAP algorithm's formulation may give substantial attributions to features that\nplay no role in the model. In this work, we illustrate how subtle differences\nin the underlying game formulations of existing methods can cause large\ndifferences in the attributions for a prediction. We then present a general\ngame formulation that unifies existing methods, and enables straightforward\nconfidence intervals on their attributions. Furthermore, it allows us to\ninterpret the attributions as contrastive explanations of an input relative to\na distribution of reference inputs. We tie this idea to classic research in\ncognitive psychology on contrastive explanations, and propose a conceptual\nframework for generating and interpreting explanations for ML models, called\nformulate, approximate, explain (FAE). We apply this framework to explain\nblack-box models trained on two UCI datasets and a Lending Club dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 22:15:09 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 20:09:55 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 23:18:21 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Merrick", "Luke", ""], ["Taly", "Ankur", ""]]}, {"id": "1909.08161", "submitter": "Nikhil Krishnaswamy", "authors": "Nikhil Krishnaswamy and James Pustejovsky", "title": "Multimodal Continuation-style Architectures for Human-Robot Interaction", "comments": "Advances in Cognitive Systems Cognitive Vision Workshop (2019), 8\n  pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an architecture for integrating real-time, multimodal input into a\ncomputational agent's contextual model. Using a human-avatar interaction in a\nvirtual world, we treat aligned gesture and speech as an ensemble where content\nmay be communicated by either modality. With a modified nondeterministic\npushdown automaton architecture, the computer system: (1) consumes input\nincrementally using continuation-passing style until it achieves sufficient\nunderstanding the user's aim; (2) constructs and asks questions where necessary\nusing established contextual information; and (3) maintains track of prior\ndiscourse items using multimodal cues. This type of architecture supports\nspecial cases of pushdown and finite state automata as well as integrating\noutputs from machine learning models. We present examples of this\narchitecture's use in multimodal one-shot learning interactions of novel\ngestures and live action composition.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 01:42:03 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Krishnaswamy", "Nikhil", ""], ["Pustejovsky", "James", ""]]}, {"id": "1909.08191", "submitter": "Hung Nghiep Tran", "authors": "Hung Nghiep Tran and Atsuhiro Takasu", "title": "Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding\n  Space", "comments": "TPDL 2019", "journal-ref": "International Conference on Theory and Practice of Digital\n  Libraries (TPDL 2019)", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The trends of open science have enabled several open scholarly datasets which\ninclude millions of papers and authors. Managing, exploring, and utilizing such\nlarge and complicated datasets effectively are challenging. In recent years,\nthe knowledge graph has emerged as a universal data format for representing\nknowledge about heterogeneous entities and their relationships. The knowledge\ngraph can be modeled by knowledge graph embedding methods, which represent\nentities and relations as embedding vectors in semantic space, then model the\ninteractions between these embedding vectors. However, the semantic structures\nin the knowledge graph embedding space are not well-studied, thus knowledge\ngraph embedding methods are usually only used for knowledge graph completion\nbut not data representation and analysis. In this paper, we propose to analyze\nthese semantic structures based on the well-studied word embedding space and\nuse them to support data exploration. We also define the semantic queries,\nwhich are algebraic operations between the embedding vectors in the knowledge\ngraph embedding space, to solve queries such as similarity and analogy between\nthe entities on the original datasets. We then design a general framework for\ndata exploration by semantic queries and discuss the solution to some\ntraditional scholarly data exploration tasks. We also propose some new\ninteresting tasks that can be solved based on the uncanny semantic structures\nof the embedding space.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 04:32:00 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Tran", "Hung Nghiep", ""], ["Takasu", "Atsuhiro", ""]]}, {"id": "1909.08231", "submitter": "EPTCS", "authors": "Richard Taupe (Siemens AG \\\"Osterreich, Vienna, Austria, and\n  Alpen-Adria-Universit\\\"at, Klagenfurt, Austria), Konstantin Schekotihin\n  (Alpen-Adria-Universit\\\"at, Klagenfurt, Austria), Peter Sch\\\"uller\n  (Technische Universit\\\"at Wien, Institut f\\\"ur Logic and Computation, KBS\n  Group), Antonius Weinzierl (Technische Universit\\\"at Wien, Institut f\\\"ur\n  Logic and Computation, KBS Group, and Aalto University, Department of\n  Computer Science), Gerhard Friedrich (Alpen-Adria-Universit\\\"at, Klagenfurt,\n  Austria)", "title": "Exploiting Partial Knowledge in Declarative Domain-Specific Heuristics\n  for ASP", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 22-35", "doi": "10.4204/EPTCS.306.9", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific heuristics are an important technique for solving\ncombinatorial problems efficiently. We propose a novel semantics for\ndeclarative specifications of domain-specific heuristics in Answer Set\nProgramming (ASP). Decision procedures that are based on a partial solution are\na frequent ingredient of existing domain-specific heuristics, e.g., for placing\nan item that has not been placed yet in bin packing. Therefore, in our novel\nsemantics negation as failure and aggregates in heuristic conditions are\nevaluated on a partial solver state. State-of-the-art solvers do not allow such\na declarative specification. Our implementation in the lazy-grounding ASP\nsystem Alpha supports heuristic directives under this semantics. By that, we\nalso provide the first implementation for incorporating declaratively specified\ndomain-specific heuristics in a lazy-grounding setting. Experiments confirm\nthat the combination of ASP solving with lazy grounding and our novel\nheuristics can be a vital ingredient for solving industrial-size problems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 06:59:51 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Taupe", "Richard", "", "Siemens AG \u00d6sterreich, Vienna, Austria, and\n  Alpen-Adria-Universit\u00e4t, Klagenfurt, Austria"], ["Schekotihin", "Konstantin", "", "Alpen-Adria-Universit\u00e4t, Klagenfurt, Austria"], ["Sch\u00fcller", "Peter", "", "Technische Universit\u00e4t Wien, Institut f\u00fcr Logic and Computation, KBS\n  Group"], ["Weinzierl", "Antonius", "", "Technische Universit\u00e4t Wien, Institut f\u00fcr\n  Logic and Computation, KBS Group, and Aalto University, Department of\n  Computer Science"], ["Friedrich", "Gerhard", "", "Alpen-Adria-Universit\u00e4t, Klagenfurt,\n  Austria"]]}, {"id": "1909.08234", "submitter": "EPTCS", "authors": "Sarthak Ghosh (Stony Brook University), C. R. Ramakrishnan (Stony\n  Brook University)", "title": "Value of Information in Probabilistic Logic Programs", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 71-84", "doi": "10.4204/EPTCS.306.14", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical decision making, we have to choose among several expensive\ndiagnostic tests such that the certainty about a patient's health is maximized\nwhile remaining within the bounds of resources like time and money. The\nexpected increase in certainty in the patient's condition due to performing a\ntest is called the value of information (VoI) for that test. In general, VoI\nrelates to acquiring additional information to improve decision-making based on\nprobabilistic reasoning in an uncertain system. This paper presents a framework\nfor acquiring information based on VoI in uncertain systems modeled as\nProbabilistic Logic Programs (PLPs). Optimal decision-making in uncertain\nsystems modeled as PLPs have already been studied before. But, acquiring\nadditional information to further improve the results of making the optimal\ndecision has remained open in this context.\n  We model decision-making in an uncertain system with a PLP and a set of\ntop-level queries, with a set of utility measures over the distributions of\nthese queries. The PLP is annotated with a set of atoms labeled as\n\"observable\"; in the medical diagnosis example, the observable atoms will be\nresults of diagnostic tests. Each observable atom has an associated cost. This\nsetting of optimally selecting observations based on VoI is more general than\nthat considered by any prior work. Given a limited budget, optimally choosing\nobservable atoms based on VoI is intractable in general. We give a greedy\nalgorithm for constructing a \"conditional plan\" of observations: a schedule\nwhere the selection of what atom to observe next depends on earlier\nobservations. We show that, preempting the algorithm anytime before completion\nprovides a usable result, the result improves over time, and, in the absence of\na well-defined budget, converges to the optimal solution.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:01:28 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Ghosh", "Sarthak", "", "Stony Brook University"], ["Ramakrishnan", "C. R.", "", "Stony\n  Brook University"]]}, {"id": "1909.08235", "submitter": "EPTCS", "authors": "Craig Olson, Yuliya Lierler", "title": "Information Extraction Tool Text2ALM: From Narratives to Action Language\n  System Descriptions", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 87-100", "doi": "10.4204/EPTCS.306.16", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we design a narrative understanding tool Text2ALM. This tool\nuses an action language ALM to perform inferences on complex interactions of\nevents described in narratives. The methodology used to implement the Text2ALM\nsystem was originally outlined by Lierler, Inclezan, and Gelfond (2017) via a\nmanual process of converting a narrative to an ALM model. It relies on a\nconglomeration of resources and techniques from two distinct fields of\nartificial intelligence, namely, natural language processing and knowledge\nrepresentation and reasoning. The effectiveness of system Text2ALM is measured\nby its ability to correctly answer questions from the bAbI tasks published by\nFacebook Research in 2015. This tool matched or exceeded the performance of\nstate-of-the-art machine learning methods in six of the seven tested tasks. We\nalso illustrate that the Text2ALM approach generalizes to a broader spectrum of\nnarratives.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:02:45 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Olson", "Craig", ""], ["Lierler", "Yuliya", ""]]}, {"id": "1909.08238", "submitter": "EPTCS", "authors": "Bin Wang (Southeast University, China), Jun Shen (Southeast\n  University, China), Shutao Zhang (Southeast University, China), Zhizheng\n  Zhang (Southeast University, China)", "title": "On the Strong Equivalences of LPMLN Programs", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646. arXiv admin note:\n  substantial text overlap with arXiv:1909.03764", "journal-ref": "EPTCS 306, 2019, pp. 114-125", "doi": "10.4204/EPTCS.306.18", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By incorporating the methods of Answer Set Programming (ASP) and Markov Logic\nNetworks (MLN), LPMLN becomes a powerful tool for non-monotonic, inconsistent\nand uncertain knowledge representation and reasoning. To facilitate the\napplications and extend the understandings of LPMLN, we investigate the strong\nequivalences between LPMLN programs in this paper, which is regarded as an\nimportant property in the field of logic programming. In the field of ASP, two\nprograms P and Q are strongly equivalent, iff for any ASP program R, the\nprograms P and Q extended by R have the same stable models. In other words, an\nASP program can be replaced by one of its strong equivalent without considering\nits context, which helps us to simplify logic programs, enhance inference\nengines, construct human-friendly knowledge bases etc. Since LPMLN is a\ncombination of ASP and MLN, the notions of strong equivalences in LPMLN is\nquite different from that in ASP. Firstly, we present the notions of p-strong\nand w-strong equivalences between LPMLN programs. Secondly, we present a\ncharacterization of the notions by generalizing the SE-model approach in ASP.\nFinally, we show the use of strong equivalences in simplifying LPMLN programs,\nand present a sufficient and necessary syntactic condition that guarantees the\nstrong equivalence between a single LPMLN rule and the empty program.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:04:00 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Wang", "Bin", "", "Southeast University, China"], ["Shen", "Jun", "", "Southeast\n  University, China"], ["Zhang", "Shutao", "", "Southeast University, China"], ["Zhang", "Zhizheng", "", "Southeast University, China"]]}, {"id": "1909.08240", "submitter": "EPTCS", "authors": "David Spies (University of Alberta), Jia-Huai You (University of\n  Alberta), Ryan Hayward (University of Alberta)", "title": "Mutex Graphs and Multicliques: Reducing Grounding Size for Planning", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 140-153", "doi": "10.4204/EPTCS.306.20", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to representing large sets of mutual exclusions, also\nknown as mutexes or mutex constraints. These are the types of constraints that\nspecify the exclusion of some properties, events, processes, and so on. They\nare ubiquitous in many areas of applications. The size of these constraints for\na given problem can be overwhelming enough to present a bottleneck for the\nsolving efficiency of the underlying solver. In this paper, we propose a novel\ngraph-theoretic technique based on multicliques for a compact representation of\nmutex constraints and apply it to domain-independent planning in ASP. As\ncomputing a minimum multiclique covering from a mutex graph is NP-hard, we\npropose an efficient approximation algorithm for multiclique covering and show\nexperimentally that it generates substantially smaller grounding size for mutex\nconstraints in ASP than the previously known work in SAT.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:04:54 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Spies", "David", "", "University of Alberta"], ["You", "Jia-Huai", "", "University of\n  Alberta"], ["Hayward", "Ryan", "", "University of Alberta"]]}, {"id": "1909.08243", "submitter": "EPTCS", "authors": "Vincent Barichard, Igor St\\'ephan", "title": "Quantified Constraint Handling Rules", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 210-223", "doi": "10.4204/EPTCS.306.25", "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We shift the QCSP (Quantified Constraint Satisfaction Problems) framework to\nthe QCHR (Quantified Constraint Handling Rules) framework by enabling dynamic\nbinder and access to user-defined constraints. QCSP offers a natural framework\nto express PSPACE problems as finite two-players games. But to define a QCSP\nmodel, the binder must be formerly known and cannot be built dynamically even\nif the worst case won't occur. To overcome this issue, we define the new QCHR\nformalism that allows to build the binder dynamically during the solving. Our\nQCHR models exhibit state-of-the-art performances on static binder and\noutperforms previous QCSP approaches when the binder is dynamic.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:07:09 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Barichard", "Vincent", ""], ["St\u00e9phan", "Igor", ""]]}, {"id": "1909.08246", "submitter": "EPTCS", "authors": "K. Tuncay Tekle (Stony Brook University), Yanhong A. Liu (Stony Brook\n  University)", "title": "Extended Magic for Negation: Efficient Demand-Driven Evaluation of\n  Stratified Datalog with Precise Complexity Guarantees", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 241-254", "doi": "10.4204/EPTCS.306.28", "report-no": null, "categories": "cs.LO cs.AI cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of Datalog rules, facts, and a query, answers to the query can be\ninferred bottom-up starting from the facts or top-down starting from the query.\nFor efficiency, top-down evaluation is extended with memoization of inferred\nfacts, and bottom-up evaluation is performed after transformations to make\nrules driven by the demand from the query. Prior work has shown their precise\ncomplexity analysis and relationships. However, when Datalog is extended with\neven stratified negation, which has a simple and universally accepted\nsemantics, transformations to make rules demand-driven may result in\nnon-stratified negation, which has had many complex semantics and evaluation\nmethods.\n  This paper presents (1) a simple extension to demand transformation, a\ntransformation to make rules demand-driven for Datalog without negation, to\nsupport stratified negation, and (2) a simple extension to an optimal bottom-up\nevaluation method for Datalog with stratified negation, to handle\nnon-stratified negation in the resulting rules. We show that the method\nprovides precise complexity guarantees. It is also optimal in that only facts\nneeded for top-down evaluation of the query are inferred and each firing of a\nrule to infer such a fact takes worst-case constant time. We extend the precise\nrelationship between top-down evaluation and demand-driven bottom-up evaluation\nto Datalog with stratified negation. Finally, we show experimental results for\nperformance, as well as applications to previously challenging examples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:07:42 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Tekle", "K. Tuncay", "", "Stony Brook University"], ["Liu", "Yanhong A.", "", "Stony Brook\n  University"]]}, {"id": "1909.08247", "submitter": "EPTCS", "authors": "Giacomo Da Col, Erich Teppan", "title": "Google vs IBM: A Constraint Solving Challenge on the Job-Shop Scheduling\n  Problem", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 259-265", "doi": "10.4204/EPTCS.306.30", "report-no": null, "categories": "cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The job-shop scheduling is one of the most studied optimization problems from\nthe dawn of computer era to the present day. Its combinatorial nature makes it\neasily expressible as a constraint satisfaction problem. In this paper, we\ncompare the performance of two constraint solvers on the job-shop scheduling\nproblem. The solvers in question are: OR-Tools, an open-source solver developed\nby Google and winner of the last MiniZinc Challenge, and CP Optimizer, a\nproprietary IBM constraint solver targeted at industrial scheduling problems.\nThe comparison is based on the goodness of the solutions found and the time\nrequired to solve the problem instances. First, we target the classic\nbenchmarks from the literature, then we carry out the comparison on a benchmark\nthat was created with known optimal solution, with size comparable to\nreal-world industrial problems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:08:10 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Da Col", "Giacomo", ""], ["Teppan", "Erich", ""]]}, {"id": "1909.08248", "submitter": "EPTCS", "authors": "Felicidad Aguado (IRLab, CITIC Research Center, University of A\n  Coru\\~na, Spain), Pedro Cabalar (IRLab, CITIC Research Center, University of\n  A Coru\\~na, Spain), Jorge Fandinno (University of Potsdam, Germany), Brais\n  Mu\\~niz (IRLab, CITIC Research Center, University of A Coru\\~na, Spain),\n  Gilberto P\\'erez (IRLab, CITIC Research Center, University of A Coru\\~na,\n  Spain), Francisco Su\\'arez (Digestive Service, Complexo Hospitalario\n  Universitario de A Coru\\~na (CHUAC), Instituto de Investigaci\\'on Biom\\'edica\n  de A Coru\\~na (INIBIC), Coru\\~na, Spain)", "title": "A Rule-Based System for Explainable Donor-Patient Matching in Liver\n  Transplantation", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 266-272", "doi": "10.4204/EPTCS.306.31", "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present web-liver, a rule-based system for decision support\nin the medical domain, focusing on its application in a liver transplantation\nunit for implementing policies for donor-patient matching. The rule-based\nsystem is built on top of an interpreter for logic programs with partial\nfunctions, called lppf, that extends the paradigm of Answer Set Programming\n(ASP) adding two main features: (1) the inclusion of partial functions and (2)\nthe computation of causal explanations for the obtained solutions. The final\ngoal of web-liver is assisting the medical experts in the design of new\ndonor-patient matching policies that take into account not only the patient\nseverity but also the transplantation utility. As an example, we illustrate the\ntool behaviour with a set of rules that implement the utility index called\nSOFT.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:08:25 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Aguado", "Felicidad", "", "IRLab, CITIC Research Center, University of A\n  Coru\u00f1a, Spain"], ["Cabalar", "Pedro", "", "IRLab, CITIC Research Center, University of\n  A Coru\u00f1a, Spain"], ["Fandinno", "Jorge", "", "University of Potsdam, Germany"], ["Mu\u00f1iz", "Brais", "", "IRLab, CITIC Research Center, University of A Coru\u00f1a, Spain"], ["P\u00e9rez", "Gilberto", "", "IRLab, CITIC Research Center, University of A Coru\u00f1a,\n  Spain"], ["Su\u00e1rez", "Francisco", "", "Digestive Service, Complexo Hospitalario\n  Universitario de A Coru\u00f1a"]]}, {"id": "1909.08250", "submitter": "EPTCS", "authors": "Van Duc Nguyen (New Mexico State University), Tran Cao Son (New Mexico\n  State University), Enrico Pontelli (New Mexico State University)", "title": "Natural Language Generation for Non-Expert Users", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 280-294", "doi": "10.4204/EPTCS.306.33", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the difficulty in presenting computational results, especially\nwhen the results are a collection of atoms in a logical language, to users, who\nare not proficient in computer programming and/or the logical representation of\nthe results, we propose a system for automatic generation of natural language\ndescriptions for applications targeting mainstream users. Differently from many\nearlier systems with the same aim, the proposed system does not employ\ntemplates for the generation task. It assumes that there exist some natural\nlanguage sentences in the application domain and uses this repository for the\nnatural language description. It does not require, however, a large corpus as\nit is often required in machine learning approaches. The systems consist of two\nmain components. The first one aims at analyzing the sentences and constructs a\nGrammatical Framework (GF) for given sentences and is implemented using the\nStanford parser and an answer set program. The second component is for sentence\nconstruction and relies on GF Library. The paper includes two use cases to\ndemostrate the capability of the system. As the sentence construction is done\nvia GF, the paper includes a use case evaluation showing that the proposed\nsystem could also be utilized in addressing a challenge to create an abstract\nWikipedia, which is recently discussed in the BlueSky session of the 2018\nInternational Semantic Web Conference.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:09:07 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Nguyen", "Van Duc", "", "New Mexico State University"], ["Son", "Tran Cao", "", "New Mexico\n  State University"], ["Pontelli", "Enrico", "", "New Mexico State University"]]}, {"id": "1909.08251", "submitter": "EPTCS", "authors": "Tarek Khaled (Aix-Marseille University), Bela\\\"id Benhamou\n  (Aix-Marseille University)", "title": "An ASP-based Approach for Attractor Enumeration in Synchronous and\n  Asynchronous Boolean Networks", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 295-301", "doi": "10.4204/EPTCS.306.34", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean networks are conventionally used to represent and simulate gene\nregulatory networks. In the analysis of the dynamic of a Boolean network, the\nattractors are the objects of a special attention. In this work, we propose a\nnovel approach based on Answer Set Programming (ASP) to express Boolean\nnetworks and simulate the dynamics of such networks. Our work focuses on the\nidentification of the attractors, it relies on the exhaustive enumeration of\nall the attractors of synchronous and asynchronous Boolean networks. We applied\nand evaluated the proposed approach on real biological networks, and the\nobtained results indicate that this novel approach is promising.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:09:27 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Khaled", "Tarek", "", "Aix-Marseille University"], ["Benhamou", "Bela\u00efd", "", "Aix-Marseille University"]]}, {"id": "1909.08252", "submitter": "EPTCS", "authors": "Liu Liu, Miroslaw Truszczynski", "title": "Encoding Selection for Solving Hamiltonian Cycle Problems with ASP", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 302-308", "doi": "10.4204/EPTCS.306.35", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common for search and optimization problems to have alternative\nequivalent encodings in ASP. Typically none of them is uniformly better than\nothers when evaluated on broad classes of problem instances. We claim that one\ncan improve the solving ability of ASP by using machine learning techniques to\nselect encodings likely to perform well on a given instance. We substantiate\nthis claim by studying the hamiltonian cycle problem. We propose several\nequivalent encodings of the problem and several classes of hard instances. We\nbuild models to predict the behavior of each encoding, and then show that\nselecting encodings for a given instance using the learned performance\npredictors leads to significant performance gains.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:09:45 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Liu", "Liu", ""], ["Truszczynski", "Miroslaw", ""]]}, {"id": "1909.08255", "submitter": "EPTCS", "authors": "Abeer Dyoub (University of L'Aquila), Stefania Costantini (University\n  of L'Aquila), Francesca A. Lisi (University of Bari)", "title": "Towards Ethical Machines Via Logic Programming", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 333-339", "doi": "10.4204/EPTCS.306.39", "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous intelligent agents are playing increasingly important roles in our\nlives. They contain information about us and start to perform tasks on our\nbehalves. Chatbots are an example of such agents that need to engage in a\ncomplex conversations with humans. Thus, we need to ensure that they behave\nethically. In this work we propose a hybrid logic-based approach for ethical\nchatbots.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:10:55 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Dyoub", "Abeer", "", "University of L'Aquila"], ["Costantini", "Stefania", "", "University\n  of L'Aquila"], ["Lisi", "Francesca A.", "", "University of Bari"]]}, {"id": "1909.08256", "submitter": "EPTCS", "authors": "Valentina Pitoni (University of L'Aquila), Stefania Costantini\n  (University of L'Aquila)", "title": "A Temporal Module for Logical Frameworks", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 340-346", "doi": "10.4204/EPTCS.306.40", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In artificial intelligence, multi agent systems constitute an interesting\ntypology of society modeling, and have in this regard vast fields of\napplication, which extend to the human sciences. Logic is often used to model\nsuch kind of systems as it is easier to verify than other approaches, and\nprovides explainability and potential validation. In this paper we define a\ntime module suitable to add time to many logic representations of agents.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:11:22 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Pitoni", "Valentina", "", "University of L'Aquila"], ["Costantini", "Stefania", "", "University of L'Aquila"]]}, {"id": "1909.08257", "submitter": "EPTCS", "authors": "Yusuf Izmirlioglu (Sabanci University, Turkey)", "title": "Reasoning about Qualitative Direction and Distance between Extended\n  Objects using Answer Set Programming", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 371-378", "doi": "10.4204/EPTCS.306.50", "report-no": null, "categories": "cs.AI cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we introduce a novel formal framework to represent and reason\nabout qualitative direction and distance relations between extended objects\nusing Answer Set Programming (ASP). We take Cardinal Directional Calculus (CDC)\nas a starting point and extend CDC with new sorts of constraints which involve\ndefaults, preferences and negation. We call this extended version as nCDC. Then\nwe further extend nCDC by augmenting qualitative distance relation and name\nthis extension as nCDC+. For CDC, nCDC, nCDC+, we introduce an ASP-based\ngeneral framework to solve consistency checking problems, address composition\nand inversion of qualitative spatial relations, infer unknown or missing\nrelations between objects, and find a suitable configuration of objects which\nfulfills a given inquiry.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:12:13 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Izmirlioglu", "Yusuf", "", "Sabanci University, Turkey"]]}, {"id": "1909.08258", "submitter": "EPTCS", "authors": "Kinjal Basu", "title": "Conversational AI : Open Domain Question Answering and Commonsense\n  Reasoning", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 396-402", "doi": "10.4204/EPTCS.306.53", "report-no": null, "categories": "cs.AI cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research is focused on making a human-like question answering system\nwhich can answer rationally. The distinguishing characteristic of our approach\nis that it will use automated common sense reasoning to truly \"understand\"\ndialogues, allowing it to converse like a human. Humans often make many\nassumptions during conversations. We infer facts not told explicitly by using\nour common sense. Incorporating commonsense knowledge in a question answering\nsystem will simply make it more robust.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:13:46 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Basu", "Kinjal", ""]]}, {"id": "1909.08259", "submitter": "EPTCS", "authors": "Francesco Fabiano (University of Udine)", "title": "Design of a Solver for Multi-Agent Epistemic Planning", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646. arXiv admin note: text\n  overlap with arXiv:1511.01960 by other authors", "journal-ref": "EPTCS 306, 2019, pp. 403-412", "doi": "10.4204/EPTCS.306.54", "report-no": null, "categories": "cs.AI cs.IT cs.LO cs.MA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the interest in Artificial Intelligence continues to grow it is becoming\nmore and more important to investigate formalization and tools that allow us to\nexploit logic to reason about the world. In particular, given the increasing\nnumber of multi-agents systems that could benefit from techniques of automated\nreasoning, exploring new ways to define not only the world's status but also\nthe agents' information is constantly growing in importance. This type of\nreasoning, i.e., about agents' perception of the world and also about agents'\nknowledge of her and others' knowledge, is referred to as epistemic reasoning.\n  In our work we will try to formalize this concept, expressed through\nepistemic logic, for dynamic domains. In particular we will attempt to define a\nnew action-based language for multi-agent epistemic planning and to implement\nan epistemic planner based on it. This solver should provide a tool flexible\nenough to be able to reason on different domains, e.g., economy, security,\njustice and politics, where reasoning about others' beliefs could lead to\nwinning strategies or help in changing a group of agents' view of the world.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:14:28 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Fabiano", "Francesco", "", "University of Udine"]]}, {"id": "1909.08260", "submitter": "EPTCS", "authors": "Francesco Pacenza (University of Calabria - Department of Mathematics\n  and Computer Science)", "title": "Reasoning in Highly Reactive Environments", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 420-426", "doi": "10.4204/EPTCS.306.57", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of my Ph.D. thesis concerns Reasoning in Highly Reactive\nEnvironments. As reasoning in highly reactive environments, we identify the\nsetting in which a knowledge-based agent, with given goals, is deployed in an\nenvironment subject to repeated, sudden and possibly unknown changes. This is\nfor instance the typical setting in which, e.g., artificial agents for\nvideo-games (the so called \"bots\"), cleaning robots, bomb clearing robots, and\nso on are deployed. In all these settings one can follow the classical approach\nin which the operations of the agent are distinguished in \"sensing\" the\nenvironment with proper interface devices, \"thinking\", and then behaving\naccordingly using proper actuators. In order to operate in an highly reactive\nenvironment, an artificial agent needs to be: 1. Responsive -> The agent must\nbe able to react repeatedly and in a reasonable amount of time; 2. Elastic ->\nThe agent must stay reactive also under varying workload; 3. Resilient -> The\nagent must stay responsive also in case of internal failure or failure of one\nof the programmed actions in the environment. Nowadays, thanks to new\ntechnologies in the field of Artificial Intelligence, it is already technically\npossible to create AI agents that are able to operate in reactive environments.\nNevertheless, several issues stay unsolved, and are subject of ongoing\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:15:09 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Pacenza", "Francesco", "", "University of Calabria - Department of Mathematics\n  and Computer Science"]]}, {"id": "1909.08263", "submitter": "EPTCS", "authors": "Marco De Bortoli (Graz University of Technology)", "title": "Distributed Answer Set Coloring: Stable Models Computation via Graph\n  Coloring", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 441-451", "doi": "10.4204/EPTCS.306.60", "report-no": null, "categories": "cs.DC cs.AI cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a famous logic language for knowledge\nrepresentation, which has been really successful in the last years, as\nwitnessed by the great interest into the development of efficient solvers for\nASP. Yet, the great request of resources for certain types of problems, as the\nplanning ones, still constitutes a big limitation for problem solving.\nParticularly, in the case the program is grounded before the resolving phase,\nan exponential blow up of the grounding can generate a huge ground file,\ninfeasible for single machines with limited resources, thus preventing even the\ndiscovering of a single non-optimal solution. To address this problem, in this\npaper we present a distributed approach to ASP solving, exploiting distributed\ncomputation benefits in order to overcome the just explained limitations. The\nhere presented tool, which is called Distributed Answer Set Coloring (DASC), is\na pure solver based on the well-known Graph Coloring algorithm. DASC is part of\na bigger project aiming to bring logic programming into a distributed system,\nstarted in 2017 by Federico Igne with mASPreduce and continued in 2018 by\nPietro Totis with a distributed grounder. In this paper we present a low level\nimplementation of the Graph Coloring algorithm, via the Boost and MPI libraries\nfor C++. Finally, we provide a few results of the very first working version of\nour tool, at the moment without any strong optimization or heuristic.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:16:15 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["De Bortoli", "Marco", "", "Graz University of Technology"]]}, {"id": "1909.08332", "submitter": "Juan Cruz Barsce", "authors": "Juan Cruz Barsce and Jorge A. Palombarini and Ernesto Mart\\'inez", "title": "A Hierarchical Two-tier Approach to Hyper-parameter Optimization in\n  Reinforcement Learning", "comments": "Short paper presented in the Jornadas Argentinas de Inform\\'atica\n  (JAIIO) 2019 (Salta, Argentina), describing an ongoing research on RL\n  hyper-parameter tuning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of hyper-parameters in reinforcement learning (RL) algorithms is\na key task, because they determine how the agent will learn its policy by\ninteracting with its environment, and thus what data is gathered. In this work,\nan approach that uses Bayesian optimization to perform a two-step optimization\nis proposed: first, categorical RL structure hyper-parameters are taken as\nbinary variables and optimized with an acquisition function tailored for such\nvariables. Then, at a lower level of abstraction, solution-level\nhyper-parameters are optimized by resorting to the expected improvement\nacquisition function, while using the best categorical hyper-parameters found\nin the optimization at the upper-level of abstraction. This two-tier approach\nis validated in a simulated control task. Results obtained are promising and\nopen the way for more user-independent applications of reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 10:14:47 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Barsce", "Juan Cruz", ""], ["Palombarini", "Jorge A.", ""], ["Mart\u00ednez", "Ernesto", ""]]}, {"id": "1909.08387", "submitter": "Jan Toenshoff", "authors": "Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, Martin Grohe", "title": "Graph Neural Networks for Maximum Constraint Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many combinatorial optimization problems can be phrased in the language of\nconstraint satisfaction problems. We introduce a graph neural network\narchitecture for solving such optimization problems. The architecture is\ngeneric; it works for all binary constraint satisfaction problems. Training is\nunsupervised, and it is sufficient to train on relatively small instances; the\nresulting networks perform well on much larger instances (at least 10-times\nlarger). We experimentally evaluate our approach for a variety of problems,\nincluding Maximum Cut and Maximum Independent Set. Despite being generic, we\nshow that our approach matches or surpasses most greedy and semi-definite\nprogramming based algorithms and sometimes even outperforms state-of-the-art\nheuristics for the specific problems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 12:17:20 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 15:29:48 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 20:17:24 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Toenshoff", "Jan", ""], ["Ritzert", "Martin", ""], ["Wolf", "Hinrikus", ""], ["Grohe", "Martin", ""]]}, {"id": "1909.08549", "submitter": "Uwe Petersohn", "authors": "Sebastian Fl\\\"ugge, Sandra Zimmer, Uwe Petersohn", "title": "Knowledge representation and diagnostic inference using Bayesian\n  networks in the medical discourse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the diagnostic inference under uncertainty Bayesian networks are\ninvestigated. The method is based on an adequate uniform representation of the\nnecessary knowledge. This includes both generic and experience-based specific\nknowledge, which is stored in a knowledge base. For knowledge processing, a\ncombination of the problem-solving methods of concept-based and case-based\nreasoning is used. Concept-based reasoning is used for the diagnosis, therapy\nand medication recommendation and evaluation of generic knowledge. Exceptions\nin the form of specific patient cases are processed by case-based reasoning. In\naddition, the use of Bayesian networks allows to deal with uncertainty,\nfuzziness and incompleteness. Thus, the valid general concepts can be issued\naccording to their probability. To this end, various inference mechanisms are\nintroduced and subsequently evaluated within the context of a developed\nprototype. Tests are employed to assess the classification of diagnoses by the\nnetwork.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 16:16:17 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Fl\u00fcgge", "Sebastian", ""], ["Zimmer", "Sandra", ""], ["Petersohn", "Uwe", ""]]}, {"id": "1909.08552", "submitter": "Dries Van Daele", "authors": "Dries Van Daele, Nicholas Decleyre, Herman Dubois, Wannes Meert", "title": "An Automated Engineering Assistant: Learning Parsers for Technical\n  Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a set of technical drawings and expert knowledge, we automatically learn\na parser to interpret such a drawing. This enables automatic reasoning and\nlearning on top of a large database of technical drawings. In this work, we\ndevelop a similarity based search algorithm to help engineers and designers\nfind or complete designs more easily and flexibly. This is part of an ongoing\neffort to build an automated engineering assistant. The proposed methods make\nuse of both neural methods to learn to interpret images, and symbolic methods\nto learn to interpret the structure in the technical drawing and incorporate\nexpert knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 16:22:08 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Van Daele", "Dries", ""], ["Decleyre", "Nicholas", ""], ["Dubois", "Herman", ""], ["Meert", "Wannes", ""]]}, {"id": "1909.08663", "submitter": "Matthew A. Kelly", "authors": "Wang Jing (Beijing Normal University), M. A. Kelly (The Pennsylvania\n  State University), David Reitter (Google Research)", "title": "Do We Need Neural Models to Explain Human Judgments of Acceptability?", "comments": "10 pages (8 pages + 2 pages of references), 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Native speakers can judge whether a sentence is an acceptable instance of\ntheir language. Acceptability provides a means of evaluating whether\ncomputational language models are processing language in a human-like manner.\nWe test the ability of computational language models, simple language features,\nand word embeddings to predict native English speakers judgments of\nacceptability on English-language essays written by non-native speakers. We\nfind that much of the sentence acceptability variance can be captured by a\ncombination of features including misspellings, word order, and word similarity\n(Pearson's r = 0.494). While predictive neural models fit acceptability\njudgments well (r = 0.527), we find that a 4-gram model with statistical\nsmoothing is just as good (r = 0.528). Thanks to incorporating a count of\nmisspellings, our 4-gram model surpasses both the previous unsupervised\nstate-of-the art (Lau et al., 2015; r = 0.472), and the average non-expert\nnative speaker (r = 0.46). Our results demonstrate that acceptability is well\ncaptured by n-gram statistics and simple language features.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 19:02:53 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 19:30:15 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Jing", "Wang", "", "Beijing Normal University"], ["Kelly", "M. A.", "", "The Pennsylvania\n  State University"], ["Reitter", "David", "", "Google Research"]]}, {"id": "1909.08691", "submitter": "Jin-Kao Hao", "authors": "Yangming Zhou, Jin-Kao Hao, Zhang-Hua Fu, Zhe Wang, and Xiangjing Lai", "title": "Variable Population Memetic Search: A Case Study on the Critical Node\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population-based memetic algorithms have been successfully applied to solve\nmany difficult combinatorial problems. Often, a population of fixed size was\nused in such algorithms to record some best solutions sampled during the\nsearch. However, given the particular features of the problem instance under\nconsideration, a population of variable size would be more suitable to ensure\nthe best search performance possible. In this work, we propose variable\npopulation memetic search (VPMS), where a strategic population sizing mechanism\nis used to dynamically adjust the population size during the memetic search\nprocess. Our VPMS approach starts its search from a small population of only\ntwo solutions to focus on exploitation, and then adapts the population size\naccording to the search status to continuously influence the balancing between\nexploitation and exploration. We illustrate an application of the VPMS approach\nto solve the challenging critical node problem (CNP). We show that the VPMS\nalgorithm integrating a variable population, an effective local optimization\nprocedure (called diversified late acceptance search) and a backbone-based\ncrossover operator performs very well compared to state-of-the-art CNP\nalgorithms. The algorithm is able to discover new upper bounds for 13 instances\nout of the 42 popular benchmark instances, while matching 23 previous\nbest-known upper bounds.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:21:02 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Zhou", "Yangming", ""], ["Hao", "Jin-Kao", ""], ["Fu", "Zhang-Hua", ""], ["Wang", "Zhe", ""], ["Lai", "Xiangjing", ""]]}, {"id": "1909.08711", "submitter": "Egemen Sert", "authors": "Egemen Sert, Yaneer Bar-Yam, Alfredo J. Morales", "title": "Segregation Dynamics with Reinforcement Learning and Agent Based\n  Modeling", "comments": "14 pages, 4 figures + supplemental material, in review", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Societies are complex. Properties of social systems can be explained by the\ninterplay and weaving of individual actions. Incentives are key to understand\npeople's choices and decisions. For instance, individual preferences of where\nto live may lead to the emergence of social segregation. In this paper, we\ncombine Reinforcement Learning (RL) with Agent Based Models (ABM) in order to\naddress the self-organizing dynamics of social segregation and explore the\nspace of possibilities that emerge from considering different types of\nincentives. Our model promotes the creation of interdependencies and\ninteractions among multiple agents of two different kinds that want to\nsegregate from each other. For this purpose, agents use Deep Q-Networks to make\ndecisions based on the rules of the Schelling Segregation model and the\nPredator-Prey model. Despite the segregation incentive, our experiments show\nthat spatial integration can be achieved by establishing interdependencies\namong agents of different kinds. They also reveal that segregated areas are\nmore probable to host older people than diverse areas, which attract younger\nones. Through this work, we show that the combination of RL and ABMs can create\nan artificial environment for policy makers to observe potential and existing\nbehaviors associated to incentives.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 21:12:03 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Sert", "Egemen", ""], ["Bar-Yam", "Yaneer", ""], ["Morales", "Alfredo J.", ""]]}, {"id": "1909.08735", "submitter": "Macheng Shen", "authors": "Macheng Shen, Jonathan P. How", "title": "Robust Opponent Modeling via Adversarial Ensemble Reinforcement Learning\n  in Asymmetric Imperfect-Information Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithmic framework for learning robust policies in\nasymmetric imperfect-information games, where the joint reward could depend on\nthe uncertain opponent type (a private information known only to the opponent\nitself and its ally). In order to maximize the reward, the protagonist agent\nhas to infer the opponent type through agent modeling. We use multiagent\nreinforcement learning (MARL) to learn opponent models through self-play, which\ncaptures the full strategy interaction and reasoning between agents. However,\nagent policies learned from self-play can suffer from mutual overfitting.\nEnsemble training methods can be used to improve the robustness of agent policy\nagainst different opponents, but it also significantly increases the\ncomputational overhead. In order to achieve a good trade-off between the\nrobustness of the learned policy and the computation complexity, we propose to\ntrain a separate opponent policy against the protagonist agent for evaluation\npurposes. The reward achieved by this opponent is a noisy measure of the\nrobustness of the protagonist agent policy due to the intrinsic stochastic\nnature of a reinforcement learner. To handle this stochasticity, we apply a\nstochastic optimization scheme to dynamically update the opponent ensemble to\noptimize an objective function that strikes a balance between robustness and\ncomputation complexity. We empirically show that, under the same limited\ncomputational budget, the proposed method results in more robust policy\nlearning than standard ensemble training.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 23:34:22 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 17:04:46 GMT"}, {"version": "v3", "created": "Thu, 12 Dec 2019 19:07:50 GMT"}, {"version": "v4", "created": "Tue, 3 Mar 2020 22:58:52 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Shen", "Macheng", ""], ["How", "Jonathan P.", ""]]}, {"id": "1909.08748", "submitter": "Yi Chen", "authors": "Yi Chen, Aimin Zhou, Swagatam Das", "title": "Utilizing Dependence among Variables in Evolutionary Algorithms for\n  Mixed-Integer Programming: A Case Study on Multi-Objective Constrained\n  Portfolio Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several real-world applications could be modeled as Mixed-Integer Non-Linear\nProgramming (MINLP) problems, and some prominent examples include portfolio\noptimization, remote sensing technology, and so on. Most of the models for\nthese applications are non-convex and always involve some conflicting\nobjectives. The mathematical and heuristic methods have their advantages in\nsolving this category of problems. In this work, we turn to Multi-Objective\nEvolutionary Algorithms (MOEAs) for finding elegant solutions for such\nproblems. In this framework, we investigate a multi-objective constrained\nportfolio optimization problem, which can be cast as a classical financial\nproblem and can also be naturally modeled as an MINLP problem. Consequently, we\npoint out one challenge, faced by a direct coding scheme for MOEAs, to this\nproblem. It is that the dependence among variables, like the selection and\nweights for one same asset, will likely make the search difficult. We thus,\npropose a Compressed Coding Scheme (CCS), compressing the two dependent\nvariables into one variable to utilize the dependence and thereby meeting this\nchallenge. Subsequently, we carry out a detailed empirical study on two sets of\ninstances. The first part consists of 5 instances from OR-Library, which is\nsolvable for the general mathematical optimizer, like CPLEX, while the\nremaining 15 instances from NGINX are addressed only by MOEAs. The two\nbenchmarks, involving the number of assets from 31 to 2235, consistently\nindicate that CCS is not only efficient but also robust for dealing with the\nconstrained multi-objective portfolio optimization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 00:37:31 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 02:26:15 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 08:47:32 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Chen", "Yi", ""], ["Zhou", "Aimin", ""], ["Das", "Swagatam", ""]]}, {"id": "1909.08766", "submitter": "Deepali Aneja", "authors": "Deepali Aneja, Daniel McDuff, Shital Shah", "title": "A High-Fidelity Open Embodied Avatar with Lip Syncing and Expression\n  Capabilities", "comments": "International Conference on Multimodal Interaction (ICMI 2019)", "journal-ref": null, "doi": "10.1145/3340555.3353744", "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied avatars as virtual agents have many applications and provide\nbenefits over disembodied agents, allowing non-verbal social and interactional\ncues to be leveraged, in a similar manner to how humans interact with each\nother. We present an open embodied avatar built upon the Unreal Engine that can\nbe controlled via a simple python programming interface. The avatar has lip\nsyncing (phoneme control), head gesture and facial expression (using either\nfacial action units or cardinal emotion categories) capabilities. We release\ncode and models to illustrate how the avatar can be controlled like a puppet or\nused to create a simple conversational agent using public application\nprogramming interfaces (APIs). GITHUB link:\nhttps://github.com/danmcduff/AvatarSim\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 01:39:39 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 05:10:21 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Aneja", "Deepali", ""], ["McDuff", "Daniel", ""], ["Shah", "Shital", ""]]}, {"id": "1909.08774", "submitter": "Nagender Aneja", "authors": "Nagender Aneja and Sandhya Aneja", "title": "Transfer Learning using CNN for Handwritten Devanagari Character\n  Recognition", "comments": null, "journal-ref": "IEEE International Conference on Advances in Information\n  Technology (ICAIT), ICAIT - 2019", "doi": "10.1109/ICAIT47043.2019.8987286", "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an analysis of pre-trained models to recognize\nhandwritten Devanagari alphabets using transfer learning for Deep Convolution\nNeural Network (DCNN). This research implements AlexNet, DenseNet, Vgg, and\nInception ConvNet as a fixed feature extractor. We implemented 15 epochs for\neach of AlexNet, DenseNet 121, DenseNet 201, Vgg 11, Vgg 16, Vgg 19, and\nInception V3. Results show that Inception V3 performs better in terms of\naccuracy achieving 99% accuracy with average epoch time 16.3 minutes while\nAlexNet performs fastest with 2.2 minutes per epoch and achieving 98\\%\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 02:04:55 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Aneja", "Nagender", ""], ["Aneja", "Sandhya", ""]]}, {"id": "1909.08776", "submitter": "Yuchen Xiao", "authors": "Yuchen Xiao, Joshua Hoffman, Tian Xia and Christopher Amato", "title": "Learning Multi-Robot Decentralized Macro-Action-Based Policies via a\n  Centralized Q-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world multi-robot tasks, high-quality solutions often require a\nteam of robots to perform asynchronous actions under decentralized control.\nDecentralized multi-agent reinforcement learning methods have difficulty\nlearning decentralized policies because of the environment appearing to be\nnon-stationary due to other agents also learning at the same time. In this\npaper, we address this challenge by proposing a macro-action-based\ndecentralized multi-agent double deep recurrent Q-net (MacDec-MADDRQN) which\ntrains each decentralized Q-net using a centralized Q-net for action selection.\nA generalized version of MacDec-MADDRQN with two separate training\nenvironments, called Parallel-MacDec-MADDRQN, is also presented to leverage\neither centralized or decentralized exploration. The advantages and the\npractical nature of our methods are demonstrated by achieving near-centralized\nresults in simulation and having real robots accomplish a warehouse tool\ndelivery task in an efficient way.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 02:14:06 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 23:08:51 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Xiao", "Yuchen", ""], ["Hoffman", "Joshua", ""], ["Xia", "Tian", ""], ["Amato", "Christopher", ""]]}, {"id": "1909.08792", "submitter": "Khaled Refaat", "authors": "Khaled S. Refaat, Kai Ding, Natalia Ponomareva, St\\'ephane Ross", "title": "Agent Prioritization for Autonomous Navigation", "comments": "8 pages, accepted to IEEE/RSJ International Conference on Robots and\n  Systems (IROS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous navigation, a planning system reasons about other agents to\nplan a safe and plausible trajectory. Before planning starts, agents are\ntypically processed with computationally intensive models for recognition,\ntracking, motion estimation and prediction. With limited computational\nresources and a large number of agents to process in real time, it becomes\nimportant to efficiently rank agents according to their impact on the decision\nmaking process. This allows spending more time processing the most important\nagents. We propose a system to rank agents around an autonomous vehicle (AV) in\nreal time. We automatically generate a ranking data set by running the planner\nin simulation on real-world logged data, where we can afford to run more\naccurate and expensive models on all the agents. The causes of various planner\nactions are logged and used for assigning ground truth importance scores. The\ngenerated data set can be used to learn ranking models. In particular, we show\nthe utility of combining learned features, via a convolutional neural network,\nwith engineered features designed to capture domain knowledge. We show the\nbenefits of various design choices experimentally. When tested on real AVs, our\nsystem demonstrates the capability of understanding complex driving situations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 03:51:57 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Refaat", "Khaled S.", ""], ["Ding", "Kai", ""], ["Ponomareva", "Natalia", ""], ["Ross", "St\u00e9phane", ""]]}, {"id": "1909.08794", "submitter": "Rajabi Masoumi Mina", "authors": "Mina Rajabi, Saeed Hossani, Fatemeh Dehghani", "title": "A literature review on current approaches and applications of fuzzy\n  expert systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purposes of this study are to distinguish the trends of research in\npublication exits for the utilisations of the fuzzy expert and knowledge-based\nsystems that is done based on the classification of studies in the last decade.\nThe present investigation covers 60 articles from related scholastic journals,\nInternational conference proceedings and some major literature review papers.\nOur outcomes reveal an upward trend in the up-to-date publications number, that\nis evidence of growing notoriety on the various applications of fuzzy expert\nsystems. This raise in the reports is mainly in the medical neuro-fuzzy and\nfuzzy expert systems. Moreover, another most critical observation is that many\nmodern industrial applications are extended, employing knowledge-based systems\nby extracting the experts' knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 03:56:49 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Rajabi", "Mina", ""], ["Hossani", "Saeed", ""], ["Dehghani", "Fatemeh", ""]]}, {"id": "1909.08824", "submitter": "Li Du", "authors": "Li Du, Xiao Ding, Ting Liu and Zhongyang Li", "title": "Modeling Event Background for If-Then Commonsense Reasoning Using\n  Context-aware Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Understanding event and event-centered commonsense reasoning are crucial for\nnatural language processing (NLP). Given an observed event, it is trivial for\nhuman to infer its intents and effects, while this type of If-Then reasoning\nstill remains challenging for NLP systems. To facilitate this, a If-Then\ncommonsense reasoning dataset Atomic is proposed, together with an RNN-based\nSeq2Seq model to conduct such reasoning. However, two fundamental problems\nstill need to be addressed: first, the intents of an event may be multiple,\nwhile the generations of RNN-based Seq2Seq models are always semantically\nclose; second, external knowledge of the event background may be necessary for\nunderstanding events and conducting the If-Then reasoning. To address these\nissues, we propose a novel context-aware variational autoencoder effectively\nlearning event background information to guide the If-Then reasoning.\nExperimental results show that our approach improves the accuracy and diversity\nof inferences compared with state-of-the-art baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 06:46:02 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 08:22:45 GMT"}, {"version": "v3", "created": "Sun, 1 Dec 2019 07:31:56 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Du", "Li", ""], ["Ding", "Xiao", ""], ["Liu", "Ting", ""], ["Li", "Zhongyang", ""]]}, {"id": "1909.08905", "submitter": "Qian Liu", "authors": "Qian Liu, Bei Chen, Haoyan Liu, Lei Fang, Jian-Guang Lou, Bin Zhou,\n  Dongmei Zhang", "title": "A Split-and-Recombine Approach for Follow-up Query Analysis", "comments": "Accepted by EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-dependent semantic parsing has proven to be an important yet\nchallenging task. To leverage the advances in context-independent semantic\nparsing, we propose to perform follow-up query analysis, aiming to restate\ncontext-dependent natural language queries with contextual information. To\naccomplish the task, we propose STAR, a novel approach with a well-designed\ntwo-phase process. It is parser-independent and able to handle multifarious\nfollow-up scenarios in different domains. Experiments on the FollowUp dataset\nshow that STAR outperforms the state-of-the-art baseline by a large margin of\nnearly 8%. The superiority on parsing results verifies the feasibility of\nfollow-up query analysis. We also explore the extensibility of STAR on the SQA\ndataset, which is very promising.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 10:16:21 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Liu", "Qian", ""], ["Chen", "Bei", ""], ["Liu", "Haoyan", ""], ["Fang", "Lei", ""], ["Lou", "Jian-Guang", ""], ["Zhou", "Bin", ""], ["Zhang", "Dongmei", ""]]}, {"id": "1909.08927", "submitter": "Shipra Sharma Ms.", "authors": "Shipra Sharma and Balwinder Sodhi", "title": "Extracting Conceptual Knowledge from Natural Language Text Using Maximum\n  Likelihood Principle", "comments": "12 pages, Under review in IEEE TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific knowledge graphs constructed from natural language text are\nubiquitous in today's world. In many such scenarios the base text, from which\nthe knowledge graph is constructed, concerns itself with practical, on-hand,\nactual or ground-reality information about the domain. Product documentation in\nsoftware engineering domain are one example of such base texts. Other examples\ninclude blogs and texts related to digital artifacts, reports on emerging\nmarkets and business models, patient medical records, etc. Though the above\nsources contain a wealth of knowledge about their respective domains, the\nconceptual knowledge on which they are based is often missing or unclear.\nAccess to this conceptual knowledge can enormously increase the utility of\navailable data and assist in several tasks such as knowledge graph completion,\ngrounding, querying, etc.\n  Our contributions in this paper are twofold. First, we propose a novel\nMarkovian stochastic model for document generation from conceptual knowledge.\nThe uniqueness of our approach lies in the fact that the conceptual knowledge\nin the writer's mind forms a component of the parameter set of our stochastic\nmodel. Secondly, we solve the inverse problem of learning the best conceptual\nknowledge from a given document, by finding model parameters which maximize the\nlikelihood of generating the specific document over all possible parameter\nvalues. This likelihood maximization is done using an application of Baum-Welch\nalgorithm, which is a known special case of Expectation-Maximization (EM)\nalgorithm. We run our conceptualization algorithm on several well-known natural\nlanguage sources and obtain very encouraging results. The results of our\nextensive experiments concur with the hypothesis that the information contained\nin these sources has a well-defined and rigorous underlying conceptual\nstructure, which can be discovered using our method.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 11:54:55 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Sharma", "Shipra", ""], ["Sodhi", "Balwinder", ""]]}, {"id": "1909.08965", "submitter": "Newres Al Haider", "authors": "Newres Al Haider, Dilhan Thilakarathne and Joost Bosman", "title": "Solving Financial Regulatory Compliance Using Software Contracts", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring compliance with various laws and regulations is of utmost priority\nfor financial institutions. Traditional methods in this area have been shown to\nbe inefficient. Manual processing does not scale well. Automated efforts are\nhindered due to the lack of formalization of domain knowledge and problems of\nintegrating such knowledge into software systems. In this work we propose an\napproach to tackle these issues by encoding them into software contracts using\na Controlled Natural Language. In particular, we encode a portion of the Money\nMarket Statistical Reporting (MMSR) regulations into contracts specified by the\nclojure.spec framework. We show how various features of a contract framework,\nin particular clojure.spec, can help to tackle issues that occur when dealing\nwith compliance: validation with explanations and test data generation. We\nbenchmark our proposed solution and show that this approach can effectively\nsolve compliance issues in this particular use case.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 03:26:29 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Haider", "Newres Al", ""], ["Thilakarathne", "Dilhan", ""], ["Bosman", "Joost", ""]]}, {"id": "1909.08975", "submitter": "Dieuwke Hupkes", "authors": "Jaap Jumelet, Willem Zuidema and Dieuwke Hupkes", "title": "Analysing Neural Language Models: Contextual Decomposition Reveals\n  Default Reasoning in Number and Gender Assignment", "comments": "To appear at CoNLL2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive research has recently shown that recurrent neural language models\nare able to process a wide range of grammatical phenomena. How these models are\nable to perform these remarkable feats so well, however, is still an open\nquestion. To gain more insight into what information LSTMs base their decisions\non, we propose a generalisation of Contextual Decomposition (GCD). In\nparticular, this setup enables us to accurately distil which part of a\nprediction stems from semantic heuristics, which part truly emanates from\nsyntactic cues and which part arise from the model biases themselves instead.\nWe investigate this technique on tasks pertaining to syntactic agreement and\nco-reference resolution and discover that the model strongly relies on a\ndefault reasoning effect to perform these tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 13:24:29 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Jumelet", "Jaap", ""], ["Zuidema", "Willem", ""], ["Hupkes", "Dieuwke", ""]]}, {"id": "1909.08996", "submitter": "Andrea Loreggia", "authors": "Cristina Cornelio, Michele Donini, Andrea Loreggia, Maria Silvia Pini\n  and Francesca Rossi", "title": "Voting with Random Classifiers (VORACE): Theoretical and Experimental\n  Analysis", "comments": null, "journal-ref": "Autonomous Agents and Multi-Agent Systems volume 35, Article\n  number: 22 (2021)", "doi": "10.1007/s10458-021-09504-y", "report-no": null, "categories": "cs.LG cs.AI cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many machine learning scenarios, looking for the best classifier that fits\na particular dataset can be very costly in terms of time and resources.\nMoreover, it can require deep knowledge of the specific domain. We propose a\nnew technique which does not require profound expertise in the domain and\navoids the commonly used strategy of hyper-parameter tuning and model\nselection. Our method is an innovative ensemble technique that uses voting\nrules over a set of randomly-generated classifiers. Given a new input sample,\nwe interpret the output of each classifier as a ranking over the set of\npossible classes. We then aggregate these output rankings using a voting rule,\nwhich treats them as preferences over the classes. We show that our approach\nobtains good results compared to the state-of-the-art, both providing a\ntheoretical analysis and an empirical evaluation of the approach on several\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 08:13:53 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 09:37:08 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 08:00:06 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Cornelio", "Cristina", ""], ["Donini", "Michele", ""], ["Loreggia", "Andrea", ""], ["Pini", "Maria Silvia", ""], ["Rossi", "Francesca", ""]]}, {"id": "1909.08998", "submitter": "EPTCS", "authors": "Joohyung Lee (Arizona State University), Man Luo (Arizona State\n  University)", "title": "Strong Equivalence for LPMLN Programs", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646. arXiv admin note: text\n  overlap with arXiv:1905.07550", "journal-ref": "EPTCS 306, 2019, pp. 196-209", "doi": "10.4204/EPTCS.306.24", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LPMLN is a probabilistic extension of answer set programs with the weight\nscheme adapted from Markov Logic. We study the concept of strong equivalence in\nLPMLN, which is a useful mathematical tool for simplifying a part of an LPMLN\nprogram without looking at the rest of it. We show that the verification of\nstrong equivalence in LPMLN can be reduced to equivalence checking in classical\nlogic via a reduct and choice rules as well as to equivalence checking under\nthe \"soft\" logic of here-and-there. The result allows us to leverage an answer\nset solver for LPMLN strong equivalence checking. The study also suggests us a\nfew reformulations of the LPMLN semantics using choice rules, the logic of\nhere-and-there, and classical logic.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:06:51 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Lee", "Joohyung", "", "Arizona State University"], ["Luo", "Man", "", "Arizona State\n  University"]]}, {"id": "1909.09017", "submitter": "EPTCS", "authors": "Farhad Shakerin (The University of Texas at Dallas)", "title": "Induction of Non-monotonic Logic Programs To Explain Statistical\n  Learning Models", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646. arXiv admin note:\n  substantial text overlap with arXiv:1808.00629, arXiv:1905.11226,\n  arXiv:1802.06462, arXiv:1707.02693", "journal-ref": "EPTCS 306, 2019, pp. 379-388", "doi": "10.4204/EPTCS.306.51", "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast and scalable algorithm to induce non-monotonic logic\nprograms from statistical learning models. We reduce the problem of search for\nbest clauses to instances of the High-Utility Itemset Mining (HUIM) problem. In\nthe HUIM problem, feature values and their importance are treated as\ntransactions and utilities respectively. We make use of TreeExplainer, a fast\nand scalable implementation of the Explainable AI tool SHAP, to extract locally\nimportant features and their weights from ensemble tree models. Our experiments\nwith UCI standard benchmarks suggest a significant improvement in terms of\nclassification evaluation metrics and running time of the training algorithm\ncompared to ALEPH, a state-of-the-art Inductive Logic Programming (ILP) system.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:12:42 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Shakerin", "Farhad", "", "The University of Texas at Dallas"]]}, {"id": "1909.09018", "submitter": "Kuruparan Shanmugalingam", "authors": "Kuruparan Shanmugalingam, Nisal Chandrasekara, Calvin Hindle, Gihan\n  Fernando, Chanaka Gunawardhana", "title": "Corporate IT-support Help-Desk Process Hybrid-Automation Solution with\n  Machine Learning Approach", "comments": "7 pages, 8 Figures, 2 Tables", "journal-ref": "The International Conference on Digital Image Computing:\n  Techniques and Applications (DICTA) 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehensive IT support teams in large scale organizations require more man\npower for handling engagement and requests of employees from different channels\non a 24*7 basis. Automated email technical queries help desk is proposed to\nhave instant real-time quick solutions and email categorisation. Email topic\nmodelling with various machine learning, deep-learning approaches are compared\nwith different features for a scalable, generalised solution along with\nsure-shot static rules. Email's title, body, attachment, OCR text, and some\nfeature engineered custom features are given as input elements. XGBoost\ncascaded hierarchical models, Bi-LSTM model with word embeddings perform well\nshowing 77.3 overall accuracy For the real world corporate email data set. By\nintroducing the thresholding techniques, the overall automation system\narchitecture provides 85.6 percentage of accuracy for real world corporate\nemails. Combination of quick fixes, static rules, ML categorization as a low\ncost inference solution reduces 81 percentage of the human effort in the\nprocess of automation and real time implementation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 10:07:01 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Shanmugalingam", "Kuruparan", ""], ["Chandrasekara", "Nisal", ""], ["Hindle", "Calvin", ""], ["Fernando", "Gihan", ""], ["Gunawardhana", "Chanaka", ""]]}, {"id": "1909.09064", "submitter": "Xudong Liu", "authors": "Joseph Allen, Ahmed Moussa, Xudong Liu", "title": "Human-In-The-Loop Learning of Qualitative Preference Models", "comments": "Published in the Proceedings of the 32nd International Florida\n  Artificial Intelligence Research Society Conference, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel human-in-the-loop framework to help the\nhuman user understand the decision making process that involves choosing\npreferred options. We focus on qualitative preference models over alternatives\nfrom combinatorial domains. This framework is interactive: the user provides\nher behavioral data to the framework, and the framework explains the learned\nmodel to the user. It is iterative: the framework collects feedback on the\nlearned model from the user and tries to improve it accordingly till the user\nterminates the iteration. In order to communicate the learned preference model\nto the user, we develop visualization of intuitive and explainable graphic\nmodels, such as lexicographic preference trees and forests, and conditional\npreference networks. To this end, we discuss key aspects of our framework for\nlexicographic preference models.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 16:02:40 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Allen", "Joseph", ""], ["Moussa", "Ahmed", ""], ["Liu", "Xudong", ""]]}, {"id": "1909.09065", "submitter": "Adrien Bennetot", "authors": "Adrien Bennetot, Jean-Luc Laurent, Raja Chatila, Natalia\n  D\\'iaz-Rodr\\'iguez", "title": "Towards Explainable Neural-Symbolic Visual Reasoning", "comments": "Accepted at IJCAI19 Neural-Symbolic Learning and Reasoning Workshop\n  (https://sites.google.com/view/nesy2019/home)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high-performance models suffer from a lack of interpretability. There\nhas been an increasing influx of work on explainable artificial intelligence\n(XAI) in order to disentangle what is meant and expected by XAI. Nevertheless,\nthere is no general consensus on how to produce and judge explanations. In this\npaper, we discuss why techniques integrating connectionist and symbolic\nparadigms are the most efficient solutions to produce explanations for\nnon-technical users and we propose a reasoning model, based on definitions by\nDoran et al. [2017] (arXiv:1710.00794) to explain a neural network's decision.\nWe use this explanation in order to correct bias in the network's decision\nrationale. We accompany this model with an example of its potential use, based\non the image captioning method in Burns et al. [2018] (arXiv:1803.09797).\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 16:04:57 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 15:23:49 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Bennetot", "Adrien", ""], ["Laurent", "Jean-Luc", ""], ["Chatila", "Raja", ""], ["D\u00edaz-Rodr\u00edguez", "Natalia", ""]]}, {"id": "1909.09070", "submitter": "Jose Manuel Gomez-Perez", "authors": "Jose Manuel Gomez-Perez and Raul Ortega", "title": "Look, Read and Enrich. Learning from Scientific Figures and their\n  Captions", "comments": "Accepted in the 10th International Conference on Knowledge capture\n  (K-CAP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to natural images, understanding scientific figures is particularly\nhard for machines. However, there is a valuable source of information in\nscientific literature that until now has remained untapped: the correspondence\nbetween a figure and its caption. In this paper we investigate what can be\nlearnt by looking at a large number of figures and reading their captions, and\nintroduce a figure-caption correspondence learning task that makes use of our\nobservations. Training visual and language networks without supervision other\nthan pairs of unconstrained figures and captions is shown to successfully solve\nthis task. We also show that transferring lexical and semantic knowledge from a\nknowledge graph significantly enriches the resulting features. Finally, we\ndemonstrate the positive impact of such features in other tasks involving\nscientific text and figures, like multi-modal classification and machine\ncomprehension for question answering, outperforming supervised baselines and\nad-hoc approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 16:10:15 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Gomez-Perez", "Jose Manuel", ""], ["Ortega", "Raul", ""]]}, {"id": "1909.09072", "submitter": "Xudong Liu", "authors": "Ahmed Moussa, Xudong Liu", "title": "Learning Optimal and Near-Optimal Lexicographic Preference Lists", "comments": "Published in the Proceedings of the 32nd International Florida\n  Artificial Intelligence Research Society Conference, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning problems of an intuitive and concise preference model,\ncalled lexicographic preference lists (LP-lists). Given a set of examples that\nare pairwise ordinal preferences over a universe of objects built of attributes\nof discrete values, we want to learn (1) an optimal LP-list that decides the\nmaximum number of these examples, or (2) a near-optimal LP-list that decides as\nmany examples as it can. To this end, we introduce a dynamic programming based\nalgorithm and a genetic algorithm for these two learning problems,\nrespectively. Furthermore, we empirically demonstrate that the sub-optimal\nmodels computed by the genetic algorithm very well approximate the de facto\noptimal models computed by our dynamic programming based algorithm, and that\nthe genetic algorithm outperforms the baseline greedy heuristic with higher\naccuracy predicting new preferences.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 16:10:46 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Moussa", "Ahmed", ""], ["Liu", "Xudong", ""]]}, {"id": "1909.09137", "submitter": "Agnieszka Maria S{\\l}owik", "authors": "Agnieszka S{\\l}owik, Chaitanya Mangla, Mateja Jamnik, Sean B. Holden,\n  Lawrence C. Paulson", "title": "Bayesian Optimisation with Gaussian Processes for Premise Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heuristics in theorem provers are often parameterised. Modern theorem provers\nsuch as Vampire utilise a wide array of heuristics to control the search space\nexplosion, thereby requiring optimisation of a large set of parameters. An\nexhaustive search in this multi-dimensional parameter space is intractable in\nmost cases, yet the performance of the provers is highly dependent on the\nparameter assignment. In this work, we introduce a principled probablistic\nframework for heuristics optimisation in theorem provers. We present results\nusing a heuristic for premise selection and The Archive of Formal Proofs (AFP)\nas a case study.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 18:41:47 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["S\u0142owik", "Agnieszka", ""], ["Mangla", "Chaitanya", ""], ["Jamnik", "Mateja", ""], ["Holden", "Sean B.", ""], ["Paulson", "Lawrence C.", ""]]}, {"id": "1909.09141", "submitter": "Elliot Creager", "authors": "Elliot Creager, David Madras, Toniann Pitassi, Richard Zemel", "title": "Causal Modeling for Fairness in Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many application areas---lending, education, and online recommenders, for\nexample---fairness and equity concerns emerge when a machine learning system\ninteracts with a dynamically changing environment to produce both immediate and\nlong-term effects for individuals and demographic groups. We discuss causal\ndirected acyclic graphs (DAGs) as a unifying framework for the recent\nliterature on fairness in such dynamical systems. We show that this formulation\naffords several new directions of inquiry to the modeler, where causal\nassumptions can be expressed and manipulated. We emphasize the importance of\ncomputing interventional quantities in the dynamical fairness setting, and show\nhow causal assumptions enable simulation (when environment dynamics are known)\nand off-policy estimation (when dynamics are unknown) of intervention on short-\nand long-term outcomes, at both the group and individual levels.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 20:21:56 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 17:43:02 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Creager", "Elliot", ""], ["Madras", "David", ""], ["Pitassi", "Toniann", ""], ["Zemel", "Richard", ""]]}, {"id": "1909.09142", "submitter": "Sai Krishnan Chandrasekar", "authors": "Hao Ren, Sai Krishnan Chandrasekar, Anitha Murugesan", "title": "Using Quantifier Elimination to Enhance the Safety Assurance of Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in the field of Machine Learning and Deep Neural Networks (DNNs) has\nenabled rapid development of sophisticated and autonomous systems. However, the\ninherent complexity to rigorously assure the safe operation of such systems\nhinders their real-world adoption in safety-critical domains such as aerospace\nand medical devices. Hence, there is a surge in interest to explore the use of\nadvanced mathematical techniques such as formal methods to address this\nchallenge. In fact, the initial results of such efforts are promising. Along\nthese lines, we propose the use of quantifier elimination (QE) - a formal\nmethod technique, as a complimentary technique to the state-of-the-art static\nanalysis and verification procedures. Using an airborne collision avoidance DNN\nas a case example, we illustrate the use of QE to formulate the precise range\nforward propagation through a network as well as analyze its robustness. We\ndiscuss the initial results of this ongoing work and explore the future\npossibilities of extending this approach and/or integrating it with other\napproaches to perform advanced safety assurance of DNNs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 20:54:10 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Ren", "Hao", ""], ["Chandrasekar", "Sai Krishnan", ""], ["Murugesan", "Anitha", ""]]}, {"id": "1909.09143", "submitter": "Deepak Muralidharan", "authors": "Deepak Muralidharan, Justine Kao, Xiao Yang, Lin Li, Lavanya\n  Viswanathan, Mubarak Seyed Ibrahim, Kevin Luikens, Stephen Pulman, Ashish\n  Garg, Atish Kothari and Jason Williams", "title": "Leveraging User Engagement Signals For Entity Labeling in a Virtual\n  Assistant", "comments": "NeurIPS 2018 Conversational AI Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal assistant AI systems such as Siri, Cortana, and Alexa have become\nwidely used as a means to accomplish tasks through natural language commands.\nHowever, components in these systems generally rely on supervised machine\nlearning algorithms that require large amounts of hand-annotated training data,\nwhich is expensive and time consuming to collect. The ability to incorporate\nunsupervised, weakly supervised, or distantly supervised data holds significant\npromise in overcoming this bottleneck. In this paper, we describe a framework\nthat leverages user engagement signals (user behaviors that demonstrate a\npositive or negative response to content) to automatically create granular\nentity labels for training data augmentation. Strategies such as multi-task\nlearning and validation using an external knowledge base are employed to\nincorporate the engagement annotated data and to boost the model's accuracy on\na sequence labeling task. Our results show that learning from data\nautomatically labeled by user engagement signals achieves significant accuracy\ngains in a production deep learning system, when measured on both the sequence\nlabeling task as well as on user facing results produced by the system\nend-to-end. We believe this is the first use of user engagement signals to help\ngenerate training data for a sequence labeling task on a large scale, and can\nbe applied in practical settings to speed up new feature deployment when little\nhuman annotated data is available.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 21:02:35 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Muralidharan", "Deepak", ""], ["Kao", "Justine", ""], ["Yang", "Xiao", ""], ["Li", "Lin", ""], ["Viswanathan", "Lavanya", ""], ["Ibrahim", "Mubarak Seyed", ""], ["Luikens", "Kevin", ""], ["Pulman", "Stephen", ""], ["Garg", "Ashish", ""], ["Kothari", "Atish", ""], ["Williams", "Jason", ""]]}, {"id": "1909.09151", "submitter": "Kevin Guelton", "authors": "Dalel Jabri (CRESTIC), Kevin Guelton (CRESTIC), Noureddine Manamanni\n  (CRESTIC), Mohammed Naceur Abdelkrim", "title": "Synth\\`ese non quadratique H$\\infty$ de contr\\^oleurs d\\'ecentralis\\'es\n  pour un ensemble de descripteurs flous T-S interconnect\\'es", "comments": "in French", "journal-ref": "18{\\`e}mes Rencontres francophones sur la Logique Floue et ses\n  Applications (LFA'09), Nov 2009, Annecy, France", "doi": null, "report-no": null, "categories": "cs.AI math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the non-quadratic decentralized stabilization of a set\nof n Takagi-Sugeno descriptors. To ensure the stability of the whole\nclosed-loop dynamics and to minimize interconnection effects between\nsubsystems, the main result allows designing a network of non Parallel\nDistributed Compensation control laws via a H $\\infty$ criterion. Sufficient\nconditions, obtained from a non quadratic fuzzy Lyapunov approach, are provided\nin terms of Linear Matrix Inequalities. Finally, a numerical example\nillustrates the efficiency of the proposed decentralized control approach.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 10:02:44 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Jabri", "Dalel", "", "CRESTIC"], ["Guelton", "Kevin", "", "CRESTIC"], ["Manamanni", "Noureddine", "", "CRESTIC"], ["Abdelkrim", "Mohammed Naceur", ""]]}, {"id": "1909.09172", "submitter": "Peixin Chang", "authors": "Peixin Chang, Shuijing Liu, Haonan Chen, Katherine Driggs-Campbell", "title": "Robot Sound Interpretation: Combining Sight and Sound in Learning-Based\n  Control", "comments": "Published as a conference paper in IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore the interpretation of sound for robot decision making, inspired by\nhuman speech comprehension. While previous methods separate sound processing\nunit and robot controller, we propose an end-to-end deep neural network which\ndirectly interprets sound commands for visual-based decision making. The\nnetwork is trained using reinforcement learning with auxiliary losses on the\nsight and sound networks. We demonstrate our approach on two robots, a\nTurtleBot3 and a Kuka-IIWA arm, which hear a command word, identify the\nassociated target object, and perform precise control to reach the target. For\nboth robots, we show the effectiveness of our network in generalization to\nsound types and robotic tasks empirically. We successfully transfer the policy\nlearned in simulator to a real-world TurtleBot3.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 18:00:22 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 02:28:07 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Chang", "Peixin", ""], ["Liu", "Shuijing", ""], ["Chen", "Haonan", ""], ["Driggs-Campbell", "Katherine", ""]]}, {"id": "1909.09186", "submitter": "Sitao Luan", "authors": "Sitao Luan, Xiao-Wen Chang, Doina Precup", "title": "Revisit Policy Optimization in Matrix Form", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In tabular case, when the reward and environment dynamics are known, policy\nevaluation can be written as $\\bm{V}_{\\bm{\\pi}} = (I - \\gamma\nP_{\\bm{\\pi}})^{-1} \\bm{r}_{\\bm{\\pi}}$, where $P_{\\bm{\\pi}}$ is the state\ntransition matrix given policy ${\\bm{\\pi}}$ and $\\bm{r}_{\\bm{\\pi}}$ is the\nreward signal given ${\\bm{\\pi}}$. What annoys us is that $P_{\\bm{\\pi}}$ and\n$\\bm{r}_{\\bm{\\pi}}$ are both mixed with ${\\bm{\\pi}}$, which means every time\nwhen we update ${\\bm{\\pi}}$, they will change together. In this paper, we\nleverage the notation from \\cite{wang2007dual} to disentangle ${\\bm{\\pi}}$ and\nenvironment dynamics which makes optimization over policy more straightforward.\nWe show that policy gradient theorem \\cite{sutton2018reinforcement} and TRPO\n\\cite{schulman2015trust} can be put into a more general framework and such\nnotation has good potential to be extended to model-based reinforcement\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 18:43:56 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Luan", "Sitao", ""], ["Chang", "Xiao-Wen", ""], ["Precup", "Doina", ""]]}, {"id": "1909.09209", "submitter": "EPTCS", "authors": "Daoming Lyu (Auburn University), Fangkai Yang (NVIDIA Corporation), Bo\n  Liu (Auburn University), Steven Gustafson (Maana Inc.)", "title": "A Human-Centered Data-Driven Planner-Actor-Critic Architecture via Logic\n  Programming", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646. arXiv admin note:\n  significant text overlap with arXiv:1906.07268", "journal-ref": "EPTCS 306, 2019, pp. 182-195", "doi": "10.4204/EPTCS.306.23", "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes of Reinforcement Learning (RL) allow an agent to learn\npolicies that surpass human experts but suffers from being time-hungry and\ndata-hungry. By contrast, human learning is significantly faster because prior\nand general knowledge and multiple information resources are utilized. In this\npaper, we propose a Planner-Actor-Critic architecture for huMAN-centered\nplanning and learning (PACMAN), where an agent uses its prior, high-level,\ndeterministic symbolic knowledge to plan for goal-directed actions, and also\nintegrates the Actor-Critic algorithm of RL to fine-tune its behavior towards\nboth environmental rewards and human feedback. This work is the first unified\nframework where knowledge-based planning, RL, and human teaching jointly\ncontribute to the policy learning of an agent. Our experiments demonstrate that\nPACMAN leads to a significant jump-start at the early stage of learning,\nconverges rapidly and with small variance, and is robust to inconsistent,\ninfrequent, and misleading feedback.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:06:06 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Lyu", "Daoming", "", "Auburn University"], ["Yang", "Fangkai", "", "NVIDIA Corporation"], ["Liu", "Bo", "", "Auburn University"], ["Gustafson", "Steven", "", "Maana Inc."]]}, {"id": "1909.09213", "submitter": "EPTCS", "authors": "Fabio Tardivo (New Mexico State University)", "title": "Experimenting with Constraint Programming on GPU", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 427-432", "doi": "10.4204/EPTCS.306.58", "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of my PhD thesis is on exploring parallel approaches to efficiently\nsolve problems modeled by constraints and presenting a new proposal. Current\nsolvers are very advanced; they are carefully designed to effectively manage\nthe high-level problems' description and include refined strategies to avoid\nuseless work. Despite this, finding a solution can take an unacceptable amount\nof time. Parallelization can mitigate this problem when the instance of the\nproblem modeled is large, as it happens in real world problems. It is done by\npropagating constraints in parallel and concurrently exploring different parts\nof the search space. I am developing on a constraint solver that exploits the\nmany cores available on Graphics Processing Units (GPU) to speed up the search.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 19:43:51 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Tardivo", "Fabio", "", "New Mexico State University"]]}, {"id": "1909.09220", "submitter": "Yi-An Lai", "authors": "Yi-An Lai, Arshit Gupta and Yi Zhang", "title": "Goal-Embedded Dual Hierarchical Model for Task-Oriented Dialogue\n  Generation", "comments": "Accepted by CoNLL-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical neural networks are often used to model inherent structures\nwithin dialogues. For goal-oriented dialogues, these models miss a mechanism\nadhering to the goals and neglect the distinct conversational patterns between\ntwo interlocutors. In this work, we propose Goal-Embedded Dual Hierarchical\nAttentional Encoder-Decoder (G-DuHA) able to center around goals and capture\ninterlocutor-level disparity while modeling goal-oriented dialogues.\nExperiments on dialogue generation, response generation, and human evaluations\ndemonstrate that the proposed model successfully generates higher-quality, more\ndiverse and goal-centric dialogues. Moreover, we apply data augmentation via\ngoal-oriented dialogue generation for task-oriented dialog systems with better\nperformance achieved.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 20:12:10 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Lai", "Yi-An", ""], ["Gupta", "Arshit", ""], ["Zhang", "Yi", ""]]}, {"id": "1909.09228", "submitter": "James Hare", "authors": "James Z. Hare and Cesar A. Uribe and Lance Kaplan and Ali Jadbabaie", "title": "Non-Bayesian Social Learning with Uncertain Models", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.3006755", "report-no": null, "categories": "cs.AI cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Bayesian social learning theory provides a framework that models\ndistributed inference for a group of agents interacting over a social network.\nIn this framework, each agent iteratively forms and communicates beliefs about\nan unknown state of the world with their neighbors using a learning rule.\nExisting approaches assume agents have access to precise statistical models (in\nthe form of likelihoods) for the state of the world. However in many\nsituations, such models must be learned from finite data. We propose a social\nlearning rule that takes into account uncertainty in the statistical models\nusing second-order probabilities. Therefore, beliefs derived from uncertain\nmodels are sensitive to the amount of past evidence collected for each\nhypothesis. We characterize how well the hypotheses can be tested on a social\nnetwork, as consistent or not with the state of the world. We explicitly show\nthe dependency of the generated beliefs with respect to the amount of prior\nevidence. Moreover, as the amount of prior evidence goes to infinity, learning\noccurs and is consistent with traditional social learning theory.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:58:50 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 18:19:15 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Hare", "James Z.", ""], ["Uribe", "Cesar A.", ""], ["Kaplan", "Lance", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1909.09268", "submitter": "Hassan Kane", "authors": "Hassan Kan\\'e, Yusuf Kocyigit, Pelkins Ajanoh, Ali Abdalla, Mohamed\n  Coulibali", "title": "Towards Neural Language Evaluators", "comments": "Accepted to NeurIPS 2019 Document Intelligence Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review three limitations of BLEU and ROUGE -- the most popular metrics\nused to assess reference summaries against hypothesis summaries, come up with\ncriteria for what a good metric should behave like and propose concrete ways to\nuse recent Transformers-based Language Models to assess reference summaries\nagainst hypothesis summaries.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 00:24:59 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 19:56:02 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Kan\u00e9", "Hassan", ""], ["Kocyigit", "Yusuf", ""], ["Ajanoh", "Pelkins", ""], ["Abdalla", "Ali", ""], ["Coulibali", "Mohamed", ""]]}, {"id": "1909.09282", "submitter": "William Lewis Ii", "authors": "W. Cannon Lewis II, Mark Moll, and Lydia E. Kavraki", "title": "How Much Do Unstated Problem Constraints Limit Deep Robotic\n  Reinforcement Learning?", "comments": "Rice University technical report", "journal-ref": null, "doi": "10.25611/az5z-xt37", "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning is a promising paradigm for robotic control which\nhas been shown to be capable of learning policies for high-dimensional,\ncontinuous control of unmodeled systems. However, RoboticReinforcement Learning\ncurrently lacks clearly defined benchmark tasks, which makes it difficult for\nresearchers to reproduce and compare against prior work. ``Reacher'' tasks,\nwhich are fundamental to robotic manipulation, are commonly used as benchmarks,\nbut the lack of a formal specification elides details that are crucial to\nreplication. In this paper we present a novel empirical analysis which shows\nthat the unstated spatial constraints in commonly used implementations of\nReacher tasks make it dramatically easier to learn a successful control policy\nwith DeepDeterministic Policy Gradients (DDPG), a state-of-the-art Deep RL\nalgorithm. Our analysis suggests that less constrained Reacher tasks are\nsignificantly more difficult to learn, and hence that existing de facto\nbenchmarks are not representative of the difficulty of general robotic\nmanipulation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 01:12:25 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Lewis", "W. Cannon", "II"], ["Moll", "Mark", ""], ["Kavraki", "Lydia E.", ""]]}, {"id": "1909.09291", "submitter": "Monireh Dabaghchian", "authors": "Monireh Dabaghchian, Amir Alipour-Fanid, Kai Zeng", "title": "Intelligent Policing Strategy for Traffic Violation Prevention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Police officer presence at an intersection discourages a potential traffic\nviolator from violating the law. It also alerts the motorists' consciousness to\ntake precaution and follow the rules. However, due to the abundant\nintersections and shortage of human resources, it is not possible to assign a\npolice officer to every intersection. In this paper, we propose an intelligent\nand optimal policing strategy for traffic violation prevention. Our model\nconsists of a specific number of targeted intersections and two police officers\nwith no prior knowledge on the number of the traffic violations in the\ndesignated intersections. At each time interval, the proposed strategy, assigns\nthe two police officers to different intersections such that at the end of the\ntime horizon, maximum traffic violation prevention is achieved. Our proposed\nmethodology adapts the PROLA (Play and Random Observe Learning Algorithm)\nalgorithm [1] to achieve an optimal traffic violation prevention strategy.\nFinally, we conduct a case study to evaluate and demonstrate the performance of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 01:39:17 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Dabaghchian", "Monireh", ""], ["Alipour-Fanid", "Amir", ""], ["Zeng", "Kai", ""]]}, {"id": "1909.09295", "submitter": "Jingxi Xu", "authors": "David Watkins-Valls, Jingxi Xu, Nicholas Waytowich and Peter Allen", "title": "Learning Your Way Without Map or Compass: Panoramic Target Driven Visual\n  Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robot navigation system that uses an imitation learning\nframework to successfully navigate in complex environments. Our framework takes\na pre-built 3D scan of a real environment and trains an agent from\npre-generated expert trajectories to navigate to any position given a panoramic\nview of the goal and the current visual input without relying on map, compass,\nodometry, or relative position of the target at runtime. Our end-to-end trained\nagent uses RGB and depth (RGBD) information and can handle large environments\n(up to $1031m^2$) across multiple rooms (up to $40$) and generalizes to unseen\ntargets. We show that when compared to several baselines our method (1)\nrequires fewer training examples and less training time, (2) reaches the goal\nlocation with higher accuracy, and (3) produces better solutions with shorter\npaths for long-range navigation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 02:17:20 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 08:52:13 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Watkins-Valls", "David", ""], ["Xu", "Jingxi", ""], ["Waytowich", "Nicholas", ""], ["Allen", "Peter", ""]]}, {"id": "1909.09362", "submitter": "Zhe Zeng Miss", "authors": "Zhe Zeng, Fanqi Yan, Paolo Morettin, Antonio Vergari, Guy Van den\n  Broeck", "title": "Hybrid Probabilistic Inference with Logical Constraints: Tractability\n  and Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted model integration (WMI) is a very appealing framework for\nprobabilistic inference: it allows to express the complex dependencies of\nreal-world hybrid scenarios where variables are heterogeneous in nature (both\ncontinuous and discrete) via the language of Satisfiability Modulo Theories\n(SMT); as well as computing probabilistic queries with arbitrarily complex\nlogical constraints. Recent work has shown WMI inference to be reducible to a\nmodel integration (MI) problem, under some assumptions, thus effectively\nallowing hybrid probabilistic reasoning by volume computations. In this paper,\nwe introduce a novel formulation of MI via a message passing scheme that allows\nto efficiently compute the marginal densities and statistical moments of all\nthe variables in linear time. As such, we are able to amortize inference for\narbitrarily rich MI queries when they conform to the problem structure, here\nrepresented as the primal graph associated to the SMT formula. Furthermore, we\ntheoretically trace the tractability boundaries of exact MI. Indeed, we prove\nthat in terms of the structural requirements on the primal graph that make our\nMI algorithm tractable - bounding its diameter and treewidth - the bounds are\nnot only sufficient, but necessary for tractable inference via MI.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 07:56:29 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 07:19:49 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zeng", "Zhe", ""], ["Yan", "Fanqi", ""], ["Morettin", "Paolo", ""], ["Vergari", "Antonio", ""], ["Broeck", "Guy Van den", ""]]}, {"id": "1909.09418", "submitter": "Bogdan Trasnea", "authors": "Bogdan Trasnea and Claudiu Pozna and Sorin Grigorescu", "title": "AIBA: An AI Model for Behavior Arbitration in Autonomous Driving", "comments": "12 pages", "journal-ref": "MIWAI 2019 THE 13TH MULTI-DISCIPLINARY INTERNATIONAL CONFERENCE ON\n  ARTIFICIAL INTELLIGENCE MIWAI 2019 - THE 13TH MULTI-DISCIPLINARY\n  INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, NOVEMBER 17-19, 2019.\n  KUALA LUMPUR, MALAYSIA", "doi": "10.1007/978-3-030-33709-4_17", "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving in dynamically changing traffic is a highly challenging task for\nautonomous vehicles, especially in crowded urban roadways. The Artificial\nIntelligence (AI) system of a driverless car must be able to arbitrate between\ndifferent driving strategies in order to properly plan the car's path, based on\nan understandable traffic scene model. In this paper, an AI behavior\narbitration algorithm for Autonomous Driving (AD) is proposed. The method,\ncoined AIBA (AI Behavior Arbitration), has been developed in two stages: (i)\nhuman driving scene description and understanding and (ii) formal modelling.\nThe description of the scene is achieved by mimicking a human cognition model,\nwhile the modelling part is based on a formal representation which approximates\nthe human driver understanding process. The advantage of the formal\nrepresentation is that the functional safety of the system can be analytically\ninferred. The performance of the algorithm has been evaluated in Virtual Test\nDrive (VTD), a comprehensive traffic simulator, and in GridSim, a vehicle\nkinematics engine for prototypes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 10:40:14 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 13:51:17 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Trasnea", "Bogdan", ""], ["Pozna", "Claudiu", ""], ["Grigorescu", "Sorin", ""]]}, {"id": "1909.09454", "submitter": "EPTCS", "authors": "Valentina Pitoni (University of L'Aquila)", "title": "Memory Management in Resource-Bounded Agents", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646. arXiv admin note:\n  substantial text overlap with arXiv:1909.08256", "journal-ref": "EPTCS 306, 2019, pp. 452-460", "doi": "10.4204/EPTCS.306.61", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In artificial intelligence, multi agent systems constitute an interesting\ntypology of society modeling, and have in this regard vast fields of\napplication, which extend to the human sciences. Logic is often used to model\nsuch kind of systems as it is easier to verify the explainability and\nvalidation, so for this reason we have tried to manage agents' memory extending\na previous work by inserting the concept of time.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:16:45 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Pitoni", "Valentina", "", "University of L'Aquila"]]}, {"id": "1909.09461", "submitter": "Cedric Buron", "authors": "C\\'edric Buron, Zahia Guessoum (CRESTIC), Sylvain Ductor (UECE)", "title": "MCTS-based Automated Negotiation Agent", "comments": null, "journal-ref": "The 22nd International Conference on Principles and Practice of\n  Multi-Agent Systems (PRIMA2019), Oct 2019, Torino, Italy", "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new negotiating agent model for automated\nnegotiation. We focus on applications without time pressure with\nmultidi-mensional negotiation on both continuous and discrete domains. The\nagent bidding strategy relies on Monte Carlo Tree Search, which is a trendy\nmethod since it has been used with success on games with high branching factor\nsuch as Go. It also exploits opponent modeling techniques thanks to Gaussian\nprocess regression and Bayesian learning. Evaluation is done by confronting the\nexisting agents that are able to negotiate in such context: Random Walker,\nTit-for-tat and Nice Tit-for-Tat. None of those agents succeeds in beating our\nagent. Also, the modular and adaptive nature of our approach is a huge\nadvantage when it comes to optimize it in specific applicative contexts.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 11:32:48 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Buron", "C\u00e9dric", "", "CRESTIC"], ["Guessoum", "Zahia", "", "CRESTIC"], ["Ductor", "Sylvain", "", "UECE"]]}, {"id": "1909.09484", "submitter": "Tian Lan", "authors": "Tian Lan and Xianling Mao and Heyan Huang", "title": "Generative Dialog Policy for Task-oriented Dialog Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing demand for task-oriented dialogue systems which can\nassist users in various activities such as booking tickets and restaurant\nreservations. In order to complete dialogues effectively, dialogue policy plays\na key role in task-oriented dialogue systems. As far as we know, the existing\ntask-oriented dialogue systems obtain the dialogue policy through\nclassification, which can assign either a dialogue act and its corresponding\nparameters or multiple dialogue acts without their corresponding parameters for\na dialogue action. In fact, a good dialogue policy should construct multiple\ndialogue acts and their corresponding parameters at the same time. However,\nit's hard for existing classification-based methods to achieve this goal. Thus,\nto address the issue above, we propose a novel generative dialogue policy\nlearning method. Specifically, the proposed method uses attention mechanism to\nfind relevant segments of given dialogue context and input utterance and then\nconstructs the dialogue policy by a seq2seq way for task-oriented dialogue\nsystems. Extensive experiments on two benchmark datasets show that the proposed\nmodel significantly outperforms the state-of-the-art baselines. In addition, we\nhave publicly released our codes.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 15:50:56 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Lan", "Tian", ""], ["Mao", "Xianling", ""], ["Huang", "Heyan", ""]]}, {"id": "1909.09552", "submitter": "Tong Wu", "authors": "Tong Wu, Liang Tong, Yevgeniy Vorobeychik", "title": "Defending Against Physically Realizable Attacks on Image Classification", "comments": "camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of defending deep neural network approaches for image\nclassification from physically realizable attacks. First, we demonstrate that\nthe two most scalable and effective methods for learning robust models,\nadversarial training with PGD attacks and randomized smoothing, exhibit very\nlimited effectiveness against three of the highest profile physical attacks.\nNext, we propose a new abstract adversarial model, rectangular occlusion\nattacks, in which an adversary places a small adversarially crafted rectangle\nin an image, and develop two approaches for efficiently computing the resulting\nadversarial examples. Finally, we demonstrate that adversarial training using\nour new attack yields image classification models that exhibit high robustness\nagainst the physically realizable attacks we study, offering the first\neffective generic defense against such attacks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 15:11:09 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 20:07:55 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Wu", "Tong", ""], ["Tong", "Liang", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1909.09588", "submitter": "Rene Schaub", "authors": "Rene Schaub", "title": "What are Neural Networks made of?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of Deep Learning methods is not well understood, though various\nattempts at explaining it have been made, typically centered on properties of\nstochastic gradient descent. Even less clear is why certain neural network\narchitectures perform better than others. We provide a potential opening with\nthe hypothesis that neural network training is a form of Genetic Programming.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 21:59:26 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Schaub", "Rene", ""]]}, {"id": "1909.09616", "submitter": "Hankz Hankui Zhuo", "authors": "Xinghua Zheng, Ming Tang, Hankz Hankui Zhuo, Kevin X. Wen", "title": "Repositioning Bikes with Carrier Vehicles and Bike Trailers in Bike\n  Sharing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bike Sharing Systems (BSSs) have been adopted in many major cities of the\nworld due to traffic congestion and carbon emissions. Although there have been\napproaches to exploiting either bike trailers via crowdsourcing or carrier\nvehicles to reposition bikes in the ``right'' stations in the ``right'' time,\nthey do not jointly consider the usage of both bike trailers and carrier\nvehicles. In this paper, we aim to take advantage of both bike trailers and\ncarrier vehicles to reduce the loss of demand with regard to the crowdsourcing\nof bike trailers and the fuel cost of carrier vehicles. In the experiment, we\nexhibit that our approach outperforms baselines in several datasets from bike\nsharing companies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 17:09:29 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Zheng", "Xinghua", ""], ["Tang", "Ming", ""], ["Zhuo", "Hankz Hankui", ""], ["Wen", "Kevin X.", ""]]}, {"id": "1909.09656", "submitter": "Arber Zela", "authors": "Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas\n  Brox, Frank Hutter", "title": "Understanding and Robustifying Differentiable Architecture Search", "comments": "In: International Conference on Learning Representations (ICLR 2020);\n  28 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable Architecture Search (DARTS) has attracted a lot of attention\ndue to its simplicity and small search costs achieved by a continuous\nrelaxation and an approximation of the resulting bi-level optimization problem.\nHowever, DARTS does not work robustly for new problems: we identify a wide\nrange of search spaces for which DARTS yields degenerate architectures with\nvery poor test performance. We study this failure mode and show that, while\nDARTS successfully minimizes validation loss, the found solutions generalize\npoorly when they coincide with high validation loss curvature in the\narchitecture space. We show that by adding one of various types of\nregularization we can robustify DARTS to find solutions with less curvature and\nbetter generalization properties. Based on these observations, we propose\nseveral simple variations of DARTS that perform substantially more robustly in\npractice. Our observations are robust across five search spaces on three image\nclassification tasks and also hold for the very different domains of disparity\nestimation (a dense regression task) and language modelling.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 18:03:06 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 14:14:05 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Zela", "Arber", ""], ["Elsken", "Thomas", ""], ["Saikia", "Tonmoy", ""], ["Marrakchi", "Yassine", ""], ["Brox", "Thomas", ""], ["Hutter", "Frank", ""]]}, {"id": "1909.09677", "submitter": "Zhe Huang", "authors": "Zhe Huang, Weijiang Yu, Wayne Zhang, Litong Feng, Nong Xiao", "title": "Gradual Network for Single Image De-raining", "comments": "In Proceedings of the 27th ACM International Conference on Multimedia\n  (MM 2019)", "journal-ref": null, "doi": "10.1145/3343031.3350883", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most advances in single image de-raining meet a key challenge, which is\nremoving rain streaks with different scales and shapes while preserving image\ndetails. Existing single image de-raining approaches treat rain-streak removal\nas a process of pixel-wise regression directly. However, they are lacking in\nmining the balance between over-de-raining (e.g. removing texture details in\nrain-free regions) and under-de-raining (e.g. leaving rain streaks). In this\npaper, we firstly propose a coarse-to-fine network called Gradual Network\n(GraNet) consisting of coarse stage and fine stage for delving into single\nimage de-raining with different granularities. Specifically, to reveal\ncoarse-grained rain-streak characteristics (e.g. long and thick rain\nstreaks/raindrops), we propose a coarse stage by utilizing local-global spatial\ndependencies via a local-global subnetwork composed of region-aware blocks.\nTaking the residual result (the coarse de-rained result) between the rainy\nimage sample (i.e. the input data) and the output of coarse stage (i.e. the\nlearnt rain mask) as input, the fine stage continues to de-rain by removing the\nfine-grained rain streaks (e.g. light rain streaks and water mist) to get a\nrain-free and well-reconstructed output image via a unified contextual merging\nsub-network with dense blocks and a merging block. Solid and comprehensive\nexperiments on synthetic and real data demonstrate that our GraNet can\nsignificantly outperform the state-of-the-art methods by removing rain streaks\nwith various densities, scales and shapes while keeping the image details of\nrain-free regions well-preserved.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 18:56:08 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Huang", "Zhe", ""], ["Yu", "Weijiang", ""], ["Zhang", "Wayne", ""], ["Feng", "Litong", ""], ["Xiao", "Nong", ""]]}, {"id": "1909.09678", "submitter": "Argyris Kalogeratos", "authors": "Mathilde Fekom, Nicolas Vayatis, Argyris Kalogeratos", "title": "Sequential Dynamic Resource Allocation for Epidemic Control", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.DM cs.SI cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the Dynamic Resource Allocation (DRA) model, an administrator has the\nmission to allocate dynamically a limited budget of resources to the nodes of a\nnetwork in order to reduce a diffusion process (DP) (e.g. an epidemic). The\nstandard DRA assumes that the administrator has constantly full information and\ninstantaneous access to the entire network. Towards bringing such strategies\ncloser to real-life constraints, we first present the Restricted DRA model\nextension where, at each intervention round, the access is restricted to only a\nfraction of the network nodes, called sample. Then, inspired by sequential\nselection problems such as the well-known Secretary Problem, we propose the\nSequential DRA (SDRA) model. Our model introduces a sequential aspect in the\ndecision process over the sample of each round, offering a completely new\nperspective to the dynamic DP control. Finally, we incorporate several\nsequential selection algorithms to SDRA control strategies and compare their\nperformance in SIS epidemic simulations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 18:56:31 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Fekom", "Mathilde", ""], ["Vayatis", "Nicolas", ""], ["Kalogeratos", "Argyris", ""]]}, {"id": "1909.09690", "submitter": "Hossein Keshavarz", "authors": "Hossein Keshavarz, Shohreh Tabatabayi Seifi, Mohammad Izadi", "title": "A Deep Learning-Based Approach for Measuring the Domain Similarity of\n  Persian Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for measuring the degree of\nsimilarity between categories of two pieces of Persian text, which were\npublished as descriptions of two separate advertisements. We built an\nappropriate dataset for this work using a dataset which consists of\nadvertisements posted on an e-commerce website. We generated a significant\nnumber of paired texts from this dataset and assigned each pair a score from 0\nto 3, which demonstrates the degree of similarity between the domains of the\npair. In this work, we represent words with word embedding vectors derived from\nword2vec. Then deep neural network models are used to represent texts.\nEventually, we employ concatenation of absolute difference and bit-wise\nmultiplication and a fully-connected neural network to produce a probability\ndistribution vector for the score of the pairs. Through a supervised learning\napproach, we trained our model on a GPU, and our best model achieved an F1\nscore of 0.9865.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 16:29:14 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 06:20:11 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Keshavarz", "Hossein", ""], ["Seifi", "Shohreh Tabatabayi", ""], ["Izadi", "Mohammad", ""]]}, {"id": "1909.09696", "submitter": "Tuan Manh Lai", "authors": "Tuan Lai, Quan Hung Tran, Trung Bui, Daisuke Kihara", "title": "A Gated Self-attention Memory Network for Answer Selection", "comments": "Accepted at the 2019 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer selection is an important research problem, with applications in many\nareas. Previous deep learning based approaches for the task mainly adopt the\nCompare-Aggregate architecture that performs word-level comparison followed by\naggregation. In this work, we take a departure from the popular\nCompare-Aggregate architecture, and instead, propose a new gated self-attention\nmemory network for the task. Combined with a simple transfer learning technique\nfrom a large-scale online corpus, our model outperforms previous methods by a\nlarge margin, achieving new state-of-the-art results on two standard answer\nselection datasets: TrecQA and WikiQA.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 17:56:58 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Lai", "Tuan", ""], ["Tran", "Quan Hung", ""], ["Bui", "Trung", ""], ["Kihara", "Daisuke", ""]]}, {"id": "1909.09700", "submitter": "Weijia Shi", "authors": "Weijia Shi, Muhao Chen, Pei Zhou and Kai-Wei Chang", "title": "Retrofitting Contextualized Word Embeddings with Paraphrases", "comments": null, "journal-ref": "EMNLP-IJCNLP2019", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextualized word embedding models, such as ELMo, generate meaningful\nrepresentations of words and their context. These models have been shown to\nhave a great impact on downstream applications. However, in many cases, the\ncontextualized embedding of a word changes drastically when the context is\nparaphrased. As a result, the downstream model is not robust to paraphrasing\nand other linguistic variations. To enhance the stability of contextualized\nword embedding models, we propose an approach to retrofitting contextualized\nembedding models with paraphrase contexts. Our method learns an orthogonal\ntransformation on the input space, which seeks to minimize the variance of word\nrepresentations on paraphrased contexts. Experiments show that the retrofitted\nmodel significantly outperforms the original ELMo on various sentence\nclassification and language inference tasks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 22:35:53 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Shi", "Weijia", ""], ["Chen", "Muhao", ""], ["Zhou", "Pei", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "1909.09704", "submitter": "Stefan Hosein", "authors": "Stefan Hosein, Daniel Andor, and Ryan McDonald", "title": "Measuring Domain Portability and ErrorPropagation in Biomedical QA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we present Google's submission to the BioASQ 7 biomedical\nquestion answering (QA) task (specifically Task 7b, Phase B). The core of our\nsystems are based on BERT QA models, specifically the model of\n\\cite{alberti2019bert}. In this report, and via our submissions, we aimed to\ninvestigate two research questions. We start by studying how domain portable\nare QA systems that have been pre-trained and fine-tuned on general texts,\ne.g., Wikipedia. We measure this via two submissions. The first is a\nnon-adapted model that uses a public pre-trained BERT model and is fine-tuned\non the Natural Questions data set \\cite{kwiatkowski2019natural}. The second\nsystem takes this non-adapted model and fine-tunes it with the BioASQ training\ndata. Next, we study the impact of error propagation in end-to-end retrieval\nand QA systems. Again we test this via two submissions. The first uses human\nannotated relevant documents and snippets as input to the model and the second\npredicted documents and snippets. Our main findings are that domain specific\nfine-tuning can benefit Biomedical QA. However, the biggest quality bottleneck\nis at the retrieval stage, where we see large drops in metrics -- over 10pts\nabsolute -- when using non gold inputs to the QA model.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 17:25:24 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 09:23:45 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Hosein", "Stefan", ""], ["Andor", "Daniel", ""], ["McDonald", "Ryan", ""]]}, {"id": "1909.09705", "submitter": "Hossein K. Mousavi", "authors": "Hossein K. Mousavi, Guangyi Liu, Weihang Yuan, Martin Tak\\'a\\v{c},\n  H\\'ector Mu\\~noz-Avila, Nader Motee", "title": "A Layered Architecture for Active Perception: Image Classification using\n  Deep Reinforcement Learning", "comments": "Submitted to ICRA-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a planning and perception mechanism for a robot (agent), that can\nonly observe the underlying environment partially, in order to solve an image\nclassification problem. A three-layer architecture is suggested that consists\nof a meta-layer that decides the intermediate goals, an action-layer that\nselects local actions as the agent navigates towards a goal, and a\nclassification-layer that evaluates the reward and makes a prediction. We\ndesign and implement these layers using deep reinforcement learning. A\ngeneralized policy gradient algorithm is utilized to learn the parameters of\nthese layers to maximize the expected reward. Our proposed methodology is\ntested on the MNIST dataset of handwritten digits, which provides us with a\nlevel of explainability while interpreting the agent's intermediate goals and\ncourse of action.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 19:52:41 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Mousavi", "Hossein K.", ""], ["Liu", "Guangyi", ""], ["Yuan", "Weihang", ""], ["Tak\u00e1\u010d", "Martin", ""], ["Mu\u00f1oz-Avila", "H\u00e9ctor", ""], ["Motee", "Nader", ""]]}, {"id": "1909.09706", "submitter": "Hassan Hafez Kolahi", "authors": "Hassan Hafez-Kolahi, Shohreh Kasaei, Mahdiyeh Soleymani-Baghshah", "title": "Do Compressed Representations Generalize Better?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most studied problems in machine learning is finding reasonable\nconstraints that guarantee the generalization of a learning algorithm. These\nconstraints are usually expressed as some simplicity assumptions on the target.\nFor instance, in the Vapnik-Chervonenkis (VC) theory the space of possible\nhypotheses is considered to have a limited VC dimension. In this paper, the\nconstraint on the entropy $H(X)$ of the input variable $X$ is studied as a\nsimplicity assumption. It is proven that the sample complexity to achieve an\n$\\epsilon$-$\\delta$ Probably Approximately Correct (PAC) hypothesis is bounded\nby $\\frac{2^{\n\\left.6H(X)\\middle/\\epsilon\\right.}+\\log{\\frac{1}{\\delta}}}{\\epsilon^2}$ which\nis sharp up to the $\\frac{1}{\\epsilon^2}$ factor. Morever, it is shown that if\na feature learning process is employed to learn the compressed representation\nfrom the dataset, this bound no longer exists. These findings have important\nimplications on the Information Bottleneck (IB) theory which had been utilized\nto explain the generalization power of Deep Neural Networks (DNNs), but its\napplicability for this purpose is currently under debate by researchers. In\nparticular, this is a rigorous proof for the previous heuristic that compressed\nrepresentations are exponentially easier to be learned. However, our analysis\npinpoints two factors preventing the IB, in its current form, to be applicable\nin studying neural networks. Firstly, the exponential dependence of sample\ncomplexity on $\\frac{1}{\\epsilon}$, which can lead to a dramatic effect on the\nbounds in practical applications when $\\epsilon$ is small. Secondly, our\nanalysis reveals that arguments based on input compression are inherently\ninsufficient to explain generalization of methods like DNNs in which the\nfeatures are also learned using available data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 19:54:42 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 09:38:27 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Hafez-Kolahi", "Hassan", ""], ["Kasaei", "Shohreh", ""], ["Soleymani-Baghshah", "Mahdiyeh", ""]]}, {"id": "1909.09708", "submitter": "Tomas Veloz", "authors": "Tomas Veloz, Xiazhao Zhao, Diederik Aerts", "title": "Measuring Conceptual Entanglement in Collections of Documents", "comments": "14 pages, 3 figures, Symposium Quantum Interaction 2013", "journal-ref": "In International Symposium on Quantum Interaction (pp. 134-146).\n  Springer, Berlin, Heidelberg (2013, July)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conceptual entanglement is a crucial phenomenon in quantum cognition because\nit implies that classical probabilities cannot model non--compositional\nconceptual phenomena. While several psychological experiments have been\ndeveloped to test conceptual entanglement, this has not been explored in the\ncontext of Natural Language Processing. In this paper, we apply the hypothesis\nthat words of a document are traces of the concepts that a person has in mind\nwhen writing the document. Therefore, if these concepts are entangled, we\nshould be able to observe traces of their entanglement in the documents. In\nparticular, we test conceptual entanglement by contrasting language simulations\nwith results obtained from a text corpus. Our analysis indicates that\nconceptual entanglement is strongly linked to the way in which language is\nstructured. We discuss the implications of this finding in the context of\nconceptual modeling and of Natural Language Processing.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 20:22:15 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Veloz", "Tomas", ""], ["Zhao", "Xiazhao", ""], ["Aerts", "Diederik", ""]]}, {"id": "1909.09731", "submitter": "M Sadegh Riazi", "authors": "M. Sadegh Riazi and Kim Laine and Blake Pelton and Wei Dai", "title": "HEAX: An Architecture for Computing on Encrypted Data", "comments": "To appear in proceedings of ACM ASPLOS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase in cloud computing, concerns surrounding data\nprivacy, security, and confidentiality also have been increased significantly.\nNot only cloud providers are susceptible to internal and external hacks, but\nalso in some scenarios, data owners cannot outsource the computation due to\nprivacy laws such as GDPR, HIPAA, or CCPA. Fully Homomorphic Encryption (FHE)\nis a groundbreaking invention in cryptography that, unlike traditional\ncryptosystems, enables computation on encrypted data without ever decrypting\nit. However, the most critical obstacle in deploying FHE at large-scale is the\nenormous computation overhead.\n  In this paper, we present HEAX, a novel hardware architecture for FHE that\nachieves unprecedented performance improvement. HEAX leverages multiple levels\nof parallelism, ranging from ciphertext-level to fine-grained modular\narithmetic level. Our first contribution is a new highly-parallelizable\narchitecture for number-theoretic transform (NTT) which can be of independent\ninterest as NTT is frequently used in many lattice-based cryptography systems.\nBuilding on top of NTT engine, we design a novel architecture for computation\non homomorphically encrypted data. We also introduce several techniques to\nenable an end-to-end, fully pipelined design as well as reducing on-chip memory\nconsumption. Our implementation on reconfigurable hardware demonstrates\n164-268x performance improvement for a wide range of FHE parameters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 22:27:06 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 21:17:05 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Riazi", "M. Sadegh", ""], ["Laine", "Kim", ""], ["Pelton", "Blake", ""], ["Dai", "Wei", ""]]}, {"id": "1909.09742", "submitter": "Paul Tarau", "authors": "Paul Tarau and Eduardo Blanco", "title": "Dependency-based Text Graphs for Keyphrase and Summary Extraction with\n  Applications to Interactive Content Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a bridge between neural network-based machine learning and\ngraph-based natural language processing and introduce a unified approach to\nkeyphrase, summary and relation extraction by aggregating dependency graphs\nfrom links provided by a deep-learning based dependency parser.\n  We reorganize dependency graphs to focus on the most relevant content\nelements of a sentence, integrate sentence identifiers as graph nodes and after\nranking the graph, we extract our keyphrases and summaries from its largest\nstrongly-connected component. We take advantage of the implicit structural\ninformation that dependency links bring to extract subject-verb-object, is-a\nand part-of relations.\n  We put it all together into a proof-of-concept dialog engine that specializes\nthe text graph with respect to a query and reveals interactively the document's\nmost relevant content elements. The open-source code of the integrated system\nis available at https://github.com/ptarau/DeepRank .\n  Keywords: graph-based natural language processing, dependency graphs,\nkeyphrase, summary and relation extraction, query-driven salient sentence\nextraction, logic-based dialog engine, synergies between neural and symbolic\nprocessing.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 23:57:31 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Tarau", "Paul", ""], ["Blanco", "Eduardo", ""]]}, {"id": "1909.09743", "submitter": "Jianshu Chen", "authors": "Shiyang Li, Jianshu Chen, Dian Yu", "title": "Teaching Pretrained Models with Commonsense Reasoning: A Preliminary\n  KB-Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pretrained language models (e.g., BERT) have achieved great success\non many downstream natural language understanding tasks and exhibit a certain\nlevel of commonsense reasoning ability. However, their performance on\ncommonsense tasks is still far from that of humans. As a preliminary attempt,\nwe propose a simple yet effective method to teach pretrained models with\ncommonsense reasoning by leveraging the structured knowledge in ConceptNet, the\nlargest commonsense knowledge base (KB). Specifically, the structured knowledge\nin KB allows us to construct various logical forms, and then generate\nmultiple-choice questions requiring commonsense logical reasoning. Experimental\nresults demonstrate that, when refined on these training examples, the\npretrained models consistently improve their performance on tasks that require\ncommonsense reasoning, especially in the few-shot learning setting. Besides, we\nalso perform analysis to understand which logical relations are more relevant\nto commonsense reasoning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 23:58:11 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Li", "Shiyang", ""], ["Chen", "Jianshu", ""], ["Yu", "Dian", ""]]}, {"id": "1909.09756", "submitter": "Sameer Kumar", "authors": "Sameer Kumar, Victor Bitorff, Dehao Chen, Chiachen Chou, Blake\n  Hechtman, HyoukJoong Lee, Naveen Kumar, Peter Mattson, Shibo Wang, Tao Wang,\n  Yuanzhong Xu, Zongwei Zhou", "title": "Scale MLPerf-0.6 models on Google TPU-v3 Pods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent submission of Google TPU-v3 Pods to the industry wide MLPerf v0.6\ntraining benchmark demonstrates the scalability of a suite of industry relevant\nML models. MLPerf defines a suite of models, datasets and rules to follow when\nbenchmarking to ensure results are comparable across hardware, frameworks and\ncompanies. Using this suite of models, we discuss the optimizations and\ntechniques including choice of optimizer, spatial partitioning and weight\nupdate sharding necessary to scale to 1024 TPU chips. Furthermore, we identify\nproperties of models that make scaling them challenging, such as limited data\nparallelism and unscaled weights. These optimizations contribute to record\nperformance in transformer, Resnet-50 and SSD in the Google MLPerf-0.6\nsubmission.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 01:12:38 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 17:03:37 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 18:37:01 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Kumar", "Sameer", ""], ["Bitorff", "Victor", ""], ["Chen", "Dehao", ""], ["Chou", "Chiachen", ""], ["Hechtman", "Blake", ""], ["Lee", "HyoukJoong", ""], ["Kumar", "Naveen", ""], ["Mattson", "Peter", ""], ["Wang", "Shibo", ""], ["Wang", "Tao", ""], ["Xu", "Yuanzhong", ""], ["Zhou", "Zongwei", ""]]}, {"id": "1909.09758", "submitter": "Yue Ning", "authors": "Ameya Vaidya, Feng Mai, Yue Ning", "title": "Empirical Analysis of Multi-Task Learning for Reducing Model Bias in\n  Toxic Comment Detection", "comments": "ICWSM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent rise of toxicity in online conversations on social media\nplatforms, using modern machine learning algorithms for toxic comment detection\nhas become a central focus of many online applications. Researchers and\ncompanies have developed a variety of models to identify toxicity in online\nconversations, reviews, or comments with mixed successes. However, many\nexisting approaches have learned to incorrectly associate non-toxic comments\nthat have certain trigger-words (e.g. gay, lesbian, black, muslim) as a\npotential source of toxicity. In this paper, we evaluate several\nstate-of-the-art models with the specific focus of reducing model bias towards\nthese commonly-attacked identity groups. We propose a multi-task learning model\nwith an attention layer that jointly learns to predict the toxicity of a\ncomment as well as the identities present in the comments in order to reduce\nthis bias. We then compare our model to an array of shallow and deep-learning\nmodels using metrics designed especially to test for unintended model bias\nwithin these identity groups.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 01:27:32 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 14:49:42 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 16:37:43 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Vaidya", "Ameya", ""], ["Mai", "Feng", ""], ["Ning", "Yue", ""]]}, {"id": "1909.09785", "submitter": "Hunter Lang", "authors": "Hunter Lang, Pengchuan Zhang, Lin Xiao", "title": "Using Statistics to Automate Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the development of numerous adaptive optimizers, tuning the learning\nrate of stochastic gradient methods remains a major roadblock to obtaining good\npractical performance in machine learning. Rather than changing the learning\nrate at each iteration, we propose an approach that automates the most common\nhand-tuning heuristic: use a constant learning rate until \"progress stops,\"\nthen drop. We design an explicit statistical test that determines when the\ndynamics of stochastic gradient descent reach a stationary distribution. This\ntest can be performed easily during training, and when it fires, we decrease\nthe learning rate by a constant multiplicative factor. Our experiments on\nseveral deep learning tasks demonstrate that this statistical adaptive\nstochastic approximation (SASA) method can automatically find good learning\nrate schedules and match the performance of hand-tuned methods using default\nsettings of its parameters. The statistical testing helps to control the\nvariance of this procedure and improves its robustness.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 07:27:48 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Lang", "Hunter", ""], ["Zhang", "Pengchuan", ""], ["Xiao", "Lin", ""]]}, {"id": "1909.09788", "submitter": "Albert Gatt", "authors": "Somaye Jafaritazehjani and Albert Gatt and Marc Tanti", "title": "Visuallly Grounded Generation of Entailments from Premises", "comments": "Proceedings of the 12th International Conference on Natural Language\n  Generation (INLG 2019), 11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Inference (NLI) is the task of determining the semantic\nrelationship between a premise and a hypothesis. In this paper, we focus on the\n{\\em generation} of hypotheses from premises in a multimodal setting, to\ngenerate a sentence (hypothesis) given an image and/or its description\n(premise) as the input. The main goals of this paper are (a) to investigate\nwhether it is reasonable to frame NLI as a generation task; and (b) to consider\nthe degree to which grounding textual premises in visual information is\nbeneficial to generation. We compare different neural architectures, showing\nthrough automatic and human evaluation that entailments can indeed be generated\nsuccessfully. We also show that multimodal models outperform unimodal models in\nthis task, albeit marginally.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 07:56:09 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Jafaritazehjani", "Somaye", ""], ["Gatt", "Albert", ""], ["Tanti", "Marc", ""]]}, {"id": "1909.09849", "submitter": "Shayegan Omidshafiei", "authors": "Mark Rowland, Shayegan Omidshafiei, Karl Tuyls, Julien Perolat, Michal\n  Valko, Georgios Piliouras, Remi Munos", "title": "Multiagent Evaluation under Incomplete Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the evaluation of learned multiagent strategies in\nthe incomplete information setting, which plays a critical role in ranking and\ntraining of agents. Traditionally, researchers have relied on Elo ratings for\nthis purpose, with recent works also using methods based on Nash equilibria.\nUnfortunately, Elo is unable to handle intransitive agent interactions, and\nother techniques are restricted to zero-sum, two-player settings or are limited\nby the fact that the Nash equilibrium is intractable to compute. Recently, a\nranking method called {\\alpha}-Rank, relying on a new graph-based\ngame-theoretic solution concept, was shown to tractably apply to general games.\nHowever, evaluations based on Elo or {\\alpha}-Rank typically assume noise-free\ngame outcomes, despite the data often being collected from noisy simulations,\nmaking this assumption unrealistic in practice. This paper investigates\nmultiagent evaluation in the incomplete information regime, involving\ngeneral-sum many-player games with noisy outcomes. We derive sample complexity\nguarantees required to confidently rank agents in this setting. We propose\nadaptive algorithms for accurate ranking, provide correctness and sample\ncomplexity guarantees, then introduce a means of connecting uncertainties in\nnoisy match outcomes to uncertainties in rankings. We evaluate the performance\nof these approaches in several domains, including Bernoulli games, a soccer\nmeta-game, and Kuhn poker.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 16:05:31 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 16:21:02 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 10:05:19 GMT"}, {"version": "v4", "created": "Fri, 10 Jan 2020 09:59:37 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Rowland", "Mark", ""], ["Omidshafiei", "Shayegan", ""], ["Tuyls", "Karl", ""], ["Perolat", "Julien", ""], ["Valko", "Michal", ""], ["Piliouras", "Georgios", ""], ["Munos", "Remi", ""]]}, {"id": "1909.09852", "submitter": "Ashish Mani Dr.", "authors": "Arit Kumar Bishwas, Ashish Mani, and Vasile Palade", "title": "An Investigation of Quantum Deep Clustering Framework with Quantum Deep\n  SVM & Convolutional Neural Network Feature Extractor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have proposed a deep quantum SVM formulation, and further\ndemonstrated a quantum-clustering framework based on the quantum deep SVM\nformulation, deep convolutional neural networks, and quantum K-Means\nclustering. We have investigated the run time computational complexity of the\nproposed quantum deep clustering framework and compared with the possible\nclassical implementation. Our investigation shows that the proposed quantum\nversion of deep clustering formulation demonstrates a significant performance\ngain (exponential speed up gains in many sections) against the possible\nclassical implementation. The proposed theoretical quantum deep clustering\nframework is also interesting & novel research towards the quantum-classical\nmachine learning formulation to articulate the maximum performance.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 16:19:43 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Bishwas", "Arit Kumar", ""], ["Mani", "Ashish", ""], ["Palade", "Vasile", ""]]}, {"id": "1909.09859", "submitter": "Thomas Boquet", "authors": "Thomas Boquet, Laure Delisle, Denis Kochetkov, Nathan Schucher,\n  Parmida Atighehchian, Boris Oreshkin, Julien Cornebise", "title": "DECoVaC: Design of Experiments with Controlled Variability Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducible research in Machine Learning has seen a salutary abundance of\nprogress lately: workflows, transparency, and statistical analysis of\nvalidation and test performance. We build on these efforts and take them\nfurther. We offer a principled experimental design methodology, based on linear\nmixed models, to study and separate the effects of multiple factors of\nvariation in machine learning experiments. This approach allows to account for\nthe effects of architecture, optimizer, hyper-parameters, intentional\nrandomization, as well as unintended lack of determinism across reruns. We\nillustrate that methodology by analyzing Matching Networks, Prototypical\nNetworks and TADAM on the miniImagenet dataset.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 17:41:12 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Boquet", "Thomas", ""], ["Delisle", "Laure", ""], ["Kochetkov", "Denis", ""], ["Schucher", "Nathan", ""], ["Atighehchian", "Parmida", ""], ["Oreshkin", "Boris", ""], ["Cornebise", "Julien", ""]]}, {"id": "1909.09862", "submitter": "Sauptik Dhar", "authors": "Sauptik Dhar and Vladimir Cherkassky", "title": "Single Class Universum-SVM", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the idea of Universum learning [1, 2] to single-class\nlearning problems. We propose Single Class Universum-SVM setting that\nincorporates a priori knowledge (in the form of additional data samples) into\nthe single class estimation problem. These additional data samples or Universum\nbelong to the same application domain as (positive) data samples from a single\nclass (of interest), but they follow a different distribution. Proposed\nmethodology for single class U-SVM is based on the known connection between\nbinary classification and single class learning formulations [3]. Several\nempirical comparisons are presented to illustrate the utility of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 18:00:46 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Dhar", "Sauptik", ""], ["Cherkassky", "Vladimir", ""]]}, {"id": "1909.09868", "submitter": "Mithun Paul Panenghat", "authors": "Sandeep Suntwal, Mithun Paul, Rebecca Sharp, Mihai Surdeanu", "title": "On the Importance of Delexicalization for Fact Verification", "comments": "published in the proceedings at EMNLP2019", "journal-ref": null, "doi": "10.18653/v1/D19-1340", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we aim to understand and estimate the importance that a neural\nnetwork assigns to various aspects of the data while learning and making\npredictions. Here we focus on the recognizing textual entailment (RTE) task and\nits application to fact verification. In this context, the contributions of\nthis work are as follows. We investigate the attention weights a state of the\nart RTE method assigns to input tokens in the RTE component of fact\nverification systems, and confirm that most of the weight is assigned to POS\ntags of nouns (e.g., NN, NNP etc.) or their phrases. To verify that these\nlexicalized models transfer poorly, we implement a domain transfer experiment\nwhere a RTE component is trained on the FEVER data, and tested on the Fake News\nChallenge (FNC) dataset. As expected, even though this method achieves high\naccuracy when evaluated in the same domain, the performance in the target\ndomain is poor, marginally above chance.To mitigate this dependence on\nlexicalized information, we experiment with several strategies for masking out\nnames by replacing them with their semantic category, coupled with a unique\nidentifier to mark that the same or new entities are referenced between claim\nand evidence. The results show that, while the performance on the FEVER dataset\nremains at par with that of the model trained on lexicalized data, it improves\nsignificantly when tested in the FNC dataset. Thus our experiments demonstrate\nthat our strategy is successful in mitigating the dependency on lexical\ninformation.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 18:47:34 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 01:44:57 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Suntwal", "Sandeep", ""], ["Paul", "Mithun", ""], ["Sharp", "Rebecca", ""], ["Surdeanu", "Mihai", ""]]}, {"id": "1909.09906", "submitter": "Faraz Torabi", "authors": "Ruohan Zhang, Faraz Torabi, Lin Guan, Dana H. Ballard, Peter Stone", "title": "Leveraging Human Guidance for Deep Reinforcement Learning Tasks", "comments": "Proceedings of the 28th International Joint Conference on Artificial\n  Intelligence (IJCAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning agents can learn to solve sequential decision tasks by\ninteracting with the environment. Human knowledge of how to solve these tasks\ncan be incorporated using imitation learning, where the agent learns to imitate\nhuman demonstrated decisions. However, human guidance is not limited to the\ndemonstrations. Other types of guidance could be more suitable for certain\ntasks and require less human effort. This survey provides a high-level overview\nof five recent learning frameworks that primarily rely on human guidance other\nthan conventional, step-by-step action demonstrations. We review the\nmotivation, assumption, and implementation of each framework. We then discuss\npossible future research directions.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 21:48:52 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Zhang", "Ruohan", ""], ["Torabi", "Faraz", ""], ["Guan", "Lin", ""], ["Ballard", "Dana H.", ""], ["Stone", "Peter", ""]]}, {"id": "1909.09922", "submitter": "Chan Hee Song", "authors": "Arijit Sehanobish, Chan Hee Song", "title": "Using Chinese Glyphs for Named Entity Recognition", "comments": "Extended abstract accepted to AAAI-2020, student track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Named Entity Recognition (NER) systems use additional features like\npart-of-speech (POS) tags, shallow parsing, gazetteers, etc. Such kind of\ninformation requires external knowledge like unlabeled texts and trained\ntaggers. Adding these features to NER systems have been shown to have a\npositive impact. However, sometimes creating gazetteers or taggers can take a\nlot of time and may require extensive data cleaning. In this paper for Chinese\nNER systems, we do not use these traditional features but we use lexicographic\nfeatures of Chinese characters. Chinese characters are composed of graphical\ncomponents called radicals and these components often have some semantic\nindicators. We propose CNN based models that incorporate this semantic\ninformation and use them for NER. Our models show an improvement over the\nbaseline BERT-BiLSTM-CRF model. We set a new baseline score for Chinese\nOntoNotes v5.0 and show an improvement of +.64 F1 score. We present a\nstate-of-the-art F1 score on Weibo dataset of 71.81 and show a competitive\nimprovement of +0.72 over baseline on ResumeNER dataset.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 01:12:18 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 03:41:14 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Sehanobish", "Arijit", ""], ["Song", "Chan Hee", ""]]}, {"id": "1909.09953", "submitter": "Kuang-Huei Lee", "authors": "Kuang-Huei Lee, Hamid Palangi, Xi Chen, Houdong Hu, Jianfeng Gao", "title": "Learning Visual Relation Priors for Image-Text Matching and Image\n  Captioning with Neural Scene Graph Generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding language to visual relations is critical to various\nlanguage-and-vision applications. In this work, we tackle two fundamental\nlanguage-and-vision tasks: image-text matching and image captioning, and\ndemonstrate that neural scene graph generators can learn effective visual\nrelation features to facilitate grounding language to visual relations and\nsubsequently improve the two end applications. By combining relation features\nwith the state-of-the-art models, our experiments show significant improvement\non the standard Flickr30K and MSCOCO benchmarks. Our experimental results and\nanalysis show that relation features improve downstream models' capability of\ncapturing visual relations in end vision-and-language applications. We also\ndemonstrate the importance of learning scene graph generators with visually\nrelevant relations to the effectiveness of relation features.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 07:30:29 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Lee", "Kuang-Huei", ""], ["Palangi", "Hamid", ""], ["Chen", "Xi", ""], ["Hu", "Houdong", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1909.09964", "submitter": "Sai Krishna Rallabandi", "authors": "SaiKrishna Rallabandi", "title": "On Controlled DeEntanglement for Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Latest addition to the toolbox of human species is Artificial\nIntelligence(AI). Thus far, AI has made significant progress in low stake low\nrisk scenarios such as playing Go and we are currently in a transition toward\nmedium stake scenarios such as Visual Dialog. In my thesis, I argue that we\nneed to incorporate controlled de-entanglement as first class object to succeed\nin this transition. I present mathematical analysis from information theory to\nshow that employing stochasticity leads to controlled de-entanglement of\nrelevant factors of variation at various levels. Based on this, I highlight\nresults from initial experiments that depict efficacy of the proposed\nframework. I conclude this writeup by a roadmap of experiments that show the\napplicability of this framework to scalability, flexibility and\ninterpretibility.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 08:13:47 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Rallabandi", "SaiKrishna", ""]]}, {"id": "1909.09999", "submitter": "Chiranjibi Sitaula", "authors": "Chiranjibi Sitaula, Yong Xiang, Anish Basnet, Sunil Aryal, Xuequan Lu", "title": "Tag-based Semantic Features for Scene Image Classification", "comments": "Accepted by ICONIP2019 conference", "journal-ref": "In: Gedeon T., Wong K., Lee M. (eds) Neural Information\n  Processing. ICONIP 2019., vol 11955 (2019)", "doi": "10.1007/978-3-030-36718-3_8", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing image feature extraction methods are primarily based on the\ncontent and structure information of images, and rarely consider the contextual\nsemantic information. Regarding some types of images such as scenes and\nobjects, the annotations and descriptions of them available on the web may\nprovide reliable contextual semantic information for feature extraction. In\nthis paper, we introduce novel semantic features of an image based on the\nannotations and descriptions of its similar images available on the web.\nSpecifically, we propose a new method which consists of two consecutive steps\nto extract our semantic features. For each image in the training set, we\ninitially search the top $k$ most similar images from the internet and extract\ntheir annotations/descriptions (e.g., tags or keywords). The annotation\ninformation is employed to design a filter bank for each image category and\ngenerate filter words (codebook). Finally, each image is represented by the\nhistogram of the occurrences of filter words in all categories. We evaluate the\nperformance of the proposed features in scene image classification on three\ncommonly-used scene image datasets (i.e., MIT-67, Scene15 and Event8). Our\nmethod typically produces a lower feature dimension than existing feature\nextraction methods. Experimental results show that the proposed features\ngenerate better classification accuracies than vision based and tag based\nfeatures, and comparable results to deep learning based features.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 13:17:39 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Sitaula", "Chiranjibi", ""], ["Xiang", "Yong", ""], ["Basnet", "Anish", ""], ["Aryal", "Sunil", ""], ["Lu", "Xuequan", ""]]}, {"id": "1909.10008", "submitter": "Jo\\~ao Ribeiro", "authors": "Jo\\~ao Ribeiro, Francisco S. Melo and Jo\\~ao Dias", "title": "Multi-task Learning and Catastrophic Forgetting in Continual\n  Reinforcement Learning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate two hypothesis regarding the use of deep\nreinforcement learning in multiple tasks. The first hypothesis is driven by the\nquestion of whether a deep reinforcement learning algorithm, trained on two\nsimilar tasks, is able to outperform two single-task, individually trained\nalgorithms, by more efficiently learning a new, similar task, that none of the\nthree algorithms has encountered before. The second hypothesis is driven by the\nquestion of whether the same multi-task deep RL algorithm, trained on two\nsimilar tasks and augmented with elastic weight consolidation (EWC), is able to\nretain similar performance on the new task, as a similar algorithm without EWC,\nwhilst being able to overcome catastrophic forgetting in the two previous\ntasks. We show that a multi-task Asynchronous Advantage Actor-Critic (GA3C)\nalgorithm, trained on Space Invaders and Demon Attack, is in fact able to\noutperform two single-tasks GA3C versions, trained individually for each\nsingle-task, when evaluated on a new, third task, namely, Phoenix. We also show\nthat, when training two trained multi-task GA3C algorithms on the third task,\nif one is augmented with EWC, it is not only able to achieve similar\nperformance on the new task, but also capable of overcoming a substantial\namount of catastrophic forgetting on the two previous tasks.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 14:00:29 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ribeiro", "Jo\u00e3o", ""], ["Melo", "Francisco S.", ""], ["Dias", "Jo\u00e3o", ""]]}, {"id": "1909.10031", "submitter": "Peilun Wu", "authors": "Peilun Wu and Hui Guo", "title": "LuNet: A Deep Neural Network for Network Intrusion Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network attack is a significant security issue for modern society. From small\nmobile devices to large cloud platforms, almost all computing products, used in\nour daily life, are networked and potentially under the threat of network\nintrusion. With the fast-growing network users, network intrusions become more\nand more frequent, volatile and advanced. Being able to capture intrusions in\ntime for such a large scale network is critical and very challenging. To this\nend, the machine learning (or AI) based network intrusion detection (NID), due\nto its intelligent capability, has drawn increasing attention in recent years.\nCompared to the traditional signature-based approaches, the AI-based solutions\nare more capable of detecting variants of advanced network attacks. However,\nthe high detection rate achieved by the existing designs is usually accompanied\nby a high rate of false alarms, which may significantly discount the overall\neffectiveness of the intrusion detection system. In this paper, we consider the\nexistence of spatial and temporal features in the network traffic data and\npropose a hierarchical CNN+RNN neural network, LuNet. In LuNet, the\nconvolutional neural network (CNN) and the recurrent neural network (RNN) learn\ninput traffic data in sync with a gradually increasing granularity such that\nboth spatial and temporal features of the data can be effectively extracted.\nOur experiments on two network traffic datasets show that compared to the\nstate-of-the-art network intrusion detection techniques, LuNet not only offers\na high level of detection capability but also has a much low rate of false\npositive-alarm.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 15:34:27 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 16:40:12 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Wu", "Peilun", ""], ["Guo", "Hui", ""]]}, {"id": "1909.10090", "submitter": "Amro Najjar", "authors": "Yazan Mualla, Amro Najjar, Timotheus Kampik, Igor Tchappi, St\\'ephane\n  Galland, Christophe Nicolle", "title": "Towards Explainability for a Civilian UAV Fleet Management using an\n  Agent-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an initial design concept and specification of a civilian\nUnmanned Aerial Vehicle (UAV) management simulation system that focuses on\nexplainability for the human-in-the-loop control of semi-autonomous UAVs. The\ngoal of the system is to facilitate the operator intervention in critical\nscenarios (e.g. avoid safety issues or financial risks). Explainability is\nsupported via user-friendly abstractions on Belief-Desire-Intention agents. To\nevaluate the effectiveness of the system, a human-computer interaction study is\nproposed.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 20:34:09 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Mualla", "Yazan", ""], ["Najjar", "Amro", ""], ["Kampik", "Timotheus", ""], ["Tchappi", "Igor", ""], ["Galland", "St\u00e9phane", ""], ["Nicolle", "Christophe", ""]]}, {"id": "1909.10124", "submitter": "Mohammad Javad Eslamibidgoli", "authors": "Mohammad Javad Eslamibidgoli, Mehrdad Mokhtari, Michael H. Eikerling", "title": "Recurrent Neural Network-based Model for Accelerated Trajectory Analysis\n  in AIMD Simulations", "comments": "10 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presented work demonstrates the training of recurrent neural networks\n(RNNs) from distributions of atom coordinates in solid state structures that\nwere obtained using ab initio molecular dynamics (AIMD) simulations. AIMD\nsimulations on solid state structures are treated as a multi-variate\ntime-series problem. By referring interactions between atoms over the\nsimulation time to temporary correlations among them, RNNs find patterns in the\nmulti-variate time-dependent data, which enable forecasting trajectory paths\nand potential energy profiles. Two types of RNNs, namely gated recurrent unit\nand long short-term memory networks, are considered. The model is described and\ncompared against a baseline AIMD simulation on an iridium oxide slab. Findings\ndemonstrate that both networks can potentially be harnessed for accelerated\nstatistical sampling in computational materials research.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 02:11:41 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 04:24:28 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Eslamibidgoli", "Mohammad Javad", ""], ["Mokhtari", "Mehrdad", ""], ["Eikerling", "Michael H.", ""]]}, {"id": "1909.10157", "submitter": "Zhaoyi Pei Mr", "authors": "Zhaoyi Pei, Piaosong Hao, Meixiang Quan, Muhammad Zuhair Qadir, Guo Li", "title": "Active collaboration in relative observation for Multi-agent visual SLAM\n  based on Deep Q Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a unique active relative localization mechanism for\nmulti-agent Simultaneous Localization and Mapping(SLAM),in which a agent to be\nobserved are considered as a task, which is performed by others assisting that\nagent by relative observation. A task allocation algorithm based on deep\nreinforcement learning are proposed for this mechanism. Each agent can choose\nwhether to localize other agents or to continue independent SLAM on it own\ninitiative. By this way, the process of each agent SLAM will be interacted by\nthe collaboration. Firstly, based on the characteristics of ORBSLAM, a unique\nobservation function which models the whole MAS is obtained. Secondly, a novel\ntype of Deep Q network(DQN) called MAS-DQN is deployed to learn correspondence\nbetween Q Value and state-action pair,abstract representation of agents in MAS\nare learned in the process of collaboration among agents. Finally, each agent\nmust act with a certain degree of freedom according to MAS-DQN. The simulation\nresults of comparative experiments prove that this mechanism improves the\nefficiency of cooperation in the process of multi-agent SLAM.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 04:58:19 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Pei", "Zhaoyi", ""], ["Hao", "Piaosong", ""], ["Quan", "Meixiang", ""], ["Qadir", "Muhammad Zuhair", ""], ["Li", "Guo", ""]]}, {"id": "1909.10158", "submitter": "Hamidreza Shahidi", "authors": "Hamidreza Shahidi, Ming Li, and Jimmy Lin", "title": "Two Birds, One Stone: A Simple, Unified Model for Text Generation from\n  Structured and Unstructured Data", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of researchers have recently questioned the necessity of\nincreasingly complex neural network (NN) architectures. In particular, several\nrecent papers have shown that simpler, properly tuned models are at least\ncompetitive across several NLP tasks. In this work, we show that this is also\nthe case for text generation from structured and unstructured data. We consider\nneural table-to-text generation and neural question generation (NQG) tasks for\ntext generation from structured and unstructured data, respectively.\nTable-to-text generation aims to generate a description based on a given table,\nand NQG is the task of generating a question from a given passage where the\ngenerated question can be answered by a certain sub-span of the passage using\nNN models. Experimental results demonstrate that a basic attention-based\nseq2seq model trained with the exponential moving average technique achieves\nthe state of the art in both tasks. Code is available at\nhttps://github.com/h-shahidi/2birds-gen.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 05:07:06 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 01:29:26 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Shahidi", "Hamidreza", ""], ["Li", "Ming", ""], ["Lin", "Jimmy", ""]]}, {"id": "1909.10164", "submitter": "Mukesh Saini", "authors": "Mukesh Saini and Benjamin Guthier and Hao Kuang and Dwarikanath\n  Mahapatra and Abdulmotaleb El Saddik", "title": "sZoom: A Framework for Automatic Zoom into High Resolution Surveillance\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current cameras are capable of recording high resolution video. While viewing\non a mobile device, a user can manually zoom into this high resolution video to\nget more detailed view of objects and activities. However, manual zooming is\nnot suitable for surveillance and monitoring. It is tiring to continuously keep\nzooming into various regions of the video. Also, while viewing one region, the\noperator may miss activities in other regions. In this paper, we propose sZoom,\na framework to automatically zoom into a high resolution surveillance video.\nThe proposed framework selectively zooms into the sensitive regions of the\nvideo to present details of the scene, while still preserving the overall\ncontext required for situation assessment. A multi-variate Gaussian penalty is\nintroduced to ensure full coverage of the scene. The method achieves near\nreal-time performance through a number of timing optimizations. An extensive\nuser study shows that, while watching a full HD video on a mobile device, the\nsystem enhances the security operator's efficiency in understanding the details\nof the scene by 99% on the average compared to a scaled version of the original\nhigh resolution video. The produced video achieved 46% higher ratings for\nusefulness in a surveillance task.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 05:20:48 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Saini", "Mukesh", ""], ["Guthier", "Benjamin", ""], ["Kuang", "Hao", ""], ["Mahapatra", "Dwarikanath", ""], ["Saddik", "Abdulmotaleb El", ""]]}, {"id": "1909.10166", "submitter": "Zitao Liu", "authors": "Tiaoqiao Liu, Wenbiao Ding, Zhiwei Wang, Jiliang Tang, Gale Yan Huang,\n  Zitao Liu", "title": "Automatic Short Answer Grading via Multiway Attention Networks", "comments": "The 20th International Conference on Artificial Intelligence in\n  Education(AIED), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic short answer grading (ASAG), which autonomously score student\nanswers according to reference answers, provides a cost-effective and\nconsistent approach to teaching professionals and can reduce their monotonous\nand tedious grading workloads. However, ASAG is a very challenging task due to\ntwo reasons: (1) student answers are made up of free text which requires a deep\nsemantic understanding; and (2) the questions are usually open-ended and across\nmany domains in K-12 scenarios. In this paper, we propose a generalized\nend-to-end ASAG learning framework which aims to (1) autonomously extract\nlinguistic information from both student and reference answers; and (2)\naccurately model the semantic relations between free-text student and reference\nanswers in open-ended domain. The proposed ASAG model is evaluated on a large\nreal-world K-12 dataset and can outperform the state-of-the-art baselines in\nterms of various evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 05:29:04 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Liu", "Tiaoqiao", ""], ["Ding", "Wenbiao", ""], ["Wang", "Zhiwei", ""], ["Tang", "Jiliang", ""], ["Huang", "Gale Yan", ""], ["Liu", "Zitao", ""]]}, {"id": "1909.10278", "submitter": "Daniel Lerch Hostalot PhD", "authors": "Daniel Lerch-Hostalot, David Meg\\'ias", "title": "Detection of Classifier Inconsistencies in Image Steganalysis", "comments": null, "journal-ref": null, "doi": "10.1145/3335203.3335738", "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a methodology to detect inconsistencies in\nclassification-based image steganalysis is presented. The proposed approach\nuses two classifiers: the usual one, trained with a set formed by cover and\nstego images, and a second classifier trained with the set obtained after\nembedding additional random messages into the original training set. When the\ndecisions of these two classifiers are not consistent, we know that the\nprediction is not reliable. The number of inconsistencies in the predictions of\na testing set may indicate that the classifier is not performing correctly in\nthe testing scenario. This occurs, for example, in case of cover source\nmismatch, or when we are trying to detect a steganographic method that the\nclassifier is no capable of modelling accurately. We also show how the number\nof inconsistencies can be used to predict the reliability of the classifier\n(classification errors).\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:03:16 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Lerch-Hostalot", "Daniel", ""], ["Meg\u00edas", "David", ""]]}, {"id": "1909.10300", "submitter": "Edouard Pauwels", "authors": "J\\'er\\^ome Bolte and Edouard Pauwels", "title": "Conservative set valued fields, automatic differentiation, stochastic\n  gradient method and deep learning", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern problems in AI or in numerical analysis require nonsmooth approaches\nwith a flexible calculus. We introduce generalized derivatives called\nconservative fields for which we develop a calculus and provide representation\nformulas. Functions having a conservative field are called path differentiable:\nconvex, concave, Clarke regular and any semialgebraic Lipschitz continuous\nfunctions are path differentiable. Using Whitney stratification techniques for\nsemialgebraic and definable sets, our model provides variational formulas for\nnonsmooth automatic differentiation oracles, as for instance the famous\nbackpropagation algorithm in deep learning. Our differential model is applied\nto establish the convergence in values of nonsmooth stochastic gradient methods\nas they are implemented in practice.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:39:16 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 15:00:51 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 15:16:30 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2020 09:43:26 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Bolte", "J\u00e9r\u00f4me", ""], ["Pauwels", "Edouard", ""]]}, {"id": "1909.10304", "submitter": "Soroush Seifi", "authors": "Soroush Seifi, Tinne Tuytelaars", "title": "Where to Look Next: Unsupervised Active Visual Exploration on 360{\\deg}\n  Input", "comments": "Oral Presentation and best Paper Award at 360 Perception and\n  Interaction Workshop at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of active visual exploration of large 360{\\deg}\ninputs. In our setting an active agent with a limited camera bandwidth explores\nits 360{\\deg} environment by changing its viewing direction at limited discrete\ntime steps. As such, it observes the world as a sequence of narrow\nfield-of-view 'glimpses', deciding for itself where to look next. Our proposed\nmethod exceeds previous works' performance by a significant margin without the\nneed for deep reinforcement learning or training separate networks as\nsidekicks. A key component of our system are the spatial memory maps that make\nthe system aware of the glimpses' orientations (locations in the 360{\\deg}\nimage). Further, we stress the advantages of retina-like glimpses when the\nagent's sensor bandwidth and time-steps are limited. Finally, we use our\ntrained model to do classification of the whole scene using only the\ninformation observed in the glimpses.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:50:46 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 10:38:02 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Seifi", "Soroush", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1909.10305", "submitter": "Alice Othmani", "authors": "Amine Djerghri, Ahmed Rachid Hazourli, Alice Othmani", "title": "Deep Multi-Facial patches Aggregation Network for Expression\n  Classification from Face Images", "comments": "we have a new version of the paper arXiv:2002.09298", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Emotional Intelligence in Human-Computer Interaction has attracted increasing\nattention from researchers in multidisciplinary research fields including\npsychology, computer vision, neuroscience, artificial intelligence, and related\ndisciplines. Human prone to naturally interact with computers face-to-face.\nHuman Expressions is an important key to better link human and computers. Thus,\ndesigning interfaces able to understand human expressions and emotions can\nimprove Human-Computer Interaction (HCI) for better communication. In this\npaper, we investigate HCI via a deep multi-facial patches aggregation network\nfor Face Expression Recognition (FER). Deep features are extracted from facial\nparts and aggregated for expression classification. Several problems may affect\nthe performance of the proposed framework like the small size of FER datasets\nand the high number of parameters to learn. For That, two data augmentation\ntechniques are proposed for facial expression generation to expand the labeled\ntraining. The proposed framework is evaluated on the extended Cohn-Konade\ndataset (CK+) and promising results are achieved.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:52:27 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 11:52:19 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Djerghri", "Amine", ""], ["Hazourli", "Ahmed Rachid", ""], ["Othmani", "Alice", ""]]}, {"id": "1909.10312", "submitter": "Soroush Seifi", "authors": "Soroush Seifi, Tinne Tuytelaars", "title": "How to improve CNN-based 6-DoF camera pose estimation", "comments": "Accepted at Deep Learning for Visual SLAM workshop at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) and transfer learning have recently been\nused for 6 degrees of freedom (6-DoF) camera pose estimation. While they do not\nreach the same accuracy as visual SLAM-based approaches and are restricted to a\nspecific environment, they excel in robustness and can be applied even to a\nsingle image. In this paper, we study PoseNet [1] and investigate modifications\nbased on datasets' characteristics to improve the accuracy of the pose\nestimates. In particular, we emphasize the importance of field-of-view over\nimage resolution; we present a data augmentation scheme to reduce overfitting;\nwe study the effect of Long-Short-Term-Memory (LSTM) cells. Lastly, we combine\nthese modifications and improve PoseNet's performance for monocular CNN based\ncamera pose regression.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 12:12:17 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 10:38:42 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Seifi", "Soroush", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1909.10351", "submitter": "Yichun Yin", "authors": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin\n  Li, Fang Wang and Qun Liu", "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "comments": "Findings of EMNLP 2020; results have been updated; code and model:\n  https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language model pre-training, such as BERT, has significantly improved the\nperformances of many natural language processing tasks. However, pre-trained\nlanguage models are usually computationally expensive, so it is difficult to\nefficiently execute them on resource-restricted devices. To accelerate\ninference and reduce model size while maintaining accuracy, we first propose a\nnovel Transformer distillation method that is specially designed for knowledge\ndistillation (KD) of the Transformer-based models. By leveraging this new KD\nmethod, the plenty of knowledge encoded in a large teacher BERT can be\neffectively transferred to a small student Tiny-BERT. Then, we introduce a new\ntwo-stage learning framework for TinyBERT, which performs Transformer\ndistillation at both the pretraining and task-specific learning stages. This\nframework ensures that TinyBERT can capture he general-domain as well as the\ntask-specific knowledge in BERT.\n  TinyBERT with 4 layers is empirically effective and achieves more than 96.8%\nthe performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x\nsmaller and 9.4x faster on inference. TinyBERT with 4 layers is also\nsignificantly better than 4-layer state-of-the-art baselines on BERT\ndistillation, with only about 28% parameters and about 31% inference time of\nthem. Moreover, TinyBERT with 6 layers performs on-par with its teacher\nBERTBASE.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 13:05:35 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 12:39:36 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 01:29:39 GMT"}, {"version": "v4", "created": "Wed, 4 Dec 2019 01:50:34 GMT"}, {"version": "v5", "created": "Fri, 16 Oct 2020 02:12:46 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Jiao", "Xiaoqi", ""], ["Yin", "Yichun", ""], ["Shang", "Lifeng", ""], ["Jiang", "Xin", ""], ["Chen", "Xiao", ""], ["Li", "Linlin", ""], ["Wang", "Fang", ""], ["Liu", "Qun", ""]]}, {"id": "1909.10367", "submitter": "Boris Knyazev", "authors": "Boris Knyazev, Carolyn Augusta, Graham W. Taylor", "title": "Learning Temporal Attention in Dynamic Graphs with Bilinear Interactions", "comments": "15 pages, source code is available at\n  https://github.com/uoguelph-mlrg/LDG", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about graphs evolving over time is a challenging concept in many\ndomains, such as bioinformatics, physics, and social networks. We consider a\ncommon case in which edges can be short term interactions (e.g., messaging) or\nlong term structural connections (e.g., friendship). In practice, long term\nedges are often specified by humans. Human-specified edges can be both\nexpensive to produce and suboptimal for the downstream task. To alleviate these\nissues, we propose a model based on temporal point processes and variational\nautoencoders that learns to infer temporal attention between nodes by observing\nnode communication. As temporal attention drives between-node feature\npropagation, using the dynamics of node interactions to learn this key\ncomponent provides more flexibility while simultaneously avoiding issues\nassociated with human-specified edges. We also propose a bilinear\ntransformation layer for pairs of node features instead of concatenation,\ntypically used in prior work, and demonstrate its superior performance in all\ncases. In experiments on two datasets in the dynamic link prediction task, our\nmodel often outperforms the baseline model that requires a human-specified\ngraph. Moreover, our learned attention is semantically interpretable and infers\nconnections similar to actual graphs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 13:54:10 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 01:08:38 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Knyazev", "Boris", ""], ["Augusta", "Carolyn", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1909.10371", "submitter": "Jessica Rivera Villicana", "authors": "Nic Velissaris and Jessica Rivera-Villicana", "title": "Towards Intelligent Interactive Theatre: Drama Management as a way of\n  Handling Performance", "comments": "International Conference on Interactive Digital Storytelling (ICIDS)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new modality for intelligent interactive\nnarratives within the theatre domain. We discuss the possibilities of using an\nintelligent agent that serves as a drama manager and as an actor that plays a\ncharacter within the live theatre experience. We pose a set of research\nchallenges that arise from our analysis towards the implementation of such an\nagent, as well as potential methodologies as a starting point to bridge the\ngaps between current literature and the proposed modality.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 13:59:35 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Velissaris", "Nic", ""], ["Rivera-Villicana", "Jessica", ""]]}, {"id": "1909.10400", "submitter": "Yuying Chen", "authors": "Yuying Chen, Congcong Liu, Ming Liu, Bertram E. Shi", "title": "Robot Navigation in Crowds by Graph Convolutional Networks with\n  Attention Learned from Human Gaze", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe and efficient crowd navigation for mobile robot is a crucial yet\nchallenging task. Previous work has shown the power of deep reinforcement\nlearning frameworks to train efficient policies. However, their performance\ndeteriorates when the crowd size grows. We suggest that this can be addressed\nby enabling the network to identify and pay attention to the humans in the\ncrowd that are most critical to navigation. We propose a novel network\nutilizing a graph representation to learn the policy. We first train a graph\nconvolutional network based on human gaze data that accurately predicts human\nattention to different agents in the crowd. Then we incorporate the learned\nattention into a graph-based reinforcement learning architecture. The proposed\nattention mechanism enables the assignment of meaningful weightings to the\nneighbors of the robot, and has the additional benefit of interpretability.\nExperiments on real-world dense pedestrian datasets with various crowd sizes\ndemonstrate that our model outperforms state-of-art methods by 18.4% in task\naccomplishment and by 16.4% in time efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 14:46:11 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Chen", "Yuying", ""], ["Liu", "Congcong", ""], ["Liu", "Ming", ""], ["Shi", "Bertram E.", ""]]}, {"id": "1909.10414", "submitter": "Jessica Rivera Villicana", "authors": "Jessica Rivera-Villicana, Fabio Zambetta, James Harland, Marsha Berry", "title": "Informing a BDI Player Model for an Interactive Narrative", "comments": "CHI Play 2018", "journal-ref": "Proceedings of the 2018 Annual Symposium on Computer-Human\n  Interaction in Play (CHI PLAY '18). ACM, New York, NY, USA, 417-428. DOI:\n  https://doi.org/10.1145/3242671.3242700", "doi": "10.1145/3242671.3242700", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on studying players behaviour in interactive narratives\nwith the aim to simulate their choices. Besides sub-optimal player behaviour\ndue to limited knowledge about the environment, the difference in each player's\nstyle and preferences represents a challenge when trying to make an intelligent\nsystem mimic their actions. Based on observations from players interactions\nwith an extract from the interactive fiction Anchorhead, we created a player\nprofile to guide the behaviour of a generic player model based on the BDI\n(Belief-Desire-Intention) model of agency. We evaluated our approach using\nqualitative and quantitative methods and found that the player profile can\nimprove the performance of the BDI player model. However, we found that players\nself-assessment did not yield accurate data to populate their player profile\nunder our current approach.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 15:12:45 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Rivera-Villicana", "Jessica", ""], ["Zambetta", "Fabio", ""], ["Harland", "James", ""], ["Berry", "Marsha", ""]]}, {"id": "1909.10419", "submitter": "Jan P\\\"oppel", "authors": "Jan P\\\"oppel and Stefan Kopp", "title": "Satisficing Mentalizing: Bayesian Models of Theory of Mind Reasoning in\n  Scenarios with Different Uncertainties", "comments": "This paper is an extended version of the paper \"Satisficing Models of\n  Bayesian Theory of Mind for Explaining Behavior of Differently Uncertain\n  Agents\" by the same authors, submitted to AAMAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to interpret the mental state of another agent based on its\nbehavior, also called Theory of Mind (ToM), is crucial for humans in any kind\nof social interaction. Artificial systems, such as intelligent assistants,\nwould also greatly benefit from such mentalizing capabilities. However, humans\nand systems alike are bound by limitations in their available computational\nresources. This raises the need for satisficing mentalizing, reconciling\naccuracy and efficiency in mental state inference that is good enough for a\ngiven situation. In this paper, we present different Bayesian models of ToM\nreasoning and evaluate them based on actual human behavior data that were\ngenerated under different kinds of uncertainties. We propose a Switching\napproach that combines specialized models, embodying simplifying presumptions,\nin order to achieve a more statisficing mentalizing compared to a Full Bayesian\nToM model.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 15:20:51 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["P\u00f6ppel", "Jan", ""], ["Kopp", "Stefan", ""]]}, {"id": "1909.10427", "submitter": "Lorenzo Federici Mr.", "authors": "Lorenzo Federici, Alessandro Zavoli, Guido Colasurdo", "title": "A Time-Dependent TSP Formulation for the Design of an Active Debris\n  Removal Mission using Simulated Annealing", "comments": "2019 AAS/AIAA Astrodynamics Specialist Conference, Portland, ME", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI math.CO physics.space-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a formulation of the Active Debris Removal (ADR) Mission\nDesign problem as a modified Time-Dependent Traveling Salesman Problem (TDTSP).\nThe TDTSP is a well-known combinatorial optimization problem, whose solution is\nthe cheapest mono-cyclic tour connecting a number of non-stationary cities in a\nmap. The problem is tackled with an optimization procedure based on Simulated\nAnnealing, that efficiently exploits a natural encoding and a careful choice of\nmutation operators. The developed algorithm is used to simultaneously optimize\nthe targets sequence and the rendezvous epochs of an impulsive ADR mission.\nNumerical results are presented for sets comprising up to 20 targets.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 15:35:37 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Federici", "Lorenzo", ""], ["Zavoli", "Alessandro", ""], ["Colasurdo", "Guido", ""]]}, {"id": "1909.10461", "submitter": "Ferdinando Fioretto", "authors": "Ferdinando Fioretto, Terrence W.K. Mak, Pascal Van Hentenryck", "title": "Predicting AC Optimal Power Flows: Combining Deep Learning and\n  Lagrangian Dual Methods", "comments": "A version of this paper appears in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Optimal Power Flow (OPF) problem is a fundamental building block for the\noptimization of electrical power systems. It is nonlinear and nonconvex and\ncomputes the generator setpoints for power and voltage, given a set of load\ndemands. It is often needed to be solved repeatedly under various conditions,\neither in real-time or in large-scale studies. This need is further exacerbated\nby the increasing stochasticity of power systems due to renewable energy\nsources in front and behind the meter. To address these challenges, this paper\npresents a deep learning approach to the OPF. The learning model exploits the\ninformation available in the prior states of the system (which is commonly\navailable in practical applications), as well as a dual Lagrangian method to\nsatisfy the physical and engineering constraints present in the OPF. The\nproposed model is evaluated on a large collection of realistic power systems.\nThe experimental results show that its predictions are highly accurate with\naverage errors as low as 0.2%. Additionally, the proposed approach is shown to\nimprove the accuracy of widely adopted OPF linear DC approximation by at least\ntwo orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 00:39:17 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 15:25:44 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Fioretto", "Ferdinando", ""], ["Mak", "Terrence W. K.", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1909.10467", "submitter": "Hassan Rafique", "authors": "Hassan Rafique, Tong Wang, Qihang Lin", "title": "Model-Agnostic Linear Competitors -- When Interpretable Models Compete\n  and Collaborate with Black-Box Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by an increasing need for model interpretability, interpretable models\nhave become strong competitors for black-box models in many real applications.\nIn this paper, we propose a novel type of model where interpretable models\ncompete and collaborate with black-box models. We present the Model-Agnostic\nLinear Competitors (MALC) for partially interpretable classification. MALC is a\nhybrid model that uses linear models to locally substitute any black-box model,\ncapturing subspaces that are most likely to be in a class while leaving the\nrest of the data to the black-box. MALC brings together the interpretable power\nof linear models and good predictive performance of a black-box model. We\nformulate the training of a MALC model as a convex optimization. The predictive\naccuracy and transparency (defined as the percentage of data captured by the\nlinear models) balance through a carefully designed objective function and the\noptimization problem is solved with the accelerated proximal gradient method.\nExperiments show that MALC can effectively trade prediction accuracy for\ntransparency and provide an efficient frontier that spans the entire spectrum\nof transparency.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 16:41:57 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Rafique", "Hassan", ""], ["Wang", "Tong", ""], ["Lin", "Qihang", ""]]}, {"id": "1909.10470", "submitter": "Vishvak Murahari", "authors": "Vishvak Murahari, Prithvijit Chattopadhyay, Dhruv Batra, Devi Parikh,\n  Abhishek Das", "title": "Improving Generative Visual Dialog by Answering Diverse Questions", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work on training generative Visual Dialog models with reinforcement\nlearning(Das et al.) has explored a Qbot-Abot image-guessing game and shown\nthat this 'self-talk' approach can lead to improved performance at the\ndownstream dialog-conditioned image-guessing task. However, this improvement\nsaturates and starts degrading after a few rounds of interaction, and does not\nlead to a better Visual Dialog model. We find that this is due in part to\nrepeated interactions between Qbot and Abot during self-talk, which are not\ninformative with respect to the image. To improve this, we devise a simple\nauxiliary objective that incentivizes Qbot to ask diverse questions, thus\nreducing repetitions and in turn enabling Abot to explore a larger state space\nduring RL ie. be exposed to more visual concepts to talk about, and varied\nquestions to answer. We evaluate our approach via a host of automatic metrics\nand human studies, and demonstrate that it leads to better dialog, ie. dialog\nthat is more diverse (ie. less repetitive), consistent (ie. has fewer\nconflicting exchanges), fluent (ie. more human-like),and detailed, while still\nbeing comparably image-relevant as prior work and ablations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 16:47:15 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 03:01:48 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Murahari", "Vishvak", ""], ["Chattopadhyay", "Prithvijit", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Das", "Abhishek", ""]]}, {"id": "1909.10476", "submitter": "Binil Starly", "authors": "Binil Starly, Atin Angrish, Paul Cohen", "title": "Research Directions in Democratizing Innovation through Design\n  Automation, One-Click Manufacturing Services and Intelligent Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digitalization of manufacturing has created opportunities for consumers\nto customize products that fit their individualized needs which in turn would\ndrive demand for manufacturing services. However, this pull-based manufacturing\nsystem production of extremely low quantity and limitless variety for products\nis expensive to implement. New emerging technology in design automation driven\nby data-driven computational design, manufacturing-as-a-service marketplaces\nand digitally enabled micro-factories holds promise towards democratization of\ninnovation. In this paper, scientific, technology and infrastructure challenges\nare identified and if solved, the impact of these emerging technologies on\nproduct innovation and future factory organization is discussed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 16:56:08 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Starly", "Binil", ""], ["Angrish", "Atin", ""], ["Cohen", "Paul", ""]]}, {"id": "1909.10502", "submitter": "Warut Suksompong", "authors": "Mithun Chakraborty, Ayumi Igarashi, Warut Suksompong, Yair Zick", "title": "Weighted Envy-Freeness in Indivisible Item Allocation", "comments": "A preliminary version appears in Proceedings of the 19th\n  International Conference on Autonomous Agents and Multiagent Systems (AAMAS),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and analyze new envy-based fairness concepts for agents with\nweights that quantify their entitlements in the allocation of indivisible\nitems. We propose two variants of weighted envy-freeness up to one item (WEF1):\nstrong, where envy can be eliminated by removing an item from the envied\nagent's bundle, and weak, where envy can be eliminated either by removing an\nitem (as in the strong version) or by replicating an item from the envied\nagent's bundle in the envying agent's bundle. We show that for additive\nvaluations, an allocation that is both Pareto optimal and strongly WEF1 always\nexists and can be computed in pseudo-polynomial time; moreover, an allocation\nthat maximizes the weighted Nash social welfare may not be strongly WEF1, but\nalways satisfies the weak version of the property. Moreover, we establish that\na generalization of the round-robin picking sequence algorithm produces in\npolynomial time a strongly WEF1 allocation for an arbitrary number of agents;\nfor two agents, we can efficiently achieve both strong WEF1 and Pareto\noptimality by adapting the adjusted winner procedure. Our work highlights\nseveral aspects in which weighted fair division is richer and more challenging\nthan its unweighted counterpart.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 17:44:59 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 02:29:43 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 20:17:06 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2020 20:13:31 GMT"}, {"version": "v5", "created": "Thu, 30 Apr 2020 02:46:09 GMT"}, {"version": "v6", "created": "Wed, 21 Oct 2020 02:38:01 GMT"}, {"version": "v7", "created": "Sat, 6 Mar 2021 07:13:25 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chakraborty", "Mithun", ""], ["Igarashi", "Ayumi", ""], ["Suksompong", "Warut", ""], ["Zick", "Yair", ""]]}, {"id": "1909.10572", "submitter": "Sarthak Dash", "authors": "Sarthak Dash, Md Faisal Mahbub Chowdhury, Alfio Gliozzo, Nandana\n  Mihindukulasooriya, Nicolas Rodolfo Fauceglia", "title": "Hypernym Detection Using Strict Partial Order Networks", "comments": "8 pages", "journal-ref": "AAAI 2020", "doi": "10.1609/aaai.v34i05.6263", "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Strict Partial Order Networks (SPON), a novel neural\nnetwork architecture designed to enforce asymmetry and transitive properties as\nsoft constraints. We apply it to induce hypernymy relations by training with\nis-a pairs. We also present an augmented variant of SPON that can generalize\ntype information learned for in-vocabulary terms to previously unseen ones. An\nextensive evaluation over eleven benchmarks across different tasks shows that\nSPON consistently either outperforms or attains the state of the art on all but\none of these benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 18:54:52 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 22:40:23 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Dash", "Sarthak", ""], ["Chowdhury", "Md Faisal Mahbub", ""], ["Gliozzo", "Alfio", ""], ["Mihindukulasooriya", "Nandana", ""], ["Fauceglia", "Nicolas Rodolfo", ""]]}, {"id": "1909.10614", "submitter": "Shiwali Mohan", "authors": "Shiwali Mohan, Hesham Rakha, Matthew Klenk", "title": "Acceptable Planning: Influencing Individual Behavior to Reduce\n  Transportation Energy Expenditure of a City", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research 66 (2019) 555-587", "doi": "10.1613/jair.1.11352", "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research aims at developing intelligent systems to reduce the\ntransportation-related energy expenditure of a large city by influencing\nindividual behavior. We introduce COPTER - an intelligent travel assistant that\nevaluates multi-modal travel alternatives to find a plan that is acceptable to\na person given their context and preferences. We propose a formulation for\nacceptable planning that brings together ideas from AI, machine learning, and\neconomics. This formulation has been incorporated in COPTER that produces\nacceptable plans in real-time. We adopt a novel empirical evaluation framework\nthat combines human decision data with a high fidelity multi-modal\ntransportation simulation to demonstrate a 4\\% energy reduction and 20\\% delay\nreduction in a realistic deployment scenario in Los Angeles, California, USA.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 20:55:18 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Mohan", "Shiwali", ""], ["Rakha", "Hesham", ""], ["Klenk", "Matthew", ""]]}, {"id": "1909.10618", "submitter": "Ofir Nachum", "authors": "Ofir Nachum, Haoran Tang, Xingyu Lu, Shixiang Gu, Honglak Lee, Sergey\n  Levine", "title": "Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?", "comments": "Presented as an oral at the NeurIPS 2019 DeepRL Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical reinforcement learning has demonstrated significant success at\nsolving difficult reinforcement learning (RL) tasks. Previous works have\nmotivated the use of hierarchy by appealing to a number of intuitive benefits,\nincluding learning over temporally extended transitions, exploring over\ntemporally extended periods, and training and exploring in a more semantically\nmeaningful action space, among others. However, in fully observed, Markovian\nsettings, it is not immediately clear why hierarchical RL should provide\nbenefits over standard \"shallow\" RL architectures. In this work, we isolate and\nevaluate the claimed benefits of hierarchical RL on a suite of tasks\nencompassing locomotion, navigation, and manipulation. Surprisingly, we find\nthat most of the observed benefits of hierarchy can be attributed to improved\nexploration, as opposed to easier policy learning or imposed hierarchical\nstructures. Given this insight, we present exploration techniques inspired by\nhierarchy that achieve performance competitive with hierarchical RL while at\nthe same time being much simpler to use and implement.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 21:11:30 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 17:21:22 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Nachum", "Ofir", ""], ["Tang", "Haoran", ""], ["Lu", "Xingyu", ""], ["Gu", "Shixiang", ""], ["Lee", "Honglak", ""], ["Levine", "Sergey", ""]]}, {"id": "1909.10622", "submitter": "Behrouz Babaki", "authors": "Behrouz Babaki, Golnoosh Farnadi, Gilles Pesant", "title": "Compiling Stochastic Constraint Programs to And-Or Decision Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factored stochastic constraint programming (FSCP) is a formalism to represent\nmulti-stage decision making problems under uncertainty. FSCP models support\nfactorized probabilistic models and involve constraints over decision and\nrandom variables. These models have many applications in real-world problems.\nHowever, solving these problems requires evaluating the best course of action\nfor each possible outcome of the random variables and hence is computationally\nchallenging. FSCP problems often involve repeated subproblems which ideally\nshould be solved once. In this paper we show how identifying and exploiting\nthese identical subproblems can simplify solving them and leads to a compact\nrepresentation of the solution. We compile an And-Or search tree to a compact\ndecision diagram. Preliminary experiments show that our proposed method\nsignificantly improves the search efficiency by reducing the size of the\nproblem and outperforms the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 21:25:17 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Babaki", "Behrouz", ""], ["Farnadi", "Golnoosh", ""], ["Pesant", "Gilles", ""]]}, {"id": "1909.10650", "submitter": "Mohan Sridharan", "authors": "Heather Riley, Mohan Sridharan", "title": "Non-monotonic Logical Reasoning Guiding Deep Learning for Explainable\n  Visual Question Answering", "comments": "28 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art algorithms for many pattern recognition problems rely on\ndeep network models. Training these models requires a large labeled dataset and\nconsiderable computational resources. Also, it is difficult to understand the\nworking of these learned models, limiting their use in some critical\napplications. Towards addressing these limitations, our architecture draws\ninspiration from research in cognitive systems, and integrates the principles\nof commonsense logical reasoning, inductive learning, and deep learning. In the\ncontext of answering explanatory questions about scenes and the underlying\nclassification problems, the architecture uses deep networks for extracting\nfeatures from images and for generating answers to queries. Between these deep\nnetworks, it embeds components for non-monotonic logical reasoning with\nincomplete commonsense domain knowledge, and for decision tree induction. It\nalso incrementally learns and reasons with previously unknown constraints\ngoverning the domain's states. We evaluated the architecture in the context of\ndatasets of simulated and real-world images, and a simulated robot computing,\nexecuting, and providing explanatory descriptions of plans. Experimental\nresults indicate that in comparison with an ``end to end'' architecture of deep\nnetworks, our architecture provides better accuracy on classification problems\nwhen the training dataset is small, comparable accuracy with larger datasets,\nand more accurate answers to explanatory questions. Furthermore, incremental\nacquisition of previously unknown constraints improves the ability to answer\nexplanatory questions, and extending non-monotonic logical reasoning to support\nplanning and diagnostics improves the reliability and efficiency of computing\nand executing plans on a simulated robot.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 23:34:32 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Riley", "Heather", ""], ["Sridharan", "Mohan", ""]]}, {"id": "1909.10681", "submitter": "Peixiang Zhong", "authors": "Peixiang Zhong, Di Wang, Chunyan Miao", "title": "Knowledge-Enriched Transformer for Emotion Detection in Textual\n  Conversations", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Messages in human conversations inherently convey emotions. The task of\ndetecting emotions in textual conversations leads to a wide range of\napplications such as opinion mining in social networks. However, enabling\nmachines to analyze emotions in conversations is challenging, partly because\nhumans often rely on the context and commonsense knowledge to express emotions.\nIn this paper, we address these challenges by proposing a Knowledge-Enriched\nTransformer (KET), where contextual utterances are interpreted using\nhierarchical self-attention and external commonsense knowledge is dynamically\nleveraged using a context-aware affective graph attention mechanism.\nExperiments on multiple textual conversation datasets demonstrate that both\ncontext and commonsense knowledge are consistently beneficial to the emotion\ndetection performance. In addition, the experimental results show that our KET\nmodel outperforms the state-of-the-art models on most of the tested datasets in\nF1 score.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 02:08:29 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 14:00:22 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Zhong", "Peixiang", ""], ["Wang", "Di", ""], ["Miao", "Chunyan", ""]]}, {"id": "1909.10705", "submitter": "Abigail See", "authors": "Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, Christopher\n  D. Manning", "title": "Do Massively Pretrained Language Models Make Better Storytellers?", "comments": "Accepted to CoNLL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large neural language models trained on massive amounts of text have emerged\nas a formidable strategy for Natural Language Understanding tasks. However, the\nstrength of these models as Natural Language Generators is less clear. Though\nanecdotal evidence suggests that these models generate better quality text,\nthere has been no detailed study characterizing their generation abilities. In\nthis work, we compare the performance of an extensively pretrained model,\nOpenAI GPT2-117 (Radford et al., 2019), to a state-of-the-art neural story\ngeneration model (Fan et al., 2018). By evaluating the generated text across a\nwide variety of automatic metrics, we characterize the ways in which pretrained\nmodels do, and do not, make better storytellers. We find that although GPT2-117\nconditions more strongly on context, is more sensitive to ordering of events,\nand uses more unusual words, it is just as likely to produce repetitive and\nunder-diverse text when using likelihood-maximizing decoding algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 04:26:27 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["See", "Abigail", ""], ["Pappu", "Aneesh", ""], ["Saxena", "Rohun", ""], ["Yerukola", "Akhila", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1909.10707", "submitter": "Yijiong Lin", "authors": "Yijiong Lin, Jiancong Huang, Matthieu Zimmer, Yisheng Guan, Juan\n  Rojas, Paul Weng", "title": "Invariant Transform Experience Replay: Data Augmentation for Deep\n  Reinforcement Learning", "comments": "8 pages, 11 figures, additional 3 pages for appendix. IEEE Robotics\n  and Automation Letters (RAL), 2020. Also in: Intelligent Robots and Systems\n  (IROS)", "journal-ref": "IEEE Robotics and Automation Letters, Volume: 5, Issue: 4, p.\n  6615-6622, Oct. 2020", "doi": "10.1109/LRA.2020.3013937", "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (RL) is a promising approach for adaptive robot\ncontrol, but its current application to robotics is currently hindered by high\nsample requirements. To alleviate this issue, we propose to exploit the\nsymmetries present in robotic tasks. Intuitively, symmetries from observed\ntrajectories define transformations that leave the space of feasible RL\ntrajectories invariant and can be used to generate new feasible trajectories,\nwhich could be used for training. Based on this data augmentation idea, we\nformulate a general framework, called Invariant Transform Experience Replay\nthat we present with two techniques: (i) Kaleidoscope Experience Replay\nexploits reflectional symmetries and (ii) Goal-augmented Experience Replay\nwhich takes advantage of lax goal definitions. In the Fetch tasks from OpenAI\nGym, our experimental results show significant increases in learning rates and\nsuccess rates. Particularly, we attain a 13, 3, and 5 times speedup in the\npushing, sliding, and pick-and-place tasks respectively in the multi-goal\nsetting. Performance gains are also observed in similar tasks with obstacles\nand we successfully deployed a trained policy on a real Baxter robot. Our work\ndemonstrates that invariant transformations on RL trajectories are a promising\nmethodology to speed up learning in deep RL.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 04:34:58 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 07:31:08 GMT"}, {"version": "v3", "created": "Sat, 14 Dec 2019 02:42:27 GMT"}, {"version": "v4", "created": "Thu, 12 Mar 2020 09:27:44 GMT"}, {"version": "v5", "created": "Tue, 9 Jun 2020 03:51:26 GMT"}, {"version": "v6", "created": "Sun, 5 Jul 2020 03:19:06 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Lin", "Yijiong", ""], ["Huang", "Jiancong", ""], ["Zimmer", "Matthieu", ""], ["Guan", "Yisheng", ""], ["Rojas", "Juan", ""], ["Weng", "Paul", ""]]}, {"id": "1909.10754", "submitter": "Seonguk Park", "authors": "SeongUk Park, Nojun Kwak", "title": "FEED: Feature-level Ensemble for Knowledge Distillation", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) aims to transfer knowledge in a teacher-student\nframework, by providing the predictions of the teacher network to the student\nnetwork in the training stage to help the student network generalize better. It\ncan use either a teacher with high capacity or {an} ensemble of multiple\nteachers. However, the latter is not convenient when one wants to use\nfeature-map-based distillation methods. For a solution, this paper proposes a\nversatile and powerful training algorithm named FEature-level Ensemble for\nknowledge Distillation (FEED), which aims to transfer the ensemble knowledge\nusing multiple teacher networks. We introduce a couple of training algorithms\nthat transfer ensemble knowledge to the student at the feature map level. Among\nthe feature-map-based distillation methods, using several non-linear\ntransformations in parallel for transferring the knowledge of the multiple\nteacher{s} helps the student find more generalized solutions. We name this\nmethod as parallel FEED, andexperimental results on CIFAR-100 and ImageNet show\nthat our method has clear performance enhancements, without introducing any\nadditional parameters or computations at test time. We also show the\nexperimental results of sequentially feeding teacher's information to the\nstudent, hence the name sequential FEED, and discuss the lessons obtained.\nAdditionally, the empirical results on measuring the reconstruction errors at\nthe feature map give hints for the enhancements.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 08:14:40 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Park", "SeongUk", ""], ["Kwak", "Nojun", ""]]}, {"id": "1909.10823", "submitter": "Patr\\'icia Alves-Oliveira", "authors": "Patr\\'icia Alves-Oliveira, Samuel Gomes, Ankita Chandak, Patr\\'icia\n  Arriaga, Guy Hoffman, Ana Paiva", "title": "Software architecture for YOLO, a creativity-stimulating robot", "comments": "17 pages, 7 figurs, 2 tables, open-source code, submitted to\n  SoftwareX journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  YOLO is a social robot designed and developed to stimulate creativity in\nchildren through storytelling activities. Children use it as a character in\ntheir stories. This article details the artificial intelligence software\ndeveloped for YOLO. The implemented software schedules through several\nCreativity Behaviors to find the ones that stimulate creativity more\neffectively. YOLO can choose between convergent and divergent thinking\ntechniques, two important processes of creative thought. These techniques were\ndeveloped based on the psychological theories of creativity development and on\nresearch from creativity experts who work with children. Additionally, this\nsoftware allows the creation of Social Behaviors that enable the robot to\nbehave as a believable character. On top of our framework, we built 3 main\nsocial behavior parameters: Exuberant, Aloof, and Harmonious. These behaviors\nare meant to ease immersive play and the process of character creation. The 3\nsocial behaviors were based on psychological theories of personality and\ndeveloped using children's input during co-design studies. Overall, this work\npresents an attempt to design, develop, and deploy social robots that nurture\nintrinsic human abilities, such as the ability to be creative.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 11:40:05 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Alves-Oliveira", "Patr\u00edcia", ""], ["Gomes", "Samuel", ""], ["Chandak", "Ankita", ""], ["Arriaga", "Patr\u00edcia", ""], ["Hoffman", "Guy", ""], ["Paiva", "Ana", ""]]}, {"id": "1909.10838", "submitter": "Thierry Deruyttere", "authors": "Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic, Luc Van Gool\n  and Marie-Francine Moens", "title": "Talk2Car: Taking Control of Your Self-Driving Car", "comments": "14 pages, accepted at emnlp-ijcnlp 2019 - Added Talk2Nav Reference", "journal-ref": null, "doi": "10.18653/v1/D19-1215", "report-no": null, "categories": "cs.AI cs.CL cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A long-term goal of artificial intelligence is to have an agent execute\ncommands communicated through natural language. In many cases the commands are\ngrounded in a visual environment shared by the human who gives the command and\nthe agent. Execution of the command then requires mapping the command into the\nphysical visual space, after which the appropriate action can be taken. In this\npaper we consider the former. Or more specifically, we consider the problem in\nan autonomous driving setting, where a passenger requests an action that can be\nassociated with an object found in a street scene. Our work presents the\nTalk2Car dataset, which is the first object referral dataset that contains\ncommands written in natural language for self-driving cars. We provide a\ndetailed comparison with related datasets such as ReferIt, RefCOCO, RefCOCO+,\nRefCOCOg, Cityscape-Ref and CLEVR-Ref. Additionally, we include a performance\nanalysis using strong state-of-the-art models. The results show that the\nproposed object referral task is a challenging one for which the models show\npromising results but still require additional research in natural language\nprocessing, computer vision and the intersection of these fields. The dataset\ncan be found on our website: http://macchina-ai.eu/\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 12:29:27 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 06:27:52 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Deruyttere", "Thierry", ""], ["Vandenhende", "Simon", ""], ["Grujicic", "Dusan", ""], ["Van Gool", "Luc", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1909.10863", "submitter": "Philip Ball", "authors": "Noor Sajid, Philip J. Ball, Thomas Parr, Karl J. Friston", "title": "Active inference: demystified and compared", "comments": null, "journal-ref": "Neural Computation 2021", "doi": "10.1162/neco_a_01357", "report-no": null, "categories": "cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active inference is a first principle account of how autonomous agents\noperate in dynamic, non-stationary environments. This problem is also\nconsidered in reinforcement learning (RL), but limited work exists on comparing\nthe two approaches on the same discrete-state environments. In this paper, we\nprovide: 1) an accessible overview of the discrete-state formulation of active\ninference, highlighting natural behaviors in active inference that are\ngenerally engineered in RL; 2) an explicit discrete-state comparison between\nactive inference and RL on an OpenAI gym baseline. We begin by providing a\ncondensed overview of the active inference literature, in particular viewing\nthe various natural behaviors of active inference agents through the lens of\nRL. We show that by operating in a pure belief-based setting, active inference\nagents can carry out epistemic exploration, and account for uncertainty about\ntheir environment in a Bayes-optimal fashion. Furthermore, we show that the\nreliance on an explicit reward signal in RL is removed in active inference,\nwhere reward can simply be treated as another observation; even in the total\nabsence of rewards, agent behaviors are learned through preference learning. We\nmake these properties explicit by showing two scenarios in which active\ninference agents can infer behaviors in reward-free environments compared to\nboth Q-learning and Bayesian model-based RL agents; by placing zero prior\npreferences over rewards and by learning the prior preferences over the\nobservations corresponding to reward. We conclude by noting that this formalism\ncan be applied to more complex settings if appropriate generative models can be\nformulated. In short, we aim to demystify the behavior of active inference\nagents by presenting an accessible discrete state-space and time formulation,\nand demonstrate these behaviors in a OpenAI gym environment, alongside RL\nagents.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 13:08:53 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 18:39:04 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 16:23:48 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Sajid", "Noor", ""], ["Ball", "Philip J.", ""], ["Parr", "Thomas", ""], ["Friston", "Karl J.", ""]]}, {"id": "1909.10893", "submitter": "Anirudh Goyal", "authors": "Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey\n  Levine, Yoshua Bengio, Bernhard Sch\\\"olkopf", "title": "Recurrent Independent Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning modular structures which reflect the dynamics of the environment can\nlead to better generalization and robustness to changes which only affect a few\nof the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a\nnew recurrent architecture in which multiple groups of recurrent cells operate\nwith nearly independent transition dynamics, communicate only sparingly through\nthe bottleneck of attention, and are only updated at time steps where they are\nmost relevant. We show that this leads to specialization amongst the RIMs,\nwhich in turn allows for dramatically improved generalization on tasks where\nsome factors of variation differ systematically between training and\nevaluation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 13:28:00 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 18:56:25 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 19:32:36 GMT"}, {"version": "v4", "created": "Sun, 11 Oct 2020 16:47:11 GMT"}, {"version": "v5", "created": "Thu, 12 Nov 2020 00:48:33 GMT"}, {"version": "v6", "created": "Tue, 17 Nov 2020 05:23:43 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Goyal", "Anirudh", ""], ["Lamb", "Alex", ""], ["Hoffmann", "Jordan", ""], ["Sodhani", "Shagun", ""], ["Levine", "Sergey", ""], ["Bengio", "Yoshua", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1909.10941", "submitter": "Samer Chehade", "authors": "Samer Chehade (M2S, Tech-CICO), Nada Matta (Tech-CICO), Jean-Baptiste\n  Pothin (M2S), R\\'emi Cogranne (LM2S)", "title": "Data Interpretation Support in Rescue Operations: Application for French\n  Firefighters", "comments": null, "journal-ref": "2018 IEEE/ACS 15th International Conference on Computer Systems\n  and Applications (AICCSA), Oct 2018, Aqaba, Jordan", "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims at developing a system that supports French firefighters in\ndata interpretation during rescue operations. An application ontology is\nproposed based on existing crisis management ones and operational expertise\ncollection. After that, a knowledge-based system will be developed and\nintegrated in firefighters' environment. Our first studies are shown in this\npaper.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 14:09:26 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Chehade", "Samer", "", "M2S, Tech-CICO"], ["Matta", "Nada", "", "Tech-CICO"], ["Pothin", "Jean-Baptiste", "", "M2S"], ["Cogranne", "R\u00e9mi", "", "LM2S"]]}, {"id": "1909.10994", "submitter": "Martin Ritzert", "authors": "Emilie Grienenberger and Martin Ritzert", "title": "Learning definable hypotheses on trees", "comments": "Full version of ICDT 2019 paper", "journal-ref": null, "doi": "10.4230/LIPIcs.ICDT.2019.24", "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning properties of nodes in tree structures.\nThose properties are specified by logical formulas, such as formulas from\nfirst-order or monadic second-order logic. We think of the tree as a database\nencoding a large dataset and therefore aim for learning algorithms which depend\nat most sublinearly on the size of the tree. We present a learning algorithm\nfor quantifier-free formulas where the running time only depends polynomially\non the number of training examples, but not on the size of the background\nstructure. By a previous result on strings we know that for general first-order\nor monadic second-order (MSO) formulas a sublinear running time cannot be\nachieved. However, we show that by building an index on the tree in a linear\ntime preprocessing phase, we can achieve a learning algorithm for MSO formulas\nwith a logarithmic learning phase.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 15:22:39 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Grienenberger", "Emilie", ""], ["Ritzert", "Martin", ""]]}, {"id": "1909.11004", "submitter": "Mehdi Ghayoumi", "authors": "Mehdi Ghayoumi, Maryam Pourebadi", "title": "Fuzzy Knowledge-Based Architecture for Learning and Interaction in\n  Social Robots", "comments": "7 pages, AI-HRI 2019", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/21", "categories": "cs.RO cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an extension of our presented cognitive-based\nemotion model [27][28]and [30], where we enhance our knowledge-based emotion\nunit of the architecture by embedding a fuzzy rule-based system to it. The\nmodel utilizes the cognitive parameters dependency and their corresponding\nweights to regulate the robot's behavior and fuse their behavior data to\nachieve the final decision in their interaction with the environment. Using\nthis fuzzy system, our previous model can simulate linguistic parameters for\nbetter controlling and generating understandable and flexible behaviors in the\nrobots. We implement our model on an assistive healthcare robot, named Robot\nNurse Assistant (RNA) and test it with human subjects. Our model records all\nthe emotion states and essential information based on its predefined rules and\nlearning system. Our results show that our robot interacts with patients in a\nreasonable, faithful way in special conditions which are defined by rules. This\nwork has the potential to provide better on-demand service for clinical experts\nto monitor the patients' emotion states and help them make better decisions\naccordingly.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 07:15:37 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Ghayoumi", "Mehdi", ""], ["Pourebadi", "Maryam", ""]]}, {"id": "1909.11025", "submitter": "Nicholas Hoernle", "authors": "Nicholas Hoernle, Kobi Gal, Barbara Grosz, Leilah Lyons, Ada Ren,\n  Andee Rubin", "title": "Interpretable Models for Understanding Immersive Simulations", "comments": "To be published in IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes methods for comparative evaluation of the\ninterpretability of models of high dimensional time series data inferred by\nunsupervised machine learning algorithms. The time series data used in this\ninvestigation were logs from an immersive simulation like those commonly used\nin education and healthcare training. The structures learnt by the models\nprovide representations of participants' activities in the simulation which are\nintended to be meaningful to people's interpretation. To choose the model that\ninduces the best representation, we designed two interpretability tests, each\nof which evaluates the extent to which a model's output aligns with people's\nexpectations or intuitions of what has occurred in the simulation. We compared\nthe performance of the models on these interpretability tests to their\nperformance on statistical information criteria. We show that the models that\noptimize interpretability quality differ from those that optimize (statistical)\ninformation theoretic criteria. Furthermore, we found that a model using a\nfully Bayesian approach performed well on both the statistical and\nhuman-interpretability measures. The Bayesian approach is a good candidate for\nfully automated model selection, i.e., when direct empirical investigations of\ninterpretability are costly or infeasible.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 16:22:09 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 08:40:30 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hoernle", "Nicholas", ""], ["Gal", "Kobi", ""], ["Grosz", "Barbara", ""], ["Lyons", "Leilah", ""], ["Ren", "Ada", ""], ["Rubin", "Andee", ""]]}, {"id": "1909.11060", "submitter": "Shane Steinert-Threlkeld", "authors": "Shane Steinert-Threlkeld", "title": "Paying Attention to Function Words", "comments": "Emergent Communication Workshop @ NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All natural languages exhibit a distinction between content words (like nouns\nand adjectives) and function words (like determiners, auxiliaries,\nprepositions). Yet surprisingly little has been said about the emergence of\nthis universal architectural feature of natural languages. Why have human\nlanguages evolved to exhibit this division of labor between content and\nfunction words? How could such a distinction have emerged in the first place?\nThis paper takes steps towards answering these questions by showing how the\ndistinction can emerge through reinforcement learning in agents playing a\nsignaling game across contexts which contain multiple objects that possess\nmultiple perceptually salient gradable properties.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 17:18:48 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Steinert-Threlkeld", "Shane", ""]]}, {"id": "1909.11084", "submitter": "Gabriel Istrate", "authors": "Gabriel Istrate, Cosmin Bonchis and Claudiu Gatina", "title": "It's Not Whom You Know, It's What You (or Your Friends) Can Do: Succint\n  Coalitional Frameworks for Network Centralities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the representation of measures of network centrality using a\nframework that blends a social network representation with the succint\nformalism of cooperative skill games. We discuss the expressiveness of the new\nframework and highlight some of its advantages, including a fixed-parameter\ntractability result for computing centrality measures under such\nrepresentations. As an application we introduce new network centrality measures\nthat capture the extent to which neighbors of a certain node can help it\ncomplete relevant tasks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 17:59:26 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Istrate", "Gabriel", ""], ["Bonchis", "Cosmin", ""], ["Gatina", "Claudiu", ""]]}, {"id": "1909.11145", "submitter": "Timo C. Wunderlich", "authors": "Timo C. Wunderlich, Akos F. Kungl, Eric M\\\"uller, Johannes Schemmel,\n  Mihai Petrovici", "title": "Brain-Inspired Hardware for Artificial Intelligence: Accelerated\n  Learning in a Physical-Model Spiking Neural Network", "comments": null, "journal-ref": "Artificial Neural Networks and Machine Learning - ICANN 2019:\n  Theoretical Neural Computation. ICANN 2019. Lecture Notes in Computer\n  Science, vol 11727. Springer, Cham", "doi": "10.1007/978-3-030-30487-4_10", "report-no": null, "categories": "cs.NE cs.AI cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future developments in artificial intelligence will profit from the existence\nof novel, non-traditional substrates for brain-inspired computing. Neuromorphic\ncomputers aim to provide such a substrate that reproduces the brain's\ncapabilities in terms of adaptive, low-power information processing. We present\nresults from a prototype chip of the BrainScaleS-2 mixed-signal neuromorphic\nsystem that adopts a physical-model approach with a 1000-fold acceleration of\nspiking neural network dynamics relative to biological real time. Using the\nembedded plasticity processor, we both simulate the Pong arcade video game and\nimplement a local plasticity rule that enables reinforcement learning, allowing\nthe on-chip neural network to learn to play the game. The experiment\ndemonstrates key aspects of the employed approach, such as accelerated and\nflexible learning, high energy efficiency and resilience to noise.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 19:29:30 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 09:02:01 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Wunderlich", "Timo C.", ""], ["Kungl", "Akos F.", ""], ["M\u00fcller", "Eric", ""], ["Schemmel", "Johannes", ""], ["Petrovici", "Mihai", ""]]}, {"id": "1909.11173", "submitter": "Chris Amato", "authors": "Christopher Amato and Andrea Baisero", "title": "Active Goal Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To coordinate with other systems, agents must be able to determine what the\nsystems are currently doing and predict what they will be doing in the\nfuture---plan and goal recognition. There are many methods for plan and goal\nrecognition, but they assume a passive observer that continually monitors the\ntarget system. Real-world domains, where information gathering has a cost\n(e.g., moving a camera or a robot, or time taken away from another task), will\noften require a more active observer. We propose to combine goal recognition\nwith other observer tasks in order to obtain \\emph{active goal recognition}\n(AGR). We discuss this problem and provide a model and preliminary experimental\nresults for one form of this composite problem. As expected, the results show\nthat optimal behavior in AGR problems balance information gathering with other\nactions (e.g., task completion) such as to achieve all tasks jointly and\nefficiently. We hope that our formulation opens the door for extensive further\nresearch on this interesting and realistic problem.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 21:00:13 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Amato", "Christopher", ""], ["Baisero", "Andrea", ""]]}, {"id": "1909.11200", "submitter": "Yanpei Shi", "authors": "Yanpei Shi, Qiang Huang, Thomas Hain", "title": "Improving Noise Robustness In Speaker Identification Using A Two-Stage\n  Attention Model", "comments": "Submitted to Interspeech2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the use of deep neural networks has significantly boosted speaker\nrecognition performance, it is still challenging to separate speakers in poor\nacoustic environments. To improve robustness of speaker recognition system\nperformance in noise, a novel two-stage attention mechanism which can be used\nin existing architectures such as Time Delay Neural Networks (TDNNs) and\nConvolutional Neural Networks (CNNs) is proposed. Noise is known to often mask\nimportant information in both time and frequency domain. The proposed mechanism\nallows the models to concentrate on reliable time/frequency components of the\nsignal. The proposed approach is evaluated using the Voxceleb1 dataset, which\naims at assessment of speaker recognition in real world situations. In addition\nthree types of noise at different signal-noise-ratios (SNRs) were added for\nthis work. The proposed mechanism is compared with three strong baselines:\nX-vectors, Attentive X-vector, and Resnet-34. Results on both identification\nand verification tasks show that the two-stage attention mechanism consistently\nimproves upon these for all noise conditions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 21:46:42 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 22:53:15 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Shi", "Yanpei", ""], ["Huang", "Qiang", ""], ["Hain", "Thomas", ""]]}, {"id": "1909.11236", "submitter": "Bardienus Duisterhof", "authors": "Bardienus P. Duisterhof, Srivatsan Krishnan, Jonathan J. Cruz, Colby\n  R. Banbury, William Fu, Aleksandra Faust, Guido C. H. E. de Croon, Vijay\n  Janapa Reddi", "title": "Learning to Seek: Autonomous Source Seeking with Deep Reinforcement\n  Learning Onboard a Nano Drone Microcontroller", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present fully autonomous source seeking onboard a highly constrained nano\nquadcopter, by contributing application-specific system and observation feature\ndesign to enable inference of a deep-RL policy onboard a nano quadcopter. Our\ndeep-RL algorithm finds a high-performance solution to a challenging problem,\neven in presence of high noise levels and generalizes across real and\nsimulation environments with different obstacle configurations. We verify our\napproach with simulation and in-field testing on a Bitcraze CrazyFlie using\nonly the cheap and ubiquitous Cortex-M4 microcontroller unit. The results show\nthat by end-to-end application-specific system design, our contribution\nconsumes almost three times less additional power, as compared to competing\nlearning-based navigation approach onboard a nano quadcopter. Thanks to our\nobservation space, which we carefully design within the resource constraints,\nour solution achieves a 94% success rate in cluttered and randomized test\nenvironments, as compared to the previously achieved 80%. We also compare our\nstrategy to a simple finite state machine (FSM), geared towards efficient\nexploration, and demonstrate that our policy is more robust and resilient at\nobstacle avoidance as well as up to 70% more efficient in source seeking. To\nthis end, we contribute a cheap and lightweight end-to-end tiny robot learning\n(tinyRL) solution, running onboard a nano quadcopter, that proves to be robust\nand efficient in a challenging task using limited sensory input.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 00:10:33 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 14:24:07 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 15:24:27 GMT"}, {"version": "v4", "created": "Wed, 5 Feb 2020 12:50:00 GMT"}, {"version": "v5", "created": "Fri, 11 Dec 2020 07:40:07 GMT"}, {"version": "v6", "created": "Fri, 15 Jan 2021 09:30:34 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Duisterhof", "Bardienus P.", ""], ["Krishnan", "Srivatsan", ""], ["Cruz", "Jonathan J.", ""], ["Banbury", "Colby R.", ""], ["Fu", "William", ""], ["Faust", "Aleksandra", ""], ["de Croon", "Guido C. H. E.", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "1909.11287", "submitter": "Zehao Lin", "authors": "Zehao Lin, Xinjing Huang, Feng Ji, Haiqing Chen, Ying Zhang", "title": "Task-Oriented Conversation Generation Using Heterogeneous Memory\n  Networks", "comments": "Accepted as a long paper at EMNLP-IJCNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to incorporate external knowledge into a neural dialogue model is\ncritically important for dialogue systems to behave like real humans. To handle\nthis problem, memory networks are usually a great choice and a promising way.\nHowever, existing memory networks do not perform well when leveraging\nheterogeneous information from different sources. In this paper, we propose a\nnovel and versatile external memory networks called Heterogeneous Memory\nNetworks (HMNs), to simultaneously utilize user utterances, dialogue history\nand background knowledge tuples. In our method, historical sequential dialogues\nare encoded and stored into the context-aware memory enhanced by gating\nmechanism while grounding knowledge tuples are encoded and stored into the\ncontext-free memory. During decoding, the decoder augmented with HMNs\nrecurrently selects each word in one response utterance from these two memories\nand a general vocabulary. Experimental results on multiple real-world datasets\nshow that HMNs significantly outperform the state-of-the-art data-driven\ntask-oriented dialogue models in most domains.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 04:40:27 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Lin", "Zehao", ""], ["Huang", "Xinjing", ""], ["Ji", "Feng", ""], ["Chen", "Haiqing", ""], ["Zhang", "Ying", ""]]}, {"id": "1909.11288", "submitter": "Nway Han Nway", "authors": "Nway Nway Han, Aye Thida", "title": "Annotated Guidelines and Building Reference Corpus for Myanmar-English\n  Word Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reference corpus for word alignment is an important resource for developing\nand evaluating word alignment methods. For Myanmar-English language pairs,\nthere is no reference corpus to evaluate the word alignment tasks. Therefore,\nwe created the guidelines for Myanmar-English word alignment annotation between\ntwo languages over contrastive learning and built the Myanmar-English reference\ncorpus consisting of verified alignments from Myanmar ALT of the Asian Language\nTreebank (ALT). This reference corpus contains confident labels sure (S) and\npossible (P) for word alignments which are used to test for the purpose of\nevaluation of the word alignments tasks. We discuss the most linking\nambiguities to define consistent and systematic instructions to align manual\nwords. We evaluated the results of annotators agreement using our reference\ncorpus in terms of alignment error rate (AER) in word alignment tasks and\ndiscuss the words relationships in terms of BLEU scores.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 04:47:49 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Han", "Nway Nway", ""], ["Thida", "Aye", ""]]}, {"id": "1909.11334", "submitter": "Xiaoran Xu", "authors": "Xiaoran Xu, Wei Feng, Yunsheng Jiang, Xiaohui Xie, Zhiqing Sun,\n  Zhi-Hong Deng", "title": "Dynamically Pruned Message Passing Networks for Large-Scale Knowledge\n  Graph Reasoning", "comments": "ICLR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Dynamically Pruned Message Passing Networks (DPMPN) for\nlarge-scale knowledge graph reasoning. In contrast to existing models,\nembedding-based or path-based, we learn an input-dependent subgraph to\nexplicitly model reasoning process. Subgraphs are dynamically constructed and\nexpanded by applying graphical attention mechanism conditioned on input\nqueries. In this way, we not only construct graph-structured explanations but\nalso enable message passing designed in Graph Neural Networks (GNNs) to scale\nwith graph sizes. We take the inspiration from the consciousness prior proposed\nby and develop a two-GNN framework to simultaneously encode input-agnostic full\ngraph representation and learn input-dependent local one coordinated by an\nattention module. Experiments demonstrate the reasoning capability of our model\nthat is to provide clear graphical explanations as well as deliver accurate\npredictions, outperforming most state-of-the-art methods in knowledge base\ncompletion tasks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 08:15:41 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 07:57:11 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 21:39:59 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Xu", "Xiaoran", ""], ["Feng", "Wei", ""], ["Jiang", "Yunsheng", ""], ["Xie", "Xiaohui", ""], ["Sun", "Zhiqing", ""], ["Deng", "Zhi-Hong", ""]]}, {"id": "1909.11373", "submitter": "Jiachen Li", "authors": "Jiachen Li, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Keith\n  Ross, Henrik Iskov Christensen, Hao Su", "title": "Multi-task Batch Reinforcement Learning with Metric Learning", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the Multi-task Batch Reinforcement Learning problem. Given multiple\ndatasets collected from different tasks, we train a multi-task policy to\nperform well in unseen tasks sampled from the same distribution. The task\nidentities of the unseen tasks are not provided. To perform well, the policy\nmust infer the task identity from collected transitions by modelling its\ndependency on states, actions and rewards. Because the different datasets may\nhave state-action distributions with large divergence, the task inference\nmodule can learn to ignore the rewards and spuriously correlate $\\textit{only}$\nstate-action pairs to the task identity, leading to poor test time performance.\nTo robustify task inference, we propose a novel application of the triplet\nloss. To mine hard negative examples, we relabel the transitions from the\ntraining tasks by approximating their reward functions. When we allow further\ntraining on the unseen tasks, using the trained policy as an initialization\nleads to significantly faster convergence compared to randomly initialized\npolicies (up to $80\\%$ improvement and across 5 different Mujoco task\ndistributions). We name our method $\\textbf{MBML}$\n($\\textbf{M}\\text{ulti-task}$ $\\textbf{B}\\text{atch}$ RL with\n$\\textbf{M}\\text{etric}$ $\\textbf{L}\\text{earning}$).\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 09:49:01 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 20:57:44 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 08:31:42 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 03:58:33 GMT"}, {"version": "v5", "created": "Thu, 1 Oct 2020 20:12:10 GMT"}, {"version": "v6", "created": "Fri, 23 Oct 2020 20:01:46 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Li", "Jiachen", ""], ["Vuong", "Quan", ""], ["Liu", "Shuang", ""], ["Liu", "Minghua", ""], ["Ciosek", "Kamil", ""], ["Ross", "Keith", ""], ["Christensen", "Henrik Iskov", ""], ["Su", "Hao", ""]]}, {"id": "1909.11456", "submitter": "Dongrui Wu", "authors": "Yuqi Cuui and Yifan Xu and Dongrui Wu", "title": "EEG-Based Driver Drowsiness Estimation Using Feature Weighted Episodic\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drowsy driving is pervasive, and also a major cause of traffic accidents.\nEstimating a driver's drowsiness level by monitoring the electroencephalogram\n(EEG) signal and taking preventative actions accordingly may improve driving\nsafety. However, individual differences among different drivers make this task\nvery challenging. A calibration session is usually required to collect some\nsubject-specific data and tune the model parameters before applying it to a new\nsubject, which is very inconvenient and not user-friendly. Many approaches have\nbeen proposed to reduce the calibration effort, but few can completely\neliminate it. This paper proposes a novel approach, feature weighted episodic\ntraining (FWET), to completely eliminate the calibration requirement. It\nintegrates two techniques: feature weighting to learn the importance of\ndifferent features, and episodic training for domain generalization.\nExperiments on EEG-based driver drowsiness estimation demonstrated that both\nfeature weighting and episodic training are effective, and their integration\ncan further improve the generalization performance. FWET does not need any\nlabelled or unlabelled calibration data from the new subject, and hence could\nbe very useful in plug-and-play brain-computer interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 12:52:02 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Cuui", "Yuqi", ""], ["Xu", "Yifan", ""], ["Wu", "Dongrui", ""]]}, {"id": "1909.11469", "submitter": "Vinu Joseph", "authors": "Mark Van der Merwe, Vinu Joseph, Ganesh Gopalakrishnan", "title": "Message Scheduling for Performant, Many-Core Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CV cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Belief Propagation (BP) is a message-passing algorithm for approximate\ninference over Probabilistic Graphical Models (PGMs), finding many applications\nsuch as computer vision, error-correcting codes, and protein-folding. While\ngeneral, the convergence and speed of the algorithm has limited its practical\nuse on difficult inference problems. As an algorithm that is highly amenable to\nparallelization, many-core Graphical Processing Units (GPUs) could\nsignificantly improve BP performance. Improving BP through many-core systems is\nnon-trivial: the scheduling of messages in the algorithm strongly affects\nperformance. We present a study of message scheduling for BP on GPUs. We\ndemonstrate that BP exhibits a tradeoff between speed and convergence based on\nparallelism and show that existing message schedulings are not able to utilize\nthis tradeoff. To this end, we present a novel randomized message scheduling\napproach, Randomized BP (RnBP), which outperforms existing methods on the GPU.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 05:19:33 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Van der Merwe", "Mark", ""], ["Joseph", "Vinu", ""], ["Gopalakrishnan", "Ganesh", ""]]}, {"id": "1909.11538", "submitter": "Majid Moghadam", "authors": "Ali Alizadeh, Majid Moghadam, Yunus Bicer, Nazim Kemal Ure, Ugur\n  Yavas, Can Kurtulus", "title": "Automated Lane Change Decision Making using Deep Reinforcement Learning\n  in Dynamic and Uncertain Highway Environment", "comments": "Accepted to IEEE Intelligent Transportation Systems Conference - ITSC\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous lane changing is a critical feature for advanced autonomous\ndriving systems, that involves several challenges such as uncertainty in other\ndriver's behaviors and the trade-off between safety and agility. In this work,\nwe develop a novel simulation environment that emulates these challenges and\ntrain a deep reinforcement learning agent that yields consistent performance in\na variety of dynamic and uncertain traffic scenarios. Results show that the\nproposed data-driven approach performs significantly better in noisy\nenvironments compared to methods that rely solely on heuristics.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 02:27:07 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Alizadeh", "Ali", ""], ["Moghadam", "Majid", ""], ["Bicer", "Yunus", ""], ["Ure", "Nazim Kemal", ""], ["Yavas", "Ugur", ""], ["Kurtulus", "Can", ""]]}, {"id": "1909.11581", "submitter": "Andrea Micheli", "authors": "Alessandro Valentini, Andrea Micheli and Alessandro Cimatti", "title": "Temporal Planning with Intermediate Conditions and Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated temporal planning is the technology of choice when controlling\nsystems that can execute more actions in parallel and when temporal\nconstraints, such as deadlines, are needed in the model. One limitation of\nseveral action-based planning systems is that actions are modeled as intervals\nhaving conditions and effects only at the extremes and as invariants, but no\nconditions nor effects can be specified at arbitrary points or sub-intervals.\nIn this paper, we address this limitation by providing an effective\nheuristic-search technique for temporal planning, allowing the definition of\nactions with conditions and effects at any arbitrary time within the action\nduration. We experimentally demonstrate that our approach is far better than\nstandard encodings in PDDL 2.1 and is competitive with other approaches that\ncan (directly or indirectly) represent intermediate action conditions or\neffects.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 16:16:51 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Valentini", "Alessandro", ""], ["Micheli", "Andrea", ""], ["Cimatti", "Alessandro", ""]]}, {"id": "1909.11583", "submitter": "Simon Schmitt", "authors": "Simon Schmitt, Matteo Hessel, Karen Simonyan", "title": "Off-Policy Actor-Critic with Shared Experience Replay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the combination of actor-critic reinforcement learning\nalgorithms with uniform large-scale experience replay and propose solutions for\ntwo challenges: (a) efficient actor-critic learning with experience replay (b)\nstability of off-policy learning where agents learn from other agents\nbehaviour. We employ those insights to accelerate hyper-parameter sweeps in\nwhich all participating agents run concurrently and share their experience via\na common replay module. To this end we analyze the bias-variance tradeoffs in\nV-trace, a form of importance sampling for actor-critic methods. Based on our\nanalysis, we then argue for mixing experience sampled from replay with\non-policy experience, and propose a new trust region scheme that scales\neffectively to data distributions where V-trace becomes unstable. We provide\nextensive empirical validation of the proposed solution. We further show the\nbenefits of this setup by demonstrating state-of-the-art data efficiency on\nAtari among agents trained up until 200M environment frames.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 16:20:46 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 12:51:59 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Schmitt", "Simon", ""], ["Hessel", "Matteo", ""], ["Simonyan", "Karen", ""]]}, {"id": "1909.11591", "submitter": "Mohammadhosein Hasanbeig", "authors": "Lim Zun Yuan, Mohammadhosein Hasanbeig, Alessandro Abate, Daniel\n  Kroening", "title": "Modular Deep Reinforcement Learning with Temporal Logic Specifications", "comments": "arXiv admin note: text overlap with arXiv:1902.00778", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an actor-critic, model-free, and online Reinforcement Learning\n(RL) framework for continuous-state continuous-action Markov Decision Processes\n(MDPs) when the reward is highly sparse but encompasses a high-level temporal\nstructure. We represent this temporal structure by a finite-state machine and\nconstruct an on-the-fly synchronised product with the MDP and the finite\nmachine. The temporal structure acts as a guide for the RL agent within the\nproduct, where a modular Deep Deterministic Policy Gradient (DDPG) architecture\nis proposed to generate a low-level control policy. We evaluate our framework\nin a Mars rover experiment and we present the success rate of the synthesised\npolicy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 18:10:00 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 12:57:35 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Yuan", "Lim Zun", ""], ["Hasanbeig", "Mohammadhosein", ""], ["Abate", "Alessandro", ""], ["Kroening", "Daniel", ""]]}, {"id": "1909.11604", "submitter": "Xudong Liu", "authors": "Xudong Liu, Christian Fritz, Matthew Klenk", "title": "An Extensible and Personalizable Multi-Modal Trip Planner", "comments": "Published in the Proceedings of the 32nd International Florida\n  Artificial Intelligence Research Society Conference, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite a tremendous amount of work in the literature and in the commercial\nsectors, current approaches to multi-modal trip planning still fail to\nconsistently generate plans that users deem optimal in practice. We believe\nthat this is due to the fact that current planners fail to capture the true\npreferences of users, e.g., their preferences depend on aspects that are not\nmodeled. An example of this could be a preference not to walk through an unsafe\narea at night. We present a novel multi-modal trip planner that allows users to\nupload auxiliary geographic data (e.g., crime rates) and to specify temporal\nconstraints and preferences over these data in combination with typical metrics\nsuch as time and cost. Concretely, our planner supports the modes walking,\nbiking, driving, public transit, and taxi, uses linear temporal logic to\ncapture temporal constraints, and preferential cost functions to represent\npreferences. We show by examples that this allows the expression of very\ninteresting preferences and constraints that, naturally, lead to quite diverse\noptimal plans.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 16:38:49 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Liu", "Xudong", ""], ["Fritz", "Christian", ""], ["Klenk", "Matthew", ""]]}, {"id": "1909.11637", "submitter": "Haytham Elmousalami", "authors": "Haytham H. Elmousalami", "title": "Comparison of Artificial Intelligence Techniques for Project Conceptual\n  Cost Prediction", "comments": "arXiv admin note: text overlap with arXiv:1905.11804", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a reliable parametric cost model at the conceptual stage of the\nproject is crucial for projects managers and decision-makers. Existing methods,\nsuch as probabilistic and statistical algorithms have been developed for\nproject cost prediction. However, these methods are unable to produce accurate\nresults for conceptual cost prediction due to small and unstable data samples.\nArtificial intelligence (AI) and machine learning (ML) algorithms include\nnumerous models and algorithms for supervised regression applications.\nTherefore, a comparison analysis for AI models is required to guide\npractitioners to the appropriate model. The study focuses on investigating\ntwenty artificial intelligence (AI) techniques which are conducted for cost\nmodeling such as fuzzy logic (FL) model, artificial neural networks (ANNs),\nmultiple regression analysis (MRA), case-based reasoning (CBR), hybrid models,\nand ensemble methods such as scalable boosting trees (XGBoost). Field canals\nimprovement projects (FCIPs) are used as an actual case study to analyze the\nperformance of the applied ML models. Out of 20 AI techniques, the results\nshowed that the most accurate and suitable method is XGBoost with 9.091% and\n0.929 based on Mean Absolute Percentage Error (MAPE) and adjusted R2. Nonlinear\nadaptability, handling missing values and outliers, model interpretation and\nuncertainty have been discussed for the twenty developed AI models. Keywords:\nArtificial intelligence, Machine learning, ensemble methods, XGBoost,\nevolutionary fuzzy rules generation, Conceptual cost, and parametric cost\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 20:16:30 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Elmousalami", "Haytham H.", ""]]}, {"id": "1909.11700", "submitter": "Claudius Gros", "authors": "Claudius Gros", "title": "A generic framework for task selection driven by synthetic emotions", "comments": "IEEE conference on Humanized Computing, Laguna Hills 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a certain complexity level, humanized agents may select from a wide\nrange of possible tasks, with each activity corresponding to a transient goal.\nIn general there will be no overarching credit assignment scheme allowing to\ncompare available options with respect to expected utilities. For this\nsituation we propose a task selection framework that is based on time\nallocation via emotional stationarity (TAES). Emotions are argued to correspond\nto abstract criteria, such as satisfaction, challenge and boredom, along which\nactivities that have been carried out can be evaluated. The resulting timeline\nof experienced emotions is then compared with the `character' of the agent,\nwhich is defined in terms of a preferred distribution of emotional states. The\nlong-term goal of the agent, to align experience with character, is achieved by\noptimizing the frequency for selecting the individual tasks. Upon optimization,\nthe statistics of emotion experience becomes stationary.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 18:35:11 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Gros", "Claudius", ""]]}, {"id": "1909.11730", "submitter": "Andrew Hundt", "authors": "Andrew Hundt and Benjamin Killeen and Nicholas Greene and Hongtao Wu\n  and Heeyeon Kwon and Chris Paxton and Gregory D. Hager", "title": "\"Good Robot!\": Efficient Reinforcement Learning for Multi-Step Visual\n  Tasks with Sim to Real Transfer", "comments": "Accepted to the journal IEEE Robotics and Automation Letters (RA-L)\n  and to be presented at IROS 2020. This is a minor update to v3. 8 pages, 6\n  figures, 3 tables, 1 algorithm. Code is available at\n  https://github.com/jhu-lcsr/good_robot and a video overview is at\n  https://youtu.be/MbCuEZadkIw", "journal-ref": null, "doi": "10.1109/LRA.2020.3015448", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current Reinforcement Learning (RL) algorithms struggle with long-horizon\ntasks where time can be wasted exploring dead ends and task progress may be\neasily reversed. We develop the SPOT framework, which explores within action\nsafety zones, learns about unsafe regions without exploring them, and\nprioritizes experiences that reverse earlier progress to learn with remarkable\nefficiency.\n  The SPOT framework successfully completes simulated trials of a variety of\ntasks, improving a baseline trial success rate from 13% to 100% when stacking 4\ncubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when\nclearing toys arranged in adversarial patterns. Efficiency with respect to\nactions per trial typically improves by 30% or more, while training takes just\n1-20k actions, depending on the task.\n  Furthermore, we demonstrate direct sim to real transfer. We are able to\ncreate real stacks in 100% of trials with 61% efficiency and real rows in 100%\nof trials with 59% efficiency by directly loading the simulation-trained model\non the real robot with no additional real-world fine-tuning. To our knowledge,\nthis is the first instance of reinforcement learning with successful sim to\nreal transfer applied to long term multi-step tasks such as block-stacking and\nrow-making with consideration of progress reversal. Code is available at\nhttps://github.com/jhu-lcsr/good_robot .\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 19:50:36 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 21:05:14 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 21:42:30 GMT"}, {"version": "v4", "created": "Sat, 15 Aug 2020 18:10:40 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Hundt", "Andrew", ""], ["Killeen", "Benjamin", ""], ["Greene", "Nicholas", ""], ["Wu", "Hongtao", ""], ["Kwon", "Heeyeon", ""], ["Paxton", "Chris", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1909.11765", "submitter": "Zitao Liu", "authors": "Jiahao Chen, Hang Li, Wenxin Wang, Wenbiao Ding, Gale Yan Huang, Zitao\n  Liu", "title": "A Multimodal Alerting System for Online Class Quality Assurance", "comments": "The 20th International Conference on Artificial Intelligence in\n  Education(AIED), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online 1 on 1 class is created for more personalized learning experience. It\ndemands a large number of teaching resources, which are scarce in China. To\nalleviate this problem, we build a platform (marketplace), i.e., \\emph{Dahai}\nto allow college students from top Chinese universities to register as\npart-time instructors for the online 1 on 1 classes. To warn the unqualified\ninstructors and ensure the overall education quality, we build a monitoring and\nalerting system by utilizing multimodal information from the online\nenvironment. Our system mainly consists of two key components: banned word\ndetector and class quality predictor. The system performance is demonstrated\nboth offline and online. By conducting experimental evaluation of real-world\nonline courses, we are able to achieve 74.3\\% alerting accuracy in our\nproduction environment.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 10:23:44 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Chen", "Jiahao", ""], ["Li", "Hang", ""], ["Wang", "Wenxin", ""], ["Ding", "Wenbiao", ""], ["Huang", "Gale Yan", ""], ["Liu", "Zitao", ""]]}, {"id": "1909.11766", "submitter": "Lionel Robert", "authors": "Qiaoning Zhang, Connor Esterwood, X. Jessie Yang, Lionel P. Robert Jr", "title": "An Automated Vehicle (AV) like Me? The Impact of Personality\n  Similarities and Differences between Humans and AVs", "comments": "4 pages, 2 figures, 2019 AAAI Fall Symposium on Artificial\n  Intelligence for Human-Robot Interaction", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/01", "categories": "cs.HC cs.AI cs.CY cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  To better understand the impacts of similarities and dissimilarities in human\nand AV personalities we conducted an experimental study with 443 individuals.\nGenerally, similarities in human and AV personalities led to a higher\nperception of AV safety only when both were high in specific personality\ntraits. Dissimilarities in human and AV personalities also yielded a higher\nperception of AV safety, but only when the AV was higher than the human in a\nparticular personality trait.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 13:51:28 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Zhang", "Qiaoning", ""], ["Esterwood", "Connor", ""], ["Yang", "X. Jessie", ""], ["Robert", "Lionel P.", "Jr"]]}, {"id": "1909.11821", "submitter": "Yueh-Hua Wu", "authors": "Yueh-Hua Wu, Ting-Han Fan, Peter J. Ramadge, and Hao Su", "title": "Model Imitation for Model-Based Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based reinforcement learning (MBRL) aims to learn a dynamic model to\nreduce the number of interactions with real-world environments. However, due to\nestimation error, rollouts in the learned model, especially those of long\nhorizons, fail to match the ones in real-world environments. This mismatching\nhas seriously impacted the sample complexity of MBRL. The phenomenon can be\nattributed to the fact that previous works employ supervised learning to learn\nthe one-step transition models, which has inherent difficulty ensuring the\nmatching of distributions from multi-step rollouts. Based on the claim, we\npropose to learn the transition model by matching the distributions of\nmulti-step rollouts sampled from the transition model and the real ones via\nWGAN. We theoretically show that matching the two can minimize the difference\nof cumulative rewards between the real transition and the learned one. Our\nexperiments also show that the proposed Model Imitation method can compete or\noutperform the state-of-the-art in terms of sample complexity and average\nreturn.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 23:52:30 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 06:19:21 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 05:47:10 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wu", "Yueh-Hua", ""], ["Fan", "Ting-Han", ""], ["Ramadge", "Peter J.", ""], ["Su", "Hao", ""]]}, {"id": "1909.11830", "submitter": "Vitaly Kurin", "authors": "Vitaly Kurin, Saad Godil, Shimon Whiteson, Bryan Catanzaro", "title": "Can $Q$-Learning with Graph Networks Learn a Generalizable Branching\n  Heuristic for a SAT Solver?", "comments": "Camera-ready for NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Graph-$Q$-SAT, a branching heuristic for a Boolean SAT solver\ntrained with value-based reinforcement learning (RL) using Graph Neural\nNetworks for function approximation. Solvers using Graph-$Q$-SAT are complete\nSAT solvers that either provide a satisfying assignment or proof of\nunsatisfiability, which is required for many SAT applications. The branching\nheuristics commonly used in SAT solvers make poor decisions during their\nwarm-up period, whereas Graph-$Q$-SAT is trained to examine the structure of\nthe particular problem instance to make better decisions early in the search.\nTraining Graph-$Q$-SAT is data efficient and does not require elaborate dataset\npreparation or feature engineering. We train Graph-$Q$-SAT using RL interfacing\nwith MiniSat solver and show that Graph-$Q$-SAT can reduce the number of\niterations required to solve SAT problems by 2-3X. Furthermore, it generalizes\nto unsatisfiable SAT instances, as well as to problems with 5X more variables\nthan it was trained on. We show that for larger problems, reductions in the\nnumber of iterations lead to wall clock time reductions, the ultimate goal when\ndesigning heuristics. We also show positive zero-shot transfer behavior when\ntesting Graph-$Q$-SAT on a task family different from that used for training.\nWhile more work is needed to apply Graph-$Q$-SAT to reduce wall clock time in\nmodern SAT solving settings, it is a compelling proof-of-concept showing that\nRL equipped with Graph Neural Networks can learn a generalizable branching\nheuristic for SAT search.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 00:44:40 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 10:28:55 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Kurin", "Vitaly", ""], ["Godil", "Saad", ""], ["Whiteson", "Shimon", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1909.11840", "submitter": "Shushman Choudhury", "authors": "Shushman Choudhury, Kiril Solovey, Mykel J. Kochenderfer, Marco Pavone", "title": "Efficient Large-Scale Multi-Drone Delivery Using Transit Networks", "comments": "Final version of IEEE ICRA 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of controlling a large fleet of drones to deliver\npackages simultaneously across broad urban areas. To conserve energy, drones\nhop between public transit vehicles (e.g., buses and trams). We design a\ncomprehensive algorithmic framework that strives to minimize the maximum time\nto complete any delivery. We address the multifaceted complexity of the problem\nthrough a two-layer approach. First, the upper layer assigns drones to package\ndelivery sequences with a near-optimal polynomial-time task allocation\nalgorithm. Then, the lower layer executes the allocation by periodically\nrouting the fleet over the transit network while employing efficient\nbounded-suboptimal multi-agent pathfinding techniques tailored to our setting.\nExperiments demonstrate the efficiency of our approach on settings with up to\n$200$ drones, $5000$ packages, and transit networks with up to $8000$ stops in\nSan Francisco and Washington DC. Our results show that the framework computes\nsolutions typically within a few seconds on commodity hardware, and that drones\ntravel up to $360 \\%$ of their flight range with public transit.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 01:47:18 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 01:15:24 GMT"}, {"version": "v3", "created": "Sun, 26 Apr 2020 22:19:13 GMT"}, {"version": "v4", "created": "Thu, 11 Jun 2020 19:13:42 GMT"}, {"version": "v5", "created": "Tue, 5 Jan 2021 18:52:23 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Choudhury", "Shushman", ""], ["Solovey", "Kiril", ""], ["Kochenderfer", "Mykel J.", ""], ["Pavone", "Marco", ""]]}, {"id": "1909.11851", "submitter": "Christian Szegedy", "authors": "Dennis Lee, Christian Szegedy, Markus N. Rabe, Sarah M. Loos and\n  Kshitij Bansal", "title": "Mathematical Reasoning in Latent Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and conduct a simple experiment to study whether neural networks\ncan perform several steps of approximate reasoning in a fixed dimensional\nlatent space. The set of rewrites (i.e. transformations) that can be\nsuccessfully performed on a statement represents essential semantic features of\nthe statement. We can compress this information by embedding the formula in a\nvector space, such that the vector associated with a statement can be used to\npredict whether a statement can be rewritten by other theorems. Predicting the\nembedding of a formula generated by some rewrite rule is naturally viewed as\napproximate reasoning in the latent space. In order to measure the\neffectiveness of this reasoning, we perform approximate deduction sequences in\nthe latent space and use the resulting embedding to inform the semantic\nfeatures of the corresponding formal statement (which is obtained by performing\nthe corresponding rewrite sequence using real formulas). Our experiments show\nthat graph neural networks can make non-trivial predictions about the\nrewrite-success of statements, even when they propagate predicted latent\nrepresentations for several steps. Since our corpus of mathematical formulas\nincludes a wide variety of mathematical disciplines, this experiment is a\nstrong indicator for the feasibility of deduction in latent space in general.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 02:33:07 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Lee", "Dennis", ""], ["Szegedy", "Christian", ""], ["Rabe", "Markus N.", ""], ["Loos", "Sarah M.", ""], ["Bansal", "Kshitij", ""]]}, {"id": "1909.11864", "submitter": "Yao Zhu", "authors": "Yao Zhu, Hongzhi Liu, Zhonghai Wu, Yang Song and Tao Zhang", "title": "Representation Learning with Ordered Relation Paths for Knowledge Graph\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incompleteness is a common problem for existing knowledge graphs (KGs), and\nthe completion of KG which aims to predict links between entities is\nchallenging. Most existing KG completion methods only consider the direct\nrelation between nodes and ignore the relation paths which contain useful\ninformation for link prediction. Recently, a few methods take relation paths\ninto consideration but pay less attention to the order of relations in paths\nwhich is important for reasoning. In addition, these path-based models always\nignore nonlinear contributions of path features for link prediction. To solve\nthese problems, we propose a novel KG completion method named OPTransE. Instead\nof embedding both entities of a relation into the same latent space as in\nprevious methods, we project the head entity and the tail entity of each\nrelation into different spaces to guarantee the order of relations in the path.\nMeanwhile, we adopt a pooling strategy to extract nonlinear and complex\nfeatures of different paths to further improve the performance of link\nprediction. Experimental results on two benchmark datasets show that the\nproposed model OPTransE performs better than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 03:07:53 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Zhu", "Yao", ""], ["Liu", "Hongzhi", ""], ["Wu", "Zhonghai", ""], ["Song", "Yang", ""], ["Zhang", "Tao", ""]]}, {"id": "1909.11890", "submitter": "Kapil Sharma Prof.", "authors": "Ashish Kumar Tripathi, Kapil Sharma, Manju Bala", "title": "Military Dog Based Optimizer and its Application to Fake Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last three decades more then sixty meta-heuristic algorithms have\nbeen proposed by the various authors. Such algorithms are inspired from\nphysical phenomena, animal behavior or evolutionary concepts. These algorithms\nhave been widely used for solving the various real world optimization problems.\nResearchers are continuously working to improve the existing algorithms and\nalso proposing new algorithms that are giving competitive results as compared\nto the existing algorithms present in the literature. In this paper a novel\nmeta heuristic algorithm based on military dogs squad is introduced. The\nproposed algorithm mimics the searching capability of the trained military\ndogs. Military dogs have strong smell senses by which they are able to search\nthe suspicious objects like bombs, wildlife scats, currency, or blood as well\nas they can communicate with each other by their barking. The performance of\nthe proposed algorithm is tested on 17 benchmark functions and compared with\nfive other meta-heuristics namely particle swarm optimization (PSO), multiverse\noptimizer (MVO), genetic algorithm (GA), probability based learning (PBIL) and\nevolutionary strategy (ES). The results are validated in terms of mean and\nstandard deviation of the fitness value. The convergence behavior and\nconsistency of the results have been also validated by plotting convergence\ngraphs and BoxPlots. Further the, proposed algorithm is successfully utilized\nto solve the real world fake review detection problem. The experimental results\ndemonstrate that the proposed algorithm outperforms the other considered\nalgorithms on the majority of performance parameters.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 04:49:31 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Tripathi", "Ashish Kumar", ""], ["Sharma", "Kapil", ""], ["Bala", "Manju", ""]]}, {"id": "1909.11939", "submitter": "Yannis Flet-Berliac", "authors": "Yannis Flet-Berliac, Philippe Preux", "title": "MERL: Multi-Head Reinforcement Learning", "comments": "Deep Reinforcement Learning Workshop, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common challenge in reinforcement learning is how to convert the agent's\ninteractions with an environment into fast and robust learning. For instance,\nearlier work makes use of domain knowledge to improve existing reinforcement\nlearning algorithms in complex tasks. While promising, previously acquired\nknowledge is often costly and challenging to scale up. Instead, we decide to\nconsider problem knowledge with signals from quantities relevant to solve any\ntask, e.g., self-performance assessment and accurate expectations.\n$\\mathcal{V}^{ex}$ is such a quantity. It is the fraction of variance explained\nby the value function $V$ and measures the discrepancy between $V$ and the\nreturns. Taking advantage of $\\mathcal{V}^{ex}$, we propose MERL, a general\nframework for structuring reinforcement learning by injecting problem knowledge\ninto policy gradient updates. As a result, the agent is not only optimized for\na reward but learns using problem-focused quantities provided by MERL,\napplicable out-of-the-box to any task. In this paper: (a) We introduce and\ndefine MERL, the multi-head reinforcement learning framework we use throughout\nthis work. (b) We conduct experiments across a variety of standard benchmark\nenvironments, including 9 continuous control tasks, where results show improved\nperformance. (c) We demonstrate that MERL also improves transfer learning on a\nset of challenging pixel-based tasks. (d) We ponder how MERL tackles the\nproblem of reward sparsity and better conditions the feature space of\nreinforcement learning agents.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 06:57:51 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 15:31:28 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 12:17:17 GMT"}, {"version": "v4", "created": "Tue, 15 Oct 2019 09:15:11 GMT"}, {"version": "v5", "created": "Fri, 29 Nov 2019 14:24:35 GMT"}, {"version": "v6", "created": "Tue, 31 Mar 2020 07:57:20 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Flet-Berliac", "Yannis", ""], ["Preux", "Philippe", ""]]}, {"id": "1909.11942", "submitter": "Zhenzhong Lan", "authors": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\n  Sharma, Radu Soricut", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing model size when pretraining natural language representations often\nresults in improved performance on downstream tasks. However, at some point\nfurther model increases become harder due to GPU/TPU memory limitations and\nlonger training times. To address these problems, we present two\nparameter-reduction techniques to lower memory consumption and increase the\ntraining speed of BERT. Comprehensive empirical evidence shows that our\nproposed methods lead to models that scale much better compared to the original\nBERT. We also use a self-supervised loss that focuses on modeling\ninter-sentence coherence, and show it consistently helps downstream tasks with\nmulti-sentence inputs. As a result, our best model establishes new\nstate-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having\nfewer parameters compared to BERT-large. The code and the pretrained models are\navailable at https://github.com/google-research/ALBERT.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 07:06:13 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 03:22:00 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 02:19:07 GMT"}, {"version": "v4", "created": "Fri, 10 Jan 2020 19:00:02 GMT"}, {"version": "v5", "created": "Mon, 3 Feb 2020 04:01:33 GMT"}, {"version": "v6", "created": "Sun, 9 Feb 2020 03:00:18 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Lan", "Zhenzhong", ""], ["Chen", "Mingda", ""], ["Goodman", "Sebastian", ""], ["Gimpel", "Kevin", ""], ["Sharma", "Piyush", ""], ["Soricut", "Radu", ""]]}, {"id": "1909.11980", "submitter": "Lina Rojas-Barahona", "authors": "Lina M. Rojas-Barahona, Pascal Bellec, Benoit Besset, Martinho\n  Dos-Santos, Johannes Heinecke, Munshi Asadullah, Olivier Le-Blouch, Jean Y.\n  Lancien, G\\'eraldine Damnati, Emmanuel Mory and Fr\\'ed\\'eric Herledan", "title": "Spoken Conversational Search for General Knowledge", "comments": "SIGDial2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a spoken conversational question answering proof of concept that\nis able to answer questions about general knowledge from Wikidata. The dialogue\ncomponent does not only orchestrate various components but also solve\ncoreferences and ellipsis.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 08:46:02 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Rojas-Barahona", "Lina M.", ""], ["Bellec", "Pascal", ""], ["Besset", "Benoit", ""], ["Dos-Santos", "Martinho", ""], ["Heinecke", "Johannes", ""], ["Asadullah", "Munshi", ""], ["Le-Blouch", "Olivier", ""], ["Lancien", "Jean Y.", ""], ["Damnati", "G\u00e9raldine", ""], ["Mory", "Emmanuel", ""], ["Herledan", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1909.11994", "submitter": "Filippo Bistaffa", "authors": "Ewa Andrejczuk, Filippo Bistaffa, Christian Blum, Juan A.\n  Rodr\\'iguez-Aguilar, Carles Sierra", "title": "Synergistic Team Composition: A Computational Approach to Foster\n  Diversity in Teams", "comments": "Accepted version", "journal-ref": "Volume 182, 15 October 2019, page 104799", "doi": "10.1016/j.knosys.2019.06.007", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-operative learning in heterogeneous teams refers to learning methods in\nwhich teams are organised both to accomplish academic tasks and for individuals\nto gain knowledge. Competencies, personality and the gender of team members are\nkey factors that influence team performance. Here, we introduce a team\ncomposition problem, the so-called synergistic team composition problem (STCP),\nwhich incorporates such key factors when arranging teams. Thus, the goal of the\nSTCP is to partition a set of individuals into a set of synergistic teams:\nteams that are diverse in personality and gender and whose members cover all\nrequired competencies to complete a task. Furthermore, the STCP requires that\nall teams are balanced in that they are expected to exhibit similar\nperformances when completing the task. We propose two efficient algorithms to\nsolve the STCP. Our first algorithm is based on a linear programming\nformulation and is appropriate to solve small instances of the problem. Our\nsecond algorithm is an anytime heuristic that is effective for large instances\nof the STCP. Finally, we thoroughly study the computational properties of both\nalgorithms in an educational context when grouping students in a classroom into\nteams using actual-world data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 09:24:25 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Andrejczuk", "Ewa", ""], ["Bistaffa", "Filippo", ""], ["Blum", "Christian", ""], ["Rodr\u00edguez-Aguilar", "Juan A.", ""], ["Sierra", "Carles", ""]]}, {"id": "1909.12032", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek and S{\\l}awomir T. Wierzcho\\'n", "title": "Query Optimization Properties of Modified VBS", "comments": "7 pages, 2 figures; published as: M.A. K{\\l}opotek, S.T. Wierzcho\\'n:\n  Query optimization properties of modified Valuation-Based Systems. [in:] R.\n  Trappl Ed.: Cybernetics and Systems . Proc. 13th European Meeting on\n  Cybernetics and System Research, Vienna, 9-12 April 1996, Vol. I. Austrian\n  Society for Cybernetic Studies, 1996, pp. 335-340", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valuation-Based~System can represent knowledge in different domains including\nprobability theory, Dempster-Shafer theory and possibility theory. More recent\nstudies show that the framework of VBS is also appropriate for representing and\nsolving Bayesian decision problems and optimization problems.\n  In this paper after introducing the valuation based system (VBS) framework,\nwe present Markov-like properties of VBS and a method for resolving queries to\nVBS.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 11:23:41 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""], ["Wierzcho\u0144", "S\u0142awomir T.", ""]]}, {"id": "1909.12063", "submitter": "Qi Deng", "authors": "Qi Deng", "title": "Artificial Intelligence BlockCloud (AIBC) Technical Whitepaper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AIBC is an Artificial Intelligence and blockchain technology based\nlarge-scale decentralized ecosystem that allows system-wide low-cost sharing of\ncomputing and storage resources. The AIBC consists of four layers: a\nfundamental layer, a resource layer, an application layer, and an ecosystem\nlayer. The AIBC implements a two-consensus scheme to enforce upper-layer\neconomic policies and achieve fundamental layer performance and robustness: the\nDPoEV incentive consensus on the application and resource layers, and the DABFT\ndistributed consensus on the fundamental layer. The DABFT uses deep learning\ntechniques to predict and select the most suitable BFT algorithm in order to\nachieve the best balance of performance, robustness, and security. The DPoEV\nuses the knowledge map algorithm to accurately assess the economic value of\ndigital assets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 12:49:50 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Deng", "Qi", ""]]}, {"id": "1909.12066", "submitter": "Jan Deriu", "authors": "Jan Deriu, Mark Cieliebak", "title": "Towards a Metric for Automated Conversational Dialogue System Evaluation\n  and Improvement", "comments": "8 Pages, To be published at the INLG 2019 converence", "journal-ref": "Proceedings of the 12th International Conference on Natural\n  Language Generation. 2019", "doi": "10.18653/v1/W19-8654", "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present \"AutoJudge\", an automated evaluation method for conversational\ndialogue systems. The method works by first generating dialogues based on\nself-talk, i.e. dialogue systems talking to itself. Then, it uses human ratings\non these dialogues to train an automated judgement model. Our experiments show\nthat AutoJudge correlates well with the human ratings and can be used to\nautomatically evaluate dialogue systems, even in deployed systems. In a second\npart, we attempt to apply AutoJudge to improve existing systems. This works\nwell for re-ranking a set of candidate utterances. However, our experiments\nshow that AutoJudge cannot be applied as reward for reinforcement learning,\nalthough the metric can distinguish good from bad dialogues. We discuss\npotential reasons, but state here already that this is still an open question\nfor further research.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 12:55:14 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 08:00:47 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Deriu", "Jan", ""], ["Cieliebak", "Mark", ""]]}, {"id": "1909.12072", "submitter": "Wojciech Samek", "authors": "Wojciech Samek and Klaus-Robert M\\\"uller", "title": "Towards Explainable Artificial Intelligence", "comments": "19 pages", "journal-ref": null, "doi": "10.1007/978-3-030-28954-6_1", "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, machine learning (ML) has become a key enabling technology\nfor the sciences and industry. Especially through improvements in methodology,\nthe availability of large databases and increased computational power, today's\nML algorithms are able to achieve excellent performance (at times even\nexceeding the human level) on an increasing number of complex tasks. Deep\nlearning models are at the forefront of this development. However, due to their\nnested non-linear structure, these powerful models have been generally\nconsidered \"black boxes\", not providing any information about what exactly\nmakes them arrive at their predictions. Since in many applications, e.g., in\nthe medical domain, such lack of transparency may be not acceptable, the\ndevelopment of methods for visualizing, explaining and interpreting deep\nlearning models has recently attracted increasing attention. This introductory\npaper presents recent developments and applications in this field and makes a\nplea for a wider use of explainable learning algorithms in practice.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 13:05:58 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1909.12086", "submitter": "Jun Quan", "authors": "Jun Quan, Deyi Xiong, Bonnie Webber and Changjian Hu", "title": "GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution\n  Model for Task-Oriented Dialogue", "comments": "accepted to appear at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ellipsis and co-reference are common and ubiquitous especially in multi-turn\ndialogues. In this paper, we treat the resolution of ellipsis and co-reference\nin dialogue as a problem of generating omitted or referred expressions from the\ndialogue context. We therefore propose a unified end-to-end Generative Ellipsis\nand CO-reference Resolution model (GECOR) in the context of dialogue. The model\ncan generate a new pragmatically complete user utterance by alternating the\ngeneration and copy mode for each user utterance. A multi-task learning\nframework is further proposed to integrate the GECOR into an end-to-end\ntask-oriented dialogue. In order to train both the GECOR and the multi-task\nlearning framework, we manually construct a new dataset on the basis of the\npublic dataset CamRest676 with both ellipsis and co-reference annotation. On\nthis dataset, intrinsic evaluations on the resolution of ellipsis and\nco-reference show that the GECOR model significantly outperforms the\nsequence-to-sequence (seq2seq) baseline model in terms of EM, BLEU and F1 while\nextrinsic evaluations on the downstream dialogue task demonstrate that our\nmulti-task learning framework with GECOR achieves a higher success rate of task\ncompletion than TSCP, a state-of-the-art end-to-end task-oriented dialogue\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 13:34:26 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Quan", "Jun", ""], ["Xiong", "Deyi", ""], ["Webber", "Bonnie", ""], ["Hu", "Changjian", ""]]}, {"id": "1909.12104", "submitter": "Blai Bonet", "authors": "Blai Bonet and Hector Geffner", "title": "Action Selection for MDPs: Anytime AO* vs. UCT", "comments": "Proceedings AAAI-12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of non-admissible heuristics, A* and other best-first\nalgorithms can be converted into anytime optimal algorithms over OR graphs, by\nsimply continuing the search after the first solution is found. The same trick,\nhowever, does not work for best-first algorithms over AND/OR graphs, that must\nbe able to expand leaf nodes of the explicit graph that are not necessarily\npart of the best partial solution. Anytime optimal variants of AO* must thus\naddress an exploration-exploitation tradeoff: they cannot just \"exploit\", they\nmust keep exploring as well. In this work, we develop one such variant of AO*\nand apply it to finite-horizon MDPs. This Anytime AO* algorithm eventually\ndelivers an optimal policy while using non-admissible random heuristics that\ncan be sampled, as when the heuristic is the cost of a base policy that can be\nsampled with rollouts. We then test Anytime AO* for action selection over large\ninfinite-horizon MDPs that cannot be solved with existing off-line heuristic\nsearch and dynamic programming algorithms, and compare it with UCT.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 13:51:26 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Bonet", "Blai", ""], ["Geffner", "Hector", ""]]}, {"id": "1909.12135", "submitter": "Blai Bonet", "authors": "Blai Bonet, Giuseppe De Giacomo, Hector Geffner, Sasha Rubin", "title": "Generalized Planning: Non-Deterministic Abstractions and Trajectory\n  Constraints", "comments": "Proceedings IJCAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the characterization and computation of general policies for\nfamilies of problems that share a structure characterized by a common reduction\ninto a single abstract problem. Policies $\\mu$ that solve the abstract problem\nP have been shown to solve all problems Q that reduce to P provided that $\\mu$\nterminates in Q. In this work, we shed light on why this termination condition\nis needed and how it can be removed. The key observation is that the abstract\nproblem P captures the common structure among the concrete problems Q that is\nlocal (Markovian) but misses common structure that is global. We show how such\nglobal structure can be captured by means of trajectory constraints that in\nmany cases can be expressed as LTL formulas, thus reducing generalized planning\nto LTL synthesis. Moreover, for a broad class of problems that involve integer\nvariables that can be increased or decreased, trajectory constraints can be\ncompiled away, reducing generalized planning to fully observable\nnon-deterministic planning.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 14:17:04 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Bonet", "Blai", ""], ["De Giacomo", "Giuseppe", ""], ["Geffner", "Hector", ""], ["Rubin", "Sasha", ""]]}, {"id": "1909.12142", "submitter": "Blai Bonet", "authors": "Florian Pommerening, Malte Helmert, Blai Bonet", "title": "Higher-Dimensional Potential Heuristics for Optimal Classical Planning", "comments": "Proceedings AAAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Potential heuristics for state-space search are defined as weighted sums over\nsimple state features. Atomic features consider the value of a single state\nvariable in a factored state representation, while binary features consider\njoint assignments to two state variables. Previous work showed that the set of\nall admissible and consistent potential heuristics using atomic features can be\ncharacterized by a compact set of linear constraints. We generalize this result\nto binary features and prove a hardness result for features of higher\ndimension. Furthermore, we prove a tractability result based on the treewidth\nof a new graphical structure we call the context-dependency graph. Finally, we\nstudy the relationship of potential heuristics to transition cost partitioning.\nExperimental results show that binary potential heuristics are significantly\nmore informative than the previously considered atomic ones.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 14:24:17 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Pommerening", "Florian", ""], ["Helmert", "Malte", ""], ["Bonet", "Blai", ""]]}, {"id": "1909.12152", "submitter": "Hermann Kaindl", "authors": "Hermann Kaindl and Jonas Ferdigg", "title": "Superintelligence Safety: A Requirements Engineering Perspective", "comments": "First published version, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the headline \"AI safety\", a wide-reaching issue is being discussed,\nwhether in the future some \"superhuman artificial intelligence\" /\n\"superintelligence\" could could pose a threat to humanity. In addition, the\nlate Steven Hawking warned that the rise of robots may be disastrous for\nmankind. A major concern is that even benevolent superhuman artificial\nintelligence (AI) may become seriously harmful if its given goals are not\nexactly aligned with ours, or if we cannot specify precisely its objective\nfunction. Metaphorically, this is compared to king Midas in Greek mythology,\nwho expressed the wish that everything he touched should turn to gold, but\nobviously this wish was not specified precisely enough. In our view, this\nsounds like requirements problems and the challenge of their precise\nformulation. (To our best knowledge, this has not been pointed out yet.) As\nusual in requirements engineering (RE), ambiguity or incompleteness may cause\nproblems. In addition, the overall issue calls for a major RE endeavor,\nfiguring out the wishes and the needs with regard to a superintelligence, which\nwill in our opinion most likely be a very complex software-intensive system\nbased on AI. This may even entail theoretically defining an extended\nrequirements problem.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 14:38:35 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Kaindl", "Hermann", ""], ["Ferdigg", "Jonas", ""]]}, {"id": "1909.12153", "submitter": "Andreas Folkers", "authors": "Andreas Folkers, Matthias Rick, Christof B\\\"uskens", "title": "Controlling an Autonomous Vehicle with Deep Reinforcement Learning", "comments": "Award as Best Student Paper at IEEE Intelligent Vehicles Symposium\n  (IV), 2019", "journal-ref": null, "doi": "10.1109/IVS.2019.8814124", "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a control approach for autonomous vehicles based on deep\nreinforcement learning. A neural network agent is trained to map its estimated\nstate to acceleration and steering commands given the objective of reaching a\nspecific target state while considering detected obstacles. Learning is\nperformed using state-of-the-art proximal policy optimization in combination\nwith a simulated environment. Training from scratch takes five to nine hours.\nThe resulting agent is evaluated within simulation and subsequently applied to\ncontrol a full-size research vehicle. For this, the autonomous exploration of a\nparking lot is considered, including turning maneuvers and obstacle avoidance.\nAltogether, this work is among the first examples to successfully apply deep\nreinforcement learning to a real vehicle.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 06:24:23 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 08:55:20 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Folkers", "Andreas", ""], ["Rick", "Matthias", ""], ["B\u00fcskens", "Christof", ""]]}, {"id": "1909.12217", "submitter": "Amir Ehsan Niaraki Asli", "authors": "Amir Niaraki, Jeremy Roghair, Ali Jannesari", "title": "Visual Exploration and Energy-aware Path Planning via Reinforcement\n  Learning", "comments": "20 Pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual exploration and smart data collection via autonomous vehicles is an\nattractive topic in various disciplines. Disturbances like wind significantly\ninfluence both the power consumption of the flying robots and the performance\nof the camera. We propose a reinforcement learning approach which combines the\neffects of the power consumption and the object detection modules to develop a\npolicy for object detection in large areas with limited battery life. The\nlearning model enables dynamic learning of the negative rewards of each action\nbased on the drag forces that is resulted by the motion of the flying robot\nwith respect to the wind field. The algorithm is implemented in a near-real\nworld simulation environment both for the planar motion and flight in different\naltitudes. The trained agent often performed a trade-off between detecting the\nobjects with high accuracy and increasing the area coverage within its battery\nlife. The developed exploration policy outperformed the complete coverage\nalgorithm by minimizing the traveled path while finding the target objects. The\nperformance of the algorithms under various wind fields was evaluated in planar\nand 3D motion. During an exploration task with sparsely distributed goals and\nwithin a UAV's battery life, the proposed architecture could detect more than\ntwice the amount of goal objects compared to the coverage path planning\nalgorithm in moderate wind field. In high wind intensities, the energy-aware\nalgorithm could detect 4 times the amount of goal objects when compared to its\ncomplete coverage counterpart.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:15:37 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 17:40:34 GMT"}, {"version": "v3", "created": "Sat, 9 Jan 2021 00:42:43 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 21:19:51 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Niaraki", "Amir", ""], ["Roghair", "Jeremy", ""], ["Jannesari", "Ali", ""]]}, {"id": "1909.12238", "submitter": "Francis Song", "authors": "H. Francis Song, Abbas Abdolmaleki, Jost Tobias Springenberg, Aidan\n  Clark, Hubert Soyer, Jack W. Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva\n  Tirumala, Nicolas Heess, Dan Belov, Martin Riedmiller, Matthew M. Botvinick", "title": "V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete\n  and Continuous Control", "comments": "* equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some of the most successful applications of deep reinforcement learning to\nchallenging domains in discrete and continuous control have used policy\ngradient methods in the on-policy setting. However, policy gradients can suffer\nfrom large variance that may limit performance, and in practice require\ncarefully tuned entropy regularization to prevent policy collapse. As an\nalternative to policy gradient algorithms, we introduce V-MPO, an on-policy\nadaptation of Maximum a Posteriori Policy Optimization (MPO) that performs\npolicy iteration based on a learned state-value function. We show that V-MPO\nsurpasses previously reported scores for both the Atari-57 and DMLab-30\nbenchmark suites in the multi-task setting, and does so reliably without\nimportance weighting, entropy regularization, or population-based tuning of\nhyperparameters. On individual DMLab and Atari levels, the proposed algorithm\ncan achieve scores that are substantially higher than has previously been\nreported. V-MPO is also applicable to problems with high-dimensional,\ncontinuous action spaces, which we demonstrate in the context of learning to\ncontrol simulated humanoids with 22 degrees of freedom from full state\nobservations and 56 degrees of freedom from pixel observations, as well as\nexample OpenAI Gym tasks where V-MPO achieves substantially higher asymptotic\nscores than previously reported.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:34:22 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Song", "H. Francis", ""], ["Abdolmaleki", "Abbas", ""], ["Springenberg", "Jost Tobias", ""], ["Clark", "Aidan", ""], ["Soyer", "Hubert", ""], ["Rae", "Jack W.", ""], ["Noury", "Seb", ""], ["Ahuja", "Arun", ""], ["Liu", "Siqi", ""], ["Tirumala", "Dhruva", ""], ["Heess", "Nicolas", ""], ["Belov", "Dan", ""], ["Riedmiller", "Martin", ""], ["Botvinick", "Matthew M.", ""]]}, {"id": "1909.12271", "submitter": "Stephen James", "authors": "Stephen James, Zicong Ma, David Rovick Arrojo, Andrew J. Davison", "title": "RLBench: The Robot Learning Benchmark & Learning Environment", "comments": "Videos and code: https://sites.google.com/view/rlbench", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a challenging new benchmark and learning-environment for robot\nlearning: RLBench. The benchmark features 100 completely unique, hand-designed\ntasks ranging in difficulty, from simple target reaching and door opening, to\nlonger multi-stage tasks, such as opening an oven and placing a tray in it. We\nprovide an array of both proprioceptive observations and visual observations,\nwhich include rgb, depth, and segmentation masks from an over-the-shoulder\nstereo camera and an eye-in-hand monocular camera. Uniquely, each task comes\nwith an infinite supply of demos through the use of motion planners operating\non a series of waypoints given during task creation time; enabling an exciting\nflurry of demonstration-based learning. RLBench has been designed with\nscalability in mind; new tasks, along with their motion-planned demos, can be\neasily created and then verified by a series of tools, allowing users to submit\ntheir own tasks to the RLBench task repository. This large-scale benchmark aims\nto accelerate progress in a number of vision-guided manipulation research\nareas, including: reinforcement learning, imitation learning, multi-task\nlearning, geometric computer vision, and in particular, few-shot learning. With\nthe benchmark's breadth of tasks and demonstrations, we propose the first\nlarge-scale few-shot challenge in robotics. We hope that the scale and\ndiversity of RLBench offers unparalleled research opportunities in the robot\nlearning community and beyond.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 17:26:18 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["James", "Stephen", ""], ["Ma", "Zicong", ""], ["Arrojo", "David Rovick", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1909.12373", "submitter": "Lizhong Chen", "authors": "Drew D. Penney and Lizhong Chen", "title": "A Survey of Machine Learning Applied to Computer Architecture Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has enabled significant benefits in diverse fields, but,\nwith a few exceptions, has had limited impact on computer architecture. Recent\nwork, however, has explored broader applicability for design, optimization, and\nsimulation. Notably, machine learning based strategies often surpass prior\nstate-of-the-art analytical, heuristic, and human-expert approaches. This paper\nreviews machine learning applied system-wide to simulation and run-time\noptimization, and in many individual components, including memory systems,\nbranch predictors, networks-on-chip, and GPUs. The paper further analyzes\ncurrent practice to highlight useful design strategies and identify areas for\nfuture work, based on optimized implementation strategies, opportune extensions\nto existing work, and ambitious long term possibilities. Taken together, these\nstrategies and techniques present a promising future for increasingly automated\narchitectural design.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 20:23:46 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Penney", "Drew D.", ""], ["Chen", "Lizhong", ""]]}, {"id": "1909.12397", "submitter": "Moonkyung Ryu", "authors": "Moonkyung Ryu, Yinlam Chow, Ross Anderson, Christian Tjandraatmadja,\n  Craig Boutilier", "title": "CAQL: Continuous Action Q-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value-based reinforcement learning (RL) methods like Q-learning have shown\nsuccess in a variety of domains. One challenge in applying Q-learning to\ncontinuous-action RL problems, however, is the continuous action maximization\n(max-Q) required for optimal Bellman backup. In this work, we develop CAQL, a\n(class of) algorithm(s) for continuous-action Q-learning that can use several\nplug-and-play optimizers for the max-Q problem. Leveraging recent optimization\nresults for deep neural networks, we show that max-Q can be solved optimally\nusing mixed-integer programming (MIP). When the Q-function representation has\nsufficient power, MIP-based optimization gives rise to better policies and is\nmore robust than approximate methods (e.g., gradient ascent, cross-entropy\nsearch). We further develop several techniques to accelerate inference in CAQL,\nwhich despite their approximate nature, perform well. We compare CAQL with\nstate-of-the-art RL algorithms on benchmark continuous-control problems that\nhave different degrees of action constraints and show that CAQL outperforms\npolicy-based methods in heavily constrained environments, often dramatically.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 21:16:17 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 18:15:34 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 19:29:14 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Ryu", "Moonkyung", ""], ["Chow", "Yinlam", ""], ["Anderson", "Ross", ""], ["Tjandraatmadja", "Christian", ""], ["Boutilier", "Craig", ""]]}, {"id": "1909.12425", "submitter": "Zhiwen Tang", "authors": "Zhiwen Tang, Grace Hui Yang", "title": "Dynamic Search -- Optimizing the Game of Information Seeking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the emerging topic of dynamic search (DS). To position\ndynamic search in a larger research landscape, the article discusses in detail\nits relationship to related research topics and disciplines. The article\nreviews approaches to modeling dynamics during information seeking, with an\nemphasis on Reinforcement Learning (RL)-enabled methods. Details are given for\nhow different approaches are used to model interactions among the human user,\nthe search system, and the environment. The paper ends with a review of\nevaluations of dynamic search systems.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 22:50:13 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 01:40:34 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Tang", "Zhiwen", ""], ["Yang", "Grace Hui", ""]]}, {"id": "1909.12434", "submitter": "Divyansh Kaushik", "authors": "Divyansh Kaushik, Eduard Hovy, Zachary C. Lipton", "title": "Learning the Difference that Makes a Difference with\n  Counterfactually-Augmented Data", "comments": "Published at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite alarm over the reliance of machine learning systems on so-called\nspurious patterns, the term lacks coherent meaning in standard statistical\nframeworks. However, the language of causality offers clarity: spurious\nassociations are due to confounding (e.g., a common cause), but not direct or\nindirect causal effects. In this paper, we focus on natural language\nprocessing, introducing methods and resources for training models less\nsensitive to spurious patterns. Given documents and their initial labels, we\ntask humans with revising each document so that it (i) accords with a\ncounterfactual target label; (ii) retains internal coherence; and (iii) avoids\nunnecessary changes. Interestingly, on sentiment analysis and natural language\ninference tasks, classifiers trained on original data fail on their\ncounterfactually-revised counterparts and vice versa. Classifiers trained on\ncombined datasets perform remarkably well, just shy of those specialized to\neither domain. While classifiers trained on either original or manipulated data\nalone are sensitive to spurious features (e.g., mentions of genre), models\ntrained on the combined data are less sensitive to this signal. Both datasets\nare publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 23:25:25 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 22:32:46 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Kaushik", "Divyansh", ""], ["Hovy", "Eduard", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "1909.12465", "submitter": "Hua Huang", "authors": "Hua Huang, Adrian Barbu", "title": "Playing Atari Ball Games with Hierarchical Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings are particularly good at reasoning and inference from just a few\nexamples. When facing new tasks, humans will leverage knowledge and skills\nlearned before, and quickly integrate them with the new task. In addition to\nlearning by experimentation, human also learn socio-culturally through\ninstructions and learning by example. In this way humans can learn much faster\ncompared with most current artificial intelligence algorithms in many tasks. In\nthis paper, we test the idea of speeding up machine learning through social\nlearning. We argue that in solving real-world problems, especially when the\ntask is designed by humans, and/or for humans, there are typically instructions\nfrom user manuals and/or human experts which give guidelines on how to better\naccomplish the tasks. We argue that these instructions have tremendous value in\ndesigning a reinforcement learning system which can learn in human fashion, and\nwe test the idea by playing the Atari games Tennis and Pong. We experimentally\ndemonstrate that the instructions provide key information about the task, which\ncan be used to decompose the learning task into sub-systems and construct\noptions for the temporally extended planning, and dramatically accelerate the\nlearning process.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 02:09:34 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Huang", "Hua", ""], ["Barbu", "Adrian", ""]]}, {"id": "1909.12499", "submitter": "Mohamadreza Ahmadi", "authors": "Mohamadreza Ahmadi, Masahiro Ono, Michel D. Ingham, Richard M. Murray,\n  and Aaron D. Ames", "title": "Risk-Averse Planning Under Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing policies for partially observable Markov\ndecision processes (POMDPs) with dynamic coherent risk objectives. Synthesizing\nrisk-averse optimal policies for POMDPs requires infinite memory and thus\nundecidable. To overcome this difficulty, we propose a method based on bounded\npolicy iteration for designing stochastic but finite state (memory)\ncontrollers, which takes advantage of standard convex optimization methods.\nGiven a memory budget and optimality criterion, the proposed method modifies\nthe stochastic finite state controller leading to sub-optimal solutions with\nlower coherent risk.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 05:32:02 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Ahmadi", "Mohamadreza", ""], ["Ono", "Masahiro", ""], ["Ingham", "Michel D.", ""], ["Murray", "Richard M.", ""], ["Ames", "Aaron D.", ""]]}, {"id": "1909.12514", "submitter": "Han Liu", "authors": "Han Liu, Xianchao Zhang, Xiaotong Zhang, Qimai Li, Xiao-Ming Wu", "title": "Clustering Uncertain Data via Representative Possible Worlds with\n  Consistency Learning", "comments": "Accepted by IJCAI 2019 Workshops (AI for Internet of Things)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering uncertain data is an essential task in data mining for the\ninternet of things. Possible world based algorithms seem promising for\nclustering uncertain data. However, there are two issues in existing possible\nworld based algorithms: (1) They rely on all the possible worlds and treat them\nequally, but some marginal possible worlds may cause negative effects. (2) They\ndo not well utilize the consistency among possible worlds, since they conduct\nclustering or construct the affinity matrix on each possible world\nindependently. In this paper, we propose a representative possible world based\nconsistent clustering (RPC) algorithm for uncertain data. First, by introducing\nrepresentative loss and using Jensen-Shannon divergence as the distribution\nmeasure, we design a heuristic strategy for the selection of representative\npossible worlds, thus avoiding the negative effects caused by marginal possible\nworlds. Second, we integrate a consistency learning procedure into spectral\nclustering to deal with the representative possible worlds synergistically,\nthus utilizing the consistency to achieve better performance. Experimental\nresults show that our proposed algorithm performs better than the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 06:36:47 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Liu", "Han", ""], ["Zhang", "Xianchao", ""], ["Zhang", "Xiaotong", ""], ["Li", "Qimai", ""], ["Wu", "Xiao-Ming", ""]]}, {"id": "1909.12557", "submitter": "Heechang Ryu", "authors": "Heechang Ryu, Hayong Shin, Jinkyoo Park", "title": "Multi-Agent Actor-Critic with Hierarchical Graph Attention Network", "comments": "Accepted as a conference paper at the Thirty-Fourth AAAI Conference\n  on Artificial Intelligence (AAAI-20), New York, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most previous studies on multi-agent reinforcement learning focus on deriving\ndecentralized and cooperative policies to maximize a common reward and rarely\nconsider the transferability of trained policies to new tasks. This prevents\nsuch policies from being applied to more complex multi-agent tasks. To resolve\nthese limitations, we propose a model that conducts both representation\nlearning for multiple agents using hierarchical graph attention network and\npolicy learning using multi-agent actor-critic. The hierarchical graph\nattention network is specially designed to model the hierarchical relationships\namong multiple agents that either cooperate or compete with each other to\nderive more advanced strategic policies. Two attention networks, the\ninter-agent and inter-group attention layers, are used to effectively model\nindividual and group level interactions, respectively. The two attention\nnetworks have been proven to facilitate the transfer of learned policies to new\ntasks with different agent compositions and allow one to interpret the learned\nstrategies. Empirically, we demonstrate that the proposed model outperforms\nexisting methods in several mixed cooperative and competitive tasks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 08:40:01 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 02:38:58 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Ryu", "Heechang", ""], ["Shin", "Hayong", ""], ["Park", "Jinkyoo", ""]]}, {"id": "1909.12566", "submitter": "Hamid Zafar", "authors": "Hamid Zafar, Maryam Tavakol, Jens Lehmann", "title": "Distantly Supervised Question Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of structured databases for Question Answering (QA) systems has\nled to developing methods, in which the problem of learning the correct answer\nefficiently is based on a linking task between the constituents of the question\nand the corresponding entries in the database. As a result, parsing the\nquestions in order to determine their main elements, which are required for\nanswer retrieval, becomes crucial. However, most datasets for QA systems lack\ngold annotations for parsing, i.e., labels are only available in the form of\n(question, formal-query, answer). In this paper, we propose a distantly\nsupervised learning framework based on reinforcement learning to learn the\nmentions of entities and relations in questions. We leverage the provided\nformal queries to characterize delayed rewards for optimizing a policy gradient\nobjective for the parsing model. An empirical evaluation of our approach shows\na significant improvement in the performance of entity and relation linking\ncompared to the state of the art. We also demonstrate that a more accurate\nparsing component enhances the overall performance of QA systems.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 08:58:20 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 13:31:12 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Zafar", "Hamid", ""], ["Tavakol", "Maryam", ""], ["Lehmann", "Jens", ""]]}, {"id": "1909.12701", "submitter": "Ran Tian", "authors": "Ran Tian, Nan Li, Ilya Kolmanovsky, and Anouck Girard", "title": "Beating humans in a penny-matching game by leveraging cognitive\n  hierarchy theory and Bayesian learning", "comments": "IEEE 2020 American Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a long-standing goal of artificial intelligence (AI) to be superior to\nhuman beings in decision making. Games are suitable for testing AI capabilities\nof making good decisions in non-numerical tasks. In this paper, we develop a\nnew AI algorithm to play the penny-matching game considered in Shannon's\n\"mind-reading machine\" (1953) against human players. In particular, we exploit\ncognitive hierarchy theory and Bayesian learning techniques to continually\nevolve a model for predicting human player decisions, and let the AI player\nmake decisions according to the model predictions to pursue the best chance of\nwinning. Experimental results show that our AI algorithm beats 27 out of 30\nvolunteer human players.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 14:16:50 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 14:41:13 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 06:17:30 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Tian", "Ran", ""], ["Li", "Nan", ""], ["Kolmanovsky", "Ilya", ""], ["Girard", "Anouck", ""]]}, {"id": "1909.12738", "submitter": "Riccardo De Masellis", "authors": "Riccardo De Masellis and Chiara Di Francescomarino and Chiara Ghidini\n  and Sergio Tessaris", "title": "Solving reachability problems on data-aware workflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the field of Business Process Management have brought\nabout several suites able to model complex data objects along with the\ntraditional control flow perspective. Nonetheless, when it comes to formal\nverification there is still the lack of effective verification tools on\nimperative data-aware process models and executions: the data perspective is\noften abstracted away and verification tools are often missing. In this paper\nwe provide a concrete framework for formal verification of reachability\nproperties on imperative data-aware business processes. We start with an\nexpressive, yet empirically tractable class of data-aware process models, an\nextension of Workflow Nets, and we provide a rigorous mapping between the\nsemantics of such models and that of three important paradigms for reasoning\nabout dynamic systems: Action Languages, Classical Planning, and Model\nChecking. Then we perform a comprehensive assessment of the performance of\nthree popular tools supporting the above paradigms in solving reachability\nproblems for imperative data-aware business processes, which paves the way for\na theoretically well founded and practically viable exploitation of formal\nverification techniques on data-aware business processes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 15:15:55 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 15:07:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["De Masellis", "Riccardo", ""], ["Di Francescomarino", "Chiara", ""], ["Ghidini", "Chiara", ""], ["Tessaris", "Sergio", ""]]}, {"id": "1909.12756", "submitter": "Debi Prasanna Mohanty Mr", "authors": "Benu Madhab Changmai, Divija Nagaraju, Debi Prasanna Mohanty, Kriti\n  Singh, Kunal Bansal, Sukumar Moharana", "title": "On-Device User Intent Prediction for Context and Sequence Aware\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The pursuit of improved accuracy in recommender systems has led to the\nincorporation of user context. Context-aware recommender systems typically\nhandle large amounts of data which must be uploaded and stored on the cloud,\nputting the user's personal information at risk. While there have been previous\nstudies on privacy-sensitive and context-aware recommender systems, there has\nnot been a full-fledged system deployed in an isolated mobile environment. We\npropose a secure and efficient on-device mechanism to predict a user's next\nintention. The knowledge of the user's real-time intention can help recommender\nsystems to provide more relevant recommendations at the right moment. Our\nproposed algorithm is both context and sequence aware. We embed user intentions\nas weighted nodes in an n-dimensional vector space where each dimension\nrepresents a specific user context factor. Through a neighborhood searching\nmethod followed by a sequence matching algorithm, we search for the most\nrelevant node to make the prediction. An evaluation of our methodology was done\non a diverse real-world dataset where it was able to address practical\nscenarios like behavior drifts and sequential patterns efficiently and\nrobustly. Our system also outperformed most of the state-of-the-art methods\nwhen evaluated for a similar problem domain on standard datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 13:59:03 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Changmai", "Benu Madhab", ""], ["Nagaraju", "Divija", ""], ["Mohanty", "Debi Prasanna", ""], ["Singh", "Kriti", ""], ["Bansal", "Kunal", ""], ["Moharana", "Sukumar", ""]]}, {"id": "1909.12823", "submitter": "Shayegan Omidshafiei", "authors": "Paul Muller, Shayegan Omidshafiei, Mark Rowland, Karl Tuyls, Julien\n  Perolat, Siqi Liu, Daniel Hennes, Luke Marris, Marc Lanctot, Edward Hughes,\n  Zhe Wang, Guy Lever, Nicolas Heess, Thore Graepel, Remi Munos", "title": "A Generalized Training Approach for Multiagent Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a population-based training regime based on\ngame-theoretic principles called Policy-Spaced Response Oracles (PSRO). PSRO is\ngeneral in the sense that it (1) encompasses well-known algorithms such as\nfictitious play and double oracle as special cases, and (2) in principle\napplies to general-sum, many-player games. Despite this, prior studies of PSRO\nhave been focused on two-player zero-sum games, a regime wherein Nash\nequilibria are tractably computable. In moving from two-player zero-sum games\nto more general settings, computation of Nash equilibria quickly becomes\ninfeasible. Here, we extend the theoretical underpinnings of PSRO by\nconsidering an alternative solution concept, $\\alpha$-Rank, which is unique\n(thus faces no equilibrium selection issues, unlike Nash) and applies readily\nto general-sum, many-player settings. We establish convergence guarantees in\nseveral games classes, and identify links between Nash equilibria and\n$\\alpha$-Rank. We demonstrate the competitive performance of\n$\\alpha$-Rank-based PSRO against an exact Nash solver-based PSRO in 2-player\nKuhn and Leduc Poker. We then go beyond the reach of prior PSRO applications by\nconsidering 3- to 5-player poker games, yielding instances where $\\alpha$-Rank\nachieves faster convergence than approximate Nash solvers, thus establishing it\nas a favorable general games solver. We also carry out an initial empirical\nvalidation in MuJoCo soccer, illustrating the feasibility of the proposed\napproach in another complex domain.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 17:49:53 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 15:04:45 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Muller", "Paul", ""], ["Omidshafiei", "Shayegan", ""], ["Rowland", "Mark", ""], ["Tuyls", "Karl", ""], ["Perolat", "Julien", ""], ["Liu", "Siqi", ""], ["Hennes", "Daniel", ""], ["Marris", "Luke", ""], ["Lanctot", "Marc", ""], ["Hughes", "Edward", ""], ["Wang", "Zhe", ""], ["Lever", "Guy", ""], ["Heess", "Nicolas", ""], ["Graepel", "Thore", ""], ["Munos", "Remi", ""]]}, {"id": "1909.12868", "submitter": "Tong Niu", "authors": "Tong Niu, Mohit Bansal", "title": "Automatically Learning Data Augmentation Policies for Dialogue Tasks", "comments": "7 pages (EMNLP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for\noptimal perturbation policies via a controller trained using performance\nrewards of a sampled policy on the target task, hence reducing data-level model\nbias. While being a powerful algorithm, their work has focused on computer\nvision tasks, where it is comparatively easy to apply imperceptible\nperturbations without changing an image's semantic meaning. In our work, we\nadapt AutoAugment to automatically discover effective perturbation policies for\nnatural language processing (NLP) tasks such as dialogue generation. We start\nwith a pool of atomic operations that apply subtle semantic-preserving\nperturbations to the source inputs of a dialogue task (e.g., different POS-tag\ntypes of stopword dropout, grammatical errors, and paraphrasing). Next, we\nallow the controller to learn more complex augmentation policies by searching\nover the space of the various combinations of these atomic operations.\nMoreover, we also explore conditioning the controller on the source inputs of\nthe target task, since certain strategies may not apply to inputs that do not\ncontain that strategy's required linguistic features. Empirically, we\ndemonstrate that both our input-agnostic and input-aware controllers discover\nuseful data augmentation policies, and achieve significant improvements over\nthe previous state-of-the-art, including trained on manually-designed policies.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 18:40:23 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Niu", "Tong", ""], ["Bansal", "Mohit", ""]]}, {"id": "1909.12892", "submitter": "Andrew Lampinen", "authors": "Sebastien Racaniere, Andrew K. Lampinen, Adam Santoro, David P.\n  Reichert, Vlad Firoiu, Timothy P. Lillicrap", "title": "Automated curricula through setter-solver interactions", "comments": null, "journal-ref": "International Conference on Learning Representations, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning algorithms use correlations between policies and\nrewards to improve agent performance. But in dynamic or sparsely rewarding\nenvironments these correlations are often too small, or rewarding events are\ntoo infrequent to make learning feasible. Human education instead relies on\ncurricula--the breakdown of tasks into simpler, static challenges with dense\nrewards--to build up to complex behaviors. While curricula are also useful for\nartificial agents, hand-crafting them is time consuming. This has lead\nresearchers to explore automatic curriculum generation. Here we explore\nautomatic curriculum generation in rich, dynamic environments. Using a\nsetter-solver paradigm we show the importance of considering goal validity,\ngoal feasibility, and goal coverage to construct useful curricula. We\ndemonstrate the success of our approach in rich but sparsely rewarding 2D and\n3D environments, where an agent is tasked to achieve a single goal selected\nfrom a set of possible goals that varies between episodes, and identify\nchallenges for future work. Finally, we demonstrate the value of a novel\ntechnique that guides agents towards a desired goal distribution. Altogether,\nthese results represent a substantial step towards applying automatic task\ncurricula to learn complex, otherwise unlearnable goals, and to our knowledge\nare the first to demonstrate automated curriculum generation for\ngoal-conditioned agents in environments where the possible goals vary between\nepisodes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 20:11:12 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 00:01:48 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Racaniere", "Sebastien", ""], ["Lampinen", "Andrew K.", ""], ["Santoro", "Adam", ""], ["Reichert", "David P.", ""], ["Firoiu", "Vlad", ""], ["Lillicrap", "Timothy P.", ""]]}, {"id": "1909.12914", "submitter": "David Isele", "authors": "David Isele", "title": "Interactive Decision Making for Autonomous Vehicles in Dense Traffic", "comments": null, "journal-ref": "ITSC 2019", "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense urban traffic environments can produce situations where accurate\nprediction and dynamic models are insufficient for successful autonomous\nvehicle motion planning. We investigate how an autonomous agent can safely\nnegotiate with other traffic participants, enabling the agent to handle\npotential deadlocks. Specifically we consider merges where the gap between cars\nis smaller than the size of the ego vehicle. We propose a game theoretic\nframework capable of generating and responding to interactive behaviors. Our\nmain contribution is to show how game-tree decision making can be executed by\nan autonomous vehicle, including approximations and reasoning that make the\ntree-search computationally tractable. Additionally, to test our model we\ndevelop a stochastic rule-based traffic agent capable of generating interactive\nbehaviors that can be used as a benchmark for simulating traffic participants\nin a crowded merge setting.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 20:44:41 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Isele", "David", ""]]}, {"id": "1909.12925", "submitter": "David Isele", "authors": "Anahita Mohseni-Kabir, David Isele, and Kikuo Fujimura", "title": "Interaction-Aware Multi-Agent Reinforcement Learning for Mobile Agents\n  with Individual Goals", "comments": null, "journal-ref": "ICRA 2019", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multi-agent setting, the optimal policy of a single agent is largely\ndependent on the behavior of other agents. We investigate the problem of\nmulti-agent reinforcement learning, focusing on decentralized learning in\nnon-stationary domains for mobile robot navigation. We identify a cause for the\ndifficulty in training non-stationary policies: mutual adaptation to\nsub-optimal behaviors, and we use this to motivate a curriculum-based strategy\nfor learning interactive policies. The curriculum has two stages. First, the\nagent leverages policy gradient algorithms to learn a policy that is capable of\nachieving multiple goals. Second, the agent learns a modifier policy to learn\nhow to interact with other agents in a multi-agent setting. We evaluated our\napproach on both an autonomous driving lane-change domain and a robot\nnavigation domain.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 20:49:05 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Mohseni-Kabir", "Anahita", ""], ["Isele", "David", ""], ["Fujimura", "Kikuo", ""]]}, {"id": "1909.12931", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "D\\'ora Gr\\'eta Petr\\'oczy, L\\'aszl\\'o Csat\\'o", "title": "Revenue allocation in Formula One: a pairwise comparison approach", "comments": "19 pages, 3 figures, 6 tables", "journal-ref": "International Journal of General Systems, 50(3): 243-261, 2021", "doi": "10.1080/03081079.2020.1870224", "report-no": null, "categories": "econ.GN cs.AI q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model is proposed to allocate Formula One World Championship prize money\namong the constructors. The methodology is based on pairwise comparison\nmatrices, allows for the use of any weighting method, and makes possible to\ntune the level of inequality. We introduce an axiom called scale invariance,\nwhich requires the ranking of the teams to be independent of the parameter\ncontrolling inequality. The eigenvector method is revealed to violate this\ncondition in our dataset, while the row geometric mean method always satisfies\nit. The revenue allocation is not influenced by the arbitrary valuation given\nto the race prizes in the official points scoring system of Formula One and\ntakes the intensity of pairwise preferences into account, contrary to the\nstandard Condorcet method. Our approach can be used to share revenues among\ngroups when group members are ranked several times.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 17:05:32 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 13:17:18 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 07:11:47 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2020 09:36:57 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Petr\u00f3czy", "D\u00f3ra Gr\u00e9ta", ""], ["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "1909.12969", "submitter": "Matthew Olson", "authors": "Matthew L. Olson, Lawrence Neal, Fuxin Li, Weng-Keen Wong", "title": "Counterfactual States for Atari Agents via Generative Deep Learning", "comments": "IJCAI XAI Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep reinforcement learning agents have produced impressive results\nin many domains, their decision making is difficult to explain to humans. To\naddress this problem, past work has mainly focused on explaining why an action\nwas chosen in a given state. A different type of explanation that is useful is\na counterfactual, which deals with \"what if?\" scenarios. In this work, we\nintroduce the concept of a counterfactual state to help humans gain a better\nunderstanding of what would need to change (minimally) in an Atari game image\nfor the agent to choose a different action. We introduce a novel method to\ncreate counterfactual states from a generative deep learning architecture. In\naddition, we evaluate the effectiveness of counterfactual states on human\nparticipants who are not machine learning experts. Our user study results\nsuggest that our generated counterfactual states are useful in helping\nnon-expert participants gain a better understanding of an agent's decision\nmaking process.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 21:55:01 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Olson", "Matthew L.", ""], ["Neal", "Lawrence", ""], ["Li", "Fuxin", ""], ["Wong", "Weng-Keen", ""]]}, {"id": "1909.12971", "submitter": "Xinlei Pan", "authors": "Xinlei Pan, Tingnan Zhang, Brian Ichter, Aleksandra Faust, Jie Tan,\n  Sehoon Ha", "title": "Zero-shot Imitation Learning from Demonstrations for Legged Robot Visual\n  Navigation", "comments": "Accepted by ICRA 2020. Project website:\n  https://sites.google.com/berkeley.edu/zero-shot-lfd/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning is a popular approach for training visual navigation\npolicies. However, collecting expert demonstrations for legged robots is\nchallenging as these robots can be hard to control, move slowly, and cannot\noperate continuously for a long time. Here, we propose a zero-shot imitation\nlearning approach for training a visual navigation policy on legged robots from\nhuman (third-person perspective) demonstrations, enabling high-quality\nnavigation and cost-effective data collection. However, imitation learning from\nthird-person demonstrations raises unique challenges. First, these\ndemonstrations are captured from different camera perspectives, which we\naddress via a feature disentanglement network (FDN) that extracts\nperspective-invariant state features. Second, as transition dynamics vary\nacross systems, we label missing actions by either building an inverse model of\nthe robot's dynamics in the feature space and applying it to the human\ndemonstrations or developing a Graphic User Interface(GUI) to label human\ndemonstrations. To train a navigation policy we use a model-based imitation\nlearning approach with FDN and labeled human demonstrations. We show that our\nframework can learn an effective policy for a legged robot, Laikago, from human\ndemonstrations in both simulated and real-world environments. Our approach is\nzero-shot as the robot never navigates the same paths during training as those\nat testing time. We justify our framework by performing a comparative study.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 22:13:08 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 06:59:02 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Pan", "Xinlei", ""], ["Zhang", "Tingnan", ""], ["Ichter", "Brian", ""], ["Faust", "Aleksandra", ""], ["Tan", "Jie", ""], ["Ha", "Sehoon", ""]]}, {"id": "1909.13003", "submitter": "Yunbo Wang", "authors": "Yunbo Wang, Bo Liu, Jiajun Wu, Yuke Zhu, Simon S. Du, Li Fei-Fei,\n  Joshua B. Tenenbaum", "title": "DualSMC: Tunneling Differentiable Filtering and Planning under\n  Continuous POMDPs", "comments": "IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major difficulty of solving continuous POMDPs is to infer the multi-modal\ndistribution of the unobserved true states and to make the planning algorithm\ndependent on the perceived uncertainty. We cast POMDP filtering and planning\nproblems as two closely related Sequential Monte Carlo (SMC) processes, one\nover the real states and the other over the future optimal trajectories, and\ncombine the merits of these two parts in a new model named the DualSMC network.\nIn particular, we first introduce an adversarial particle filter that leverages\nthe adversarial relationship between its internal components. Based on the\nfiltering results, we then propose a planning algorithm that extends the\nprevious SMC planning approach [Piche et al., 2018] to continuous POMDPs with\nan uncertainty-dependent policy. Crucially, not only can DualSMC handle complex\nobservations such as image input but also it remains highly interpretable. It\nis shown to be effective in three continuous POMDP domains: the floor\npositioning domain, the 3D light-dark navigation domain, and a modified Reacher\ndomain.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 01:52:27 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 07:35:53 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 04:23:39 GMT"}, {"version": "v4", "created": "Thu, 7 May 2020 06:27:36 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Wang", "Yunbo", ""], ["Liu", "Bo", ""], ["Wu", "Jiajun", ""], ["Zhu", "Yuke", ""], ["Du", "Simon S.", ""], ["Fei-Fei", "Li", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1909.13004", "submitter": "Tianyi Luo", "authors": "Tianyi Luo and Yang Liu", "title": "Machine Truth Serum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wisdom of the crowd revealed a striking fact that the majority answer from a\ncrowd is often more accurate than any individual expert. We observed the same\nstory in machine learning--ensemble methods leverage this idea to combine\nmultiple learning algorithms to obtain better classification performance. Among\nmany popular examples is the celebrated Random Forest, which applies the\nmajority voting rule in aggregating different decision trees to make the final\nprediction. Nonetheless, these aggregation rules would fail when the majority\nis more likely to be wrong. In this paper, we extend the idea proposed in\nBayesian Truth Serum that \"a surprisingly more popular answer is more likely\nthe true answer\" to classification problems. The challenge for us is to define\nor detect when an answer should be considered as being \"surprising\". We present\ntwo machine learning aided methods which aim to reveal the truth when it is\nminority instead of majority who has the true answer. Our experiments over\nreal-world datasets show that better classification performance can be obtained\ncompared to always trusting the majority voting. Our proposed methods also\noutperform popular ensemble algorithms. Our approach can be generically applied\nas a subroutine in ensemble methods to replace majority voting rule.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 01:59:14 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Luo", "Tianyi", ""], ["Liu", "Yang", ""]]}, {"id": "1909.13072", "submitter": "Danfei Xu", "authors": "Danfei Xu, Roberto Mart\\'in-Mart\\'in, De-An Huang, Yuke Zhu, Silvio\n  Savarese, Li Fei-Fei", "title": "Regression Planning Networks", "comments": "Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent learning-to-plan methods have shown promising results on planning\ndirectly from observation space. Yet, their ability to plan for long-horizon\ntasks is limited by the accuracy of the prediction model. On the other hand,\nclassical symbolic planners show remarkable capabilities in solving\nlong-horizon tasks, but they require predefined symbolic rules and symbolic\nstates, restricting their real-world applicability. In this work, we combine\nthe benefits of these two paradigms and propose a learning-to-plan method that\ncan directly generate a long-term symbolic plan conditioned on high-dimensional\nobservations. We borrow the idea of regression (backward) planning from\nclassical planning literature and introduce Regression Planning Networks (RPN),\na neural network architecture that plans backward starting at a task goal and\ngenerates a sequence of intermediate goals that reaches the current\nobservation. We show that our model not only inherits many favorable traits\nfrom symbolic planning, e.g., the ability to solve previously unseen tasks but\nalso can learn from visual inputs in an end-to-end manner. We evaluate the\ncapabilities of RPN in a grid world environment and a simulated 3D kitchen\nenvironment featuring complex visual scenes and long task horizons, and show\nthat it achieves near-optimal performance in completely new task instances.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 11:30:24 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Xu", "Danfei", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Huang", "De-An", ""], ["Zhu", "Yuke", ""], ["Savarese", "Silvio", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1909.13121", "submitter": "Quentin Cappart", "authors": "Antoine Fran\\c{c}ois, Quentin Cappart, Louis-Martin Rousseau", "title": "How to Evaluate Machine Learning Approaches for Combinatorial\n  Optimization: Application to the Travelling Salesman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial optimization is the field devoted to the study and practice of\nalgorithms that solve NP-hard problems. As Machine Learning (ML) and deep\nlearning have popularized, several research groups have started to use ML to\nsolve combinatorial optimization problems, such as the well-known Travelling\nSalesman Problem (TSP). Based on deep (reinforcement) learning, new models and\narchitecture for the TSP have been successively developed and have gained\nincreasing performances. At the time of writing, state-of-the-art models\nprovide solutions to TSP instances of 100 cities that are roughly 1.33% away\nfrom optimal solutions. However, despite these apparently positive results, the\nperformances remain far from those that can be achieved using a specialized\nsearch procedure. In this paper, we address the limitations of ML approaches\nfor solving the TSP and investigate two fundamental questions: (1) how can we\nmeasure the level of accuracy of the pure ML component of such methods; and (2)\nwhat is the impact of a search procedure plugged inside a ML model on the\nperformances? To answer these questions, we propose a new metric, ratio of\noptimal decisions (ROD), based on a fair comparison with a parametrized oracle,\nmimicking a ML model with a controlled accuracy. All the experiments are\ncarried out on four state-of-the-art ML approaches dedicated to solve the TSP.\nFinally, we made ROD open-source in order to ease future research in the field.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 16:35:38 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Fran\u00e7ois", "Antoine", ""], ["Cappart", "Quentin", ""], ["Rousseau", "Louis-Martin", ""]]}, {"id": "1909.13123", "submitter": "Zhengming Ding", "authors": "Zhengming Ding and Ming Shao and Handong Zhao and Sheng Li", "title": "Learning Robust Data Representation: A Knowledge Flow Perspective", "comments": "7 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  It is always demanding to learn robust visual representation for various\nlearning problems; however, this learning and maintenance process usually\nsuffers from noise, incompleteness or knowledge domain mismatch. Thus, robust\nrepresentation learning by removing noisy features or samples, complementing\nincomplete data, and mitigating the distribution difference becomes the key.\nAlong this line of research, low-rank modeling has been widely-applied to\nsolving representation learning challenges. This survey covers the topic from a\nknowledge flow perspective in terms of: (1) robust knowledge recovery, (2)\nrobust knowledge transfer, and (3) robust knowledge fusion, centered around\nseveral major applications. First of all, we deliver a unified formulation for\nrobust knowledge discovery given single dataset. Second, we discuss robust\nknowledge transfer and fusion given multiple datasets with different knowledge\nflows, followed by practical challenges, model variations, and remarks.\nFinally, we highlight future research of robust knowledge discovery for\nincomplete, unbalance, large-scale data analysis. This would benefit AI\ncommunity from literature review to future direction.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 17:15:38 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 18:35:43 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Ding", "Zhengming", ""], ["Shao", "Ming", ""], ["Zhao", "Handong", ""], ["Li", "Sheng", ""]]}, {"id": "1909.13158", "submitter": "Daniel Pirutinsky", "authors": "Wesley Cowan, Michael N. Katehakis, and Daniel Pirutinsky", "title": "Accelerating the Computation of UCB and Related Indices for\n  Reinforcement Learning", "comments": "A version of some of the algorithms and comparisons has appeared in a\n  previous technical note by Cowan, Katehakis, and Pirutinsky (2019)\n  arXiv:1909.06019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive an efficient method for computing the indices\nassociated with an asymptotically optimal upper confidence bound algorithm\n(MDP-UCB) of Burnetas and Katehakis (1997) that only requires solving a system\nof two non-linear equations with two unknowns, irrespective of the cardinality\nof the state space of the Markovian decision process (MDP). In addition, we\ndevelop a similar acceleration for computing the indices for the\nMDP-Deterministic Minimum Empirical Divergence (MDP-DMED) algorithm developed\nin Cowan et al. (2019), based on ideas from Honda and Takemura (2011), that\ninvolves solving a single equation of one variable. We provide experimental\nresults demonstrating the computational time savings and regret performance of\nthese algorithms. In these comparison we also consider the Optimistic Linear\nProgramming (OLP) algorithm (Tewari and Bartlett, 2008) and a method based on\nPosterior sampling (MDP-PS).\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 21:56:11 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Cowan", "Wesley", ""], ["Katehakis", "Michael N.", ""], ["Pirutinsky", "Daniel", ""]]}, {"id": "1909.13165", "submitter": "Changan Chen", "authors": "Changan Chen, Sha Hu, Payam Nikdel, Greg Mori, Manolis Savva", "title": "Relational Graph Learning for Crowd Navigation", "comments": "Accepted to IROS 2020. Added links to codes and video demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a relational graph learning approach for robotic crowd navigation\nusing model-based deep reinforcement learning that plans actions by looking\ninto the future. Our approach reasons about the relations between all agents\nbased on their latent features and uses a Graph Convolutional Network to encode\nhigher-order interactions in each agent's state representation, which is\nsubsequently leveraged for state prediction and value estimation. The ability\nto predict human motion allows us to perform multi-step lookahead planning,\ntaking into account the temporal evolution of human crowds. We evaluate our\napproach against a state-of-the-art baseline for crowd navigation and ablations\nof our model to demonstrate that navigation with our approach is more\nefficient, results in fewer collisions, and avoids failure cases involving\noscillatory and freezing behaviors.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 22:31:46 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 04:22:25 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 19:07:08 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Chen", "Changan", ""], ["Hu", "Sha", ""], ["Nikdel", "Payam", ""], ["Mori", "Greg", ""], ["Savva", "Manolis", ""]]}, {"id": "1909.13221", "submitter": "Chao Wei", "authors": "Chao Wei, Weiru Zhang, Shengjie Sun, Fei Li, Xiaonan Meng, Yi Hu and\n  Hao Wang", "title": "Optimal Delivery with Budget Constraint in E-Commerce Advertising", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online advertising in E-commerce platforms provides sellers an opportunity to\nachieve potential audiences with different target goals. Ad serving systems\n(like display and search advertising systems) that assign ads to pages should\nsatisfy objectives such as plenty of audience for branding advertisers, clicks\nor conversions for performance-based advertisers, at the same time try to\nmaximize overall revenue of the platform. In this paper, we propose an approach\nbased on linear programming subjects to constraints in order to optimize the\nrevenue and improve different performance goals simultaneously. We have\nvalidated our algorithm by implementing an offline simulation system in Alibaba\nE-commerce platform and running the auctions from online requests which takes\nsystem performance, ranking and pricing schemas into account. We have also\ncompared our algorithm with related work, and the results show that our\nalgorithm can effectively improve campaign performance and revenue of the\nplatform.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 07:11:10 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 12:52:24 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Wei", "Chao", ""], ["Zhang", "Weiru", ""], ["Sun", "Shengjie", ""], ["Li", "Fei", ""], ["Meng", "Xiaonan", ""], ["Hu", "Yi", ""], ["Wang", "Hao", ""]]}, {"id": "1909.13404", "submitter": "Renato Negrinho", "authors": "Renato Negrinho, Darshan Patil, Nghia Le, Daniel Ferreira, Matthew\n  Gormley, Geoffrey Gordon", "title": "Towards modular and programmable architecture search", "comments": "Published at NeurIPS 2019. Code and documentation for the language\n  implementation can be found at https://github.com/negrinho/deep_architect", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search methods are able to find high performance deep\nlearning architectures with minimal effort from an expert. However, current\nsystems focus on specific use-cases (e.g. convolutional image classifiers and\nrecurrent language models), making them unsuitable for general use-cases that\nan expert might wish to write. Hyperparameter optimization systems are\ngeneral-purpose but lack the constructs needed for easy application to\narchitecture search. In this work, we propose a formal language for encoding\nsearch spaces over general computational graphs. The language constructs allow\nus to write modular, composable, and reusable search space encodings and to\nreason about search space design. We use our language to encode search spaces\nfrom the architecture search literature. The language allows us to decouple the\nimplementations of the search space and the search algorithm, allowing us to\nexpose search spaces to search algorithms through a consistent interface. Our\nexperiments show the ease with which we can experiment with different\ncombinations of search spaces and search algorithms without having to implement\neach combination from scratch. We release an implementation of our language\nwith this paper.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 00:18:56 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Negrinho", "Renato", ""], ["Patil", "Darshan", ""], ["Le", "Nghia", ""], ["Ferreira", "Daniel", ""], ["Gormley", "Matthew", ""], ["Gordon", "Geoffrey", ""]]}, {"id": "1909.13430", "submitter": "Toby Walsh", "authors": "Ian Gent, Toby Walsh", "title": "CSPLib: Twenty Years On", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1999, we introduced CSPLib, a benchmark library for the constraints\ncommunity. Our CP-1999 poster paper about CSPLib discussed the advantages and\ndisadvantages of building such a library. Unlike some other domains such as\ntheorem proving, or machine learning, representation was then and remains today\na major issue in the success or failure to solve problems. Benchmarks in CSPLib\nare therefore specified in natural language as this allows users to find good\nrepresentations for themselves. The community responded positively and CSPLib\nhas become a valuable resource but, as we discuss here, we cannot rest.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 02:24:09 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Gent", "Ian", ""], ["Walsh", "Toby", ""]]}, {"id": "1909.13434", "submitter": "Lifu Tu", "authors": "Lifu Tu, Xiaoan Ding, Dong Yu, Kevin Gimpel", "title": "Generating Diverse Story Continuations with Controllable Semantics", "comments": "WNGT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a simple and effective modeling framework for controlled\ngeneration of multiple, diverse outputs. We focus on the setting of generating\nthe next sentence of a story given its context. As controllable dimensions, we\nconsider several sentence attributes, including sentiment, length, predicates,\nframes, and automatically-induced clusters. Our empirical results demonstrate:\n(1) our framework is accurate in terms of generating outputs that match the\ntarget control values; (2) our model yields increased maximum metric scores\ncompared to standard n-best list generation via beam search; (3) controlling\ngeneration with semantic frames leads to a stronger combination of diversity\nand quality than other control variables as measured by automatic metrics. We\nalso conduct a human evaluation to assess the utility of providing multiple\nsuggestions for creative writing, demonstrating promising results for the\npotential of controllable, diverse generation in a collaborative writing\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 02:40:48 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 02:22:10 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Tu", "Lifu", ""], ["Ding", "Xiaoan", ""], ["Yu", "Dong", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1909.13485", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern", "title": "The Book of Why: Review", "comments": "To appear in \"Artificial Intelligence\" journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a review of \"The Book of Why\", by Judea Pearl.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 07:11:50 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Halpern", "Joseph Y.", ""]]}, {"id": "1909.13486", "submitter": "Stuart Eiffert", "authors": "Stuart Eiffert and Salah Sukkarieh", "title": "Predicting Responses to a Robot's Future Motion using Generative\n  Recurrent Neural Networks", "comments": "Accepted at Australasian Conference on Robotics and Automation (ACRA)\n  2019", "journal-ref": "Proceedings of the Australasian Conference on Robotics and\n  Automation (ACRA) 2019", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic navigation through crowds or herds requires the ability to both\npredict the future motion of nearby individuals and understand how these\npredictions might change in response to a robot's future action. State of the\nart trajectory prediction models using Recurrent Neural Networks (RNNs) do not\ncurrently account for a planned future action of a robot, and so cannot predict\nhow an individual will move in response to a robot's planned path. We propose\nan approach that adapts RNNs to use a robot's next planned action as an input\nalongside the current position of nearby individuals. This allows the model to\nlearn the response of individuals with regards to a robot's motion from real\nworld observations. By linking a robot's actions to the response of those\naround it in training, we show that we are able to not only improve prediction\naccuracy in close range interactions, but also to predict the likely response\nof surrounding individuals to simulated actions. This allows the use of the\nmodel to simulate state transitions, without requiring any assumptions on agent\ninteraction. We apply this model to varied datasets, including crowds of\npedestrians interacting with vehicles and bicycles, and livestock interacting\nwith a robotic vehicle.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 07:15:29 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 00:36:01 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Eiffert", "Stuart", ""], ["Sukkarieh", "Salah", ""]]}, {"id": "1909.13518", "submitter": "Gabriel Kalweit", "authors": "Gabriel Kalweit, Maria Huegle, Joschka Boedecker", "title": "Composite Q-learning: Multi-scale Q-function Decomposition and Separable\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, off-policy reinforcement learning methods have shown\npromising results in their application for robot control. Deep Q-learning,\nhowever, still suffers from poor data-efficiency and is susceptible to\nstochasticity in the environment or reward functions which is limiting with\nregard to real-world applications. We alleviate these problems by proposing two\nnovel off-policy Temporal-Difference formulations: (1) Truncated Q-functions\nwhich represent the return for the first n steps of a target-policy rollout\nw.r.t. the full action-value and (2) Shifted Q-functions, acting as the\nfarsighted return after this truncated rollout. This decomposition allows us to\noptimize both parts with their individual learning rates, achieving significant\nlearning speedup. We prove that the combination of these short- and long-term\npredictions is a representation of the full return, leading to the Composite\nQ-learning algorithm. We show the efficacy of Composite Q-learning in the\ntabular case and compare Deep Composite Q-learning with TD3 and TD3(Delta),\nwhich we introduce as an off-policy variant of TD(Delta). Moreover, we show\nthat Composite TD3 outperforms TD3 as well as state-of-the-art compositional\nQ-learning approaches significantly in terms of data-efficiency in multiple\nsimulated robot tasks and that Composite Q-learning is robust to stochastic\nenvironments and reward functions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 08:40:09 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 08:32:55 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Kalweit", "Gabriel", ""], ["Huegle", "Maria", ""], ["Boedecker", "Joschka", ""]]}, {"id": "1909.13576", "submitter": "Lukas Brinkmeyer", "authors": "Lukas Brinkmeyer, Rafael Rego Drumond, Randolf Scholz, Josif Grabocka,\n  Lars Schmidt-Thieme", "title": "Chameleon: Learning Model Initializations Across Tasks With Different\n  Schemas", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric models, and particularly neural networks, require weight\ninitialization as a starting point for gradient-based optimization. Recent work\nshows that a specific initial parameter set can be learned from a population of\nsupervised learning tasks. Using this initial parameter set enables a fast\nconvergence for unseen classes even when only a handful of instances is\navailable (model-agnostic meta-learning). Currently, methods for learning model\ninitializations are limited to a population of tasks sharing the same schema,\ni.e., the same number, order, type, and semantics of predictor and target\nvariables. In this paper, we address the problem of meta-learning parameter\ninitialization across tasks with different schemas, i.e., if the number of\npredictors varies across tasks, while they still share some variables. We\npropose Chameleon, a model that learns to align different predictor schemas to\na common representation. In experiments on 23 datasets of the OpenML-CC18\nbenchmark, we show that Chameleon can successfully learn parameter\ninitializations across tasks with different schemas, presenting, to the best of\nour knowledge, the first cross-dataset few-shot classification approach for\nunstructured data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 10:42:44 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 12:36:22 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 17:13:37 GMT"}, {"version": "v4", "created": "Thu, 11 Jun 2020 16:34:37 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Brinkmeyer", "Lukas", ""], ["Drumond", "Rafael Rego", ""], ["Scholz", "Randolf", ""], ["Grabocka", "Josif", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1909.13582", "submitter": "Maria H\\\"ugle", "authors": "Maria Huegle, Gabriel Kalweit, Moritz Werling, Joschka Boedecker", "title": "Dynamic Interaction-Aware Scene Understanding for Reinforcement Learning\n  in Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common pipeline in autonomous driving systems is highly modular and\nincludes a perception component which extracts lists of surrounding objects and\npasses these lists to a high-level decision component. In this case, leveraging\nthe benefits of deep reinforcement learning for high-level decision making\nrequires special architectures to deal with multiple variable-length sequences\nof different object types, such as vehicles, lanes or traffic signs. At the\nsame time, the architecture has to be able to cover interactions between\ntraffic participants in order to find the optimal action to be taken. In this\nwork, we propose the novel Deep Scenes architecture, that can learn complex\ninteraction-aware scene representations based on extensions of either 1) Deep\nSets or 2) Graph Convolutional Networks. We present the Graph-Q and DeepScene-Q\noff-policy reinforcement learning algorithms, both outperforming\nstate-of-the-art methods in evaluations with the publicly available traffic\nsimulator SUMO.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 10:59:11 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Huegle", "Maria", ""], ["Kalweit", "Gabriel", ""], ["Werling", "Moritz", ""], ["Boedecker", "Joschka", ""]]}, {"id": "1909.13599", "submitter": "Efe Camci", "authors": "Efe Camci, Erdal Kayacan", "title": "End-to-End Motion Planning of Quadrotors Using Deep Reinforcement\n  Learning", "comments": "IROS 2019 Workshop, Learning Representations for Planning and Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a novel, end-to-end motion planning method is proposed for\nquadrotor navigation in cluttered environments. The proposed method circumvents\nthe explicit sensing-reconstructing-planning in contrast to conventional\nnavigation algorithms. It uses raw depth images obtained from a front-facing\ncamera and directly generates local motion plans in the form of smooth motion\nprimitives that move a quadrotor to a goal by avoiding obstacles. Promising\ntraining and testing results are presented in both AirSim simulations and real\nflights with DJI F330 Quadrotor equipped with Intel RealSense D435. The\nproposed system in action can be found in https://youtu.be/pYvKhc8wrTM.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 11:31:59 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 04:22:10 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Camci", "Efe", ""], ["Kayacan", "Erdal", ""]]}, {"id": "1909.13607", "submitter": "Haotian Fu", "authors": "Haotian Fu, Hongyao Tang, Jianye Hao, Wulong Liu, Chen Chen", "title": "MGHRL: Meta Goal-generation for Hierarchical Reinforcement Learning", "comments": "Accepted to the ICLR 2020 workshop: Beyond tabula rasa in RL\n  (BeTR-RL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most meta reinforcement learning (meta-RL) methods learn to adapt to new\ntasks by directly optimizing the parameters of policies over primitive action\nspace. Such algorithms work well in tasks with relatively slight difference.\nHowever, when the task distribution becomes wider, it would be quite\ninefficient to directly learn such a meta-policy. In this paper, we propose a\nnew meta-RL algorithm called Meta Goal-generation for Hierarchical RL (MGHRL).\nInstead of directly generating policies over primitive action space for new\ntasks, MGHRL learns to generate high-level meta strategies over subgoals given\npast experience and leaves the rest of how to achieve subgoals as independent\nRL subtasks. Our empirical results on several challenging simulated robotics\nenvironments show that our method enables more efficient and generalized\nmeta-learning from past experience.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 11:55:17 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 03:39:47 GMT"}, {"version": "v3", "created": "Sun, 10 Nov 2019 05:26:55 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2020 08:36:01 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Fu", "Haotian", ""], ["Tang", "Hongyao", ""], ["Hao", "Jianye", ""], ["Liu", "Wulong", ""], ["Chen", "Chen", ""]]}, {"id": "1909.13778", "submitter": "Blai Bonet", "authors": "Blai Bonet and Hector Geffner", "title": "Causal Belief Decomposition for Planning with Sensing: Completeness\n  Results and Practical Approximation", "comments": "Proceedings IJCAI-13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief tracking is a basic problem in planning with sensing. While the\nproblem is intractable, it has been recently shown that for both deterministic\nand non-deterministic systems expressed in compact form, it can be done in time\nand space that are exponential in the problem width. The width measures the\nmaximum number of state variables that are all relevant to a given precondition\nor goal. In this work, we extend this result both theoretically and\npractically. First, we introduce an alternative decomposition scheme and\nalgorithm with the same time complexity but different completeness guarantees,\nwhose space complexity is much smaller: exponential in the causal width of the\nproblem that measures the number of state variables that are causally relevant\nto a given precondition, goal, or observable. Second, we introduce a fast,\nmeaningful, and powerful approximation that trades completeness by speed, and\nis both time and space exponential in the problem causal width. It is then\nshown empirically that the algorithm combined with simple heuristics yields\nstate-of-the-art real-time performance in domains with high widths but low\ncausal widths such as Minesweeper, Battleship, and Wumpus.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 13:53:21 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Bonet", "Blai", ""], ["Geffner", "Hector", ""]]}, {"id": "1909.13779", "submitter": "Blai Bonet", "authors": "Blai Bonet and Hector Geffner", "title": "Factored Probabilistic Belief Tracking", "comments": "Proceedings IJCAI-13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of belief tracking in the presence of stochastic actions and\nobservations is pervasive and yet computationally intractable. In this work we\nshow however that probabilistic beliefs can be maintained in factored form\nexactly and efficiently across a number of causally closed beams, when the\nstate variables that appear in more than one beam obey a form of backward\ndeterminism. Since computing marginals from the factors is still\ncomputationally intractable in general, and variables appearing in several\nbeams are not always backward-deterministic, the basic formulation is extended\nwith two approximations: forms of belief propagation for computing marginals\nfrom factors, and sampling of non-backward-deterministic variables for making\nsuch variables backward-deterministic given their sampled history. Unlike,\nRao-Blackwellized particle-filtering, the sampling is not used for making\ninference tractable but for making the factorization sound. The resulting\nalgorithm involves sampling and belief propagation or just one of them as\ndetermined by the structure of the model.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 12:48:25 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Bonet", "Blai", ""], ["Geffner", "Hector", ""]]}, {"id": "1909.13870", "submitter": "Rohan Chitnis", "authors": "Rohan Chitnis, Tom\\'as Lozano-P\\'erez", "title": "Learning Compact Models for Planning with Exogenous Processes", "comments": "CoRL 2019 final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of approximate model minimization for MDPs in which\nthe state is partitioned into endogenous and (much larger) exogenous\ncomponents. An exogenous state variable is one whose dynamics are independent\nof the agent's actions. We formalize the mask-learning problem, in which the\nagent must choose a subset of exogenous state variables to reason about when\nplanning; doing planning in such a reduced state space can often be\nsignificantly more efficient than planning in the full model. We then explore\nthe various value functions at play within this setting, and describe\nconditions under which a policy for a reduced model will be optimal for the\nfull MDP. The analysis leads us to a tractable approximate algorithm that draws\nupon the notion of mutual information among exogenous state variables. We\nvalidate our approach in simulated robotic manipulation domains where a robot\nis placed in a busy environment, in which there are many other agents also\ninteracting with the objects. Visit http://tinyurl.com/chitnis-exogenous for a\nsupplementary video.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 17:51:12 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Chitnis", "Rohan", ""], ["Lozano-P\u00e9rez", "Tom\u00e1s", ""]]}]