[{"id": "1508.00019", "submitter": "Michael S. Gashler Ph.D.", "authors": "Michael S. Gashler and Zachariah Kindle and Michael R. Smith", "title": "A Minimal Architecture for General Cognition", "comments": "8 pages, 8 figures, conference, Proceedings of the 2015 International\n  Joint Conference on Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A minimalistic cognitive architecture called MANIC is presented. The MANIC\narchitecture requires only three function approximating models, and one state\nmachine. Even with so few major components, it is theoretically sufficient to\nachieve functional equivalence with all other cognitive architectures, and can\nbe practically trained. Instead of seeking to transfer architectural\ninspiration from biology into artificial intelligence, MANIC seeks to minimize\nnovelty and follow the most well-established constructs that have evolved\nwithin various sub-fields of data science. From this perspective, MANIC offers\nan alternate approach to a long-standing objective of artificial intelligence.\nThis paper provides a theoretical analysis of the MANIC architecture.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 20:21:38 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Gashler", "Michael S.", ""], ["Kindle", "Zachariah", ""], ["Smith", "Michael R.", ""]]}, {"id": "1508.00037", "submitter": "Luiz Capretz Dr.", "authors": "Danny Ho, Luiz Fernando Capretz, Xishi Huang, Jing Ren", "title": "Neuro-Fuzzy Algorithmic (NFA) Models and Tools for Estimation", "comments": "20th International Forum on COCOMO and Software Cost Modeling, Los\n  Angeles, USA, 5 pages, 2005", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation such as cost estimation, quality estimation and risk\nanalysis is a major issue in management. We propose a patent pending soft\ncomputing framework to tackle this challenging problem. Our generic framework\nis independent of the nature and type of estimation. It consists of neural\nnetwork, fuzzy logic, and an algorithmic estimation model. We made use of the\nConstructive Cost Model (COCOMO), Analysis of Variance (ANOVA), and Function\nPoint Analysis as the algorithmic models and validated the accuracy of the\nNeuro-Fuzzy Algorithmic (NFA) Model in software cost estimation using\nindustrial project data. Our model produces more accurate estimation than using\nan algorithmic model alone. We also discuss the prototypes of our tools that\nimplement the NFA Model. We conclude with our roadmap and direction to enrich\nthe model in tackling different estimation challenges.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 21:30:42 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Ho", "Danny", ""], ["Capretz", "Luiz Fernando", ""], ["Huang", "Xishi", ""], ["Ren", "Jing", ""]]}, {"id": "1508.00059", "submitter": "Mohan Sridharan", "authors": "Zenon Colaco and Mohan Sridharan", "title": "Mixed Logical and Probabilistic Reasoning for Planning and Explanation\n  Generation in Robotics", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots assisting humans in complex domains have to represent knowledge and\nreason at both the sensorimotor level and the social level. The architecture\ndescribed in this paper couples the non-monotonic logical reasoning\ncapabilities of a declarative language with probabilistic belief revision,\nenabling robots to represent and reason with qualitative and quantitative\ndescriptions of knowledge and degrees of belief. Specifically, incomplete\ndomain knowledge, including information that holds in all but a few exceptional\nsituations, is represented as a Answer Set Prolog (ASP) program. The answer set\nobtained by solving this program is used for inference, planning, and for\njointly explaining (a) unexpected action outcomes due to exogenous actions and\n(b) partial scene descriptions extracted from sensor input. For any given task,\neach action in the plan contained in the answer set is executed\nprobabilistically. The subset of the domain relevant to the action is\nidentified automatically, and observations extracted from sensor inputs perform\nincremental Bayesian updates to a belief distribution defined over this domain\nsubset, with highly probable beliefs being committed to the ASP program. The\narchitecture's capabilities are illustrated in simulation and on a mobile robot\nin the context of a robot waiter operating in the dining room of a restaurant.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 00:26:46 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Colaco", "Zenon", ""], ["Sridharan", "Mohan", ""]]}, {"id": "1508.00116", "submitter": "Arjun Bhardwaj", "authors": "Arjun Bhardwaj", "title": "Extending SROIQ with Constraint Networks and Grounded Circumscription", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developments in semantic web technologies have promoted ontological encoding\nof knowledge from diverse domains. However, modelling many practical domains\nrequires more expressiveness than what the standard description logics (most\nprominently SROIQ) support. In this paper, we extend the expressive DL SROIQ\nwith constraint networks (resulting in the logic SROIQc) and grounded\ncircumscription (resulting in the logic GC-SROIQ). Applications of constraint\nmodelling include embedding ontologies with temporal or spatial information,\nwhile those of grounded circumscription include defeasible inference and closed\nworld reasoning.\n  We describe the syntax and semantics of the logic formed by including\nconstraint modelling constructs in SROIQ, and provide a sound, complete and\nterminating tableau algorithm for it. We further provide an intuitive algorithm\nfor Grounded Circumscription in SROIQc, which adheres to the general framework\nof grounded circumscription, and which can be applied to a whole range of\nexpressive logics for which no such specific algorithm presently exists.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 13:07:46 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Bhardwaj", "Arjun", ""]]}, {"id": "1508.00212", "submitter": "Jakub Kowalski", "authors": "Jakub Kowalski, Marek Szyku{\\l}a", "title": "Procedural Content Generation for GDL Descriptions of Simplified\n  Boardgames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present initial research towards procedural generation of Simplified\nBoardgames and translating them into an efficient GDL code. This is a step\ntowards establishing Simplified Boardgames as a comparison class for General\nGame Playing agents. To generate playable, human readable, and balanced\nchess-like games we use an adaptive evolutionary algorithm with the fitness\nfunction based on simulated playouts. In future, we plan to use the proposed\nmethod to diversify and extend the set of GGP tournament games by those with\nfully automatically generated rules.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 10:11:38 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Kowalski", "Jakub", ""], ["Szyku\u0142a", "Marek", ""]]}, {"id": "1508.00280", "submitter": "Johannes Textor", "authors": "Johannes Textor, Alexander Idelberger, Maciej Li\\'skiewicz", "title": "Learning from Pairwise Marginal Independencies", "comments": "10 pages, 6 figures, 2 tables. Published at the 31st Conference on\n  Uncertainty in Artificial Intelligence (UAI 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider graphs that represent pairwise marginal independencies amongst a\nset of variables (for instance, the zero entries of a covariance matrix for\nnormal data). We characterize the directed acyclic graphs (DAGs) that\nfaithfully explain a given set of independencies, and derive algorithms to\nefficiently enumerate such structures. Our results map out the space of\nfaithful causal models for a given set of pairwise marginal independence\nrelations. This allows us to show the extent to which causal inference is\npossible without using conditional independence tests.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 20:13:41 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Textor", "Johannes", ""], ["Idelberger", "Alexander", ""], ["Li\u015bkiewicz", "Maciej", ""]]}, {"id": "1508.00377", "submitter": "Martin Cerny", "authors": "Martin \\v{C}ern\\'y, Tom\\'a\\v{s} Plch, Mat\\v{e}j Marko, Jakub Gemrot,\n  Petr Ondr\\'a\\v{c}ek, Cyril Brom", "title": "Using Behavior Objects to Manage Complexity in Virtual Worlds", "comments": "Currently under review in IEEE Transactions on Computational\n  Intelligence and AI in Games", "journal-ref": null, "doi": "10.1109/TCIAIG.2016.2528499", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of high-level AI of non-player characters (NPCs) in commercial\nopen-world games (OWGs) has been increasing during the past years. However, due\nto constraints specific to the game industry, this increase has been slow and\nit has been driven by larger budgets rather than adoption of new complex AI\ntechniques. Most of the contemporary AI is still expressed as hard-coded\nscripts. The complexity and manageability of the script codebase is one of the\nkey limiting factors for further AI improvements. In this paper we address this\nissue. We present behavior objects - a general approach to development of NPC\nbehaviors for large OWGs. Behavior objects are inspired by object-oriented\nprogramming and extend the concept of smart objects. Our approach promotes\nencapsulation of data and code for multiple related behaviors in one place,\nhiding internal details and embedding intelligence in the environment. Behavior\nobjects are a natural abstraction of five different techniques that we have\nimplemented to manage AI complexity in an upcoming AAA OWG. We report the\ndetails of the implementations in the context of behavior trees and the lessons\nlearned during development. Our work should serve as inspiration for AI\narchitecture designers from both the academia and the industry.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 11:29:21 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 19:05:13 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["\u010cern\u00fd", "Martin", ""], ["Plch", "Tom\u00e1\u0161", ""], ["Marko", "Mat\u011bj", ""], ["Gemrot", "Jakub", ""], ["Ondr\u00e1\u010dek", "Petr", ""], ["Brom", "Cyril", ""]]}, {"id": "1508.00457", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong", "title": "Evolutionary Multimodal Optimization: A Short Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world problems always have different multiple solutions. For instance,\noptical engineers need to tune the recording parameters to get as many optimal\nsolutions as possible for multiple trials in the varied-line-spacing\nholographic grating design problem. Unfortunately, most traditional\noptimization techniques focus on solving for a single optimal solution. They\nneed to be applied several times; yet all solutions are not guaranteed to be\nfound. Thus the multimodal optimization problem was proposed. In that problem,\nwe are interested in not only a single optimal point, but also the others. With\nstrong parallel search capability, evolutionary algorithms are shown to be\nparticularly effective in solving this type of problem. In particular, the\nevolutionary algorithms for multimodal optimization usually not only locate\nmultiple optima in a single run, but also preserve their population diversity\nthroughout a run, resulting in their global optimization ability on multimodal\nfunctions. In addition, the techniques for multimodal optimization are borrowed\nas diversity maintenance techniques to other problems. In this chapter, we\ndescribe and review the state-of-the-arts evolutionary algorithms for\nmultimodal optimization in terms of methodology, benchmarking, and application.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 15:45:38 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Wong", "Ka-Chun", ""]]}, {"id": "1508.00507", "submitter": "Tameem Adel", "authors": "Tameem Adel, Alexander Wong, Daniel Stashuk", "title": "A Weakly Supervised Learning Approach based on Spectral Graph-Theoretic\n  Grouping", "comments": "Submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a spectral graph-theoretic grouping strategy for weakly\nsupervised classification is introduced, where a limited number of labelled\nsamples and a larger set of unlabelled samples are used to construct a larger\nannotated training set composed of strongly labelled and weakly labelled\nsamples. The inherent relationship between the set of strongly labelled samples\nand the set of unlabelled samples is established via spectral grouping, with\nthe unlabelled samples subsequently weakly annotated based on the strongly\nlabelled samples within the associated spectral groups. A number of similarity\ngraph models for spectral grouping, including two new similarity graph models\nintroduced in this study, are explored to investigate their performance in the\ncontext of weakly supervised classification in handling different types of\ndata. Experimental results using benchmark datasets as well as real EMG\ndatasets demonstrate that the proposed approach to weakly supervised\nclassification can provide noticeable improvements in classification\nperformance, and that the proposed similarity graph models can lead to ultimate\nlearning results that are either better than or on a par with existing\nsimilarity graph models in the context of spectral grouping for weakly\nsupervised classification.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 18:08:04 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Adel", "Tameem", ""], ["Wong", "Alexander", ""], ["Stashuk", "Daniel", ""]]}, {"id": "1508.00509", "submitter": "Christoph Jahnz", "authors": "Christoph Jahnz", "title": "Maintaining prediction quality under the condition of a growing\n  knowledge space", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligence can be understood as an agent's ability to predict its\nenvironment's dynamic by a level of precision which allows it to effectively\nforesee opportunities and threats. Under the assumption that such intelligence\nrelies on a knowledge space any effective reasoning would benefit from a\nmaximum portion of useful and a minimum portion of misleading knowledge\nfragments. It begs the question of how the quality of such knowledge space can\nbe kept high as the amount of knowledge keeps growing. This article proposes a\nmathematical model to describe general principles of how quality of a growing\nknowledge space evolves depending on error rate, error propagation and\ncountermeasures. There is also shown to which extend the quality of a knowledge\nspace collapses as removal of low quality knowledge fragments occurs too slowly\nfor a given knowledge space's growth rate.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 18:19:41 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 18:50:57 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Jahnz", "Christoph", ""]]}, {"id": "1508.00655", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry\n  Wasserman", "title": "Adaptivity and Computation-Statistics Tradeoffs for Kernel and Distance\n  based High Dimensional Two Sample Testing", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric two sample testing is a decision theoretic problem that\ninvolves identifying differences between two random variables without making\nparametric assumptions about their underlying distributions. We refer to the\nmost common settings as mean difference alternatives (MDA), for testing\ndifferences only in first moments, and general difference alternatives (GDA),\nwhich is about testing for any difference in distributions. A large number of\ntest statistics have been proposed for both these settings. This paper connects\nthree classes of statistics - high dimensional variants of Hotelling's t-test,\nstatistics based on Reproducing Kernel Hilbert Spaces, and energy statistics\nbased on pairwise distances. We ask the question: how much statistical power do\npopular kernel and distance based tests for GDA have when the unknown\ndistributions differ in their means, compared to specialized tests for MDA?\n  We formally characterize the power of popular tests for GDA like the Maximum\nMean Discrepancy with the Gaussian kernel (gMMD) and bandwidth-dependent\nvariants of the Energy Distance with the Euclidean norm (eED) in the\nhigh-dimensional MDA regime. Some practically important properties include (a)\neED and gMMD have asymptotically equal power; furthermore they enjoy a free\nlunch because, while they are additionally consistent for GDA, they also have\nthe same power as specialized high-dimensional t-test variants for MDA. All\nthese tests are asymptotically optimal (including matching constants) under MDA\nfor spherical covariances, according to simple lower bounds, (b) The power of\ngMMD is independent of the kernel bandwidth, as long as it is larger than the\nchoice made by the median heuristic, (c) There is a clear and smooth\ncomputation-statistics tradeoff for linear-time, subquadratic-time and\nquadratic-time versions of these tests, with more computation resulting in\nhigher power.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 04:10:05 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Reddi", "Sashank J.", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1508.00689", "submitter": "Pascal Vontobel", "authors": "Hans-Andrea Loeliger and Pascal O. Vontobel", "title": "Factor Graphs for Quantum Probabilities", "comments": "To appear in IEEE Transactions on Information Theory, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT math.ST quant-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A factor-graph representation of quantum-mechanical probabilities (involving\nany number of measurements) is proposed. Unlike standard statistical models,\nthe proposed representation uses auxiliary variables (state variables) that are\nnot random variables. All joint probability distributions are marginals of some\ncomplex-valued function $q$, and it is demonstrated how the basic concepts of\nquantum mechanics relate to factorizations and marginals of $q$.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 07:38:51 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 13:16:37 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 04:29:07 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Loeliger", "Hans-Andrea", ""], ["Vontobel", "Pascal O.", ""]]}, {"id": "1508.00749", "submitter": "Indre Zliobaite", "authors": "Tomas Krilavicius, Indre Zliobaite, Henrikas Simonavicius and Laimonas\n  Jarusevicius", "title": "Predicting respiratory motion for real-time tumour tracking in\n  radiotherapy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose. Radiation therapy is a local treatment aimed at cells in and around\na tumor. The goal of this study is to develop an algorithmic solution for\npredicting the position of a target in 3D in real time, aiming for the short\nfixed calibration time for each patient at the beginning of the procedure.\nAccurate predictions of lung tumor motion are expected to improve the precision\nof radiation treatment by controlling the position of a couch or a beam in\norder to compensate for respiratory motion during radiation treatment.\n  Methods. For developing the algorithmic solution, data mining techniques are\nused. A model form from the family of exponential smoothing is assumed, and the\nmodel parameters are fitted by minimizing the absolute disposition error, and\nthe fluctuations of the prediction signal (jitter). The predictive performance\nis evaluated retrospectively on clinical datasets capturing different behavior\n(being quiet, talking, laughing), and validated in real-time on a prototype\nsystem with respiratory motion imitation.\n  Results. An algorithmic solution for respiratory motion prediction (called\nExSmi) is designed. ExSmi achieves good accuracy of prediction (error $4-9$\nmm/s) with acceptable jitter values (5-7 mm/s), as tested on out-of-sample\ndata. The datasets, the code for algorithms and the experiments are openly\navailable for research purposes on a dedicated website.\n  Conclusions. The developed algorithmic solution performs well to be\nprototyped and deployed in applications of radiotherapy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 12:26:00 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Krilavicius", "Tomas", ""], ["Zliobaite", "Indre", ""], ["Simonavicius", "Henrikas", ""], ["Jarusevicius", "Laimonas", ""]]}, {"id": "1508.00801", "submitter": "Mehdi Kaytoue", "authors": "Olivier Cavadenti and Victor Codocedo and Jean-Fran\\c{c}ois Boulicaut\n  and Mehdi Kaytoue", "title": "Identifying Avatar Aliases in Starcraft 2", "comments": "Machine Learning and Data Mining for Sports Analytics ECML/PKDD 2015\n  workshop, 11 September 2015, Porto, Portugal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In electronic sports, cyberathletes conceal their online training using\ndifferent avatars (virtual identities), allowing them not being recognized by\nthe opponents they may face in future competitions. In this article, we propose\na method to tackle this avatar aliases identification problem. Our method\ntrains a classifier on behavioural data and processes the confusion matrix to\noutput label pairs which concentrate confusion. We experimented with Starcraft\n2 and report our first results.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 15:37:44 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Cavadenti", "Olivier", ""], ["Codocedo", "Victor", ""], ["Boulicaut", "Jean-Fran\u00e7ois", ""], ["Kaytoue", "Mehdi", ""]]}, {"id": "1508.00879", "submitter": "Ankit Agrawal", "authors": "Ankit Agrawal", "title": "Qualitative Decision Methods for Multi-Attribute Decision Making", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental problem underlying all multi-criteria decision analysis\n(MCDA) problems is that of dominance between any two alternatives: \"Given two\nalternatives A and B, each described by a set criteria, is A preferred to B\nwith respect to a set of decision maker (DM) preferences over the criteria?\".\nDepending on the application in which MCDA is performed, the alternatives may\nrepresent strategies and policies for business, potential locations for setting\nup new facilities, designs of buildings, etc. The general objective of MCDA is\nto enable the DM to order all alternatives in order of the stated preferences,\nand choose the ones that are best, i.e., optimal with respect to the\npreferences over the criteria. This article presents and summarizes a recently\ndeveloped MCDA framework that orders the set of alternatives when the relative\nimportance preferences are incomplete, imprecise, or qualitative in nature.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 19:27:21 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Agrawal", "Ankit", ""]]}, {"id": "1508.00986", "submitter": "Zhuoran Wang", "authors": "Zhuoran Wang, Paul A. Crook, Wenshuo Tang, Oliver Lemon", "title": "On the Linear Belief Compression of POMDPs: A re-examination of current\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief compression improves the tractability of large-scale partially\nobservable Markov decision processes (POMDPs) by finding projections from\nhigh-dimensional belief space onto low-dimensional approximations, where\nsolving to obtain action selection policies requires fewer computations. This\npaper develops a unified theoretical framework to analyse three existing linear\nbelief compression approaches, including value-directed compression and two\nnon-negative matrix factorisation (NMF) based algorithms. The results indicate\nthat all the three known belief compression methods have their own critical\ndeficiencies. Therefore, projective NMF belief compression is proposed (P-NMF),\naiming to overcome the drawbacks of the existing techniques. The performance of\nthe proposed algorithm is examined on four POMDP problems of reasonably large\nscale, in comparison with existing techniques. Additionally, the\ncompetitiveness of belief compression is compared empirically to a\nstate-of-the-art heuristic search based POMDP solver and their relative merits\nin solving large-scale POMDPs are investigated.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 06:45:09 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Wang", "Zhuoran", ""], ["Crook", "Paul A.", ""], ["Tang", "Wenshuo", ""], ["Lemon", "Oliver", ""]]}, {"id": "1508.01083", "submitter": "Paolo Nesi", "authors": "Pierfrancesco Bellini, Paolo Nesi, Nadia Rauch", "title": "Ontology Bulding vs Data Harvesting and Cleaning for Smart-city Services", "comments": "DMS 2014, Distributed Multimedia Systems", "journal-ref": "Journal of Visual Languages & Computing, Vo. 25, n.6, december\n  2014", "doi": "10.1016/j.jvlc.2014.10.023", "report-no": null, "categories": "cs.DB cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Presently, a very large number of public and private data sets are available\naround the local governments. In most cases, they are not semantically\ninteroperable and a huge human effort is needed to create integrated ontologies\nand knowledge base for smart city. Smart City ontology is not yet standardized,\nand a lot of research work is needed to identify models that can easily support\nthe data reconciliation, the management of the complexity and reasoning. In\nthis paper, a system for data ingestion and reconciliation smart cities related\naspects as road graph, services available on the roads, traffic sensors etc.,\nis proposed. The system allows managing a big volume of data coming from a\nvariety of sources considering both static and dynamic data. These data are\nmapped to smart-city ontology and stored into an RDF-Store where they are\navailable for applications via SPARQL queries to provide new services to the\nusers. The paper presents the process adopted to produce the ontology and the\nknowledge base and the mechanisms adopted for the verification, reconciliation\nand validation. Some examples about the possible usage of the coherent\nknowledge base produced are also offered and are accessible from the RDF-Store.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 14:17:10 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Bellini", "Pierfrancesco", ""], ["Nesi", "Paolo", ""], ["Rauch", "Nadia", ""]]}, {"id": "1508.01086", "submitter": "Paolo Nesi", "authors": "Pierfrancesco Bellini, Monica Benigni, Riccardo Billero, Paolo Nesi,\n  Nadia Rauch", "title": "Km4City Ontology Building vs Data Harvesting and Cleaning for Smart-city\n  Services", "comments": null, "journal-ref": null, "doi": "10.1016/j.jvlc.2014.10.023", "report-no": null, "categories": "cs.DB cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Presently, a very large number of public and private data sets are available\nfrom local governments. In most cases, they are not semantically interoperable\nand a huge human effort would be needed to create integrated ontologies and\nknowledge base for smart city. Smart City ontology is not yet standardized, and\na lot of research work is needed to identify models that can easily support the\ndata reconciliation, the management of the complexity, to allow the data\nreasoning. In this paper, a system for data ingestion and reconciliation of\nsmart cities related aspects as road graph, services available on the roads,\ntraffic sensors etc., is proposed. The system allows managing a big data volume\nof data coming from a variety of sources considering both static and dynamic\ndata. These data are mapped to a smart-city ontology, called KM4City (Knowledge\nModel for City), and stored into an RDF-Store where they are available for\napplications via SPARQL queries to provide new services to the users via\nspecific applications of public administration and enterprises. The paper\npresents the process adopted to produce the ontology and the big data\narchitecture for the knowledge base feeding on the basis of open and private\ndata, and the mechanisms adopted for the data verification, reconciliation and\nvalidation. Some examples about the possible usage of the coherent big data\nknowledge base produced are also offered and are accessible from the RDF-Store\nand related services. The article also presented the work performed about\nreconciliation algorithms and their comparative assessment and selection.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 14:24:23 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Bellini", "Pierfrancesco", ""], ["Benigni", "Monica", ""], ["Billero", "Riccardo", ""], ["Nesi", "Paolo", ""], ["Rauch", "Nadia", ""]]}, {"id": "1508.01191", "submitter": "Waldemar Koczkodaj Prof.", "authors": "J. Fueloep, W.W. Koczkodaj, S.J. Szarek", "title": "A different perspective on a scale for pairwise comparisons", "comments": "16 pages, 1 figure; the mathematical theory has been provided for the\n  use of small scale (1 to 3) for pairwise comparisons (but not only)", "journal-ref": "Logic Journal of the IGPL Volume: 18 Issue: 6 Pages: 859-869\n  Published: DEC 2010", "doi": "10.1093/jigpal/jzp062", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges for collective intelligence is inconsistency,\nwhich is unavoidable whenever subjective assessments are involved. Pairwise\ncomparisons allow one to represent such subjective assessments and to process\nthem by analyzing, quantifying and identifying the inconsistencies.\n  We propose using smaller scales for pairwise comparisons and provide\nmathematical and practical justifications for this change. Our postulate's aim\nis to initiate a paradigm shift in the search for a better scale construction\nfor pairwise comparisons. Beyond pairwise comparisons, the results presented\nmay be relevant to other methods using subjective scales.\n  Keywords: pairwise comparisons, collective intelligence, scale, subjective\nassessment, inaccuracy, inconsistency.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 19:50:21 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Fueloep", "J.", ""], ["Koczkodaj", "W. W.", ""], ["Szarek", "S. J.", ""]]}, {"id": "1508.01192", "submitter": "Paulo Shakarian", "authors": "Andrew Stanton, Amanda Thart, Ashish Jain, Priyank Vyas, Arpan\n  Chatterjee, Paulo Shakarian", "title": "Mining for Causal Relationships: A Data-Driven Study of the Islamic\n  State", "comments": null, "journal-ref": "Final version presented at KDD 2015", "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Islamic State of Iraq and al-Sham (ISIS) is a dominant insurgent group\noperating in Iraq and Syria that rose to prominence when it took over Mosul in\nJune, 2014. In this paper, we present a data-driven approach to analyzing this\ngroup using a dataset consisting of 2200 incidents of military activity\nsurrounding ISIS and the forces that oppose it (including Iraqi, Syrian, and\nthe American-led coalition). We combine ideas from logic programming and causal\nreasoning to mine for association rules for which we present evidence of\ncausality. We present relationships that link ISIS vehicle-bourne improvised\nexplosive device (VBIED) activity in Syria with military operations in Iraq,\ncoalition air strikes, and ISIS IED activity, as well as rules that may serve\nas indicators of spikes in indirect fire, suicide attacks, and arrests.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 19:50:54 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Stanton", "Andrew", ""], ["Thart", "Amanda", ""], ["Jain", "Ashish", ""], ["Vyas", "Priyank", ""], ["Chatterjee", "Arpan", ""], ["Shakarian", "Paulo", ""]]}, {"id": "1508.01306", "submitter": "Michael Minock", "authors": "Michael Minock and Nils Everling", "title": "Replication and Generalization of PRECISE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes an initial replication study of the PRECISE system and\ndevelops a clearer, more formal description of the approach. Based on our\nevaluation, we conclude that the PRECISE results do not fully replicate.\nHowever the formalization developed here suggests a road map to further enhance\nand extend the approach pioneered by PRECISE.\n  After a long, productive discussion with Ana-Maria Popescu (one of the\nauthors of PRECISE) we got more clarity on the PRECISE approach and how the\nlexicon was authored for the GEO evaluation. Based on this we built a more\ndirect implementation over a repaired formalism. Although our new evaluation is\nnot yet complete, it is clear that the system is performing much better now. We\nwill continue developing our ideas and implementation and generate a future\nreport/publication that more accurately evaluates PRECISE like approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 07:56:59 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 09:41:34 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Minock", "Michael", ""], ["Everling", "Nils", ""]]}, {"id": "1508.01345", "submitter": "Fatih Korkmaz", "authors": "Fatih Korkmaz, \\.Ismail Topalo\\u{g}lu, Hayati Mamur", "title": "Fuzzy Logic Based Direct Torque Control Of Induction Motor With Space\n  Vector Modulation", "comments": "10 pages", "journal-ref": null, "doi": "10.5121/ijscai.2013.2603", "report-no": null, "categories": "cs.SY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The induction motors have wide range of applications for due to its\nwell-known advantages like brushless structures, low costs and robust\nperformances. Over the past years, many kind of control methods are proposed\nfor the induction motors and direct torque control has gained huge importance\ninside of them due to fast dynamic torque responses and simple control\nstructures. However, the direct torque control method has still some handicaps\nagainst the other control methods and most of the important of these handicaps\nis high torque ripple. This paper suggests a new approach, Fuzzy logic based\nspace vector modulation, on the direct torque controlled induction motors and\naim of the approach is to overcome high torque ripple disadvantages of\nconventional direct torque control. In order to test and compare the proposed\ndirect torque control method with conventional direct torque control method\nsimulations, in Matlab/Simulink,have been carried out in different working\nconditions. The simulation results showed that a significant improvement in the\ndynamic torque and speed responses when compared to the conventional direct\ntorque control method.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 10:12:02 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Korkmaz", "Fatih", ""], ["Topalo\u011flu", "\u0130smail", ""], ["Mamur", "Hayati", ""]]}, {"id": "1508.01447", "submitter": "Iyad AlAgha", "authors": "Iyad AlAgha", "title": "Using Linguistic Analysis to Translate Arabic Natural Language Queries\n  to SPARQL", "comments": "Journal Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logic-based machine-understandable framework of the Semantic Web often\nchallenges naive users when they try to query ontology-based knowledge bases.\nExisting research efforts have approached this problem by introducing Natural\nLanguage (NL) interfaces to ontologies. These NL interfaces have the ability to\nconstruct SPARQL queries based on NL user queries. However, most efforts were\nrestricted to queries expressed in English, and they often benefited from the\nadvancement of English NLP tools. However, little research has been done to\nsupport querying the Arabic content on the Semantic Web by using NL queries.\nThis paper presents a domain-independent approach to translate Arabic NL\nqueries to SPARQL by leveraging linguistic analysis. Based on a special\nconsideration on Noun Phrases (NPs), our approach uses a language parser to\nextract NPs and the relations from Arabic parse trees and match them to the\nunderlying ontology. It then utilizes knowledge in the ontology to group NPs\ninto triple-based representations. A SPARQL query is finally generated by\nextracting targets and modifiers, and interpreting them into SPARQL. The\ninterpretation of advanced semantic features including negation, conjunctive\nand disjunctive modifiers is also supported. The approach was evaluated by\nusing two datasets consisting of OWL test data and queries, and the obtained\nresults have confirmed its feasibility to translate Arabic NL queries to\nSPARQL.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 16:10:21 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["AlAgha", "Iyad", ""]]}, {"id": "1508.01834", "submitter": "Bryant Chen", "authors": "Bryant Chen", "title": "Decomposition and Identification of Linear Structural Equation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of identifying linear structural\nequation models. We first extend the edge set half-trek criterion to cover a\nbroader class of models. We then show that any semi-Markovian linear model can\nbe recursively decomposed into simpler sub-models, resulting in improved\nidentification power. Finally, we show that, unlike the existing methods\ndeveloped for linear models, the resulting method subsumes the identification\nalgorithm of non-parametric models.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 23:05:13 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Chen", "Bryant", ""]]}, {"id": "1508.02035", "submitter": "Hossein Khani", "authors": "Hossein Khani and Mohsen Afsharchi", "title": "Security Games with Ambiguous Beliefs of Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently the Dempster-Shafer based algorithm and Uniform Random Probability\nbased algorithm are the preferred method of resolving security games, in which\ndefenders are able to identify attackers and only strategy remained ambiguous.\nHowever this model is inefficient in situations where resources are limited and\nboth the identity of the attackers and their strategies are ambiguous. The\nintent of this study is to find a more effective algorithm to guide the\ndefenders in choosing which outside agents with which to cooperate given both\nambiguities. We designed an experiment where defenders were compelled to engage\nwith outside agents in order to maximize protection of their targets. We\nintroduced two important notions: the behavior of each agent in target\nprotection and the tolerance threshold in the target protection process. From\nthese, we proposed an algorithm that was applied by each defender to determine\nthe best potential assistant(s) with which to cooperate. Our results showed\nthat our proposed algorithm is safer than the Dempster-Shafer based algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 14:54:57 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Khani", "Hossein", ""], ["Afsharchi", "Mohsen", ""]]}, {"id": "1508.02050", "submitter": "Tahani Almanie", "authors": "Tahani Almanie, Rsha Mirza and Elizabeth Lor", "title": "Crime Prediction Based On Crime Types And Using Spatial And Temporal\n  Criminal Hotspots", "comments": "19 pages, 18 figures, 7 tables", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.5, No.4, July 2015", "doi": "10.5121/ijdkp.2015.5401", "report-no": null, "categories": "cs.AI cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on finding spatial and temporal criminal hotspots. It\nanalyses two different real-world crimes datasets for Denver, CO and Los\nAngeles, CA and provides a comparison between the two datasets through a\nstatistical analysis supported by several graphs. Then, it clarifies how we\nconducted Apriori algorithm to produce interesting frequent patterns for\ncriminal hotspots. In addition, the paper shows how we used Decision Tree\nclassifier and Naive Bayesian classifier in order to predict potential crime\ntypes. To further analyse crimes datasets, the paper introduces an analysis\nstudy by combining our findings of Denver crimes dataset with its demographics\ninformation in order to capture the factors that might affect the safety of\nneighborhoods. The results of this solution could be used to raise awareness\nregarding the dangerous locations and to help agencies to predict future crimes\nin a specific location within a particular time.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 17:15:56 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Almanie", "Tahani", ""], ["Mirza", "Rsha", ""], ["Lor", "Elizabeth", ""]]}, {"id": "1508.02103", "submitter": "Sanghack Lee", "authors": "Sanghack Lee and Vasant Honavar", "title": "Lifted Representation of Relational Causal Models Revisited:\n  Implications for Reasoning and Structure Learning", "comments": "Workshop on Advances in Causal Inference, Conference on Uncertainty\n  in Artificial Intelligence, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maier et al. (2010) introduced the relational causal model (RCM) for\nrepresenting and inferring causal relationships in relational data. A lifted\nrepresentation, called abstract ground graph (AGG), plays a central role in\nreasoning with and learning of RCM. The correctness of the algorithm proposed\nby Maier et al. (2013a) for learning RCM from data relies on the soundness and\ncompleteness of AGG for relational d-separation to reduce the learning of an\nRCM to learning of an AGG. We revisit the definition of AGG and show that AGG,\nas defined in Maier et al. (2013b), does not correctly abstract all ground\ngraphs. We revise the definition of AGG to ensure that it correctly abstracts\nall ground graphs. We further show that AGG representation is not complete for\nrelational d-separation, that is, there can exist conditional independence\nrelations in an RCM that are not entailed by AGG. A careful examination of the\nrelationship between the lack of completeness of AGG for relational\nd-separation and faithfulness conditions suggests that weaker notions of\ncompleteness, namely adjacency faithfulness and orientation faithfulness\nbetween an RCM and its AGG, can be used to learn an RCM from data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 00:52:14 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2015 11:56:30 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Lee", "Sanghack", ""], ["Honavar", "Vasant", ""]]}, {"id": "1508.02354", "submitter": "Dimitri Kartsaklis", "authors": "Jianpeng Cheng and Dimitri Kartsaklis", "title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models\n  of Meaning", "comments": "Accepted for presentation at EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep compositional models of meaning acting on distributional representations\nof words in order to produce vectors of larger text constituents are evolving\nto a popular area of NLP research. We detail a compositional distributional\nframework based on a rich form of word embeddings that aims at facilitating the\ninteractions between words in the context of a sentence. Embeddings and\ncomposition layers are jointly learned against a generic objective that\nenhances the vectors with syntactic information from the surrounding context.\nFurthermore, each word is associated with a number of senses, the most\nplausible of which is selected dynamically during the composition process. We\nevaluate the produced vectors qualitatively and quantitatively with positive\nresults. At the sentence level, the effectiveness of the framework is\ndemonstrated on the MSRPar task, for which we report results within the\nstate-of-the-art range.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 19:04:18 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 12:50:43 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Cheng", "Jianpeng", ""], ["Kartsaklis", "Dimitri", ""]]}, {"id": "1508.02593", "submitter": "Denis Krompass", "authors": "Denis Krompa{\\ss} and Stephan Baier and Volker Tresp", "title": "Type-Constrained Representation Learning in Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large knowledge graphs increasingly add value to various applications that\nrequire machines to recognize and understand queries and their semantics, as in\nsearch or question answering systems. Latent variable models have increasingly\ngained attention for the statistical modeling of knowledge graphs, showing\npromising results in tasks related to knowledge graph completion and cleaning.\nBesides storing facts about the world, schema-based knowledge graphs are backed\nby rich semantic descriptions of entities and relation-types that allow\nmachines to understand the notion of things and their semantic relationships.\nIn this work, we study how type-constraints can generally support the\nstatistical modeling with latent variable models. More precisely, we integrated\nprior knowledge in form of type-constraints in various state of the art latent\nvariable approaches. Our experimental results show that prior knowledge on\nrelation-types significantly improves these models up to 77% in link-prediction\ntasks. The achieved improvements are especially prominent when a low model\ncomplexity is enforced, a crucial requirement when these models are applied to\nvery large datasets. Unfortunately, type-constraints are neither always\navailable nor always complete e.g., they can become fuzzy when entities lack\nproper typing. We show that in these cases, it can be beneficial to apply a\nlocal closed-world assumption that approximates the semantics of relation-types\nbased on observations made in the data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 13:49:07 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 09:00:31 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Krompa\u00df", "Denis", ""], ["Baier", "Stephan", ""], ["Tresp", "Volker", ""]]}, {"id": "1508.02626", "submitter": "Stefan Borgwardt", "authors": "Stefan Borgwardt and Theofilos Mailis and Rafael Pe\\~naloza and\n  Anni-Yasmin Turhan", "title": "Answering Fuzzy Conjunctive Queries over Finitely Valued Fuzzy\n  Ontologies", "comments": "submitted to the Journal on Data Semantics, v1: 19 pages, v2: 20\n  pages, improved evaluation section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy Description Logics (DLs) provide a means for representing vague\nknowledge about an application domain. In this paper, we study fuzzy extensions\nof conjunctive queries (CQs) over the DL $\\mathcal{SROIQ}$ based on finite\nchains of degrees of truth. To answer such queries, we extend a well-known\ntechnique that reduces the fuzzy ontology to a classical one, and use classical\nDL reasoners as a black box. We improve the complexity of previous reduction\ntechniques for finitely valued fuzzy DLs, which allows us to prove tight\ncomplexity results for answering certain kinds of fuzzy CQs. We conclude with\nan experimental evaluation of a prototype implementation, showing the\nfeasibility of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 15:25:21 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 09:10:31 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Borgwardt", "Stefan", ""], ["Mailis", "Theofilos", ""], ["Pe\u00f1aloza", "Rafael", ""], ["Turhan", "Anni-Yasmin", ""]]}, {"id": "1508.02681", "submitter": "Fatemeh Jahedpari", "authors": "Fatemeh Jahedpari, Marina De Vos, Sattar Hashemi, Benjamin Hirsch,\n  Julian Padget", "title": "Artificial Prediction Markets for Online Prediction of Continuous\n  Variables-A Preliminary Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Artificial Continuous Prediction Market (ACPM) as a means to\npredict a continuous real value, by integrating a range of data sources and\naggregating the results of different machine learning (ML) algorithms. ACPM\nadapts the concept of the (physical) prediction market to address the\nprediction of real values instead of discrete events. Each ACPM participant has\na data source, a ML algorithm and a local decision-making procedure that\ndetermines what to bid on what value. The contributions of ACPM are: (i)\nadaptation to changes in data quality by the use of learning in: (a) the\nmarket, which weights each market participant to adjust the influence of each\non the market prediction and (b) the participants, which use a Q-learning based\ntrading strategy to incorporate the market prediction into their subsequent\npredictions, (ii) resilience to a changing population of low- and\nhigh-performing participants. We demonstrate the effectiveness of ACPM by\napplication to an influenza-like illnesses data set, showing ACPM out-performs\na range of well-known regression models and is resilient to variation in data\nsource quality.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 18:37:29 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Jahedpari", "Fatemeh", ""], ["De Vos", "Marina", ""], ["Hashemi", "Sattar", ""], ["Hirsch", "Benjamin", ""], ["Padget", "Julian", ""]]}, {"id": "1508.03032", "submitter": "Kostyantyn Shchekotykhin", "authors": "Andreas Falkner and Anna Ryabokon and Gottfried Schenner and\n  Kostyantyn Shchekotykhin", "title": "OOASP: Connecting Object-oriented and Logic Programming", "comments": "13 pages, 4 figures, accepted for publication at LPNMR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of contemporary software systems are implemented using an\nobject-oriented approach. Modeling phases -- during which software engineers\nanalyze requirements to the future system using some modeling language -- are\nan important part of the development process, since modeling errors are often\nhard to recognize and correct.\n  In this paper we present a framework which allows the integration of Answer\nSet Programming into the object-oriented software development process. OOASP\nsupports reasoning about object-oriented software models and their\ninstantiations. Preliminary results of the OOASP application in CSL Studio,\nwhich is a Siemens internal modeling environment for product configurators,\nshow that it can be used as a lightweight approach to verify, create and\ntransform instantiations of object models at runtime and to support the\nsoftware development process during design and testing.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 18:59:41 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Falkner", "Andreas", ""], ["Ryabokon", "Anna", ""], ["Schenner", "Gottfried", ""], ["Shchekotykhin", "Kostyantyn", ""]]}, {"id": "1508.03064", "submitter": "Yves Lucet", "authors": "Yasha Pushak, Warren Hare, Yves Lucet", "title": "Multiple-Path Selection for new Highway Alignments using Discrete\n  Algorithms", "comments": "to be published in European Journal of Operational Research", "journal-ref": null, "doi": "10.1016/j.ejor.2015.07.039", "report-no": null, "categories": "cs.DS cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of finding multiple near-optimal,\nspatially-dissimilar paths that can be considered as alternatives in the\ndecision making process, for finding optimal corridors in which to construct a\nnew road. We further consider combinations of techniques for reducing the costs\nassociated with the computation and increasing the accuracy of the cost\nformulation. Numerical results for five algorithms to solve the dissimilar\nmultipath problem show that a \"bidirectional approach\" yields the fastest\nrunning times and the most robust algorithm. Further modifications of the\nalgorithms to reduce the running time were tested and it is shown that running\ntime can be reduced by an average of 56 percent without compromising the\nquality of the results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 05:12:47 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Pushak", "Yasha", ""], ["Hare", "Warren", ""], ["Lucet", "Yves", ""]]}, {"id": "1508.03170", "submitter": "Paulo Figueiredo", "authors": "Paulo Figueiredo and Marta Apar\\'icio and David Martins de Matos and\n  Ricardo Ribeiro", "title": "Generation of Multimedia Artifacts: An Extractive Summarization-based\n  Approach", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore methods for content selection and address the issue of coherence\nin the context of the generation of multimedia artifacts. We use audio and\nvideo to present two case studies: generation of film tributes, and\nlecture-driven science talks. For content selection, we use centrality-based\nand diversity-based summarization, along with topic analysis. To establish\ncoherence, we use the emotional content of music, for film tributes, and ensure\ntopic similarity between lectures and documentaries, for science talks.\nComposition techniques for the production of multimedia artifacts are addressed\nas a means of organizing content, in order to improve coherence. We discuss our\nresults considering the above aspects.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 10:56:42 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Figueiredo", "Paulo", ""], ["Apar\u00edcio", "Marta", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""]]}, {"id": "1508.03276", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt and Harshita Jhavar", "title": "Talking about the Moving Image: A Declarative Model for Image Schema\n  Based Embodied Perception Grounding and Language Generation", "comments": "19 pages. Unpublished report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general theory and corresponding declarative model for the\nembodied grounding and natural language based analytical summarisation of\ndynamic visuo-spatial imagery. The declarative model ---ecompassing\nspatio-linguistic abstractions, image schemas, and a spatio-temporal feature\nbased language generator--- is modularly implemented within Constraint Logic\nProgramming (CLP). The implemented model is such that primitives of the theory,\ne.g., pertaining to space and motion, image schemata, are available as\nfirst-class objects with `deep semantics' suited for inference and query. We\ndemonstrate the model with select examples broadly motivated by areas such as\nfilm, design, geography, smart environments where analytical natural language\nbased externalisations of the moving image are central from the viewpoint of\nhuman interaction, evidence-based qualitative analysis, and sensemaking.\n  Keywords: moving image, visual semantics and embodiment, visuo-spatial\ncognition and computation, cognitive vision, computational models of narrative,\ndeclarative spatial reasoning\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 17:34:07 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Jhavar", "Harshita", ""]]}, {"id": "1508.03523", "submitter": "Jesus Cerquides", "authors": "Jordi Roca-Lacostena and Jesus Cerquides and Marc Pouly", "title": "Sufficient and necessary conditions for Dynamic Programming in\n  Valuation-Based Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valuation algebras abstract a large number of formalisms for automated\nreasoning and enable the definition of generic inference procedures. Many of\nthese formalisms provide some notion of solution. Typical examples are\nsatisfying assignments in constraint systems, models in logics or solutions to\nlinear equation systems.\n  Many widely used dynamic programming algorithms for optimization problems\nrely on low treewidth decompositions and can be understood as particular cases\nof a single algorithmic scheme for finding solutions in a valuation algebra.\nThe most encompassing description of this algorithmic scheme to date has been\nproposed by Pouly and Kohlas together with sufficient conditions for its\ncorrectness. Unfortunately, the formalization relies on a theorem for which we\nprovide counterexamples. In spite of that, the mainline of Pouly and Kohlas'\ntheory is correct, although some of the necessary conditions have to be\nrevised. In this paper we analyze the impact that the counter-examples have on\nthe theory, and rebuild the theory providing correct sufficient conditions for\nthe algorithms. Furthermore, we also provide necessary conditions for the\nalgorithms, allowing for a sharper characterization of when the algorithmic\nscheme can be applied.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 14:51:54 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Roca-Lacostena", "Jordi", ""], ["Cerquides", "Jesus", ""], ["Pouly", "Marc", ""]]}, {"id": "1508.03671", "submitter": "Ibrahim Ozkan", "authors": "Ibrahim Ozkan, I. Burhan Turksen", "title": "Fuzzy Longest Common Subsequence Matching With FCM Using R", "comments": "Prepared April 17, 2013. 26 Pages, updated March 10, 2016 included R\n  code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Capturing the interdependencies between real valued time series can be\nachieved by finding common similar patterns. The abstraction of time series\nmakes the process of finding similarities closer to the way as humans do.\nTherefore, the abstraction by means of a symbolic levels and finding the common\npatterns attracts researchers. One particular algorithm, Longest Common\nSubsequence, has been used successfully as a similarity measure between two\nsequences including real valued time series. In this paper, we propose Fuzzy\nLongest Common Subsequence matching for time series.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 22:19:48 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 17:53:53 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Ozkan", "Ibrahim", ""], ["Turksen", "I. Burhan", ""]]}, {"id": "1508.03686", "submitter": "Diederik Aerts", "authors": "Diederik Aerts and Massimiliano Sassoli de Bianchi", "title": "Beyond-Quantum Modeling of Question Order Effects and Response\n  Replicability in Psychological Measurements", "comments": "32 pages, 5 figures", "journal-ref": "Journal of Mathematical Psychology Volume 79, August 2017, Pages\n  104-120", "doi": "10.1016/j.jmp.2017.03.004", "report-no": null, "categories": "cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general tension-reduction (GTR) model was recently considered to derive\nquantum probabilities as (universal) averages over all possible forms of\nnon-uniform fluctuations, and explain their considerable success in describing\nexperimental situations also outside of the domain of physics, for instance in\nthe ambit of quantum models of cognition and decision. Yet, this result also\nhighlighted the possibility of observing violations of the predictions of the\nBorn rule, in those situations where the averaging would not be large enough,\nor would be altered because of the combination of multiple measurements. In\nthis article we show that this is indeed the case in typical psychological\nmeasurements exhibiting question order effects, by showing that their\nstatistics of outcomes are inherently non-Hilbertian, and require the larger\nframework of the GTR-model to receive an exact mathematical description. We\nalso consider another unsolved problem of quantum cognition: response\nreplicability. It is has been observed that when question order effects and\nresponse replicability occur together, the situation cannot be handled anymore\nby quantum theory. However, we show that it can be easily and naturally\ndescribed in the GTR-model. Based on these findings, we motivate the adoption\nin cognitive science of a hidden-measurements interpretation of the quantum\nformalism, and of its GTR-model generalization, as the natural interpretational\nframework explaining the data of psychological measurements on conceptual\nentities.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 02:07:30 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Aerts", "Diederik", ""], ["de Bianchi", "Massimiliano Sassoli", ""]]}, {"id": "1508.03812", "submitter": "Thuc Le Ph.D", "authors": "Jiuyong Li, Saisai Ma, Thuc Duy Le, Lin Liu and Jixue Liu", "title": "Causal Decision Trees", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2016.2619350", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncovering causal relationships in data is a major objective of data\nanalytics. Causal relationships are normally discovered with designed\nexperiments, e.g. randomised controlled trials, which, however are expensive or\ninfeasible to be conducted in many cases. Causal relationships can also be\nfound using some well designed observational studies, but they require domain\nexperts' knowledge and the process is normally time consuming. Hence there is a\nneed for scalable and automated methods for causal relationship exploration in\ndata. Classification methods are fast and they could be practical substitutes\nfor finding causal signals in data. However, classification methods are not\ndesigned for causal discovery and a classification method may find false causal\nsignals and miss the true ones. In this paper, we develop a causal decision\ntree where nodes have causal interpretations. Our method follows a well\nestablished causal inference framework and makes use of a classic statistical\ntest. The method is practical for finding causal signals in large data sets.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 11:31:49 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Li", "Jiuyong", ""], ["Ma", "Saisai", ""], ["Le", "Thuc Duy", ""], ["Liu", "Lin", ""], ["Liu", "Jixue", ""]]}, {"id": "1508.03819", "submitter": "Thuc Le Ph.D", "authors": "Jiuyong Li, Thuc Duy Le, Lin Liu, Jixue Liu, Zhou Jin, Bingyu Sun,\n  Saisai Ma", "title": "From Observational Studies to Causal Rule Mining", "comments": "This paper has been accepted by ACM TIST journal and will be\n  available soon", "journal-ref": "ACM Trans. Intell. Syst. Technol. 7, 2, Article 14 (November\n  2015), 27 pages", "doi": "10.1145/2746410", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomised controlled trials (RCTs) are the most effective approach to causal\ndiscovery, but in many circumstances it is impossible to conduct RCTs.\nTherefore observational studies based on passively observed data are widely\naccepted as an alternative to RCTs. However, in observational studies, prior\nknowledge is required to generate the hypotheses about the cause-effect\nrelationships to be tested, hence they can only be applied to problems with\navailable domain knowledge and a handful of variables. In practice, many data\nsets are of high dimensionality, which leaves observational studies out of the\nopportunities for causal discovery from such a wealth of data sources. In\nanother direction, many efficient data mining methods have been developed to\nidentify associations among variables in large data sets. The problem is,\ncausal relationships imply associations, but the reverse is not always true.\nHowever we can see the synergy between the two paradigms here. Specifically,\nassociation rule mining can be used to deal with the high-dimensionality\nproblem while observational studies can be utilised to eliminate non-causal\nassociations. In this paper we propose the concept of causal rules (CRs) and\ndevelop an algorithm for mining CRs in large data sets. We use the idea of\nretrospective cohort studies to detect CRs based on the results of association\nrule mining. Experiments with both synthetic and real world data sets have\ndemonstrated the effectiveness and efficiency of CR mining. In comparison with\nthe commonly used causal discovery methods, the proposed approach in general is\nfaster and has better or competitive performance in finding correct or sensible\ncauses. It is also capable of finding a cause consisting of multiple variables,\na feature that other causal discovery methods do not possess.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 12:33:18 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Li", "Jiuyong", ""], ["Le", "Thuc Duy", ""], ["Liu", "Lin", ""], ["Liu", "Jixue", ""], ["Jin", "Zhou", ""], ["Sun", "Bingyu", ""], ["Ma", "Saisai", ""]]}, {"id": "1508.03846", "submitter": "Jose Picado", "authors": "Jose Picado, Arash Termehchy, Alan Fern, Parisa Ataei", "title": "Schema Independent Relational Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning novel concepts and relations from relational databases is an\nimportant problem with many applications in database systems and machine\nlearning. Relational learning algorithms learn the definition of a new relation\nin terms of existing relations in the database. Nevertheless, the same data set\nmay be represented under different schemas for various reasons, such as\nefficiency, data quality, and usability. Unfortunately, the output of current\nrelational learning algorithms tends to vary quite substantially over the\nchoice of schema, both in terms of learning accuracy and efficiency. This\nvariation complicates their off-the-shelf application. In this paper, we\nintroduce and formalize the property of schema independence of relational\nlearning algorithms, and study both the theoretical and empirical dependence of\nexisting algorithms on the common class of (de) composition schema\ntransformations. We study both sample-based learning algorithms, which learn\nfrom sets of labeled examples, and query-based algorithms, which learn by\nasking queries to an oracle. We prove that current relational learning\nalgorithms are generally not schema independent. For query-based learning\nalgorithms we show that the (de) composition transformations influence their\nquery complexity. We propose Castor, a sample-based relational learning\nalgorithm that achieves schema independence by leveraging data dependencies. We\nsupport the theoretical results with an empirical study that demonstrates the\nschema dependence/independence of several algorithms on existing benchmark and\nreal-world datasets under (de) compositions.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 16:57:20 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 20:35:32 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Picado", "Jose", ""], ["Termehchy", "Arash", ""], ["Fern", "Alan", ""], ["Ataei", "Parisa", ""]]}, {"id": "1508.03863", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Discrete Route/Trajectory Decision Making Problems", "comments": "25 pages, 34 figures, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper focuses on composite multistage decision making problems which are\ntargeted to design a route/trajectory from an initial decision situation\n(origin) to goal (destination) decision situation(s). Automobile routing\nproblem is considered as a basic physical metaphor. The problems are based on a\ndiscrete (combinatorial) operations/states design/solving space (e.g.,\ndigraph). The described types of discrete decision making problems can be\nconsidered as intelligent design of a route (trajectory, strategy) and can be\nused in many domains: (a) education (planning of student educational\ntrajectory), (b) medicine (medical treatment), (c) economics (trajectory of\nstart-up development). Several types of the route decision making problems are\ndescribed: (i) basic route decision making, (ii) multi-goal route decision\nmaking, (iii) multi-route decision making, (iv) multi-route decision making\nwith route/trajectory change(s), (v) composite multi-route decision making\n(solution is a composition of several routes/trajectories at several\ncorresponding domains), and (vi) composite multi-route decision making with\ncoordinated routes/trajectories. In addition, problems of modeling and building\nthe design spaces are considered. Numerical examples illustrate the suggested\napproach. Three applications are considered: educational trajectory\n(orienteering problem), plan of start-up company (modular three-stage design),\nand plan of medical treatment (planning over digraph with two-component\nvertices).\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 20:37:35 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1508.03891", "submitter": "Mohan Sridharan", "authors": "Mohan Sridharan, Michael Gelfond, Shiqi Zhang, Jeremy Wyatt", "title": "REBA: A Refinement-Based Architecture for Knowledge Representation and\n  Reasoning in Robotics", "comments": "72 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an architecture for robots that combines the\ncomplementary strengths of probabilistic graphical models and declarative\nprogramming to represent and reason with logic-based and probabilistic\ndescriptions of uncertainty and domain knowledge. An action language is\nextended to support non-boolean fluents and non-deterministic causal laws. This\naction language is used to describe tightly-coupled transition diagrams at two\nlevels of granularity, with a fine-resolution transition diagram defined as a\nrefinement of a coarse-resolution transition diagram of the domain. The\ncoarse-resolution system description, and a history that includes (prioritized)\ndefaults, are translated into an Answer Set Prolog (ASP) program. For any given\ngoal, inference in the ASP program provides a plan of abstract actions. To\nimplement each such abstract action, the robot automatically zooms to the part\nof the fine-resolution transition diagram relevant to this action. A\nprobabilistic representation of the uncertainty in sensing and actuation is\nthen included in this zoomed fine-resolution system description, and used to\nconstruct a partially observable Markov decision process (POMDP). The policy\nobtained by solving the POMDP is invoked repeatedly to implement the abstract\naction as a sequence of concrete actions, with the corresponding observations\nbeing recorded in the coarse-resolution history and used for subsequent\nreasoning. The architecture is evaluated in simulation and on a mobile robot\nmoving objects in an indoor domain, to show that it supports reasoning with\nviolation of defaults, noisy observations and unreliable actions, in complex\ndomains.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 01:17:49 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 11:01:57 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 20:50:13 GMT"}, {"version": "v4", "created": "Fri, 21 Sep 2018 12:47:50 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Sridharan", "Mohan", ""], ["Gelfond", "Michael", ""], ["Zhang", "Shiqi", ""], ["Wyatt", "Jeremy", ""]]}, {"id": "1508.04032", "submitter": "Yexiang Xue", "authors": "Yexiang Xue, Stefano Ermon, Ronan Le Bras, Carla P. Gomes, Bart Selman", "title": "Variable Elimination in the Fourier Domain", "comments": "Proceedings of the 33rd International Conference on Machine Learning\n  (ICML), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to represent complex high dimensional probability distributions\nin a compact form is one of the key insights in the field of graphical models.\nFactored representations are ubiquitous in machine learning and lead to major\ncomputational advantages. We explore a different type of compact representation\nbased on discrete Fourier representations, complementing the classical approach\nbased on conditional independencies. We show that a large class of\nprobabilistic graphical models have a compact Fourier representation. This\ntheoretical result opens up an entirely new way of approximating a probability\ndistribution. We demonstrate the significance of this approach by applying it\nto the variable elimination algorithm. Compared with the traditional bucket\nrepresentation and other approximate inference algorithms, we obtain\nsignificant improvements.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 14:04:07 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 03:18:10 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Xue", "Yexiang", ""], ["Ermon", "Stefano", ""], ["Bras", "Ronan Le", ""], ["Gomes", "Carla P.", ""], ["Selman", "Bart", ""]]}, {"id": "1508.04087", "submitter": "J. G. Wolff", "authors": "J. G. Wolff", "title": "The SP theory of intelligence: distinctive features and advantages", "comments": null, "journal-ref": "IEEE Access, 4, 216-246, 2016", "doi": "10.1109/ACCESS.2015.2513822", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper highlights distinctive features of the \"SP theory of intelligence\"\nand its apparent advantages compared with some AI-related alternatives.\nDistinctive features and advantages are: simplification and integration of\nobservations and concepts; simplification and integration of structures and\nprocesses in computing systems; the theory is itself a theory of computing; it\ncan be the basis for new architectures for computers; information compression\nvia the matching and unification of patterns and, more specifically, via\nmultiple alignment, is fundamental; transparency in the representation and\nprocessing of knowledge; the discovery of 'natural' structures via information\ncompression (DONSVIC); interpretations of mathematics; interpretations in human\nperception and cognition; and realisation of abstract concepts in terms of\nneurons and their inter-connections (\"SP-neural\"). These things relate to\nAI-related alternatives: minimum length encoding and related concepts; deep\nlearning in neural networks; unified theories of cognition and related\nresearch; universal search; Bayesian networks and more; pattern recognition and\nvision; the analysis, production, and translation of natural language;\nUnsupervised learning of natural language; exact and inexact forms of\nreasoning; representation and processing of diverse forms of knowledge; IBM's\nWatson; software engineering; solving problems associated with big data, and in\nthe development of intelligence in autonomous robots. In conclusion, the SP\nsystem can provide a firm foundation for the long-term development of AI, with\nmany potential benefits and applications. It may also deliver useful results on\nrelatively short timescales. A high-parallel, open-source version of the SP\nmachine, derived from the SP computer model, would be a means for researchers\neverywhere to explore what can be done with the system, and to create new\nversions of it.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 17:15:13 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 08:48:08 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2015 10:16:04 GMT"}, {"version": "v4", "created": "Fri, 6 Nov 2015 17:59:52 GMT"}, {"version": "v5", "created": "Sun, 20 Dec 2015 12:05:50 GMT"}, {"version": "v6", "created": "Tue, 15 Mar 2016 16:09:02 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Wolff", "J. G.", ""]]}, {"id": "1508.04112", "submitter": "Tao Lei", "authors": "Tao Lei, Regina Barzilay and Tommi Jaakkola", "title": "Molding CNNs for text: non-linear, non-consecutive convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning often derives from well-chosen operational\nbuilding blocks. In this work, we revise the temporal convolution operation in\nCNNs to better adapt it to text processing. Instead of concatenating word\nrepresentations, we appeal to tensor algebra and use low-rank n-gram tensors to\ndirectly exploit interactions between words already at the convolution stage.\nMoreover, we extend the n-gram convolution to non-consecutive words to\nrecognize patterns with intervening words. Through a combination of low-rank\ntensors, and pattern weighting, we can efficiently evaluate the resulting\nconvolution operation via dynamic programming. We test the resulting\narchitecture on standard sentiment classification and news categorization\ntasks. Our model achieves state-of-the-art performance both in terms of\naccuracy and training speed. For instance, we obtain 51.2% accuracy on the\nfine-grained sentiment classification task.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 19:02:45 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 02:52:40 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Lei", "Tao", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1508.04145", "submitter": "Benja Fallenstein", "authors": "Benja Fallenstein, Jessica Taylor, Paul F. Christiano", "title": "Reflective Oracles: A Foundation for Classical Game Theory", "comments": "Extended version of \"Reflective Oracles: A Foundation for Game Theory\n  in Artificial Intelligence\" accepted to LORI-V", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical game theory treats players as special---a description of a game\ncontains a full, explicit enumeration of all players---even though in the real\nworld, \"players\" are no more fundamentally special than rocks or clouds. It\nisn't trivial to find a decision-theoretic foundation for game theory in which\nan agent's coplayers are a non-distinguished part of the agent's environment.\nAttempts to model both players and the environment as Turing machines, for\nexample, fail for standard diagonalization reasons.\n  In this paper, we introduce a \"reflective\" type of oracle, which is able to\nanswer questions about the outputs of oracle machines with access to the same\noracle. These oracles avoid diagonalization by answering some queries randomly.\nWe show that machines with access to a reflective oracle can be used to define\nrational agents using causal decision theory. These agents model their\nenvironment as a probabilistic oracle machine, which may contain other agents\nas a non-distinguished part.\n  We show that if such agents interact, they will play a Nash equilibrium, with\nthe randomization in mixed strategies coming from the randomization in the\noracle's answers. This can be seen as providing a foundation for classical game\ntheory in which players aren't special.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 20:08:37 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Fallenstein", "Benja", ""], ["Taylor", "Jessica", ""], ["Christiano", "Paul F.", ""]]}, {"id": "1508.04159", "submitter": "Andr\\'e Dietrich", "authors": "Andr\\'e Dietrich, Sebastian Zug, Luigi Nardi, J\\\"org Kaiser", "title": "Reasoning in complex environments with the SelectScript declarative\n  language", "comments": "15 pages, 7 figures, 6th International Workshop on Domain-Specific\n  Languages and models for ROBotic systems (DSLRob-15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.DB cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SelectScript is an extendable, adaptable, and declarative domain-specific\nlanguage aimed at information retrieval from simulation environments and\nrobotic world models in an SQL-like manner. In this work we have extended the\nlanguage in two directions. First, we have implemented hierarchical queries;\nsecond, we improve efficiency enabling manual design space exploration on\ndifferent \"search\" strategies. We demonstrate the applicability of such\nextensions in two application problems; the basic language concepts are\nexplained by solving the classical problem of the Towers of Hanoi and then a\ncommon path planning problem in a complex 3D environment is implemented.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 21:26:39 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2015 15:53:29 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Dietrich", "Andr\u00e9", ""], ["Zug", "Sebastian", ""], ["Nardi", "Luigi", ""], ["Kaiser", "J\u00f6rg", ""]]}, {"id": "1508.04186", "submitter": "Hao Yi Ong", "authors": "Hao Yi Ong, Kevin Chavez, Augustus Hong", "title": "Distributed Deep Q-Learning", "comments": "Updated figure of distributed deep learning architecture, updated\n  content throughout paper including dealing with minor grammatical issues and\n  highlighting differences of our paper with respect to prior work. arXiv admin\n  note: text overlap with arXiv:1312.5602 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed deep learning model to successfully learn control\npolicies directly from high-dimensional sensory input using reinforcement\nlearning. The model is based on the deep Q-network, a convolutional neural\nnetwork trained with a variant of Q-learning. Its input is raw pixels and its\noutput is a value function estimating future rewards from taking an action\ngiven a system state. To distribute the deep Q-network training, we adapt the\nDistBelief software framework to the context of efficiently training\nreinforcement learning agents. As a result, the method is completely\nasynchronous and scales well with the number of machines. We demonstrate that\nthe deep Q-network agent, receiving only the pixels and the game score as\ninputs, was able to achieve reasonable success on a simple game with minimal\nparameter tuning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 01:00:32 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 09:06:38 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Ong", "Hao Yi", ""], ["Chavez", "Kevin", ""], ["Hong", "Augustus", ""]]}, {"id": "1508.04261", "submitter": "Paolo Campigotto", "authors": "Paolo Campigotto, Roberto Battiti, Andrea Passerini", "title": "Learning Modulo Theories for preference elicitation in hybrid domains", "comments": "50 pages, 3 figures, submitted to Artificial Intelligence Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces CLEO, a novel preference elicitation algorithm capable\nof recommending complex objects in hybrid domains, characterized by both\ndiscrete and continuous attributes and constraints defined over them. The\nalgorithm assumes minimal initial information, i.e., a set of catalog\nattributes, and defines decisional features as logic formulae combining Boolean\nand algebraic constraints over the attributes. The (unknown) utility of the\ndecision maker (DM) is modelled as a weighted combination of features. CLEO\niteratively alternates a preference elicitation step, where pairs of candidate\nsolutions are selected based on the current utility model, and a refinement\nstep where the utility is refined by incorporating the feedback received. The\nelicitation step leverages a Max-SMT solver to return optimal hybrid solutions\naccording to the current utility model. The refinement step is implemented as\nlearning to rank, and a sparsifying norm is used to favour the selection of few\ninformative features in the combinatorial space of candidate decisional\nfeatures.\n  CLEO is the first preference elicitation algorithm capable of dealing with\nhybrid domains, thanks to the use of Max-SMT technology, while retaining\nuncertainty in the DM utility and noisy feedback. Experimental results on\ncomplex recommendation tasks show the ability of CLEO to quickly focus towards\noptimal solutions, as well as its capacity to recover from suboptimal initial\nchoices. While no competitors exist in the hybrid setting, CLEO outperforms a\nstate-of-the-art Bayesian preference elicitation algorithm when applied to a\npurely discrete task.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 09:50:33 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 10:29:29 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2015 09:37:08 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Campigotto", "Paolo", ""], ["Battiti", "Roberto", ""], ["Passerini", "Andrea", ""]]}, {"id": "1508.04395", "submitter": "Dzmitry Bahdanau", "authors": "Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel,\n  Yoshua Bengio", "title": "End-to-End Attention-based Large Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the current state-of-the-art Large Vocabulary Continuous Speech\nRecognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov\nModels (HMMs). Most of these systems contain separate components that deal with\nthe acoustic modelling, language modelling and sequence decoding. We\ninvestigate a more direct approach in which the HMM is replaced with a\nRecurrent Neural Network (RNN) that performs sequence prediction directly at\nthe character level. Alignment between the input features and the desired\ncharacter sequence is learned automatically by an attention mechanism built\ninto the RNN. For each predicted character, the attention mechanism scans the\ninput sequence and chooses relevant frames. We propose two methods to speed up\nthis operation: limiting the scan to a subset of most promising frames and\npooling over time the information contained in neighboring frames, thereby\nreducing source sequence length. Integrating an n-gram language model into the\ndecoding process yields recognition accuracies similar to other HMM-free\nRNN-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 17:40:00 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 23:07:20 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Bahdanau", "Dzmitry", ""], ["Chorowski", "Jan", ""], ["Serdyuk", "Dmitriy", ""], ["Brakel", "Philemon", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1508.04522", "submitter": "Palash Dey", "authors": "Arnab Bhattacharyya and Palash Dey", "title": "Fishing out Winners from Vote Streams", "comments": "Adding Acknowledgement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DM cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of winner determination from computational social\nchoice theory in the data stream model. Specifically, we consider the task of\nsummarizing an arbitrarily ordered stream of $n$ votes on $m$ candidates into a\nsmall space data structure so as to be able to obtain the winner determined by\npopular voting rules. As we show, finding the exact winner requires storing\nessentially all the votes. So, we focus on the problem of finding an {\\em\n$\\eps$-winner}, a candidate who could win by a change of at most $\\eps$\nfraction of the votes. We show non-trivial upper and lower bounds on the space\ncomplexity of $\\eps$-winner determination for several voting rules, including\n$k$-approval, $k$-veto, scoring rules, approval, maximin, Bucklin, Copeland,\nand plurality with run off.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 04:09:03 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 04:27:41 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Dey", "Palash", ""]]}, {"id": "1508.04561", "submitter": "Wlodzislaw Duch", "authors": "W{\\l}odzis{\\l}aw Duch", "title": "Memetics and Neural Models of Conspiracy Theories", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conspiracy theories, or in general seriously distorted beliefs, are\nwidespread. How and why are they formed in the brain is still more a matter of\nspeculation rather than science. In this paper one plausible mechanisms is\ninvestigated: rapid freezing of high neuroplasticity (RFHN). Emotional arousal\nincreases neuroplasticity and leads to creation of new pathways spreading\nneural activation. Using the language of neurodynamics a meme is defined as\nquasi-stable associative memory attractor state. Depending on the temporal\ncharacteristics of the incoming information and the plasticity of the network,\nmemory may self-organize creating memes with large attractor basins, linking\nmany unrelated input patterns. Memes with fake rich associations distort\nrelations between memory states. Simulations of various neural network models\ntrained with competitive Hebbian learning (CHL) on stationary and\nnon-stationary data lead to the same conclusion: short learning with high\nplasticity followed by rapid decrease of plasticity leads to memes with large\nattraction basins, distorting input pattern representations in associative\nmemory. Such system-level models may be used to understand creation of\ndistorted beliefs and formation of conspiracy memes, understood as strong\nattractor states of the neurodynamics.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 08:20:17 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 17:38:40 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Duch", "W\u0142odzis\u0142aw", ""]]}, {"id": "1508.04570", "submitter": "J. G. Wolff", "authors": "J. Gerard Wolff, Vasile Palade", "title": "Proposal for the creation of a research facility for the development of\n  the SP machine", "comments": "arXiv admin note: text overlap with arXiv:1508.04087. substantial\n  text overlap with arXiv:1409.8027", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a proposal to create a research facility for the development of a\nhigh-parallel version of the \"SP machine\", based on the \"SP theory of\nintelligence\". We envisage that the new version of the SP machine will be an\nopen-source software virtual machine, derived from the existing \"SP computer\nmodel\", and hosted on an existing high-performance computer. It will be a means\nfor researchers everywhere to explore what can be done with the system and to\ncreate new versions of it. The SP system is a unique attempt to simplify and\nintegrate observations and concepts across artificial intelligence, mainstream\ncomputing, mathematics, and human perception and cognition, with information\ncompression as a unifying theme. Potential benefits and applications include\nhelping to solve problems associated with big data; facilitating the\ndevelopment of autonomous robots; unsupervised learning, natural language\nprocessing, several kinds of reasoning, fuzzy pattern recognition at multiple\nlevels of abstraction, computer vision, best-match and semantic forms of\ninformation retrieval, software engineering, medical diagnosis, simplification\nof computing systems, and the seamless integration of diverse kinds of\nknowledge and diverse aspects of intelligence. Additional motivations include\nthe potential of the SP system to help solve problems in defence, security, and\nthe detection and prevention of crime; potential in terms of economic, social,\nenvironmental, and academic criteria, and in terms of publicity; and the\npotential for international influence in research. The main elements of the\nproposed facility are described, including support for the development of\n\"SP-neural\", a neural version of the SP machine. The facility should be\npermanent in the sense that it should be available for the foreseeable future,\nand it should be designed to facilitate its use by researchers anywhere in the\nworld.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 09:03:18 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Wolff", "J. Gerard", ""], ["Palade", "Vasile", ""]]}, {"id": "1508.04633", "submitter": "Johannes Textor", "authors": "Johannes Textor", "title": "Drawing and Analyzing Causal DAGs with DAGitty", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DAGitty is a software for drawing and analyzing causal diagrams, also known\nas directed acyclic graphs (DAGs). Functions include identification of minimal\nsufficient adjustment sets for estimating causal effects, diagnosis of\ninsufficient or invalid adjustment via the identification of biasing paths,\nidentification of instrumental variables, and derivation of testable\nimplications. DAGitty is provided in the hope that it is useful for researchers\nand students in Epidemiology, Sociology, Psychology, and other empirical\ndisciplines. The software should run in any web browser that supports modern\nJavaScript, HTML, and SVG. This is the user manual for DAGitty version 2.3. The\nmanual is updated with every release of a new stable version. DAGitty is\navailable at dagitty.net.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 13:11:32 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Textor", "Johannes", ""]]}, {"id": "1508.04872", "submitter": "Yustinus Soelistio Eko", "authors": "Ardy Wibowo Haryanto, Adhi Kusnadi, Yustinus Eko Soelistio", "title": "Warehouse Layout Method Based on Ant Colony and Backtracking Algorithm", "comments": "5 pages, published in proceeding of the 14th IAPR International\n  Conference on Quality in Research (QIR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Warehouse is one of the important aspects of a company. Therefore, it is\nnecessary to improve Warehouse Management System (WMS) to have a simple\nfunction that can determine the layout of the storage goods. In this paper we\npropose an improved warehouse layout method based on ant colony algorithm and\nbacktracking algorithm. The method works on two steps. First, it generates a\nsolutions parameter tree from backtracking algorithm. Then second, it deducts\nthe solutions parameter by using a combination of ant colony algorithm and\nbacktracking algorithm. This method was tested by measuring the time needed to\nbuild the tree and to fill up the space using two scenarios. The method needs\n0.294 to 33.15 seconds to construct the tree and 3.23 seconds (best case) to\n61.41 minutes (worst case) to fill up the warehouse. This method is proved to\nbe an attractive alternative solution for warehouse layout system.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 04:12:54 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Haryanto", "Ardy Wibowo", ""], ["Kusnadi", "Adhi", ""], ["Soelistio", "Yustinus Eko", ""]]}, {"id": "1508.04885", "submitter": "Michelle Blom", "authors": "Michelle Blom, Peter J. Stuckey, Vanessa J. Teague and Ron Tidhar", "title": "Efficient Computation of Exact IRV Margins", "comments": "20 pages, 6 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The margin of victory is easy to compute for many election schemes but\ndifficult for Instant Runoff Voting (IRV). This is important because arguments\nabout the correctness of an election outcome usually rely on the size of the\nelectoral margin. For example, risk-limiting audits require a knowledge of the\nmargin of victory in order to determine how much auditing is necessary. This\npaper presents a practical branch-and-bound algorithm for exact IRV margin\ncomputation that substantially improves on the current best-known approach.\nAlthough exponential in the worst case, our algorithm runs efficiently in\npractice on all the real examples we could find. We can efficiently discover\nexact margins on election instances that cannot be solved by the current\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 05:56:53 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Blom", "Michelle", ""], ["Stuckey", "Peter J.", ""], ["Teague", "Vanessa J.", ""], ["Tidhar", "Ron", ""]]}, {"id": "1508.04928", "submitter": "Hiromi Narimatsu", "authors": "Hiromi Narimatsu and Hiroyuki Kasai", "title": "Duration and Interval Hidden Markov Model for Sequential Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of sequential event data has been recognized as one of the essential\ntools in data modeling and analysis field. In this paper, after the examination\nof its technical requirements and issues to model complex but practical\nsituation, we propose a new sequential data model, dubbed Duration and Interval\nHidden Markov Model (DI-HMM), that efficiently represents \"state duration\" and\n\"state interval\" of data events. This has significant implications to play an\nimportant role in representing practical time-series sequential data. This\neventually provides an efficient and flexible sequential data retrieval.\nNumerical experiments on synthetic and real data demonstrate the efficiency and\naccuracy of the proposed DI-HMM.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 09:09:45 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Narimatsu", "Hiromi", ""], ["Kasai", "Hiroyuki", ""]]}, {"id": "1508.05013", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh", "title": "Message Passing and Combinatorial Optimization", "comments": "Ravanbakhsh, S. (2015), Message Passing and Combinatorial\n  Optimization, PhD thesis, University of Alberta", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DS math.AC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models use the intuitive and well-studied methods of graph theory\nto implicitly represent dependencies between variables in large systems. They\ncan model the global behaviour of a complex system by specifying only local\nfactors. This thesis studies inference in discrete graphical models from an\nalgebraic perspective and the ways inference can be used to express and\napproximate NP-hard combinatorial problems.\n  We investigate the complexity and reducibility of various inference problems,\nin part by organizing them in an inference hierarchy. We then investigate\ntractable approximations for a subset of these problems using distributive law\nin the form of message passing. The quality of the resulting message passing\nprocedure, called Belief Propagation (BP), depends on the influence of loops in\nthe graphical model. We contribute to three classes of approximations that\nimprove BP for loopy graphs A) loop correction techniques; B) survey\npropagation, another message passing technique that surpasses BP in some\nsettings; and C) hybrid methods that interpolate between deterministic message\npassing and Markov Chain Monte Carlo inference.\n  We then review the existing message passing solutions and provide novel\ngraphical models and inference techniques for combinatorial problems under\nthree broad classes: A) constraint satisfaction problems such as\nsatisfiability, coloring, packing, set / clique-cover and dominating /\nindependent set and their optimization counterparts; B) clustering problems\nsuch as hierarchical clustering, K-median, K-clustering, K-center and\nmodularity optimization; C) problems over permutations including assignment,\ngraph morphisms and alignment, finding symmetries and traveling salesman\nproblem. In many cases we show that message passing is able to find solutions\nthat are either near optimal or favourably compare with today's\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 15:50:45 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Ravanbakhsh", "Siamak", ""]]}, {"id": "1508.05117", "submitter": "Federico Ricci-Tersenghi", "authors": "Raffaele Marino, Giorgio Parisi and Federico Ricci-Tersenghi", "title": "The backtracking survey propagation algorithm for solving random K-SAT\n  problems", "comments": "11 pages, 10 figures. v2: data largely improved and manuscript\n  rewritten", "journal-ref": "Nature Communications 7, 12996 (2016)", "doi": "10.1038/ncomms12996", "report-no": null, "categories": "cs.CC cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete combinatorial optimization has a central role in many scientific\ndisciplines, however, for hard problems we lack linear time algorithms that\nwould allow us to solve very large instances. Moreover, it is still unclear\nwhat are the key features that make a discrete combinatorial optimization\nproblem hard to solve. Here we study random K-satisfiability problems with\n$K=3,4$, which are known to be very hard close to the SAT-UNSAT threshold,\nwhere problems stop having solutions. We show that the backtracking survey\npropagation algorithm, in a time practically linear in the problem size, is\nable to find solutions very close to the threshold, in a region unreachable by\nany other algorithm. All solutions found have no frozen variables, thus\nsupporting the conjecture that only unfrozen solutions can be found in linear\ntime, and that a problem becomes impossible to solve in linear time when all\nsolutions contain frozen variables.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 20:41:29 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 17:00:45 GMT"}, {"version": "v3", "created": "Tue, 19 Apr 2016 22:26:58 GMT"}, {"version": "v4", "created": "Thu, 6 Oct 2016 07:37:19 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Marino", "Raffaele", ""], ["Parisi", "Giorgio", ""], ["Ricci-Tersenghi", "Federico", ""]]}, {"id": "1508.05128", "submitter": "Ondrej Kuzelka", "authors": "Gustav Sourek, Vojtech Aschenbrenner, Filip Zelezny, Ondrej Kuzelka", "title": "Lifted Relational Neural Networks", "comments": "Expanded section on weight learning, added explanation of\n  relationship to convolutional neural networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method combining relational-logic representations with neural\nnetwork learning. A general lifted architecture, possibly reflecting some\nbackground domain knowledge, is described through relational rules which may be\nhandcrafted or learned. The relational rule-set serves as a template for\nunfolding possibly deep neural networks whose structures also reflect the\nstructures of given training or testing relational examples. Different networks\ncorresponding to different examples share their weights, which co-evolve during\ntraining by stochastic gradient descent algorithm. The framework allows for\nhierarchical relational modeling constructs and learning of latent relational\nconcepts through shared hidden layers weights corresponding to the rules.\nDiscovery of notable relational concepts and experiments on 78 relational\nlearning benchmarks demonstrate favorable performance of the method.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 21:18:25 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 12:55:45 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Sourek", "Gustav", ""], ["Aschenbrenner", "Vojtech", ""], ["Zelezny", "Filip", ""], ["Kuzelka", "Ondrej", ""]]}, {"id": "1508.05328", "submitter": "Chunlin Chen", "authors": "Luowei Zhou, Pei Yang, Chunlin Chen, Yang Gao", "title": "Multi-agent Reinforcement Learning with Sparse Interactions by\n  Negotiation and Knowledge Transfer", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": "10.1109/TCYB.2016.2543238", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning has significant applications for multi-agent systems,\nespecially in unknown dynamic environments. However, most multi-agent\nreinforcement learning (MARL) algorithms suffer from such problems as\nexponential computation complexity in the joint state-action space, which makes\nit difficult to scale up to realistic multi-agent problems. In this paper, a\nnovel algorithm named negotiation-based MARL with sparse interactions (NegoSI)\nis presented. In contrast to traditional sparse-interaction based MARL\nalgorithms, NegoSI adopts the equilibrium concept and makes it possible for\nagents to select the non-strict Equilibrium Dominating Strategy Profile\n(non-strict EDSP) or Meta equilibrium for their joint actions. The presented\nNegoSI algorithm consists of four parts: the equilibrium-based framework for\nsparse interactions, the negotiation for the equilibrium set, the minimum\nvariance method for selecting one joint action and the knowledge transfer of\nlocal Q-values. In this integrated algorithm, three techniques, i.e., unshared\nvalue functions, equilibrium solutions and sparse interactions are adopted to\nachieve privacy protection, better coordination and lower computational\ncomplexity, respectively. To evaluate the performance of the presented NegoSI\nalgorithm, two groups of experiments are carried out regarding three criteria:\nsteps of each episode (SEE), rewards of each episode (REE) and average runtime\n(AR). The first group of experiments is conducted using six grid world games\nand shows fast convergence and high scalability of the presented algorithm.\nThen in the second group of experiments NegoSI is applied to an intelligent\nwarehouse problem and simulated results demonstrate the effectiveness of the\npresented NegoSI algorithm compared with other state-of-the-art MARL\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 16:30:25 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 15:44:26 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Zhou", "Luowei", ""], ["Yang", "Pei", ""], ["Chen", "Chunlin", ""], ["Gao", "Yang", ""]]}, {"id": "1508.05508", "submitter": "Baolin Peng", "authors": "Baolin Peng, Zhengdong Lu, Hang Li and Kam-Fai Wong", "title": "Towards Neural Network-based Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Neural Reasoner, a framework for neural network-based reasoning\nover natural language sentences. Given a question, Neural Reasoner can infer\nover multiple supporting facts and find an answer to the question in specific\nforms. Neural Reasoner has 1) a specific interaction-pooling mechanism,\nallowing it to examine multiple facts, and 2) a deep architecture, allowing it\nto model the complicated logical relations in reasoning tasks. Assuming no\nparticular structure exists in the question and facts, Neural Reasoner is able\nto accommodate different types of reasoning and different forms of language\nexpressions. Despite the model complexity, Neural Reasoner can still be trained\neffectively in an end-to-end manner. Our empirical studies show that Neural\nReasoner can outperform existing neural reasoning systems with remarkable\nmargins on two difficult artificial tasks (Positional Reasoning and Path\nFinding) proposed in [8]. For example, it improves the accuracy on Path\nFinding(10K) from 33.4% [6] to over 98%.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 13:15:09 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Peng", "Baolin", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "1508.05608", "submitter": "Yahel David", "authors": "Yahel David and Nahum Shimkin", "title": "The Max $K$-Armed Bandit: A PAC Lower Bound and tighter Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Max $K$-Armed Bandit problem, where a learning agent is faced\nwith several sources (arms) of items (rewards), and interested in finding the\nbest item overall. At each time step the agent chooses an arm, and obtains a\nrandom real valued reward. The rewards of each arm are assumed to be i.i.d.,\nwith an unknown probability distribution that generally differs among the arms.\nUnder the PAC framework, we provide lower bounds on the sample complexity of\nany $(\\epsilon,\\delta)$-correct algorithm, and propose algorithms that attain\nthis bound up to logarithmic factors. We compare the performance of this\nmulti-arm algorithms to the variant in which the arms are not distinguishable\nby the agent and are chosen randomly at each stage. Interestingly, when the\nmaximal rewards of the arms happen to be similar, the latter approach may\nprovide better performance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 13:38:15 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["David", "Yahel", ""], ["Shimkin", "Nahum", ""]]}, {"id": "1508.05804", "submitter": "Bernardo Gon\\c{c}alves", "authors": "Bernardo Gon\\c{c}alves, Fabio Porto", "title": "A note on the complexity of the causal ordering problem", "comments": "25 pages, 4 figures", "journal-ref": "Artificial Intelligence 238:154-65, 2016", "doi": "10.1016/j.artint.2016.06.004", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we provide a concise report on the complexity of the causal\nordering problem, originally introduced by Simon to reason about causal\ndependencies implicit in systems of mathematical equations. We show that\nSimon's classical algorithm to infer causal ordering is NP-Hard---an\nintractability previously guessed but never proven. We present then a detailed\naccount based on Nayak's suggested algorithmic solution (the best available),\nwhich is dominated by computing transitive closure---bounded in time by\n$O(|\\mathcal V|\\cdot |\\mathcal S|)$, where $\\mathcal S(\\mathcal E, \\mathcal V)$\nis the input system structure composed of a set $\\mathcal E$ of equations over\na set $\\mathcal V$ of variables with number of variable appearances (density)\n$|\\mathcal S|$. We also comment on the potential of causal ordering for\nemerging applications in large-scale hypothesis management and analytics.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 13:56:32 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 02:54:54 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Gon\u00e7alves", "Bernardo", ""], ["Porto", "Fabio", ""]]}, {"id": "1508.06013", "submitter": "Leopoldo Bertossi", "authors": "Zeinab Bahmani, Leopoldo Bertossi and Nikolaos Vasiloglou", "title": "ERBlox: Combining Matching Dependencies with Machine Learning for Entity\n  Resolution", "comments": "To appear in Proc. SUM, 2015", "journal-ref": "Proc. SUM'15, 2015, Springer LNAI 9310, pp. 399-414", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER), an important and common data cleaning problem, is\nabout detecting data duplicate representations for the same external entities,\nand merging them into single representations. Relatively recently, declarative\nrules called matching dependencies (MDs) have been proposed for specifying\nsimilarity conditions under which attribute values in database records are\nmerged. In this work we show the process and the benefits of integrating three\ncomponents of ER: (a) Classifiers for duplicate/non-duplicate record pairs\nbuilt using machine learning (ML) techniques, (b) MDs for supporting both the\nblocking phase of ML and the merge itself; and (c) The use of the declarative\nlanguage LogiQL -an extended form of Datalog supported by the LogicBlox\nplatform- for data processing, and the specification and enforcement of MDs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 02:35:58 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Bahmani", "Zeinab", ""], ["Bertossi", "Leopoldo", ""], ["Vasiloglou", "Nikolaos", ""]]}, {"id": "1508.06096", "submitter": "Nicholas Downing", "authors": "Nicholas Downing, Thibaut Feydy, Peter J. Stuckey", "title": "Unsatisfiable Cores and Lower Bounding for Constraint Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint Programming (CP) solvers typically tackle optimization problems by\nrepeatedly finding solutions to a problem while placing tighter and tighter\nbounds on the solution cost. This approach is somewhat naive, especially for\nsoft-constraint optimization problems in which the soft constraints are mostly\nsatisfied. Unsatisfiable-core approaches to solving soft constraint problems in\nSAT (e.g. MAXSAT) force all soft constraints to be hard initially. When solving\nfails they return an unsatisfiable core, as a set of soft constraints that\ncannot hold simultaneously. These are reverted to soft and solving continues.\nSince lazy clause generation solvers can also return unsatisfiable cores we can\nadapt this approach to constraint programming. We adapt the original MAXSAT\nunsatisfiable core solving approach to be usable for constraint programming and\ndefine a number of extensions. Experimental results show that our methods are\nbeneficial on a broad class of CP-optimization benchmarks involving soft\nconstraints, cardinality or preferences.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 10:11:42 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Downing", "Nicholas", ""], ["Feydy", "Thibaut", ""], ["Stuckey", "Peter J.", ""]]}, {"id": "1508.06161", "submitter": "Scott Bronikowski", "authors": "Daniel Paul Barrett, Scott Alan Bronikowski, Haonan Yu, and Jeffrey\n  Mark Siskind", "title": "Robot Language Learning, Generation, and Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework which supports grounding natural-language\nsemantics in robotic driving. This framework supports acquisition (learning\ngrounded meanings of nouns and prepositions from human annotation of robotic\ndriving paths), generation (using such acquired meanings to generate sentential\ndescription of new robotic driving paths), and comprehension (using such\nacquired meanings to support automated driving to accomplish navigational goals\nspecified in natural language). We evaluate the performance of these three\ntasks by having independent human judges rate the semantic fidelity of the\nsentences associated with paths, achieving overall average correctness of 94.6%\nand overall average completeness of 85.6%.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 14:10:21 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Barrett", "Daniel Paul", ""], ["Bronikowski", "Scott Alan", ""], ["Yu", "Haonan", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1508.06191", "submitter": "Luiz Capretz Dr.", "authors": "Justin Wong, Danny Ho, Luiz Fernando Capretz", "title": "A Neuro-Fuzzy Method to Improving Backfiring Conversion Ratios", "comments": "International Conference on Soft Computing, Intelligent System and\n  Information Technology, Bali, Indonesia, pp. 12-17, 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software project estimation is crucial aspect in delivering software on time\nand on budget. Software size is an important metric in determining the effort,\ncost, and productivity. Today, source lines of code and function point are the\nmost used sizing metrics. Backfiring is a well-known technique for converting\nbetween function points and source lines of code. However when backfiring is\nused, there is a high margin of error. This study introduces a method to\nimprove the accuracy of backfiring. Intelligent systems have been used in\nsoftware prediction models to improve performance over traditional techniques.\nFor this reason, a hybrid Neuro-Fuzzy is used because it takes advantages of\nthe neural networks learning and fuzzy logic human-like reasoning. This paper\ndescribes an improved backfiring technique which uses Neuro-Fuzzy and compares\nthe new method against the default conversion ratios currently used by software\npractitioners.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 15:40:44 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Wong", "Justin", ""], ["Ho", "Danny", ""], ["Capretz", "Luiz Fernando", ""]]}, {"id": "1508.06235", "submitter": "Daniel Khashabi Mr.", "authors": "Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang", "title": "Clustering With Side Information: From a Probabilistic Model to a\n  Deterministic Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a model-based clustering method (TVClust) that\nrobustly incorporates noisy side information as soft-constraints and aims to\nseek a consensus between side information and the observed data. Our method is\nbased on a nonparametric Bayesian hierarchical model that combines the\nprobabilistic model for the data instance and the one for the side-information.\nAn efficient Gibbs sampling algorithm is proposed for posterior inference.\nUsing the small-variance asymptotics of our probabilistic model, we then derive\na new deterministic clustering algorithm (RDP-means). It can be viewed as an\nextension of K-means that allows for the inclusion of side information and has\nthe additional property that the number of clusters does not need to be\nspecified a priori. Empirical studies have been carried out to compare our work\nwith many constrained clustering algorithms from the literature on both a\nvariety of data sets and under a variety of conditions such as using noisy side\ninformation and erroneous k values. The results of our experiments show strong\nresults for our probabilistic and deterministic approaches under these\nconditions when compared to other algorithms in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 18:13:27 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 17:46:36 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 17:48:54 GMT"}, {"version": "v4", "created": "Sat, 31 Oct 2015 05:38:15 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Khashabi", "Daniel", ""], ["Wieting", "John", ""], ["Liu", "Jeffrey Yufei", ""], ["Liang", "Feng", ""]]}, {"id": "1508.06538", "submitter": "Hector Zenil", "authors": "Hector Zenil, Angelika Schmidt and Jesper Tegn\\'er", "title": "Causality, Information and Biological Computation: An algorithmic\n  software approach to life, disease and the immune system", "comments": "30 pages, 8 figures. Invited chapter contribution to Information and\n  Causality: From Matter to Life. Sara I. Walker, Paul C.W. Davies and George\n  Ellis (eds.), Cambridge University Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biology has taken strong steps towards becoming a computer science aiming at\nreprogramming nature after the realisation that nature herself has reprogrammed\norganisms by harnessing the power of natural selection and the digital\nprescriptive nature of replicating DNA. Here we further unpack ideas related to\ncomputability, algorithmic information theory and software engineering, in the\ncontext of the extent to which biology can be (re)programmed, and with how we\nmay go about doing so in a more systematic way with all the tools and concepts\noffered by theoretical computer science in a translation exercise from\ncomputing to molecular biology and back. These concepts provide a means to a\nhierarchical organization thereby blurring previously clear-cut lines between\nconcepts like matter and life, or between tumour types that are otherwise taken\nas different and may not have however a different cause. This does not diminish\nthe properties of life or make its components and functions less interesting.\nOn the contrary, this approach makes for a more encompassing and integrated\nview of nature, one that subsumes observer and observed within the same system,\nand can generate new perspectives and tools with which to view complex diseases\nlike cancer, approaching them afresh from a software-engineering viewpoint that\ncasts evolution in the role of programmer, cells as computing machines, DNA and\ngenes as instructions and computer programs, viruses as hacking devices, the\nimmune system as a software debugging tool, and diseases as an\ninformation-theoretic battlefield where all these forces deploy. We show how\ninformation theory and algorithmic programming may explain fundamental\nmechanisms of life and death.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 19:38:03 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 10:06:12 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2015 01:03:18 GMT"}, {"version": "v4", "created": "Fri, 15 Jan 2016 00:55:07 GMT"}, {"version": "v5", "created": "Wed, 20 Jan 2016 01:54:15 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Zenil", "Hector", ""], ["Schmidt", "Angelika", ""], ["Tegn\u00e9r", "Jesper", ""]]}, {"id": "1508.06781", "submitter": "Shreyas Sekar", "authors": "Elliot Anshelevich and Shreyas Sekar", "title": "Computing Stable Coalitions: Approximation Algorithms for Reward Sharing", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a setting where selfish agents are to be assigned to coalitions or\nprojects from a fixed set P. Each project k is characterized by a valuation\nfunction; v_k(S) is the value generated by a set S of agents working on project\nk. We study the following classic problem in this setting: \"how should the\nagents divide the value that they collectively create?\". One traditional\napproach in cooperative game theory is to study core stability with the\nimplicit assumption that there are infinite copies of one project, and agents\ncan partition themselves into any number of coalitions. In contrast, we\nconsider a model with a finite number of non-identical projects; this makes\ncomputing both high-welfare solutions and core payments highly non-trivial.\n  The main contribution of this paper is a black-box mechanism that reduces the\nproblem of computing a near-optimal core stable solution to the purely\nalgorithmic problem of welfare maximization; we apply this to compute an\napproximately core stable solution that extracts one-fourth of the optimal\nsocial welfare for the class of subadditive valuations. We also show much\nstronger results for several popular sub-classes: anonymous, fractionally\nsubadditive, and submodular valuations, as well as provide new approximation\nalgorithms for welfare maximization with anonymous functions. Finally, we\nestablish a connection between our setting and the well-studied simultaneous\nauctions with item bidding; we adapt our results to compute approximate pure\nNash equilibria for these auctions.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 09:53:40 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Anshelevich", "Elliot", ""], ["Sekar", "Shreyas", ""]]}, {"id": "1508.06924", "submitter": "Erik Mueller", "authors": "Erik T. Mueller and Henry Minsky", "title": "Using Thought-Provoking Children's Questions to Drive Artificial\n  Intelligence Research", "comments": "update for EGPAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use thought-provoking children's questions (TPCQs), namely\nHighlights BrainPlay questions, as a new method to drive artificial\nintelligence research and to evaluate the capabilities of general-purpose AI\nsystems. These questions are designed to stimulate thought and learning in\nchildren, and they can be used to do the same thing in AI systems, while\ndemonstrating the system's reasoning capabilities to the evaluator. We\nintroduce the TPCQ task, which which takes a TPCQ question as input and\nproduces as output (1) answers to the question and (2) learned generalizations.\nWe discuss how BrainPlay questions stimulate learning. We analyze 244 BrainPlay\nquestions, and we report statistics on question type, question class, answer\ncardinality, answer class, types of knowledge needed, and types of reasoning\nneeded. We find that BrainPlay questions span many aspects of intelligence.\nBecause the answers to BrainPlay questions and the generalizations learned from\nthem are often highly open-ended, we suggest using human judges for evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 16:23:49 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 13:01:00 GMT"}, {"version": "v3", "created": "Wed, 26 Jul 2017 00:34:24 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Mueller", "Erik T.", ""], ["Minsky", "Henry", ""]]}, {"id": "1508.06973", "submitter": "Catarina Moreira", "authors": "Catarina Moreira and Andreas Wichert", "title": "The Relation Between Acausality and Interference in Quantum-Like\n  Bayesian Networks", "comments": "In proceedings of the 9th International Conference on Quantum\n  Interactions, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse a quantum-like Bayesian Network that puts together cause/effect\nrelationships and semantic similarities between events. These semantic\nsimilarities constitute acausal connections according to the Synchronicity\nprinciple and provide new relationships to quantum like probabilistic graphical\nmodels. As a consequence, beliefs (or any other event) can be represented in\nvector spaces, in which quantum parameters are determined by the similarities\nthat these vectors share between them. Events attached by a semantic meaning do\nnot need to have an explanation in terms of cause and effect.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 17:37:01 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Moreira", "Catarina", ""], ["Wichert", "Andreas", ""]]}, {"id": "1508.06976", "submitter": "Byung Suk Lee", "authors": "Saurav Acharya, Byung Suk Lee and Paul Hines", "title": "Real-time Top-K Predictive Query Processing over Event Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of predicting the k events that are most\nlikely to occur next, over historical real-time event streams. Existing\napproaches to causal prediction queries have a number of limitations. First,\nthey exhaustively search over an acyclic causal network to find the most likely\nk effect events; however, data from real event streams frequently reflect\ncyclic causality. Second, they contain conservative assumptions intended to\nexclude all possible non-causal links in the causal network; it leads to the\nomission of many less-frequent but important causal links. We overcome these\nlimitations by proposing a novel event precedence model and a run-time causal\ninference mechanism. The event precedence model constructs a first order\nabsorbing Markov chain incrementally over event streams, where an edge between\ntwo events signifies a temporal precedence relationship between them, which is\na necessary condition for causality. Then, the run-time causal inference\nmechanism learns causal relationships dynamically during query processing. This\nis done by removing some of the temporal precedence relationships that do not\nexhibit causality in the presence of other events in the event precedence\nmodel. This paper presents two query processing algorithms -- one performs\nexhaustive search on the model and the other performs a more efficient reduced\nsearch with early termination. Experiments using two real datasets (cascading\nblackouts in power systems and web page views) verify the effectiveness of the\nprobabilistic top-k prediction queries and the efficiency of the algorithms.\nSpecifically, the reduced search algorithm reduced runtime, relative to\nexhaustive search, by 25-80% (depending on the application) with only a small\nreduction in accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 15:02:09 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Acharya", "Saurav", ""], ["Lee", "Byung Suk", ""], ["Hines", "Paul", ""]]}, {"id": "1508.07092", "submitter": "Saisai Ma", "authors": "Saisai Ma, Jiuyong Li, Lin Liu, Thuc Duy Le", "title": "Mining Combined Causes in Large Data Sets", "comments": "This paper has been accepted by Knowledge-Based Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, many methods have been developed for detecting causal\nrelationships in observational data. Some of them have the potential to tackle\nlarge data sets. However, these methods fail to discover a combined cause, i.e.\na multi-factor cause consisting of two or more component variables which\nindividually are not causes. A straightforward approach to uncovering a\ncombined cause is to include both individual and combined variables in the\ncausal discovery using existing methods, but this scheme is computationally\ninfeasible due to the huge number of combined variables. In this paper, we\npropose a novel approach to address this practical causal discovery problem,\ni.e. mining combined causes in large data sets. The experiments with both\nsynthetic and real world data sets show that the proposed method can obtain\nhigh-quality causal discoveries with a high computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 04:42:23 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 05:17:19 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Ma", "Saisai", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""], ["Le", "Thuc Duy", ""]]}, {"id": "1508.07275", "submitter": "Luiz Capretz Dr.", "authors": "Ali Bou Nassif, Mohammad Azzeh, Luiz Fernando Capretz, Danny Ho", "title": "A Comparison Between Decision Trees and Decision Tree Forest Models for\n  Software Development Effort Estimation", "comments": "3rd International Conference on Communications and Information\n  Technology (ICCIT), Beirut, Lebanon, pp. 220-224, 2013", "journal-ref": null, "doi": "10.1109/ICCITechnology.2013.6579553", "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate software effort estimation has been a challenge for many software\npractitioners and project managers. Underestimation leads to disruption in the\nprojects estimated cost and delivery. On the other hand, overestimation causes\noutbidding and financial losses in business. Many software estimation models\nexist; however, none have been proven to be the best in all situations. In this\npaper, a decision tree forest (DTF) model is compared to a traditional decision\ntree (DT) model, as well as a multiple linear regression model (MLR). The\nevaluation was conducted using ISBSG and Desharnais industrial datasets.\nResults show that the DTF model is competitive and can be used as an\nalternative in software effort prediction.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 16:52:21 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Nassif", "Ali Bou", ""], ["Azzeh", "Mohammad", ""], ["Capretz", "Luiz Fernando", ""], ["Ho", "Danny", ""]]}, {"id": "1508.07678", "submitter": "Shahriar Shariat", "authors": "Shahriar Shariat, Burkay Orten, Ali Dasdan", "title": "Online Model Evaluation in a Large-Scale Computational Advertising\n  Platform", "comments": "Accepted to ICDM2015", "journal-ref": "ICDM (2015) pp. 369 - 378", "doi": "10.1109/ICDM.2015.32", "report-no": null, "categories": "cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online media provides opportunities for marketers through which they can\ndeliver effective brand messages to a wide range of audiences. Advertising\ntechnology platforms enable advertisers to reach their target audience by\ndelivering ad impressions to online users in real time. In order to identify\nthe best marketing message for a user and to purchase impressions at the right\nprice, we rely heavily on bid prediction and optimization models. Even though\nthe bid prediction models are well studied in the literature, the equally\nimportant subject of model evaluation is usually overlooked. Effective and\nreliable evaluation of an online bidding model is crucial for making faster\nmodel improvements as well as for utilizing the marketing budgets more\nefficiently. In this paper, we present an experimentation framework for bid\nprediction models where our focus is on the practical aspects of model\nevaluation. Specifically, we outline the unique challenges we encounter in our\nplatform due to a variety of factors such as heterogeneous goal definitions,\nvarying budget requirements across different campaigns, high seasonality and\nthe auction-based environment for inventory purchasing. Then, we introduce\nreturn on investment (ROI) as a unified model performance (i.e., success)\nmetric and explain its merits over more traditional metrics such as\nclick-through rate (CTR) or conversion rate (CVR). Most importantly, we discuss\ncommonly used evaluation and metric summarization approaches in detail and\npropose a more accurate method for online evaluation of new experimental models\nagainst the baseline. Our meta-analysis-based approach addresses various\nshortcomings of other methods and yields statistically robust conclusions that\nallow us to conclude experiments more quickly in a reliable manner. We\ndemonstrate the effectiveness of our evaluation strategy on real campaign data\nthrough some experiments.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 04:11:50 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Shariat", "Shahriar", ""], ["Orten", "Burkay", ""], ["Dasdan", "Ali", ""]]}, {"id": "1508.07680", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and W. Bastiaan Kleijn and Mengjie Zhang and David\n  Balduzzi", "title": "Domain Generalization for Object Recognition with Multi-task\n  Autoencoders", "comments": "accepted in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of domain generalization is to take knowledge acquired from a\nnumber of related domains where training data is available, and to then\nsuccessfully apply it to previously unseen domains. We propose a new feature\nlearning algorithm, Multi-Task Autoencoder (MTAE), that provides good\ngeneralization performance for cross-domain object recognition.\n  Our algorithm extends the standard denoising autoencoder framework by\nsubstituting artificially induced corruption with naturally occurring\ninter-domain variability in the appearance of objects. Instead of\nreconstructing images from noisy versions, MTAE learns to transform the\noriginal image into analogs in multiple related domains. It thereby learns\nfeatures that are robust to variations across domains. The learnt features are\nthen used as inputs to a classifier.\n  We evaluated the performance of the algorithm on benchmark image recognition\ndatasets, where the task is to learn features from multiple datasets and to\nthen predict the image label from unseen datasets. We found that (denoising)\nMTAE outperforms alternative autoencoder-based models as well as the current\nstate-of-the-art algorithms for domain generalization.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 04:15:31 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""], ["Balduzzi", "David", ""]]}, {"id": "1508.07753", "submitter": "Pekka Parviainen", "authors": "Pekka Parviainen and Samuel Kaski", "title": "Learning Structures of Bayesian Networks for Variable Groups", "comments": "To appear at the International Journal of Approximate Reasoning. A\n  preliminary version appeared in Proceedings of the Eighth International\n  Conference on Probabilistic Graphical Models", "journal-ref": null, "doi": "10.1016/j.ijar.2017.05.006", "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks, and especially their structures, are powerful tools for\nrepresenting conditional independencies and dependencies between random\nvariables. In applications where related variables form a priori known groups,\nchosen to represent different \"views\" to or aspects of the same entities, one\nmay be more interested in modeling dependencies between groups of variables\nrather than between individual variables. Motivated by this, we study prospects\nof representing relationships between variable groups using Bayesian network\nstructures. We show that for dependency structures between groups to be\nexpressible exactly, the data have to satisfy the so-called groupwise\nfaithfulness assumption. We also show that one cannot learn causal relations\nbetween groups using only groupwise conditional independencies, but also\nvariable-wise relations are needed. Additionally, we present algorithms for\nfinding the groupwise dependency structures.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 10:19:41 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 12:59:49 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 11:36:43 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Parviainen", "Pekka", ""], ["Kaski", "Samuel", ""]]}]