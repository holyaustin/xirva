[{"id": "1401.0102", "submitter": "Mahdi Zamani", "authors": "Mahdi Zamani and Mahnush Movahedi and Mohammad Ebadzadeh and Hossein\n  Pedram", "title": "A DDoS-Aware IDS Model Based on Danger Theory and Mobile Agents", "comments": "10 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CR cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an artificial immune model for intrusion detection in distributed\nsystems based on a relatively recent theory in immunology called Danger theory.\nBased on Danger theory, immune response in natural systems is a result of\nsensing corruption as well as sensing unknown substances. In contrast,\ntraditional self-nonself discrimination theory states that immune response is\nonly initiated by sensing nonself (unknown) patterns. Danger theory solves many\nproblems that could only be partially explained by the traditional model.\nAlthough the traditional model is simpler, such problems result in high false\npositive rates in immune-inspired intrusion detection systems. We believe using\ndanger theory in a multi-agent environment that computationally emulates the\nbehavior of natural immune systems is effective in reducing false positive\nrates. We first describe a simplified scenario of immune response in natural\nsystems based on danger theory and then, convert it to a computational model as\na network protocol. In our protocol, we define several immune signals and model\ncell signaling via message passing between agents that emulate cells. Most\nmessages include application-specific patterns that must be meaningfully\nextracted from various system properties. We show how to model these messages\nin practice by performing a case study on the problem of detecting distributed\ndenial-of-service attacks in wireless sensor networks. We conduct a set of\nsystematic experiments to find a set of performance metrics that can accurately\ndistinguish malicious patterns. The results indicate that the system can be\nefficiently used to detect malicious patterns with a high level of accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 06:18:18 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2014 22:07:13 GMT"}, {"version": "v3", "created": "Sun, 28 Dec 2014 05:14:01 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Zamani", "Mahdi", ""], ["Movahedi", "Mahnush", ""], ["Ebadzadeh", "Mohammad", ""], ["Pedram", "Hossein", ""]]}, {"id": "1401.0104", "submitter": "Tao Xiong", "authors": "Yukun Bao, Tao Xiong, Zhongyi Hu", "title": "PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction", "comments": "14 pages. IEEE Transactions on Cybernetics. 2013", "journal-ref": null, "doi": "10.1109/TCYB.2013.2265084", "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-step-ahead time series prediction is one of the most challenging\nresearch topics in the field of time series modeling and prediction, and is\ncontinually under research. Recently, the multiple-input several\nmultiple-outputs (MISMO) modeling strategy has been proposed as a promising\nalternative for multi-step-ahead time series prediction, exhibiting advantages\ncompared with the two currently dominating strategies, the iterated and the\ndirect strategies. Built on the established MISMO strategy, this study proposes\na particle swarm optimization (PSO)-based MISMO modeling strategy, which is\ncapable of determining the number of sub-models in a self-adaptive mode, with\nvarying prediction horizons. Rather than deriving crisp divides with equal-size\ns prediction horizons from the established MISMO, the proposed PSO-MISMO\nstrategy, implemented with neural networks, employs a heuristic to create\nflexible divides with varying sizes of prediction horizons and to generate\ncorresponding sub-models, providing considerable flexibility in model\nconstruction, which has been validated with simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 07:09:02 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Bao", "Yukun", ""], ["Xiong", "Tao", ""], ["Hu", "Zhongyi", ""]]}, {"id": "1401.0166", "submitter": "Alex James Dr", "authors": "A.P. James, B. V. Dasarathy", "title": "Medical Image Fusion: A survey of the state of the art", "comments": "Information Fusion, 2014", "journal-ref": null, "doi": "10.1016/j.inffus.2013.12.002", "report-no": null, "categories": "cs.CV cs.AI physics.med-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Medical image fusion is the process of registering and combining multiple\nimages from single or multiple imaging modalities to improve the imaging\nquality and reduce randomness and redundancy in order to increase the clinical\napplicability of medical images for diagnosis and assessment of medical\nproblems. Multi-modal medical image fusion algorithms and devices have shown\nnotable achievements in improving clinical accuracy of decisions based on\nmedical images. This review article provides a factual listing of methods and\nsummarizes the broad scientific challenges faced in the field of medical image\nfusion. We characterize the medical image fusion research based on (1) the\nwidely used image fusion methods, (2) imaging modalities, and (3) imaging of\norgans that are under study. This review concludes that even though there\nexists several open ended technological and scientific challenges, the fusion\nof medical images has proved to be useful for advancing the clinical\nreliability of using medical imaging for medical diagnostics and analysis, and\nis a scientific discipline that has the potential to significantly grow in the\ncoming years.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 16:03:17 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["James", "A. P.", ""], ["Dasarathy", "B. V.", ""]]}, {"id": "1401.0180", "submitter": "Steve N'Guyen", "authors": "Steve N'Guyen (ISIR, LPPA), Cl\\'ement Moulin-Frier (INRIA Bordeaux -\n  Sud-Ouest, GIPSA-lab), Jacques Droulez (LPPA)", "title": "Decision Making under Uncertainty: A Quasimetric Approach", "comments": null, "journal-ref": "PLoS ONE 8, 12 (2013) e83411", "doi": "10.1371/journal.pone.0083411", "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for solving a class of discrete decision making\nproblems under uncertainty with positive cost. This issue concerns multiple and\ndiverse fields such as engineering, economics, artificial intelligence,\ncognitive science and many others. Basically, an agent has to choose a single\nor series of actions from a set of options, without knowing for sure their\nconsequences. Schematically, two main approaches have been followed: either the\nagent learns which option is the correct one to choose in a given situation by\ntrial and error, or the agent already has some knowledge on the possible\nconsequences of his decisions; this knowledge being generally expressed as a\nconditional probability distribution. In the latter case, several optimal or\nsuboptimal methods have been proposed to exploit this uncertain knowledge in\nvarious contexts. In this work, we propose following a different approach,\nbased on the geometric intuition of distance. More precisely, we define a goal\nindependent quasimetric structure on the state space, taking into account both\ncost function and transition probability. We then compare precision and\ncomputation time with classical approaches.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 16:56:04 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["N'Guyen", "Steve", "", "ISIR, LPPA"], ["Moulin-Frier", "Cl\u00e9ment", "", "INRIA Bordeaux -\n  Sud-Ouest, GIPSA-lab"], ["Droulez", "Jacques", "", "LPPA"]]}, {"id": "1401.0245", "submitter": "Sujit Gath", "authors": "S.J Gath and R.V Kulkarni", "title": "A Review: Expert System for Diagnosis of Myocardial Infarction", "comments": "7 pages. arXiv admin note: text overlap with arXiv:1006.4544 by other\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computer Program Capable of performing at a human-expert level in a narrow\nproblem domain area is called an expert system. Management of uncertainty is an\nintrinsically important issue in the design of expert systems because much of\nthe information in the knowledge base of a typical expert system is imprecise,\nincomplete or not totally reliable. In this paper, the author present s the\nreview of past work that has been carried out by various researchers based on\ndevelopment of expert systems for the diagnosis of cardiac disease\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2014 03:59:22 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Gath", "S. J", ""], ["Kulkarni", "R. V", ""]]}, {"id": "1401.0282", "submitter": "Reza Nourjou", "authors": "Reza Nourjou, Michinori Hatayama, Stephen F. Smith, Atabak Sadeghi and\n  Pedro Szekely", "title": "Design of a GIS-based Assistant Software Agent for the Incident\n  Commander to Coordinate Emergency Response Operations", "comments": "3 pages, 1 figure, In Workshop on Robots and Sensors integration in\n  future rescue INformation system (ROSIN' 13). In Conjunction of the IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS' 13), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problem: This paper addresses the design of an intelligent software system\nfor the IC (incident commander) of a team in order to coordinate actions of\nagents (field units or robots) in the domain of emergency/crisis response\noperations. Objective: This paper proposes GICoordinator. It is a GIS-based\nassistant software agent that assists and collaborates with the human planner\nin strategic planning and macro tasks assignment for centralized multi-agent\ncoordination. Method: Our approach to design GICoordinator was to: analyze the\nproblem, design a complete data model, design an architecture of GICoordinator,\nspecify required capabilities of human and system in coordination problem\nsolving, specify development tools, and deploy. Result: The result was an\narchitecture/design of GICoordinator that contains system requirements.\nFindings: GICoordinator efficiently integrates geoinformatics with artifice\nintelligent techniques in order to provide a spatial intelligent coordinator\nsystem for an IC to efficiently coordinate and control agents by making\nmacro/strategic decisions. Results define a framework for future works to\ndevelop this system.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2014 13:35:06 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Nourjou", "Reza", ""], ["Hatayama", "Michinori", ""], ["Smith", "Stephen F.", ""], ["Sadeghi", "Atabak", ""], ["Szekely", "Pedro", ""]]}, {"id": "1401.0708", "submitter": "Taraka Rama Kasicheyanula", "authors": "Taraka Rama, Sudheer Kolachina, Lakshmi Bai B", "title": "Quantitative methods for Phylogenetic Inference in Historical\n  Linguistics: An experimental case study of South Central Dravidian", "comments": null, "journal-ref": "Indian Linguistics, Volume 70, 2009", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we examine the usefulness of two classes of algorithms Distance\nMethods, Discrete Character Methods (Felsenstein and Felsenstein 2003) widely\nused in genetics, for predicting the family relationships among a set of\nrelated languages and therefore, diachronic language change. Applying these\nalgorithms to the data on the numbers of shared cognates- with-change and\nchanged as well as unchanged cognates for a group of six languages belonging to\na Dravidian language sub-family given in Krishnamurti et al. (1983), we\nobserved that the resultant phylogenetic trees are largely in agreement with\nthe linguistic family tree constructed using the comparative method of\nreconstruction with only a few minor differences. Furthermore, we studied these\nminor differences and found that they were cases of genuine ambiguity even for\na well-trained historical linguist. We evaluated the trees obtained through our\nexperiments using a well-defined criterion and report the results here. We\nfinally conclude that quantitative methods like the ones we examined are quite\nuseful in predicting family relationships among languages. In addition, we\nconclude that a modest degree of confidence attached to the intuition that\nthere could indeed exist a parallelism between the processes of linguistic and\ngenetic change is not totally misplaced.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 20:17:47 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Rama", "Taraka", ""], ["Kolachina", "Sudheer", ""], ["B", "Lakshmi Bai", ""]]}, {"id": "1401.0742", "submitter": "Ishanu Chattopadhyay", "authors": "Ishanu Chattopadhyay and Hod Lipson", "title": "Data Smashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CE cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of the underlying physics or biology from empirical data\nrequires a quantifiable notion of similarity - when do two observed data sets\nindicate nearly identical generating processes, and when they do not. The\ndiscriminating characteristics to look for in data is often determined by\nheuristics designed by experts, $e.g.$, distinct shapes of \"folded\" lightcurves\nmay be used as \"features\" to classify variable stars, while determination of\npathological brain states might require a Fourier analysis of brainwave\nactivity. Finding good features is non-trivial. Here, we propose a universal\nsolution to this problem: we delineate a principle for quantifying similarity\nbetween sources of arbitrary data streams, without a priori knowledge, features\nor training. We uncover an algebraic structure on a space of symbolic models\nfor quantized data, and show that such stochastic generators may be added and\nuniquely inverted; and that a model and its inverse always sum to the generator\nof flat white noise. Therefore, every data stream has an anti-stream: data\ngenerated by the inverse model. Similarity between two streams, then, is the\ndegree to which one, when summed to the other's anti-stream, mutually\nannihilates all statistical structure to noise. We call this data smashing. We\npresent diverse applications, including disambiguation of brainwaves pertaining\nto epileptic seizures, detection of anomalous cardiac rhythms, and\nclassification of astronomical objects from raw photometry. In our examples,\nthe data smashing principle, without access to any domain knowledge, meets or\nexceeds the performance of specialized algorithms tuned by domain experts.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 22:15:17 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Chattopadhyay", "Ishanu", ""], ["Lipson", "Hod", ""]]}, {"id": "1401.0802", "submitter": "Michael  Gr. Voskoglou Prof. Dr.", "authors": "Michael Gr. Voskoglou", "title": "A stochastic model for Case-Based Reasoning", "comments": "7 pages, 2 figures", "journal-ref": "Journal of Mathematical Modelling and Application, 1(3), 33-39,\n  2010", "doi": null, "report-no": null, "categories": "cs.AI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Case-Bsed Reasoning (CBR) is a recent theory for problem-solving and learning\nin computers and people.Broadly construed it is the process of solving new\nproblems based on the solution of similar past problems. In the present paper\nwe introduce an absorbing Markov chain on the main steps of the CBR process.In\nthis way we succeed in obtaining the probabilities for the above process to be\nin a certain step at a certain phase of the solution of the corresponding\nproblem, and a measure for the efficiency of a CBR system. Examples are given\nto illustrate our results.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2014 12:00:59 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Voskoglou", "Michael Gr.", ""]]}, {"id": "1401.0943", "submitter": "Adeyinka Akanbi MR", "authors": "Adeyinka K Akanbi", "title": "LB2CO: A Semantic Ontology Framework for B2C eCommerce Transaction on\n  the Internet", "comments": "9 Pages, 7 figures, Research Paper", "journal-ref": "International Journal of Research in Computer Science, 4 (1): pp.\n  1-9, January 2014", "doi": "10.7815/ijorcs.41.2014.075", "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business ontology can enhance the successful development of complex\nenterprise system; this is being achieved through knowledge sharing and the\nease of communication between every entity in the domain. Through human\nsemantic interaction with the web resources, machines to interpret the data\npublished in a machine interpretable form under web. However, the theoretical\npractice of business ontology in eCommerce domain is quite a few especially in\nthe section of electronic transaction, and the various techniques used to\nobtain efficient communication across spheres are error prone and are not\nalways guaranteed to be efficient in obtaining desired result due to poor\nsemantic integration between entities. To overcome the poor semantic\nintegration this research focuses on proposed ontology called LB2CO, which\ncombines the framework of IDEF5 & SNAP as an analysis tool, for automated\nrecommendation of product and services and create effective ontological\nframework for B2C transaction & communication across different business domains\nthat facilitates the interoperability & integration of B2C transactions over\nthe web.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 21:31:26 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Akanbi", "Adeyinka K", ""]]}, {"id": "1401.1024", "submitter": "Marius Lindauer", "authors": "Holger Hoos and Roland Kaminski and Marius Lindauer and Torsten Schaub", "title": "Solver Scheduling via Answer Set Programming", "comments": "To appear in Theory and Practice of Logic Programming (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Boolean Constraint Technology has made tremendous progress over the\nlast decade, the efficacy of state-of-the-art solvers is known to vary\nconsiderably across different types of problem instances and is known to depend\nstrongly on algorithm parameters. This problem was addressed by means of a\nsimple, yet effective approach using handmade, uniform and unordered schedules\nof multiple solvers in ppfolio, which showed very impressive performance in the\n2011 SAT Competition. Inspired by this, we take advantage of the modeling and\nsolving capacities of Answer Set Programming (ASP) to automatically determine\nmore refined, that is, non-uniform and ordered solver schedules from existing\nbenchmarking data. We begin by formulating the determination of such schedules\nas multi-criteria optimization problems and provide corresponding ASP\nencodings. The resulting encodings are easily customizable for different\nsettings and the computation of optimum schedules can mostly be done in the\nblink of an eye, even when dealing with large runtime data sets stemming from\nmany solvers on hundreds to thousands of instances. Also, the fact that our\napproach can be customized easily enabled us to swiftly adapt it to generate\nparallel schedules for multi-processor machines.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 09:58:02 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Hoos", "Holger", ""], ["Kaminski", "Roland", ""], ["Lindauer", "Marius", ""], ["Schaub", "Torsten", ""]]}, {"id": "1401.1031", "submitter": "Noreen Jamil", "authors": "Noreen Jamil", "title": "Constraint Solvers for User Interface Layout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraints have played an important role in the construction of GUIs, where\nthey are mainly used to define the layout of the widgets. Resizing behavior is\nvery important in GUIs because areas have domain specific parameters such as\nform the resizing of windows. If linear objective function is used and window\nis resized then error is not distributed equally. To distribute the error\nequally, a quadratic objective function is introduced. Different algorithms are\nwidely used for solving linear constraints and quadratic problems in a variety\nof different scientific areas. The linear relxation, Kaczmarz, direct and\nlinear programming methods are common methods for solving linear constraints\nfor GUI layout. The interior point and active set methods are most commonly\nused techniques to solve quadratic programming problems. Current constraint\nsolvers designed for GUI layout do not use interior point methods for solving a\nquadratic objective function subject to linear equality and inequality\nconstraints. In this paper, performance aspects and the convergence speed of\ninterior point and active set methods are compared along with one most commonly\nused linear programming method when they are implemented for graphical user\ninterface layout. The performance and convergence of the proposed algorithms\nare evaluated empirically using randomly generated UI layout specifications of\nvarious sizes. The results show that the interior point algorithms perform\nsignificantly better than the Simplex method and QOCA-solver, which uses the\nactive set method implementation for solving quadratic optimization.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 10:29:07 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Jamil", "Noreen", ""]]}, {"id": "1401.1061", "submitter": "Yingqian Zhang", "authors": "Sicco Verwer, Yingqian Zhang, Qing Chuan Ye", "title": "Learning optimization models in the presence of unknown relations", "comments": "37 pages. Working paper", "journal-ref": null, "doi": "10.1016/j.artint.2015.05.004", "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a sequential auction with multiple bidding agents, it is highly\nchallenging to determine the ordering of the items to sell in order to maximize\nthe revenue due to the fact that the autonomy and private information of the\nagents heavily influence the outcome of the auction.\n  The main contribution of this paper is two-fold. First, we demonstrate how to\napply machine learning techniques to solve the optimal ordering problem in\nsequential auctions. We learn regression models from historical auctions, which\nare subsequently used to predict the expected value of orderings for new\nauctions. Given the learned models, we propose two types of optimization\nmethods: a black-box best-first search approach, and a novel white-box approach\nthat maps learned models to integer linear programs (ILP) which can then be\nsolved by any ILP-solver. Although the studied auction design problem is hard,\nour proposed optimization methods obtain good orderings with high revenues.\n  Our second main contribution is the insight that the internal structure of\nregression models can be efficiently evaluated inside an ILP solver for\noptimization purposes. To this end, we provide efficient encodings of\nregression trees and linear regression models as ILP constraints. This new way\nof using learned models for optimization is promising. As the experimental\nresults show, it significantly outperforms the black-box best-first search in\nnearly all settings.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 12:42:47 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 11:22:03 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Verwer", "Sicco", ""], ["Zhang", "Yingqian", ""], ["Ye", "Qing Chuan", ""]]}, {"id": "1401.1247", "submitter": "Guy Van den Broeck", "authors": "Mathias Niepert and Guy Van den Broeck", "title": "Tractability through Exchangeability: A New Perspective on Efficient\n  Probabilistic Inference", "comments": "In Proceedings of the 28th AAAI Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exchangeability is a central notion in statistics and probability theory. The\nassumption that an infinite sequence of data points is exchangeable is at the\ncore of Bayesian statistics. However, finite exchangeability as a statistical\nproperty that renders probabilistic inference tractable is less\nwell-understood. We develop a theory of finite exchangeability and its relation\nto tractable probabilistic inference. The theory is complementary to that of\nindependence and conditional independence. We show that tractable inference in\nprobabilistic models with high treewidth and millions of variables can be\nunderstood using the notion of finite (partial) exchangeability. We also show\nthat existing lifted inference algorithms implicitly utilize a combination of\nconditional independence and partial exchangeability.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 00:30:25 GMT"}, {"version": "v2", "created": "Tue, 22 Apr 2014 22:21:16 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Niepert", "Mathias", ""], ["Broeck", "Guy Van den", ""]]}, {"id": "1401.1465", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Cortical prediction markets", "comments": "To appear, AAMAS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LG cs.MA q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate cortical learning from the perspective of mechanism design.\nFirst, we show that discretizing standard models of neurons and synaptic\nplasticity leads to rational agents maximizing simple scoring rules. Second,\nour main result is that the scoring rules are proper, implying that neurons\nfaithfully encode expected utilities in their synaptic weights and encode\nhigh-scoring outcomes in their spikes. Third, with this foundation in hand, we\npropose a biologically plausible mechanism whereby neurons backpropagate\nincentives which allows them to optimize their usefulness to the rest of\ncortex. Finally, experiments show that networks that backpropagate incentives\ncan learn simple tasks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 18:28:20 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1401.1475", "submitter": "Paulo Shakarian", "authors": "Paulo Shakarian, Gerardo I. Simari, Marcelo A. Falappa", "title": "Belief Revision in Structured Probabilistic Argumentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In real-world applications, knowledge bases consisting of all the information\nat hand for a specific domain, along with the current state of affairs, are\nbound to contain contradictory data coming from different sources, as well as\ndata with varying degrees of uncertainty attached. Likewise, an important\naspect of the effort associated with maintaining knowledge bases is deciding\nwhat information is no longer useful; pieces of information (such as\nintelligence reports) may be outdated, may come from sources that have recently\nbeen discovered to be of low quality, or abundant evidence may be available\nthat contradicts them. In this paper, we propose a probabilistic structured\nargumentation framework that arises from the extension of Presumptive\nDefeasible Logic Programming (PreDeLP) with probabilistic models, and argue\nthat this formalism is capable of addressing the basic issues of handling\ncontradictory and uncertain data. Then, to address the last issue, we focus on\nthe study of non-prioritized belief revision operations over probabilistic\nPreDeLP programs. We propose a set of rationality postulates -- based on\nwell-known ones developed for classical knowledge bases -- that characterize\nhow such operations should behave, and study a class of operators along with\ntheoretical relationships with the proposed postulates, including a\nrepresentation theorem stating the equivalence between this class and the class\nof operators characterized by the postulates.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 18:53:24 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Shakarian", "Paulo", ""], ["Simari", "Gerardo I.", ""], ["Falappa", "Marcelo A.", ""]]}, {"id": "1401.1533", "submitter": "Devis Pantano", "authors": "Devis Pantano", "title": "Proposta di nuovi strumenti per comprendere come funziona la cognizione\n  (Novel tools to understand how cognition works)", "comments": "In Italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I think that the main reason why we do not understand the general principles\nof how knowledge works (and probably also the reason why we have not yet\ndesigned and built efficient machines capable of artificial intelligence), is\nnot the excessive complexity of cognitive phenomena, but the lack of the\nconceptual and methodological tools to properly address the problem. It is like\ntrying to build up Physics without the concept of number, or to understand the\norigin of species without including the mechanism of natural selection. In this\npaper I propose some new conceptual and methodological tools, which seem to\noffer a real opportunity to understand the logic of cognitive processes. I\npropose a new method to properly treat the concepts of structure and schema,\nand to perform on them operations of structural analysis. These operations\nallow to move straightforwardly from concrete to more abstract representations.\nWith these tools I will suggest a definition for the concept of rule, of\nregularity and of emergent phenomena. From the analysis of some important\naspects of the rules, I suggest to distinguish them in operational and\nassociative rules. I propose that associative rules assume a dominant role in\ncognition. I also propose a definition for the concept of problem. At the end I\nwill briefly illustrate a possible general model for cognitive systems.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 22:38:18 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2014 22:26:33 GMT"}, {"version": "v3", "created": "Fri, 18 Apr 2014 19:39:37 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Pantano", "Devis", ""]]}, {"id": "1401.1549", "submitter": "Zheng Wen", "authors": "Zheng Wen, Daniel O'Neill and Hamid Reza Maei", "title": "Optimal Demand Response Using Device Based Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand response (DR) for residential and small commercial buildings is\nestimated to account for as much as 65% of the total energy savings potential\nof DR, and previous work shows that a fully automated Energy Management System\n(EMS) is a necessary prerequisite to DR in these areas. In this paper, we\npropose a novel EMS formulation for DR problems in these sectors. Specifically,\nwe formulate a fully automated EMS's rescheduling problem as a reinforcement\nlearning (RL) problem, and argue that this RL problem can be approximately\nsolved by decomposing it over device clusters. Compared with existing\nformulations, our new formulation (1) does not require explicitly modeling the\nuser's dissatisfaction on job rescheduling, (2) enables the EMS to\nself-initiate jobs, (3) allows the user to initiate more flexible requests and\n(4) has a computational complexity linear in the number of devices. We also\ndemonstrate the simulation results of applying Q-learning, one of the most\npopular and classical RL algorithms, to a representative example.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 00:49:01 GMT"}, {"version": "v2", "created": "Sat, 28 Jun 2014 04:24:47 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Wen", "Zheng", ""], ["O'Neill", "Daniel", ""], ["Maei", "Hamid Reza", ""]]}, {"id": "1401.1560", "submitter": "Tao Xiong", "authors": "Tao Xiong, Yukun Bao, Zhongyi Hu", "title": "Beyond One-Step-Ahead Forecasting: Evaluation of Alternative\n  Multi-Step-Ahead Forecasting Models for Crude Oil Prices", "comments": "32 pages", "journal-ref": "Energy Economics. 40, 2013: 405-415", "doi": "10.1016/j.eneco.2013.07.028", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate prediction of crude oil prices over long future horizons is\nchallenging and of great interest to governments, enterprises, and investors.\nThis paper proposes a revised hybrid model built upon empirical mode\ndecomposition (EMD) based on the feed-forward neural network (FNN) modeling\nframework incorporating the slope-based method (SBM), which is capable of\ncapturing the complex dynamic of crude oil prices. Three commonly used\nmulti-step-ahead prediction strategies proposed in the literature, including\niterated strategy, direct strategy, and MIMO (multiple-input multiple-output)\nstrategy, are examined and compared, and practical considerations for the\nselection of a prediction strategy for multi-step-ahead forecasting relating to\ncrude oil prices are identified. The weekly data from the WTI (West Texas\nIntermediate) crude oil spot price are used to compare the performance of the\nalternative models under the EMD-SBM-FNN modeling framework with selected\ncounterparts. The quantitative and comprehensive assessments are performed on\nthe basis of prediction accuracy and computational cost. The results obtained\nin this study indicate that the proposed EMD-SBM-FNN model using the MIMO\nstrategy is the best in terms of prediction accuracy with accredited\ncomputational load.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 01:59:53 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Xiong", "Tao", ""], ["Bao", "Yukun", ""], ["Hu", "Zhongyi", ""]]}, {"id": "1401.1669", "submitter": "J. G. Wolff", "authors": "J. Gerard Wolff", "title": "Smart machines and the SP theory of intelligence", "comments": "arXiv admin note: substantial text overlap with arXiv:1306.3890", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These notes describe how the \"SP theory of intelligence\", and its embodiment\nin the \"SP machine\", may help to realise cognitive computing, as described in\nthe book \"Smart Machines\". In the SP system, information compression and a\nconcept of \"multiple alignment\" are centre stage. The system is designed to\nintegrate such things as unsupervised learning, pattern recognition,\nprobabilistic reasoning, and more. It may help to overcome the problem of\nvariety in big data, it may serve in pattern recognition and in the\nunsupervised learning of structure in data, and it may facilitate the\nmanagement and transmission of big data. There is potential, via information\ncompression, for substantial gains in computational efficiency, especially in\nthe use of energy. The SP system may help to realise data-centric computing,\nperhaps via a development of Hebb's concept of a \"cell assembly\", or via the\nuse of light or DNA for the processing of information. It has potential in the\nmanagement of errors and uncertainty in data, in medical diagnosis, in\nprocessing streams of data, and in promoting adaptability in robots.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 11:32:56 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Wolff", "J. Gerard", ""]]}, {"id": "1401.1752", "submitter": "Noreen Jamil", "authors": "Noreen Jamil, Johannes M\\\"uller, Christof Lutteroth and Gerald Weber", "title": "Speeding up SOR Solvers for Constraint-based GUIs with a Warm-Start\n  Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer programs have graphical user interfaces (GUIs), which need good\nlayout to make efficient use of the available screen real estate. Most GUIs do\nnot have a fixed layout, but are resizable and able to adapt themselves.\nConstraints are a powerful tool for specifying adaptable GUI layouts: they are\nused to specify a layout in a general form, and a constraint solver is used to\nfind a satisfying concrete layout, e.g.\\ for a specific GUI size. The\nconstraint solver has to calculate a new layout every time a GUI is resized or\nchanged, so it needs to be efficient to ensure a good user experience. One\napproach for constraint solvers is based on the Gauss-Seidel algorithm and\nsuccessive over-relaxation (SOR).\n  Our observation is that a solution after resizing or changing is similar in\nstructure to a previous solution. Thus, our hypothesis is that we can increase\nthe computational performance of an SOR-based constraint solver if we reuse the\nsolution of a previous layout to warm-start the solving of a new layout. In\nthis paper we report on experiments to test this hypothesis experimentally for\nthree common use cases: big-step resizing, small-step resizing and constraint\nchange. In our experiments, we measured the solving time for randomly generated\nGUI layout specifications of various sizes. For all three cases we found that\nthe performance is improved if an existing solution is used as a starting\nsolution for a new layout.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 10:29:32 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Jamil", "Noreen", ""], ["M\u00fcller", "Johannes", ""], ["Lutteroth", "Christof", ""], ["Weber", "Gerald", ""]]}, {"id": "1401.1926", "submitter": "Zhongyi Hu", "authors": "Yukun Bao, Zhongyi Hu, Tao Xiong", "title": "A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters\n  Optimization", "comments": "27 pages. Neurocomputing, 2013", "journal-ref": null, "doi": "10.1016/j.neucom.2013.01.027", "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addressing the issue of SVMs parameters optimization, this study proposes an\nefficient memetic algorithm based on Particle Swarm Optimization algorithm\n(PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO is\nresponsible for exploration of the search space and the detection of the\npotential regions with optimum solutions, while pattern search (PS) is used to\nproduce an effective exploitation on the potential regions obtained by PSO.\nMoreover, a novel probabilistic selection strategy is proposed to select the\nappropriate individuals among the current population to undergo local\nrefinement, keeping a well balance between exploration and exploitation.\nExperimental results confirm that the local refinement with PS and our proposed\nselection strategy are effective, and finally demonstrate effectiveness and\nrobustness of the proposed PSO-PS based MA for SVMs parameters optimization.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 08:41:55 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Bao", "Yukun", ""], ["Hu", "Zhongyi", ""], ["Xiong", "Tao", ""]]}, {"id": "1401.2011", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern and Willemien Kets", "title": "A logic for reasoning about ambiguity", "comments": "Some of the material in this paper appeared in preliminary form in\n  \"Ambiguous langage and differences of belief\" (see arXiv:1203.0699)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard models of multi-agent modal logic do not capture the fact that\ninformation is often \\emph{ambiguous}, and may be interpreted in different ways\nby different agents. We propose a framework that can model this, and consider\ndifferent semantics that capture different assumptions about the agents'\nbeliefs regarding whether or not there is ambiguity. We examine the expressive\npower of logics of ambiguity compared to logics that cannot model ambiguity,\nwith respect to the different semantics that we propose.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 14:19:54 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Halpern", "Joseph Y.", ""], ["Kets", "Willemien", ""]]}, {"id": "1401.2121", "submitter": "Carlos Pedro dos Santos Gon\\c{c}alves", "authors": "Carlos Pedro Gon\\c{c}alves", "title": "Emotional Responses in Artificial Agent-Based Systems: Reflexivity and\n  Adaptation in Artificial Life", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current work addresses a virtual environment with self-replicating agents\nwhose decisions are based on a form of \"somatic computation\" (soma - body) in\nwhich basic emotional responses, taken in parallelism to actual living\norganisms, are introduced as a way to provide the agents with greater reflexive\nabilities. The work provides a contribution to the field of Artificial\nIntelligence (AI) and Artificial Life (ALife) in connection to a\nneurobiology-based cognitive framework for artificial systems and virtual\nenvironments' simulations. The performance of the agents capable of emotional\nresponses is compared with that of self-replicating automata, and the\nimplications of research on emotions and AI, in connection to both virtual\nagents as well as robots, is addressed regarding possible future directions and\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 19:13:02 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Gon\u00e7alves", "Carlos Pedro", ""]]}, {"id": "1401.2153", "submitter": "V Karthikeyan VKK", "authors": "V.Karthikeyan, V.J.Vijayalakshmi and P.Jeyakumar", "title": "Ontology - Based Dynamic Business Process Customization", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interaction between business models is used in consumer centric manner\ninstead of using a producer centric approach for customizing the business\nprocess in cloud environment. The knowledge based human semantic web is used\nfor customizing the business process It introduces the Human Semantic Web as a\nconceptual interface, providing human-understandable semantics on top of the\nordinary Semantic Web, which provides machine-readable semantics based on RDF\nin this mismatching is a major problem. To overcome this following technique\nautomatic customization detection is an automated process of detecting possible\nelements or variables of a business process that needto be especially treated\nin order to suit the requirement of the other process. To the business\nprocessto be customized as the primary business process and those that it\ncollaborates with as secondary business process or SBP Automatic customization\nenactment is an automated process of taking actions to perform the\ncustomization on the PBP according to the detected customization spots and the\nautomatic reasoning on the customization conceptualization knowledge framework.\nThe process of customizing businessprocesses by composite the web pages by\nusing web service.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 08:23:27 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 08:21:00 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2014 03:39:52 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Karthikeyan", "V.", ""], ["Vijayalakshmi", "V. J.", ""], ["Jeyakumar", "P.", ""]]}, {"id": "1401.2184", "submitter": "Alexandre Gondran", "authors": "Laurent Moalic (IRTES - SET), Alexandre Gondran (MAIAA)", "title": "Variations on Memetic Algorithms for Graph Coloring Problems", "comments": "11 pages, 8 figures, 3 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph vertex coloring with a given number of colors is a well-known and\nmuch-studied NP-complete problem.The most effective methods to solve this\nproblem are proved to be hybrid algorithms such as memetic algorithms or\nquantum annealing. Those hybrid algorithms use a powerful local search inside a\npopulation-based algorithm.This paper presents a new memetic algorithm based on\none of the most effective algorithms: the Hybrid Evolutionary Algorithm HEA\nfrom Galinier and Hao (1999).The proposed algorithm, denoted HEAD - for HEA in\nDuet - works with a population of only two individuals.Moreover, a new way of\nmanaging diversity is brought by HEAD.These two main differences greatly\nimprove the results, both in terms of solution quality and computational\ntime.HEAD has produced several good results for the popular DIMACS benchmark\ngraphs, such as 222-colorings for \\textless{}dsjc1000.9\\textgreater{},\n81-colorings for \\textless{}flat1000\\_76\\_0\\textgreater{} and even 47-colorings\nfor \\textless{}dsjc500.5\\textgreater{} and 82-colorings for\n\\textless{}dsjc1000.5\\textgreater{}.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 19:50:07 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 14:10:44 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Moalic", "Laurent", "", "IRTES - SET"], ["Gondran", "Alexandre", "", "MAIAA"]]}, {"id": "1401.2474", "submitter": "Barry Hurley", "authors": "Barry Hurley, Serdar Kadioglu, Yuri Malitsky, Barry O'Sullivan", "title": "Transformation-based Feature Computation for Algorithm Portfolios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance-specific algorithm configuration and algorithm portfolios have been\nshown to offer significant improvements over single algorithm approaches in a\nvariety of application domains. In the SAT and CSP domains algorithm portfolios\nhave consistently dominated the main competitions in these fields for the past\nfive years. For a portfolio approach to be effective there are two crucial\nconditions that must be met. First, there needs to be a collection of\ncomplementary solvers with which to make a portfolio. Second, there must be a\ncollection of problem features that can accurately identify structural\ndifferences between instances. This paper focuses on the latter issue: feature\nrepresentation, because, unlike SAT, not every problem has well-studied\nfeatures. We employ the well-known SATzilla feature set, but compute\nalternative sets on different SAT encodings of CSPs. We show that regardless of\nwhat encoding is used to convert the instances, adequate structural information\nis maintained to differentiate between problem instances, and that this can be\nexploited to make an effective portfolio-based CSP solver.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 22:05:39 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Hurley", "Barry", ""], ["Kadioglu", "Serdar", ""], ["Malitsky", "Yuri", ""], ["O'Sullivan", "Barry", ""]]}, {"id": "1401.2482", "submitter": "Marko Horvat", "authors": "Marko Horvat, Nikola Bogunovi\\'c, Kre\\v{s}imir \\'Cosi\\'c", "title": "STIMONT: A core ontology for multimedia stimuli description", "comments": "27 pages, 13 figures", "journal-ref": "Multimedia tools and applications, 11042, July 2013", "doi": "10.1007/s11042-013-1624-4", "report-no": null, "categories": "cs.MM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective multimedia documents such as images, sounds or videos elicit\nemotional responses in exposed human subjects. These stimuli are stored in\naffective multimedia databases and successfully used for a wide variety of\nresearch in psychology and neuroscience in areas related to attention and\nemotion processing. Although important all affective multimedia databases have\nnumerous deficiencies which impair their applicability. These problems, which\nare brought forward in the paper, result in low recall and precision of\nmultimedia stimuli retrieval which makes creating emotion elicitation\nprocedures difficult and labor-intensive. To address these issues a new core\nontology STIMONT is introduced. The STIMONT is written in OWL-DL formalism and\nextends W3C EmotionML format with an expressive and formal representation of\naffective concepts, high-level semantics, stimuli document metadata and the\nelicited physiology. The advantages of ontology in description of affective\nmultimedia stimuli are demonstrated in a document retrieval experiment and\ncompared against contemporary keyword-based querying methods. Also, a software\ntool Intelligent Stimulus Generator for retrieval of affective multimedia and\nconstruction of stimuli sequences is presented.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 23:36:51 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Horvat", "Marko", ""], ["Bogunovi\u0107", "Nikola", ""], ["\u0106osi\u0107", "Kre\u0161imir", ""]]}, {"id": "1401.2483", "submitter": "Andino Maseleno", "authors": "Andino Maseleno and Md. Mahmud Hasan", "title": "Dempster-Shafer Theory for Move Prediction in Start Kicking of The\n  Bicycle Kick of Sepak Takraw Game", "comments": "Middle-East Journal of Scientific Research, Vol. 16, No. 7, 2013.\n  ISSN 1990-9233, pp. 896 - 903", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents Dempster-Shafer theory for move prediction in start\nkicking of the bicycle kick of sepak takraw game. Sepak takraw is a highly\ncomplex net-barrier kicking sport that involves dazzling displays of quick\nreflexes, acrobatic twists, turns and swerves of the agile human body movement.\nA Bicycle kick or Scissor kick is a physical move made by throwing the body up\ninto the air, making a shearing movement with the legs to get one leg in front\nof the other without holding on to the ground. Specifically, this paper\nconsiders bicycle kick of sepak takraw game in start kicking of the ball with\nuncertainty where player has different awareness regarding the contingencies.\nWe have chosen Dempster-Shafer theory because the advantages of the\nDempster-Shafer theory which include the ability to model information in a\nflexible way without requiring a probability to be assigned to each element in\na set, providing a convenient and simple mechanism for combining two or more\npieces of evidence under certain conditions, it can model ignorance explicitly,\nrejection of the law of additivity for belief in disjoint propositions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 23:48:40 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Maseleno", "Andino", ""], ["Hasan", "Md. Mahmud", ""]]}, {"id": "1401.2503", "submitter": "Tao Xiong", "authors": "Tao Xiong, Yukun Bao, Zhongyi Hu", "title": "Does Restraining End Effect Matter in EMD-Based Modeling Framework for\n  Time Series Prediction? Some Experimental Evidences", "comments": "28 pages", "journal-ref": "Neurocomputing. 123, 2013: 174-184", "doi": "10.1016/j.neucom.2013.07.004", "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the \"decomposition-and-ensemble\" principle, the empirical mode\ndecomposition (EMD)-based modeling framework has been widely used as a\npromising alternative for nonlinear and nonstationary time series modeling and\nprediction. The end effect, which occurs during the sifting process of EMD and\nis apt to distort the decomposed sub-series and hurt the modeling process\nfollowed, however, has been ignored in previous studies. Addressing the end\neffect issue, this study proposes to incorporate end condition methods into\nEMD-based decomposition and ensemble modeling framework for one- and multi-step\nahead time series prediction. Four well-established end condition methods,\nMirror method, Coughlin's method, Slope-based method, and Rato's method, are\nselected, and support vector regression (SVR) is employed as the modeling\ntechnique. For the purpose of justification and comparison, well-known NN3\ncompetition data sets are used and four well-established prediction models are\nselected as benchmarks. The experimental results demonstrated that significant\nimprovement can be achieved by the proposed EMD-based SVR models with end\ncondition methods. The EMD-SBM-SVR model and EMD-Rato-SVR model, in particular,\nachieved the best prediction performances in terms of goodness of forecast\nmeasures and equality of accuracy of competing forecasts test.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 06:08:04 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Xiong", "Tao", ""], ["Bao", "Yukun", ""], ["Hu", "Zhongyi", ""]]}, {"id": "1401.2657", "submitter": "Vincenzo De Florio", "authors": "Hong Sun, Vincenzo De Florio, Ning Gui and Chris Blondia", "title": "The Missing Ones: Key Ingredients Towards Effective Ambient Assisted\n  Living Systems", "comments": null, "journal-ref": "Journal of Ambient Intelligence and Smart Environments, Volume 2\n  Issue 2, April 2010 Pages 109-120 IOS Press Amsterdam, The Netherlands, The\n  Netherlands", "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The population of elderly people keeps increasing rapidly, which becomes a\npredominant aspect of our societies. As such, solutions both efficacious and\ncost-effective need to be sought. Ambient Assisted Living (AAL) is a new\napproach which promises to address the needs from elderly people. In this\npaper, we claim that human participation is a key ingredient towards effective\nAAL systems, which not only saves social resources, but also has positive\nrelapses on the psychological health of the elderly people. Challenges in\nincreasing the human participation in ambient assisted living are discussed in\nthis paper and solutions to meet those challenges are also proposed. We use our\nproposed mutual assistance community, which is built with service oriented\napproach, as an example to demonstrate how to integrate human tasks in AAL\nsystems. Our preliminary simulation results are presented, which support the\neffectiveness of human participation.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2014 18:51:43 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Sun", "Hong", ""], ["De Florio", "Vincenzo", ""], ["Gui", "Ning", ""], ["Blondia", "Chris", ""]]}, {"id": "1401.3426", "submitter": "Ya'akov Gal", "authors": "Yaakov Gal, Avi Pfeffer", "title": "Networks of Influence Diagrams: A Formalism for Representing Agents'\n  Beliefs and Decision-Making Processes", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 33, pages\n  109-147, 2008", "doi": "10.1613/jair.2503", "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Networks of Influence Diagrams (NID), a compact, natural\nand highly expressive language for reasoning about agents beliefs and\ndecision-making processes. NIDs are graphical structures in which agents mental\nmodels are represented as nodes in a network; a mental model for an agent may\nitself use descriptions of the mental models of other agents. NIDs are\ndemonstrated by examples, showing how they can be used to describe conflicting\nand cyclic belief structures, and certain forms of bounded rationality. In an\nopponent modeling domain, NIDs were able to outperform other computational\nagents whose strategies were not known in advance. NIDs are equivalent in\nrepresentation to Bayesian games but they are more compact and structured than\nthis formalism. In particular, the equilibrium definition for NIDs makes an\nexplicit distinction between agents optimal strategies, and how they actually\nbehave in reality.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:38:59 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Gal", "Yaakov", ""], ["Pfeffer", "Avi", ""]]}, {"id": "1401.3427", "submitter": "Laurent Miclet", "authors": "Laurent Miclet, Sabri Bayoudh, Arnaud Delhay", "title": "Analogical Dissimilarity: Definition, Algorithms and Two Experiments in\n  Machine Learning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  793-824, 2008", "doi": "10.1613/jair.2519", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines the notion of analogical dissimilarity between four\nobjects, with a special focus on objects structured as sequences. Firstly, it\nstudies the case where the four objects have a null analogical dissimilarity,\ni.e. are in analogical proportion. Secondly, when one of these objects is\nunknown, it gives algorithms to compute it. Thirdly, it tackles the problem of\ndefining analogical dissimilarity, which is a measure of how far four objects\nare from being in analogical proportion. In particular, when objects are\nsequences, it gives a definition and an algorithm based on an optimal alignment\nof the four sequences. It gives also learning algorithms, i.e. methods to find\nthe triple of objects in a learning sample which has the least analogical\ndissimilarity with a given object. Two practical experiments are described: the\nfirst is a classification problem on benchmarks of binary and nominal data, the\nsecond shows how the generation of sequences by solving analogical equations\nenables a handwritten character recognition system to rapidly be adapted to a\nnew writer.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:42:13 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Miclet", "Laurent", ""], ["Bayoudh", "Sabri", ""], ["Delhay", "Arnaud", ""]]}, {"id": "1401.3428", "submitter": "Nicolas Meuleau", "authors": "Nicolas Meuleau, Emmanuel Benazera, Ronen I. Brafman, Eric A. Hansen,\n  Mausam", "title": "A Heuristic Search Approach to Planning with Continuous Resources in\n  Stochastic Domains", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  27-59, 2009", "doi": "10.1613/jair.2529", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimal planning in stochastic domains with\nresource constraints, where the resources are continuous and the choice of\naction at each step depends on resource availability. We introduce the HAO*\nalgorithm, a generalization of the AO* algorithm that performs search in a\nhybrid state space that is modeled using both discrete and continuous state\nvariables, where the continuous variables represent monotonic resources. Like\nother heuristic search algorithms, HAO* leverages knowledge of the start state\nand an admissible heuristic to focus computational effort on those parts of the\nstate space that could be reached from the start state by following an optimal\npolicy. We show that this approach is especially effective when resource\nconstraints limit how much of the state space is reachable. Experimental\nresults demonstrate its effectiveness in the domain that motivates our\nresearch: automated planning for planetary exploration rovers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:46:00 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Meuleau", "Nicolas", ""], ["Benazera", "Emmanuel", ""], ["Brafman", "Ronen I.", ""], ["Hansen", "Eric A.", ""], ["Mausam", "", ""]]}, {"id": "1401.3430", "submitter": "Lucas Bordeaux", "authors": "Lucas Bordeaux, Marco Cadoli, Toni Mancini", "title": "A Unifying Framework for Structural Properties of CSPs: Definitions,\n  Complexity, Tractability", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  607-629, 2008", "doi": "10.1613/jair.2538", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature on Constraint Satisfaction exhibits the definition of several\nstructural properties that can be possessed by CSPs, like (in)consistency,\nsubstitutability or interchangeability. Current tools for constraint solving\ntypically detect such properties efficiently by means of incomplete yet\neffective algorithms, and use them to reduce the search space and boost search.\n  In this paper, we provide a unifying framework encompassing most of the\nproperties known so far, both in CSP and other fields literature, and shed\nlight on the semantical relationships among them. This gives a unified and\ncomprehensive view of the topic, allows new, unknown, properties to emerge, and\nclarifies the computational complexity of the various detection problems.\n  In particular, among the others, two new concepts, fixability and\nremovability emerge, that come out to be the ideal characterisations of values\nthat may be safely assigned or removed from a variables domain, while\npreserving problem satisfiability. These two notions subsume a large number of\nknown properties, including inconsistency, substitutability and others.\n  Because of the computational intractability of all the property-detection\nproblems, by following the CSP approach we then determine a number of\nrelaxations which provide sufficient conditions for their tractability. In\nparticular, we exploit forms of language restrictions and local reasoning.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:47:30 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Bordeaux", "Lucas", ""], ["Cadoli", "Marco", ""], ["Mancini", "Toni", ""]]}, {"id": "1401.3431", "submitter": "James Delgrande", "authors": "James Delgrande, Yi Jin, Francis Jeffry Pelletier", "title": "Compositional Belief Update", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  757-791, 2008", "doi": "10.1613/jair.2539", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore a class of belief update operators, in which the\ndefinition of the operator is compositional with respect to the sentence to be\nadded. The goal is to provide an update operator that is intuitive, in that its\ndefinition is based on a recursive decomposition of the update sentences\nstructure, and that may be reasonably implemented. In addressing update, we\nfirst provide a definition phrased in terms of the models of a knowledge base.\nWhile this operator satisfies a core group of the benchmark Katsuno-Mendelzon\nupdate postulates, not all of the postulates are satisfied. Other\nKatsuno-Mendelzon postulates can be obtained by suitably restricting the\nsyntactic form of the sentence for update, as we show. In restricting the\nsyntactic form of the sentence for update, we also obtain a hierarchy of update\noperators with Winsletts standard semantics as the most basic interesting\napproach captured. We subsequently give an algorithm which captures this\napproach; in the general case the algorithm is exponential, but with some\nnot-unreasonable assumptions we obtain an algorithm that is linear in the size\nof the knowledge base. Hence the resulting approach has much better complexity\ncharacteristics than other operators in some situations. We also explore other\ncompositional belief change operators: erasure is developed as a dual operator\nto update; we show that a forget operator is definable in terms of update; and\nwe give a definition of the compositional revision operator. We obtain that\ncompositional revision, under the most natural definition, yields the Satoh\nrevision operator.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:48:21 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Delgrande", "James", ""], ["Jin", "Yi", ""], ["Pelletier", "Francis Jeffry", ""]]}, {"id": "1401.3432", "submitter": "Tinne De Laet", "authors": "Tinne De Laet, Joris De Schutter, Herman Bruyninckx", "title": "A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for\n  Range Finders in Dynamic Environments", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 33, pages\n  179-222, 2008", "doi": "10.1613/jair.2540", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes and experimentally validates a Bayesian network model of\na range finder adapted to dynamic environments. All modeling assumptions are\nrigorously explained, and all model parameters have a physical interpretation.\nThis approach results in a transparent and intuitive model. With respect to the\nstate of the art beam model this paper: (i) proposes a different functional\nform for the probability of range measurements caused by unmodeled objects,\n(ii) intuitively explains the discontinuity encountered in te state of the art\nbeam model, and (iii) reduces the number of model parameters, while maintaining\nthe same representational power for experimental data. The proposed beam model\nis called RBBM, short for Rigorously Bayesian Beam Model. A maximum likelihood\nand a variational Bayesian estimator (both based on expectation-maximization)\nare proposed to learn the model parameters.\n  Furthermore, the RBBM is extended to a full scan model in two steps: first,\nto a full scan model for static environments and next, to a full scan model for\ngeneral, dynamic environments. The full scan model accounts for the dependency\nbetween beams and adapts to the local sample density when using a particle\nfilter. In contrast to Gaussian-based state of the art models, the proposed\nfull scan model uses a sample-based approximation. This sample-based\napproximation enables handling dynamic environments and capturing\nmulti-modality, which occurs even in simple static environments.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:49:23 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["De Laet", "Tinne", ""], ["De Schutter", "Joris", ""], ["Bruyninckx", "Herman", ""]]}, {"id": "1401.3436", "submitter": "St\\'ephane Ross", "authors": "St\\'ephane Ross, Joelle Pineau, S\\'ebastien Paquet, Brahim Chaib-draa", "title": "Online Planning Algorithms for POMDPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  663-704, 2008", "doi": "10.1613/jair.2567", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially Observable Markov Decision Processes (POMDPs) provide a rich\nframework for sequential decision-making under uncertainty in stochastic\ndomains. However, solving a POMDP is often intractable except for small\nproblems due to their complexity. Here, we focus on online approaches that\nalleviate the computational complexity by computing good local policies at each\ndecision step during the execution. Online algorithms generally consist of a\nlookahead search to find the best action to execute at each time step in an\nenvironment. Our objectives here are to survey the various existing online\nPOMDP methods, analyze their properties and discuss their advantages and\ndisadvantages; and to thoroughly evaluate these online approaches in different\nenvironments under various metrics (return, error bound reduction, lower bound\nimprovement). Our experimental results indicate that state-of-the-art online\nheuristic search methods can handle large POMDP domains efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:52:25 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Ross", "St\u00e9phane", ""], ["Pineau", "Joelle", ""], ["Paquet", "S\u00e9bastien", ""], ["Chaib-draa", "Brahim", ""]]}, {"id": "1401.3437", "submitter": "Eyal Amir", "authors": "Eyal Amir, Allen Chang", "title": "Learning Partially Observable Deterministic Action Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 33, pages\n  349-402, 2008", "doi": "10.1613/jair.2575", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present exact algorithms for identifying deterministic-actions effects and\npreconditions in dynamic partially observable domains. They apply when one does\nnot know the action model(the way actions affect the world) of a domain and\nmust learn it from partial observations over time. Such scenarios are common in\nreal world applications. They are challenging for AI tasks because traditional\ndomain structures that underly tractability (e.g., conditional independence)\nfail there (e.g., world features become correlated). Our work departs from\ntraditional assumptions about partial observations and action models. In\nparticular, it focuses on problems in which actions are deterministic of simple\nlogical structure and observation models have all features observed with some\nfrequency. We yield tractable algorithms for the modified problem for such\ndomains.\n  Our algorithms take sequences of partial observations over time as input, and\noutput deterministic action models that could have lead to those observations.\nThe algorithms output all or one of those models (depending on our choice), and\nare exact in that no model is misclassified given the observations. Our\nalgorithms take polynomial time in the number of time steps and state features\nfor some traditional action classes examined in the AI-planning literature,\ne.g., STRIPS actions. In contrast, traditional approaches for HMMs and\nReinforcement Learning are inexact and exponentially intractable for such\ndomains. Our experiments verify the theoretical tractability guarantees, and\nshow that we identify action models exactly. Several applications in planning,\nautonomous exploration, and adventure-game playing already use these results.\nThey are also promising for probabilistic settings, partially observable\nreinforcement learning, and diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:52:56 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Amir", "Eyal", ""], ["Chang", "Allen", ""]]}, {"id": "1401.3438", "submitter": "Neil C.A. Moore", "authors": "Neil C.A. Moore, Patrick Prosser", "title": "The Ultrametric Constraint and its Application to Phylogenetics", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  901-938, 2008", "doi": "10.1613/jair.2580", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A phylogenetic tree shows the evolutionary relationships among species.\nInternal nodes of the tree represent speciation events and leaf nodes\ncorrespond to species. A goal of phylogenetics is to combine such trees into\nlarger trees, called supertrees, whilst respecting the relationships in the\noriginal trees. A rooted tree exhibits an ultrametric property; that is, for\nany three leaves of the tree it must be that one pair has a deeper most recent\ncommon ancestor than the other pairs, or that all three have the same most\nrecent common ancestor. This inspires a constraint programming encoding for\nrooted trees. We present an efficient constraint that enforces the ultrametric\nproperty over a symmetric array of constrained integer variables, with the\ninevitable property that the lower bounds of any three variables are mutually\nsupportive. We show that this allows an efficient constraint-based solution to\nthe supertree construction problem. We demonstrate that the versatility of\nconstraint programming can be exploited to allow solutions to variants of the\nsupertree construction problem.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:53:22 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Moore", "Neil C. A.", ""], ["Prosser", "Patrick", ""]]}, {"id": "1401.3439", "submitter": "Sonia Chernova", "authors": "Sonia Chernova, Manuela Veloso", "title": "Interactive Policy Learning through Confidence-Based Autonomy", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  1-25, 2009", "doi": "10.1613/jair.2584", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Confidence-Based Autonomy (CBA), an interactive algorithm for\npolicy learning from demonstration. The CBA algorithm consists of two\ncomponents which take advantage of the complimentary abilities of humans and\ncomputer agents. The first component, Confident Execution, enables the agent to\nidentify states in which demonstration is required, to request a demonstration\nfrom the human teacher and to learn a policy based on the acquired data. The\nalgorithm selects demonstrations based on a measure of action selection\nconfidence, and our results show that using Confident Execution the agent\nrequires fewer demonstrations to learn the policy than when demonstrations are\nselected by a human teacher. The second algorithmic component, Corrective\nDemonstration, enables the teacher to correct any mistakes made by the agent\nthrough additional demonstrations in order to improve the policy and future\ntask performance. CBA and its individual components are compared and evaluated\nin a complex simulated driving domain. The complete CBA algorithm results in\nthe best overall learning performance, successfully reproducing the behavior of\nthe teacher while balancing the tradeoff between number of demonstrations and\nnumber of incorrect actions during learning.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:53:48 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Chernova", "Sonia", ""], ["Veloso", "Manuela", ""]]}, {"id": "1401.3441", "submitter": "Ran El-Yaniv", "authors": "Ran El-Yaniv, Dmitry Pechyony", "title": "Transductive Rademacher Complexity and its Applications", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  193-234, 2009", "doi": "10.1613/jair.2587", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a technique for deriving data-dependent error bounds for\ntransductive learning algorithms based on transductive Rademacher complexity.\nOur technique is based on a novel general error bound for transduction in terms\nof transductive Rademacher complexity, together with a novel bounding technique\nfor Rademacher averages for particular algorithms, in terms of their\n\"unlabeled-labeled\" representation. This technique is relevant to many advanced\ngraph-based transductive algorithms and we demonstrate its effectiveness by\nderiving error bounds to three well known algorithms. Finally, we present a new\nPAC-Bayesian bound for mixtures of transductive algorithms based on our\nRademacher bounds.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:54:14 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["El-Yaniv", "Ran", ""], ["Pechyony", "Dmitry", ""]]}, {"id": "1401.3442", "submitter": "Amir Gershman", "authors": "Amir Gershman, Amnon Meisels, Roie Zivan", "title": "Asynchronous Forward Bounding for Distributed COPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  61-88, 2009", "doi": "10.1613/jair.2591", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new search algorithm for solving distributed constraint optimization\nproblems (DisCOPs) is presented. Agents assign variables sequentially and\ncompute bounds on partial assignments asynchronously. The asynchronous bounds\ncomputation is based on the propagation of partial assignments. The\nasynchronous forward-bounding algorithm (AFB) is a distributed optimization\nsearch algorithm that keeps one consistent partial assignment at all times. The\nalgorithm is described in detail and its correctness proven. Experimental\nevaluation shows that AFB outperforms synchronous branch and bound by many\norders of magnitude, and produces a phase transition as the tightness of the\nproblem increases. This is an analogous effect to the phase transition that has\nbeen observed when local consistency maintenance is applied to MaxCSPs. The AFB\nalgorithm is further enhanced by the addition of a backjumping mechanism,\nresulting in the AFB-BJ algorithm. Distributed backjumping is based on\naccumulated information on bounds of all values and on processing concurrently\na queue of candidate goals for the next move back. The AFB-BJ algorithm is\ncompared experimentally to other DisCOP algorithms (ADOPT, DPOP, OptAPO) and is\nshown to be a very efficient algorithm for DisCOPs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:54:38 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Gershman", "Amir", ""], ["Meisels", "Amnon", ""], ["Zivan", "Roie", ""]]}, {"id": "1401.3443", "submitter": "Antonis  Kakas", "authors": "Antonis Kakas, Paolo Mancarella, Fariba Sadri, Kostas Stathis,\n  Francesca Toni", "title": "Computational Logic Foundations of KGP Agents", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 33, pages\n  285-348, 2008", "doi": "10.1613/jair.2596", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the computational logic foundations of a model of agency\ncalled the KGP (Knowledge, Goals and Plan model. This model allows the\nspecification of heterogeneous agents that can interact with each other, and\ncan exhibit both proactive and reactive behaviour allowing them to function in\ndynamic environments by adjusting their goals and plans when changes happen in\nsuch environments. KGP provides a highly modular agent architecture that\nintegrates a collection of reasoning and physical capabilities, synthesised\nwithin transitions that update the agents state in response to reasoning,\nsensing and acting. Transitions are orchestrated by cycle theories that specify\nthe order in which transitions are executed while taking into account the\ndynamic context and agent preferences, as well as selection operators for\nproviding inputs to transitions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:54:59 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Kakas", "Antonis", ""], ["Mancarella", "Paolo", ""], ["Sadri", "Fariba", ""], ["Stathis", "Kostas", ""], ["Toni", "Francesca", ""]]}, {"id": "1401.3444", "submitter": "Didier Dubois", "authors": "Didier Dubois, H\\'el\\`ene Fargier, Jean-Fran\\c{c}ois Bonnefon", "title": "On the Qualitative Comparison of Decisions Having Positive and Negative\n  Features", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  385-417, 2008", "doi": "10.1613/jair.2520", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making a decision is often a matter of listing and comparing positive and\nnegative arguments. In such cases, the evaluation scale for decisions should be\nconsidered bipolar, that is, negative and positive values should be explicitly\ndistinguished. That is what is done, for example, in Cumulative Prospect\nTheory. However, contraryto the latter framework that presupposes genuine\nnumerical assessments, human agents often decide on the basis of an ordinal\nranking of the pros and the cons, and by focusing on the most salient\narguments. In other terms, the decision process is qualitative as well as\nbipolar. In this article, based on a bipolar extension of possibility theory,\nwe define and axiomatically characterize several decision rules tailored for\nthe joint handling of positive and negative arguments in an ordinal setting.\nThe simplest rules can be viewed as extensions of the maximin and maximax\ncriteria to the bipolar case, and consequently suffer from poor decisive power.\nMore decisive rules that refine the former are also proposed. These refinements\nagree both with principles of efficiency and with the spirit of\norder-of-magnitude reasoning, that prevails in qualitative decision theory. The\nmost refined decision rule uses leximin rankings of the pros and the cons, and\nthe ideas of counting arguments of equal strength and cancelling pros by cons.\nIt is shown to come down to a special case of Cumulative Prospect Theory, and\nto subsume the Take the Best heuristic studied by cognitive psychologists.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:55:20 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Dubois", "Didier", ""], ["Fargier", "H\u00e9l\u00e8ne", ""], ["Bonnefon", "Jean-Fran\u00e7ois", ""]]}, {"id": "1401.3448", "submitter": "Robert Mateescu", "authors": "Robert Mateescu, Rina Dechter, Radu Marinescu", "title": "AND/OR Multi-Valued Decision Diagrams (AOMDDs) for Graphical Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 33, pages\n  465-519, 2008", "doi": "10.1613/jair.2605", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recently introduced framework of AND/OR search spaces for\ngraphical models, we propose to augment Multi-Valued Decision Diagrams (MDD)\nwith AND nodes, in order to capture function decomposition structure and to\nextend these compiled data structures to general weighted graphical models\n(e.g., probabilistic models). We present the AND/OR Multi-Valued Decision\nDiagram (AOMDD) which compiles a graphical model into a canonical form that\nsupports polynomial (e.g., solution counting, belief updating) or constant time\n(e.g. equivalence of graphical models) queries. We provide two algorithms for\ncompiling the AOMDD of a graphical model. The first is search-based, and works\nby applying reduction rules to the trace of the memory intensive AND/OR search\nalgorithm. The second is inference-based and uses a Bucket Elimination schedule\nto combine the AOMDDs of the input functions via the the APPLY operator. For\nboth algorithms, the compilation time and the size of the AOMDD are, in the\nworst case, exponential in the treewidth of the graphical model, rather than\npathwidth as is known for ordered binary decision diagrams (OBDDs). We\nintroduce the concept of semantic treewidth, which helps explain why the size\nof a decision diagram is often much smaller than the worst case bound. We\nprovide an experimental evaluation that demonstrates the potential of AOMDDs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:09:35 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Mateescu", "Robert", ""], ["Dechter", "Rina", ""], ["Marinescu", "Radu", ""]]}, {"id": "1401.3450", "submitter": "Tal Grinshpoun", "authors": "Tal Grinshpoun, Amnon Meisels", "title": "Completeness and Performance Of The APO Algorithm", "comments": "arXiv admin note: substantial text overlap with arXiv:1109.6052 by\n  other authors", "journal-ref": "Journal Of Artificial Intelligence Research, Volume 33, pages\n  223-258, 2008", "doi": "10.1613/jair.2611", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous Partial Overlay (APO) is a search algorithm that uses\ncooperative mediation to solve Distributed Constraint Satisfaction Problems\n(DisCSPs). The algorithm partitions the search into different subproblems of\nthe DisCSP. The original proof of completeness of the APO algorithm is based on\nthe growth of the size of the subproblems. The present paper demonstrates that\nthis expected growth of subproblems does not occur in some situations, leading\nto a termination problem of the algorithm. The problematic parts in the APO\nalgorithm that interfere with its completeness are identified and necessary\nmodifications to the algorithm that fix these problematic parts are given. The\nresulting version of the algorithm, Complete Asynchronous Partial Overlay\n(CompAPO), ensures its completeness. Formal proofs for the soundness and\ncompleteness of CompAPO are given. A detailed performance evaluation of CompAPO\ncomparing it to other DisCSP algorithms is presented, along with an extensive\nexperimental evaluation of the algorithm's unique behavior. Additionally, an\noptimization version of the algorithm, CompOptAPO, is presented, discussed, and\nevaluated.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:10:44 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Grinshpoun", "Tal", ""], ["Meisels", "Amnon", ""]]}, {"id": "1401.3453", "submitter": "Judy  Goldsmith", "authors": "Judy Goldsmith, Jerome Lang, Miroslaw Truszczyski, Nic Wilson", "title": "The Computational Complexity of Dominance and Consistency in CP-Nets", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 33, pages\n  403-432, 2008", "doi": "10.1613/jair.2627", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the computational complexity of testing dominance and\nconsistency in CP-nets. Previously, the complexity of dominance has been\ndetermined for restricted classes in which the dependency graph of the CP-net\nis acyclic. However, there are preferences of interest that define cyclic\ndependency graphs; these are modeled with general CP-nets. In our main results,\nwe show here that both dominance and consistency for general CP-nets are\nPSPACE-complete. We then consider the concept of strong dominance, dominance\nequivalence and dominance incomparability, and several notions of optimality,\nand identify the complexity of the corresponding decision problems. The\nreductions used in the proofs are from STRIPS planning, and thus reinforce the\nearlier established connections between both areas.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:13:25 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Goldsmith", "Judy", ""], ["Lang", "Jerome", ""], ["Truszczyski", "Miroslaw", ""], ["Wilson", "Nic", ""]]}, {"id": "1401.3455", "submitter": "Prashant Doshi", "authors": "Prashant Doshi, Piotr J. Gmytrasiewicz", "title": "Monte Carlo Sampling Methods for Approximating Interactive POMDPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  297-337, 2009", "doi": "10.1613/jair.2630", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially observable Markov decision processes (POMDPs) provide a principled\nframework for sequential planning in uncertain single agent settings. An\nextension of POMDPs to multiagent settings, called interactive POMDPs\n(I-POMDPs), replaces POMDP belief spaces with interactive hierarchical belief\nsystems which represent an agent's belief about the physical world, about\nbeliefs of other agents, and about their beliefs about others' beliefs. This\nmodification makes the difficulties of obtaining solutions due to complexity of\nthe belief and policy spaces even more acute. We describe a general method for\nobtaining approximate solutions of I-POMDPs based on particle filtering (PF).\nWe introduce the interactive PF, which descends the levels of the interactive\nbelief hierarchies and samples and propagates beliefs at each level. The\ninteractive PF is able to mitigate the belief space complexity, but it does not\naddress the policy space complexity. To mitigate the policy space complexity --\nsometimes also called the curse of history -- we utilize a complementary method\nbased on sampling likely observations while building the look ahead\nreachability tree. While this approach does not completely address the curse of\nhistory, it beats back the curse's impact substantially. We provide\nexperimental results and chart future work.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:14:08 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Doshi", "Prashant", ""], ["Gmytrasiewicz", "Piotr J.", ""]]}, {"id": "1401.3458", "submitter": "Fahiem Bacchus", "authors": "Fahiem Bacchus, Shannon Dalmao, Toniann Pitassi", "title": "Solving #SAT and Bayesian Inference with Backtracking Search", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  391-442, 2009", "doi": "10.1613/jair.2648", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in Bayes Nets (BAYES) is an important problem with numerous\napplications in probabilistic reasoning. Counting the number of satisfying\nassignments of a propositional formula (#SAT) is a closely related problem of\nfundamental theoretical importance. Both these problems, and others, are\nmembers of the class of sum-of-products (SUMPROD) problems. In this paper we\nshow that standard backtracking search when augmented with a simple memoization\nscheme (caching) can solve any sum-of-products problem with time complexity\nthat is at least as good any other state-of-the-art exact algorithm, and that\nit can also achieve the best known time-space tradeoff. Furthermore,\nbacktracking's ability to utilize more flexible variable orderings allows us to\nprove that it can achieve an exponential speedup over other standard algorithms\nfor SUMPROD on some instances.\n  The ideas presented here have been utilized in a number of solvers that have\nbeen applied to various types of sum-of-product problems. These system's have\nexploited the fact that backtracking can naturally exploit more of the\nproblem's structure to achieve improved performance on a range of\nprobleminstances. Empirical evidence of this performance gain has appeared in\npublished works describing these solvers, and we provide references to these\nworks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:17:49 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Bacchus", "Fahiem", ""], ["Dalmao", "Shannon", ""], ["Pitassi", "Toniann", ""]]}, {"id": "1401.3459", "submitter": "Maxim Binshtok", "authors": "Maxim Binshtok, Ronen I. Brafman, Carmel Domshlak, Solomon Eyal\n  Shimony", "title": "Generic Preferences over Subsets of Structured Objects", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  133-164, 2009", "doi": "10.1613/jair.2653", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various tasks in decision making and decision support systems require\nselecting a preferred subset of a given set of items. Here we focus on problems\nwhere the individual items are described using a set of characterizing\nattributes, and a generic preference specification is required, that is, a\nspecification that can work with an arbitrary set of items. For example,\npreferences over the content of an online newspaper should have this form: At\neach viewing, the newspaper contains a subset of the set of articles currently\navailable. Our preference specification over this subset should be provided\noffline, but we should be able to use it to select a subset of any currently\navailable set of articles, e.g., based on their tags. We present a general\napproach for lifting formalisms for specifying preferences over objects with\nmultiple attributes into ones that specify preferences over subsets of such\nobjects. We also show how we can compute an optimal subset given such a\nspecification in a relatively efficient manner. We provide an empirical\nevaluation of the approach as well as some worst-case complexity results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:18:50 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Binshtok", "Maxim", ""], ["Brafman", "Ronen I.", ""], ["Domshlak", "Carmel", ""], ["Shimony", "Solomon Eyal", ""]]}, {"id": "1401.3460", "submitter": "Daniel S. Bernstein", "authors": "Daniel S. Bernstein, Christopher Amato, Eric A. Hansen, Shlomo\n  Zilberstein", "title": "Policy Iteration for Decentralized Control of Markov Decision Processes", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  89-132, 2009", "doi": "10.1613/jair.2667", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordination of distributed agents is required for problems arising in many\nareas, including multi-robot systems, networking and e-commerce. As a formal\nframework for such problems, we use the decentralized partially observable\nMarkov decision process (DEC-POMDP). Though much work has been done on optimal\ndynamic programming algorithms for the single-agent version of the problem,\noptimal algorithms for the multiagent case have been elusive. The main\ncontribution of this paper is an optimal policy iteration algorithm for solving\nDEC-POMDPs. The algorithm uses stochastic finite-state controllers to represent\npolicies. The solution can include a correlation device, which allows agents to\ncorrelate their actions without communicating. This approach alternates between\nexpanding the controller and performing value-preserving transformations, which\nmodify the controller without sacrificing value. We present two efficient\nvalue-preserving transformations: one can reduce the size of the controller and\nthe other can improve its value while keeping the size fixed. Empirical results\ndemonstrate the usefulness of value-preserving transformations in increasing\nvalue while keeping controller size to a minimum. To broaden the applicability\nof the approach, we also present a heuristic version of the policy iteration\nalgorithm, which sacrifices convergence to optimality. This algorithm further\nreduces the size of the controllers at each step by assuming that probability\ndistributions over the other agents actions are known. While this assumption\nmay not hold in general, it helps produce higher quality solutions in our test\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:20:25 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Bernstein", "Daniel S.", ""], ["Amato", "Christopher", ""], ["Hansen", "Eric A.", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1401.3461", "submitter": "Marek Petrik", "authors": "Marek Petrik, Shlomo Zilberstein", "title": "A Bilinear Programming Approach for Multiagent Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  235-274, 2009", "doi": "10.1613/jair.2673", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiagent planning and coordination problems are common and known to be\ncomputationally hard. We show that a wide range of two-agent problems can be\nformulated as bilinear programs. We present a successive approximation\nalgorithm that significantly outperforms the coverage set algorithm, which is\nthe state-of-the-art method for this class of multiagent problems. Because the\nalgorithm is formulated for bilinear programs, it is more general and simpler\nto implement. The new algorithm can be terminated at any time and-unlike the\ncoverage set algorithm-it facilitates the derivation of a useful online\nperformance bound. It is also much more efficient, on average reducing the\ncomputation time of the optimal solution by about four orders of magnitude.\nFinally, we introduce an automatic dimensionality reduction method that\nimproves the effectiveness of the algorithm, extending its applicability to new\ndomains and providing a new way to analyze a subclass of bilinear programs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:21:26 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Petrik", "Marek", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1401.3462", "submitter": "Amarjeet Singh", "authors": "Amarjeet Singh, Andreas Krause, Carlos Guestrin, William J. Kaiser", "title": "Efficient Informative Sensing using Multiple Robots", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  707-755, 2009", "doi": "10.1613/jair.2674", "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for efficient monitoring of spatio-temporal dynamics in large\nenvironmental applications, such as the water quality monitoring in rivers and\nlakes, motivates the use of robotic sensors in order to achieve sufficient\nspatial coverage. Typically, these robots have bounded resources, such as\nlimited battery or limited amounts of time to obtain measurements. Thus,\ncareful coordination of their paths is required in order to maximize the amount\nof information collected, while respecting the resource constraints. In this\npaper, we present an efficient approach for near-optimally solving the NP-hard\noptimization problem of planning such informative paths. In particular, we\nfirst develop eSIP (efficient Single-robot Informative Path planning), an\napproximation algorithm for optimizing the path of a single robot. Hereby, we\nuse a Gaussian Process to model the underlying phenomenon, and use the mutual\ninformation between the visited locations and remainder of the space to\nquantify the amount of information collected. We prove that the mutual\ninformation collected using paths obtained by using eSIP is close to the\ninformation obtained by an optimal solution. We then provide a general\ntechnique, sequential allocation, which can be used to extend any single robot\nplanning algorithm, such as eSIP, for the multi-robot problem. This procedure\napproximately generalizes any guarantees for the single-robot problem to the\nmulti-robot case. We extensively evaluate the effectiveness of our approach on\nseveral experiments performed in-field for two important environmental sensing\napplications, lake and river monitoring, and simulation experiments performed\nusing several real world sensor network data sets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:21:51 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Singh", "Amarjeet", ""], ["Krause", "Andreas", ""], ["Guestrin", "Carlos", ""], ["Kaiser", "William J.", ""]]}, {"id": "1401.3463", "submitter": "Roberto Sebastiani", "authors": "Roberto Sebastiani, Michele Vescovi", "title": "Automated Reasoning in Modal and Description Logics via SAT Encoding:\n  the Case Study of K(m)/ALC-Satisfiability", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  343-389, 2009", "doi": "10.1613/jair.2675", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, modal and description logics have been applied to\nnumerous areas of computer science, including knowledge representation, formal\nverification, database theory, distributed computing and, more recently,\nsemantic web and ontologies. For this reason, the problem of automated\nreasoning in modal and description logics has been thoroughly investigated. In\nparticular, many approaches have been proposed for efficiently handling the\nsatisfiability of the core normal modal logic K(m), and of its notational\nvariant, the description logic ALC. Although simple in structure, K(m)/ALC is\ncomputationally very hard to reason on, its satisfiability being\nPSPACE-complete.\n  In this paper we start exploring the idea of performing automated reasoning\ntasks in modal and description logics by encoding them into SAT, so that to be\nhandled by state-of-the-art SAT tools; as with most previous approaches, we\nbegin our investigation from the satisfiability in K(m). We propose an\nefficient encoding, and we test it on an extensive set of benchmarks, comparing\nthe approach with the main state-of-the-art tools available. Although the\nencoding is necessarily worst-case exponential, from our experiments we notice\nthat, in practice, this approach can handle most or all the problems which are\nat the reach of the other approaches, with performances which are comparable\nwith, or even better than, those of the current state-of-the-art tools.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:22:19 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Sebastiani", "Roberto", ""], ["Vescovi", "Michele", ""]]}, {"id": "1401.3464", "submitter": "R\\'on\\'an Daly", "authors": "R\\'on\\'an Daly, Qiang Shen", "title": "Learning Bayesian Network Equivalence Classes with Ant Colony\n  Optimization", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  391-447, 2009", "doi": "10.1613/jair.2681", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are a useful tool in the representation of uncertain\nknowledge. This paper proposes a new algorithm called ACO-E, to learn the\nstructure of a Bayesian network. It does this by conducting a search through\nthe space of equivalence classes of Bayesian networks using Ant Colony\nOptimization (ACO). To this end, two novel extensions of traditional ACO\ntechniques are proposed and implemented. Firstly, multiple types of moves are\nallowed. Secondly, moves can be given in terms of indices that are not based on\nconstruction graph nodes. The results of testing show that ACO-E performs\nbetter than a greedy search and other state-of-the-art and metaheuristic\nalgorithms whilst searching in the space of equivalence classes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:22:48 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Daly", "R\u00f3n\u00e1n", ""], ["Shen", "Qiang", ""]]}, {"id": "1401.3466", "submitter": "Talal  Rahwan", "authors": "Talal Rahwan, Sarvapali Dyanand Ramchurn, Nicholas Robert Jennings,\n  Andrea Giovannucci", "title": "An Anytime Algorithm for Optimal Coalition Structure Generation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  521-567, 2009", "doi": "10.1613/jair.2695", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coalition formation is a fundamental type of interaction that involves the\ncreation of coherent groupings of distinct, autonomous, agents in order to\nefficiently achieve their individual or collective goals. Forming effective\ncoalitions is a major research challenge in the field of multi-agent systems.\nCentral to this endeavour is the problem of determining which of the many\npossible coalitions to form in order to achieve some goal. This usually\nrequires calculating a value for every possible coalition, known as the\ncoalition value, which indicates how beneficial that coalition would be if it\nwas formed. Once these values are calculated, the agents usually need to find a\ncombination of coalitions, in which every agent belongs to exactly one\ncoalition, and by which the overall outcome of the system is maximized.\nHowever, this coalition structure generation problem is extremely challenging\ndue to the number of possible solutions that need to be examined, which grows\nexponentially with the number of agents involved. To date, therefore, many\nalgorithms have been proposed to solve this problem using different techniques\nranging from dynamic programming, to integer programming, to stochastic search\nall of which suffer from major limitations relating to execution time, solution\nquality, and memory requirements.\n  With this in mind, we develop an anytime algorithm to solve the coalition\nstructure generation problem. Specifically, the algorithm uses a novel\nrepresentation of the search space, which partitions the space of possible\nsolutions into sub-spaces such that it is possible to compute upper and lower\nbounds on the values of the best coalition structures in them. These bounds are\nthen used to identify the sub-spaces that have no potential of containing the\noptimal solution so that they can be pruned. The algorithm, then, searches\nthrough the remaining sub-spaces very efficiently using a branch-and-bound\ntechnique to avoid examining all the solutions within the searched subspace(s).\nIn this setting, we prove that our algorithm enumerates all coalition\nstructures efficiently by avoiding redundant and invalid solutions\nautomatically. Moreover, in order to effectively test our algorithm we develop\na new type of input distribution which allows us to generate more reliable\nbenchmarks compared to the input distributions previously used in the field.\nGiven this new distribution, we show that for 27 agents our algorithm is able\nto find solutions that are optimal in 0.175% of the time required by the\nfastest available algorithm in the literature. The algorithm is anytime, and if\ninterrupted before it would have normally terminated, it can still provide a\nsolution that is guaranteed to be within a bound from the optimal one.\nMoreover, the guarantees we provide on the quality of the solution are\nsignificantly better than those provided by the previous state of the art\nalgorithms designed for this purpose. For example, for the worst case\ndistribution given 25 agents, our algorithm is able to find a 90% efficient\nsolution in around 10% of time it takes to find the optimal solution.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:23:56 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Rahwan", "Talal", ""], ["Ramchurn", "Sarvapali Dyanand", ""], ["Jennings", "Nicholas Robert", ""], ["Giovannucci", "Andrea", ""]]}, {"id": "1401.3467", "submitter": "Omer Gim\\'enez", "authors": "Omer Gim\\'enez, Anders Jonsson", "title": "Planning over Chain Causal Graphs for Variables with Domains of Size 5\n  Is NP-Hard", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  675-706, 2009", "doi": "10.1613/jair.2742", "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, considerable focus has been given to the problem of determining the\nboundary between tractable and intractable planning problems. In this paper, we\nstudy the complexity of planning in the class C_n of planning problems,\ncharacterized by unary operators and directed path causal graphs. Although this\nis one of the simplest forms of causal graphs a planning problem can have, we\nshow that planning is intractable for C_n (unless P = NP), even if the domains\nof state variables have bounded size. In particular, we show that plan\nexistence for C_n^k is NP-hard for k>=5 by reduction from CNFSAT. Here, k\ndenotes the upper bound on the size of the state variable domains. Our result\nreduces the complexity gap for the class C_n^k to cases k=3 and k=4 only, since\nC_n^2 is known to be tractable.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:25:29 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Gim\u00e9nez", "Omer", ""], ["Jonsson", "Anders", ""]]}, {"id": "1401.3468", "submitter": "Hector Geffner", "authors": "Hector Palacios, Hector Geffner", "title": "Compiling Uncertainty Away in Conformant Planning Problems with Bounded\n  Width", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  623-675, 2009", "doi": "10.1613/jair.2708", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformant planning is the problem of finding a sequence of actions for\nachieving a goal in the presence of uncertainty in the initial state or action\neffects. The problem has been approached as a path-finding problem in belief\nspace where good belief representations and heuristics are critical for scaling\nup. In this work, a different formulation is introduced for conformant problems\nwith deterministic actions where they are automatically converted into\nclassical ones and solved by an off-the-shelf classical planner. The\ntranslation maps literals L and sets of assumptions t about the initial\nsituation, into new literals KL/t that represent that L must be true if t is\ninitially true. We lay out a general translation scheme that is sound and\nestablish the conditions under which the translation is also complete. We show\nthat the complexity of the complete translation is exponential in a parameter\nof the problem called the conformant width, which for most benchmarks is\nbounded. The planner based on this translation exhibits good performance in\ncomparison with existing planners, and is the basis for T0, the best performing\nplanner in the Conformant Track of the 2006 International Planning Competition.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:27:00 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Palacios", "Hector", ""], ["Geffner", "Hector", ""]]}, {"id": "1401.3469", "submitter": "Vicente Ruiz de Angulo", "authors": "Vicente Ruiz de Angulo, Carme Torras", "title": "Exploiting Single-Cycle Symmetries in Continuous Constraint Problems", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  499-520, 2009", "doi": "10.1613/jair.2711", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetries in discrete constraint satisfaction problems have been explored\nand exploited in the last years, but symmetries in continuous constraint\nproblems have not received the same attention. Here we focus on permutations of\nthe variables consisting of one single cycle. We propose a procedure that takes\nadvantage of these symmetries by interacting with a continuous constraint\nsolver without interfering with it. A key concept in this procedure are the\nclasses of symmetric boxes formed by bisecting a n-dimensional cube at the same\npoint in all dimensions at the same time. We analyze these classes and quantify\nthem as a function of the cube dimensionality. Moreover, we propose a simple\nalgorithm to generate the representatives of all these classes for any number\nof variables at very high rates. A problem example from the chemical\nand#64257;eld and the cyclic n-roots problem are used to show the performance\nof the approach in practice.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:27:27 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["de Angulo", "Vicente Ruiz", ""], ["Torras", "Carme", ""]]}, {"id": "1401.3470", "submitter": "J\\\"org Hoffmann", "authors": "J\\\"org Hoffmann, Piergiorgio Bertoli, Malte Helmert, Marco Pistore", "title": "Message-Based Web Service Composition, Integrity Constraints, and\n  Planning under Uncertainty: A New Connection", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  49-117, 2009", "doi": "10.1613/jair.2716", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to recent advances, AI Planning has become the underlying technique\nfor several applications. Figuring prominently among these is automated Web\nService Composition (WSC) at the \"capability\" level, where services are\ndescribed in terms of preconditions and effects over ontological concepts. A\nkey issue in addressing WSC as planning is that ontologies are not only formal\nvocabularies; they also axiomatize the possible relationships between concepts.\nSuch axioms correspond to what has been termed \"integrity constraints\" in the\nactions and change literature, and applying a web service is essentially a\nbelief update operation. The reasoning required for belief update is known to\nbe harder than reasoning in the ontology itself. The support for belief update\nis severely limited in current planning tools.\n  Our first contribution consists in identifying an interesting special case of\nWSC which is both significant and more tractable. The special case, which we\nterm \"forward effects\", is characterized by the fact that every ramification of\na web service application involves at least one new constant generated as\noutput by the web service. We show that, in this setting, the reasoning\nrequired for belief update simplifies to standard reasoning in the ontology\nitself. This relates to, and extends, current notions of \"message-based\" WSC,\nwhere the need for belief update is removed by a strong (often implicit or\ninformal) assumption of \"locality\" of the individual messages. We clarify the\ncomputational properties of the forward effects case, and point out a strong\nrelation to standard notions of planning under uncertainty, suggesting that\neffective tools for the latter can be successfully adapted to address the\nformer.\n  Furthermore, we identify a significant sub-case, named \"strictly forward\neffects\", where an actual compilation into planning under uncertainty exists.\nThis enables us to exploit off-the-shelf planning tools to solve message-based\nWSC in a general form that involves powerful ontologies, and requires reasoning\nabout partial matches between concepts. We provide empirical evidence that this\napproach may be quite effective, using Conformant-FF as the underlying planner.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:27:56 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Hoffmann", "J\u00f6rg", ""], ["Bertoli", "Piergiorgio", ""], ["Helmert", "Malte", ""], ["Pistore", "Marco", ""]]}, {"id": "1401.3471", "submitter": "Marco Zaffalon", "authors": "Marco Zaffalon, Enrique Miranda", "title": "Conservative Inference Rule for Uncertain Reasoning under Incompleteness", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  757-821, 2009", "doi": "10.1613/jair.2736", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we formulate the problem of inference under incomplete\ninformation in very general terms. This includes modelling the process\nresponsible for the incompleteness, which we call the incompleteness process.\nWe allow the process behaviour to be partly unknown. Then we use Walleys theory\nof coherent lower previsions, a generalisation of the Bayesian theory to\nimprecision, to derive the rule to update beliefs under incompleteness that\nlogically follows from our assumptions, and that we call conservative inference\nrule. This rule has some remarkable properties: it is an abstract rule to\nupdate beliefs that can be applied in any situation or domain; it gives us the\nopportunity to be neither too optimistic nor too pessimistic about the\nincompleteness process, which is a necessary condition to draw reliable while\nstrong enough conclusions; and it is a coherent rule, in the sense that it\ncannot lead to inconsistencies. We give examples to show how the new rule can\nbe applied in expert systems, in parametric statistical inference, and in\npattern classification, and discuss more generally the view of incompleteness\nprocesses defended here as well as some of its consequences.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:28:54 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Zaffalon", "Marco", ""], ["Miranda", "Enrique", ""]]}, {"id": "1401.3472", "submitter": "Kaile Su", "authors": "Kaile Su, Abdul Sattar, Guanfeng Lv, Yan Zhang", "title": "Variable Forgetting in Reasoning about Knowledge", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  677-716, 2009", "doi": "10.1613/jair.2750", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate knowledge reasoning within a simple framework\ncalled knowledge structure. We use variable forgetting as a basic operation for\none agent to reason about its own or other agents\\ knowledge. In our framework,\ntwo notions namely agents\\ observable variables and the weakest sufficient\ncondition play important roles in knowledge reasoning. Given a background\nknowledge base and a set of observable variables for each agent, we show that\nthe notion of an agent knowing a formula can be defined as a weakest sufficient\ncondition of the formula under background knowledge base. Moreover, we show how\nto capture the notion of common knowledge by using a generalized notion of\nweakest sufficient condition. Also, we show that public announcement operator\ncan be conveniently dealt with via our notion of knowledge structure. Further,\nwe explore the computational complexity of the problem whether an epistemic\nformula is realized in a knowledge structure. In the general case, this problem\nis PSPACE-hard; however, for some interesting subcases, it can be reduced to\nco-NP. Finally, we discuss possible applications of our framework in some\ninteresting domains such as the automated analysis of the well-known muddy\nchildren puzzle and the verification of the revised Needham-Schroeder protocol.\nWe believe that there are many scenarios where the natural presentation of the\navailable information about knowledge is under the form of a knowledge\nstructure. What makes it valuable compared with the corresponding multi-agent\nS5 Kripke structure is that it can be much more succinct.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:29:17 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Su", "Kaile", ""], ["Sattar", "Abdul", ""], ["Lv", "Guanfeng", ""], ["Zhang", "Yan", ""]]}, {"id": "1401.3474", "submitter": "Andreas Krause", "authors": "Andreas Krause, Carlos Guestrin", "title": "Optimal Value of Information in Graphical Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  557-591, 2009", "doi": "10.1613/jair.2737", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world decision making tasks require us to choose among several\nexpensive observations. In a sensor network, for example, it is important to\nselect the subset of sensors that is expected to provide the strongest\nreduction in uncertainty. In medical decision making tasks, one needs to select\nwhich tests to administer before deciding on the most effective treatment. It\nhas been general practice to use heuristic-guided procedures for selecting\nobservations. In this paper, we present the first efficient optimal algorithms\nfor selecting observations for a class of probabilistic graphical models. For\nexample, our algorithms allow to optimally label hidden variables in Hidden\nMarkov Models (HMMs). We provide results for both selecting the optimal subset\nof observations, and for obtaining an optimal conditional observation plan.\n  Furthermore we prove a surprising result: In most graphical models tasks, if\none designs an efficient algorithm for chain graphs, such as HMMs, this\nprocedure can be generalized to polytree graphical models. We prove that the\noptimizing value of information is $NP^{PP}$-hard even for polytrees. It also\nfollows from our results that just computing decision theoretic value of\ninformation objective functions, which are commonly used in practice, is a\n#P-complete problem even on Naive Bayes models (a simple special case of\npolytrees).\n  In addition, we consider several extensions, such as using our algorithms for\nscheduling observation selection for multiple sensors. We demonstrate the\neffectiveness of our approach on several real-world datasets, including a\nprototype sensor network deployment for energy conservation in buildings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:30:52 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Krause", "Andreas", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1401.3475", "submitter": "Meghyn  Bienvenu", "authors": "Meghyn Bienvenu", "title": "Prime Implicates and Prime Implicants: From Propositional to Modal Logic", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  71-128, 2009", "doi": "10.1613/jair.2754", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prime implicates and prime implicants have proven relevant to a number of\nareas of artificial intelligence, most notably abductive reasoning and\nknowledge compilation. The purpose of this paper is to examine how these\nnotions might be appropriately extended from propositional logic to the modal\nlogic K. We begin the paper by considering a number of potential definitions of\nclauses and terms for K. The different definitions are evaluated with respect\nto a set of syntactic, semantic, and complexity-theoretic properties\ncharacteristic of the propositional definition. We then compare the definitions\nwith respect to the properties of the notions of prime implicates and prime\nimplicants that they induce. While there is no definition that perfectly\ngeneralizes the propositional notions, we show that there does exist one\ndefinition which satisfies many of the desirable properties of the\npropositional case. In the second half of the paper, we consider the\ncomputational properties of the selected definition. To this end, we provide\nsound and complete algorithms for generating and recognizing prime implicates,\nand we show the prime implicate recognition task to be PSPACE-complete. We also\nprove upper and lower bounds on the size and number of prime implicates. While\nthe paper focuses on the logic K, all of our results hold equally well for\nmulti-modal K and for concept expressions in the description logic ALC.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:31:28 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Bienvenu", "Meghyn", ""]]}, {"id": "1401.3476", "submitter": "Piero A. Bonatti", "authors": "Piero A. Bonatti, Carsten Lutz, Frank Wolter", "title": "The Complexity of Circumscription in DLs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  717-773, 2009", "doi": "10.1613/jair.2763", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As fragments of first-order logic, Description logics (DLs) do not provide\nnonmonotonic features such as defeasible inheritance and default rules. Since\nmany applications would benefit from the availability of such features, several\nfamilies of nonmonotonic DLs have been developed that are mostly based on\ndefault logic and autoepistemic logic. In this paper, we consider\ncircumscription as an interesting alternative approach to nonmonotonic DLs\nthat, in particular, supports defeasible inheritance in a natural way. We study\nDLs extended with circumscription under different language restrictions and\nunder different constraints on the sets of minimized, fixed, and varying\npredicates, and pinpoint the exact computational complexity of reasoning for\nDLs ranging from ALC to ALCIO and ALCQO. When the minimized and fixed\npredicates include only concept names but no role names, then reasoning is\ncomplete for NExpTime^NP. It becomes complete for NP^NExpTime when the number\nof minimized and fixed predicates is bounded by a constant. If roles can be\nminimized or fixed, then complexity ranges from NExpTime^NP to undecidability.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:32:08 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Bonatti", "Piero A.", ""], ["Lutz", "Carsten", ""], ["Wolter", "Frank", ""]]}, {"id": "1401.3477", "submitter": "Jos\\'e Enrique Gallardo", "authors": "Jos\\'e Enrique Gallardo, Carlos Cotta, Antonio Jos\\'e Fern\\'andez", "title": "Solving Weighted Constraint Satisfaction Problems with Memetic/Exact\n  Hybrid Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:0812.4170", "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  533-555, 2009", "doi": "10.1613/jair.2770", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weighted constraint satisfaction problem (WCSP) is a constraint\nsatisfaction problem in which preferences among solutions can be expressed.\nBucket elimination is a complete technique commonly used to solve this kind of\nconstraint satisfaction problem. When the memory required to apply bucket\nelimination is too high, a heuristic method based on it (denominated\nmini-buckets) can be used to calculate bounds for the optimal solution.\nNevertheless, the curse of dimensionality makes these techniques impractical on\nlarge scale problems. In response to this situation, we present a memetic\nalgorithm for WCSPs in which bucket elimination is used as a mechanism for\nrecombining solutions, providing the best possible child from the parental set.\nSubsequently, a multi-level model in which this exact/metaheuristic hybrid is\nfurther hybridized with branch-and-bound techniques and mini-buckets is\nstudied. As a case study, we have applied these algorithms to the resolution of\nthe maximum density still life problem, a hard constraint optimization problem\nbased on Conways game of life. The resulting algorithm consistently finds\noptimal patterns for up to date solved instances in less time than current\napproaches. Moreover, it is shown that this proposal provides new best known\nsolutions for very large instances.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:32:38 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Gallardo", "Jos\u00e9 Enrique", ""], ["Cotta", "Carlos", ""], ["Fern\u00e1ndez", "Antonio Jos\u00e9", ""]]}, {"id": "1401.3478", "submitter": "Facundo Bromberg", "authors": "Facundo Bromberg, Dimitris Margaritis, Vasant Honavar", "title": "Efficient Markov Network Structure Discovery Using Independence Tests", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  449-484, 2009", "doi": "10.1613/jair.2773", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two algorithms for learning the structure of a Markov network from\ndata: GSMN* and GSIMN. Both algorithms use statistical independence tests to\ninfer the structure by successively constraining the set of structures\nconsistent with the results of these tests. Until very recently, algorithms for\nstructure learning were based on maximum likelihood estimation, which has been\nproved to be NP-hard for Markov networks due to the difficulty of estimating\nthe parameters of the network, needed for the computation of the data\nlikelihood. The independence-based approach does not require the computation of\nthe likelihood, and thus both GSMN* and GSIMN can compute the structure\nefficiently (as shown in our experiments). GSMN* is an adaptation of the\nGrow-Shrink algorithm of Margaritis and Thrun for learning the structure of\nBayesian networks. GSIMN extends GSMN* by additionally exploiting Pearls\nwell-known properties of the conditional independence relation to infer novel\nindependences from known ones, thus avoiding the performance of statistical\ntests to estimate them. To accomplish this efficiently GSIMN uses the Triangle\ntheorem, also introduced in this work, which is a simplified version of the set\nof Markov axioms. Experimental comparisons on artificial and real-world data\nsets show GSIMN can yield significant savings with respect to GSMN*, while\ngenerating a Markov network with comparable or in some cases improved quality.\nWe also compare GSIMN to a forward-chaining implementation, called GSIMN-FCH,\nthat produces all possible conditional independences resulting from repeatedly\napplying Pearls theorems on the known conditional independence tests. The\nresults of this comparison show that GSIMN, by the sole use of the Triangle\ntheorem, is nearly optimal in terms of the set of independences tests that it\ninfers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:33:29 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Bromberg", "Facundo", ""], ["Margaritis", "Dimitris", ""], ["Honavar", "Vasant", ""]]}, {"id": "1401.3481", "submitter": "Matthias Zytnicki", "authors": "Matthias Zytnicki, Christine Gaspin, Simon de Givry, Thomas Schiex", "title": "Bounds Arc Consistency for Weighted CSPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  593-621, 2009", "doi": "10.1613/jair.2797", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Weighted Constraint Satisfaction Problem (WCSP) framework allows\nrepresenting and solving problems involving both hard constraints and cost\nfunctions. It has been applied to various problems, including resource\nallocation, bioinformatics, scheduling, etc. To solve such problems, solvers\nusually rely on branch-and-bound algorithms equipped with local consistency\nfiltering, mostly soft arc consistency. However, these techniques are not well\nsuited to solve problems with very large domains. Motivated by the resolution\nof an RNA gene localization problem inside large genomic sequences, and in the\nspirit of bounds consistency for large domains in crisp CSPs, we introduce soft\nbounds arc consistency, a new weighted local consistency specifically designed\nfor WCSP with very large domains. Compared to soft arc consistency, BAC\nprovides significantly improved time and space asymptotic complexity. In this\npaper, we show how the semantics of cost functions can be exploited to further\nimprove the time complexity of BAC. We also compare both in theory and in\npractice the efficiency of BAC on a WCSP with bounds consistency enforced on a\ncrisp CSP using cost variables. On two different real problems modeled as WCSP,\nincluding our RNA gene localization problem, we observe that maintaining bounds\narc consistency outperforms arc consistency and also improves over bounds\nconsistency enforced on a constraint model with cost variables.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:34:30 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Zytnicki", "Matthias", ""], ["Gaspin", "Christine", ""], ["de Givry", "Simon", ""], ["Schiex", "Thomas", ""]]}, {"id": "1401.3482", "submitter": "Estela Saquete", "authors": "Estela Saquete, Jose Luis Vicedo, Patricio Mart\\'inez-Barco, Rafael\n  Mu\\~noz, Hector Llorens", "title": "Enhancing QA Systems with Complex Temporal Question Processing\n  Capabilities", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  775-811, 2009", "doi": "10.1613/jair.2805", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multilayered architecture that enhances the\ncapabilities of current QA systems and allows different types of complex\nquestions or queries to be processed. The answers to these questions need to be\ngathered from factual information scattered throughout different documents.\nSpecifically, we designed a specialized layer to process the different types of\ntemporal questions. Complex temporal questions are first decomposed into simple\nquestions, according to the temporal relations expressed in the original\nquestion. In the same way, the answers to the resulting simple questions are\nrecomposed, fulfilling the temporal restrictions of the original complex\nquestion. A novel aspect of this approach resides in the decomposition which\nuses a minimal quantity of resources, with the final aim of obtaining a\nportable platform that is easily extensible to other languages. In this paper\nwe also present a methodology for evaluation of the decomposition of the\nquestions as well as the ability of the implemented temporal layer to perform\nat a multilingual level. The temporal layer was first performed for English,\nthen evaluated and compared with: a) a general purpose QA system (F-measure\n65.47% for QA plus English temporal layer vs. 38.01% for the general QA\nsystem), and b) a well-known QA system. Much better results were obtained for\ntemporal questions with the multilayered system. This system was therefore\nextended to Spanish and very good results were again obtained in the evaluation\n(F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the general\nQA system).\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:35:49 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Saquete", "Estela", ""], ["Vicedo", "Jose Luis", ""], ["Mart\u00ednez-Barco", "Patricio", ""], ["Mu\u00f1oz", "Rafael", ""], ["Llorens", "Hector", ""]]}, {"id": "1401.3483", "submitter": "Hai Leong Chieu", "authors": "Hai Leong Chieu, Wee Sun Sun Lee", "title": "Relaxed Survey Propagation for The Weighted Maximum Satisfiability\n  Problem", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  229-266, 2009", "doi": "10.1613/jair.2808", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The survey propagation (SP) algorithm has been shown to work well on large\ninstances of the random 3-SAT problem near its phase transition. It was shown\nthat SP estimates marginals over covers that represent clusters of solutions.\nThe SP-y algorithm generalizes SP to work on the maximum satisfiability\n(Max-SAT) problem, but the cover interpretation of SP does not generalize to\nSP-y. In this paper, we formulate the relaxed survey propagation (RSP)\nalgorithm, which extends the SP algorithm to apply to the weighted Max-SAT\nproblem. We show that RSP has an interpretation of estimating marginals over\ncovers violating a set of clauses with minimal weight. This naturally\ngeneralizes the cover interpretation of SP. Empirically, we show that RSP\noutperforms SP-y and other state-of-the-art Max-SAT solvers on random Max-SAT\ninstances. RSP also outperforms state-of-the-art weighted Max-SAT solvers on\nrandom weighted Max-SAT instances.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:36:10 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Chieu", "Hai Leong", ""], ["Lee", "Wee Sun Sun", ""]]}, {"id": "1401.3484", "submitter": "Tomi Janhunen", "authors": "Tomi Janhunen, Emilia Oikarinen, Hans Tompits, Stefan Woltran", "title": "Modularity Aspects of Disjunctive Stable Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  813-857, 2009", "doi": "10.1613/jair.2810", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practically all programming languages allow the programmer to split a program\ninto several modules which brings along several advantages in software\ndevelopment. In this paper, we are interested in the area of answer-set\nprogramming where fully declarative and nonmonotonic languages are applied. In\nthis context, obtaining a modular structure for programs is by no means\nstraightforward since the output of an entire program cannot in general be\ncomposed from the output of its components. To better understand the effects of\ndisjunctive information on modularity we restrict the scope of analysis to the\ncase of disjunctive logic programs (DLPs) subject to stable-model semantics. We\ndefine the notion of a DLP-function, where a well-defined input/output\ninterface is provided, and establish a novel module theorem which indicates the\ncompositionality of stable-model semantics for DLP-functions. The module\ntheorem extends the well-known splitting-set theorem and enables the\ndecomposition of DLP-functions given their strongly connected components based\non positive dependencies induced by rules. In this setting, it is also possible\nto split shared disjunctive rules among components using a generalized shifting\ntechnique. The concept of modular equivalence is introduced for the mutual\ncomparison of DLP-functions using a generalization of a translation-based\nverification method.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:36:41 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Janhunen", "Tomi", ""], ["Oikarinen", "Emilia", ""], ["Tompits", "Hans", ""], ["Woltran", "Stefan", ""]]}, {"id": "1401.3485", "submitter": "Boris Motik", "authors": "Boris Motik, Rob Shearer, Ian Horrocks", "title": "Hypertableau Reasoning for Description Logics", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  165-228, 2009", "doi": "10.1613/jair.2811", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel reasoning calculus for the description logic SHOIQ^+---a\nknowledge representation formalism with applications in areas such as the\nSemantic Web. Unnecessary nondeterminism and the construction of large models\nare two primary sources of inefficiency in the tableau-based reasoning calculi\nused in state-of-the-art reasoners. In order to reduce nondeterminism, we base\nour calculus on hypertableau and hyperresolution calculi, which we extend with\na blocking condition to ensure termination. In order to reduce the size of the\nconstructed models, we introduce anywhere pairwise blocking. We also present an\nimproved nominal introduction rule that ensures termination in the presence of\nnominals, inverse roles, and number restrictions---a combination of DL\nconstructs that has proven notoriously difficult to handle. Our implementation\nshows significant performance improvements over state-of-the-art reasoners on\nseveral well-known ontologies.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:37:06 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Motik", "Boris", ""], ["Shearer", "Rob", ""], ["Horrocks", "Ian", ""]]}, {"id": "1401.3486", "submitter": "Anders Jonsson", "authors": "Anders Jonsson", "title": "The Role of Macros in Tractable Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  471-511, 2009", "doi": "10.1613/jair.2891", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents several new tractability results for planning based on\nmacros. We describe an algorithm that optimally solves planning problems in a\nclass that we call inverted tree reducible, and is provably tractable for\nseveral subclasses of this class. By using macros to store partial plans that\nrecur frequently in the solution, the algorithm is polynomial in time and space\neven for exponentially long plans. We generalize the inverted tree reducible\nclass in several ways and describe modifications of the algorithm to deal with\nthese new classes. Theoretical results are validated in experiments.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:37:49 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Jonsson", "Anders", ""]]}, {"id": "1401.3487", "submitter": "Alessandro  Artale", "authors": "Alessandro Artale, Diego Calvanese, Roman Kontchakov, Michael\n  Zakharyaschev", "title": "The DL-Lite Family and Relations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  1-69, 2009", "doi": "10.1613/jair.2820", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced series of description logics under the common moniker\nDL-Lite has attracted attention of the description logic and semantic web\ncommunities due to the low computational complexity of inference, on the one\nhand, and the ability to represent conceptual modeling formalisms, on the\nother. The main aim of this article is to carry out a thorough and systematic\ninvestigation of inference in extensions of the original DL-Lite logics along\nfive axes: by (i) adding the Boolean connectives and (ii) number restrictions\nto concept constructs, (iii) allowing role hierarchies, (iv) allowing role\ndisjointness, symmetry, asymmetry, reflexivity, irreflexivity and transitivity\nconstraints, and (v) adopting or dropping the unique same assumption. We\nanalyze the combined complexity of satisfiability for the resulting logics, as\nwell as the data complexity of instance checking and answering positive\nexistential queries. Our approach is based on embedding DL-Lite logics in\nsuitable fragments of the one-variable first-order logic, which provides useful\ninsights into their properties and, in particular, computational behavior.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:37:57 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Artale", "Alessandro", ""], ["Calvanese", "Diego", ""], ["Kontchakov", "Roman", ""], ["Zakharyaschev", "Michael", ""]]}, {"id": "1401.3489", "submitter": "Robert Mateescu", "authors": "Robert Mateescu, Kalev Kask, Vibhav Gogate, Rina Dechter", "title": "Join-Graph Propagation Algorithms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 37, pages\n  279-328, 2010", "doi": "10.1613/jair.2842", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper investigates parameterized approximate message-passing schemes that\nare based on bounded inference and are inspired by Pearl's belief propagation\nalgorithm (BP). We start with the bounded inference mini-clustering algorithm\nand then move to the iterative scheme called Iterative Join-Graph Propagation\n(IJGP), that combines both iteration and bounded inference. Algorithm IJGP\nbelongs to the class of Generalized Belief Propagation algorithms, a framework\nthat allowed connections with approximate algorithms from statistical physics\nand is shown empirically to surpass the performance of mini-clustering and\nbelief propagation, as well as a number of other state-of-the-art algorithms on\nseveral classes of networks. We also provide insight into the accuracy of\niterative BP and IJGP by relating these algorithms to well known classes of\nconstraint propagation schemes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:38:39 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Mateescu", "Robert", ""], ["Kask", "Kalev", ""], ["Gogate", "Vibhav", ""], ["Dechter", "Rina", ""]]}, {"id": "1401.3490", "submitter": "William Yeoh", "authors": "William Yeoh, Ariel Felner, Sven Koenig", "title": "BnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  85-133, 2010", "doi": "10.1613/jair.2849", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed constraint optimization (DCOP) problems are a popular way of\nformulating and solving agent-coordination problems. A DCOP problem is a\nproblem where several agents coordinate their values such that the sum of the\nresulting constraint costs is minimal. It is often desirable to solve DCOP\nproblems with memory-bounded and asynchronous algorithms. We introduce\nBranch-and-Bound ADOPT (BnB-ADOPT), a memory-bounded asynchronous DCOP search\nalgorithm that uses the message-passing and communication framework of ADOPT\n(Modi, Shen, Tambe, and Yokoo, 2005), a well known memory-bounded asynchronous\nDCOP search algorithm, but changes the search strategy of ADOPT from best-first\nsearch to depth-first branch-and-bound search. Our experimental results show\nthat BnB-ADOPT finds cost-minimal solutions up to one order of magnitude faster\nthan ADOPT for a variety of large DCOP problems and is as fast as NCBB, a\nmemory-bounded synchronous DCOP search algorithm, for most of these DCOP\nproblems. Additionally, it is often desirable to find bounded-error solutions\nfor DCOP problems within a reasonable amount of time since finding cost-minimal\nsolutions is NP-hard. The existing bounded-error approximation mechanism allows\nusers only to specify an absolute error bound on the solution cost but a\nrelative error bound is often more intuitive. Thus, we present two new\nbounded-error approximation mechanisms that allow for relative error bounds and\nimplement them on top of BnB-ADOPT.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:39:26 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Yeoh", "William", ""], ["Felner", "Ariel", ""], ["Koenig", "Sven", ""]]}, {"id": "1401.3491", "submitter": "Emil Keyder", "authors": "Emil Keyder, Hector Geffner", "title": "Soft Goals Can Be Compiled Away", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  547-556, 2009", "doi": "10.1613/jair.2857", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soft goals extend the classical model of planning with a simple model of\npreferences. The best plans are then not the ones with least cost but the ones\nwith maximum utility, where the utility of a plan is the sum of the utilities\nof the soft goals achieved minus the plan cost. Finding plans with high utility\nappears to involve two linked problems: choosing a subset of soft goals to\nachieve and finding a low-cost plan to achieve them. New search algorithms and\nheuristics have been developed for planning with soft goals, and a new track\nhas been introduced in the International Planning Competition (IPC) to test\ntheir performance. In this note, we show however that these extensions are not\nneeded: soft goals do not increase the expressive power of the basic model of\nplanning with action costs, as they can easily be compiled away. We apply this\ncompilation to the problems of the net-benefit track of the most recent IPC,\nand show that optimal and satisficing cost-based planners do better on the\ncompiled problems than optimal and satisficing net-benefit planners on the\noriginal problems with explicit soft goals. Furthermore, we show that\npenalties, or negative preferences expressing conditions to avoid, can also be\ncompiled away using a similar idea.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:39:49 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Keyder", "Emil", ""], ["Geffner", "Hector", ""]]}, {"id": "1401.3492", "submitter": "Frank  Hutter", "authors": "Frank Hutter, Thomas Stuetzle, Kevin Leyton-Brown, Holger H. Hoos", "title": "ParamILS: An Automatic Algorithm Configuration Framework", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  267-306, 2009", "doi": "10.1613/jair.2861", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of performance-optimizing parameter settings is an\nimportant part of the development and application of algorithms. We describe an\nautomatic framework for this algorithm configuration problem. More formally, we\nprovide methods for optimizing a target algorithm's performance on a given\nclass of problem instances by varying a set of ordinal and/or categorical\nparameters. We review a family of local-search-based algorithm configuration\nprocedures and present novel techniques for accelerating them by adaptively\nlimiting the time spent for evaluating individual configurations. We describe\nthe results of a comprehensive experimental evaluation of our methods, based on\nthe configuration of prominent complete and incomplete algorithms for SAT. We\nalso present what is, to our knowledge, the first published work on\nautomatically configuring the CPLEX mixed integer programming solver. All the\nalgorithms we considered had default parameter settings that were manually\nidentified with considerable effort. Nevertheless, using our automated\nalgorithm configuration procedures, we achieved substantial and consistent\nperformance improvements.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:40:11 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Hutter", "Frank", ""], ["Stuetzle", "Thomas", ""], ["Leyton-Brown", "Kevin", ""], ["Hoos", "Holger H.", ""]]}, {"id": "1401.3493", "submitter": "Uzi Zahavi", "authors": "Uzi Zahavi, Ariel Felner, Neil Burch, Robert C. Holte", "title": "Predicting the Performance of IDA* using Conditional Distributions", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 37, pages\n  41-83, 2010", "doi": "10.1613/jair.2890", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Korf, Reid, and Edelkamp introduced a formula to predict the number of nodes\nIDA* will expand on a single iteration for a given consistent heuristic, and\nexperimentally demonstrated that it could make very accurate predictions. In\nthis paper we show that, in addition to requiring the heuristic to be\nconsistent, their formulas predictions are accurate only at levels of the\nbrute-force search tree where the heuristic values obey the unconditional\ndistribution that they defined and then used in their formula. We then propose\na new formula that works well without these requirements, i.e., it can make\naccurate predictions of IDA*s performance for inconsistent heuristics and if\nthe heuristic values in any level do not obey the unconditional distribution.\nIn order to achieve this we introduce the conditional distribution of heuristic\nvalues which is a generalization of their unconditional heuristic distribution.\nWe also provide extensions of our formula that handle individual start states\nand the augmentation of IDA* with bidirectional pathmax (BPMX), a technique for\npropagating heuristic values when inconsistent heuristics are used.\nExperimental results demonstrate the accuracy of our new method and all its\nvariations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:41:44 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Zahavi", "Uzi", ""], ["Felner", "Ariel", ""], ["Burch", "Neil", ""], ["Holte", "Robert C.", ""]]}, {"id": "1401.3531", "submitter": "Ben Fulcher", "authors": "Ben D. Fulcher and Nick S. Jones", "title": "Highly comparative feature-based time-series classification", "comments": null, "journal-ref": "IEEE Trans. Knowl. Data Eng. 26, 3026 (2014)", "doi": "10.1109/TKDE.2014.2316504", "report-no": null, "categories": "cs.LG cs.AI cs.DB physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A highly comparative, feature-based approach to time series classification is\nintroduced that uses an extensive database of algorithms to extract thousands\nof interpretable features from time series. These features are derived from\nacross the scientific time-series analysis literature, and include summaries of\ntime series in terms of their correlation structure, distribution, entropy,\nstationarity, scaling properties, and fits to a range of time-series models.\nAfter computing thousands of features for each time series in a training set,\nthose that are most informative of the class structure are selected using\ngreedy forward feature selection with a linear classifier. The resulting\nfeature-based classifiers automatically learn the differences between classes\nusing a reduced number of time-series properties, and circumvent the need to\ncalculate distances between time series. Representing time series in this way\nresults in orders of magnitude of dimensionality reduction, allowing the method\nto perform well on very large datasets containing long time series or time\nseries of different lengths. For many of the datasets studied, classification\nperformance exceeded that of conventional instance-based classifiers, including\none nearest neighbor classifiers using Euclidean distances and dynamic time\nwarping and, most importantly, the features selected provide an understanding\nof the properties of the dataset, insight that can guide further scientific\ninvestigation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 09:41:50 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 00:05:57 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Fulcher", "Ben D.", ""], ["Jones", "Nick S.", ""]]}, {"id": "1401.3579", "submitter": "Keyvan Yahya", "authors": "Keyvan Yahya", "title": "A Supervised Goal Directed Algorithm in Economical Choice Behaviour: An\n  Actor-Critic Approach", "comments": "Must be more flourished and fostered", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to find an algorithmic structure that affords to predict and\nexplain economical choice behaviour particularly under uncertainty(random\npolicies) by manipulating the prevalent Actor-Critic learning method to comply\nwith the requirements we have been entrusted ever since the field of\nneuroeconomics dawned on us. Whilst skimming some basics of neuroeconomics that\nseem relevant to our discussion, we will try to outline some of the important\nworks which have so far been done to simulate choice making processes.\nConcerning neurological findings that suggest the existence of two specific\nfunctions that are executed through Basal Ganglia all the way up to sub-\ncortical areas, namely 'rewards' and 'beliefs', we will offer a modified\nversion of actor/critic algorithm to shed a light on the relation between these\nfunctions and most importantly resolve what is referred to as a challenge for\nactor-critic algorithms, that is, the lack of inheritance or hierarchy which\navoids the system being evolved in continuous time tasks whence the convergence\nmight not be emerged.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 05:54:58 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2014 19:15:53 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 01:47:30 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yahya", "Keyvan", ""]]}, {"id": "1401.3626", "submitter": "Diederik Aerts", "authors": "Diederik Aerts and Sandro Sozzo", "title": "Modeling Concept Combinations in a Quantum-theoretic Framework", "comments": "5 pages. arXiv admin note: substantial text overlap with\n  arXiv:1311.6050", "journal-ref": "Advances in Cognitive Neurodynamics (IV), pp 393-399, 2014", "doi": "10.1007/978-94-017-9548-7_55", "report-no": null, "categories": "cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present modeling for conceptual combinations which uses the mathematical\nformalism of quantum theory. Our model faithfully describes a large amount of\nexperimental data collected by different scholars on concept conjunctions and\ndisjunctions. Furthermore, our approach sheds a new light on long standing\ndrawbacks connected with vagueness, or fuzziness, of concepts, and puts forward\na completely novel possible solution to the 'combination problem' in concept\ntheory. Additionally, we introduce an explanation for the occurrence of quantum\nstructures in the mechanisms and dynamics of concepts and, more generally, in\ncognitive and decision processes, according to which human thought is a well\nstructured superposition of a 'logical thought' and a 'conceptual thought', and\nthe latter usually prevails over the former, at variance with some widespread\nbeliefs\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 15:27:04 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Aerts", "Diederik", ""], ["Sozzo", "Sandro", ""]]}, {"id": "1401.3825", "submitter": "Wiebe van der Hoek", "authors": "Wiebe van der Hoek, Dirk Walther, Michael Wooldridge", "title": "Reasoning About the Transfer of Control", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 37, pages\n  437-477, 2010", "doi": "10.1613/jair.2901", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DCL-PC: a logic for reasoning about how the abilities of agents\nand coalitions of agents are altered by transferring control from one agent to\nanother. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about\ncooperation in which the abilities of agents and coalitions of agents stem from\na distribution of atomic Boolean variables to individual agents -- the choices\navailable to a coalition correspond to assignments to the variables the\ncoalition controls. The basic modal constructs of DCL-PC are of the form\ncoalition C can cooperate to bring about phi. DCL-PC extends CL-PC with dynamic\nlogic modalities in which atomic programs are of the form agent i gives control\nof variable p to agent j; as usual in dynamic logic, these atomic programs may\nbe combined using sequence, iteration, choice, and test operators to form\ncomplex programs. By combining such dynamic transfer programs with cooperation\nmodalities, it becomes possible to reason about how the power of agents and\ncoalitions is affected by the transfer of control. We give two alternative\nsemantics for the logic: a direct semantics, in which we capture the\ndistributions of Boolean variables to agents; and a more conventional Kripke\nsemantics. We prove that these semantics are equivalent, and then present an\naxiomatization for the logic. We investigate the computational complexity of\nmodel checking and satisfiability for DCL-PC, and show that both problems are\nPSPACE-complete (and hence no worse than the underlying logic CL-PC). Finally,\nwe investigate the characterisation of control in DCL-PC. We distinguish\nbetween first-order control -- the ability of an agent or coalition to control\nsome state of affairs through the assignment of values to the variables under\nthe control of the agent or coalition -- and second-order control -- the\nability of an agent to exert control over the control that other agents have by\ntransferring variables to other agents. We give a logical characterisation of\nsecond-order control.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:27:53 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["van der Hoek", "Wiebe", ""], ["Walther", "Dirk", ""], ["Wooldridge", "Michael", ""]]}, {"id": "1401.3827", "submitter": "Ruijie He", "authors": "Ruijie He, Emma Brunskill, Nicholas Roy", "title": "Efficient Planning under Uncertainty with Macro-actions", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  523-570, 2011", "doi": "10.1613/jair.3171", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciding how to act in partially observable environments remains an active\narea of research. Identifying good sequences of decisions is particularly\nchallenging when good control performance requires planning multiple steps into\nthe future in domains with many states. Towards addressing this challenge, we\npresent an online, forward-search algorithm called the Posterior Belief\nDistribution (PBD). PBD leverages a novel method for calculating the posterior\ndistribution over beliefs that result after a sequence of actions is taken,\ngiven the set of observation sequences that could be received during this\nprocess. This method allows us to efficiently evaluate the expected reward of a\nsequence of primitive actions, which we refer to as macro-actions. We present a\nformal analysis of our approach, and examine its performance on two very large\nsimulation experiments: scientific exploration and a target monitoring domain.\nWe also demonstrate our algorithm being used to control a real robotic\nhelicopter in a target monitoring experiment, which suggests that our approach\nhas practical potential for planning in real-world, large partially observable\ndomains where a multi-step lookahead is required to achieve good performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:36:09 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["He", "Ruijie", ""], ["Brunskill", "Emma", ""], ["Roy", "Nicholas", ""]]}, {"id": "1401.3830", "submitter": "Henrik Reif Andersen", "authors": "Henrik Reif Andersen, Tarik Hadzic, David Pisinger", "title": "Interactive Cost Configuration Over Decision Diagrams", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 37, pages\n  99-139, 2010", "doi": "10.1613/jair.2905", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many AI domains such as product configuration, a user should interactively\nspecify a solution that must satisfy a set of constraints. In such scenarios,\noffline compilation of feasible solutions into a tractable representation is an\nimportant approach to delivering efficient backtrack-free user interaction\nonline. In particular,binary decision diagrams (BDDs) have been successfully\nused as a compilation target for product and service configuration. In this\npaper we discuss how to extend BDD-based configuration to scenarios involving\ncost functions which express user preferences.\n  We first show that an efficient, robust and easy to implement extension is\npossible if the cost function is additive, and feasible solutions are\nrepresented using multi-valued decision diagrams (MDDs). We also discuss the\neffect on MDD size if the cost function is non-additive or if it is encoded\nexplicitly into MDD. We then discuss interactive configuration in the presence\nof multiple cost functions. We prove that even in its simplest form,\nmultiple-cost configuration is NP-hard in the input MDD. However, for solving\ntwo-cost configuration we develop a pseudo-polynomial scheme and a fully\npolynomial approximation scheme. The applicability of our approach is\ndemonstrated through experiments over real-world configuration models and\nproduct-catalogue datasets. Response times are generally within a fraction of a\nsecond even for very large instances.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:48:15 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Andersen", "Henrik Reif", ""], ["Hadzic", "Tarik", ""], ["Pisinger", "David", ""]]}, {"id": "1401.3831", "submitter": "Raghav Aras", "authors": "Raghav Aras, Alain Dutech", "title": "An Investigation into Mathematical Programming for Finite Horizon\n  Decentralized POMDPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 37, pages\n  329-396, 2010", "doi": "10.1613/jair.2915", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized planning in uncertain environments is a complex task generally\ndealt with by using a decision-theoretic approach, mainly through the framework\nof Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs).\nAlthough DEC-POMDPS are a general and powerful modeling tool, solving them is a\ntask with an overwhelming complexity that can be doubly exponential. In this\npaper, we study an alternate formulation of DEC-POMDPs relying on a\nsequence-form representation of policies. From this formulation, we show how to\nderive Mixed Integer Linear Programming (MILP) problems that, once solved, give\nexact optimal solutions to the DEC-POMDPs. We show that these MILPs can be\nderived either by using some combinatorial characteristics of the optimal\nsolutions of the DEC-POMDPs or by using concepts borrowed from game theory.\nThrough an experimental validation on classical test problems from the\nDEC-POMDP literature, we compare our approach to existing algorithms. Results\nshow that mathematical programming outperforms dynamic programming but is less\nefficient than forward search, except for some particular problems. The main\ncontributions of this work are the use of mathematical programming for\nDEC-POMDPs and a better understanding of DEC-POMDPs and of their solutions.\nBesides, we argue that our alternate representation of DEC-POMDPs could be\nhelpful for designing novel algorithms looking for approximate solutions to\nDEC-POMDPs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:49:14 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Aras", "Raghav", ""], ["Dutech", "Alain", ""]]}, {"id": "1401.3833", "submitter": "Bozhena Bidyuk", "authors": "Bozhena Bidyuk, Rina Dechter, Emma Rollon", "title": "Active Tuples-based Scheme for Bounding Posterior Beliefs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  335-371, 2010", "doi": "10.1613/jair.2945", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a scheme for computing lower and upper bounds on the\nposterior marginals in Bayesian networks with discrete variables. Its power\nlies in its ability to use any available scheme that bounds the probability of\nevidence or posterior marginals and enhance its performance in an anytime\nmanner. The scheme uses the cutset conditioning principle to tighten existing\nbounding schemes and to facilitate anytime behavior, utilizing a fixed number\nof cutset tuples. The accuracy of the bounds improves as the number of used\ncutset tuples increases and so does the computation time. We demonstrate\nempirically the value of our scheme for bounding posterior marginals and\nprobability of evidence using a variant of the bound propagation algorithm as a\nplug-in scheme.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:50:19 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Bidyuk", "Bozhena", ""], ["Dechter", "Rina", ""], ["Rollon", "Emma", ""]]}, {"id": "1401.3835", "submitter": "Ivan Jos\\'e Varzinczak", "authors": "Ivan Jos\\'e Varzinczak", "title": "On Action Theory Change", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 37, pages\n  189-246, 2010", "doi": "10.1613/jair.2959", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As historically acknowledged in the Reasoning about Actions and Change\ncommunity, intuitiveness of a logical domain description cannot be fully\nautomated. Moreover, like any other logical theory, action theories may also\nevolve, and thus knowledge engineers need revision methods to help in\naccommodating new incoming information about the behavior of actions in an\nadequate manner. The present work is about changing action domain descriptions\nin multimodal logic. Its contribution is threefold: first we revisit the\nsemantics of action theory contraction proposed in previous work, giving more\nrobust operators that express minimal change based on a notion of distance\nbetween Kripke-models. Second we give algorithms for syntactical action theory\ncontraction and establish their correctness with respect to our semantics for\nthose action theories that satisfy a principle of modularity investigated in\nprevious work. Since modularity can be ensured for every action theory and, as\nwe show here, needs to be computed at most once during the evolution of a\ndomain description, it does not represent a limitation at all to the method\nhere studied. Finally we state AGM-like postulates for action theory\ncontraction and assess the behavior of our operators with respect to them.\nMoreover, we also address the revision counterpart of action theory change,\nshowing that it benefits from our semantics for contraction.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:51:08 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Varzinczak", "Ivan Jos\u00e9", ""]]}, {"id": "1401.3838", "submitter": "Claudette Cayrol", "authors": "Claudette Cayrol, Florence Dupin de Saint-Cyr, Marie-Christine\n  Lagasquie-Schiex", "title": "Change in Abstract Argumentation Frameworks: Adding an Argument", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  49-84, 2010", "doi": "10.1613/jair.2965", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of change in an abstract argumentation\nsystem. We focus on a particular change: the addition of a new argument which\ninteracts with previous arguments. We study the impact of such an addition on\nthe outcome of the argumentation system, more particularly on the set of its\nextensions. Several properties for this change operation are defined by\ncomparing the new set of extensions to the initial one, these properties are\ncalled structural when the comparisons are based on set-cardinality or\nset-inclusion relations. Several other properties are proposed where\ncomparisons are based on the status of some particular arguments: the accepted\narguments; these properties refer to the evolution of this status during the\nchange, e.g., Monotony and Priority to Recency. All these properties may be\nmore or less desirable according to specific applications. They are studied\nunder two particular semantics: the grounded and preferred semantics.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:52:08 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Cayrol", "Claudette", ""], ["de Saint-Cyr", "Florence Dupin", ""], ["Lagasquie-Schiex", "Marie-Christine", ""]]}, {"id": "1401.3839", "submitter": "Silvia Richter", "authors": "Silvia Richter, Matthias Westphal", "title": "The LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  127-177, 2010", "doi": "10.1613/jair.2972", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LAMA is a classical planning system based on heuristic forward search. Its\ncore feature is the use of a pseudo-heuristic derived from landmarks,\npropositional formulas that must be true in every solution of a planning task.\nLAMA builds on the Fast Downward planning system, using finite-domain rather\nthan binary state variables and multi-heuristic search. The latter is employed\nto combine the landmark heuristic with a variant of the well-known FF\nheuristic. Both heuristics are cost-sensitive, focusing on high-quality\nsolutions in the case where actions have non-uniform cost. A weighted A* search\nis used with iteratively decreasing weights, so that the planner continues to\nsearch for plans of better quality until the search is terminated. LAMA showed\nbest performance among all planners in the sequential satisficing track of the\nInternational Planning Competition 2008. In this paper we present the system in\ndetail and investigate which features of LAMA are crucial for its performance.\nWe present individual results for some of the domains used at the competition,\ndemonstrating good and bad cases for the techniques implemented in LAMA.\nOverall, we find that using landmarks improves performance, whereas the\nincorporation of action costs into the heuristic estimators proves not to be\nbeneficial. We show that in some domains a search that ignores cost solves far\nmore problems, raising the question of how to deal with action costs more\neffectively in the future. The iterated weighted A* search greatly improves\nresults, and shows synergy effects with the use of landmarks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:52:55 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Richter", "Silvia", ""], ["Westphal", "Matthias", ""]]}, {"id": "1401.3840", "submitter": "Johan Wittocx", "authors": "Johan Wittocx, Maarten Mari\\\"en, Marc Denecker", "title": "Grounding FO and FO(ID) with Bounds", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  223-269, 2010", "doi": "10.1613/jair.2980", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding is the task of reducing a first-order theory and finite domain to\nan equivalent propositional theory. It is used as preprocessing phase in many\nlogic-based reasoning systems. Such systems provide a rich first-order input\nlanguage to a user and can rely on efficient propositional solvers to perform\nthe actual reasoning. Besides a first-order theory and finite domain, the input\nfor grounders contains in many applications also additional data. By exploiting\nthis data, the size of the grounders output can often be reduced significantly.\nA common practice to improve the efficiency of a grounder in this context is by\nmanually adding semantically redundant information to the input theory,\nindicating where and when the grounder should exploit the data. In this paper\nwe present a method to compute and add such redundant information\nautomatically. Our method therefore simplifies the task of writing input\ntheories that can be grounded efficiently by current systems. We first present\nour method for classical first-order logic (FO) theories. Then we extend it to\nFO(ID), the extension of FO with inductive definitions, which allows for more\nconcise and comprehensive input theories. We discuss implementation issues and\nexperimentally validate the practical applicability of our method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:53:20 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Wittocx", "Johan", ""], ["Mari\u00ebn", "Maarten", ""], ["Denecker", "Marc", ""]]}, {"id": "1401.3841", "submitter": "Mark Owen Riedl", "authors": "Mark Owen Riedl, Robert Michael Young", "title": "Narrative Planning: Balancing Plot and Character", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  217-268, 2010", "doi": "10.1613/jair.2989", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Narrative, and in particular storytelling, is an important part of the human\nexperience. Consequently, computational systems that can reason about narrative\ncan be more effective communicators, entertainers, educators, and trainers. One\nof the central challenges in computational narrative reasoning is narrative\ngeneration, the automated creation of meaningful event sequences. There are\nmany factors -- logical and aesthetic -- that contribute to the success of a\nnarrative artifact. Central to this success is its understandability. We argue\nthat the following two attributes of narratives are universal: (a) the logical\ncausal progression of plot, and (b) character believability. Character\nbelievability is the perception by the audience that the actions performed by\ncharacters do not negatively impact the audiences suspension of disbelief.\nSpecifically, characters must be perceived by the audience to be intentional\nagents. In this article, we explore the use of refinement search as a technique\nfor solving the narrative generation problem -- to find a sound and believable\nsequence of character actions that transforms an initial world state into a\nworld state in which goal propositions hold. We describe a novel refinement\nsearch planning algorithm -- the Intent-based Partial Order Causal Link (IPOCL)\nplanner -- that, in addition to creating causally sound plot progression,\nreasons about character intentionality by identifying possible character goals\nthat explain their actions and creating plan structures that explain why those\ncharacters commit to their goals. We present the results of an empirical\nevaluation that demonstrates that narrative plans generated by the IPOCL\nalgorithm support audience comprehension of character intentions better than\nplans generated by conventional partial-order planners.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:54:07 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Riedl", "Mark Owen", ""], ["Young", "Robert Michael", ""]]}, {"id": "1401.3842", "submitter": "David Lesaint", "authors": "David Lesaint, Deepak Mehta, Barry O'Sullivan, Luis Quesada, Nic\n  Wilson", "title": "Developing Approaches for Solving a Telecommunications Feature\n  Subscription Problem", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  271-305, 2010", "doi": "10.1613/jair.2992", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Call control features (e.g., call-divert, voice-mail) are primitive options\nto which users can subscribe off-line to personalise their service. The\nconfiguration of a feature subscription involves choosing and sequencing\nfeatures from a catalogue and is subject to constraints that prevent\nundesirable feature interactions at run-time. When the subscription requested\nby a user is inconsistent, one problem is to find an optimal relaxation, which\nis a generalisation of the feedback vertex set problem on directed graphs, and\nthus it is an NP-hard task. We present several constraint programming\nformulations of the problem. We also present formulations using partial\nweighted maximum Boolean satisfiability and mixed integer linear programming.\nWe study all these formulations by experimentally comparing them on a variety\nof randomly generated instances of the feature subscription problem.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:54:27 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Lesaint", "David", ""], ["Mehta", "Deepak", ""], ["O'Sullivan", "Barry", ""], ["Quesada", "Luis", ""], ["Wilson", "Nic", ""]]}, {"id": "1401.3843", "submitter": "Kenny Daniel", "authors": "Kenny Daniel, Alex Nash, Sven Koenig, Ariel Felner", "title": "Theta*: Any-Angle Path Planning on Grids", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  533-579, 2010", "doi": "10.1613/jair.2994", "report-no": null, "categories": "cs.CG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grids with blocked and unblocked cells are often used to represent terrain in\nrobotics and video games. However, paths formed by grid edges can be longer\nthan true shortest paths in the terrain since their headings are artificially\nconstrained. We present two new correct and complete any-angle path-planning\nalgorithms that avoid this shortcoming. Basic Theta* and Angle-Propagation\nTheta* are both variants of A* that propagate information along grid edges\nwithout constraining paths to grid edges. Basic Theta* is simple to understand\nand implement, fast and finds short paths. However, it is not guaranteed to\nfind true shortest paths. Angle-Propagation Theta* achieves a better worst-case\ncomplexity per vertex expansion than Basic Theta* by propagating angle ranges\nwhen it expands vertices, but is more complex, not as fast and finds slightly\nlonger paths. We refer to Basic Theta* and Angle-Propagation Theta*\ncollectively as Theta*. Theta* has unique properties, which we analyze in\ndetail. We show experimentally that it finds shorter paths than both A* with\npost-smoothed paths and Field D* (the only other version of A* we know of that\npropagates information along grid edges without constraining paths to grid\nedges) with a runtime comparable to that of A* on grids. Finally, we extend\nTheta* to grids that contain unblocked cells with non-uniform traversal costs\nand introduce variants of Theta* which provide different tradeoffs between path\nlength and runtime.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:55:01 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Daniel", "Kenny", ""], ["Nash", "Alex", ""], ["Koenig", "Sven", ""], ["Felner", "Ariel", ""]]}, {"id": "1401.3844", "submitter": "Yagil Engel", "authors": "Yagil Engel, Michael P. Wellman", "title": "Multiattribute Auctions Based on Generalized Additive Independence", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 37, pages\n  479-525, 2010", "doi": "10.1613/jair.3002", "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop multiattribute auctions that accommodate generalized additive\nindependent (GAI) preferences. We propose an iterative auction mechanism that\nmaintains prices on potentially overlapping GAI clusters of attributes, thus\ndecreases elicitation and computational burden, and creates an open competition\namong suppliers over a multidimensional domain. Most significantly, the auction\nis guaranteed to achieve surplus which approximates optimal welfare up to a\nsmall additive factor, under reasonable equilibrium strategies of traders. The\nmain departure of GAI auctions from previous literature is to accommodate\nnon-additive trader preferences, hence allowing traders to condition their\nevaluation of specific attributes on the value of other attributes. At the same\ntime, the GAI structure supports a compact representation of prices, enabling a\ntractable auction process. We perform a simulation study, demonstrating and\nquantifying the significant efficiency advantage of more expressive preference\nmodeling. We draw random GAI-structured utility functions with various internal\nstructures, generate additive functions that approximate the GAI utility, and\ncompare the performance of the auctions using the two representations. We find\nthat allowing traders to express existing dependencies among attributes\nimproves the economic efficiency of multiattribute auctions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:55:39 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Engel", "Yagil", ""], ["Wellman", "Michael P.", ""]]}, {"id": "1401.3845", "submitter": "Jianhui Wu", "authors": "Jianhui Wu, Edmund H. Durfee", "title": "Resource-Driven Mission-Phasing Techniques for Constrained Agents in\n  Stochastic Environments", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  415-473, 2010", "doi": "10.1613/jair.3004", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because an agents resources dictate what actions it can possibly take, it\nshould plan which resources it holds over time carefully, considering its\ninherent limitations (such as power or payload restrictions), the competing\nneeds of other agents for the same resources, and the stochastic nature of the\nenvironment. Such agents can, in general, achieve more of their objectives if\nthey can use --- and even create --- opportunities to change which resources\nthey hold at various times. Driven by resource constraints, the agents could\nbreak their overall missions into an optimal series of phases, optimally\nreconfiguring their resources at each phase, and optimally using their assigned\nresources in each phase, given their knowledge of the stochastic environment.\nIn this paper, we formally define and analyze this constrained, sequential\noptimization problem in both the single-agent and multi-agent contexts. We\npresent a family of mixed integer linear programming (MILP) formulations of\nthis problem that can optimally create phases (when phases are not predefined)\naccounting for costs and limitations in phase creation. Because our\nformulations multaneously also find the optimal allocations of resources at\neach phase and the optimal policies for using the allocated resources at each\nphase, they exploit structure across these coupled problems. This allows them\nto find solutions significantly faster(orders of magnitude faster in larger\nproblems) than alternative solution techniques, as we demonstrate empirically.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:56:30 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Wu", "Jianhui", ""], ["Durfee", "Edmund H.", ""]]}, {"id": "1401.3846", "submitter": "Graeme Gange", "authors": "Graeme Gange, Peter James Stuckey, Vitaly Lagoon", "title": "Fast Set Bounds Propagation Using a BDD-SAT Hybrid", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  307-338, 2010", "doi": "10.1613/jair.3014", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Decision Diagram (BDD) based set bounds propagation is a powerful\napproach to solving set-constraint satisfaction problems. However, prior BDD\nbased techniques in- cur the significant overhead of constructing and\nmanipulating graphs during search. We present a set-constraint solver which\ncombines BDD-based set-bounds propagators with the learning abilities of a\nmodern SAT solver. Together with a number of improvements beyond the basic\nalgorithm, this solver is highly competitive with existing propagation based\nset constraint solvers.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:56:56 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Gange", "Graeme", ""], ["Stuckey", "Peter James", ""], ["Lagoon", "Vitaly", ""]]}, {"id": "1401.3847", "submitter": "Jia-Hong Wu", "authors": "Jia-Hong Wu, Robert Givan", "title": "Automatic Induction of Bellman-Error Features for Probabilistic Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  687-755, 2010", "doi": "10.1613/jair.3021", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific features are important in representing problem structure\nthroughout machine learning and decision-theoretic planning. In planning, once\nstate features are provided, domain-independent algorithms such as approximate\nvalue iteration can learn weighted combinations of those features that often\nperform well as heuristic estimates of state value (e.g., distance to the\ngoal). Successful applications in real-world domains often require features\ncrafted by human experts. Here, we propose automatic processes for learning\nuseful domain-specific feature sets with little or no human intervention. Our\nmethods select and add features that describe state-space regions of high\ninconsistency in the Bellman equation (statewise Bellman error) during\napproximate value iteration. Our method can be applied using any\nreal-valued-feature hypothesis space and corresponding learning method for\nselecting features from training sets of state-value pairs. We evaluate the\nmethod with hypothesis spaces defined by both relational and propositional\nfeature languages, using nine probabilistic planning domains. We show that\napproximate value iteration using a relational feature space performs at the\nstate-of-the-art in domain-independent stochastic relational planning. Our\nmethod provides the first domain-independent approach that plays Tetris\nsuccessfully (without human-engineered features).\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:57:22 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Wu", "Jia-Hong", ""], ["Givan", "Robert", ""]]}, {"id": "1401.3848", "submitter": "Alexander Feldman", "authors": "Alexander Feldman, Gregory Provan, Arjan van Gemund", "title": "Approximate Model-Based Diagnosis Using Greedy Stochastic Search", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  371-413, 2010", "doi": "10.1613/jair.3025", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a StochAstic Fault diagnosis AlgoRIthm, called SAFARI, which\ntrades off guarantees of computing minimal diagnoses for computational\nefficiency. We empirically demonstrate, using the 74XXX and ISCAS-85 suites of\nbenchmark combinatorial circuits, that SAFARI achieves several\norders-of-magnitude speedup over two well-known deterministic algorithms, CDA*\nand HA*, for multiple-fault diagnoses; further, SAFARI can compute a range of\nmultiple-fault diagnoses that CDA* and HA* cannot. We also prove that SAFARI is\noptimal for a range of propositional fault models, such as the widely-used\nweak-fault models (models with ignorance of abnormal behavior). We discuss the\noptimality of SAFARI in a class of strong-fault circuit models with stuck-at\nfailure modes. By modeling the algorithm itself as a Markov chain, we provide\nexact bounds on the minimality of the diagnosis computed. SAFARI also displays\nstrong anytime behavior, and will return a diagnosis after any non-trivial\ninference time.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:57:50 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Feldman", "Alexander", ""], ["Provan", "Gregory", ""], ["van Gemund", "Arjan", ""]]}, {"id": "1401.3849", "submitter": "Sebastian Rudolph", "authors": "Sebastian Rudolph, Birte Glimm", "title": "Nominals, Inverses, Counting, and Conjunctive Queries or: Why Infinity\n  is your Friend!", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  429-481, 2010", "doi": "10.1613/jair.3029", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Description Logics are knowledge representation formalisms that provide, for\nexample, the logical underpinning of the W3C OWL standards. Conjunctive\nqueries, the standard query language in databases, have recently gained\nsignificant attention as an expressive formalism for querying Description Logic\nknowledge bases. Several different techniques for deciding conjunctive query\nentailment are available for a wide range of DLs. Nevertheless, the combination\nof nominals, inverse roles, and number restrictions in OWL 1 and OWL 2 DL\ncauses unsolvable problems for the techniques hitherto available. We tackle\nthis problem and present a decidability result for entailment of unions of\nconjunctive queries in the DL ALCHOIQb that contains all three problematic\nconstructors simultaneously. Provided that queries contain only simple roles,\nour result also shows decidability of entailment of (unions of) conjunctive\nqueries in the logic that underpins OWL 1 DL and we believe that the presented\nresults will pave the way for further progress towards conjunctive query\nentailment decision procedures for the Description Logics underlying the OWL\nstandards.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:58:22 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Rudolph", "Sebastian", ""], ["Glimm", "Birte", ""]]}, {"id": "1401.3850", "submitter": "Alexander Feldman", "authors": "Alexander Feldman, Gregory Provan, Arjan van Gemund", "title": "A Model-Based Active Testing Approach to Sequential Diagnosis", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  301-334, 2010", "doi": "10.1613/jair.3031", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based diagnostic reasoning often leads to a large number of diagnostic\nhypotheses. The set of diagnoses can be reduced by taking into account extra\nobservations (passive monitoring), measuring additional variables (probing) or\nexecuting additional tests (sequential diagnosis/test sequencing). In this\npaper we combine the above approaches with techniques from Automated Test\nPattern Generation (ATPG) and Model-Based Diagnosis (MBD) into a framework\ncalled FRACTAL (FRamework for ACtive Testing ALgorithms). Apart from the inputs\nand outputs that connect a system to its environment, in active testing we\nconsider additional input variables to which a sequence of test vectors can be\nsupplied. We address the computationally hard problem of computing optimal\ncontrol assignments (as defined in FRACTAL) in terms of a greedy approximation\nalgorithm called FRACTAL-G. We compare the decrease in the number of remaining\nminimal cardinality diagnoses of FRACTAL-G to that of two more FRACTAL\nalgorithms: FRACTAL-ATPG and FRACTAL-P. FRACTAL-ATPG is based on ATPG and\nsequential diagnosis while FRACTAL-P is based on probing and, although not an\nactive testing algorithm, provides a baseline for comparing the lower bound on\nthe number of reachable diagnoses for the FRACTAL algorithms. We empirically\nevaluate the trade-offs of the three FRACTAL algorithms by performing extensive\nexperimentation on the ISCAS85/74XXX benchmark of combinational circuits.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:58:46 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Feldman", "Alexander", ""], ["Provan", "Gregory", ""], ["van Gemund", "Arjan", ""]]}, {"id": "1401.3851", "submitter": "Jing Xu", "authors": "Jing Xu, Christian R. Shelton", "title": "Intrusion Detection using Continuous Time Bayesian Networks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  745-774, 2010", "doi": "10.1613/jair.3050", "report-no": null, "categories": "cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrusion detection systems (IDSs) fall into two high-level categories:\nnetwork-based systems (NIDS) that monitor network behaviors, and host-based\nsystems (HIDS) that monitor system calls. In this work, we present a general\ntechnique for both systems. We use anomaly detection, which identifies patterns\nnot conforming to a historic norm. In both types of systems, the rates of\nchange vary dramatically over time (due to burstiness) and over components (due\nto service difference). To efficiently model such systems, we use continuous\ntime Bayesian networks (CTBNs) and avoid specifying a fixed update interval\ncommon to discrete-time models. We build generative models from the normal\ntraining data, and abnormal behaviors are flagged based on their likelihood\nunder this norm. For NIDS, we construct a hierarchical CTBN model for the\nnetwork packet traces and use Rao-Blackwellized particle filtering to learn the\nparameters. We illustrate the power of our method through experiments on\ndetecting real worms and identifying hosts on two publicly available network\ntraces, the MAWI dataset and the LBNL dataset. For HIDS, we develop a novel\nlearning method to deal with the finite resolution of system log file time\nstamps, without losing the benefits of our continuous time model. We\ndemonstrate the method by detecting intrusions in the DARPA 1998 BSM dataset.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:59:06 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Xu", "Jing", ""], ["Shelton", "Christian R.", ""]]}, {"id": "1401.3853", "submitter": "Michael Katz", "authors": "Michael Katz, Carmel Domshlak", "title": "Implicit Abstraction Heuristics", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  51-126, 2010", "doi": "10.1613/jair.3063", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space search with explicit abstraction heuristics is at the state of\nthe art of cost-optimal planning. These heuristics are inherently limited,\nnonetheless, because the size of the abstract space must be bounded by some,\neven if a very large, constant. Targeting this shortcoming, we introduce the\nnotion of (additive) implicit abstractions, in which the planning task is\nabstracted by instances of tractable fragments of optimal planning. We then\nintroduce a concrete setting of this framework, called fork-decomposition, that\nis based on two novel fragments of tractable cost-optimal planning. The induced\nadmissible heuristics are then studied formally and empirically. This study\ntestifies for the accuracy of the fork decomposition heuristics, yet our\nempirical evaluation also stresses the tradeoff between their accuracy and the\nruntime complexity of computing them. Indeed, some of the power of the explicit\nabstraction heuristics comes from precomputing the heuristic function offline\nand then determining h(s) for each evaluated state s by a very fast lookup in a\ndatabase. By contrast, while fork-decomposition heuristics can be calculated in\npolynomial time, computing them is far from being fast. To address this\nproblem, we show that the time-per-node complexity bottleneck of the\nfork-decomposition heuristics can be successfully overcome. We demonstrate that\nan equivalent of the explicit abstraction notion of a database exists for the\nfork-decomposition abstractions as well, despite their exponential-size\nabstract spaces. We then verify empirically that heuristic search with the\ndatabased\" fork-decomposition heuristics favorably competes with the state of\nthe art of cost-optimal planning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:59:55 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Katz", "Michael", ""], ["Domshlak", "Carmel", ""]]}, {"id": "1401.3854", "submitter": "Bonny Banerjee", "authors": "Bonny Banerjee, B. Chandrasekaran", "title": "A Constraint Satisfaction Framework for Executing Perceptions and\n  Actions in Diagrammatic Reasoning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  373-427, 2010", "doi": "10.1613/jair.3069", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagrammatic reasoning (DR) is pervasive in human problem solving as a\npowerful adjunct to symbolic reasoning based on language-like representations.\nThe research reported in this paper is a contribution to building a general\npurpose DR system as an extension to a SOAR-like problem solving architecture.\nThe work is in a framework in which DR is modeled as a process where subtasks\nare solved, as appropriate, either by inference from symbolic representations\nor by interaction with a diagram, i.e., perceiving specified information from a\ndiagram or modifying/creating objects in a diagram in specified ways according\nto problem solving needs. The perceptions and actions in most DR systems built\nso far are hand-coded for the specific application, even when the rest of the\nsystem is built using the general architecture. The absence of a general\nframework for executing perceptions/actions poses as a major hindrance to using\nthem opportunistically -- the essence of open-ended search in problem solving.\nOur goal is to develop a framework for executing a wide variety of specified\nperceptions and actions across tasks/domains without human intervention. We\nobserve that the domain/task-specific visual perceptions/actions can be\ntransformed into domain/task-independent spatial problems. We specify a spatial\nproblem as a quantified constraint satisfaction problem in the real domain\nusing an open-ended vocabulary of properties, relations and actions involving\nthree kinds of diagrammatic objects -- points, curves, regions. Solving a\nspatial problem from this specification requires computing the equivalent\nsimplified quantifier-free expression, the complexity of which is inherently\ndoubly exponential. We represent objects as configuration of simple elements to\nfacilitate decomposition of complex problems into simpler and similar\nsubproblems. We show that, if the symbolic solution to a subproblem can be\nexpressed concisely, quantifiers can be eliminated from spatial problems in\nlow-order polynomial time using similar previously solved subproblems. This\nrequires determining the similarity of two problems, the existence of a mapping\nbetween them computable in polynomial time, and designing a memory for storing\npreviously solved problems so as to facilitate search. The efficacy of the idea\nis shown by time complexity analysis. We demonstrate the proposed approach by\nexecuting perceptions and actions involved in DR tasks in two army\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:00:46 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Banerjee", "Bonny", ""], ["Chandrasekaran", "B.", ""]]}, {"id": "1401.3855", "submitter": "Michael Benisch", "authors": "Michael Benisch, George B. Davis, Tuomas Sandholm", "title": "Algorithms for Closed Under Rational Behavior (CURB) Sets", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  513-534, 2010", "doi": "10.1613/jair.3070", "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a series of algorithms demonstrating that solutions according to\nthe fundamental game-theoretic solution concept of closed under rational\nbehavior (CURB) sets in two-player, normal-form games can be computed in\npolynomial time (we also discuss extensions to n-player games). First, we\ndescribe an algorithm that identifies all of a player's best responses\nconditioned on the belief that the other player will play from within a given\nsubset of its strategy space. This algorithm serves as a subroutine in a series\nof polynomial-time algorithms for finding all minimal CURB sets, one minimal\nCURB set, and the smallest minimal CURB set in a game. We then show that the\ncomplexity of finding a Nash equilibrium can be exponential only in the size of\na game's smallest CURB set. Related to this, we show that the smallest CURB set\ncan be an arbitrarily small portion of the game, but it can also be arbitrarily\nlarger than the supports of its only enclosed Nash equilibrium. We test our\nalgorithms empirically and find that most commonly studied academic games tend\nto have either very large or very small minimal CURB sets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:01:13 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Benisch", "Michael", ""], ["Davis", "George B.", ""], ["Sandholm", "Tuomas", ""]]}, {"id": "1401.3857", "submitter": "Vadim Bulitko", "authors": "Vadim Bulitko, Yngvi Bj\\\"ornsson, Ramon Lawrence", "title": "Case-Based Subgoaling in Real-Time Heuristic Search for Video Game\n  Pathfinding", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  269-300, 2010", "doi": "10.1613/jair.3076", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time heuristic search algorithms satisfy a constant bound on the amount\nof planning per action, independent of problem size. As a result, they scale up\nwell as problems become larger. This property would make them well suited for\nvideo games where Artificial Intelligence controlled agents must react quickly\nto user commands and to other agents actions. On the downside, real-time search\nalgorithms employ learning methods that frequently lead to poor solution\nquality and cause the agent to appear irrational by re-visiting the same\nproblem states repeatedly. The situation changed recently with a new algorithm,\nD LRTA*, which attempted to eliminate learning by automatically selecting\nsubgoals. D LRTA* is well poised for video games, except it has a complex and\nmemory-demanding pre-computation phase during which it builds a database of\nsubgoals. In this paper, we propose a simpler and more memory-efficient way of\npre-computing subgoals thereby eliminating the main obstacle to applying\nstate-of-the-art real-time search methods in video games. The new algorithm\nsolves a number of randomly chosen problems off-line, compresses the solutions\ninto a series of subgoals and stores them in a database. When presented with a\nnovel problem on-line, it queries the database for the most similar previously\nsolved case and uses its subgoals to solve the problem. In the domain of\npathfinding on four large video game maps, the new algorithm delivers solutions\neight times better while using 57 times less memory and requiring 14% less\npre-computation time.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:02:02 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Bulitko", "Vadim", ""], ["Bj\u00f6rnsson", "Yngvi", ""], ["Lawrence", "Ramon", ""]]}, {"id": "1401.3858", "submitter": "Jos de Bruijn", "authors": "Jos de Bruijn, Stijn Heymans", "title": "Logical Foundations of RDF(S) with Datatypes", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  535-568, 2010", "doi": "10.1613/jair.3088", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Resource Description Framework (RDF) is a Semantic Web standard that\nprovides a data language, simply called RDF, as well as a lightweight ontology\nlanguage, called RDF Schema. We investigate embeddings of RDF in logic and show\nhow standard logic programming and description logic technology can be used for\nreasoning with RDF. We subsequently consider extensions of RDF with datatype\nsupport, considering D entailment, defined in the RDF semantics specification,\nand D* entailment, a semantic weakening of D entailment, introduced by ter\nHorst. We use the embeddings and properties of the logics to establish novel\nupper bounds for the complexity of deciding entailment. We subsequently\nestablish two novel lower bounds, establishing that RDFS entailment is\nPTime-complete and that simple-D entailment is coNP-hard, when considering\narbitrary datatypes, both in the size of the entailing graph. The results\nindicate that RDFS may not be as lightweight as one may expect.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:02:49 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["de Bruijn", "Jos", ""], ["Heymans", "Stijn", ""]]}, {"id": "1401.3859", "submitter": "Andreas Krause", "authors": "Andreas Krause, Eric Horvitz", "title": "A Utility-Theoretic Approach to Privacy in Online Services", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  633-662, 2010", "doi": "10.1613/jair.3089", "report-no": null, "categories": "cs.AI cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online offerings such as web search, news portals, and e-commerce\napplications face the challenge of providing high-quality service to a large,\nheterogeneous user base. Recent efforts have highlighted the potential to\nimprove performance by introducing methods to personalize services based on\nspecial knowledge about users and their context. For example, a users\ndemographics, location, and past search and browsing may be useful in enhancing\nthe results offered in response to web search queries. However, reasonable\nconcerns about privacy by both users, providers, and government agencies acting\non behalf of citizens, may limit access by services to such information. We\nintroduce and explore an economics of privacy in personalization, where people\ncan opt to share personal information, in a standing or on-demand manner, in\nreturn for expected enhancements in the quality of an online service. We focus\non the example of web search and formulate realistic objective functions for\nsearch efficacy and privacy. We demonstrate how we can find a provably\nnear-optimal optimization of the utility-privacy tradeoff in an efficient\nmanner. We evaluate our methodology on data drawn from a log of the search\nactivity of volunteer participants. We separately assess users' preferences\nabout privacy and utility via a large-scale survey, aimed at eliciting\npreferences about peoples' willingness to trade the sharing of personal data in\nreturns for gains in search efficiency. We show that a significant level of\npersonalization can be achieved using a relatively small amount of information\nabout users.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:03:13 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Krause", "Andreas", ""], ["Horvitz", "Eric", ""]]}, {"id": "1401.3860", "submitter": "Tobias Lang", "authors": "Tobias Lang, Marc Toussaint", "title": "Planning with Noisy Probabilistic Relational Rules", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  1-49, 2010", "doi": "10.1613/jair.3093", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy probabilistic relational rules are a promising world model\nrepresentation for several reasons. They are compact and generalize over world\ninstantiations. They are usually interpretable and they can be learned\neffectively from the action experiences in complex worlds. We investigate\nreasoning with such rules in grounded relational domains. Our algorithms\nexploit the compactness of rules for efficient and flexible decision-theoretic\nplanning. As a first approach, we combine these rules with the Upper Confidence\nBounds applied to Trees (UCT) algorithm based on look-ahead trees. Our second\napproach converts these rules into a structured dynamic Bayesian network\nrepresentation and predicts the effects of action sequences using approximate\ninference and beliefs over world states. We evaluate the effectiveness of our\napproaches for planning in a simulated complex 3D robot manipulation scenario\nwith an articulated manipulator and realistic physics and in domains of the\nprobabilistic planning competition. Empirical results show that our methods can\nsolve problems where existing methods fail.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:03:40 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Lang", "Tobias", ""], ["Toussaint", "Marc", ""]]}, {"id": "1401.3861", "submitter": "Ethan Burns", "authors": "Ethan Burns, Sofia Lemons, Wheeler Ruml, Rong Zhou", "title": "Best-First Heuristic Search for Multicore Machines", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  689-743, 2010", "doi": "10.1613/jair.3094", "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To harness modern multicore processors, it is imperative to develop parallel\nversions of fundamental algorithms. In this paper, we compare different\napproaches to parallel best-first search in a shared-memory setting. We present\na new method, PBNF, that uses abstraction to partition the state space and to\ndetect duplicate states without requiring frequent locking. PBNF allows\nspeculative expansions when necessary to keep threads busy. We identify and fix\npotential livelock conditions in our approach, proving its correctness using\ntemporal logic. Our approach is general, allowing it to extend easily to\nsuboptimal and anytime heuristic search. In an empirical comparison on STRIPS\nplanning, grid pathfinding, and sliding tile puzzle problems using 8-core\nmachines, we show that A*, weighted A* and Anytime weighted A* implemented\nusing PBNF yield faster search than improved versions of previous parallel\nsearch proposals.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:04:02 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Burns", "Ethan", ""], ["Lemons", "Sofia", ""], ["Ruml", "Wheeler", ""], ["Zhou", "Rong", ""]]}, {"id": "1401.3863", "submitter": "Gerold J\\\"ager", "authors": "Gerold J\\\"ager, Weixiong Zhang", "title": "An Effective Algorithm for and Phase Transitions of the Directed\n  Hamiltonian Cycle Problem", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  663-687, 2010", "doi": "10.1613/jair.3109", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hamiltonian cycle problem (HCP) is an important combinatorial problem\nwith applications in many areas. It is among the first problems used for\nstudying intrinsic properties, including phase transitions, of combinatorial\nproblems. While thorough theoretical and experimental analyses have been made\non the HCP in undirected graphs, a limited amount of work has been done for the\nHCP in directed graphs (DHCP). The main contribution of this work is an\neffective algorithm for the DHCP. Our algorithm explores and exploits the close\nrelationship between the DHCP and the Assignment Problem (AP) and utilizes a\ntechnique based on Boolean satisfiability (SAT). By combining effective\nalgorithms for the AP and SAT, our algorithm significantly outperforms previous\nexact DHCP algorithms, including an algorithm based on the award-winning\nConcorde TSP algorithm. The second result of the current study is an\nexperimental analysis of phase transitions of the DHCP, verifying and refining\na known phase transition of the DHCP.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:04:57 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["J\u00e4ger", "Gerold", ""], ["Zhang", "Weixiong", ""]]}, {"id": "1401.3864", "submitter": "Yi Zhou", "authors": "Yi Zhou, Yan Zhang", "title": "A Logical Study of Partial Entailment", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  25-56, 2011", "doi": "10.1613/jair.3117", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel logical notion--partial entailment--to propositional\nlogic. In contrast with classical entailment, that a formula P partially\nentails another formula Q with respect to a background formula set \\Gamma\nintuitively means that under the circumstance of \\Gamma, if P is true then some\n\"part\" of Q will also be true. We distinguish three different kinds of partial\nentailments and formalize them by using an extended notion of prime implicant.\nWe study their semantic properties, which show that, surprisingly, partial\nentailments fail for many simple inference rules. Then, we study the related\ncomputational properties, which indicate that partial entailments are\nrelatively difficult to be computed. Finally, we consider a potential\napplication of partial entailments in reasoning about rational agents.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:05:21 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Zhou", "Yi", ""], ["Zhang", "Yan", ""]]}, {"id": "1401.3866", "submitter": "Christian Geist", "authors": "Christian Geist, Ulle Endriss", "title": "Automated Search for Impossibility Theorems in Social Choice Theory:\n  Ranking Sets of Objects", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  143-174, 2011", "doi": "10.1613/jair.3126", "report-no": null, "categories": "cs.AI cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for using standard techniques from satisfiability\nchecking to automatically verify and discover theorems in an area of economic\ntheory known as ranking sets of objects. The key question in this area, which\nhas important applications in social choice theory and decision making under\nuncertainty, is how to extend an agents preferences over a number of objects to\na preference relation over nonempty sets of such objects. Certain combinations\nof seemingly natural principles for this kind of preference extension can\nresult in logical inconsistencies, which has led to a number of important\nimpossibility theorems. We first prove a general result that shows that for a\nwide range of such principles, characterised by their syntactic form when\nexpressed in a many-sorted first-order logic, any impossibility exhibited at a\nfixed (small) domain size will necessarily extend to the general case. We then\nshow how to formulate candidates for impossibility theorems at a fixed domain\nsize in propositional logic, which in turn enables us to automatically search\nfor (general) impossibility theorems using a SAT solver. When applied to a\nspace of 20 principles for preference extension familiar from the literature,\nthis method yields a total of 84 impossibility theorems, including both known\nand nontrivial new results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:06:28 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Geist", "Christian", ""], ["Endriss", "Ulle", ""]]}, {"id": "1401.3867", "submitter": "Aaron Hunter", "authors": "Aaron Hunter, James P. Delgrande", "title": "Iterated Belief Change Due to Actions and Observations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  269-304, 2011", "doi": "10.1613/jair.3132", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In action domains where agents may have erroneous beliefs, reasoning about\nthe effects of actions involves reasoning about belief change. In this paper,\nwe use a transition system approach to reason about the evolution of an agents\nbeliefs as actions are executed. Some actions cause an agent to perform belief\nrevision while others cause an agent to perform belief update, but the\ninteraction between revision and update can be non-elementary. We present a set\nof rationality properties describing the interaction between revision and\nupdate, and we introduce a new class of belief change operators for reasoning\nabout alternating sequences of revisions and updates. Our belief change\noperators can be characterized in terms of a natural shifting operation on\ntotal pre-orderings over interpretations. We compare our approach with related\nwork on iterated belief change due to action, and we conclude with some\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:06:49 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Hunter", "Aaron", ""], ["Delgrande", "James P.", ""]]}, {"id": "1401.3868", "submitter": "Albert Atserias", "authors": "Albert Atserias, Johannes Klaus Fichte, Marc Thurley", "title": "Clause-Learning Algorithms with Many Restarts and Bounded-Width\n  Resolution", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  353-373, 2011", "doi": "10.1613/jair.3152", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer a new understanding of some aspects of practical SAT-solvers that\nare based on DPLL with unit-clause propagation, clause-learning, and restarts.\nWe do so by analyzing a concrete algorithm which we claim is faithful to what\npractical solvers do. In particular, before making any new decision or restart,\nthe solver repeatedly applies the unit-resolution rule until saturation, and\nleaves no component to the mercy of non-determinism except for some internal\nrandomness. We prove the perhaps surprising fact that, although the solver is\nnot explicitly designed for it, with high probability it ends up behaving as\nwidth-k resolution after no more than O(n^2k+2) conflicts and restarts, where n\nis the number of variables. In other words, width-k resolution can be thought\nof as O(n^2k+2) restarts of the unit-resolution rule with learning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:07:08 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Atserias", "Albert", ""], ["Fichte", "Johannes Klaus", ""], ["Thurley", "Marc", ""]]}, {"id": "1401.3870", "submitter": "Erik Talvitie", "authors": "Erik Talvitie, Satinder Singh", "title": "Learning to Make Predictions In Partially Observable Environments\n  Without a Generative Model", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  353-392, 2011", "doi": "10.1613/jair.3396", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When faced with the problem of learning a model of a high-dimensional\nenvironment, a common approach is to limit the model to make only a restricted\nset of predictions, thereby simplifying the learning problem. These partial\nmodels may be directly useful for making decisions or may be combined together\nto form a more complete, structured model. However, in partially observable\n(non-Markov) environments, standard model-learning methods learn generative\nmodels, i.e. models that provide a probability distribution over all possible\nfutures (such as POMDPs). It is not straightforward to restrict such models to\nmake only certain predictions, and doing so does not always simplify the\nlearning problem. In this paper we present prediction profile models:\nnon-generative partial models for partially observable systems that make only a\ngiven set of predictions, and are therefore far simpler than generative models\nin some cases. We formalize the problem of learning a prediction profile model\nas a transformation of the original model-learning problem, and show\nempirically that one can learn prediction profile models that make a small set\nof important predictions even in systems that are too complex for standard\ngenerative models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:08:29 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Talvitie", "Erik", ""], ["Singh", "Satinder", ""]]}, {"id": "1401.3871", "submitter": "Mahdi Milani Fard", "authors": "Mahdi Milani Fard, Joelle Pineau", "title": "Non-Deterministic Policies in Markovian Decision Processes", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  1-24, 2011", "doi": "10.1613/jair.3175", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markovian processes have long been used to model stochastic environments.\nReinforcement learning has emerged as a framework to solve sequential planning\nand decision-making problems in such environments. In recent years, attempts\nwere made to apply methods from reinforcement learning to construct decision\nsupport systems for action selection in Markovian environments. Although\nconventional methods in reinforcement learning have proved to be useful in\nproblems concerning sequential decision-making, they cannot be applied in their\ncurrent form to decision support systems, such as those in medical domains, as\nthey suggest policies that are often highly prescriptive and leave little room\nfor the users input. Without the ability to provide flexible guidelines, it is\nunlikely that these methods can gain ground with users of such systems. This\npaper introduces the new concept of non-deterministic policies to allow more\nflexibility in the users decision-making process, while constraining decisions\nto remain near optimal solutions. We provide two algorithms to compute\nnon-deterministic policies in discrete domains. We study the output and running\ntime of these method on a set of synthetic and real-world problems. In an\nexperiment with human subjects, we show that humans assisted by hints based on\nnon-deterministic policies outperform both human-only and computer-only agents\nin a web navigation task.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:09:10 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Fard", "Mahdi Milani", ""], ["Pineau", "Joelle", ""]]}, {"id": "1401.3872", "submitter": "Christophe Lecoutre", "authors": "Christophe Lecoutre, Stephane Cardon, Julien Vion", "title": "Second-Order Consistencies", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  175-219, 2011", "doi": "10.1613/jair.3180", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a comprehensive study of second-order consistencies\n(i.e., consistencies identifying inconsistent pairs of values) for constraint\nsatisfaction. We build a full picture of the relationships existing between\nfour basic second-order consistencies, namely path consistency (PC),\n3-consistency (3C), dual consistency (DC) and 2-singleton arc consistency\n(2SAC), as well as their conservative and strong variants. Interestingly, dual\nconsistency is an original property that can be established by using the\noutcome of the enforcement of generalized arc consistency (GAC), which makes it\nrather easy to obtain since constraint solvers typically maintain GAC during\nsearch. On binary constraint networks, DC is equivalent to PC, but its\nrestriction to existing constraints, called conservative dual consistency\n(CDC), is strictly stronger than traditional conservative consistencies derived\nfrom path consistency, namely partial path consistency (PPC) and conservative\npath consistency (CPC). After introducing a general algorithm to enforce strong\n(C)DC, we present the results of an experimentation over a wide range of\nbenchmarks that demonstrate the interest of (conservative) dual consistency. In\nparticular, we show that enforcing (C)DC before search clearly improves the\nperformance of MAC (the algorithm that maintains GAC during search) on several\nbinary and non-binary structured problems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:09:30 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Lecoutre", "Christophe", ""], ["Cardon", "Stephane", ""], ["Vion", "Julien", ""]]}, {"id": "1401.3875", "submitter": "Wheeler Ruml", "authors": "Wheeler Ruml, Minh Binh Do, Rong Zhou, Markus P.J. Fromherz", "title": "On-line Planning and Scheduling: An Application to Controlling Modular\n  Printers", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  415-468, 2011", "doi": "10.1613/jair.3184", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case study of artificial intelligence techniques applied to the\ncontrol of production printing equipment. Like many other real-world\napplications, this complex domain requires high-speed autonomous\ndecision-making and robust continual operation. To our knowledge, this work\nrepresents the first successful industrial application of embedded\ndomain-independent temporal planning. Our system handles execution failures and\nmulti-objective preferences. At its heart is an on-line algorithm that combines\ntechniques from state-space planning and partial-order scheduling. We suggest\nthat this general architecture may prove useful in other applications as more\nintelligent systems operate in continual, on-line settings. Our system has been\nused to drive several commercial prototypes and has enabled a new product\narchitecture for our industrial partner. When compared with state-of-the-art\noff-line planners, our system is hundreds of times faster and often finds\nbetter plans. Our experience demonstrates that domain-independent AI planning\nbased on heuristic search can flexibly handle time, resources, replanning, and\nmultiple objectives in a high-speed practical application without requiring\nhand-coded control knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:10:17 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Ruml", "Wheeler", ""], ["Do", "Minh Binh", ""], ["Zhou", "Rong", ""], ["Fromherz", "Markus P. J.", ""]]}, {"id": "1401.3877", "submitter": "Botond Cseke", "authors": "Botond Cseke, Tom Heskes", "title": "Properties of Bethe Free Energies and Message Passing in Gaussian Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  1-24, 2011", "doi": "10.1613/jair.3195", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of computing approximate marginals in Gaussian\nprobabilistic models by using mean field and fractional Bethe approximations.\nWe define the Gaussian fractional Bethe free energy in terms of the moment\nparameters of the approximate marginals, derive a lower and an upper bound on\nthe fractional Bethe free energy and establish a necessary condition for the\nlower bound to be bounded from below. It turns out that the condition is\nidentical to the pairwise normalizability condition, which is known to be a\nsufficient condition for the convergence of the message passing algorithm. We\nshow that stable fixed points of the Gaussian message passing algorithm are\nlocal minima of the Gaussian Bethe free energy. By a counterexample, we\ndisprove the conjecture stating that the unboundedness of the free energy\nimplies the divergence of the message passing algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:11:12 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Cseke", "Botond", ""], ["Heskes", "Tom", ""]]}, {"id": "1401.3878", "submitter": "Alessandro Cimatti", "authors": "Alessandro Cimatti, Alberto Griggio, Roberto Sebastiani", "title": "Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  701-728, 2011", "doi": "10.1613/jair.3196", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding small unsatisfiable cores for SAT formulas has\nrecently received a lot of interest, mostly for its applications in formal\nverification. However, propositional logic is often not expressive enough for\nrepresenting many interesting verification problems, which can be more\nnaturally addressed in the framework of Satisfiability Modulo Theories, SMT.\nSurprisingly, the problem of finding unsatisfiable cores in SMT has received\nvery little attention in the literature. In this paper we present a novel\napproach to this problem, called the Lemma-Lifting approach. The main idea is\nto combine an SMT solver with an external propositional core extractor. The SMT\nsolver produces the theory lemmas found during the search, dynamically lifting\nthe suitable amount of theory information to the Boolean level. The core\nextractor is then called on the Boolean abstraction of the original SMT problem\nand of the theory lemmas. This results in an unsatisfiable core for the\noriginal SMT problem, once the remaining theory lemmas are removed. The\napproach is conceptually interesting, and has several advantages in practice.\nIn fact, it is extremely simple to implement and to update, and it can be\ninterfaced with every propositional core extractor in a plug-and-play manner,\nso as to benefit for free of all unsat-core reduction techniques which have\nbeen or will be made available.\n  We have evaluated our algorithm with a very extensive empirical test on\nSMT-LIB benchmarks, which confirms the validity and potential of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:11:40 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Cimatti", "Alessandro", ""], ["Griggio", "Alberto", ""], ["Sebastiani", "Roberto", ""]]}, {"id": "1401.3879", "submitter": "Emmanuel Hebrard", "authors": "Emmanuel Hebrard, D\\'aniel Marx, Barry O'Sullivan, Igor Razgon", "title": "Soft Constraints of Difference and Equality", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  97-130, 2011", "doi": "10.1613/jair.3197", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many combinatorial problems one may need to model the diversity or\nsimilarity of assignments in a solution. For example, one may wish to maximise\nor minimise the number of distinct values in a solution. To formulate problems\nof this type, we can use soft variants of the well known AllDifferent and\nAllEqual constraints. We present a taxonomy of six soft global constraints,\ngenerated by combining the two latter ones and the two standard cost functions,\nwhich are either maximised or minimised. We characterise the complexity of\nachieving arc and bounds consistency on these constraints, resolving those\ncases for which NP-hardness was neither proven nor disproven. In particular, we\nexplore in depth the constraint ensuring that at least k pairs of variables\nhave a common value. We show that achieving arc consistency is NP-hard, however\nachieving bounds consistency can be done in polynomial time through dynamic\nprogramming. Moreover, we show that the maximum number of pairs of equal\nvariables can be approximated by a factor 1/2 with a linear time greedy\nalgorithm. Finally, we provide a fixed parameter tractable algorithm with\nrespect to the number of values appearing in more than two distinct domains.\nInterestingly, this taxonomy shows that enforcing equality is harder than\nenforcing difference.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:11:58 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Hebrard", "Emmanuel", ""], ["Marx", "D\u00e1niel", ""], ["O'Sullivan", "Barry", ""], ["Razgon", "Igor", ""]]}, {"id": "1401.3881", "submitter": "Mustafa Bilgic", "authors": "Mustafa Bilgic, Lise Getoor", "title": "Value of Information Lattice: Exploiting Probabilistic Independence for\n  Effective Feature Subset Acquisition", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  69-95, 2011", "doi": "10.1613/jair.3200", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the cost-sensitive feature acquisition problem, where\nmisclassifying an instance is costly but the expected misclassification cost\ncan be reduced by acquiring the values of the missing features. Because\nacquiring the features is costly as well, the objective is to acquire the right\nset of features so that the sum of the feature acquisition cost and\nmisclassification cost is minimized. We describe the Value of Information\nLattice (VOILA), an optimal and efficient feature subset acquisition framework.\nUnlike the common practice, which is to acquire features greedily, VOILA can\nreason with subsets of features. VOILA efficiently searches the space of\npossible feature subsets by discovering and exploiting conditional independence\nproperties between the features and it reuses probabilistic inference\ncomputations to further speed up the process. Through empirical evaluation on\nfive medical datasets, we show that the greedy strategy is often reluctant to\nacquire features, as it cannot forecast the benefit of acquiring multiple\nfeatures in combination.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:12:42 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Bilgic", "Mustafa", ""], ["Getoor", "Lise", ""]]}, {"id": "1401.3882", "submitter": "Saket Joshi", "authors": "Saket Joshi, Roni Khardon", "title": "Probabilistic Relational Planning with First Order Decision Diagrams", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  231-266, 2011", "doi": "10.1613/jair.3205", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic programming algorithms have been successfully applied to\npropositional stochastic planning problems by using compact representations, in\nparticular algebraic decision diagrams, to capture domain dynamics and value\nfunctions. Work on symbolic dynamic programming lifted these ideas to first\norder logic using several representation schemes. Recent work introduced a\nfirst order variant of decision diagrams (FODD) and developed a value iteration\nalgorithm for this representation. This paper develops several improvements to\nthe FODD algorithm that make the approach practical. These include, new\nreduction operators that decrease the size of the representation, several\nspeedup techniques, and techniques for value approximation. Incorporating\nthese, the paper presents a planning system, FODD-Planner, for solving\nrelational stochastic planning problems. The system is evaluated on several\ndomains, including problems from the recent international planning competition,\nand shows competitive performance with top ranking systems. This is the first\ndemonstration of feasibility of this approach and it shows that abstraction\nthrough compact representation is a promising approach to stochastic planning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:13:02 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Joshi", "Saket", ""], ["Khardon", "Roni", ""]]}, {"id": "1401.3885", "submitter": "Tomas De la Rosa", "authors": "Tomas De la Rosa, Sergio Jimenez, Raquel Fuentetaja, Daniel Borrajo", "title": "Scaling up Heuristic Planning with Relational Decision Trees", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  767-813, 2011", "doi": "10.1613/jair.3231", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current evaluation functions for heuristic planning are expensive to compute.\nIn numerous planning problems these functions provide good guidance to the\nsolution, so they are worth the expense. However, when evaluation functions are\nmisguiding or when planning problems are large enough, lots of node evaluations\nmust be computed, which severely limits the scalability of heuristic planners.\nIn this paper, we present a novel solution for reducing node evaluations in\nheuristic planning based on machine learning. Particularly, we define the task\nof learning search control for heuristic planning as a relational\nclassification task, and we use an off-the-shelf relational classification tool\nto address this learning task. Our relational classification task captures the\npreferred action to select in the different planning contexts of a specific\nplanning domain. These planning contexts are defined by the set of helpful\nactions of the current state, the goals remaining to be achieved, and the\nstatic predicates of the planning task. This paper shows two methods for\nguiding the search of a heuristic planner with the learned classifiers. The\nfirst one consists of using the resulting classifier as an action policy. The\nsecond one consists of applying the classifier to generate lookahead states\nwithin a Best First Search algorithm. Experiments over a variety of domains\nreveal that our heuristic planner using the learned classifiers solves larger\nproblems than state-of-the-art planners.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:14:42 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["De la Rosa", "Tomas", ""], ["Jimenez", "Sergio", ""], ["Fuentetaja", "Raquel", ""], ["Borrajo", "Daniel", ""]]}, {"id": "1401.3886", "submitter": "Wei Li", "authors": "Wei Li, Pascal Poupart, Peter van Beek", "title": "Exploiting Structure in Weighted Model Counting Approaches to\n  Probabilistic Inference", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  729-765, 2011", "doi": "10.1613/jair.3232", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have demonstrated that encoding a Bayesian network into a\nSAT formula and then performing weighted model counting using a backtracking\nsearch algorithm can be an effective method for exact inference. In this paper,\nwe present techniques for improving this approach for Bayesian networks with\nnoisy-OR and noisy-MAX relations---two relations that are widely used in\npractice as they can dramatically reduce the number of probabilities one needs\nto specify. In particular, we present two SAT encodings for noisy-OR and two\nencodings for noisy-MAX that exploit the structure or semantics of the\nrelations to improve both time and space efficiency, and we prove the\ncorrectness of the encodings. We experimentally evaluated our techniques on\nlarge-scale real and randomly generated Bayesian networks. On these benchmarks,\nour techniques gave speedups of up to two orders of magnitude over the best\nprevious approaches for networks with noisy-OR/MAX relations and scaled up to\nlarger networks. As well, our techniques extend the weighted model counting\napproach for exact inference to networks that were previously intractable for\nthe approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:15:08 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Li", "Wei", ""], ["Poupart", "Pascal", ""], ["van Beek", "Peter", ""]]}, {"id": "1401.3887", "submitter": "Lucas Bordeaux", "authors": "Lucas Bordeaux, George Katsirelos, Nina Narodytska, Moshe Y. Vardi", "title": "The Complexity of Integer Bound Propagation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  657-676, 2011", "doi": "10.1613/jair.3248", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bound propagation is an important Artificial Intelligence technique used in\nConstraint Programming tools to deal with numerical constraints. It is\ntypically embedded within a search procedure (\"branch and prune\") and used at\nevery node of the search tree to narrow down the search space, so it is\ncritical that it be fast. The procedure invokes constraint propagators until a\ncommon fixpoint is reached, but the known algorithms for this have a\npseudo-polynomial worst-case time complexity: they are fast indeed when the\nvariables have a small numerical range, but they have the well-known problem of\nbeing prohibitively slow when these ranges are large. An important question is\ntherefore whether strongly-polynomial algorithms exist that compute the common\nbound consistent fixpoint of a set of constraints. This paper answers this\nquestion. In particular we show that this fixpoint computation is in fact\nNP-complete, even when restricted to binary linear constraints.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:15:30 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Bordeaux", "Lucas", ""], ["Katsirelos", "George", ""], ["Narodytska", "Nina", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1401.3890", "submitter": "Joerg Hoffmann", "authors": "Joerg Hoffmann", "title": "Analyzing Search Topology Without Running Any Search: On the Connection\n  Between Causal Graphs and h+", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  155-229, 2011", "doi": "10.1613/jair.3276", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ignoring delete lists relaxation is of paramount importance for both\nsatisficing and optimal planning. In earlier work, it was observed that the\noptimal relaxation heuristic h+ has amazing qualities in many classical\nplanning benchmarks, in particular pertaining to the complete absence of local\nminima. The proofs of this are hand-made, raising the question whether such\nproofs can be lead automatically by domain analysis techniques. In contrast to\nearlier disappointing results -- the analysis method has exponential runtime\nand succeeds only in two extremely simple benchmark domains -- we herein answer\nthis question in the affirmative. We establish connections between causal graph\nstructure and h+ topology. This results in low-order polynomial time analysis\nmethods, implemented in a tool we call TorchLight. Of the 12 domains where the\nabsence of local minima has been proved, TorchLight gives strong success\nguarantees in 8 domains. Empirically, its analysis exhibits strong performance\nin a further 2 of these domains, plus in 4 more domains where local minima may\nexist but are rare. In this way, TorchLight can distinguish easy domains from\nhard ones. By summarizing structural reasons for analysis failure, TorchLight\nalso provides diagnostic output indicating domain aspects that may cause local\nminima.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:16:17 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Hoffmann", "Joerg", ""]]}, {"id": "1401.3892", "submitter": "Sajjad Ahmed Siddiqi", "authors": "Sajjad Ahmed Siddiqi, Jinbo Huang", "title": "Sequential Diagnosis by Abstraction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  329-365, 2011", "doi": "10.1613/jair.3296", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a system behaves abnormally, sequential diagnosis takes a sequence of\nmeasurements of the system until the faults causing the abnormality are\nidentified, and the goal is to reduce the diagnostic cost, defined here as the\nnumber of measurements. To propose measurement points, previous work employs a\nheuristic based on reducing the entropy over a computed set of diagnoses. This\napproach generally has good performance in terms of diagnostic cost, but can\nfail to diagnose large systems when the set of diagnoses is too large. Focusing\non a smaller set of probable diagnoses scales the approach but generally leads\nto increased average diagnostic costs. In this paper, we propose a new\ndiagnostic framework employing four new techniques, which scales to much larger\nsystems with good performance in terms of diagnostic cost. First, we propose a\nnew heuristic for measurement point selection that can be computed efficiently,\nwithout requiring the set of diagnoses, once the system is modeled as a\nBayesian network and compiled into a logical form known as d-DNNF. Second, we\nextend hierarchical diagnosis, a technique based on system abstraction from our\nprevious work, to handle probabilities so that it can be applied to sequential\ndiagnosis to allow larger systems to be diagnosed. Third, for the largest\nsystems where even hierarchical diagnosis fails, we propose a novel method that\nconverts the system into one that has a smaller abstraction and whose diagnoses\nform a superset of those of the original system; the new system can then be\ndiagnosed and the result mapped back to the original system. Finally, we\npropose a novel cost estimation function which can be used to choose an\nabstraction of the system that is more likely to provide optimal average cost.\nExperiments with ISCAS-85 benchmark circuits indicate that our approach scales\nto all circuits in the suite except one that has a flat structure not\nsusceptible to useful abstraction.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:16:38 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Siddiqi", "Sajjad Ahmed", ""], ["Huang", "Jinbo", ""]]}, {"id": "1401.3893", "submitter": "Changhe Yuan", "authors": "Changhe Yuan, Heejin Lim, Tsai-Ching Lu", "title": "Most Relevant Explanation in Bayesian Networks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  309-352, 2011", "doi": "10.1613/jair.3301", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major inference task in Bayesian networks is explaining why some variables\nare observed in their particular states using a set of target variables.\nExisting methods for solving this problem often generate explanations that are\neither too simple (underspecified) or too complex (overspecified). In this\npaper, we introduce a method called Most Relevant Explanation (MRE) which finds\na partial instantiation of the target variables that maximizes the generalized\nBayes factor (GBF) as the best explanation for the given evidence. Our study\nshows that GBF has several theoretical properties that enable MRE to\nautomatically identify the most relevant target variables in forming its\nexplanation. In particular, conditional Bayes factor (CBF), defined as the GBF\nof a new explanation conditioned on an existing explanation, provides a soft\nmeasure on the degree of relevance of the variables in the new explanation in\nexplaining the evidence given the existing explanation. As a result, MRE is\nable to automatically prune less relevant variables from its explanation. We\nalso show that CBF is able to capture well the explaining-away phenomenon that\nis often represented in Bayesian networks. Moreover, we define two dominance\nrelations between the candidate solutions and use the relations to generalize\nMRE to find a set of top explanations that is both diverse and representative.\nCase studies on several benchmark diagnostic Bayesian networks show that MRE is\noften able to find explanatory hypotheses that are not only precise but also\nconcise.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:17:05 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Yuan", "Changhe", ""], ["Lim", "Heejin", ""], ["Lu", "Tsai-Ching", ""]]}, {"id": "1401.3894", "submitter": "Andr\\'as Gy\\\"orgy", "authors": "Andr\\'as Gy\\\"orgy, Levente Kocsis", "title": "Efficient Multi-Start Strategies for Local Search Algorithms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  407-444, 2011", "doi": "10.1613/jair.3313", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local search algorithms applied to optimization problems often suffer from\ngetting trapped in a local optimum. The common solution for this deficiency is\nto restart the algorithm when no progress is observed. Alternatively, one can\nstart multiple instances of a local search algorithm, and allocate\ncomputational resources (in particular, processing time) to the instances\ndepending on their behavior. Hence, a multi-start strategy has to decide\n(dynamically) when to allocate additional resources to a particular instance\nand when to start new instances. In this paper we propose multi-start\nstrategies motivated by works on multi-armed bandit problems and Lipschitz\noptimization with an unknown constant. The strategies continuously estimate the\npotential performance of each algorithm instance by supposing a convergence\nrate of the local search algorithm up to an unknown constant, and in every\nphase allocate resources to those instances that could converge to the optimum\nfor a particular range of the constant. Asymptotic bounds are given on the\nperformance of the strategies. In particular, we prove that at most a quadratic\nincrease in the number of times the target function is evaluated is needed to\nachieve the performance of a local search algorithm started from the attraction\nregion of the optimum. Experiments are provided using SPSA (Simultaneous\nPerturbation Stochastic Approximation) and k-means as local search algorithms,\nand the results indicate that the proposed strategies work well in practice,\nand, in all cases studied, need only logarithmically more evaluations of the\ntarget function as opposed to the theoretically suggested quadratic increase.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:17:32 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Kocsis", "Levente", ""]]}, {"id": "1401.3895", "submitter": "Wolfgang Dvorak", "authors": "Wolfgang Dvorak, Stefan Woltran", "title": "On the Intertranslatability of Argumentation Semantics", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  445-475, 2011", "doi": "10.1613/jair.3318", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translations between different nonmonotonic formalisms always have been an\nimportant topic in the field, in particular to understand the\nknowledge-representation capabilities those formalisms offer. We provide such\nan investigation in terms of different semantics proposed for abstract\nargumentation frameworks, a nonmonotonic yet simple formalism which received\nincreasing interest within the last decade. Although the properties of these\ndifferent semantics are nowadays well understood, there are no explicit results\nabout intertranslatability. We provide such translations wrt. different\nproperties and also give a few novel complexity results which underlie some\nnegative results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:17:48 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Dvorak", "Wolfgang", ""], ["Woltran", "Stefan", ""]]}, {"id": "1401.3897", "submitter": "Dov Gabbay", "authors": "Dov Gabbay, David Pearce, Agust\\'in Valverde", "title": "Interpolable Formulas in Equilibrium Logic and Answer Set Programming", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  917-943, 2011", "doi": "10.1613/jair.3329", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpolation is an important property of classical and many non-classical\nlogics that has been shown to have interesting applications in computer science\nand AI. Here we study the Interpolation Property for the the non-monotonic\nsystem of equilibrium logic, establishing weaker or stronger forms of\ninterpolation depending on the precise interpretation of the inference\nrelation. These results also yield a form of interpolation for ground logic\nprograms under the answer sets semantics. For disjunctive logic programs we\nalso study the property of uniform interpolation that is closely related to the\nconcept of variable forgetting. The first-order version of equilibrium logic\nhas analogous Interpolation properties whenever the collection of equilibrium\nmodels is (first-order) definable. Since this is the case for so-called safe\nprograms and theories, it applies to the usual situations that arise in\npractical answer set programming.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:18:24 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Gabbay", "Dov", ""], ["Pearce", "David", ""], ["Valverde", "Agust\u00edn", ""]]}, {"id": "1401.3898", "submitter": "Joohyung Lee", "authors": "Joohyung Lee, Yunsong Meng", "title": "First-Order Stable Model Semantics and First-Order Loop Formulas", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  125-180, 2011", "doi": "10.1613/jair.3337", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lin and Zhaos theorem on loop formulas states that in the propositional case\nthe stable model semantics of a logic program can be completely characterized\nby propositional loop formulas, but this result does not fully carry over to\nthe first-order case. We investigate the precise relationship between the\nfirst-order stable model semantics and first-order loop formulas, and study\nconditions under which the former can be represented by the latter. In order to\nfacilitate the comparison, we extend the definition of a first-order loop\nformula which was limited to a nondisjunctive program, to a disjunctive program\nand to an arbitrary first-order theory. Based on the studied relationship we\nextend the syntax of a logic program with explicit quantifiers, which allows us\nto do reasoning involving non-Herbrand stable models using first-order\nreasoners. Such programs can be viewed as a special class of first-order\ntheories under the stable model semantics, which yields more succinct loop\nformulas than the general language due to their restricted syntax.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:18:54 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Lee", "Joohyung", ""], ["Meng", "Yunsong", ""]]}, {"id": "1401.3899", "submitter": "Ganesh Ram Santhanam", "authors": "Ganesh Ram Santhanam, Samik Basu, Vasant Honavar", "title": "Representing and Reasoning with Qualitative Preferences for\n  Compositional Systems", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  211-274, 2011", "doi": "10.1613/jair.3339", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications, e.g., Web service composition, complex system design, team\nformation, etc., rely on methods for identifying collections of objects or\nentities satisfying some functional requirement. Among the collections that\nsatisfy the functional requirement, it is often necessary to identify one or\nmore collections that are optimal with respect to user preferences over a set\nof attributes that describe the non-functional properties of the collection.\n  We develop a formalism that lets users express the relative importance among\nattributes and qualitative preferences over the valuations of each attribute.\nWe define a dominance relation that allows us to compare collections of objects\nin terms of preferences over attributes of the objects that make up the\ncollection. We establish some key properties of the dominance relation. In\nparticular, we show that the dominance relation is a strict partial order when\nthe intra-attribute preference relations are strict partial orders and the\nrelative importance preference relation is an interval order.\n  We provide algorithms that use this dominance relation to identify the set of\nmost preferred collections. We show that under certain conditions, the\nalgorithms are guaranteed to return only (sound), all (complete), or at least\none (weakly complete) of the most preferred collections. We present results of\nsimulation experiments comparing the proposed algorithms with respect to (a)\nthe quality of solutions (number of most preferred solutions) produced by the\nalgorithms, and (b) their performance and efficiency. We also explore some\ninteresting conjectures suggested by the results of our experiments that relate\nthe properties of the user preferences, the dominance relation, and the\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:19:43 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Santhanam", "Ganesh Ram", ""], ["Basu", "Samik", ""], ["Honavar", "Vasant", ""]]}, {"id": "1401.3900", "submitter": "Vincent Aravantinos", "authors": "Vincent Aravantinos, Ricardo Caferra, Nicolas Peltier", "title": "Decidability and Undecidability Results for Propositional Schemata", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  599-656, 2011", "doi": "10.1613/jair.3351", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a logic of propositional formula schemata adding to the syntax of\npropositional logic indexed propositions and iterated connectives ranging over\nintervals parameterized by arithmetic variables. The satisfiability problem is\nshown to be undecidable for this new logic, but we introduce a very general\nclass of schemata, called bound-linear, for which this problem becomes\ndecidable. This result is obtained by reduction to a particular class of\nschemata called regular, for which we provide a sound and complete terminating\nproof procedure. This schemata calculus allows one to capture proof patterns\ncorresponding to a large class of problems specified in propositional logic. We\nalso show that the satisfiability problem becomes again undecidable for slight\nextensions of this class, thus demonstrating that bound-linear schemata\nrepresent a good compromise between expressivity and decidability.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:20:30 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Aravantinos", "Vincent", ""], ["Caferra", "Ricardo", ""], ["Peltier", "Nicolas", ""]]}, {"id": "1401.3901", "submitter": "Piero A. Bonatti", "authors": "Piero A. Bonatti, Marco Faella, Luigi Sauro", "title": "Defeasible Inclusions in Low-Complexity DLs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  719-764, 2011", "doi": "10.1613/jair.3360", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some of the applications of OWL and RDF (e.g. biomedical knowledge\nrepresentation and semantic policy formulation) call for extensions of these\nlanguages with nonmonotonic constructs such as inheritance with overriding.\nNonmonotonic description logics have been studied for many years, however no\npractical such knowledge representation languages exist, due to a combination\nof semantic difficulties and high computational complexity. Independently,\nlow-complexity description logics such as DL-lite and EL have been introduced\nand incorporated in the OWL standard. Therefore, it is interesting to see\nwhether the syntactic restrictions characterizing DL-lite and EL bring\ncomputational benefits to their nonmonotonic versions, too. In this paper we\nextensively investigate the computational complexity of Circumscription when\nknowledge bases are formulated in DL-lite_R, EL, and fragments thereof. We\nidentify fragments whose complexity ranges from P to the second level of the\npolynomial hierarchy, as well as fragments whose complexity raises to PSPACE\nand beyond.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:20:50 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Bonatti", "Piero A.", ""], ["Faella", "Marco", ""], ["Sauro", "Luigi", ""]]}, {"id": "1401.3902", "submitter": "Richard Booth", "authors": "Richard Booth, Thomas Meyer, Ivan Varzinczak, Renata Wassermann", "title": "On the Link between Partial Meet, Kernel, and Infra Contraction and its\n  Application to Horn Logic", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  31-53, 2011", "doi": "10.1613/jair.3364", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard belief change assumes an underlying logic containing full classical\npropositional logic. However, there are good reasons for considering belief\nchange in less expressive logics as well. In this paper we build on recent\ninvestigations by Delgrande on contraction for Horn logic. We show that the\nstandard basic form of contraction, partial meet, is too strong in the Horn\ncase. This result stands in contrast to Delgrande's conjecture that orderly\nmaxichoice is the appropriate form of contraction for Horn logic. We then\ndefine a more appropriate notion of basic contraction for the Horn case,\ninfluenced by the convexity property holding for full propositional logic and\nwhich we refer to as infra contraction. The main contribution of this work is a\nresult which shows that the construction method for Horn contraction for belief\nsets based on our infra remainder sets corresponds exactly to Hansson's\nclassical kernel contraction for belief sets, when restricted to Horn logic.\nThis result is obtained via a detour through contraction for belief bases. We\nprove that kernel contraction for belief bases produces precisely the same\nresults as the belief base version of infra contraction. The use of belief\nbases to obtain this result provides evidence for the conjecture that Horn\nbelief change is best viewed as a hybrid version of belief set change and\nbelief base change. One of the consequences of the link with base contraction\nis the provision of a representation result for Horn contraction for belief\nsets in which a version of the Core-retainment postulate features.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:21:09 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Booth", "Richard", ""], ["Meyer", "Thomas", ""], ["Varzinczak", "Ivan", ""], ["Wassermann", "Renata", ""]]}, {"id": "1401.3905", "submitter": "Ko-Hsin Cindy Wang", "authors": "Ko-Hsin Cindy Wang, Adi Botea", "title": "MAPP: a Scalable Multi-Agent Path Planning Algorithm with Tractability\n  and Completeness Guarantees", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  55-90, 2011", "doi": "10.1613/jair.3370", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent path planning is a challenging problem with numerous real-life\napplications. Running a centralized search such as A* in the combined state\nspace of all units is complete and cost-optimal, but scales poorly, as the\nstate space size is exponential in the number of mobile units. Traditional\ndecentralized approaches, such as FAR and WHCA*, are faster and more scalable,\nbeing based on problem decomposition. However, such methods are incomplete and\nprovide no guarantees with respect to the running time or the solution quality.\nThey are not necessarily able to tell in a reasonable time whether they would\nsucceed in finding a solution to a given instance. We introduce MAPP, a\ntractable algorithm for multi-agent path planning on undirected graphs. We\npresent a basic version and several extensions. They have low-polynomial\nworst-case upper bounds for the running time, the memory requirements, and the\nlength of solutions. Even though all algorithmic versions are incomplete in the\ngeneral case, each provides formal guarantees on problems it can solve. For\neach version, we discuss the algorithms completeness with respect to clearly\ndefined subclasses of instances. Experiments were run on realistic game grid\nmaps. MAPP solved 99.86% of all mobile units, which is 18--22% better than the\npercentage of FAR and WHCA*. MAPP marked 98.82% of all units as provably\nsolvable during the first stage of plan computation. Parts of MAPPs computation\ncan be re-used across instances on the same map. Speed-wise, MAPP is\ncompetitive or significantly faster than WHCA*, depending on whether MAPP\nperforms all computations from scratch. When data that MAPP can re-use are\npreprocessed offline and readily available, MAPP is slower than the very fast\nFAR algorithm by a factor of 2.18 on average. MAPPs solutions are on average\n20% longer than FARs solutions and 7--31% longer than WHCA*s solutions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:21:59 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Wang", "Ko-Hsin Cindy", ""], ["Botea", "Adi", ""]]}, {"id": "1401.3906", "submitter": "Peter D Grunwald", "authors": "Peter D Grunwald, Joseph Y Halpern", "title": "Making Decisions Using Sets of Probabilities: Updating, Time\n  Consistency, and Calibration", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  393-426, 2011", "doi": "10.1613/jair.3374", "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider how an agent should update her beliefs when her beliefs are\nrepresented by a set P of probability distributions, given that the agent makes\ndecisions using the minimax criterion, perhaps the best-studied and most\ncommonly-used criterion in the literature. We adopt a game-theoretic framework,\nwhere the agent plays against a bookie, who chooses some distribution from P.\nWe consider two reasonable games that differ in what the bookie knows when he\nmakes his choice. Anomalies that have been observed before, like time\ninconsistency, can be understood as arising because different games are being\nplayed, against bookies with different information. We characterize the\nimportant special cases in which the optimal decision rules according to the\nminimax criterion amount to either conditioning or simply ignoring the\ninformation. Finally, we consider the relationship between updating and\ncalibration when uncertainty is described by sets of probabilities. Our results\nemphasize the key role of the rectangularity condition of Epstein and\nSchneider.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:22:33 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Grunwald", "Peter D", ""], ["Halpern", "Joseph Y", ""]]}, {"id": "1401.3909", "submitter": "Richard Hoshino", "authors": "Richard Hoshino, Ken-ichi Kawarabayashi", "title": "Scheduling Bipartite Tournaments to Minimize Total Travel Distance", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  91-124, 2011", "doi": "10.1613/jair.3388", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many professional sports leagues, teams from opposing leagues/conferences\ncompete against one another, playing inter-league games. This is an example of\na bipartite tournament. In this paper, we consider the problem of reducing the\ntotal travel distance of bipartite tournaments, by analyzing inter-league\nscheduling from the perspective of discrete optimization. This research has\nnatural applications to sports scheduling, especially for leagues such as the\nNational Basketball Association (NBA) where teams must travel long distances\nacross North America to play all their games, thus consuming much time, money,\nand greenhouse gas emissions. We introduce the Bipartite Traveling Tournament\nProblem (BTTP), the inter-league variant of the well-studied Traveling\nTournament Problem. We prove that the 2n-team BTTP is NP-complete, but for\nsmall values of n, a distance-optimal inter-league schedule can be generated\nfrom an algorithm based on minimum-weight 4-cycle-covers. We apply our\ntheoretical results to the 12-team Nippon Professional Baseball (NPB) league in\nJapan, producing a provably-optimal schedule requiring 42950 kilometres of\ntotal team travel, a 16% reduction compared to the actual distance traveled by\nthese teams during the 2010 NPB season. We also develop a nearly-optimal\ninter-league tournament for the 30-team NBA league, just 3.8% higher than the\ntrivial theoretical lower bound.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:24:01 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Hoshino", "Richard", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1401.3910", "submitter": "Peng Dai", "authors": "Peng Dai, Mausam, Daniel Sabby Weld, Judy Goldsmith", "title": "Topological Value Iteration Algorithms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  181-209, 2011", "doi": "10.1613/jair.3390", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value iteration is a powerful yet inefficient algorithm for Markov decision\nprocesses (MDPs) because it puts the majority of its effort into backing up the\nentire state space, which turns out to be unnecessary in many cases. In order\nto overcome this problem, many approaches have been proposed. Among them, ILAO*\nand variants of RTDP are state-of-the-art ones. These methods use reachability\nanalysis and heuristic search to avoid some unnecessary backups. However, none\nof these approaches build the graphical structure of the state transitions in a\npre-processing step or use the structural information to systematically\ndecompose a problem, whereby generating an intelligent backup sequence of the\nstate space. In this paper, we present two optimal MDP algorithms. The first\nalgorithm, topological value iteration (TVI), detects the structure of MDPs and\nbacks up states based on topological sequences. It (1) divides an MDP into\nstrongly-connected components (SCCs), and (2) solves these components\nsequentially. TVI outperforms VI and other state-of-the-art algorithms vastly\nwhen an MDP has multiple, close-to-equal-sized SCCs. The second algorithm,\nfocused topological value iteration (FTVI), is an extension of TVI. FTVI\nrestricts its attention to connected components that are relevant for solving\nthe MDP. Specifically, it uses a small amount of heuristic search to eliminate\nprovably sub-optimal actions; this pruning allows FTVI to find smaller\nconnected components, thus running faster. We demonstrate that FTVI outperforms\nTVI by an order of magnitude, averaged across several domains. Surprisingly,\nFTVI also significantly outperforms popular heuristically-informed MDP\nalgorithms such as ILAO*, LRTDP, BRTDP and Bayesian-RTDP in many domains,\nsometimes by as much as two orders of magnitude. Finally, we characterize the\ntype of domains where FTVI excels --- suggesting a way to an informed choice of\nsolver.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:24:38 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Dai", "Peng", ""], ["Mausam", "", ""], ["Weld", "Daniel Sabby", ""], ["Goldsmith", "Judy", ""]]}, {"id": "1401.4082", "submitter": "Shakir Mohamed", "authors": "Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra", "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative\n  Models", "comments": "Appears In Proceedings of the 31st International Conference on\n  Machine Learning (ICML), JMLR: W\\&CP volume 32, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We marry ideas from deep neural networks and approximate Bayesian inference\nto derive a generalised class of deep, directed generative models, endowed with\na new algorithm for scalable inference and learning. Our algorithm introduces a\nrecognition model to represent approximate posterior distributions, and that\nacts as a stochastic encoder of the data. We develop stochastic\nback-propagation -- rules for back-propagation through stochastic variables --\nand use this to develop an algorithm that allows for joint optimisation of the\nparameters of both the generative and recognition model. We demonstrate on\nseveral real-world data sets that the model generates realistic samples,\nprovides accurate imputations of missing data and is a useful tool for\nhigh-dimensional data visualisation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 16:33:23 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 12:53:17 GMT"}, {"version": "v3", "created": "Fri, 30 May 2014 10:00:36 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""], ["Wierstra", "Daan", ""]]}, {"id": "1401.4144", "submitter": "Yves Moinard", "authors": "Philippe Besnard (INRIA - IRISA, IRIT), Marie-Odile Cordier (INRIA -\n  IRISA, UR1), Yves Moinard (INRIA - IRISA)", "title": "Arguments using ontological and causal knowledge", "comments": null, "journal-ref": "JIAF 2013 (Septi\\`emes Journ\\'ees de l'Intelligence Artificielle\n  Fondamentale) (2013) 41-48", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an approach to reasoning about causes through argumentation.\nWe consider a causal model for a physical system, and look for arguments about\nfacts. Some arguments are meant to provide explanations of facts whereas some\nchallenge these explanations and so on. At the root of argumentation here, are\ncausal links ({A_1, ... ,A_n} causes B) and ontological links (o_1 is_a o_2).\nWe present a system that provides a candidate explanation ({A_1, ... ,A_n}\nexplains {B_1, ... ,B_m}) by resorting to an underlying causal link\nsubstantiated with appropriate ontological links. Argumentation is then at work\nfrom these various explaining links. A case study is developed: a severe storm\nXynthia that devastated part of France in 2010, with an unaccountably high\nnumber of casualties.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 19:49:42 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Besnard", "Philippe", "", "INRIA - IRISA, IRIT"], ["Cordier", "Marie-Odile", "", "INRIA -\n  IRISA, UR1"], ["Moinard", "Yves", "", "INRIA - IRISA"]]}, {"id": "1401.4539", "submitter": "S.M. Ferdous", "authors": "S.M. Ferdous, M. Sohel Rahman", "title": "Solving the Minimum Common String Partition Problem with the Help of\n  Ants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of finding a minimum common partition\nof two strings. The problem has its application in genome comparison. As it is\nan NP-hard, discrete combinatorial optimization problem, we employ a\nmetaheuristic technique, namely, MAX-MIN ant system to solve this problem. To\nachieve better efficiency we first map the problem instance into a special kind\nof graph. Subsequently, we employ a MAX-MIN ant system to achieve high quality\nsolutions for the problem. Experimental results show the superiority of our\nalgorithm in comparison with the state of art algorithm in the literature. The\nimprovement achieved is also justified by standard statistical test.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 13:15:30 GMT"}, {"version": "v2", "created": "Wed, 21 May 2014 06:35:41 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Ferdous", "S. M.", ""], ["Rahman", "M. Sohel", ""]]}, {"id": "1401.4590", "submitter": "Enrique Amig\\'o", "authors": "Enrique Amig\\'o, Julio Gonzalo, Javier Artiles, Felisa Verdejo", "title": "Combining Evaluation Metrics via the Unanimous Improvement Ratio and its\n  Application to Clustering Tasks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  689-718, 2011", "doi": "10.1613/jair.3401", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Artificial Intelligence tasks cannot be evaluated with a single quality\ncriterion and some sort of weighted combination is needed to provide system\nrankings. A problem of weighted combination measures is that slight changes in\nthe relative weights may produce substantial changes in the system rankings.\nThis paper introduces the Unanimous Improvement Ratio (UIR), a measure that\ncomplements standard metric combination criteria (such as van Rijsbergen's\nF-measure) and indicates how robust the measured differences are to changes in\nthe relative weights of the individual metrics. UIR is meant to elucidate\nwhether a perceived difference between two systems is an artifact of how\nindividual metrics are weighted.\n  Besides discussing the theoretical foundations of UIR, this paper presents\nempirical results that confirm the validity and usefulness of the metric for\nthe Text Clustering problem, where there is a tradeoff between precision and\nrecall based metrics and results are particularly sensitive to the weighting\nscheme used to combine them. Remarkably, our experiments show that UIR can be\nused as a predictor of how well differences between systems measured on a given\ntest bed will also hold in a different test bed.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:03:23 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Amig\u00f3", "Enrique", ""], ["Gonzalo", "Julio", ""], ["Artiles", "Javier", ""], ["Verdejo", "Felisa", ""]]}, {"id": "1401.4592", "submitter": "Jiri Baum", "authors": "Jiri Baum, Ann E. Nicholson, Trevor I. Dix", "title": "Proximity-Based Non-uniform Abstractions for Approximate Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  477-522, 2012", "doi": "10.1613/jair.3414", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a deterministic world, a planning agent can be certain of the consequences\nof its planned sequence of actions. Not so, however, in dynamic, stochastic\ndomains where Markov decision processes are commonly used. Unfortunately these\nsuffer from the curse of dimensionality: if the state space is a Cartesian\nproduct of many small sets (dimensions), planning is exponential in the number\nof those dimensions.\n  Our new technique exploits the intuitive strategy of selectively ignoring\nvarious dimensions in different parts of the state space. The resulting\nnon-uniformity has strong implications, since the approximation is no longer\nMarkovian, requiring the use of a modified planner. We also use a spatial and\ntemporal proximity measure, which responds to continued planning as well as\nmovement of the agent through the state space, to dynamically adapt the\nabstraction as planning progresses.\n  We present qualitative and quantitative results across a range of\nexperimental domains showing that an agent exploiting this novel approximation\nmethod successfully finds solutions to the planning problem using much less\nthan the full state space. We assess and analyse the features of domains which\nour method can exploit.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:03:58 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Baum", "Jiri", ""], ["Nicholson", "Ann E.", ""], ["Dix", "Trevor I.", ""]]}, {"id": "1401.4593", "submitter": "Adam Sadilek", "authors": "Adam Sadilek, Henry Kautz", "title": "Location-Based Reasoning about Complex Multi-Agent Behavior", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  87-133, 2012", "doi": "10.1613/jair.3421", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that surprisingly rich models of human activity can\nbe learned from GPS (positional) data. However, most effort to date has\nconcentrated on modeling single individuals or statistical properties of groups\nof people. Moreover, prior work focused solely on modeling actual successful\nexecutions (and not failed or attempted executions) of the activities of\ninterest. We, in contrast, take on the task of understanding human\ninteractions, attempted interactions, and intentions from noisy sensor data in\na fully relational multi-agent setting. We use a real-world game of capture the\nflag to illustrate our approach in a well-defined domain that involves many\ndistinct cooperative and competitive joint activities. We model the domain\nusing Markov logic, a statistical-relational language, and learn a theory that\njointly denoises the data and infers occurrences of high-level activities, such\nas a player capturing an enemy. Our unified model combines constraints imposed\nby the geometry of the game area, the motion model of the players, and by the\nrules and dynamics of the game in a probabilistically and logically sound\nfashion. We show that while it may be impossible to directly detect a\nmulti-agent activity due to sensor noise or malfunction, the occurrence of the\nactivity can still be inferred by considering both its impact on the future\nbehaviors of the people involved as well as the events that could have preceded\nit. Further, we show that given a model of successfully performed multi-agent\nactivities, along with a set of examples of failed attempts at the same\nactivities, our system automatically learns an augmented model that is capable\nof recognizing success and failure, as well as goals of peoples actions with\nhigh accuracy. We compare our approach with other alternatives and show that\nour unified model, which takes into account not only relationships among\nindividual players, but also relationships among activities over the entire\nlength of a game, although more computationally costly, is significantly more\naccurate. Finally, we demonstrate that explicitly modeling unsuccessful\nattempts boosts performance on other important recognition tasks.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:04:39 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Sadilek", "Adam", ""], ["Kautz", "Henry", ""]]}, {"id": "1401.4595", "submitter": "Na Fu", "authors": "Na Fu, Hoong Chuin Lau, Pradeep R. Varakantham, Fei Xiao", "title": "Robust Local Search for Solving RCPSP/max with Durational Uncertainty", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  43-86, 2012", "doi": "10.1613/jair.3424", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling problems in manufacturing, logistics and project management have\nfrequently been modeled using the framework of Resource Constrained Project\nScheduling Problems with minimum and maximum time lags (RCPSP/max). Due to the\nimportance of these problems, providing scalable solution schedules for\nRCPSP/max problems is a topic of extensive research. However, all existing\nmethods for solving RCPSP/max assume that durations of activities are known\nwith certainty, an assumption that does not hold in real world scheduling\nproblems where unexpected external events such as manpower availability,\nweather changes, etc. lead to delays or advances in completion of activities.\nThus, in this paper, our focus is on providing a scalable method for solving\nRCPSP/max problems with durational uncertainty. To that end, we introduce the\nrobust local search method consisting of three key ideas: (a) Introducing and\nstudying the properties of two decision rule approximations used to compute\nstart times of activities with respect to dynamic realizations of the\ndurational uncertainty; (b) Deriving the expression for robust makespan of an\nexecution strategy based on decision rule approximations; and (c) A robust\nlocal search mechanism to efficiently compute activity execution strategies\nthat are robust against durational uncertainty. Furthermore, we also provide\nenhancements to local search that exploit temporal dependencies between\nactivities. Our experimental results illustrate that robust local search is\nable to provide robust execution strategies efficiently.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:04:55 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Fu", "Na", ""], ["Lau", "Hoong Chuin", ""], ["Varakantham", "Pradeep R.", ""], ["Xiao", "Fei", ""]]}, {"id": "1401.4596", "submitter": "Mario Alviano", "authors": "Mario Alviano, Francesco Calimeri, Wolfgang Faber, Nicola Leone,\n  Simona Perri", "title": "Unfounded Sets and Well-Founded Semantics of Answer Set Programs with\n  Aggregates", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  487-527, 2011", "doi": "10.1613/jair.3432", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic programs with aggregates (LPA) are one of the major linguistic\nextensions to Logic Programming (LP). In this work, we propose a generalization\nof the notions of unfounded set and well-founded semantics for programs with\nmonotone and antimonotone aggregates (LPAma programs). In particular, we\npresent a new notion of unfounded set for LPAma programs, which is a sound\ngeneralization of the original definition for standard (aggregate-free) LP. On\nthis basis, we define a well-founded operator for LPAma programs, the fixpoint\nof which is called well-founded model (or well-founded semantics) for LPAma\nprograms. The most important properties of unfounded sets and the well-founded\nsemantics for standard LP are retained by this generalization, notably\nexistence and uniqueness of the well-founded model, together with a strong\nrelationship to the answer set semantics for LPAma programs. We show that one\nof the D-well-founded semantics, defined by Pelov, Denecker, and Bruynooghe for\na broader class of aggregates using approximating operators, coincides with the\nwell-founded model as defined in this work on LPAma programs. We also discuss\nsome complexity issues, most importantly we give a formal proof of tractable\ncomputation of the well-founded model for LPA programs. Moreover, we prove that\nfor general LPA programs, which may contain aggregates that are neither\nmonotone nor antimonotone, deciding satisfaction of aggregate expressions with\nrespect to partial interpretations is coNP-complete. As a consequence, a\nwell-founded semantics for general LPA programs that allows for tractable\ncomputation is unlikely to exist, which justifies the restriction on LPAma\nprograms. Finally, we present a prototype system extending DLV, which supports\nthe well-founded semantics for LPAma programs, at the time of writing the only\nimplemented system that does so. Experiments with this prototype show\nsignificant computational advantages of aggregate constructs over equivalent\naggregate-free encodings.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:05:13 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Alviano", "Mario", ""], ["Calimeri", "Francesco", ""], ["Faber", "Wolfgang", ""], ["Leone", "Nicola", ""], ["Perri", "Simona", ""]]}, {"id": "1401.4597", "submitter": "Matthew L. Ginsberg", "authors": "Matthew L. Ginsberg", "title": "Dr.Fill: Crosswords and an Implemented Solver for Singly Weighted CSPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  851-886, 2011", "doi": "10.1613/jair.3437", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Dr.Fill, a program that solves American-style crossword puzzles.\nFrom a technical perspective, Dr.Fill works by converting crosswords to\nweighted CSPs, and then using a variety of novel techniques to find a solution.\nThese techniques include generally applicable heuristics for variable and value\nselection, a variant of limited discrepancy search, and postprocessing and\npartitioning ideas. Branch and bound is not used, as it was incompatible with\npostprocessing and was determined experimentally to be of little practical\nvalue. Dr.Fillls performance on crosswords from the American Crossword Puzzle\nTournament suggests that it ranks among the top fifty or so crossword solvers\nin the world.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:05:30 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Ginsberg", "Matthew L.", ""]]}, {"id": "1401.4598", "submitter": "Ruoyun Huang", "authors": "Ruoyun Huang, Yixin Chen, Weixiong Zhang", "title": "SAS+ Planning as Satisfiability", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  293-328, 2012", "doi": "10.1613/jair.3442", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning as satisfiability is a principal approach to planning with many\neminent advantages. The existing planning as satisfiability techniques usually\nuse encodings compiled from STRIPS. We introduce a novel SAT encoding scheme\n(SASE) based on the SAS+ formalism. The new scheme exploits the structural\ninformation in SAS+, resulting in an encoding that is both more compact and\nefficient for planning. We prove the correctness of the new encoding by\nestablishing an isomorphism between the solution plans of SASE and that of\nSTRIPS based encodings. We further analyze the transition variables newly\nintroduced in SASE to explain why it accommodates modern SAT solving algorithms\nand improves performance. We give empirical statistical results to support our\nanalysis. We also develop a number of techniques to further reduce the encoding\nsize of SASE, and conduct experimental studies to show the strength of each\nindividual technique. Finally, we report extensive experimental results to\ndemonstrate significant improvements of SASE over the state-of-the-art STRIPS\nbased encoding schemes in terms of both time and memory efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:05:52 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Huang", "Ruoyun", ""], ["Chen", "Yixin", ""], ["Zhang", "Weixiong", ""]]}, {"id": "1401.4599", "submitter": "Freek Stulp", "authors": "Freek Stulp, Andreas Fedrizzi, Lorenz M\\\"osenlechner, Michael Beetz", "title": "Learning and Reasoning with Action-Related Places for Robust Mobile\n  Manipulation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  1-42, 2012", "doi": "10.1613/jair.3451", "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the concept of Action-Related Place (ARPlace) as a powerful and\nflexible representation of task-related place in the context of mobile\nmanipulation. ARPlace represents robot base locations not as a single position,\nbut rather as a collection of positions, each with an associated probability\nthat the manipulation action will succeed when located there. ARPlaces are\ngenerated using a predictive model that is acquired through experience-based\nlearning, and take into account the uncertainty the robot has about its own\nlocation and the location of the object to be manipulated.\n  When executing the task, rather than choosing one specific goal position\nbased only on the initial knowledge about the task context, the robot\ninstantiates an ARPlace, and bases its decisions on this ARPlace, which is\nupdated as new information about the task becomes available. To show the\nadvantages of this least-commitment approach, we present a transformational\nplanner that reasons about ARPlaces in order to optimize symbolic plans. Our\nempirical evaluation demonstrates that using ARPlaces leads to more robust and\nefficient mobile manipulation in the face of state estimation uncertainty on\nour simulated robot.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:08:22 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Stulp", "Freek", ""], ["Fedrizzi", "Andreas", ""], ["M\u00f6senlechner", "Lorenz", ""], ["Beetz", "Michael", ""]]}, {"id": "1401.4600", "submitter": "Yifeng Zeng", "authors": "Yifeng Zeng, Prashant Doshi", "title": "Exploiting Model Equivalences for Solving Interactive Dynamic Influence\n  Diagrams", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  211-255, 2012", "doi": "10.1613/jair.3461", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of sequential decision making in partially observable\nenvironments shared with other agents of uncertain types having similar or\nconflicting objectives. This problem has been previously formalized by multiple\nframeworks one of which is the interactive dynamic influence diagram (I-DID),\nwhich generalizes the well-known influence diagram to the multiagent setting.\nI-DIDs are graphical models and may be used to compute the policy of an agent\ngiven its belief over the physical state and others models, which changes as\nthe agent acts and observes in the multiagent setting.\n  As we may expect, solving I-DIDs is computationally hard. This is\npredominantly due to the large space of candidate models ascribed to the other\nagents and its exponential growth over time. We present two methods for\nreducing the size of the model space and stemming its exponential growth. Both\nthese methods involve aggregating individual models into equivalence classes.\nOur first method groups together behaviorally equivalent models and selects\nonly those models for updating which will result in predictive behaviors that\nare distinct from others in the updated model space. The second method further\ncompacts the model space by focusing on portions of the behavioral predictions.\nSpecifically, we cluster actionally equivalent models that prescribe identical\nactions at a single time step. Exactly identifying the equivalences would\nrequire us to solve all models in the initial set. We avoid this by selectively\nsolving some of the models, thereby introducing an approximation. We discuss\nthe error introduced by the approximation, and empirically demonstrate the\nimproved efficiency in solving I-DIDs due to the equivalences.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:09:03 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Zeng", "Yifeng", ""], ["Doshi", "Prashant", ""]]}, {"id": "1401.4601", "submitter": "Gilles Pesant", "authors": "Gilles Pesant, Claude-Guy Quimper, Alessandro Zanarini", "title": "Counting-Based Search: Branching Heuristics for Constraint Satisfaction\n  Problems", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  173-210, 2012", "doi": "10.1613/jair.3463", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a search heuristic for constraint programming that is reliable\nacross problem domains has been an important research topic in recent years.\nThis paper concentrates on one family of candidates: counting-based search.\nSuch heuristics seek to make branching decisions that preserve most of the\nsolutions by determining what proportion of solutions to each individual\nconstraint agree with that decision. Whereas most generic search heuristics in\nconstraint programming rely on local information at the level of the individual\nvariable, our search heuristics are based on more global information at the\nconstraint level. We design several algorithms that are used to count the\nnumber of solutions to specific families of constraints and propose some search\nheuristics exploiting such information. The experimental part of the paper\nconsiders eight problem domains ranging from well-established benchmark puzzles\nto rostering and sport scheduling. An initial empirical analysis identifies\nheuristic maxSD as a robust candidate among our proposals.eWe then evaluate the\nlatter against the state of the art, including the latest generic search\nheuristics, restarts, and discrepancy-based tree traversals. Experimental\nresults show that counting-based search generally outperforms other generic\nheuristics.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:09:25 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Pesant", "Gilles", ""], ["Quimper", "Claude-Guy", ""], ["Zanarini", "Alessandro", ""]]}, {"id": "1401.4603", "submitter": "Esperanza Albacete", "authors": "Esperanza Albacete, Javier Calle, Elena Castro, Dolores Cuadra", "title": "Semantic Similarity Measures Applied to an Ontology for Human-Like\n  Interaction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  397-421, 2012", "doi": "10.1613/jair.3612", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is the calculation of similarity between two concepts\nfrom an ontology for a Human-Like Interaction system. In order to facilitate\nthis calculation, a similarity function is proposed based on five dimensions\n(sort, compositional, essential, restrictive and descriptive) constituting the\nstructure of ontological knowledge. The paper includes a proposal for computing\na similarity function for each dimension of knowledge. Later on, the similarity\nvalues obtained are weighted and aggregated to obtain a global similarity\nmeasure. In order to calculate those weights associated to each dimension, four\ntraining methods have been proposed. The training methods differ in the element\nto fit: the user, concepts or pairs of concepts, and a hybrid approach. For\nevaluating the proposal, the knowledge base was fed from WordNet and extended\nby using a knowledge editing toolkit (Cognos). The evaluation of the proposal\nis carried out through the comparison of system responses with those given by\nhuman test subjects, both providing a measure of the soundness of the procedure\nand revealing ways in which the proposal may be improved.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:09:42 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Albacete", "Esperanza", ""], ["Calle", "Javier", ""], ["Castro", "Elena", ""], ["Cuadra", "Dolores", ""]]}, {"id": "1401.4604", "submitter": "Bernardo Cuenca Grau", "authors": "Bernardo Cuenca Grau, Boris Motik, Giorgos Stoilos, Ian Horrocks", "title": "Completeness Guarantees for Incomplete Ontology Reasoners: Theory and\n  Practice", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  419-476, 2012", "doi": "10.1613/jair.3470", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve scalability of query answering, the developers of Semantic Web\napplications are often forced to use incomplete OWL 2 reasoners, which fail to\nderive all answers for at least one query, ontology, and data set. The lack of\ncompleteness guarantees, however, may be unacceptable for applications in areas\nsuch as health care and defence, where missing answers can adversely affect the\napplications functionality. Furthermore, even if an application can tolerate\nsome level of incompleteness, it is often advantageous to estimate how many and\nwhat kind of answers are being lost.\n  In this paper, we present a novel logic-based framework that allows one to\ncheck whether a reasoner is complete for a given query Q and ontology T---that\nis, whether the reasoner is guaranteed to compute all answers to Q w.r.t. T and\nan arbitrary data set A. Since ontologies and typical queries are often fixed\nat application design time, our approach allows application developers to check\nwhether a reasoner known to be incomplete in general is actually complete for\nthe kinds of input relevant for the application.\n  We also present a technique that, given a query Q, an ontology T, and\nreasoners R_1 and R_2 that satisfy certain assumptions, can be used to\ndetermine whether, for each data set A, reasoner R_1 computes more answers to Q\nw.r.t. T and A than reasoner R_2. This allows application developers to select\nthe reasoner that provides the highest degree of completeness for Q and T that\nis compatible with the applications scalability requirements.\n  Our results thus provide a theoretical and practical foundation for the\ndesign of future ontology-based information systems that maximise scalability\nwhile minimising or even eliminating incompleteness of query answers.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:10:03 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Grau", "Bernardo Cuenca", ""], ["Motik", "Boris", ""], ["Stoilos", "Giorgos", ""], ["Horrocks", "Ian", ""]]}, {"id": "1401.4605", "submitter": "J.H.M. Lee", "authors": "J.H.M. Lee, Ka Lun Leung", "title": "Consistency Techniques for Flow-Based Projection-Safe Global Cost\n  Functions in Weighted Constraint Satisfaction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  257-292, 2012", "doi": "10.1613/jair.3476", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many combinatorial problems deal with preferences and violations, the goal of\nwhich is to find solutions with the minimum cost. Weighted constraint\nsatisfaction is a framework for modeling such problems, which consists of a set\nof cost functions to measure the degree of violation or preferences of\ndifferent combinations of variable assignments. Typical solution methods for\nweighted constraint satisfaction problems (WCSPs) are based on branch-and-bound\nsearch, which are made practical through the use of powerful consistency\ntechniques such as AC*, FDAC*, EDAC* to deduce hidden cost information and\nvalue pruning during search. These techniques, however, are designed to be\nefficient only on binary and ternary cost functions which are represented in\ntable form. In tackling many real-life problems, high arity (or global) cost\nfunctions are required. We investigate efficient representation scheme and\nalgorithms to bring the benefits of the consistency techniques to also high\narity cost functions, which are often derived from hard global constraints from\nclassical constraint satisfaction. The literature suggests some global cost\nfunctions can be represented as flow networks, and the minimum cost flow\nalgorithm can be used to compute the minimum costs of such networks in\npolynomial time. We show that naive adoption of this flow-based algorithmic\nmethod for global cost functions can result in a stronger form of null-inverse\nconsistency. We further show how the method can be modified to handle cost\nprojections and extensions to maintain generalized versions of AC* and FDAC*\nfor cost functions with more than two variables. Similar generalization for the\nstronger EDAC* is less straightforward. We reveal the oscillation problem when\nenforcing EDAC* on cost functions sharing more than one variable. To avoid\noscillation, we propose a weak version of EDAC* and generalize it to weak\nEDGAC* for non-binary cost functions. Using various benchmarks involving the\nsoft variants of hard global constraints ALLDIFFERENT, GCC, SAME, and REGULAR,\nempirical results demonstrate that our proposal gives improvements of up to an\norder of magnitude when compared with the traditional constraint optimization\napproach, both in terms of time and pruning.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:10:22 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Lee", "J. H. M.", ""], ["Leung", "Ka Lun", ""]]}, {"id": "1401.4606", "submitter": "Patrick Raymond Conrad", "authors": "Patrick Raymond Conrad, Brian Williams", "title": "Drake: An Efficient Executive for Temporal Plans with Choice", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  607-659, 2011", "doi": "10.1613/jair.3478", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents Drake, a dynamic executive for temporal plans with choice.\nDynamic plan execution strategies allow an autonomous agent to react quickly to\nunfolding events, improving the robustness of the agent. Prior work developed\nmethods for dynamically dispatching Simple Temporal Networks, and further\nresearch enriched the expressiveness of the plans executives could handle,\nincluding discrete choices, which are the focus of this work. However, in some\napproaches to date, these additional choices induce significant storage or\nlatency requirements to make flexible execution possible.\n  Drake is designed to leverage the low latency made possible by a\npreprocessing step called compilation, while avoiding high memory costs through\na compact representation. We leverage the concepts of labels and environments,\ntaken from prior work in Assumption-based Truth Maintenance Systems (ATMS), to\nconcisely record the implications of the discrete choices, exploiting the\nstructure of the plan to avoid redundant reasoning or storage. Our labeling and\nmaintenance scheme, called the Labeled Value Set Maintenance System, is\ndistinguished by its focus on properties fundamental to temporal problems, and,\nmore generally, weighted graph algorithms. In particular, the maintenance\nsystem focuses on maintaining a minimal representation of non-dominated\nconstraints. We benchmark Drakes performance on random structured problems, and\nfind that Drake reduces the size of the compiled representation by a factor of\nover 500 for large problems, while incurring only a modest increase in run-time\nlatency, compared to prior work in compiled executives for temporal plans with\ndiscrete choices.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:10:40 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Conrad", "Patrick Raymond", ""], ["Williams", "Brian", ""]]}, {"id": "1401.4607", "submitter": "Joohyung Lee", "authors": "Joohyung Lee, Ravi Palla", "title": "Reformulating the Situation Calculus and the Event Calculus in the\n  General Theory of Stable Models and in Answer Set Programming", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  571-620, 2012", "doi": "10.1613/jair.3489", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Circumscription and logic programs under the stable model semantics are two\nwell-known nonmonotonic formalisms. The former has served as a basis of\nclassical logic based action formalisms, such as the situation calculus, the\nevent calculus and temporal action logics; the latter has served as a basis of\na family of action languages, such as language A and several of its\ndescendants. Based on the discovery that circumscription and the stable model\nsemantics coincide on a class of canonical formulas, we reformulate the\nsituation calculus and the event calculus in the general theory of stable\nmodels. We also present a translation that turns the reformulations further\ninto answer set programs, so that efficient answer set solvers can be applied\nto compute the situation calculus and the event calculus.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:11:15 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Lee", "Joohyung", ""], ["Palla", "Ravi", ""]]}, {"id": "1401.4609", "submitter": "L\\'eon R. Planken", "authors": "L\\'eon R. Planken, Mathijs M. de Weerdt, Roman P.J. van der Krogt", "title": "Computing All-Pairs Shortest Paths by Leveraging Low Treewidth", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  353-388, 2012", "doi": "10.1613/jair.3509", "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new and efficient algorithms for computing all-pairs shortest\npaths. The algorithms operate on directed graphs with real (possibly negative)\nweights. They make use of directed path consistency along a vertex ordering d.\nBoth algorithms run in O(n^2 w_d) time, where w_d is the graph width induced by\nthis vertex ordering. For graphs of constant treewidth, this yields O(n^2)\ntime, which is optimal. On chordal graphs, the algorithms run in O(nm) time. In\naddition, we present a variant that exploits graph separators to arrive at a\nrun time of O(n w_d^2 + n^2 s_d) on general graphs, where s_d andlt= w_d is the\nsize of the largest minimal separator induced by the vertex ordering d. We show\nempirically that on both constructed and realistic benchmarks, in many cases\nthe algorithms outperform Floyd-Warshalls as well as Johnsons algorithm, which\nrepresent the current state of the art with a run time of O(n^3) and O(nm + n^2\nlog n), respectively. Our algorithms can be used for spatial and temporal\nreasoning, such as for the Simple Temporal Problem, which underlines their\nrelevance to the planning and scheduling community.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:23:48 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Planken", "L\u00e9on R.", ""], ["de Weerdt", "Mathijs M.", ""], ["van der Krogt", "Roman P. J.", ""]]}, {"id": "1401.4613", "submitter": "Peter Jeavons", "authors": "Peter Jeavons, Justyna Petke", "title": "Local Consistency and SAT-Solvers", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  329-351, 2012", "doi": "10.1613/jair.3531", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local consistency techniques such as k-consistency are a key component of\nspecialised solvers for constraint satisfaction problems. In this paper we show\nthat the power of using k-consistency techniques on a constraint satisfaction\nproblem is precisely captured by using a particular inference rule, which we\ncall negative-hyper-resolution, on the standard direct encoding of the problem\ninto Boolean clauses. We also show that current clause-learning SAT-solvers\nwill discover in expected polynomial time any inconsistency that can be deduced\nfrom a given set of clauses using negative-hyper-resolvents of a fixed size. We\ncombine these two results to show that, without being explicitly designed to do\nso, current clause-learning SAT-solvers efficiently simulate k-consistency\ntechniques, for all fixed values of k. We then give some experimental results\nto show that this feature allows clause-learning SAT-solvers to efficiently\nsolve certain families of constraint problems which are challenging for\nconventional constraint-programming solvers.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:39:00 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Jeavons", "Peter", ""], ["Petke", "Justyna", ""]]}, {"id": "1401.4869", "submitter": "Taraka Rama Kasicheyanula", "authors": "Taraka Rama, Karthik Gali, Avinesh PVS", "title": "Does Syntactic Knowledge help English-Hindi SMT?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we explore various parameter settings of the state-of-art\nStatistical Machine Translation system to improve the quality of the\ntranslation for a `distant' language pair like English-Hindi. We proposed new\ntechniques for efficient reordering. A slight improvement over the baseline is\nreported using these techniques. We also show that a simple pre-processing step\ncan improve the quality of the translation significantly.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 11:49:11 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Rama", "Taraka", ""], ["Gali", "Karthik", ""], ["PVS", "Avinesh", ""]]}, {"id": "1401.4942", "submitter": "Gordana Dodig Crnkovic", "authors": "Gordana Dodig-Crnkovic", "title": "Info-computational constructivism in modelling of life as cognition", "comments": "5 pages, SMLC conference University of Bergamo 12-14.09.2013,\n  http://www.pt-ai.org/smlc/2013/schedule", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper addresses the open question formulated as: Which levels of\nabstraction are appropriate in the synthetic modelling of life and cognition?\nwithin the framework of info-computational constructivism, treating natural\nphenomena as computational processes on informational structures. At present we\nlack the common understanding of the processes of life and cognition in living\norganisms with the details of co-construction of informational structures and\ncomputational processes in embodied, embedded cognizing agents, both living and\nartifactual ones. Starting with the definition of an agent as an entity capable\nof acting on its own behalf, as an actor in Hewitt Actor model of computation,\neven so simple systems as molecules can be modelled as actors exchanging\nmessages (information). We adopt Kauffmans view of a living agent as something\nthat can reproduce and undergoes at least one thermodynamic work cycle. This\ndefinition of living agents leads to the Maturana and Varelas identification of\nlife with cognition. Within the info-computational constructive approach to\nliving beings as cognizing agents, from the simplest to the most complex living\nsystems, mechanisms of cognition can be studied in order to construct synthetic\nmodel classes of artifactual cognizing agents on different levels of\norganization.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 21:43:45 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Dodig-Crnkovic", "Gordana", ""]]}, {"id": "1401.5031", "submitter": "Joseph Ramsey", "authors": "Joseph D. Ramsey", "title": "A Scalable Conditional Independence Test for Nonlinear, Non-Gaussian\n  Data", "comments": "4 Figures, 2 Boxes, 1 Table, 15 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many relations of scientific interest are nonlinear, and even in linear\nsystems distributions are often non-Gaussian, for example in fMRI BOLD data. A\nclass of search procedures for causal relations in high dimensional data relies\non sample derived conditional independence decisions. The most common\napplications rely on Gaussian tests that can be systematically erroneous in\nnonlinear non-Gaussian cases. Recent work (Gretton et al. (2009), Tillman et\nal. (2009), Zhang et al. (2011)) has proposed conditional independence tests\nusing Reproducing Kernel Hilbert Spaces (RKHS). Among these, perhaps the most\nefficient has been KCI (Kernel Conditional Independence, Zhang et al. (2011)),\nwith computational requirements that grow effectively at least as O(N3),\nplacing it out of range of large sample size analysis, and restricting its\napplicability to high dimensional data sets. We propose a class of O(N2) tests\nusing conditional correlation independence (CCI) that require a few seconds on\na standard workstation for tests that require tens of minutes to hours for the\nKCI method, depending on degree of parallelization, with similar accuracy. For\naccuracy on difficult nonlinear, non-Gaussian data sets, we also compare a\nrecent test due to Harris & Drton (2012), applicable to nonlinear, non-Gaussian\ndistributions in the Gaussian copula, as well as to partial correlation, a\nlinear Gaussian test.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 19:54:27 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2014 16:05:12 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Ramsey", "Joseph D.", ""]]}, {"id": "1401.5054", "submitter": "Jose Alberto Garc\\'ia Guti\\'errez Sr.", "authors": "Jos\\'e Alberto Garc\\'ia Guti\\'errez, Alejandro Mateo Hern\\'andez\n  D\\'iaz", "title": "An\\'alisis e implementaci\\'on de algoritmos evolutivos para la\n  optimizaci\\'on de simulaciones en ingenier\\'ia civil. (draft)", "comments": "In Spanish. The authors acknowledge corrections, comments and\n  suggestions to this paper, thanks.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the applicability of evolutionary algorithms,\nparticularly, the evolution strategies family in order to estimate a\ndegradation parameter in the shear design of reinforced concrete members. This\nproblem represents a great computational task and is highly relevant in the\nframework of the structural engineering that for the first time is solved using\ngenetic algorithms.\n  You are viewing a draft, the authors appreciate corrections, comments and\nsuggestions to this work.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 20:55:13 GMT"}, {"version": "v2", "created": "Sun, 6 Apr 2014 19:29:19 GMT"}, {"version": "v3", "created": "Sat, 21 Jun 2014 20:07:11 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Guti\u00e9rrez", "Jos\u00e9 Alberto Garc\u00eda", ""], ["D\u00edaz", "Alejandro Mateo Hern\u00e1ndez", ""]]}, {"id": "1401.5156", "submitter": "Juliana Wahid", "authors": "Juliana Wahid, Naimah Mohd Hussin", "title": "Harmony Search Algorithm for Curriculum-Based Course Timetabling Problem", "comments": null, "journal-ref": "International Journal of Soft Computing and Software Engineering\n  [JSCSE], Vol. 3, No. 3, pp. 365-371, 2013", "doi": "10.7321/jscse.v3.n3.55", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, harmony search algorithm is applied to curriculum-based course\ntimetabling. The implementation, specifically the process of improvisation\nconsists of memory consideration, random consideration and pitch adjustment. In\nmemory consideration, the value of the course number for new solution was\nselected from all other course number located in the same column of the Harmony\nMemory. This research used the highest occurrence of the course number to be\nscheduled in a new harmony. The remaining courses that have not been scheduled\nby memory consideration will go through random consideration, i.e. will select\nany feasible location available to be scheduled in the new harmony solution.\nEach course scheduled out of memory consideration is examined as to whether it\nshould be pitch adjusted with probability of eight procedures. However, the\nalgorithm produced results that were not comparatively better than those\npreviously known as best solution. With proper modification in terms of the\napproach in this algorithm would make the algorithm perform better on\ncurriculum-based course timetabling.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 03:01:50 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Wahid", "Juliana", ""], ["Hussin", "Naimah Mohd", ""]]}, {"id": "1401.5157", "submitter": "Toshiyuki Maeda", "authors": "Toshiyuki Maeda, Masanori Fujii, Isao Hayashi", "title": "Skill Analysis with Time Series Image Data", "comments": "5 pages, 6 figures", "journal-ref": "International Journal of Soft Computing and Software Engineering\n  [JSCSE], Vol. 3, No. 3, pp. 576-580, 2013", "doi": "10.7321/jscse.v3.n3.87", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a skill analysis with time series image data using data mining\nmethods, focused on table tennis. We do not use body model, but use only\nhi-speed movies, from which time series data are obtained and analyzed using\ndata mining methods such as C4.5 and so on. We identify internal models for\ntechnical skills as evaluation skillfulness for the forehand stroke of table\ntennis, and discuss mono and meta-functional skills for improving skills.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 03:03:56 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Maeda", "Toshiyuki", ""], ["Fujii", "Masanori", ""], ["Hayashi", "Isao", ""]]}, {"id": "1401.5327", "submitter": "Dimitri Kartsaklis", "authors": "Dimitri Kartsaklis", "title": "Compositional Operators in Distributional Semantics", "comments": null, "journal-ref": null, "doi": "10.1007/s40362-014-0017-z", "report-no": null, "categories": "cs.CL cs.AI math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey presents in some detail the main advances that have been recently\ntaking place in Computational Linguistics towards the unification of the two\nprominent semantic paradigms: the compositional formal semantics view and the\ndistributional models of meaning based on vector spaces. After an introduction\nto these two approaches, I review the most important models that aim to provide\ncompositionality in distributional semantics. Then I proceed and present in\nmore detail a particular framework by Coecke, Sadrzadeh and Clark (2010) based\non the abstract mathematical setting of category theory, as a more complete\nexample capable to demonstrate the diversity of techniques and scientific\ndisciplines that this kind of research can draw from. This paper concludes with\na discussion about important open issues that need to be addressed by the\nresearchers in the future.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 14:28:01 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Kartsaklis", "Dimitri", ""]]}, {"id": "1401.5334", "submitter": "Laurent  Michel D", "authors": "Laurent Michel and Pascal Van Hentenryck", "title": "A Microkernel Architecture for Constraint Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a microkernel architecture for constraint programming\norganized around a number of small number of core functionalities and minimal\ninterfaces. The architecture contrasts with the monolithic nature of many\nimplementations. Experimental results indicate that the software engineering\nbenefits are not incompatible with runtime efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 14:56:14 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Michel", "Laurent", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1401.5341", "submitter": "Laurent  Michel D", "authors": "Pascal Van Hentenryck and Laurent Michel", "title": "Domain Views for Constraint Programming", "comments": "Workshop: TRICS13: Techniques foR Implementing Constraint\n  programming, September 2013, CP, Uppsala", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Views are a standard abstraction in constraint programming: They make it\npossible to implement a single version of each constraint, while avoiding to\ncreate new variables and constraints that would slow down propagation.\nTraditional constraint-programming systems provide the concept of {\\em variable\nviews} which implement a view of the type $y = f(x)$ by delegating all (domain\nand constraint) operations on variable $y$ to variable $x$. This paper proposes\nthe alternative concept of {\\em domain views} which only delegate domain\noperations. Domain views preserve the benefits of variable views but simplify\nthe implementation of value-based propagation. Domain views also support\nnon-injective views compositionally, expanding the scope of views\nsignificantly. Experimental results demonstrate the practical benefits of\ndomain views.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 15:22:29 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Van Hentenryck", "Pascal", ""], ["Michel", "Laurent", ""]]}, {"id": "1401.5390", "submitter": "S.R.K. Branavan", "authors": "S.R.K. Branavan, David Silver, Regina Barzilay", "title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  661-704, 2012", "doi": "10.1613/jair.3484", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain knowledge is crucial for effective performance in autonomous control\nsystems. Typically, human effort is required to encode this knowledge into a\ncontrol algorithm. In this paper, we present an approach to language grounding\nwhich automatically interprets text in the context of a complex control\napplication, such as a game, and uses domain knowledge extracted from the text\nto improve control performance. Both text analysis and control strategies are\nlearned jointly using only a feedback signal inherent to the application. To\neffectively leverage textual information, our method automatically extracts the\ntext segment most relevant to the current game state, and labels it with a\ntask-centric predicate structure. This labeled text is then used to bias an\naction selection policy for the game, guiding it towards promising regions of\nthe action space. We encode our model for text analysis and game playing in a\nmulti-layer neural network, representing linguistic decisions via latent\nvariables in the hidden layers, and game action quality via the output layer.\nOperating within the Monte-Carlo Search framework, we estimate model parameters\nusing feedback from simulated games. We apply our approach to the complex\nstrategy game Civilization II using the official game manual as the text guide.\nOur results show that a linguistically-informed game-playing agent\nsignificantly outperforms its language-unaware counterpart, yielding a 34%\nabsolute improvement and winning over 65% of games when playing against the\nbuilt-in AI of Civilization.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:10:57 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Branavan", "S. R. K.", ""], ["Silver", "David", ""], ["Barzilay", "Regina", ""]]}, {"id": "1401.5424", "submitter": "Roy Hayes Jr", "authors": "Roy Hayes, Peter Beling, William Scherer", "title": "Real Time Strategy Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real Time Strategy (RTS) games provide complex domain to test the latest\nartificial intelligence (AI) research. In much of the literature, AI systems\nhave been limited to playing one game. Although, this specialization has\nresulted in stronger AI gaming systems it does not address the key concerns of\nAI researcher. AI researchers seek the development of AI agents that can\nautonomously interpret learn, and apply new knowledge. To achieve human level\nperformance, current AI systems rely on game specific knowledge of an expert.\nThe paper presents the full RTS language in hopes of shifting the current\nresearch focus to the development of general RTS agents. General RTS agents are\nAI gaming systems that can play any RTS games, defined in the RTS language.\nThis prevents game specific knowledge from being hard coded into the system,\nthereby facilitating research that addresses the fundamental concerns of\nartificial intelligence.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 19:14:22 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Hayes", "Roy", ""], ["Beling", "Peter", ""], ["Scherer", "William", ""]]}, {"id": "1401.5813", "submitter": "Adrian Lancucki", "authors": "Adrian {\\L}a\\'ncucki", "title": "GGP with Advanced Reasoning and Board Knowledge Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality of General Game Playing (GGP) matches suffers from slow\nstate-switching and weak knowledge modules. Instantiation and Propositional\nNetworks offer great performance gains over Prolog-based reasoning, but do not\nscale well. In this publication mGDL, a variant of GDL stripped of function\nconstants, has been defined as a basis for simple reasoning machines. mGDL\nallows to easily map rules to C++ functions. 253 out of 270 tested GDL rule\nsheets conformed to mGDL without any modifications; the rest required minor\nchanges. A revised (m)GDL to C++ translation scheme has been reevaluated; it\nbrought gains ranging from 28% to 7300% over YAP Prolog, managing to compile\neven demanding rule sheets under few seconds. For strengthening game knowledge,\nspatial features inspired by similar successful techniques from computer Go\nhave been proposed. For they required an Euclidean metric, a small board\nextension to GDL has been defined through a set of ground atomic sentences. An\nSGA-based genetic algorithm has been designed for tweaking game parameters and\nconducting self-plays, so the features could be mined from meaningful game\nrecords. The approach has been tested on a small cluster, giving performance\ngains up to 20% more wins against the baseline UCT player. Implementations of\nproposed ideas constitutes the core of GGP Spatium - a small C++/Python GGP\nframework, created for developing compact GGP Players and problem solvers.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 21:52:49 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["\u0141a\u0144cucki", "Adrian", ""]]}, {"id": "1401.5848", "submitter": "Christer B\\\"ackstr\\\"om", "authors": "Christer B\\\"ackstr\\\"om, Peter Jonsson", "title": "Algorithms and Limits for Compact Plan Representations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  141-177, 2012", "doi": "10.1613/jair.3534", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compact representations of objects is a common concept in computer science.\nAutomated planning can be viewed as a case of this concept: a planning instance\nis a compact implicit representation of a graph and the problem is to find a\npath (a plan) in this graph. While the graphs themselves are represented\ncompactly as planning instances, the paths are usually represented explicitly\nas sequences of actions. Some cases are known where the plans always have\ncompact representations, for example, using macros. We show that these results\ndo not extend to the general case, by proving a number of bounds for compact\nrepresentations of plans under various criteria, like efficient sequential or\nrandom access of actions. In addition to this, we show that our results have\nconsequences for what can be gained from reformulating planning into some other\nproblem. As a contrast to this we also prove a number of positive results,\ndemonstrating restricted cases where plans do have useful compact\nrepresentations, as well as proving that macro plans have favourable access\nproperties. Our results are finally discussed in relation to other relevant\ncontexts.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:41:51 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["B\u00e4ckstr\u00f6m", "Christer", ""], ["Jonsson", "Peter", ""]]}, {"id": "1401.5849", "submitter": "Francesco Belardinelli", "authors": "Francesco Belardinelli, Alessio Lomuscio", "title": "Interactions between Knowledge and Time in a First-Order Logic for\n  Multi-Agent Systems: Completeness Results", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  1-45, 2012", "doi": "10.1613/jair.3547", "report-no": null, "categories": "cs.MA cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a class of first-order temporal-epistemic logics for reasoning\nabout multi-agent systems. We encode typical properties of systems including\nperfect recall, synchronicity, no learning, and having a unique initial state\nin terms of variants of quantified interpreted systems, a first-order extension\nof interpreted systems. We identify several monodic fragments of first-order\ntemporal-epistemic logic and show their completeness with respect to their\ncorresponding classes of quantified interpreted systems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:43:00 GMT"}], "update_date": "2014-01-25", "authors_parsed": [["Belardinelli", "Francesco", ""], ["Lomuscio", "Alessio", ""]]}, {"id": "1401.5850", "submitter": "Boris  Konev", "authors": "Boris Konev, Michel Ludwig, Dirk Walther, Frank Wolter", "title": "The Logical Difference for the Lightweight Description Logic EL", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  633-708, 2012", "doi": "10.1613/jair.3552", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a logic-based approach to versioning of ontologies. Under this view,\nontologies provide answers to queries about some vocabulary of interest. The\ndifference between two versions of an ontology is given by the set of queries\nthat receive different answers. We investigate this approach for terminologies\ngiven in the description logic EL extended with role inclusions and domain and\nrange restrictions for three distinct types of queries: subsumption, instance,\nand conjunctive queries. In all three cases, we present polynomial-time\nalgorithms that decide whether two terminologies give the same answers to\nqueries over a given vocabulary and compute a succinct representation of the\ndifference if it is non- empty. We present an implementation, CEX2, of the\ndeveloped algorithms for subsumption and instance queries and apply it to\ndistinct versions of Snomed CT and the NCI ontology.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:43:23 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Konev", "Boris", ""], ["Ludwig", "Michel", ""], ["Walther", "Dirk", ""], ["Wolter", "Frank", ""]]}, {"id": "1401.5852", "submitter": "Priyankar Ghosh", "authors": "Priyankar Ghosh, Amit Sharma, P.P. Chakrabarti, Pallab Dasgupta", "title": "Algorithms for Generating Ordered Solutions for Explicit AND/OR\n  Structures", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  275-333, 2012", "doi": "10.1613/jair.3576", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms for generating alternative solutions for explicit\nacyclic AND/OR structures in non-decreasing order of cost. The proposed\nalgorithms use a best first search technique and report the solutions using an\nimplicit representation ordered by cost. In this paper, we present two versions\nof the search algorithm -- (a) an initial version of the best first search\nalgorithm, ASG, which may present one solution more than once while generating\nthe ordered solutions, and (b) another version, LASG, which avoids the\nconstruction of the duplicate solutions. The actual solutions can be\nreconstructed quickly from the implicit compact representation used. We have\napplied the methods on a few test domains, some of them are synthetic while the\nothers are based on well known problems including the search space of the 5-peg\nTower of Hanoi problem, the matrix-chain multiplication problem and the problem\nof finding secondary structure of RNA. Experimental results show the efficacy\nof the proposed algorithms over the existing approach. Our proposed algorithms\nhave potential use in various domains ranging from knowledge based frameworks\nto service composition, where the AND/OR structure is widely used for\nrepresenting problems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:44:07 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Ghosh", "Priyankar", ""], ["Sharma", "Amit", ""], ["Chakrabarti", "P. P.", ""], ["Dasgupta", "Pallab", ""]]}, {"id": "1401.5853", "submitter": "Bernardo Cuenca Grau", "authors": "Bernardo Cuenca Grau, Boris Motik", "title": "Reasoning over Ontologies with Hidden Content: The Import-by-Query\n  Approach", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  197-255, 2012", "doi": "10.1613/jair.3579", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is currently a growing interest in techniques for hiding parts of the\nsignature of an ontology Kh that is being reused by another ontology Kv.\nTowards this goal, in this paper we propose the import-by-query framework,\nwhich makes the content of Kh accessible through a limited query interface. If\nKv reuses the symbols from Kh in a certain restricted way, one can reason over\nKv U Kh by accessing only Kv and the query interface. We map out the landscape\nof the import-by-query problem. In particular, we outline the limitations of\nour framework and prove that certain restrictions on the expressivity of Kh and\nthe way in which Kv reuses symbols from Kh are strictly necessary to enable\nreasoning in our setting. We also identify cases in which reasoning is possible\nand we present suitable import-by-query reasoning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:44:34 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Grau", "Bernardo Cuenca", ""], ["Motik", "Boris", ""]]}, {"id": "1401.5854", "submitter": "Carlos Hern\\'andez", "authors": "Carlos Hern\\'andez, Jorge A Baier", "title": "Avoiding and Escaping Depressions in Real-Time Heuristic Search", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  523-570, 2012", "doi": "10.1613/jair.3590", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heuristics used for solving hard real-time search problems have regions with\ndepressions. Such regions are bounded areas of the search space in which the\nheuristic function is inaccurate compared to the actual cost to reach a\nsolution. Early real-time search algorithms, like LRTA*, easily become trapped\nin those regions since the heuristic values of their states may need to be\nupdated multiple times, which results in costly solutions. State-of-the-art\nreal-time search algorithms, like LSS-LRTA* or LRTA*(k), improve LRTA*s\nmechanism to update the heuristic, resulting in improved performance. Those\nalgorithms, however, do not guide search towards avoiding depressed regions.\nThis paper presents depression avoidance, a simple real-time search principle\nto guide search towards avoiding states that have been marked as part of a\nheuristic depression. We propose two ways in which depression avoidance can be\nimplemented: mark-and-avoid and move-to-border. We implement these strategies\non top of LSS-LRTA* and RTAA*, producing 4 new real-time heuristic search\nalgorithms: aLSS-LRTA*, daLSS-LRTA*, aRTAA*, and daRTAA*. When the objective is\nto find a single solution by running the real-time search algorithm once, we\nshow that daLSS-LRTA* and daRTAA* outperform their predecessors sometimes by\none order of magnitude. Of the four new algorithms, daRTAA* produces the best\nsolutions given a fixed deadline on the average time allowed per planning\nepisode. We prove all our algorithms have good theoretical properties: in\nfinite search spaces, they find a solution if one exists, and converge to an\noptimal after a number of trials.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:45:02 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Hern\u00e1ndez", "Carlos", ""], ["Baier", "Jorge A", ""]]}, {"id": "1401.5855", "submitter": "Martin C. Cooper", "authors": "Martin C. Cooper, Stanislav \\v{Z}ivn\\'y", "title": "Tractable Triangles and Cross-Free Convexity in Discrete Optimisation", "comments": "arXiv admin note: text overlap with arXiv:1008.4035 by other authors", "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  455-490, 2012", "doi": "10.1613/jair.3598", "report-no": null, "categories": "cs.CC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimisation problem of a sum of unary and pairwise functions of discrete\nvariables is a general NP-hard problem with wide applications such as computing\nMAP configurations in Markov Random Fields (MRF), minimising Gibbs energy, or\nsolving binary Valued Constraint Satisfaction Problems (VCSPs).\n  We study the computational complexity of classes of discrete optimisation\nproblems given by allowing only certain types of costs in every triangle of\nvariable-value assignments to three distinct variables. We show that for\nseveral computational problems, the only non- trivial tractable classes are the\nwell known maximum matching problem and the recently discovered joint-winner\nproperty. Our results, apart from giving complete classifications in the\nstudied cases, provide guidance in the search for hybrid tractable classes;\nthat is, classes of problems that are not captured by restrictions on the\nfunctions (such as submodularity) or the structure of the problem graph (such\nas bounded treewidth).\n  Furthermore, we introduce a class of problems with convex cardinality\nfunctions on cross-free sets of assignments. We prove that while imposing only\none of the two conditions renders the problem NP-hard, the conjunction of the\ntwo gives rise to a novel tractable class satisfying the cross-free convexity\nproperty, which generalises the joint-winner property to problems of unbounded\narity.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:45:30 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Cooper", "Martin C.", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "1401.5856", "submitter": "Patrik Haslum", "authors": "Patrik Haslum", "title": "Narrative Planning: Compilations to Classical Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  383-395, 2012", "doi": "10.1613/jair.3602", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model of story generation recently proposed by Riedl and Young casts it as\nplanning, with the additional condition that story characters behave\nintentionally. This means that characters have perceivable motivation for the\nactions they take. I show that this condition can be compiled away (in more\nways than one) to produce a classical planning problem that can be solved by an\noff-the-shelf classical planner, more efficiently than by Riedl and Youngs\nspecialised planner.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:46:00 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Haslum", "Patrik", ""]]}, {"id": "1401.5857", "submitter": "Amanda J. Coles", "authors": "Amanda J. Coles, Andrew I. Coles, Maria Fox, Derek Long", "title": "COLIN: Planning with Continuous Linear Numeric Change", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  1-96, 2012", "doi": "10.1613/jair.3608", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe COLIN, a forward-chaining heuristic search planner,\ncapable of reasoning with COntinuous LINear numeric change, in addition to the\nfull temporal semantics of PDDL. Through this work we make two advances to the\nstate-of-the-art in terms of expressive reasoning capabilities of planners: the\nhandling of continuous linear change, and the handling of duration-dependent\neffects in combination with duration inequalities, both of which require\ntightly coupled temporal and numeric reasoning during planning. COLIN combines\nFF-style forward chaining search, with the use of a Linear Program (LP) to\ncheck the consistency of the interacting temporal and numeric constraints at\neach state. The LP is used to compute bounds on the values of variables in each\nstate, reducing the range of actions that need to be considered for\napplication. In addition, we develop an extension of the Temporal Relaxed\nPlanning Graph heuristic of CRIKEY3, to support reasoning directly with\ncontinuous change. We extend the range of task variables considered to be\nsuitable candidates for specifying the gradient of the continuous numeric\nchange effected by an action. Finally, we explore the potential for employing\nmixed integer programming as a tool for optimising the timestamps of the\nactions in the plan, once a solution has been found. To support this, we\nfurther contribute a selection of extended benchmark domains that include\ncontinuous numeric effects. We present results for COLIN that demonstrate its\nscalability on a range of benchmarks, and compare to existing state-of-the-art\nplanners.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:46:26 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Coles", "Amanda J.", ""], ["Coles", "Andrew I.", ""], ["Fox", "Maria", ""], ["Long", "Derek", ""]]}, {"id": "1401.5858", "submitter": "Joerg Hoffman", "authors": "Joerg Hoffman, Ingo Weber, Frank Michael Kraft", "title": "SAP Speaks PDDL: Exploiting a Software-Engineering Model for Planning in\n  Business Process Management", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  587-632, 2012", "doi": "10.1613/jair.3636", "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning is concerned with the automated solution of action sequencing\nproblems described in declarative languages giving the action preconditions and\neffects. One important application area for such technology is the creation of\nnew processes in Business Process Management (BPM), which is essential in an\never more dynamic business environment. A major obstacle for the application of\nPlanning in this area lies in the modeling. Obtaining a suitable model to plan\nwith -- ideally a description in PDDL, the most commonly used planning language\n-- is often prohibitively complicated and/or costly. Our core observation in\nthis work is that this problem can be ameliorated by leveraging synergies with\nmodel-based software development. Our application at SAP, one of the leading\nvendors of enterprise software, demonstrates that even one-to-one model re-use\nis possible.\n  The model in question is called Status and Action Management (SAM). It\ndescribes the behavior of Business Objects (BO), i.e., large-scale data\nstructures, at a level of abstraction corresponding to the language of business\nexperts. SAM covers more than 400 kinds of BOs, each of which is described in\nterms of a set of status variables and how their values are required for, and\naffected by, processing steps (actions) that are atomic from a business\nperspective. SAM was developed by SAP as part of a major model-based software\nengineering effort. We show herein that one can use this same model for\nplanning, thus obtaining a BPM planning application that incurs no modeling\noverhead at all.\n  We compile SAM into a variant of PDDL, and adapt an off-the-shelf planner to\nsolve this kind of problem. Thanks to the resulting technology, business\nexperts may create new processes simply by specifying the desired behavior in\nterms of status variable value changes: effectively, by describing the process\nin their own language.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:47:21 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Hoffman", "Joerg", ""], ["Weber", "Ingo", ""], ["Kraft", "Frank Michael", ""]]}, {"id": "1401.5859", "submitter": "Maria Fox", "authors": "Maria Fox, Derek Long, Daniele Magazzeni", "title": "Plan-based Policies for Efficient Multiple Battery Load Management", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  335-382, 2012", "doi": "10.1613/jair.3643", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient use of multiple batteries is a practical problem with wide and\ngrowing application. The problem can be cast as a planning problem under\nuncertainty. We describe the approach we have adopted to modelling and solving\nthis problem, seen as a Markov Decision Problem, building effective policies\nfor battery switching in the face of stochastic load profiles.\n  Our solution exploits and adapts several existing techniques: planning for\ndeterministic mixed discrete-continuous problems and Monte Carlo sampling for\npolicy learning. The paper describes the development of planning techniques to\nallow solution of the non-linear continuous dynamic models capturing the\nbattery behaviours. This approach depends on carefully handled discretisation\nof the temporal dimension. The construction of policies is performed using a\nclassification approach and this idea offers opportunities for wider\nexploitation in other problems. The approach and its generality are described\nin the paper.\n  Application of the approach leads to construction of policies that, in\nsimulation, significantly outperform those that are currently in use and the\nbest published solutions to the battery management problem. We achieve\nsolutions that achieve more than 99% efficiency in simulation compared with the\ntheoretical limit and do so with far fewer battery switches than existing\npolicies. Behaviour of physical batteries does not exactly match the simulated\nmodels for many reasons, so to confirm that our theoretical results can lead to\nreal measured improvements in performance we also conduct and report\nexperiments using a physical test system. These results demonstrate that we can\nobtain 5%-15% improvement in lifetimes in the case of a two battery system.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:47:47 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Fox", "Maria", ""], ["Long", "Derek", ""], ["Magazzeni", "Daniele", ""]]}, {"id": "1401.5860", "submitter": "Ignasi Ab\\'io", "authors": "Ignasi Ab\\'io, Robert Nieuwenhuis, Albert Oliveras, Enric\n  Rodriguez-Carbonell, Valentin Mayer-Eichberger", "title": "A New Look at BDDs for Pseudo-Boolean Constraints", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  443-480, 2012", "doi": "10.1613/jair.3653", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-Boolean constraints are omnipresent in practical applications, and\nthus a significant effort has been devoted to the development of good SAT\nencoding techniques for them. Some of these encodings first construct a Binary\nDecision Diagram (BDD) for the constraint, and then encode the BDD into a\npropositional formula. These BDD-based approaches have some important\nadvantages, such as not being dependent on the size of the coefficients, or\nbeing able to share the same BDD for representing many constraints.\n  We first focus on the size of the resulting BDDs, which was considered to be\nan open problem in our research community. We report on previous work where it\nwas proved that there are Pseudo-Boolean constraints for which no polynomial\nBDD exists. We also give an alternative and simpler proof assuming that NP is\ndifferent from Co-NP. More interestingly, here we also show how to overcome the\npossible exponential blowup of BDDs by phcoefficient decomposition. This allows\nus to give the first polynomial generalized arc-consistent ROBDD-based encoding\nfor Pseudo-Boolean constraints.\n  Finally, we focus on practical issues: we show how to efficiently construct\nsuch ROBDDs, how to encode them into SAT with only 2 clauses per node, and\npresent experimental results that confirm that our approach is competitive with\nother encodings and state-of-the-art Pseudo-Boolean solvers.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:48:33 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Ab\u00edo", "Ignasi", ""], ["Nieuwenhuis", "Robert", ""], ["Oliveras", "Albert", ""], ["Rodriguez-Carbonell", "Enric", ""], ["Mayer-Eichberger", "Valentin", ""]]}, {"id": "1401.5861", "submitter": "Carmel Domshlak", "authors": "Carmel Domshlak, Erez Karpas, Shaul Markovitch", "title": "Online Speedup Learning for Optimal Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  709-755, 2012", "doi": "10.1613/jair.3676", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-independent planning is one of the foundational areas in the field of\nArtificial Intelligence. A description of a planning task consists of an\ninitial world state, a goal, and a set of actions for modifying the world\nstate. The objective is to find a sequence of actions, that is, a plan, that\ntransforms the initial world state into a goal state. In optimal planning, we\nare interested in finding not just a plan, but one of the cheapest plans. A\nprominent approach to optimal planning these days is heuristic state-space\nsearch, guided by admissible heuristic functions. Numerous admissible\nheuristics have been developed, each with its own strengths and weaknesses, and\nit is well known that there is no single \"best heuristic for optimal planning\nin general. Thus, which heuristic to choose for a given planning task is a\ndifficult question. This difficulty can be avoided by combining several\nheuristics, but that requires computing numerous heuristic estimates at each\nstate, and the tradeoff between the time spent doing so and the time saved by\nthe combined advantages of the different heuristics might be high. We present a\nnovel method that reduces the cost of combining admissible heuristics for\noptimal planning, while maintaining its benefits. Using an idealized search\nspace model, we formulate a decision rule for choosing the best heuristic to\ncompute at each state. We then present an active online learning approach for\nlearning a classifier with that decision rule as the target concept, and employ\nthe learned classifier to decide which heuristic to compute at each state. We\nevaluate this technique empirically, and show that it substantially outperforms\nthe standard method for combining several heuristics via their pointwise\nmaximum.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:49:53 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Domshlak", "Carmel", ""], ["Karpas", "Erez", ""], ["Markovitch", "Shaul", ""]]}, {"id": "1401.5869", "submitter": "Zizhen Zhang", "authors": "Zizhen Zhang, Hu Qin, Xiaocong Liang, Andrew Lim", "title": "An Enhanced Branch-and-bound Algorithm for the Talent Scheduling Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The talent scheduling problem is a simplified version of the real-world film\nshooting problem, which aims to determine a shooting sequence so as to minimize\nthe total cost of the actors involved. In this article, we first formulate the\nproblem as an integer linear programming model. Next, we devise a\nbranch-and-bound algorithm to solve the problem. The branch-and-bound algorithm\nis enhanced by several accelerating techniques, including preprocessing,\ndominance rules and caching search states. Extensive experiments over two sets\nof benchmark instances suggest that our algorithm is superior to the current\nbest exact algorithm. Finally, the impacts of different parameter settings are\ndisclosed by some additional experiments.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 04:09:45 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Zhang", "Zizhen", ""], ["Qin", "Hu", ""], ["Liang", "Xiaocong", ""], ["Lim", "Andrew", ""]]}, {"id": "1401.5980", "submitter": "Dimitri Kartsaklis", "authors": "Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, Stephen Pulman, Bob Coecke", "title": "Reasoning about Meaning in Natural Language with Compact Closed\n  Categories and Frobenius Algebras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compact closed categories have found applications in modeling quantum\ninformation protocols by Abramsky-Coecke. They also provide semantics for\nLambek's pregroup algebras, applied to formalizing the grammatical structure of\nnatural language, and are implicit in a distributional model of word meaning\nbased on vector spaces. Specifically, in previous work Coecke-Clark-Sadrzadeh\nused the product category of pregroups with vector spaces and provided a\ndistributional model of meaning for sentences. We recast this theory in terms\nof strongly monoidal functors and advance it via Frobenius algebras over vector\nspaces. The former are used to formalize topological quantum field theories by\nAtiyah and Baez-Dolan, and the latter are used to model classical data in\nquantum protocols by Coecke-Pavlovic-Vicary. The Frobenius algebras enable us\nto work in a single space in which meanings of words, phrases, and sentences of\nany structure live. Hence we can compare meanings of different language\nconstructs and enhance the applicability of the theory. We report on\nexperimental results on a number of language tasks and verify the theoretical\npredictions.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 14:20:58 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Kartsaklis", "Dimitri", ""], ["Sadrzadeh", "Mehrnoosh", ""], ["Pulman", "Stephen", ""], ["Coecke", "Bob", ""]]}, {"id": "1401.6048", "submitter": "Ronen I. Brafman", "authors": "Ronen I. Brafman, Guy Shani", "title": "Replanning in Domains with Partial Information and Sensing Actions", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  565-600, 2012", "doi": "10.1613/jair.3711", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replanning via determinization is a recent, popular approach for online\nplanning in MDPs. In this paper we adapt this idea to classical, non-stochastic\ndomains with partial information and sensing actions, presenting a new planner:\nSDR (Sample, Determinize, Replan). At each step we generate a solution plan to\na classical planning problem induced by the original problem. We execute this\nplan as long as it is safe to do so. When this is no longer the case, we\nreplan. The classical planning problem we generate is based on the\ntranslation-based approach for conformant planning introduced by Palacios and\nGeffner. The state of the classical planning problem generated in this approach\ncaptures the belief state of the agent in the original problem. Unfortunately,\nwhen this method is applied to planning problems with sensing, it yields a\nnon-deterministic planning problem that is typically very large. Our main\ncontribution is the introduction of state sampling techniques for overcoming\nthese two problems. In addition, we introduce a novel, lazy, regression-based\nmethod for querying the agents belief state during run-time. We provide a\ncomprehensive experimental evaluation of the planner, showing that it scales\nbetter than the state-of-the-art CLG planner on existing benchmark problems,\nbut also highlighting its weaknesses with new domains. We also discuss its\ntheoretical guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 16:44:51 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Brafman", "Ronen I.", ""], ["Shani", "Guy", ""]]}, {"id": "1401.6049", "submitter": "Richard Hoshino", "authors": "Richard Hoshino, Ken-ichi Kawarabayashi", "title": "Generating Approximate Solutions to the TTP using a Linear Distance\n  Relaxation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  257-286, 2012", "doi": "10.1613/jair.3713", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some domestic professional sports leagues, the home stadiums are located\nin cities connected by a common train line running in one direction. For these\ninstances, we can incorporate this geographical information to determine\noptimal or nearly-optimal solutions to the n-team Traveling Tournament Problem\n(TTP), an NP-hard sports scheduling problem whose solution is a double\nround-robin tournament schedule that minimizes the sum total of distances\ntraveled by all n teams. We introduce the Linear Distance Traveling Tournament\nProblem (LD-TTP), and solve it for n=4 and n=6, generating the complete set of\npossible solutions through elementary combinatorial techniques. For larger n,\nwe propose a novel \"expander construction\" that generates an approximate\nsolution to the LD-TTP. For n congruent to 4 modulo 6, we show that our\nexpander construction produces a feasible double round-robin tournament\nschedule whose total distance is guaranteed to be no worse than 4/3 times the\noptimal solution, regardless of where the n teams are located. This\n4/3-approximation for the LD-TTP is stronger than the currently best-known\nratio of 5/3 + epsilon for the general TTP. We conclude the paper by applying\nthis linear distance relaxation to general (non-linear) n-team TTP instances,\nwhere we develop fast approximate solutions by simply \"assuming\" the n teams\nlie on a straight line and solving the modified problem. We show that this\ntechnique surprisingly generates the distance-optimal tournament on all\nbenchmark sets on 6 teams, as well as close-to-optimal schedules for larger n,\neven when the teams are located around a circle or positioned in\nthree-dimensional space.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 16:45:07 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Hoshino", "Richard", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1401.6098", "submitter": "Guohua Wu", "authors": "Guohua Wu, Huilin Wang, Haifeng Li, Witold Pedrycz, Dishan Qiu, Manhao\n  Ma, Jin Liu", "title": "An adaptive Simulated Annealing-based satellite observation scheduling\n  method combined with a dynamic task clustering strategy", "comments": "23 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient scheduling is of great significance to rationally make use of\nscarce satellite resources. Task clustering has been demonstrated to realize an\neffective strategy to improve the efficiency of satellite scheduling. However,\nthe previous task clustering strategy is static. That is, it is integrated into\nthe scheduling in a two-phase manner rather than in a dynamic fashion, without\nexpressing its full potential in improving the satellite scheduling\nperformance. In this study, we present an adaptive Simulated Annealing based\nscheduling algorithm aggregated with a dynamic task clustering strategy (or\nASA-DTC for short) for satellite observation scheduling problems (SOSPs).\nFirst, we develop a formal model for the scheduling of Earth observing\nsatellites. Second, we analyze the related constraints involved in the\nobservation task clustering process. Thirdly, we detail an implementation of\nthe dynamic task clustering strategy and the adaptive Simulated Annealing\nalgorithm. The adaptive Simulated Annealing algorithm is efficient, with the\nendowment of some sophisticated mechanisms, i.e. adaptive temperature control,\ntabu-list based revisiting avoidance mechanism, and intelligent combination of\nneighborhood structures. Finally, we report on experimental simulation studies\nto demonstrate the competitive performance of ASA-DTC. Moreover, we show that\nASA-DTC is especially effective when SOSPs contain a large number of targets or\nthese targets are densely distributed in a certain area.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 22:46:27 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Wu", "Guohua", ""], ["Wang", "Huilin", ""], ["Li", "Haifeng", ""], ["Pedrycz", "Witold", ""], ["Qiu", "Dishan", ""], ["Ma", "Manhao", ""], ["Liu", "Jin", ""]]}, {"id": "1401.6226", "submitter": "Tunji Adebiyi", "authors": "Adetunji Adebiyi, Chris Imafidon", "title": "Using Neural Network to Propose Solutions to Threats in Attack Patterns", "comments": "11 Pages", "journal-ref": "International Journal of Soft Computing and Software Engineering\n  [JSCSE], Vol. 3, No. 1, 2013", "doi": "10.7321/jscse.v3.n1.1", "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, a lot of effort has been put into securing software\napplication during development in the software industry. Software security is a\nresearch field in this area which looks at how security can be weaved into\nsoftware at each phase of software development lifecycle (SDLC). The use of\nattack patterns is one of the approaches that have been proposed for\nintegrating security during the design phase of SDLC. While this approach help\ndevelopers in identify security flaws in their software designs, the need to\napply the proper security capability that will mitigate the threat identified\nis very important. To assist in this area, the uses of security patterns have\nbeen proposed to help developers to identify solutions to recurring security\nproblems. However due to different types of security patterns and their\ntaxonomy, software developers are faced with the challenge of finding and\nselecting appropriate security patterns that addresses the security risks in\ntheir design. In this paper, we propose a tool based on Neural Network for\nproposing solutions in form of security patterns to threats in attack patterns\nmatching attacking patterns. From the result of performance of the neural\nnetwork, we found out that the neural network was able to match attack patterns\nto security patterns that can mitigate the threat in the attack pattern. With\nthis information developers are better informed in making decision on the\nsolution for securing their application.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 00:42:53 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Adebiyi", "Adetunji", ""], ["Imafidon", "Chris", ""]]}, {"id": "1401.6307", "submitter": "Florent Capelli", "authors": "Florent Capelli, Arnaud Durand, Stefan Mengel", "title": "Hypergraph Acyclicity and Propositional Model Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the propositional model counting problem #SAT for CNF- formulas\nwith hypergraphs that allow a disjoint branches decomposition can be solved in\npolynomial time. We show that this class of hypergraphs is incomparable to\nhypergraphs of bounded incidence cliquewidth which were the biggest class of\nhypergraphs for which #SAT was known to be solvable in polynomial time so far.\nFurthermore, we present a polynomial time algorithm that computes a disjoint\nbranches decomposition of a given hypergraph if it exists and rejects\notherwise. Finally, we show that some slight extensions of the class of\nhypergraphs with disjoint branches decompositions lead to intractable #SAT,\nleaving open how to generalize the counting result of this paper.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 10:33:23 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Capelli", "Florent", ""], ["Durand", "Arnaud", ""], ["Mengel", "Stefan", ""]]}, {"id": "1401.6679", "submitter": "Robert Jeansoulin", "authors": "Robert Jeansoulin and Nic Wilson", "title": "Quality of Geographic Information: Ontological approach and Artificial\n  Intelligence Tools", "comments": "12 pages, 8th EC-GIS Workshop (European Commission), Dublin, Ireland,\n  July, 3-5, 2002", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective is to present one important aspect of the European IST-FET\nproject \"REV!GIS\"1: the methodology which has been developed for the\ntranslation (interpretation) of the quality of the data into a \"fitness for\nuse\" information, that we can confront to the user needs in its application.\nThis methodology is based upon the notion of \"ontologies\" as a conceptual\nframework able to capture the explicit and implicit knowledge involved in the\napplication. We do not address the general problem of formalizing such\nontologies, instead, we rather try to illustrate this with three applications\nwhich are particular cases of the more general \"data fusion\" problem. In each\napplication, we show how to deploy our methodology, by comparing several\npossible solutions, and we try to enlighten where are the quality issues, and\nwhat kind of solution to privilege, even at the expense of a highly complex\ncomputational approach. The expectation of the REV!GIS project is that\ncomputationally tractable solutions will be available among the next generation\nAI tools.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2014 18:52:05 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Jeansoulin", "Robert", ""], ["Wilson", "Nic", ""]]}, {"id": "1401.6686", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh, Russell Greiner", "title": "Perturbed Message Passing for Constraint Satisfaction Problems", "comments": null, "journal-ref": "JMLR 16(Jul):1249-1274, 2015", "doi": null, "report-no": null, "categories": "cs.AI cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an efficient message passing scheme for solving Constraint\nSatisfaction Problems (CSPs), which uses stochastic perturbation of Belief\nPropagation (BP) and Survey Propagation (SP) messages to bypass decimation and\ndirectly produce a single satisfying assignment. Our first CSP solver, called\nPerturbed Blief Propagation, smoothly interpolates two well-known inference\nprocedures; it starts as BP and ends as a Gibbs sampler, which produces a\nsingle sample from the set of solutions. Moreover we apply a similar\nperturbation scheme to SP to produce another CSP solver, Perturbed Survey\nPropagation. Experimental results on random and real-world CSPs show that\nPerturbed BP is often more successful and at the same time tens to hundreds of\ntimes more efficient than standard BP guided decimation. Perturbed BP also\ncompares favorably with state-of-the-art SP-guided decimation, which has a\ncomputational complexity that generally scales exponentially worse than our\nmethod (wrt the cardinality of variable domains and constraints). Furthermore,\nour experiments with random satisfiability and coloring problems demonstrate\nthat Perturbed SP can outperform SP-guided decimation, making it the best\nincomplete random CSP-solver in difficult regimes.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2014 20:32:35 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 00:05:44 GMT"}, {"version": "v3", "created": "Tue, 3 Feb 2015 00:13:28 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Greiner", "Russell", ""]]}, {"id": "1401.7249", "submitter": "Atif Khan", "authors": "Atif Ali Khan, Oumair Naseer, Daciana Iliescu, Evor Hines", "title": "Fuzzy Controller Design for Assisted Omni-Directional Treadmill Therapy", "comments": "Presented at: \"The International Conference on Soft Computing and\n  Software Engineering (SCSE 2013)\" at San Francisco State University at\n  Downtown Campus, in San Francisco, California, USA, March 1-2, 2013", "journal-ref": "The International Journal of Soft Computing and Software\n  Engineering [JSCSE], Vol. 3, No. 3, pp. 30-37, 2013", "doi": "10.7321/jscse.v3.n3.8", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the defining characteristic of human being is their ability to walk\nupright. Loss or restriction of such ability whether due to the accident, spine\nproblem, stroke or other neurological injuries can cause tremendous stress on\nthe patients and hence will contribute negatively to their quality of life.\nModern research shows that physical exercise is very important for maintaining\nphysical fitness and adopting a healthier life style. In modern days treadmill\nis widely used for physical exercises and training which enables the user to\nset up an exercise regime that can be adhered to irrespective of the weather\nconditions. Among the users of treadmills today are medical facilities such as\nhospitals, rehabilitation centres, medical and physiotherapy clinics etc. The\nprocess of assisted training or doing rehabilitation exercise through treadmill\nis referred to as treadmill therapy. A modern treadmill is an automated machine\nhaving built in functions and predefined features. Most of the treadmills used\ntoday are one dimensional and user can only walk in one direction. This paper\npresents the idea of using omnidirectional treadmills which will be more\nappealing to the patients as they can walk in any direction, hence encouraging\nthem to do exercises more frequently. This paper proposes a fuzzy control\ndesign and possible implementation strategy to assist patients in treadmill\ntherapy. By intelligently controlling the safety belt attached to the treadmill\nuser, one can help them steering left, right or in any direction. The use of\nintelligent treadmill therapy can help patients to improve their walking\nability without being continuously supervised by the specialists. The patients\ncan walk freely within a limited space and the support system will provide\ncontinuous evaluation of their position and can adjust the control parameters\nof treadmill accordingly to provide best possible assistance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 14:25:53 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["Khan", "Atif Ali", ""], ["Naseer", "Oumair", ""], ["Iliescu", "Daciana", ""], ["Hines", "Evor", ""]]}, {"id": "1401.7463", "submitter": "Pierre Flener", "authors": "Pierre Flener and Justin Pearson", "title": "Propagators and Violation Functions for Geometric and Workload\n  Constraints Arising in Airspace Sectorisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Airspace sectorisation provides a partition of a given airspace into sectors,\nsubject to geometric constraints and workload constraints, so that some cost\nmetric is minimised. We make a study of the constraints that arise in airspace\nsectorisation. For each constraint, we give an analysis of what algorithms and\nproperties are required under systematic search and stochastic local search.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 10:36:39 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Flener", "Pierre", ""], ["Pearson", "Justin", ""]]}, {"id": "1401.7941", "submitter": "Stefano Albrecht", "authors": "Stefano V. Albrecht, Subramanian Ramamoorthy", "title": "Exploiting Causality for Selective Belief Filtering in Dynamic Bayesian\n  Networks", "comments": "44 pages; final manuscript published in Journal of Artificial\n  Intelligence Research (JAIR)", "journal-ref": null, "doi": "10.1613/jair.5044", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Bayesian networks (DBNs) are a general model for stochastic processes\nwith partially observed states. Belief filtering in DBNs is the task of\ninferring the belief state (i.e. the probability distribution over process\nstates) based on incomplete and noisy observations. This can be a hard problem\nin complex processes with large state spaces. In this article, we explore the\nidea of accelerating the filtering task by automatically exploiting causality\nin the process. We consider a specific type of causal relation, called\npassivity, which pertains to how state variables cause changes in other\nvariables. We present the Passivity-based Selective Belief Filtering (PSBF)\nmethod, which maintains a factored belief representation and exploits passivity\nto perform selective updates over the belief factors. PSBF produces exact\nbelief states under certain assumptions and approximate belief states\notherwise, where the approximation error is bounded by the degree of\nuncertainty in the process. We show empirically, in synthetic processes with\nvarying sizes and degrees of passivity, that PSBF is faster than several\nalternative methods while achieving competitive accuracy. Furthermore, we\ndemonstrate how passivity occurs naturally in a complex system such as a\nmulti-robot warehouse, and how PSBF can exploit this to accelerate the\nfiltering task.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2014 18:05:48 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 14:54:34 GMT"}, {"version": "v3", "created": "Mon, 25 Apr 2016 17:51:09 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Albrecht", "Stefano V.", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1401.8175", "submitter": "Toshio Suzuki", "authors": "Toshio Suzuki and Yoshinao Niida", "title": "Equilibrium Points of an AND-OR Tree: under Constraints on Probability", "comments": "13 pages, 3 figures", "journal-ref": "ANN PURE APPL LOGIC 166, pp. 1150--1164 (2015)", "doi": "10.1016/j.apal.2015.07.002", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a probability distribution d on the truth assignments to a uniform\nbinary AND-OR tree. Liu and Tanaka [2007, Inform. Process. Lett.] showed the\nfollowing: If d achieves the equilibrium among independent distributions (ID)\nthen d is an independent identical distribution (IID). We show a stronger form\nof the above result. Given a real number r such that 0 < r < 1, we consider a\nconstraint that the probability of the root node having the value 0 is r. Our\nmain result is the following: When we restrict ourselves to IDs satisfying this\nconstraint, the above result of Liu and Tanaka still holds. The proof employs\nclever tricks of induction. In particular, we show two fundamental\nrelationships between expected cost and probability in an IID on an OR-AND\ntree: (1) The ratio of the cost to the probability (of the root having the\nvalue 0) is a decreasing function of the probability x of the leaf. (2) The\nratio of derivative of the cost to the derivative of the probability is a\ndecreasing function of x, too.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 14:22:05 GMT"}, {"version": "v2", "created": "Thu, 10 Jul 2014 09:43:16 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2015 11:56:18 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Suzuki", "Toshio", ""], ["Niida", "Yoshinao", ""]]}, {"id": "1401.8269", "submitter": "Peter Turney", "authors": "Peter D. Turney and Saif M. Mohammad", "title": "Experiments with Three Approaches to Recognizing Lexical Entailment", "comments": "to appear in Natural Language Engineering", "journal-ref": "Natural Language Engineering, 21 (3), (2015), 437-476", "doi": "10.1017/S1351324913000387", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in natural language often involves recognizing lexical entailment\n(RLE); that is, identifying whether one word entails another. For example,\n\"buy\" entails \"own\". Two general strategies for RLE have been proposed: One\nstrategy is to manually construct an asymmetric similarity measure for context\nvectors (directional similarity) and another is to treat RLE as a problem of\nlearning to recognize semantic relations using supervised machine learning\ntechniques (relation classification). In this paper, we experiment with two\nrecent state-of-the-art representatives of the two general strategies. The\nfirst approach is an asymmetric similarity measure (an instance of the\ndirectional similarity strategy), designed to capture the degree to which the\ncontexts of a word, a, form a subset of the contexts of another word, b. The\nsecond approach (an instance of the relation classification strategy)\nrepresents a word pair, a:b, with a feature vector that is the concatenation of\nthe context vectors of a and b, and then applies supervised learning to a\ntraining set of labeled feature vectors. Additionally, we introduce a third\napproach that is a new instance of the relation classification strategy. The\nthird approach represents a word pair, a:b, with a feature vector in which the\nfeatures are the differences in the similarities of a and b to a set of\nreference words. All three approaches use vector space models (VSMs) of\nsemantics, based on word-context matrices. We perform an extensive evaluation\nof the three approaches using three different datasets. The proposed new\napproach (similarity differences) performs significantly better than the other\ntwo approaches on some datasets and there is no dataset for which it is\nsignificantly worse. Our results suggest it is beneficial to make connections\nbetween the research in lexical entailment and the research in semantic\nrelation classification.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 19:42:19 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Turney", "Peter D.", ""], ["Mohammad", "Saif M.", ""]]}]