[{"id": "1405.0034", "submitter": "Aaron Hunter", "authors": "Aaron Hunter", "title": "Belief Revision and Trust", "comments": "Appears in the Proceedings of the 15th International Workshop on\n  Non-Monotonic Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief revision is the process in which an agent incorporates a new piece of\ninformation together with a pre-existing set of beliefs. When the new\ninformation comes in the form of a report from another agent, then it is clear\nthat we must first determine whether or not that agent should be trusted. In\nthis paper, we provide a formal approach to modeling trust as a pre-processing\nstep before belief revision. We emphasize that trust is not simply a relation\nbetween agents; the trust that one agent has in another is often restricted to\na particular domain of expertise. We demonstrate that this form of trust can be\ncaptured by associating a state-partition with each agent, then relativizing\nall reports to this state partition before performing belief revision. In this\nmanner, we incorporate only the part of a report that falls under the perceived\ndomain of expertise of the reporting agent. Unfortunately, state partitions\nbased on expertise do not allow us to compare the relative strength of trust\nheld with respect to different agents. To address this problem, we introduce\npseudometrics over states to represent differing degrees of trust. This allows\nus to incorporate simultaneous reports from multiple agents in a way that\nensures the most trusted reports will be believed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 21:08:55 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Hunter", "Aaron", ""]]}, {"id": "1405.0054", "submitter": "Marco Montali", "authors": "Giuseppe De Giacomo and Riccardo De Masellis and Marco Grasso and\n  Fabrizio Maggi and Marco Montali", "title": "LTLf and LDLf Monitoring: A Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime monitoring is one of the central tasks to provide operational\ndecision support to running business processes, and check on-the-fly whether\nthey comply with constraints and rules. We study runtime monitoring of\nproperties expressed in LTL on finite traces (LTLf) and in its extension LDLf.\nLDLf is a powerful logic that captures all monadic second order logic on finite\ntraces, which is obtained by combining regular expressions and LTLf, adopting\nthe syntax of propositional dynamic logic (PDL). Interestingly, in spite of its\ngreater expressivity, LDLf has exactly the same computational complexity of\nLTLf. We show that LDLf is able to capture, in the logic itself, not only the\nconstraints to be monitored, but also the de-facto standard RV-LTL monitors.\nThis makes it possible to declaratively capture monitoring metaconstraints, and\ncheck them by relying on usual logical services instead of ad-hoc algorithms.\nThis, in turn, enables to flexibly monitor constraints depending on the\nmonitoring state of other constraints, e.g., \"compensation\" constraints that\nare only checked when others are detected to be violated. In addition, we\ndevise a direct translation of LDLf formulas into nondeterministic automata,\navoiding to detour to Buechi automata or alternating automata, and we use it to\nimplement a monitoring plug-in for the PROM suite.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 23:16:16 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["De Giacomo", "Giuseppe", ""], ["De Masellis", "Riccardo", ""], ["Grasso", "Marco", ""], ["Maggi", "Fabrizio", ""], ["Montali", "Marco", ""]]}, {"id": "1405.0406", "submitter": "Sylwia Polberg", "authors": "Sylwia Polberg", "title": "Extension-based Semantics of Abstract Dialectical Frameworks", "comments": "To appear in the Proceedings of the 15th International Workshop on\n  Non-Monotonic Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most prominent tools for abstract argumentation is the Dung's\nframework, AF for short. It is accompanied by a variety of semantics including\ngrounded, complete, preferred and stable. Although powerful, AFs have their\nshortcomings, which led to development of numerous enrichments. Among the most\ngeneral ones are the abstract dialectical frameworks, also known as the ADFs.\nThey make use of the so-called acceptance conditions to represent arbitrary\nrelations. This level of abstraction brings not only new challenges, but also\nrequires addressing existing problems in the field. One of the most\ncontroversial issues, recognized not only in argumentation, concerns the\nsupport cycles. In this paper we introduce a new method to ensure acyclicity of\nthe chosen arguments and present a family of extension-based semantics built on\nit. We also continue our research on the semantics that permit cycles and fill\nin the gaps from the previous works. Moreover, we provide ADF versions of the\nproperties known from the Dung setting. Finally, we also introduce a\nclassification of the developed sub-semantics and relate them to the existing\nlabeling-based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 14:04:50 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Polberg", "Sylwia", ""]]}, {"id": "1405.0423", "submitter": "Sachin Lakra", "authors": "Sachin Lakra, T.V. Prasad and G. Ramakrishna", "title": "Representation of a Sentence using a Polar Fuzzy Neutrosophic Semantic\n  Net", "comments": "arXiv admin note: text overlap with arXiv:math/0101228,\n  arXiv:math/0412424, arXiv:math/0306384 by other authors", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, Special Issue on Natural Language Processing, Volume 4, Issue\n  1, April 2014, pp. 1-8", "doi": "10.14569/SpecialIssue.2014.040101", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semantic net can be used to represent a sentence. A sentence in a language\ncontains semantics which are polar in nature, that is, semantics which are\npositive, neutral and negative. Neutrosophy is a relatively new field of\nscience which can be used to mathematically represent triads of concepts. These\ntriads include truth, indeterminacy and falsehood, and so also positivity,\nneutrality and negativity. Thus a conventional semantic net has been extended\nin this paper using neutrosophy into a Polar Fuzzy Neutrosophic Semantic Net. A\nPolar Fuzzy Neutrosophic Semantic Net has been implemented in MATLAB and has\nbeen used to illustrate a polar sentence in English language. The paper\ndemonstrates a method for the representation of polarity in a computers memory.\nThus, polar concepts can be applied to imbibe a machine such as a robot, with\nemotions, making machine emotion representation possible.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 15:05:55 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Lakra", "Sachin", ""], ["Prasad", "T. V.", ""], ["Ramakrishna", "G.", ""]]}, {"id": "1405.0446", "submitter": "Hai-Jun Zhou", "authors": "Shao-Meng Qin and Hai-Jun Zhou", "title": "Solving the undirected feedback vertex set problem by local search", "comments": "6 pages", "journal-ref": null, "doi": "10.1140/epjb/e2014-50289-7", "report-no": null, "categories": "cs.AI cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An undirected graph consists of a set of vertices and a set of undirected\nedges between vertices. Such a graph may contain an abundant number of cycles,\nthen a feedback vertex set (FVS) is a set of vertices intersecting with each of\nthese cycles. Constructing a FVS of cardinality approaching the global minimum\nvalue is a optimization problem in the nondeterministic polynomial-complete\ncomplexity class, therefore it might be extremely difficult for some large\ngraph instances. In this paper we develop a simulated annealing local search\nalgorithm for the undirected FVS problem. By defining an order for the vertices\noutside the FVS, we replace the global cycle constraints by a set of local\nvertex constraints on this order. Under these local constraints the cardinality\nof the focal FVS is then gradually reduced by the simulated annealing dynamical\nprocess. We test this heuristic algorithm on large instances of Er\\\"odos-Renyi\nrandom graph and regular random graph, and find that this algorithm is\ncomparable in performance to the belief propagation-guided decimation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 09:18:25 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Qin", "Shao-Meng", ""], ["Zhou", "Hai-Jun", ""]]}, {"id": "1405.0501", "submitter": "Mathias Niepert", "authors": "Mathias Niepert and Pedro Domingos", "title": "Exchangeable Variable Models", "comments": "ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sequence of random variables is exchangeable if its joint distribution is\ninvariant under variable permutations. We introduce exchangeable variable\nmodels (EVMs) as a novel class of probabilistic models whose basic building\nblocks are partially exchangeable sequences, a generalization of exchangeable\nsequences. We prove that a family of tractable EVMs is optimal under zero-one\nloss for a large class of functions, including parity and threshold functions,\nand strictly subsumes existing tractable independence-based model families.\nExtensive experiments show that EVMs outperform state of the art classifiers\nsuch as SVMs and probabilistic models which are solely based on independence\nassumptions.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 20:13:06 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Niepert", "Mathias", ""], ["Domingos", "Pedro", ""]]}, {"id": "1405.0546", "submitter": "Antti Puurula", "authors": "Antti Puurula, Jesse Read, Albert Bifet", "title": "Kaggle LSHTC4 Winning Solution", "comments": "Kaggle LSHTC winning solution description", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our winning submission to the 2014 Kaggle competition for Large Scale\nHierarchical Text Classification (LSHTC) consists mostly of an ensemble of\nsparse generative models extending Multinomial Naive Bayes. The\nbase-classifiers consist of hierarchically smoothed models combining document,\nlabel, and hierarchy level Multinomials, with feature pre-processing using\nvariants of TF-IDF and BM25. Additional diversification is introduced by\ndifferent types of folds and random search optimization for different measures.\nThe ensemble algorithm optimizes macroFscore by predicting the documents for\neach label, instead of the usual prediction of labels per document. Scores for\ndocuments are predicted by weighted voting of base-classifier outputs with a\nvariant of Feature-Weighted Linear Stacking. The number of documents per label\nis chosen using label priors and thresholding of vote scores. This document\ndescribes the models and software used to build our solution. Reproducing the\nresults for our solution can be done by running the scripts included in the\nKaggle package. A package omitting precomputed result files is also\ndistributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0\nfor Weka and Meka dependencies.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 01:41:27 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 04:57:19 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Puurula", "Antti", ""], ["Read", "Jesse", ""], ["Bifet", "Albert", ""]]}, {"id": "1405.0647", "submitter": "Djamal Ziani", "authors": "Djamal Ziani", "title": "Feature Selection On Boolean Symbolic Objects", "comments": "20 pages, 10 figures", "journal-ref": "Ziani D., Feature Selection on Boolean Symbolic Objects, in\n  International Journal of Computational Sciences and Information Technology\n  (IJCSITY), November 2013, volume 1, Number 4, 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the boom in IT technology, the data sets used in application are more\nand more larger and are described by a huge number of attributes, therefore,\nthe feature selection become an important discipline in Knowledge discovery and\ndata mining, allowing the experts to select the most relevant features to\nimprove the quality of their studies and to reduce the time processing of their\nalgorithm. In addition to that, the data used by the applications become\nricher. They are now represented by a set of complex and structured objects,\ninstead of simple numerical matrixes. The purpose of our algorithm is to do\nfeature selection on rich data, called Boolean Symbolic Objects (BSOs). These\nobjects are described by multivalued features. The BSOs are considered as\nhigher level units which can model complex data, such as cluster of\nindividuals, aggregated data or taxonomies. In this paper we will introduce a\nnew feature selection criterion for BSOs, and we will explain how we improved\nits complexity.\n", "versions": [{"version": "v1", "created": "Sun, 4 May 2014 05:02:53 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Ziani", "Djamal", ""]]}, {"id": "1405.0720", "submitter": "Matthias Nickles", "authors": "Matthias Nickles, Alessandra Mileo", "title": "Probabilistic Inductive Logic Programming Based on Answer Set\n  Programming", "comments": "Appears in the Proceedings of the 15th International Workshop on\n  Non-Monotonic Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new formal language for the expressive representation of\nprobabilistic knowledge based on Answer Set Programming (ASP). It allows for\nthe annotation of first-order formulas as well as ASP rules and facts with\nprobabilities and for learning of such weights from data (parameter\nestimation). Weighted formulas are given a semantics in terms of soft and hard\nconstraints which determine a probability distribution over answer sets. In\ncontrast to related approaches, we approach inference by optionally utilizing\nso-called streamlining XOR constraints, in order to reduce the number of\ncomputed answer sets. Our approach is prototypically implemented. Examples\nillustrate the introduced concepts and point at issues and topics for future\nresearch.\n", "versions": [{"version": "v1", "created": "Sun, 4 May 2014 17:18:49 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Nickles", "Matthias", ""], ["Mileo", "Alessandra", ""]]}, {"id": "1405.0795", "submitter": "Valmi Dufour-Lussier", "authors": "Valmi Dufour-Lussier (INRIA Nancy - Grand Est / LORIA), Alice Hermann\n  (INRIA Nancy - Grand Est / LORIA), Florence Le Ber (ICube), Jean Lieber\n  (INRIA Nancy - Grand Est / LORIA)", "title": "Belief revision in the propositional closure of a qualitative algebra\n  (extended version)", "comments": "This is the extended version of an article originally presented at\n  the 14th International Conference on Principles of Knowledge Representation\n  and Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief revision is an operation that aims at modifying old beliefs so that\nthey become consistent with new ones. The issue of belief revision has been\nstudied in various formalisms, in particular, in qualitative algebras (QAs) in\nwhich the result is a disjunction of belief bases that is not necessarily\nrepresentable in a QA. This motivates the study of belief revision in\nformalisms extending QAs, namely, their propositional closures: in such a\nclosure, the result of belief revision belongs to the formalism. Moreover, this\nmakes it possible to define a contraction operator thanks to the Harper\nidentity. Belief revision in the propositional closure of QAs is studied, an\nalgorithm for a family of revision operators is designed, and an open-source\nimplementation is made freely available on the web. (This is the extended\nversion of an article originally presented at the 14th International Conference\non Principles of Knowledge Representation and Reasoning.)\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 07:04:01 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Dufour-Lussier", "Valmi", "", "INRIA Nancy - Grand Est / LORIA"], ["Hermann", "Alice", "", "INRIA Nancy - Grand Est / LORIA"], ["Ber", "Florence Le", "", "ICube"], ["Lieber", "Jean", "", "INRIA Nancy - Grand Est / LORIA"]]}, {"id": "1405.0805", "submitter": "Hannes Strass", "authors": "Hannes Strass", "title": "On the Relative Expressiveness of Argumentation Frameworks, Normal Logic\n  Programs and Abstract Dialectical Frameworks", "comments": "Proceedings of the 15th International Workshop on Non-Monotonic\n  Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the expressiveness of the two-valued semantics of abstract\nargumentation frameworks, normal logic programs and abstract dialectical\nframeworks. By expressiveness we mean the ability to encode a desired set of\ntwo-valued interpretations over a given propositional signature using only\natoms from that signature. While the computational complexity of the two-valued\nmodel existence problem for all these languages is (almost) the same, we show\nthat the languages form a neat hierarchy with respect to their expressiveness.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 07:39:50 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Strass", "Hannes", ""]]}, {"id": "1405.0809", "submitter": "Hannes Strass", "authors": "Jianmin Ji and Hannes Strass", "title": "Implementing Default and Autoepistemic Logics via the Logic of GK", "comments": "Proceedings of the 15th International Workshop on Non-Monotonic\n  Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logic of knowledge and justified assumptions, also known as logic of\ngrounded knowledge (GK), was proposed by Lin and Shoham as a general logic for\nnonmonotonic reasoning. To date, it has been used to embed in it default logic\n(propositional case), autoepistemic logic, Turner's logic of universal\ncausation, and general logic programming under stable model semantics. Besides\nshowing the generality of GK as a logic for nonmonotonic reasoning, these\nembeddings shed light on the relationships among these other logics. In this\npaper, for the first time, we show how the logic of GK can be embedded into\ndisjunctive logic programming in a polynomial but non-modular translation with\nnew variables. The result can then be used to compute the extension/expansion\nsemantics of default logic, autoepistemic logic and Turner's logic of universal\ncausation by disjunctive ASP solvers such as claspD(-2), DLV, GNT and cmodels.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 07:45:30 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Ji", "Jianmin", ""], ["Strass", "Hannes", ""]]}, {"id": "1405.0868", "submitter": "Zhana Bao", "authors": "Zhana Bao", "title": "Finding Inner Outliers in High Dimensional Space", "comments": "9 pages, 9 Figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Outlier detection in a large-scale database is a significant and complex\nissue in knowledge discovering field. As the data distributions are obscure and\nuncertain in high dimensional space, most existing solutions try to solve the\nissue taking into account the two intuitive points: first, outliers are\nextremely far away from other points in high dimensional space; second,\noutliers are detected obviously different in projected-dimensional subspaces.\nHowever, for a complicated case that outliers are hidden inside the normal\npoints in all dimensions, existing detection methods fail to find such inner\noutliers. In this paper, we propose a method with twice dimension-projections,\nwhich integrates primary subspace outlier detection and secondary\npoint-projection between subspaces, and sums up the multiple weight values for\neach point. The points are computed with local density ratio separately in\ntwice-projected dimensions. After the process, outliers are those points\nscoring the largest values of weight. The proposed method succeeds to find all\ninner outliers on the synthetic test datasets with the dimension varying from\n100 to 10000. The experimental results also show that the proposed algorithm\ncan work in low dimensional space and can achieve perfect performance in high\ndimensional space. As for this reason, our proposed approach has considerable\npotential to apply it in multimedia applications helping to process images or\nvideo with large-scale attributes.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 12:01:14 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Bao", "Zhana", ""]]}, {"id": "1405.0869", "submitter": "Zhana Bao", "authors": "Zhana Bao", "title": "Robust Subspace Outlier Detection in High Dimensional Space", "comments": "10 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Rare data in a large-scale database are called outliers that reveal\nsignificant information in the real world. The subspace-based outlier detection\nis regarded as a feasible approach in very high dimensional space. However, the\noutliers found in subspaces are only part of the true outliers in high\ndimensional space, indeed. The outliers hidden in normal-clustered points are\nsometimes neglected in the projected dimensional subspace. In this paper, we\npropose a robust subspace method for detecting such inner outliers in a given\ndataset, which uses two dimensional-projections: detecting outliers in\nsubspaces with local density ratio in the first projected dimensions; finding\noutliers by comparing neighbor's positions in the second projected dimensions.\nEach point's weight is calculated by summing up all related values got in the\ntwo steps projected dimensions, and then the points scoring the largest weight\nvalues are taken as outliers. By taking a series of experiments with the number\nof dimensions from 10 to 10000, the results show that our proposed method\nachieves high precision in the case of extremely high dimensional space, and\nworks well in low dimensional space.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 12:01:24 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Bao", "Zhana", ""]]}, {"id": "1405.0876", "submitter": "Luca Pulina", "authors": "Marco Maratea, Luca Pulina, Francesco Ricca", "title": "The Multi-engine ASP Solver ME-ASP: Progress Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MEASP is a multi-engine solver for ground ASP programs. It exploits algorithm\nselection techniques based on classification to select one among a set of\nout-of-the-box heterogeneous ASP solvers used as black-box engines. In this\npaper we report on (i) a new optimized implementation of MEASP; and (ii) an\nattempt of applying algorithm selection to non-ground programs. An experimental\nanalysis reported in the paper shows that (i) the new implementation of \\measp\nis substantially faster than the previous version; and (ii) the multi-engine\nrecipe can be applied to the evaluation of non-ground programs with some\nbenefits.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 12:45:07 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Maratea", "Marco", ""], ["Pulina", "Luca", ""], ["Ricca", "Francesco", ""]]}, {"id": "1405.0915", "submitter": "Riccardo Zese", "authors": "Riccardo Zese", "title": "Reasoning with Probabilistic Logics", "comments": "An extended abstract / full version of a paper accepted to be\n  presented at the Doctoral Consortium of the 30th International Conference on\n  Logic Programming (ICLP 2014), July 19-22, Vienna, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest in the combination of probability with logics for modeling the\nworld has rapidly increased in the last few years. One of the most effective\napproaches is the Distribution Semantics which was adopted by many logic\nprogramming languages and in Descripion Logics. In this paper, we illustrate\nthe work we have done in this research field by presenting a probabilistic\nsemantics for description logics and reasoning and learning algorithms. In\nparticular, we present in detail the system TRILL P, which computes the\nprobability of queries w.r.t. probabilistic knowledge bases, which has been\nimplemented in Prolog. Note: An extended abstract / full version of a paper\naccepted to be presented at the Doctoral Consortium of the 30th International\nConference on Logic Programming (ICLP 2014), July 19-22, Vienna, Austria\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 14:57:08 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 13:18:59 GMT"}, {"version": "v3", "created": "Thu, 29 Jan 2015 09:26:41 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Zese", "Riccardo", ""]]}, {"id": "1405.0921", "submitter": "Chandranath  Adak", "authors": "Chandranath Adak", "title": "Gabor Filter and Rough Clustering Based Edge Detection", "comments": "Proc. IEEE Conf. #30853, International Conference on Human Computer\n  Interactions (ICHCI'13), Chennai, India, 23-24 Aug., 2013", "journal-ref": null, "doi": "10.1109/ICHCI-IEEE.2013.6887768", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an efficient edge detection method based on Gabor\nfilter and rough clustering. The input image is smoothed by Gabor function, and\nthe concept of rough clustering is used to focus on edge detection with soft\ncomputational approach. Hysteresis thresholding is used to get the actual\noutput, i.e. edges of the input image. To show the effectiveness, the proposed\ntechnique is compared with some other edge detection methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 06:30:19 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Adak", "Chandranath", ""]]}, {"id": "1405.0936", "submitter": "Mohit Jha", "authors": "Mohit Jha, Shailja Shukla", "title": "Design Of Fuzzy Logic Traffic Controller For Isolated Intersections With\n  Emergency Vehicle Priority System Using MATLAB Simulation", "comments": "7 Pages,7 Figure,CSIR Sponsored X Control Instrumentation System\n  Conference 2013; ISBN 978-93-82338-93-2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic is the chief puzzle problem which every country faces because of the\nenhancement in number of vehicles throughout the world, especially in large\nurban towns. Hence the need arises for simulating and optimizing traffic\ncontrol algorithms to better accommodate this increasing demand. Fuzzy\noptimization deals with finding the values of input parameters of a complex\nsimulated system which result in desired output. This paper presents a MATLAB\nsimulation of fuzzy logic traffic controller for controlling flow of traffic in\nisolated intersections. This controller is based on the waiting time and queue\nlength of vehicles at present green phase and vehicles queue lengths at the\nother phases. The controller controls the traffic light timings and phase\ndifference to ascertain sebaceous flow of traffic with least waiting time and\nqueue length. In this paper, the isolated intersection model used consists of\ntwo alleyways in each approach. Every outlook has different value of queue\nlength and waiting time, systematically, at the intersection. The maximum value\nof waiting time and vehicle queue length has to be selected by using proximity\nsensors as inputs to controller for the ameliorate control traffic flow at the\nintersection. An intelligent traffic model and fuzzy logic traffic controller\nare developed to evaluate the performance of traffic controller under different\npre-defined conditions for oleaginous flow of traffic. Additionally, this fuzzy\nlogic traffic controller has emergency vehicle siren sensors which detect\nemergency vehicle movement like ambulance, fire brigade, Police Van etc. and\ngives maximum priority to him and pass preferred signal to it.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 15:47:31 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Jha", "Mohit", ""], ["Shukla", "Shailja", ""]]}, {"id": "1405.0941", "submitter": "Serena Villata", "authors": "Elena Cabrio and Serena Villata", "title": "Towards a Benchmark of Natural Language Arguments", "comments": null, "journal-ref": "Proceedings of the 15th International Workshop on Non-Monotonic\n  Reasoning (NMR 2014)", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connections among natural language processing and argumentation theory\nare becoming stronger in the latest years, with a growing amount of works going\nin this direction, in different scenarios and applying heterogeneous\ntechniques. In this paper, we present two datasets we built to cope with the\ncombination of the Textual Entailment framework and bipolar abstract\nargumentation. In our approach, such datasets are used to automatically\nidentify through a Textual Entailment system the relations among the arguments\n(i.e., attack, support), and then the resulting bipolar argumentation graphs\nare analyzed to compute the accepted arguments.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 16:03:04 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Cabrio", "Elena", ""], ["Villata", "Serena", ""]]}, {"id": "1405.0961", "submitter": "Ulrich Furbach", "authors": "Ulrike Barthelmess and Ulrich Furbach", "title": "Do we need Asimov's Laws?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this essay the stance on robots is discussed. The attitude against robots\nin history, starting in Ancient Greek culture until the industrial revolution\nis described. The uncanny valley and some possible explanations are given. Some\ndifferences in Western and Asian understanding of robots are listed and finally\nwe answer the question raised with the title.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 08:30:49 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Barthelmess", "Ulrike", ""], ["Furbach", "Ulrich", ""]]}, {"id": "1405.0999", "submitter": "Shiqi Zhang", "authors": "Shiqi Zhang, Mohan Sridharan, Michael Gelfond, Jeremy Wyatt", "title": "KR$^3$: An Architecture for Knowledge Representation and Reasoning in\n  Robotics", "comments": "The paper appears in the Proceedings of the 15th International\n  Workshop on Non-Monotonic Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an architecture that combines the complementary\nstrengths of declarative programming and probabilistic graphical models to\nenable robots to represent, reason with, and learn from, qualitative and\nquantitative descriptions of uncertainty and knowledge. An action language is\nused for the low-level (LL) and high-level (HL) system descriptions in the\narchitecture, and the definition of recorded histories in the HL is expanded to\nallow prioritized defaults. For any given goal, tentative plans created in the\nHL using default knowledge and commonsense reasoning are implemented in the LL\nusing probabilistic algorithms, with the corresponding observations used to\nupdate the HL history. Tight coupling between the two levels enables automatic\nselection of relevant variables and generation of suitable action policies in\nthe LL for each HL action, and supports reasoning with violation of defaults,\nnoisy observations and unreliable actions in large and complex domains. The\narchitecture is evaluated in simulation and on physical robots transporting\nobjects in indoor domains; the benefit on robots is a reduction in task\nexecution time of 39% compared with a purely probabilistic, but still\nhierarchical, approach.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 19:13:06 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Zhang", "Shiqi", ""], ["Sridharan", "Mohan", ""], ["Gelfond", "Michael", ""], ["Wyatt", "Jeremy", ""]]}, {"id": "1405.1027", "submitter": "Zhana Bao", "authors": "Zhana Bao", "title": "K-NS: Section-Based Outlier Detection in High Dimensional Space", "comments": "10 pages, 6 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:1405.0869", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Finding rare information hidden in a huge amount of data from the Internet is\na necessary but complex issue. Many researchers have studied this issue and\nhave found effective methods to detect anomaly data in low dimensional space.\nHowever, as the dimension increases, most of these existing methods perform\npoorly in detecting outliers because of \"high dimensional curse\". Even though\nsome approaches aim to solve this problem in high dimensional space, they can\nonly detect some anomaly data appearing in low dimensional space and cannot\ndetect all of anomaly data which appear differently in high dimensional space.\nTo cope with this problem, we propose a new k-nearest section-based method\n(k-NS) in a section-based space. Our proposed approach not only detects\noutliers in low dimensional space with section-density ratio but also detects\noutliers in high dimensional space with the ratio of k-nearest section against\naverage value. After taking a series of experiments with the dimension from 10\nto 10000, the experiment results show that our proposed method achieves 100%\nprecision and 100% recall result in the case of extremely high dimensional\nspace, and better improvement in low dimensional space compared to our\npreviously proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 12:06:06 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Bao", "Zhana", ""]]}, {"id": "1405.1071", "submitter": "Jean-Fran\\c{c}ois Baget", "authors": "Jean-Fran\\c{c}ois Baget and Fabien Garreau and Marie-Laure Mugnier and\n  Swan Rocher", "title": "Revisiting Chase Termination for Existential Rules and their Extension\n  to Nonmonotonic Negation", "comments": "This paper appears in the Proceedings of the 15th International\n  Workshop on Non-Monotonic Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existential rules have been proposed for representing ontological knowledge,\nspecifically in the context of Ontology- Based Data Access. Entailment with\nexistential rules is undecidable. We focus in this paper on conditions that\nensure the termination of a breadth-first forward chaining algorithm known as\nthe chase. Several variants of the chase have been proposed. In the first part\nof this paper, we propose a new tool that allows to extend existing acyclicity\nconditions ensuring chase termination, while keeping good complexity\nproperties. In the second part, we study the extension to existential rules\nwith nonmonotonic negation under stable model semantics, discuss the relevancy\nof the chase variants for these rules and further extend acyclicity results\nobtained in the positive case.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 20:58:01 GMT"}, {"version": "v2", "created": "Fri, 25 Jul 2014 12:49:54 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Baget", "Jean-Fran\u00e7ois", ""], ["Garreau", "Fabien", ""], ["Mugnier", "Marie-Laure", ""], ["Rocher", "Swan", ""]]}, {"id": "1405.1124", "submitter": "Marcello Balduccini", "authors": "Marcello Balduccini, William C. Regli, Duc N. Nguyen", "title": "An ASP-Based Architecture for Autonomous UAVs in Dynamic Environments:\n  Progress Report", "comments": "Proceedings of the 15th International Workshop on Non-Monotonic\n  Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional AI reasoning techniques have been used successfully in many\ndomains, including logistics, scheduling and game playing. This paper is part\nof a project aimed at investigating how such techniques can be extended to\ncoordinate teams of unmanned aerial vehicles (UAVs) in dynamic environments.\nSpecifically challenging are real-world environments where UAVs and other\nnetwork-enabled devices must communicate to coordinate---and communication\nactions are neither reliable nor free. Such network-centric environments are\ncommon in military, public safety and commercial applications, yet most\nresearch (even multi-agent planning) usually takes communications among\ndistributed agents as a given. We address this challenge by developing an agent\narchitecture and reasoning algorithms based on Answer Set Programming (ASP).\nASP has been chosen for this task because it enables high flexibility of\nrepresentation, both of knowledge and of reasoning tasks. Although ASP has been\nused successfully in a number of applications, and ASP-based architectures have\nbeen studied for about a decade, to the best of our knowledge this is the first\npractical application of a complete ASP-based agent architecture. It is also\nthe first practical application of ASP involving a combination of centralized\nreasoning, decentralized reasoning, execution monitoring, and reasoning about\nnetwork communications. This work has been empirically validated using a\ndistributed network-centric software evaluation testbed and the results provide\nguidance to designers in how to understand and control intelligent systems that\noperate in these environments.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 02:05:04 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Balduccini", "Marcello", ""], ["Regli", "William C.", ""], ["Nguyen", "Duc N.", ""]]}, {"id": "1405.1183", "submitter": "Daniel Le Berre", "authors": "Daniel Le Berre", "title": "Some thoughts about benchmarks for NMR", "comments": "Proceedings of the 15th International Workshop on Non-Monotonic\n  Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NMR community would like to build a repository of benchmarks to push\nforward the design of systems implementing NMR as it has been the case for many\nother areas in AI. There are a number of lessons which can be learned from the\nexperience of other communi- ties. Here are a few thoughts about the\nrequirements and choices to make before building such a repository.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 08:09:13 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Berre", "Daniel Le", ""]]}, {"id": "1405.1287", "submitter": "Mario Alviano", "authors": "Mario Alviano and Wolfgang Faber", "title": "Semantics and Compilation of Answer Set Programming with Generalized\n  Atoms", "comments": "The paper appears in the Proceedings of the 15th International\n  Workshop on Non-Monotonic Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is logic programming under the stable model or\nanswer set semantics. During the last decade, this paradigm has seen several\nextensions by generalizing the notion of atom used in these programs. Among\nthese, there are aggregate atoms, HEX atoms, generalized quantifiers, and\nabstract constraints. In this paper we refer to these constructs collectively\nas generalized atoms. The idea common to all of these constructs is that their\nsatisfaction depends on the truth values of a set of (non-generalized) atoms,\nrather than the truth value of a single (non-generalized) atom. Motivated by\nseveral examples, we argue that for some of the more intricate generalized\natoms, the previously suggested semantics provide unintuitive results and\nprovide an alternative semantics, which we call supportedly stable or SFLP\nanswer sets. We show that it is equivalent to the major previously proposed\nsemantics for programs with convex generalized atoms, and that it in general\nadmits more intended models than other semantics in the presence of non-convex\ngeneralized atoms. We show that the complexity of supportedly stable models is\non the second level of the polynomial hierarchy, similar to previous proposals\nand to stable models of disjunctive logic programs. Given these complexity\nresults, we provide a compilation method that compactly transforms programs\nwith generalized atoms in disjunctive normal form to programs without\ngeneralized atoms. Variants are given for the new supportedly stable and the\nexisting FLP semantics, for which a similar compilation technique has not been\nknown so far.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 14:44:40 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Alviano", "Mario", ""], ["Faber", "Wolfgang", ""]]}, {"id": "1405.1397", "submitter": "Shamim Ripon", "authors": "Shamim Ripon, Aoyan Barua, and Mohammad Salah Uddin", "title": "Analysis Tool for UNL-Based Knowledge Representation", "comments": "8 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:cs/0404030 by other authors", "journal-ref": "Journal of Advanced Computer Science and Technology Research\n  (JACSTR) Vol. 2, No. 4, pp. 176-183, 2012", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental issue in knowledge representation is to provide a precise\ndefinition of the knowledge that they possess in a manner that is independent\nof procedural considerations, context free and easy to manipulate, exchange and\nreason about. Knowledge must be accessible to everyone regardless of their\nnative languages. Universal Networking Language (UNL) is a declarative formal\nlanguage and a generalized form of human language in a machine independent\ndigital platform for defining, recapitulating, amending, storing and\ndissipating knowledge among people of different affiliations. UNL extracts\nsemantic data from a native language for Interlingua machine translation. This\npaper presents the development of a graphical tool that incorporates UNL to\nprovide a visual mean to represent the semantic data available in a native\ntext. UNL represents the semantics of a sentence as a conceptual hyper-graph.\nWe translate this information into XML format and create a graph from XML,\nrepresenting the actual concepts available in the native language\n", "versions": [{"version": "v1", "created": "Sun, 4 May 2014 19:50:49 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Ripon", "Shamim", ""], ["Barua", "Aoyan", ""], ["Uddin", "Mohammad Salah", ""]]}, {"id": "1405.1513", "submitter": "Ibrahim Alabdulmohsin", "authors": "Ibrahim Alabdulmohsin", "title": "A Mathematical Theory of Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a mathematical theory of learning is proposed that has many\nparallels with information theory. We consider Vapnik's General Setting of\nLearning in which the learning process is defined to be the act of selecting a\nhypothesis in response to a given training set. Such hypothesis can, for\nexample, be a decision boundary in classification, a set of centroids in\nclustering, or a set of frequent item-sets in association rule mining.\nDepending on the hypothesis space and how the final hypothesis is selected, we\nshow that a learning process can be assigned a numeric score, called learning\ncapacity, which is analogous to Shannon's channel capacity and satisfies\nsimilar interesting properties as well such as the data-processing inequality\nand the information-cannot-hurt inequality. In addition, learning capacity\nprovides the tightest possible bound on the difference between true risk and\nempirical risk of the learning process for all loss functions that are\nparametrized by the chosen hypothesis. It is also shown that the notion of\nlearning capacity equivalently quantifies how sensitive the choice of the final\nhypothesis is to a small perturbation in the training set. Consequently,\nalgorithmic stability is both necessary and sufficient for generalization.\nWhile the theory does not rely on concentration inequalities, we finally show\nthat analogs to classical results in learning theory using the Probably\nApproximately Correct (PAC) model can be immediately deduced using this theory,\nand conclude with information-theoretic bounds to learning capacity.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 06:10:47 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Alabdulmohsin", "Ibrahim", ""]]}, {"id": "1405.1520", "submitter": "Marius Lindauer", "authors": "Holger Hoos and Marius Lindauer and Torsten Schaub", "title": "claspfolio 2: Advances in Algorithm Selection for Answer Set Programming", "comments": "To appear in Theory and Practice of Logic Programming (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To appear in Theory and Practice of Logic Programming (TPLP). Building on the\naward-winning, portfolio-based ASP solver claspfolio, we present claspfolio 2,\na modular and open solver architecture that integrates several different\nportfolio-based algorithm selection approaches and techniques. The claspfolio 2\nsolver framework supports various feature generators, solver selection\napproaches, solver portfolios, as well as solver-schedule-based pre-solving\ntechniques. The default configuration of claspfolio 2 relies on a light-weight\nversion of the ASP solver clasp to generate static and dynamic instance\nfeatures. The flexible open design of claspfolio 2 is a distinguishing factor\neven beyond ASP. As such, it provides a unique framework for comparing and\ncombining existing portfolio-based algorithm selection approaches and\ntechniques in a single, unified framework. Taking advantage of this, we\nconducted an extensive experimental study to assess the impact of different\nfeature sets, selection approaches and base solver portfolios. In addition to\ngaining substantial insights into the utility of the various approaches and\ntechniques, we identified a default configuration of claspfolio 2 that achieves\nsubstantial performance gains not only over clasp's default configuration and\nthe earlier version of claspfolio 2, but also over manually tuned\nconfigurations of clasp.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 07:25:58 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Hoos", "Holger", ""], ["Lindauer", "Marius", ""], ["Schaub", "Torsten", ""]]}, {"id": "1405.1524", "submitter": "Mohammad Mohammadi", "authors": "Mohammad Mohammadi, Shahram Jafari", "title": "An expert system for recommending suitable ornamental fish addition to\n  an aquarium based on aquarium condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expert systems prove to be suitable replacement for human experts when human\nexperts are unavailable for different reasons. Various expert system has been\ndeveloped for wide range of application. Although some expert systems in the\nfield of fishery and aquaculture has been developed but a system that aids user\nin process of selecting a new addition to their aquarium tank never been\ndesigned. This paper proposed an expert system that suggests new addition to an\naquarium tank based on current environmental condition of aquarium and\ncurrently existing fishes in aquarium. The system suggest the best fit for\naquarium condition and most compatible to other fishes in aquarium.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 07:45:09 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Mohammadi", "Mohammad", ""], ["Jafari", "Shahram", ""]]}, {"id": "1405.1544", "submitter": "Alexander Semenov", "authors": "Ilya Otpuschennikov, Alexander Semenov, Stepan Kochemazov", "title": "Transalg: a Tool for Translating Procedural Descriptions of Discrete\n  Functions to SAT", "comments": null, "journal-ref": "Proceedings of The 5th International Workshop on Computer Science\n  and Engineering: Information Processing and Control Engineering (WCSE\n  2015-IPCE) (2015) 289-294", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the Transalg system, designed to produce SAT\nencodings for discrete functions, written as programs in a specific language.\nTranslation of such programs to SAT is based on propositional encoding methods\nfor formal computing models and on the concept of symbolic execution. We used\nthe Transalg system to make SAT encodings for a number of cryptographic\nfunctions.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 09:30:55 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 10:34:47 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Otpuschennikov", "Ilya", ""], ["Semenov", "Alexander", ""], ["Kochemazov", "Stepan", ""]]}, {"id": "1405.1675", "submitter": "Stefano Teso", "authors": "Stefano Teso and Roberto Sebastiani and Andrea Passerini", "title": "Structured Learning Modulo Theories", "comments": "46 pages, 11 figures, submitted to Artificial Intelligence Journal\n  Special Issue on Combining Constraint Solving with Mining and Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling problems containing a mixture of Boolean and numerical variables is\na long-standing interest of Artificial Intelligence. However, performing\ninference and learning in hybrid domains is a particularly daunting task. The\nability to model this kind of domains is crucial in \"learning to design\" tasks,\nthat is, learning applications where the goal is to learn from examples how to\nperform automatic {\\em de novo} design of novel objects. In this paper we\npresent Structured Learning Modulo Theories, a max-margin approach for learning\nin hybrid domains based on Satisfiability Modulo Theories, which allows to\ncombine Boolean reasoning and optimization over continuous linear arithmetical\nconstraints. The main idea is to leverage a state-of-the-art generalized\nSatisfiability Modulo Theory solver for implementing the inference and\nseparation oracles of Structured Output SVMs. We validate our method on\nartificial and real world scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 17:41:43 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 15:57:29 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Teso", "Stefano", ""], ["Sebastiani", "Roberto", ""], ["Passerini", "Andrea", ""]]}, {"id": "1405.1734", "submitter": "Tiep Le", "authors": "Tiep Le, Enrico Pontelli, Tran Cao Son, William Yeoh", "title": "Logic and Constraint Logic Programming for Distributed Constraint\n  Optimization", "comments": "To appear in Theory and Practice of Logic Programming (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of Distributed Constraint Optimization Problems (DCOPs) has gained\nmomentum, thanks to its suitability in capturing complex problems (e.g.,\nmulti-agent coordination and resource allocation problems) that are naturally\ndistributed and cannot be realistically addressed in a centralized manner. The\nstate of the art in solving DCOPs relies on the use of ad-hoc infrastructures\nand ad-hoc constraint solving procedures. This paper investigates an\ninfrastructure for solving DCOPs that is completely built on logic programming\ntechnologies. In particular, the paper explores the use of a general constraint\nsolver (a constraint logic programming system in this context) to handle the\nagent-level constraint solving. The preliminary experiments show that logic\nprogramming provides benefits over a state-of-the-art DCOP system, in terms of\nperformance and scalability, opening the doors to the use of more advanced\ntechnology (e.g., search strategies and complex constraints) for solving DCOPs.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 20:02:09 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 22:55:33 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Le", "Tiep", ""], ["Pontelli", "Enrico", ""], ["Son", "Tran Cao", ""], ["Yeoh", "William", ""]]}, {"id": "1405.1833", "submitter": "Bart Bogaerts", "authors": "Bart Bogaerts, Joost Vennekens, Marc Denecker, Jan Van den Bussche", "title": "FO(C): A Knowledge Representation Language of Causality", "comments": "To appear in Theory and Practice of Logic Programming (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cause-effect relations are an important part of human knowledge. In real\nlife, humans often reason about complex causes linked to complex effects. By\ncomparison, existing formalisms for representing knowledge about causal\nrelations are quite limited in the kind of specifications of causes and effects\nthey allow. In this paper, we present the new language C-Log, which offers a\nsignificantly more expressive representation of effects, including such\nfeatures as the creation of new objects. We show how C-Log integrates with\nfirst-order logic, resulting in the language FO(C). We also compare FO(C) with\nseveral related languages and paradigms, including inductive definitions,\ndisjunctive logic programming, business rules and extensions of Datalog.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 08:25:52 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 08:20:22 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Bogaerts", "Bart", ""], ["Vennekens", "Joost", ""], ["Denecker", "Marc", ""], ["Bussche", "Jan Van den", ""]]}, {"id": "1405.1864", "submitter": "Jesse Alama", "authors": "Jesse Alama", "title": "Dialogues for proof search", "comments": "Submitted to ARQNL (Automated Reasoning in Quantified Non-Classical\n  Logics)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue games are a two-player semantics for a variety of logics, including\nintuitionistic and classical logic. Dialogues can be viewed as a kind of\nanalytic calculus not unlike tableaux. Can dialogue games be an effective\nfoundation for proof search in intuitionistic logic (both first-order and\npropositional)? We announce Kuno, an automated theorem prover for\nintuitionistic first-order logic based on dialogue games.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 10:10:28 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Alama", "Jesse", ""]]}, {"id": "1405.1958", "submitter": "Mohamed Hassan", "authors": "Mohamed Hassan", "title": "A Self-Adaptive Network Protection System", "comments": "91. arXiv admin note: text overlap with arXiv:1204.1336 by other\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this treatise we aim to build a hybrid network automated (self-adaptive)\nsecurity threats discovery and prevention system; by using unconventional\ntechniques and methods, including fuzzy logic and biological inspired\nalgorithms under the context of soft computing.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 15:05:28 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2015 00:51:00 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Hassan", "Mohamed", ""]]}, {"id": "1405.2058", "submitter": "Ari Saptawijaya", "authors": "Ari Saptawijaya and Lu\\'is Moniz Pereira", "title": "Joint Tabling of Logic Program Abductions and Updates", "comments": "To appear in Theory and Practice of Logic Programming (TPLP), 10\n  pages plus bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abductive logic programs offer a formalism to declaratively represent and\nreason about problems in a variety of areas: diagnosis, decision making,\nhypothetical reasoning, etc. On the other hand, logic program updates allow us\nto express knowledge changes, be they internal (or self) and external (or\nworld) changes. Abductive logic programs and logic program updates thus\nnaturally coexist in problems that are susceptible to hypothetical reasoning\nabout change. Taking this as a motivation, in this paper we integrate abductive\nlogic programs and logic program updates by jointly exploiting tabling features\nof logic programming. The integration is based on and benefits from the two\nimplementation techniques we separately devised previously, viz., tabled\nabduction and incremental tabling for query-driven propagation of logic program\nupdates. A prototype of the integrated system is implemented in XSB Prolog.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 19:39:01 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Saptawijaya", "Ari", ""], ["Pereira", "Lu\u00eds Moniz", ""]]}, {"id": "1405.2501", "submitter": "Roman Bart\\'ak", "authors": "Roman Bart\\'ak, Neng-Fa Zhou", "title": "Using Tabled Logic Programming to Solve the Petrobras Planning Problem", "comments": "To appear in Theory and Practice of Logic Programming (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tabling has been used for some time to improve efficiency of Prolog programs\nby memorizing answered queries. The same idea can be naturally used to memorize\nvisited states during search for planning. In this paper we present a planner\ndeveloped in the Picat language to solve the Petrobras planning problem. Picat\nis a novel Prolog-like language that provides pattern matching, deterministic\nand non-deterministic rules, and tabling as its core modelling and solving\nfeatures. We demonstrate these capabilities using the Petrobras problem, where\nthe goal is to plan transport of cargo items from ports to platforms using\nvessels with limited capacity. Monte Carlo Tree Search has been so far the best\ntechnique to tackle this problem and we will show that by using tabling we can\nachieve much better runtime efficiency and better plan quality.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 06:38:25 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Bart\u00e1k", "Roman", ""], ["Zhou", "Neng-Fa", ""]]}, {"id": "1405.2590", "submitter": "Ilias Tachmazidis", "authors": "Ilias Tachmazidis, Grigoris Antoniou and Wolfgang Faber", "title": "Efficient Computation of the Well-Founded Semantics over Big Data", "comments": "16 pages, 4 figures, ICLP 2014, 30th International Conference on\n  Logic Programming July 19-22, Vienna, Austria", "journal-ref": "Theory and Practice of Logic Programming 14 (2014) 445-459", "doi": "10.1017/S1471068414000131", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data originating from the Web, sensor readings and social media result in\nincreasingly huge datasets. The so called Big Data comes with new scientific\nand technological challenges while creating new opportunities, hence the\nincreasing interest in academia and industry. Traditionally, logic programming\nhas focused on complex knowledge structures/programs, so the question arises\nwhether and how it can work in the face of Big Data. In this paper, we examine\nhow the well-founded semantics can process huge amounts of data through mass\nparallelization. More specifically, we propose and evaluate a parallel approach\nusing the MapReduce framework. Our experimental results indicate that our\napproach is scalable and that well-founded semantics can be applied to billions\nof facts. To the best of our knowledge, this is the first work that addresses\nlarge scale nonmonotonic reasoning without the restriction of stratification\nfor predicates of arbitrary arity. To appear in Theory and Practice of Logic\nProgramming (TPLP).\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 21:57:50 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Tachmazidis", "Ilias", ""], ["Antoniou", "Grigoris", ""], ["Faber", "Wolfgang", ""]]}, {"id": "1405.2600", "submitter": "Yuyi Wang", "authors": "Yuyi Wang and Jan Ramon and Zheng-Chu Guo", "title": "Learning from networked examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms are based on the assumption that training\nexamples are drawn independently. However, this assumption does not hold\nanymore when learning from a networked sample because two or more training\nexamples may share some common objects, and hence share the features of these\nshared objects. We show that the classic approach of ignoring this problem\npotentially can have a harmful effect on the accuracy of statistics, and then\nconsider alternatives. One of these is to only use independent examples,\ndiscarding other information. However, this is clearly suboptimal. We analyze\nsample error bounds in this networked setting, providing significantly improved\nresults. An important component of our approach is formed by efficient sample\nweighting schemes, which leads to novel concentration inequalities.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 23:11:52 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 20:24:18 GMT"}, {"version": "v3", "created": "Sat, 18 Feb 2017 00:23:06 GMT"}, {"version": "v4", "created": "Sat, 3 Jun 2017 12:03:19 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Wang", "Yuyi", ""], ["Ramon", "Jan", ""], ["Guo", "Zheng-Chu", ""]]}, {"id": "1405.2642", "submitter": "Radhakrishnan Delhibabu", "authors": "Radhakrishnan Delhibabu", "title": "An Abductive Framework for Horn Knowledge Base Dynamics", "comments": "Applied Mathematics & Information Sciences, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The dynamics of belief and knowledge is one of the major components of any\nautonomous system that should be able to incorporate new pieces of information.\nWe introduced the Horn knowledge base dynamics to deal with two important\npoints: first, to handle belief states that need not be deductively closed; and\nthe second point is the ability to declare certain parts of the belief as\nimmutable. In this paper, we address another, radically new approach to this\nproblem. This approach is very close to the Hansson's dyadic representation of\nbelief. Here, we consider the immutable part as defining a new logical system.\nBy a logical system, we mean that it defines its own consequence relation and\nclosure operator. Based on this, we provide an abductive framework for Horn\nknowledge base dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 06:54:12 GMT"}, {"version": "v2", "created": "Thu, 22 May 2014 16:29:54 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Delhibabu", "Radhakrishnan", ""]]}, {"id": "1405.2664", "submitter": "Ji Zhao", "authors": "Ji Zhao, Deyu Meng", "title": "FastMMD: Ensemble of Circular Discrepancy for Efficient Two-Sample Test", "comments": null, "journal-ref": "Neural Computation, 2015 June, Vol. 27, No. 6, Pages 1345-1372", "doi": "10.1162/NECO_a_00732", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum mean discrepancy (MMD) is a recently proposed test statistic for\ntwo-sample test. Its quadratic time complexity, however, greatly hampers its\navailability to large-scale applications. To accelerate the MMD calculation, in\nthis study we propose an efficient method called FastMMD. The core idea of\nFastMMD is to equivalently transform the MMD with shift-invariant kernels into\nthe amplitude expectation of a linear combination of sinusoid components based\non Bochner's theorem and Fourier transform (Rahimi & Recht, 2007). Taking\nadvantage of sampling of Fourier transform, FastMMD decreases the time\ncomplexity for MMD calculation from $O(N^2 d)$ to $O(L N d)$, where $N$ and $d$\nare the size and dimension of the sample set, respectively. Here $L$ is the\nnumber of basis functions for approximating kernels which determines the\napproximation accuracy. For kernels that are spherically invariant, the\ncomputation can be further accelerated to $O(L N \\log d)$ by using the Fastfood\ntechnique (Le et al., 2013). The uniform convergence of our method has also\nbeen theoretically proved in both unbiased and biased estimates. We have\nfurther provided a geometric explanation for our method, namely ensemble of\ncircular discrepancy, which facilitates us to understand the insight of MMD,\nand is hopeful to help arouse more extensive metrics for assessing two-sample\ntest. Experimental results substantiate that FastMMD is with similar accuracy\nas exact MMD, while with faster computation speed and lower variance than the\nexisting MMD approximation methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 08:20:21 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 12:06:14 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Zhao", "Ji", ""], ["Meng", "Deyu", ""]]}, {"id": "1405.2798", "submitter": "Jun Wang", "authors": "Jun Wang, Ke Sun, Fei Sha, Stephane Marchand-Maillet, Alexandros\n  Kalousis", "title": "Two-Stage Metric Learning", "comments": "Accepted for publication in ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel two-stage metric learning algorithm. We\nfirst map each learning instance to a probability distribution by computing its\nsimilarities to a set of fixed anchor points. Then, we define the distance in\nthe input data space as the Fisher information distance on the associated\nstatistical manifold. This induces in the input data space a new family of\ndistance metric with unique properties. Unlike kernelized metric learning, we\ndo not require the similarity measure to be positive semi-definite. Moreover,\nit can also be interpreted as a local metric learning algorithm with well\ndefined distance approximation. We evaluate its performance on a number of\ndatasets. It outperforms significantly other metric learning methods and SVM.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 15:18:15 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Wang", "Jun", ""], ["Sun", "Ke", ""], ["Sha", "Fei", ""], ["Marchand-Maillet", "Stephane", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1405.2874", "submitter": "EPTCS", "authors": "Dimitri Kartsaklis (University of Oxford), Mehrnoosh Sadrzadeh (Queen\n  Mary University of London)", "title": "A Study of Entanglement in a Categorical Framework of Natural Language", "comments": "In Proceedings QPL 2014, arXiv:1412.8102", "journal-ref": "EPTCS 172, 2014, pp. 249-261", "doi": "10.4204/EPTCS.172.17", "report-no": null, "categories": "cs.CL cs.AI math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In both quantum mechanics and corpus linguistics based on vector spaces, the\nnotion of entanglement provides a means for the various subsystems to\ncommunicate with each other. In this paper we examine a number of\nimplementations of the categorical framework of Coecke, Sadrzadeh and Clark\n(2010) for natural language, from an entanglement perspective. Specifically,\nour goal is to better understand in what way the level of entanglement of the\nrelational tensors (or the lack of it) affects the compositional structures in\npractical situations. Our findings reveal that a number of proposals for verb\nconstruction lead to almost separable tensors, a fact that considerably\nsimplifies the interactions between the words. We examine the ramifications of\nthis fact, and we show that the use of Frobenius algebras mitigates the\npotential problems to a great extent. Finally, we briefly examine a machine\nlearning method that creates verb tensors exhibiting a sufficient level of\nentanglement.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 18:48:54 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 03:02:56 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Kartsaklis", "Dimitri", "", "University of Oxford"], ["Sadrzadeh", "Mehrnoosh", "", "Queen\n  Mary University of London"]]}, {"id": "1405.2878", "submitter": "Bruno Scherrer", "authors": "Bruno Scherrer (INRIA Nancy - Grand Est / LORIA)", "title": "Approximate Policy Iteration Schemes: A Comparison", "comments": "ICML (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the infinite-horizon discounted optimal control problem\nformalized by Markov Decision Processes. We focus on several approximate\nvariations of the Policy Iteration algorithm: Approximate Policy Iteration,\nConservative Policy Iteration (CPI), a natural adaptation of the Policy Search\nby Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\\infty$),\nand the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all\nalgorithms, we describe performance bounds, and make a comparison by paying a\nparticular attention to the concentrability constants involved, the number of\niterations and the memory required. Our analysis highlights the following\npoints: 1) The performance guarantee of CPI can be arbitrarily better than that\nof API/API($\\alpha$), but this comes at the cost of a relative---exponential in\n$\\frac{1}{\\epsilon}$---increase of the number of iterations. 2) PSDP$_\\infty$\nenjoys the best of both worlds: its performance guarantee is similar to that of\nCPI, but within a number of iterations similar to that of API. 3) Contrary to\nAPI that requires a constant memory, the memory needed by CPI and PSDP$_\\infty$\nis proportional to their number of iterations, which may be problematic when\nthe discount factor $\\gamma$ is close to 1 or the approximation error\n$\\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make\nan overall trade-off between memory and performance. Simulations with these\nschemes confirm our analysis.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 19:11:03 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Scherrer", "Bruno", "", "INRIA Nancy - Grand Est / LORIA"]]}, {"id": "1405.2883", "submitter": "Kartik Talamadupula", "authors": "Kartik Talamadupula and David E. Smith and Subbarao Kambhampati", "title": "The Metrics Matter! On the Incompatibility of Different Flavors of\n  Replanning", "comments": "Prior version appears in DMAP 2013 at ICAPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When autonomous agents are executing in the real world, the state of the\nworld as well as the objectives of the agent may change from the agent's\noriginal model. In such cases, the agent's planning process must modify the\nplan under execution to make it amenable to the new conditions, and to resume\nexecution. This brings up the replanning problem, and the various techniques\nthat have been proposed to solve it. In all, three main techniques -- based on\nthree different metrics -- have been proposed in prior automated planning work.\nAn open question is whether these metrics are interchangeable; answering this\nrequires a normalized comparison of the various replanning quality metrics. In\nthis paper, we show that it is possible to support such a comparison by\ncompiling all the respective techniques into a single substrate. Using this\nnovel compilation, we demonstrate that these different metrics are not\ninterchangeable, and that they are not good surrogates for each other. Thus we\nfocus attention on the incompatibility of the various replanning flavors with\neach other, founded in the differences between the metrics that they\nrespectively seek to optimize.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 19:17:21 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Talamadupula", "Kartik", ""], ["Smith", "David E.", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1405.3175", "submitter": "Yong Deng", "authors": "Yong Deng", "title": "D numbers theory: a generalization of Dempster-Shafer evidence theory", "comments": "31 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1404.0540", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient modeling of uncertain information in real world is still an open\nissue. Dempster-Shafer evidence theory is one of the most commonly used\nmethods. However, the Dempster-Shafer evidence theory has the assumption that\nthe hypothesis in the framework of discernment is exclusive of each other. This\ncondition can be violated in real applications, especially in linguistic\ndecision making since the linguistic variables are not exclusive of each others\nessentially. In this paper, a new theory, called as D numbers theory (DNT), is\nsystematically developed to address this issue. The combination rule of two D\nnumbers is presented. An coefficient is defined to measure the exclusive degree\namong the hypotheses in the framework of discernment. The combination rule of\ntwo D numbers is presented. If the exclusive coefficient is one which means\nthat the hypothesis in the framework of discernment is exclusive of each other\ntotally, the D combination is degenerated as the classical Dempster combination\nrule. Finally, a linguistic variables transformation of D numbers is presented\nto make a decision. A numerical example on linguistic evidential decision\nmaking is used to illustrate the efficiency of the proposed D numbers theory.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 14:50:23 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Deng", "Yong", ""]]}, {"id": "1405.3218", "submitter": "Riccardo Zese", "authors": "Elena Bellodi, Evelina Lamma, Fabrizio Riguzzi, Vitor Santos Costa and\n  Riccardo Zese", "title": "Lifted Variable Elimination for Probabilistic Logic Programming", "comments": "To appear in Theory and Practice of Logic Programming (TPLP). arXiv\n  admin note: text overlap with arXiv:1402.0565 by other authors", "journal-ref": "Theory and Practice of Logic Programming 14 (2014) 681-695", "doi": "10.1017/S1471068414000283", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifted inference has been proposed for various probabilistic logical\nframeworks in order to compute the probability of queries in a time that\ndepends on the size of the domains of the random variables rather than the\nnumber of instances. Even if various authors have underlined its importance for\nprobabilistic logic programming (PLP), lifted inference has been applied up to\nnow only to relational languages outside of logic programming. In this paper we\nadapt Generalized Counting First Order Variable Elimination (GC-FOVE) to the\nproblem of computing the probability of queries to probabilistic logic programs\nunder the distribution semantics. In particular, we extend the Prolog Factor\nLanguage (PFL) to include two new types of factors that are needed for\nrepresenting ProbLog programs. These factors take into account the existing\ncausal independence relationships among random variables and are managed by the\nextension to variable elimination proposed by Zhang and Poole for dealing with\nconvergent variables and heterogeneous factors. Two new operators are added to\nGC-FOVE for treating heterogeneous factors. The resulting algorithm, called\nLP$^2$ for Lifted Probabilistic Logic Programming, has been implemented by\nmodifying the PFL implementation of GC-FOVE and tested on three benchmarks for\nlifted inference. A comparison with PITA and ProbLog2 shows the potential of\nthe approach.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 16:29:37 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 19:48:43 GMT"}, {"version": "v3", "created": "Fri, 16 May 2014 08:28:20 GMT"}, {"version": "v4", "created": "Fri, 10 Oct 2014 12:00:38 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bellodi", "Elena", ""], ["Lamma", "Evelina", ""], ["Riguzzi", "Fabrizio", ""], ["Costa", "Vitor Santos", ""], ["Zese", "Riccardo", ""]]}, {"id": "1405.3229", "submitter": "Bruno Scherrer", "authors": "Manel Tagorti (INRIA Nancy - Grand Est / LORIA), Bruno Scherrer (INRIA\n  Nancy - Grand Est / LORIA)", "title": "Rate of Convergence and Error Bounds for LSTD($\\lambda$)", "comments": "(2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider LSTD($\\lambda$), the least-squares temporal-difference algorithm\nwith eligibility traces algorithm proposed by Boyan (2002). It computes a\nlinear approximation of the value function of a fixed policy in a large Markov\nDecision Process. Under a $\\beta$-mixing assumption, we derive, for any value\nof $\\lambda \\in (0,1)$, a high-probability estimate of the rate of convergence\nof this algorithm to its limit. We deduce a high-probability bound on the error\nof this algorithm, that extends (and slightly improves) that derived by Lazaric\net al. (2012) in the specific case where $\\lambda=0$. In particular, our\nanalysis sheds some light on the choice of $\\lambda$ with respect to the\nquality of the chosen linear space and the number of samples, that complies\nwith simulations.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 16:51:54 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Tagorti", "Manel", "", "INRIA Nancy - Grand Est / LORIA"], ["Scherrer", "Bruno", "", "INRIA\n  Nancy - Grand Est / LORIA"]]}, {"id": "1405.3250", "submitter": "Eric Gribkoff", "authors": "Eric Gribkoff, Guy Van den Broeck, and Dan Suciu", "title": "Understanding the Complexity of Lifted Inference and Asymmetric Weighted\n  Model Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study lifted inference for the Weighted First-Order Model\nCounting problem (WFOMC), which counts the assignments that satisfy a given\nsentence in first-order logic (FOL); it has applications in Statistical\nRelational Learning (SRL) and Probabilistic Databases (PDB). We present several\nresults. First, we describe a lifted inference algorithm that generalizes prior\napproaches in SRL and PDB. Second, we provide a novel dichotomy result for a\nnon-trivial fragment of FO CNF sentences, showing that for each sentence the\nWFOMC problem is either in PTIME or #P-hard in the size of the input domain; we\nprove that, in the first case our algorithm solves the WFOMC problem in PTIME,\nand in the second case it fails. Third, we present several properties of the\nalgorithm. Finally, we discuss limitations of lifted inference for symmetric\nprobabilistic databases (where the weights of ground literals depend only on\nthe relation name, and not on the constants of the domain), and prove the\nimpossibility of a dichotomy result for the complexity of probabilistic\ninference for the entire language FOL.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 18:39:11 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 17:31:31 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Gribkoff", "Eric", ""], ["Broeck", "Guy Van den", ""], ["Suciu", "Dan", ""]]}, {"id": "1405.3318", "submitter": "James Neufeld", "authors": "James Neufeld, Andr\\'as Gy\\\"orgy, Dale Schuurmans, Csaba Szepesv\\'ari", "title": "Adaptive Monte Carlo via Bandit Allocation", "comments": "The 31st International Conference on Machine Learning (ICML 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sequentially choosing between a set of unbiased\nMonte Carlo estimators to minimize the mean-squared-error (MSE) of a final\ncombined estimate. By reducing this task to a stochastic multi-armed bandit\nproblem, we show that well developed allocation strategies can be used to\nachieve an MSE that approaches that of the best estimator chosen in retrospect.\nWe then extend these developments to a scenario where alternative estimators\nhave different, possibly stochastic costs. The outcome is a new set of adaptive\nMonte Carlo strategies that provide stronger guarantees than previous\napproaches while offering practical advantages.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 22:29:14 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Neufeld", "James", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Schuurmans", "Dale", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1405.3342", "submitter": "Ali Bakhshi", "authors": "M. Ehsan Shafiee, Emily M. Zechman", "title": "An Agent-based Modeling Framework for Sociotechnical Simulation of Water\n  Distribution Contamination Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the event that a bacteriological or chemical toxin is intro- duced to a\nwater distribution network, a large population of consumers may become exposed\nto the contaminant. A contamination event may be poorly predictable dynamic\nprocess due to the interactions of consumers and utility managers during an\nevent. Consumers that become aware of a threat may select protective actions\nthat change their water demands from typical demand patterns, and new hydraulic\nconditions can arise that differ from conditions that are predicted when\ndemands are considered as exogenous inputs. Consequently, the movement of the\ncontaminant plume in the pipe network may shift from its expected trajectory. A\nsociotechnical model is developed here to integrate agent-based models of\nconsumers with an engineering water distribution system model and capture the\ndynamics between consumer behaviors and the water distribution system for\npredicting contaminant transport and public exposure. Consumers are simulated\nas agents with behaviors defined for water use activities, mobility,\nword-of-mouth communication, and demand reduction, based on a set of rules\nrepresenting an agents autonomy and reaction to health impacts, the\nenvironment, and the actions of other agents. As consumers decrease their water\nuse, the demand exerted on the water distribution system is updated; as the\nflow directions and volumes shift in response, the location of the contaminant\nplume is updated and the amount of contaminant consumed by each agent is\ncalculated. The framework is tested through simulating realistic contamination\nscenarios for a virtual city and water distribution system.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 02:01:58 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Shafiee", "M. Ehsan", ""], ["Zechman", "Emily M.", ""]]}, {"id": "1405.3362", "submitter": "Rehan Abdul Aziz", "authors": "Rehan Abdul Aziz and Geoffrey Chu and Peter James Stuckey", "title": "Grounding Bound Founded Answer Set Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To appear in Theory and Practice of Logic Programming (TPLP)\n  Bound Founded Answer Set Programming (BFASP) is an extension of Answer Set\nProgramming (ASP) that extends stable model semantics to numeric variables.\nWhile the theory of BFASP is defined on ground rules, in practice BFASP\nprograms are written as complex non-ground expressions. Flattening of BFASP is\na technique used to simplify arbitrary expressions of the language to a small\nand well defined set of primitive expressions. In this paper, we first show how\nwe can flatten arbitrary BFASP rule expressions, to give equivalent BFASP\nprograms. Next, we extend the bottom-up grounding technique and magic set\ntransformation used by ASP to BFASP programs. Our implementation shows that for\nBFASP problems, these techniques can significantly reduce the ground program\nsize, and improve subsequent solving.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 05:06:55 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Aziz", "Rehan Abdul", ""], ["Chu", "Geoffrey", ""], ["Stuckey", "Peter James", ""]]}, {"id": "1405.3367", "submitter": "Rehan Abdul Aziz", "authors": "Rehan Abdul Aziz", "title": "Bound Founded Answer Set Programming", "comments": "An extended abstract / full version of a paper accepted to be\n  presented at the Doctoral Consortium of the 30th International Conference on\n  Logic Programming (ICLP 2014), July 19-22, Vienna, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a powerful modelling formalism that is very\nefficient in solving combinatorial problems. ASP solvers implement the stable\nmodel semantics that eliminates circular derivations between Boolean variables\nfrom the solutions of a logic program. Due to this, ASP solvers are better\nsuited than propositional satisfiability (SAT) and Constraint Programming (CP)\nsolvers to solve a certain class of problems whose specification includes\ninductive definitions such as reachability in a graph. On the other hand, ASP\nsolvers suffer from the grounding bottleneck that occurs due to their inability\nto model finite domain variables. Furthermore, the existing stable model\nsemantics are not sufficient to disallow circular reasoning on the bounds of\nnumeric variables. An example where this is required is in modelling shortest\npaths between nodes in a graph. Just as reachability can be encoded as an\ninductive definition with one or more base cases and recursive rules, shortest\npaths between nodes can also be modelled with similar base cases and recursive\nrules for their upper bounds. This deficiency of stable model semantics\nintroduces another type of grounding bottleneck in ASP systems that cannot be\nremoved by naively merging ASP with CP solvers, but requires a theoretical\nextension of the semantics from Booleans and normal rules to bounds over\nnumeric variables and more general rules. In this work, we propose Bound\nFounded Answer Set Programming (BFASP) that resolves this issue and\nconsequently, removes all types of grounding bottleneck inherent in ASP\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 05:40:16 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Aziz", "Rehan Abdul", ""]]}, {"id": "1405.3376", "submitter": "Matthias Thimm", "authors": "Anthony Hunter and Matthias Thimm", "title": "Probabilistic Argumentation with Epistemic Extensions and Incomplete\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract argumentation offers an appealing way of representing and evaluating\narguments and counterarguments. This approach can be enhanced by a probability\nassignment to each argument. There are various interpretations that can be\nascribed to this assignment. In this paper, we regard the assignment as\ndenoting the belief that an agent has that an argument is justifiable, i.e.,\nthat both the premises of the argument and the derivation of the claim of the\nargument from its premises are valid. This leads to the notion of an epistemic\nextension which is the subset of the arguments in the graph that are believed\nto some degree (which we defined as the arguments that have a probability\nassignment greater than 0.5). We consider various constraints on the\nprobability assignment. Some constraints correspond to standard notions of\nextensions, such as grounded or stable extensions, and some constraints give us\nnew kinds of extensions.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 06:38:15 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Hunter", "Anthony", ""], ["Thimm", "Matthias", ""]]}, {"id": "1405.3451", "submitter": "Josef Urban", "authors": "Cezary Kaliszyk and Josef Urban and Jiri Vyskocil and Herman Geuvers", "title": "Developing Corpus-based Translation Methods between Informal and Formal\n  Mathematics: Project Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this project is to (i) accumulate annotated informal/formal\nmathematical corpora suitable for training semi-automated translation between\ninformal and formal mathematics by statistical machine-translation methods,\n(ii) to develop such methods oriented at the formalization task, and in\nparticular (iii) to combine such methods with learning-assisted automated\nreasoning that will serve as a strong semantic component. We describe these\nideas, the initial set of corpora, and some initial experiments done over them.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 11:01:48 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""], ["Vyskocil", "Jiri", ""], ["Geuvers", "Herman", ""]]}, {"id": "1405.3486", "submitter": "Zhizheng Zhang", "authors": "Zhizheng Zhang and Kaikai Zhao", "title": "ESmodels: An Epistemic Specification Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (To appear in Theory and Practice of Logic Programming (TPLP))\n  ESmodels is designed and implemented as an experiment platform to investigate\nthe semantics, language, related reasoning algorithms, and possible\napplications of epistemic specifications.We first give the epistemic\nspecification language of ESmodels and its semantics. The language employs only\none modal operator K but we prove that it is able to represent luxuriant modal\noperators by presenting transformation rules. Then, we describe basic\nalgorithms and optimization approaches used in ESmodels. After that, we discuss\npossible applications of ESmodels in conformant planning and constraint\nsatisfaction. Finally, we conclude with perspectives.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 13:26:11 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Zhang", "Zhizheng", ""], ["Zhao", "Kaikai", ""]]}, {"id": "1405.3487", "submitter": "Petr Baudi\\v{s}", "authors": "Petr Baudi\\v{s}", "title": "COCOpf: An Algorithm Portfolio Framework", "comments": "POSTER2014. arXiv admin note: text overlap with arXiv:1206.5780 by\n  other authors without attribution", "journal-ref": "Poster 2014 --- the 18th International Student Conference on\n  Electrical Engineering. Czech Technical University, Prague, Czech Republic\n  (2014)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Algorithm portfolios represent a strategy of composing multiple heuristic\nalgorithms, each suited to a different class of problems, within a single\ngeneral solver that will choose the best suited algorithm for each input. This\napproach recently gained popularity especially for solving combinatoric\nproblems, but optimization applications are still emerging. The COCO platform\nof the BBOB workshop series is the current standard way to measure performance\nof continuous black-box optimization algorithms.\n  As an extension to the COCO platform, we present the Python-based COCOpf\nframework that allows composing portfolios of optimization algorithms and\nrunning experiments with different selection strategies. In our framework, we\nfocus on black-box algorithm portfolio and online adaptive selection. As a\ndemonstration, we measure the performance of stock SciPy optimization\nalgorithms and the popular CMA algorithm alone and in a portfolio with two\nsimple selection strategies. We confirm that even a naive selection strategy\ncan provide improved performance across problem classes.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 13:26:57 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Baudi\u0161", "Petr", ""]]}, {"id": "1405.3539", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh, Adam Ganz", "title": "Pattern Recognition in Narrative: Tracking Emotional Expression in\n  Context", "comments": "21 pages, 7 figures", "journal-ref": "Journal of Data Mining & Digital Humanities, 2015 (May 26, 2015)\n  jdmdh:647", "doi": "10.46298/jdmdh.647", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using geometric data analysis, our objective is the analysis of narrative,\nwith narrative of emotion being the focus in this work. The following two\nprinciples for analysis of emotion inform our work. Firstly, emotion is\nrevealed not as a quality in its own right but rather through interaction. We\nstudy the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the\n3-way relationship of Emma, Charles and Rodolphe in the novel {\\em Madame\nBovary}. Secondly, emotion, that is expression of states of mind of subjects,\nis formed and evolves within the narrative that expresses external events and\n(personal, social, physical) context. In addition to the analysis methodology\nwith key aspects that are innovative, the input data used is crucial. We use,\nfirstly, dialogue, and secondly, broad and general description that\nincorporates dialogue. In a follow-on study, we apply our unsupervised\nnarrative mapping to data streams with very low emotional expression. We map\nthe narrative of Twitter streams. Thus we demonstrate map analysis of general\nnarratives.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 15:29:48 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 14:00:50 GMT"}, {"version": "v3", "created": "Mon, 4 May 2015 22:16:39 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Murtagh", "Fionn", ""], ["Ganz", "Adam", ""]]}, {"id": "1405.3546", "submitter": "Mario Alviano", "authors": "Mario Alviano, Carmine Dodaro and Francesco Ricca", "title": "Anytime Computation of Cautious Consequences in Answer Set Programming", "comments": "To appear in Theory and Practice of Logic Programming", "journal-ref": "Theory and Practice of Logic Programming 14 (2014) 755-770", "doi": "10.1017/S1471068414000325", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query answering in Answer Set Programming (ASP) is usually solved by\ncomputing (a subset of) the cautious consequences of a logic program. This task\nis computationally very hard, and there are programs for which computing\ncautious consequences is not viable in reasonable time. However, current ASP\nsolvers produce the (whole) set of cautious consequences only at the end of\ntheir computation. This paper reports on strategies for computing cautious\nconsequences, also introducing anytime algorithms able to produce sound answers\nduring the computation.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 15:46:33 GMT"}, {"version": "v2", "created": "Thu, 15 May 2014 09:27:38 GMT"}, {"version": "v3", "created": "Mon, 15 Sep 2014 12:29:17 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Alviano", "Mario", ""], ["Dodaro", "Carmine", ""], ["Ricca", "Francesco", ""]]}, {"id": "1405.3559", "submitter": "Andrea Mignatti", "authors": "Giorgio Corani and Andrea Mignatti", "title": "Credal Model Averaging for classification: representing prior ignorance\n  and expert opinions", "comments": "15 pages 6 figures Preprint submitted to the International Journal of\n  Approximate Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model averaging (BMA) is the state of the art approach for\novercoming model uncertainty. Yet, especially on small data sets, the results\nyielded by BMA might be sensitive to the prior over the models. Credal Model\nAveraging (CMA) addresses this problem by substituting the single prior over\nthe models by a set of priors (credal set). Such approach solves the problem of\nhow to choose the prior over the models and automates sensitivity analysis. We\ndiscuss various CMA algorithms for building an ensemble of logistic regressors\ncharacterized by different sets of covariates. We show how CMA can be\nappropriately tuned to the case in which one is prior-ignorant and to the case\nin which instead domain knowledge is available. CMA detects prior-dependent\ninstances, namely instances in which a different class is more probable\ndepending on the prior over the models. On such instances CMA suspends the\njudgment, returning multiple classes. We thoroughly compare different BMA and\nCMA variants on a real case study, predicting presence of Alpine marmot burrows\nin an Alpine valley. We find that BMA is almost a random guesser on the\ninstances recognized as prior-dependent by CMA.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 16:06:39 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Corani", "Giorgio", ""], ["Mignatti", "Andrea", ""]]}, {"id": "1405.3570", "submitter": "Daniel Gall", "authors": "Daniel Gall and Thom Fr\\\"uhwirth", "title": "Exchanging Conflict Resolution in an Adaptable Implementation of ACT-R", "comments": "To appear in Theory and Practice of Logic Programming (TPLP).\n  Accepted paper for ICLP 2014. 12 pages + appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational cognitive science, the cognitive architecture ACT-R is very\npopular. It describes a model of cognition that is amenable to computer\nimplementation, paving the way for computational psychology. Its underlying\npsychological theory has been investigated in many psychological experiments,\nbut ACT-R lacks a formal definition of its underlying concepts from a\nmathematical-computational point of view. Although the canonical implementation\nof ACT-R is now modularized, this production rule system is still hard to adapt\nand extend in central components like the conflict resolution mechanism (which\ndecides which of the applicable rules to apply next).\n  In this work, we present a concise implementation of ACT-R based on\nConstraint Handling Rules which has been derived from a formalization in prior\nwork. To show the adaptability of our approach, we implement several different\nconflict resolution mechanisms discussed in the ACT-R literature. This results\nin the first implementation of one such mechanism. For the other mechanisms, we\nempirically evaluate if our implementation matches the results of reference\nimplementations of ACT-R.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 16:57:36 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Gall", "Daniel", ""], ["Fr\u00fchwirth", "Thom", ""]]}, {"id": "1405.3637", "submitter": "Yuanlin Zhang", "authors": "Michael Gelfond and Yuanlin Zhang", "title": "Vicious Circle Principle and Logic Programs with Aggregates", "comments": null, "journal-ref": "Theory and Practice of Logic Programming 14 (2014) 587-601", "doi": "10.1017/S1471068414000222", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a knowledge representation language $\\mathcal{A}log$ which\nextends ASP with aggregates. The goal is to have a language based on simple\nsyntax and clear intuitive and mathematical semantics. We give some properties\nof $\\mathcal{A}log$, an algorithm for computing its answer sets, and comparison\nwith other approaches.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 19:36:38 GMT"}, {"version": "v2", "created": "Thu, 15 May 2014 02:42:50 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Gelfond", "Michael", ""], ["Zhang", "Yuanlin", ""]]}, {"id": "1405.3710", "submitter": "Martin Gebser", "authors": "Francesco Calimeri, Martin Gebser, Marco Maratea, Francesco Ricca", "title": "The Design of the Fifth Answer Set Programming Competition", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a well-established paradigm of declarative\nprogramming that has been developed in the field of logic programming and\nnonmonotonic reasoning. Advances in ASP solving technology are customarily\nassessed in competition events, as it happens for other closely-related\nproblem-solving technologies like SAT/SMT, QBF, Planning and Scheduling. ASP\nCompetitions are (usually) biennial events; however, the Fifth ASP Competition\ndeparts from tradition, in order to join the FLoC Olympic Games at the Vienna\nSummer of Logic 2014, which is expected to be the largest event in the history\nof logic. This edition of the ASP Competition series is jointly organized by\nthe University of Calabria (Italy), the Aalto University (Finland), and the\nUniversity of Genova (Italy), and is affiliated with the 30th International\nConference on Logic Programming (ICLP 2014). It features a completely\nre-designed setup, with novelties involving the design of tracks, the scoring\nschema, and the adherence to a fixed modeling language in order to push the\nadoption of the ASP-Core-2 standard. Benchmark domains are taken from past\neditions, and best system packages submitted in 2013 are compared with new\nversions and solvers.\n  To appear in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 22:15:50 GMT"}, {"version": "v2", "created": "Fri, 16 May 2014 07:35:23 GMT"}, {"version": "v3", "created": "Mon, 26 May 2014 05:01:26 GMT"}, {"version": "v4", "created": "Tue, 3 Jun 2014 17:16:36 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Calimeri", "Francesco", ""], ["Gebser", "Martin", ""], ["Maratea", "Marco", ""], ["Ricca", "Francesco", ""]]}, {"id": "1405.3713", "submitter": "Emmanuelle-Anna Dietz", "authors": "Lu\\'is Moniz Pereira, Emmanuelle-Anna Dietz, Steffen H\\\"olldobler", "title": "Contextual Abductive Reasoning with Side-Effects", "comments": "14 pages, no figures, 1 table", "journal-ref": "Theory and Practice of Logic Programming 14 (2014) 633-648", "doi": "10.1017/S1471068414000258", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The belief bias effect is a phenomenon which occurs when we think that we\njudge an argument based on our reasoning, but are actually influenced by our\nbeliefs and prior knowledge. Evans, Barston and Pollard carried out a\npsychological syllogistic reasoning task to prove this effect. Participants\nwere asked whether they would accept or reject a given syllogism. We discuss\none specific case which is commonly assumed to be believable but which is\nactually not logically valid. By introducing abnormalities, abduction and\nbackground knowledge, we adequately model this case under the weak completion\nsemantics. Our formalization reveals new questions about possible extensions in\nabductive reasoning. For instance, observations and their explanations might\ninclude some relevant prior abductive contextual information concerning some\nside-effect or leading to a contestable or refutable side-effect. A weaker\nnotion indicates the support of some relevant consequences by a prior abductive\ncontext. Yet another definition describes jointly supported relevant\nconsequences, which captures the idea of two observations containing mutually\nsupportive side-effects. Though motivated with and exemplified by the running\npsychology application, the various new general abductive context definitions\nare introduced here and given a declarative semantics for the first time, and\nhave a much wider scope of application. Inspection points, a concept introduced\nby Pereira and Pinto, allows us to express these definitions syntactically and\nintertwine them into an operational semantics.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 23:12:45 GMT"}, {"version": "v2", "created": "Fri, 16 May 2014 02:27:40 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Pereira", "Lu\u00eds Moniz", ""], ["Dietz", "Emmanuelle-Anna", ""], ["H\u00f6lldobler", "Steffen", ""]]}, {"id": "1405.3727", "submitter": "Sweta Rai", "authors": "Sweta Rai", "title": "Student Dropout Risk Assessment in Undergraduate Course at Residential\n  University", "comments": "arXiv admin note: text overlap with arXiv:1202.4815, arXiv:1203.3832,\n  arXiv:1002.1144 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Student dropout prediction is an indispensable for numerous intelligent\nsystems to measure the education system and success rate of any university as\nwell as throughout the university in the world. Therefore, it becomes essential\nto develop efficient methods for prediction of the students at risk of dropping\nout, enabling the adoption of proactive process to minimize the situation.\nThus, this research work propose a prototype machine learning tool which can\nautomatically recognize whether the student will continue their study or drop\ntheir study using classification technique based on decision tree and extract\nhidden information from large data about what factors are responsible for\ndropout student. Further the contribution of factors responsible for dropout\nrisk was studied using discriminant analysis and to extract interesting\ncorrelations, frequent patterns, associations or casual structures among\nsignificant datasets, Association rule mining was applied. In this study, the\ndescriptive statistics analysis was carried out to measure the quality of data\nusing SPSS 20.0 statistical software and application of decision tree and\nassociation rule were carried out by using WEKA data mining tool.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 02:35:41 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Rai", "Sweta", ""]]}, {"id": "1405.3729", "submitter": "Priyanka Saini", "authors": "Priyanka Saini", "title": "Building a Classification Model for Enrollment In Higher Educational\n  Courses using Data Mining Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Mining is the process of extracting useful patterns from the huge amount\nof database and many data mining techniques are used for mining these patterns.\nRecently, one of the remarkable facts in higher educational institute is the\nrapid growth data and this educational data is expanding quickly without any\nadvantage to the educational management. The main aim of the management is to\nrefine the education standard; therefore by applying the various data mining\ntechniques on this data one can get valuable information. This research study\nproposed the \"classification model for the student's enrollment process in\nhigher educational courses using data mining techniques\". Additionally, this\nstudy contributes to finding some patterns that are meaningful to management.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 02:53:44 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Saini", "Priyanka", ""]]}, {"id": "1405.3790", "submitter": "Ana Sofia Gomes", "authors": "Ana Sofia Gomes and Jos\\'e J\\'ulio Alferes", "title": "Transaction Logic with (Complex) Events", "comments": "To appear in Theory and Practice of Logic Programming (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with the problem of combining reactive features, such as the\nability to respond to events and define complex events, with the execution of\ntransactions over general Knowledge Bases (KBs).\n  With this as goal, we build on Transaction Logic (TR), a logic precisely\ndesigned to model and execute transactions in KBs defined by arbitrary logic\ntheories. In it, transactions are written in a logic-programming style, by\ncombining primitive update operations over a general KB, with the usual logic\nprogramming connectives and some additional connectives e.g. to express\nsequence of actions. While TR is a natural choice to deal with transactions, it\nremains the question whether TR can be used to express complex events, but also\nto deal simultaneously with the detection of complex events and the execution\nof transactions. In this paper we show that the former is possible while the\nlatter is not. For that, we start by illustrating how TR can express complex\nevents, and in particular, how SNOOP event expressions can be translated in the\nlogic. Afterwards, we show why TR fails to deal with the two issues together,\nand to solve the intended problem propose Transaction Logic with Events, its\nsyntax, model theory and executional semantics. The achieved solution is a\nnon-monotonic extension of TR, which guarantees that every complex event\ndetected in a transaction is necessarily responded.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 10:18:12 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Gomes", "Ana Sofia", ""], ["Alferes", "Jos\u00e9 J\u00falio", ""]]}, {"id": "1405.3792", "submitter": "Angelos Charalambidis", "authors": "Angelos Charalambidis, Zolt\\'an \\'Esik, Panos Rondogiannis", "title": "Minimum Model Semantics for Extensional Higher-order Logic Programming\n  with Negation", "comments": null, "journal-ref": "Theory and Practice of Logic Programming 14 (2014) 725-737", "doi": "10.1017/S1471068414000313", "report-no": null, "categories": "cs.PL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensional higher-order logic programming has been introduced as a\ngeneralization of classical logic programming. An important characteristic of\nthis paradigm is that it preserves all the well-known properties of traditional\nlogic programming. In this paper we consider the semantics of negation in the\ncontext of the new paradigm. Using some recent results from non-monotonic\nfixed-point theory, we demonstrate that every higher-order logic program with\nnegation has a unique minimum infinite-valued model. In this way we obtain the\nfirst purely model-theoretic semantics for negation in extensional higher-order\nlogic programming. Using our approach, we resolve an old paradox that was\nintroduced by W. W. Wadge in order to demonstrate the semantic difficulties of\nhigher-order logic programming.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 10:37:42 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Charalambidis", "Angelos", ""], ["\u00c9sik", "Zolt\u00e1n", ""], ["Rondogiannis", "Panos", ""]]}, {"id": "1405.3824", "submitter": "Federico Chesani", "authors": "Marco Gavanelli, Stefano Bragaglia, Michela Milano, Federico Chesani,\n  Elisa Marengo, Paolo Cagnoli", "title": "Multi-Criteria Optimal Planning for Energy Policies in CLP", "comments": "Accepted at ICLP2014 Conference as Technical Communication, due to\n  appear in Theory and Practice of Logic Programming (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the policy making process a number of disparate and diverse issues such as\neconomic development, environmental aspects, as well as the social acceptance\nof the policy, need to be considered. A single person might not have all the\nrequired expertises, and decision support systems featuring optimization\ncomponents can help to assess policies. Leveraging on previous work on\nStrategic Environmental Assessment, we developed a fully-fledged system that is\nable to provide optimal plans with respect to a given objective, to perform\nmulti-objective optimization and provide sets of Pareto optimal plans, and to\nvisually compare them. Each plan is environmentally assessed and its footprint\nis evaluated. The heart of the system is an application developed in a popular\nConstraint Logic Programming system on the Reals sort. It has been equipped\nwith a web service module that can be queried through standard interfaces, and\nan intuitive graphic user interface.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 12:48:35 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Gavanelli", "Marco", ""], ["Bragaglia", "Stefano", ""], ["Milano", "Michela", ""], ["Chesani", "Federico", ""], ["Marengo", "Elisa", ""], ["Cagnoli", "Paolo", ""]]}, {"id": "1405.3826", "submitter": "Heike Stephan", "authors": "Heike Stephan", "title": "Application of Methods for Syntax Analysis of Context-Free Languages to\n  Query Evaluation of Logic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  My research goal is to employ a parser generation algorithm based on the\nEarley parsing algorithm to the evaluation and compilation of queries to logic\nprograms, especially to deductive databases. By means of partial deduction,\nfrom a query to a logic program a parameterized automaton is to be generated\nthat models the evaluation of this query. This automaton can be compiled to\nexecutable code; thus we expect a speedup in runtime of query evaluation. An\nextended abstract/ full version of a paper accepted to be presented at the\nDoctoral Consortium of the 30th International Conference on Logic Programming\n(ICLP 2014), July 19-22, Vienna, Austria\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 12:56:03 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Stephan", "Heike", ""]]}, {"id": "1405.3896", "submitter": "Mario Abrantes", "authors": "M\\'ario Abrantes, Lu\\'is Moniz Pereira", "title": "Properties of Stable Model Semantics Extensions", "comments": "To appear in Theory and Practice of Logic Programming (TPLP), 10\n  pages plus appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stable model (SM) semantics lacks the properties of existence, relevance\nand cumulativity. If we prospectively consider the class of conservative\nextensions of SM semantics (i.e., semantics that for each normal logic program\nP retrieve a superset of the set of stable models of P), one may wander how do\nthe semantics of this class behave in what concerns the aforementioned\nproperties. That is the type of issue dealt with in this paper. We define a\nlarge class of conservative extensions of the SM semantics, dubbed affix stable\nmodel semantics, ASM, and study the above referred properties into two\nnon-disjoint subfamilies of the class ASM, here dubbed ASMh and ASMm. From this\nstudy a number of results stem which facilitate the assessment of semantics in\nthe class ASMh U ASMm with respect to the properties of existence, relevance\nand cumulativity, whilst unveiling relations among these properties. As a\nresult of the approach taken in our work, light is shed on the characterization\nof the SM semantics, as we show that the properties of (lack of) existence and\n(lack of) cautious monotony are equivalent, which opposes statements on this\nissue that may be found in the literature; we also characterize the relevance\nfailure of SM semantics in a more clear way than usually stated in the\nliterature.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 16:13:47 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Abrantes", "M\u00e1rio", ""], ["Pereira", "Lu\u00eds Moniz", ""]]}, {"id": "1405.3939", "submitter": "Aadhityan  A", "authors": "Aadhityan A", "title": "A Novel Method for Developing Robotics via Artificial Intelligence and\n  Internet of Things", "comments": null, "journal-ref": "IJCA Proceedings on National Conference on Future Computing 2014\n  NCFC 2014(1):1-4, January 2014. Published by Foundation of Computer Science,\n  New York, USA", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describe about a new methodology for developing and improving the\nrobotics field via artificial intelligence and internet of things. Now a day,\nwe can say Artificial Intelligence take the world into robotics. Almost all\nindustries use robots for lot of works. They are use co-operative robots to\nmake different kind of works. But there was some problem to make robot for\nmulti tasks. So there was a necessary new methodology to made multi tasking\nrobots. It will be done only by artificial intelligence and internet of things.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 21:27:42 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["A", "Aadhityan", ""]]}, {"id": "1405.4008", "submitter": "Aya Saad", "authors": "Aya Saad", "title": "CDF-Intervals: A Reliable Framework to Reason about Data with\n  Uncertainty", "comments": "10 pages, 15 Postscript figures, uses new_tlp.cls and acmtrans.bst,\n  full version of a paper accepted to be presented at the Doctoral Consortium\n  of the 30th International Conference on Logic Programming (ICLP 2014), July\n  19-22, Vienna, Austria. arXiv admin note: substantial text overlap with\n  arXiv:1405.2801", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research introduces a new constraint domain for reasoning about data\nwith uncertainty. It extends convex modeling with the notion of p-box to gain\nadditional quantifiable information on the data whereabouts. Unlike existing\napproaches, the p-box envelops an unknown probability instead of approximating\nits representation. The p-box bounds are uniform cumulative distribution\nfunctions (cdf) in order to employ linear computations in the probabilistic\ndomain. The reasoning by means of p-box cdf-intervals is an interval\ncomputation which is exerted on the real domain then it is projected onto the\ncdf domain. This operation conveys additional knowledge represented by the\nobtained probabilistic bounds. Empirical evaluation shows that, with minimal\noverhead, the output solution set realizes a full enclosure of the data along\nwith tighter bounds on its probabilistic distributions.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 20:48:41 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Saad", "Aya", ""]]}, {"id": "1405.4053", "submitter": "Quoc Le", "authors": "Quoc V. Le and Tomas Mikolov", "title": "Distributed Representations of Sentences and Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms require the input to be represented as a\nfixed-length feature vector. When it comes to texts, one of the most common\nfixed-length features is bag-of-words. Despite their popularity, bag-of-words\nfeatures have two major weaknesses: they lose the ordering of the words and\nthey also ignore semantics of the words. For example, \"powerful,\" \"strong\" and\n\"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an\nunsupervised algorithm that learns fixed-length feature representations from\nvariable-length pieces of texts, such as sentences, paragraphs, and documents.\nOur algorithm represents each document by a dense vector which is trained to\npredict words in the document. Its construction gives our algorithm the\npotential to overcome the weaknesses of bag-of-words models. Empirical results\nshow that Paragraph Vectors outperform bag-of-words models as well as other\ntechniques for text representations. Finally, we achieve new state-of-the-art\nresults on several text classification and sentiment analysis tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 07:12:16 GMT"}, {"version": "v2", "created": "Thu, 22 May 2014 23:23:19 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Le", "Quoc V.", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1405.4138", "submitter": "Reza Azizi", "authors": "Reza Azizi", "title": "Empirical Study of Artificial Fish Swarm Algorithm", "comments": null, "journal-ref": "International Journal of Computing, Communications and Networking\n  (IJCCN) , Volume 3, No.1, Pages 01-07, March 2014", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial fish swarm algorithm (AFSA) is one of the swarm intelligence\noptimization algorithms that works based on population and stochastic search.\nIn order to achieve acceptable result, there are many parameters needs to be\nadjusted in AFSA. Among these parameters, visual and step are very significant\nin view of the fact that artificial fish basically move based on these\nparameters. In standard AFSA, these two parameters remain constant until the\nalgorithm termination. Large values of these parameters increase the capability\nof algorithm in global search, while small values improve the local search\nability of the algorithm. In this paper, we empirically study the performance\nof the AFSA and different approaches to balance between local and global\nexploration have been tested based on the adaptive modification of visual and\nstep during algorithm execution. The proposed approaches have been evaluated\nbased on the four well-known benchmark functions. Experimental results show\nconsiderable positive impact on the performance of AFSA.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 12:02:42 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Azizi", "Reza", ""]]}, {"id": "1405.4180", "submitter": "Liang Chang", "authors": "Liang Chang, Uli Sattler, Tianlong Gu", "title": "Algorithm for Adapting Cases Represented in a Tractable Description\n  Logic", "comments": "21 pages. ICCBR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Case-based reasoning (CBR) based on description logics (DLs) has gained a lot\nof attention lately. Adaptation is a basic task in the CBR inference that can\nbe modeled as the knowledge base revision problem and solved in propositional\nlogic. However, in DLs, it is still a challenge problem since existing revision\noperators only work well for strictly restricted DLs of the \\emph{DL-Lite}\nfamily, and it is difficult to design a revision algorithm which is\nsyntax-independent and fine-grained. In this paper, we present a new method for\nadaptation based on the DL $\\mathcal{EL_{\\bot}}$. Following the idea of\nadaptation as revision, we firstly extend the logical basis for describing\ncases from propositional logic to the DL $\\mathcal{EL_{\\bot}}$, and present a\nformalism for adaptation based on $\\mathcal{EL_{\\bot}}$. Then we present an\nadaptation algorithm for this formalism and demonstrate that our algorithm is\nsyntax-independent and fine-grained. Our work provides a logical basis for\nadaptation in CBR systems where cases and domain knowledge are described by the\ntractable DL $\\mathcal{EL_{\\bot}}$.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 14:24:42 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Chang", "Liang", ""], ["Sattler", "Uli", ""], ["Gu", "Tianlong", ""]]}, {"id": "1405.4206", "submitter": "Joachim Jansen", "authors": "Joachim Jansen", "title": "Model revision inference for extensions of first order logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I am Joachim Jansen and this is my research summary, part of my application\nto the Doctoral Consortium at ICLP'14. I am a PhD student in the Knowledge\nRepresentation and Reasoning (KRR) research group, a subgroup of the\nDeclarative Languages and Artificial Intelligence (DTAI) group at the\ndepartment of Computer Science at KU Leuven. I started my PhD in September\n2012. My promotor is prof. dr. ir. Gerda Janssens and my co-promotor is prof.\ndr. Marc Denecker. I can be contacted at joachim.jansen@cs.kuleuven.be or at:\nRoom 01.167 Celestijnenlaan 200A 3001 Heverlee Belgium An extended abstract /\nfull version of a paper accepted to be presented at the Doctoral Consortium of\nthe 30th International Conference on Logic Programming (ICLP 2014), July 19-22,\nVienna, Austria\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 15:23:24 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Jansen", "Joachim", ""]]}, {"id": "1405.4392", "submitter": "Sunny Mitra", "authors": "Sunny Mitra, Ritwik Mitra, Martin Riedl, Chris Biemann, Animesh\n  Mukherjee, Pawan Goyal", "title": "That's sick dude!: Automatic identification of word sense change across\n  different timescales", "comments": "10 pages, 2 figures, ACL-2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an unsupervised method to identify noun sense\nchanges based on rigorous analysis of time-varying text data available in the\nform of millions of digitized books. We construct distributional thesauri based\nnetworks from data at different time points and cluster each of them separately\nto obtain word-centric sense clusters corresponding to the different time\npoints. Subsequently, we compare these sense clusters of two different time\npoints to find if (i) there is birth of a new sense or (ii) if an older sense\nhas got split into more than one sense or (iii) if a newer sense has been\nformed from the joining of older senses or (iv) if a particular sense has died.\nWe conduct a thorough evaluation of the proposed methodology both manually as\nwell as through comparison with WordNet. Manual evaluation indicates that the\nalgorithm could correctly identify 60.4% birth cases from a set of 48 randomly\npicked samples and 57% split/join cases from a set of 21 randomly picked\nsamples. Remarkably, in 44% cases the birth of a novel sense is attested by\nWordNet, while in 46% cases and 43% cases split and join are respectively\nconfirmed by WordNet. Our approach can be applied for lexicography, as well as\nfor applications like word sense disambiguation or semantic search.\n", "versions": [{"version": "v1", "created": "Sat, 17 May 2014 13:29:13 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Mitra", "Sunny", ""], ["Mitra", "Ritwik", ""], ["Riedl", "Martin", ""], ["Biemann", "Chris", ""], ["Mukherjee", "Animesh", ""], ["Goyal", "Pawan", ""]]}, {"id": "1405.4917", "submitter": "Hubie Chen", "authors": "Hubie Chen", "title": "An Algebraic Hardness Criterion for Surjective Constraint Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constraint satisfaction problem (CSP) on a relational structure B is to\ndecide, given a set of constraints on variables where the relations come from\nB, whether or not there is a assignment to the variables satisfying all of the\nconstraints; the surjective CSP is the variant where one decides the existence\nof a surjective satisfying assignment onto the universe of B. We present an\nalgebraic condition on the polymorphism clone of B and prove that it is\nsufficient for the hardness of the surjective CSP on a finite structure B, in\nthe sense that this problem admits a reduction from a certain fixed-structure\nCSP. To our knowledge, this is the first result that allows one to use\nalgebraic information from a relational structure B to infer information on the\ncomplexity hardness of surjective constraint satisfaction on B. A corollary of\nour result is that, on any finite non-trivial structure having only essentially\nunary polymorphisms, surjective constraint satisfaction is NP-complete.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 23:23:24 GMT"}, {"version": "v2", "created": "Mon, 2 Jun 2014 10:29:27 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Chen", "Hubie", ""]]}, {"id": "1405.5048", "submitter": "Mihai Polceanu M.Sc.", "authors": "Mihai Polceanu, C\\'edric Buche", "title": "Towards A Theory-Of-Mind-Inspired Generic Decision-Making Framework", "comments": "7 pages, 5 figures, IJCAI 2013 Symposium on AI in Angry Birds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation is widely used to make model-based predictions, but few approaches\nhave attempted this technique in dynamic physical environments of medium to\nhigh complexity or in general contexts. After an introduction to the cognitive\nscience concepts from which this work is inspired and the current development\nin the use of simulation as a decision-making technique, we propose a generic\nframework based on theory of mind, which allows an agent to reason and perform\nactions using multiple simulations of automatically created or externally\ninputted models of the perceived environment. A description of a partial\nimplementation is given, which aims to solve a popular game within the\nIJCAI2013 AIBirds contest. Results of our approach are presented, in comparison\nwith the competition benchmark. Finally, future developments regarding the\nframework are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 11:54:21 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Polceanu", "Mihai", ""], ["Buche", "C\u00e9dric", ""]]}, {"id": "1405.5066", "submitter": "Erik Cuevas E", "authors": "Erik Cuevas, Alonso Echavarria and Marte A. Ramirez-Ortegon", "title": "An optimization algorithm inspired by the States of Matter that improves\n  the balance between exploration and exploitation", "comments": "22 pages", "journal-ref": "Applied Intelligence, 40(2) , (2014), 256-272", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of an Evolutionary Algorithm (EA) to find a global optimal\nsolution depends on its capacity to find a good rate between exploitation of\nfound so far elements and exploration of the search space. Inspired by natural\nphenomena, researchers have developed many successful evolutionary algorithms\nwhich, at original versions, define operators that mimic the way nature solves\ncomplex problems, with no actual consideration of the exploration/exploitation\nbalance. In this paper, a novel nature-inspired algorithm called the States of\nMatter Search (SMS) is introduced. The SMS algorithm is based on the simulation\nof the states of matter phenomenon. In SMS, individuals emulate molecules which\ninteract to each other by using evolutionary operations which are based on the\nphysical principles of the thermal-energy motion mechanism. The algorithm is\ndevised by considering each state of matter at one different\nexploration/exploitation ratio. The evolutionary process is divided into three\nphases which emulate the three states of matter: gas, liquid and solid. In each\nstate, molecules (individuals) exhibit different movement capacities. Beginning\nfrom the gas state (pure exploration), the algorithm modifies the intensities\nof exploration and exploitation until the solid state (pure exploitation) is\nreached. As a result, the approach can substantially improve the balance\nbetween exploration/exploitation, yet preserving the good search capabilities\nof an evolutionary approach.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 13:03:18 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Cuevas", "Erik", ""], ["Echavarria", "Alonso", ""], ["Ramirez-Ortegon", "Marte A.", ""]]}, {"id": "1405.5156", "submitter": "Liping Liu", "authors": "Li-Ping Liu, Daniel Sheldon, Thomas G. Dietterich", "title": "Gaussian Approximation of Collective Graphical Models", "comments": "Accepted by ICML 2014. 10 page version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Collective Graphical Model (CGM) models a population of independent and\nidentically distributed individuals when only collective statistics (i.e.,\ncounts of individuals) are observed. Exact inference in CGMs is intractable,\nand previous work has explored Markov Chain Monte Carlo (MCMC) and MAP\napproximations for learning and inference. This paper studies Gaussian\napproximations to the CGM. As the population grows large, we show that the CGM\ndistribution converges to a multivariate Gaussian distribution (GCGM) that\nmaintains the conditional independence properties of the original CGM. If the\nobservations are exact marginals of the CGM or marginals that are corrupted by\nGaussian noise, inference in the GCGM approximation can be computed efficiently\nin closed form. If the observations follow a different noise model (e.g.,\nPoisson), then expectation propagation provides efficient and accurate\napproximate inference. The accuracy and speed of GCGM inference is compared to\nthe MCMC and MAP methods on a simulated bird migration problem. The GCGM\nmatches or exceeds the accuracy of the MAP method while being significantly\nfaster.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 17:12:56 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Liu", "Li-Ping", ""], ["Sheldon", "Daniel", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "1405.5172", "submitter": "Erik Cuevas E", "authors": "Erik Cuevas, Diego Oliva, Daniel Zaldivar, Marco Perez and Gonzalo\n  Pajares", "title": "Opposition Based ElectromagnetismLike for Global Optimization", "comments": "27 Pages", "journal-ref": "International Journal of Innovative Computing, Information and\n  Control, 8 (12) , (2012), pp. 8181-8198", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electromagnetismlike Optimization (EMO) is a global optimization algorithm,\nparticularly well suited to solve problems featuring nonlinear and multimodal\ncost functions. EMO employs searcher agents that emulate a population of\ncharged particles which interact to each other according to electromagnetisms\nlaws of attraction and repulsion. However, EMO usually requires a large number\nof iterations for a local search procedure; any reduction or cancelling over\nsuch number, critically perturb other issues such as convergence, exploration,\npopulation diversity and accuracy. This paper presents an enhanced EMO\nalgorithm called OBEMO, which employs the Opposition-Based Learning (OBL)\napproach to accelerate the global convergence speed. OBL is a machine\nintelligence strategy which considers the current candidate solution and its\nopposite value at the same time, achieving a faster exploration of the search\nspace. The proposed OBEMO method significantly reduces the required\ncomputational effort yet avoiding any detriment to the good search capabilities\nof the original EMO algorithm. Experiments are conducted over a comprehensive\nset of benchmark functions, showing that OBEMO obtains promising performance\nfor most of the discussed test problems.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 17:52:57 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Cuevas", "Erik", ""], ["Oliva", "Diego", ""], ["Zaldivar", "Daniel", ""], ["Perez", "Marco", ""], ["Pajares", "Gonzalo", ""]]}, {"id": "1405.5208", "submitter": "Alexander M. Rush", "authors": "Alexander M. Rush, Michael Collins", "title": "A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference\n  in Natural Language Processing", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  305-362, 2012", "doi": "10.1613/jair.3680", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual decomposition, and more generally Lagrangian relaxation, is a classical\nmethod for combinatorial optimization; it has recently been applied to several\ninference problems in natural language processing (NLP). This tutorial gives an\noverview of the technique. We describe example algorithms, describe formal\nguarantees for the method, and describe practical issues in implementing the\nalgorithms. While our examples are predominantly drawn from the NLP literature,\nthe material should be of general relevance to inference problems in machine\nlearning. A central theme of this tutorial is that Lagrangian relaxation is\nnaturally applied in conjunction with a broad class of combinatorial\nalgorithms, allowing inference in models that go significantly beyond previous\nwork on Lagrangian relaxation for inference in graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:50:15 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Rush", "Alexander M.", ""], ["Collins", "Michael", ""]]}, {"id": "1405.5345", "submitter": "Rapha\\\"el Lallement", "authors": "Rapha\\\"el Lallement and Lavindra de Silva and Rachid Alami", "title": "HATP: An HTN Planner for Robotics", "comments": "2nd ICAPS Workshop on Planning and Robotics, PlanRob 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Hierarchical Task Network (HTN) planning is a popular approach that cuts down\non the classical planning search space by relying on a given hierarchical\nlibrary of domain control knowledge. This provides an intuitive methodology for\nspecifying high-level instructions on how robots and agents should perform\ntasks, while also giving the planner enough flexibility to choose the\nlower-level steps and their ordering. In this paper we present the HATP\n(Hierarchical Agent-based Task Planner) planning framework which extends the\ntraditional HTN planning domain representation and semantics by making them\nmore suitable for roboticists, and treating agents as \"first class\" entities in\nthe language. The former is achieved by allowing \"social rules\" to be defined\nwhich specify what behaviour is acceptable/unacceptable by the agents/robots in\nthe domain, and interleaving planning with geometric reasoning in order to\nvalidate online -with respect to a detailed geometric 3D world- the human/robot\nactions currently being pursued by HATP.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 09:32:15 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 13:53:15 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Lallement", "Rapha\u00ebl", ""], ["de Silva", "Lavindra", ""], ["Alami", "Rachid", ""]]}, {"id": "1405.5358", "submitter": "Anna Harutyunyan", "authors": "Anna Harutyunyan and Tim Brys and Peter Vrancx and Ann Nowe", "title": "Off-Policy Shaping Ensembles in Reinforcement Learning", "comments": "Full version of the paper to appear in Proc. ECAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances of gradient temporal-difference methods allow to learn\noff-policy multiple value functions in parallel with- out sacrificing\nconvergence guarantees or computational efficiency. This opens up new\npossibilities for sound ensemble techniques in reinforcement learning. In this\nwork we propose learning an ensemble of policies related through\npotential-based shaping rewards. The ensemble induces a combination policy by\nusing a voting mechanism on its components. Learning happens in real time, and\nwe empirically show the combination policy to outperform the individual\npolicies of the ensemble.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 10:20:15 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Harutyunyan", "Anna", ""], ["Brys", "Tim", ""], ["Vrancx", "Peter", ""], ["Nowe", "Ann", ""]]}, {"id": "1405.5443", "submitter": "Marcello Balduccini", "authors": "Marcello Balduccini, William C. Regli, Duc N. Nguyen", "title": "Towards an ASP-Based Architecture for Autonomous UAVs in Dynamic\n  Environments (Extended Abstract)", "comments": "To appear in Theory and Practice of Logic Programming (TPLP). arXiv\n  admin note: substantial text overlap with arXiv:1405.1124", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional AI reasoning techniques have been used successfully in many\ndomains, including logistics, scheduling and game playing. This paper is part\nof a project aimed at investigating how such techniques can be extended to\ncoordinate teams of unmanned aerial vehicles (UAVs) in dynamic environments.\nSpecifically challenging are real-world environments where UAVs and other\nnetwork-enabled devices must communicate to coordinate -- and communication\nactions are neither reliable nor free. Such network-centric environments are\ncommon in military, public safety and commercial applications, yet most\nresearch (even multi-agent planning) usually takes communications among\ndistributed agents as a given. We address this challenge by developing an agent\narchitecture and reasoning algorithms based on Answer Set Programming (ASP).\nAlthough ASP has been used successfully in a number of applications, to the\nbest of our knowledge this is the first practical application of a complete\nASP-based agent architecture. It is also the first practical application of ASP\ninvolving a combination of centralized reasoning, decentralized reasoning,\nexecution monitoring, and reasoning about network communications.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 14:55:12 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Balduccini", "Marcello", ""], ["Regli", "William C.", ""], ["Nguyen", "Duc N.", ""]]}, {"id": "1405.5459", "submitter": "Adi Makmal", "authors": "Alexey A. Melnikov, Adi Makmal, and Hans J. Briegel", "title": "Projective simulation applied to the grid-world and the mountain-car\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the model of projective simulation (PS) which is a novel approach to\nartificial intelligence (AI). Recently it was shown that the PS agent performs\nwell in a number of simple task environments, also when compared to standard\nmodels of reinforcement learning (RL). In this paper we study the performance\nof the PS agent further in more complicated scenarios. To that end we chose two\nwell-studied benchmarking problems, namely the \"grid-world\" and the\n\"mountain-car\" problem, which challenge the model with large and continuous\ninput space. We compare the performance of the PS agent model with those of\nexisting models and show that the PS agent exhibits competitive performance\nalso in such scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 15:51:18 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Melnikov", "Alexey A.", ""], ["Makmal", "Adi", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1405.5498", "submitter": "Vishal Gupta", "authors": "Dimitris Bertsimas, J. Daniel Griffith, Vishal Gupta, Mykel J.\n  Kochenderfer, Velibor V. Mi\\v{s}i\\'c and Robert Moss", "title": "A Comparison of Monte Carlo Tree Search and Mathematical Optimization\n  for Large Scale Dynamic Resource Allocation", "comments": "37 pages, 13 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic resource allocation (DRA) problems are an important class of dynamic\nstochastic optimization problems that arise in a variety of important\nreal-world applications. DRA problems are notoriously difficult to solve to\noptimality since they frequently combine stochastic elements with intractably\nlarge state and action spaces. Although the artificial intelligence and\noperations research communities have independently proposed two successful\nframeworks for solving dynamic stochastic optimization problems---Monte Carlo\ntree search (MCTS) and mathematical optimization (MO), respectively---the\nrelative merits of these two approaches are not well understood. In this paper,\nwe adapt both MCTS and MO to a problem inspired by tactical wildfire and\nmanagement and undertake an extensive computational study comparing the two\nmethods on large scale instances in terms of both the state and the action\nspaces. We show that both methods are able to greatly improve on a baseline,\nproblem-specific heuristic. On smaller instances, the MCTS and MO approaches\nperform comparably, but the MO approach outperforms MCTS as the size of the\nproblem increases for a fixed computational budget.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 18:01:20 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Griffith", "J. Daniel", ""], ["Gupta", "Vishal", ""], ["Kochenderfer", "Mykel J.", ""], ["Mi\u0161i\u0107", "Velibor V.", ""], ["Moss", "Robert", ""]]}, {"id": "1405.5643", "submitter": "Martin Josef Geiger", "authors": "Sandra Huber, Martin Josef Geiger, Marc Sevaux", "title": "Interactive Reference Point-Based Guided Local Search for the\n  Bi-objective Inventory Routing Problem", "comments": null, "journal-ref": "Proceedings of the 10th Metaheuristics International Conference\n  MIC 2013, August 5-8, 2013, Singapore, Pages 152-161", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eliciting preferences of a decision maker is a key factor to successfully\ncombine search and decision making in an interactive method. Therefore, the\nprogressively integration and simulation of the decision maker is a main\nconcern in an application. We contribute in this direction by proposing an\ninteractive method based on a reference point-based guided local search to the\nbi-objective Inventory Routing Problem. A local search metaheuristic, working\non the delivery intervals, and the Clarke & Wright savings heuristic is\nemployed for the subsequently obtained Vehicle Routing Problem. To elicit\npreferences, the decision maker selects a reference point to guide the search\nin interesting subregions. Additionally, the reference point is used as a\nreservation point to discard solutions outside the cone, introduced as a\nconvergence criterion. Computational results of the reference point-based\nguided local search are reported and analyzed on benchmark data in order to\nshow the applicability of the approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 07:23:38 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Huber", "Sandra", ""], ["Geiger", "Martin Josef", ""], ["Sevaux", "Marc", ""]]}, {"id": "1405.5646", "submitter": "Christian Blum", "authors": "Christian Blum and Jos\\'e A. Lozano and Pedro Pinacho Davidson", "title": "Mathematical Programming Strategies for Solving the Minimum Common\n  String Partition Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum common string partition problem is an NP-hard combinatorial\noptimization problem with applications in computational biology. In this work\nwe propose the first integer linear programming model for solving this problem.\nMoreover, on the basis of the integer linear programming model we develop a\ndeterministic 2-phase heuristic which is applicable to larger problem\ninstances. The results show that provenly optimal solutions can be obtained for\nproblem instances of small and medium size from the literature by solving the\nproposed integer linear programming model with CPLEX. Furthermore, new\nbest-known solutions are obtained for all considered problem instances from the\nliterature. Concerning the heuristic, we were able to show that it outperforms\nheuristic competitors from the related literature.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 07:37:56 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Blum", "Christian", ""], ["Lozano", "Jos\u00e9 A.", ""], ["Davidson", "Pedro Pinacho", ""]]}, {"id": "1405.6043", "submitter": "Florent Capelli", "authors": "Johann Brault-Baron, Florent Capelli, Stefan Mengel", "title": "Understanding model counting for $\\beta$-acyclic CNF-formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the knowledge about so-called structural restrictions of\n$\\mathrm{\\#SAT}$ by giving a polynomial time algorithm for $\\beta$-acyclic\n$\\mathrm{\\#SAT}$. In contrast to previous algorithms in the area, our algorithm\ndoes not proceed by dynamic programming but works along an elimination order,\nsolving a weighted version of constraint satisfaction. Moreover, we give\nevidence that this deviation from more standard algorithm is not a coincidence,\nbut that there is likely no dynamic programming algorithm of the usual style\nfor $\\beta$-acyclic $\\mathrm{\\#SAT}$.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 12:38:18 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Brault-Baron", "Johann", ""], ["Capelli", "Florent", ""], ["Mengel", "Stefan", ""]]}, {"id": "1405.6142", "submitter": "Phil Maguire", "authors": "Phil Maguire, Philippe Moser, Rebecca Maguire, Mark Keane", "title": "A Computational Theory of Subjective Probability", "comments": "Maguire, P., Moser, P. Maguire, R. & Keane, M.T. (2013) \"A\n  computational theory of subjective probability.\" In M. Knauff, M. Pauen, N.\n  Sebanz, & I. Wachsmuth (Eds.), Proceedings of the 35th Annual Conference of\n  the Cognitive Science Society (pp. 960-965). Austin, TX: Cognitive Science\n  Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we demonstrate how algorithmic probability theory is applied\nto situations that involve uncertainty. When people are unsure of their model\nof reality, then the outcome they observe will cause them to update their\nbeliefs. We argue that classical probability cannot be applied in such cases,\nand that subjective probability must instead be used. In Experiment 1 we show\nthat, when judging the probability of lottery number sequences, people apply\nsubjective rather than classical probability. In Experiment 2 we examine the\nconjunction fallacy and demonstrate that the materials used by Tversky and\nKahneman (1983) involve model uncertainty. We then provide a formal\nmathematical proof that, for every uncertain model, there exists a conjunction\nof outcomes which is more subjectively probable than either of its constituents\nin isolation.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 13:15:32 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Maguire", "Phil", ""], ["Moser", "Philippe", ""], ["Maguire", "Rebecca", ""], ["Keane", "Mark", ""]]}, {"id": "1405.6164", "submitter": "Ion Androutsopoulos", "authors": "Ion Androutsopoulos, Gerasimos Lampouras, Dimitrios Galanis", "title": "Generating Natural Language Descriptions from OWL Ontologies: the\n  NaturalOWL System", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 48, pages\n  671-715, 2013", "doi": "10.1613/jair.4017", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NaturalOWL, a natural language generation system that produces\ntexts describing individuals or classes of OWL ontologies. Unlike simpler OWL\nverbalizers, which typically express a single axiom at a time in controlled,\noften not entirely fluent natural language primarily for the benefit of domain\nexperts, we aim to generate fluent and coherent multi-sentence texts for\nend-users. With a system like NaturalOWL, one can publish information in OWL on\nthe Web, along with automatically produced corresponding texts in multiple\nlanguages, making the information accessible not only to computer programs and\ndomain experts, but also end-users. We discuss the processing stages of\nNaturalOWL, the optional domain-dependent linguistic resources that the system\ncan use at each stage, and why they are useful. We also present trials showing\nthat when the domain-dependent llinguistic resources are available, NaturalOWL\nproduces significantly better texts compared to a simpler verbalizer, and that\nthe resources can be created with relatively light effort.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 02:47:37 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Androutsopoulos", "Ion", ""], ["Lampouras", "Gerasimos", ""], ["Galanis", "Dimitrios", ""]]}, {"id": "1405.6341", "submitter": "Stefanos Nikolaidis", "authors": "Stefanos Nikolaidis, Keren Gu, Ramya Ramakrishnan, and Julie Shah", "title": "Efficient Model Learning for Human-Robot Collaborative Tasks", "comments": null, "journal-ref": "Proceedings of the Tenth Annual ACM/IEEE International Conference\n  on Human-Robot Interaction (HRI 2015)", "doi": "10.1145/2696454.2696455", "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for learning human user models from joint-action\ndemonstrations that enables the robot to compute a robust policy for a\ncollaborative task with a human. The learning takes place completely\nautomatically, without any human intervention. First, we describe the\nclustering of demonstrated action sequences into different human types using an\nunsupervised learning algorithm. These demonstrated sequences are also used by\nthe robot to learn a reward function that is representative for each type,\nthrough the employment of an inverse reinforcement learning algorithm. The\nlearned model is then used as part of a Mixed Observability Markov Decision\nProcess formulation, wherein the human type is a partially observable variable.\nWith this framework, we can infer, either offline or online, the human type of\na new user that was not included in the training set, and can compute a policy\nfor the robot that will be aligned to the preference of this new user and will\nbe robust to deviations of the human actions from prior demonstrations. Finally\nwe validate the approach using data collected in human subject experiments, and\nconduct proof-of-concept demonstrations in which a person performs a\ncollaborative task with a small industrial robot.\n", "versions": [{"version": "v1", "created": "Sat, 24 May 2014 20:44:26 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Nikolaidis", "Stefanos", ""], ["Gu", "Keren", ""], ["Ramakrishnan", "Ramya", ""], ["Shah", "Julie", ""]]}, {"id": "1405.6369", "submitter": "Ben Ruijl", "authors": "Ben Ruijl, Jos Vermaseren, Aske Plaat, Jaap van den Herik", "title": "HEPGAME and the Simplification of Expressions", "comments": "Keynote at the 11th International Workshop on Boolean Problems,\n  Freiberg Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in high energy physics have created the need to increase\ncomputational capacity. Project HEPGAME was composed to address this challenge.\nOne of the issues is that numerical integration of expressions of current\ninterest have millions of terms and takes weeks to compute. We have\ninvestigated ways to simplify these expressions, using Horner schemes and\ncommon subexpression elimination. Our approach applies MCTS, a search procedure\nthat has been successful in AI. We use it to find near-optimal Horner schemes.\nAlthough MCTS finds better solutions, this approach gives rise to two further\nchallenges. (1) MCTS (with UCT) introduces a constant, $C_p$ that governs the\nbalance between exploration and exploitation. This constant has to be tuned\nmanually. (2) There should be more guided exploration at the bottom of the\ntree, since the current approach reduces the quality of the solution towards\nthe end of the expression. We investigate NMCS (Nested Monte Carlo Search) to\naddress both issues, but find that NMCS is computationally unfeasible for our\nproblem. Then, we modify the MCTS formula by introducing a dynamic\nexploration-exploitation parameter $T$ that decreases linearly with the\niteration number. Consequently, we provide a performance analysis. We observe\nthat a variable $C_p$ solves our domain: it yields more exploration at the\nbottom and as a result the tuning problem has been simplified. The region in\n$C_p$ for which good values are found is increased by more than a tenfold. This\nresult encourages us to continue our research to solve other prominent problems\nin High Energy Physics.\n", "versions": [{"version": "v1", "created": "Sun, 25 May 2014 10:13:50 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Ruijl", "Ben", ""], ["Vermaseren", "Jos", ""], ["Plaat", "Aske", ""], ["Herik", "Jaap van den", ""]]}, {"id": "1405.6509", "submitter": "Edmond Awad", "authors": "Edmond Awad, Richard Booth, Fernando Tohme, Iyad Rahwan", "title": "Judgment Aggregation in Multi-Agent Argumentation", "comments": null, "journal-ref": "J Logic Computation (2017) 27 (1): 227-259", "doi": "10.1093/logcom/exv055", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of conflicting arguments, there can exist multiple plausible\nopinions about which arguments should be accepted, rejected, or deemed\nundecided. We study the problem of how multiple such judgments can be\naggregated. We define the problem by adapting various classical\nsocial-choice-theoretic properties for the argumentation domain. We show that\nwhile argument-wise plurality voting satisfies many properties, it fails to\nguarantee the collective rationality of the outcome, and struggles with ties.\nWe then present more general results, proving multiple impossibility results on\nthe existence of any good aggregation operator. After characterising the\nsufficient and necessary conditions for satisfying collective rationality, we\nstudy whether restricting the domain of argument-wise plurality voting to\nclassical semantics allows us to escape the impossibility result. We close by\nlisting graph-theoretic restrictions under which argument-wise plurality rule\ndoes produce collectively rational outcomes. In addition to identifying\nfundamental barriers to collective argument evaluation, our results open up the\ndoor for a new research agenda for the argumentation and computational social\nchoice communities.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 09:13:38 GMT"}, {"version": "v2", "created": "Thu, 19 Jun 2014 15:31:03 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2015 10:53:38 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Awad", "Edmond", ""], ["Booth", "Richard", ""], ["Tohme", "Fernando", ""], ["Rahwan", "Iyad", ""]]}, {"id": "1405.6662", "submitter": "Suma Dawn", "authors": "Suma Dawn, Vikas Saxena, Bhudev Sharma", "title": "Cognitive-mapping and contextual pyramid based Digital Elevation Model\n  Registration and its effective storage using fractal based compression", "comments": "17 pages, 8 tables, and 3 figures; IJCSI International Journal of\n  Computer Science Issues, Vol. 10, Issue 3, No 1, May 2013, ISSN (Print):\n  1694-0814 | ISSN (Online): 1694-0784 (www.IJCSI.org)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Elevation models (DEM) are images having terrain information embedded\ninto them. Using cognitive mapping concepts for DEM registration, has evolved\nfrom this basic idea of using the mapping between the space to objects and\ndefining their relationships to form the basic landmarks that need to be\nmarked, stored and manipulated in and about the environment or other candidate\nenvironments, namely, in our case, the DEMs. The progressive two-level\nencapsulation of methods of geo-spatial cognition includes landmark knowledge\nand layout knowledge and can be useful for DEM registration. Space-based\napproach, that emphasizes on explicit extent of the environment under\nconsideration, and object-based approach, that emphasizes on the relationships\nbetween objects in the local environment being the two paradigms of cognitive\nmapping can be methodically integrated in this three-architecture for DEM\nregistration. Initially, P-model based segmentation is performed followed by\nlandmark formation for contextual mapping that uses contextual pyramid\nformation. Apart from landmarks being used for registration key-point finding,\nEuclidean distance based deformation calculation has been used for\ntransformation and change detection. Landmarks have been categorized to belong\nto either being flat-plain areas without much variation in the land heights;\npeaks that can be found when there is gradual increase in height as compared to\nthe flat areas; valleys, marked with gradual decrease in the height seen in\nDEM; and finally, ripple areas with very shallow crests and nadirs. Fractal\nbased compression was used for storage of co-registered DEMs. This method may\nfurther be extended for DEM-topographic map and DEM-to-remote sensed image\nregistration. Experimental results further cement the fact that DEM\nregistration may be effectively done using the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 05:59:01 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Dawn", "Suma", ""], ["Saxena", "Vikas", ""], ["Sharma", "Bhudev", ""]]}, {"id": "1405.7076", "submitter": "Vilem Vychodil", "authors": "Vilem Vychodil", "title": "On minimal sets of graded attribute implications", "comments": null, "journal-ref": "Information Sciences 294 (2015), 478-488", "doi": "10.1016/j.ins.2014.09.059", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the structure of non-redundant and minimal sets consisting of\ngraded if-then rules. The rules serve as graded attribute implications in\nobject-attribute incidence data and as similarity-based functional dependencies\nin a similarity-based generalization of the relational model of data. Based on\nour observations, we derive a polynomial-time algorithm which transforms a\ngiven finite set of rules into an equivalent one which has the least size in\nterms of the number of rules.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 22:00:35 GMT"}, {"version": "v2", "created": "Wed, 20 Aug 2014 19:07:05 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Vychodil", "Vilem", ""]]}, {"id": "1405.7192", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "The PeerRank Method for Peer Assessment", "comments": "To appear in Proc. of ECAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the PeerRank method for peer assessment. This constructs a grade\nfor an agent based on the grades proposed by the agents evaluating the agent.\nSince the grade of an agent is a measure of their ability to grade correctly,\nthe PeerRank method weights grades by the grades of the grading agent. The\nPeerRank method also provides an incentive for agents to grade correctly. As\nthe grades of an agent depend on the grades of the grading agents, and as these\ngrades themselves depend on the grades of other agents, we define the PeerRank\nmethod by a fixed point equation similar to the PageRank method for ranking\nweb-pages. We identify some formal properties of the PeerRank method (for\nexample, it satisfies axioms of unanimity, no dummy, no discrimination and\nsymmetry), discuss some examples, compare with related work and evaluate the\nperformance on some synthetic data. Our results show considerable promise,\nreducing the error in grade predictions by a factor of 2 or more in many cases\nover the natural baseline of averaging peer grades.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 10:51:57 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "1405.7253", "submitter": "Florian Lonsing", "authors": "Uwe Egly and Martin Kronegger and Florian Lonsing and Andreas Pfandler", "title": "Conformant Planning as a Case Study of Incremental QBF Solving", "comments": "added reference to extended journal article; revision (camera-ready,\n  to appear in the proceedings of AISC 2014, volume 8884 of LNAI, Springer)", "journal-ref": null, "doi": "10.1007/978-3-319-13770-4_11", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider planning with uncertainty in the initial state as a case study of\nincremental quantified Boolean formula (QBF) solving. We report on experiments\nwith a workflow to incrementally encode a planning instance into a sequence of\nQBFs. To solve this sequence of incrementally constructed QBFs, we use our\ngeneral-purpose incremental QBF solver DepQBF. Since the generated QBFs have\nmany clauses and variables in common, our approach avoids redundancy both in\nthe encoding phase and in the solving phase. Experimental results show that\nincremental QBF solving outperforms non-incremental QBF solving. Our results\nare the first empirical study of incremental QBF solving in the context of\nplanning and motivate its use in other application domains.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 14:23:50 GMT"}, {"version": "v2", "created": "Wed, 8 Oct 2014 12:04:01 GMT"}, {"version": "v3", "created": "Mon, 4 Apr 2016 11:06:57 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Egly", "Uwe", ""], ["Kronegger", "Martin", ""], ["Lonsing", "Florian", ""], ["Pfandler", "Andreas", ""]]}, {"id": "1405.7295", "submitter": "Peter Nov\\'ak", "authors": "Peter Nov\\'ak and Cees Witteveen", "title": "On the cost-complexity of multi-context systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-context systems provide a powerful framework for modelling\ninformation-aggregation systems featuring heterogeneous reasoning components.\nTheir execution can, however, incur non-negligible cost. Here, we focus on\ncost-complexity of such systems. To that end, we introduce cost-aware\nmulti-context systems, an extension of non-monotonic multi-context systems\nframework taking into account costs incurred by execution of semantic operators\nof the individual contexts. We formulate the notion of cost-complexity for\nconsistency and reasoning problems in MCSs. Subsequently, we provide a series\nof results related to gradually more and more constrained classes of MCSs and\nfinally introduce an incremental cost-reducing algorithm solving the reasoning\nproblem for definite MCSs.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 16:13:53 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Nov\u00e1k", "Peter", ""], ["Witteveen", "Cees", ""]]}, {"id": "1405.7567", "submitter": "Michael  Gr. Voskoglou Prof. Dr.", "authors": "Michael Gr. Voskoglou, Abdel-Badeeh M. Salem", "title": "Analogy-Based and Case-Based Reasoning: Two sides of the same coin", "comments": "47 pages, 2 figures, 1 table, 124 references", "journal-ref": "International Journal of Applications of Fuzzy Sets and Artificial\n  Intelligence (IJAFSAI), Vol. 4, 5-51, 2014", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analogy-Based (or Analogical) and Case-Based Reasoning (ABR and CBR) are two\nsimilar problem solving processes based on the adaptation of the solution of\npast problems for use with a new analogous problem. In this paper we review\nthese two processes and we give some real world examples with emphasis to the\nfield of Medicine, where one can find some of the most common and useful CBR\napplications. We also underline the differences between CBR and the classical\nrule-induction algorithms, we discuss the criticism for CBR methods and we\nfocus on the future trends of research in the area of CBR.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 14:38:52 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Voskoglou", "Michael Gr.", ""], ["Salem", "Abdel-Badeeh M.", ""]]}, {"id": "1405.7714", "submitter": "Toby Walsh", "authors": "Nina Narodytska and Toby Walsh", "title": "The Computational Impact of Partial Votes on Strategic Voting", "comments": "To appear in Proceedings of ECAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real world elections, agents are not required to rank all candidates.\nWe study three of the most common methods used to modify voting rules to deal\nwith such partial votes. These methods modify scoring rules (like the Borda\ncount), elimination style rules (like single transferable vote) and rules based\non the tournament graph (like Copeland) respectively. We argue that with an\nelimination style voting rule like single transferable vote, partial voting\ndoes not change the situations where strategic voting is possible. However,\nwith scoring rules and rules based on the tournament graph, partial voting can\nincrease the situations where strategic voting is possible. As a consequence,\nthe computational complexity of computing a strategic vote can change. For\nexample, with Borda count, the complexity of computing a strategic vote can\ndecrease or stay the same depending on how we score partial votes.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 12:13:52 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Narodytska", "Nina", ""], ["Walsh", "Toby", ""]]}, {"id": "1405.7716", "submitter": "Sukru Burc Eryilmaz", "authors": "S. Burc Eryilmaz, Duygu Kuzum, Rakesh G. D. Jeyasingh, SangBum Kim,\n  Matthew BrightSky, Chung Lam and H.-S. Philip Wong", "title": "Experimental Demonstration of Array-level Learning with Phase Change\n  Synaptic Devices", "comments": "IEDM 2013", "journal-ref": "Electron Devices Meeting (IEDM), 2013 IEEE International,\n  pp.25.5.1,25.5.4, 9-11 Dec. 2013", "doi": "10.1109/IEDM.2013.6724691", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational performance of the biological brain has long attracted\nsignificant interest and has led to inspirations in operating principles,\nalgorithms, and architectures for computing and signal processing. In this\nwork, we focus on hardware implementation of brain-like learning in a\nbrain-inspired architecture. We demonstrate, in hardware, that 2-D crossbar\narrays of phase change synaptic devices can achieve associative learning and\nperform pattern recognition. Device and array-level studies using an\nexperimental 10x10 array of phase change synaptic devices have shown that\npattern recognition is robust against synaptic resistance variations and large\nvariations can be tolerated by increasing the number of training iterations.\nOur measurements show that increase in initial variation from 9 % to 60 %\ncauses required training iterations to increase from 1 to 11.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 20:22:00 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 05:45:54 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Eryilmaz", "S. Burc", ""], ["Kuzum", "Duygu", ""], ["Jeyasingh", "Rakesh G. D.", ""], ["Kim", "SangBum", ""], ["BrightSky", "Matthew", ""], ["Lam", "Chung", ""], ["Wong", "H. -S. Philip", ""]]}, {"id": "1405.7752", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Zheng Wen, Azin Ashkan, and Michal Valko", "title": "Learning to Act Greedily: Polymatroid Semi-Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important optimization problems, such as the minimum spanning tree and\nminimum-cost flow, can be solved optimally by a greedy method. In this work, we\nstudy a learning variant of these problems, where the model of the problem is\nunknown and has to be learned by interacting repeatedly with the environment in\nthe bandit setting. We formalize our learning problem quite generally, as\nlearning how to maximize an unknown modular function on a known polymatroid. We\npropose a computationally efficient algorithm for solving our problem and bound\nits expected cumulative regret. Our gap-dependent upper bound is tight up to a\nconstant and our gap-free upper bound is tight up to polylogarithmic factors.\nFinally, we evaluate our method on three problems and demonstrate that it is\npractical.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 00:35:34 GMT"}, {"version": "v2", "created": "Fri, 6 Jun 2014 21:26:40 GMT"}, {"version": "v3", "created": "Fri, 21 Nov 2014 10:13:34 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Ashkan", "Azin", ""], ["Valko", "Michal", ""]]}, {"id": "1405.7868", "submitter": "Priya Bajaj", "authors": "Priya Bajaj and Supriya Raheja", "title": "A Vague Improved Markov Model Approach for Web Page Prediction", "comments": "8 pages, 4 figures, 1 table, International Journal of Computer\n  Science & Engineering Survey (IJCSES) Vol.5, No.2, April 2014", "journal-ref": null, "doi": "10.5121/ijcses.2014.5205", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today most of the information in all areas is available over the web. It\nincreases the web utilization as well as attracts the interest of researchers\nto improve the effectiveness of web access and web utilization. As the number\nof web clients gets increased, the bandwidth sharing is performed that\ndecreases the web access efficiency. Web page prefetching improves the\neffectiveness of web access by availing the next required web page before the\nuser demand. It is an intelligent predictive mining that analyze the user web\naccess history and predict the next page. In this work, vague improved markov\nmodel is presented to perform the prediction. In this work, vague rules are\nsuggested to perform the pruning at different levels of markov model. Once the\nprediction table is generated, the association mining will be implemented to\nidentify the most effective next page. In this paper, an integrated model is\nsuggested to improve the prediction accuracy and effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 07:52:20 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Bajaj", "Priya", ""], ["Raheja", "Supriya", ""]]}, {"id": "1405.7869", "submitter": "Priya Bajaj", "authors": "Priya Bajaj and Supriya Raheja", "title": "Integrating Vague Association Mining with Markov Model", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing demand of world wide web raises the need of predicting the\nuser's web page request.The most widely used approach to predict the web pages\nis the pattern discovery process of Web usage mining. This process involves\ninevitability of many techniques like Markov model, association rules and\nclustering. Fuzzy theory with different techniques has been introduced for the\nbetter results. Our focus is on Markov models. This paper is introducing the\nvague Rules with Markov models for more accuracy using the vague set theory.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 07:44:39 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Bajaj", "Priya", ""], ["Raheja", "Supriya", ""]]}, {"id": "1405.7908", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Semantic Composition and Decomposition: From Recognition to Generation", "comments": "National Research Council Canada - Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic composition is the task of understanding the meaning of text by\ncomposing the meanings of the individual words in the text. Semantic\ndecomposition is the task of understanding the meaning of an individual word by\ndecomposing it into various aspects (factors, constituents, components) that\nare latent in the meaning of the word. We take a distributional approach to\nsemantics, in which a word is represented by a context vector. Much recent work\nhas considered the problem of recognizing compositions and decompositions, but\nwe tackle the more difficult generation problem. For simplicity, we focus on\nnoun-modifier bigrams and noun unigrams. A test for semantic composition is,\ngiven context vectors for the noun and modifier in a noun-modifier bigram (\"red\nsalmon\"), generate a noun unigram that is synonymous with the given bigram\n(\"sockeye\"). A test for semantic decomposition is, given a context vector for a\nnoun unigram (\"snifter\"), generate a noun-modifier bigram that is synonymous\nwith the given unigram (\"brandy glass\"). With a vocabulary of about 73,000\nunigrams from WordNet, there are 73,000 candidate unigram compositions for a\nbigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a\nunigram. We generate ranked lists of potential solutions in two passes. A fast\nunsupervised learning algorithm generates an initial list of candidates and\nthen a slower supervised learning algorithm refines the list. We evaluate the\ncandidate solutions by comparing them to WordNet synonym sets. For\ndecomposition (unigram to bigram), the top 100 most highly ranked bigrams\ninclude a WordNet synonym of the given unigram 50.7% of the time. For\ncomposition (bigram to unigram), the top 100 most highly ranked unigrams\ninclude a WordNet synonym of the given bigram 77.8% of the time.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 16:36:07 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1405.7944", "submitter": "Shruti Jadon", "authors": "Shruti Jadon, Anubhav Singhal, Suma Dawn", "title": "Military Simulator - A Case Study of Behaviour Tree and Unity based\n  architecture", "comments": "4 pages, 4 figures. International Journal of Computer Applications\n  @2014", "journal-ref": null, "doi": "10.5120/15350-3691", "report-no": "Volume 88 - Number 5", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how the combination of Behaviour Tree and Utility Based\nAI architecture can be used to design more realistic bots for Military\nSimulators. In this work, we have designed a mathematical model of a simulator\nsystem which in turn helps in analyzing the results and finding out the various\nspaces on which our favorable situation might exist, this is done\ngeometrically. In the mathematical model, we have explained the matrix\nformation and its significance followed up in dynamic programming approach we\nexplained the possible graph formation which will led improvisation of AI,\nlatter we explained the possible geometrical structure of the matrix operations\nand its impact on a particular decision, we also explained the conditions under\nwhich it tend to fail along with a possible solution in future works.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 18:22:59 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Jadon", "Shruti", ""], ["Singhal", "Anubhav", ""], ["Dawn", "Suma", ""]]}, {"id": "1405.7964", "submitter": "Faruk Karaaslan", "authors": "Faruk Karaaslan", "title": "Neutrosophic soft sets with applications in decision making", "comments": "arXiv admin note: text overlap with arXiv:1305.2724 by other authors", "journal-ref": "International Journal of Information Science and Intelligent\n  System, 4(2), 1-20, 2015", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We firstly present definitions and properties in study of Maji\n\\cite{maji-2013} on neutrosophic soft sets. We then give a few notes on his\nstudy. Next, based on \\c{C}a\\u{g}man \\cite{cagman-2014}, we redefine the notion\nof neutrosophic soft set and neutrosophic soft set operations to make more\nfunctional. By using these new definitions we construct a decision making\nmethod and a group decision making method which selects a set of optimum\nelements from the alternatives. We finally present examples which shows that\nthe methods can be successfully applied to many problems that contain\nuncertainties.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 19:32:36 GMT"}, {"version": "v2", "created": "Mon, 2 Jun 2014 21:44:43 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Karaaslan", "Faruk", ""]]}]