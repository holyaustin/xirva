[{"id": "1112.0508", "submitter": "Weiwei Cheng", "authors": "Weiwei Cheng, Eyke H\\\"ullermeier", "title": "Label Ranking with Abstention: Predicting Partial Orders by Thresholding\n  Probability Distributions (Extended Abstract)", "comments": "4 pages, 1 figure, appeared at NIPS 2011 Choice Models and Preference\n  Learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an extension of the setting of label ranking, in which the\nlearner is allowed to make predictions in the form of partial instead of total\norders. Predictions of that kind are interpreted as a partial abstention: If\nthe learner is not sufficiently certain regarding the relative order of two\nalternatives, it may abstain from this decision and instead declare these\nalternatives as being incomparable. We propose a new method for learning to\npredict partial orders that improves on an existing approach, both\ntheoretically and empirically. Our method is based on the idea of thresholding\nthe probabilities of pairwise preferences between labels as induced by a\npredicted (parameterized) probability distribution on the set of all rankings.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2011 17:09:43 GMT"}], "update_date": "2011-12-05", "authors_parsed": [["Cheng", "Weiwei", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "1112.0698", "submitter": "Theja Tulabandhula", "authors": "Theja Tulabandhula and Cynthia Rudin", "title": "Machine Learning with Operational Costs", "comments": "Current version: Final version appearing in JMLR 2013. v2: Many parts\n  have been rewritten including the introduction, Minor correction of Theorem\n  6. 38 pages. Previously: v1: 36 pages, 8 figures. Short version appears in\n  Proceedings of the International Symposium on Artificial Intelligence and\n  Mathematics, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a way to align statistical modeling with decision making.\nWe provide a method that propagates the uncertainty in predictive modeling to\nthe uncertainty in operational cost, where operational cost is the amount spent\nby the practitioner in solving the problem. The method allows us to explore the\nrange of operational costs associated with the set of reasonable statistical\nmodels, so as to provide a useful way for practitioners to understand\nuncertainty. To do this, the operational cost is cast as a regularization term\nin a learning algorithm's objective function, allowing either an optimistic or\npessimistic view of possible costs, depending on the regularization parameter.\nFrom another perspective, if we have prior knowledge about the operational\ncost, for instance that it should be low, this knowledge can help to restrict\nthe hypothesis space, and can help with generalization. We provide a\ntheoretical generalization bound for this scenario. We also show that learning\nwith operational costs is related to robust optimization.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2011 22:32:04 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2011 04:17:04 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2012 14:00:02 GMT"}, {"version": "v4", "created": "Wed, 19 Jun 2013 03:24:33 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Tulabandhula", "Theja", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1112.0791", "submitter": "Wolfgang Faber", "authors": "Wolfgang Faber, Miros{\\l}aw Truszczy\\'nski, Stefan Woltran", "title": "Strong Equivalence of Qualitative Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the framework of qualitative optimization problems (or, simply,\noptimization problems) to represent preference theories. The formalism uses\nseparate modules to describe the space of outcomes to be compared (the\ngenerator) and the preferences on outcomes (the selector). We consider two\ntypes of optimization problems. They differ in the way the generator, which we\nmodel by a propositional theory, is interpreted: by the standard propositional\nlogic semantics, and by the equilibrium-model (answer-set) semantics. Under the\nlatter interpretation of generators, optimization problems directly generalize\nanswer-set optimization programs proposed previously. We study strong\nequivalence of optimization problems, which guarantees their interchangeability\nwithin any larger context. We characterize several versions of strong\nequivalence obtained by restricting the class of optimization problems that can\nbe used as extensions and establish the complexity of associated reasoning\ntasks. Understanding strong equivalence is essential for modular representation\nof optimization problems and rewriting techniques to simplify them without\nchanging their inherent properties.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2011 20:06:29 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Faber", "Wolfgang", ""], ["Truszczy\u0144ski", "Miros\u0142aw", ""], ["Woltran", "Stefan", ""]]}, {"id": "1112.0922", "submitter": "J\\\"org P\\\"uhrer", "authors": "Johannes Oetsch, J\\\"org P\\\"uhrer, Hans Tompits", "title": "Extending Object-Oriented Languages by Declarative Specifications of\n  Complex Objects using Answer-Set Programming", "comments": "Submitted to the 34th International Conference on Software\n  Engineering (ICSE), New Ideas and Emerging Results (NIER) track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require complexly structured data objects. Developing new\nor adapting existing algorithmic solutions for creating such objects can be a\nnon-trivial and costly task if the considered objects are subject to different\napplication-specific constraints. Often, however, it is comparatively easy to\ndeclaratively describe the required objects. In this paper, we propose to use\nanswer-set programming (ASP)---a well-established declarative programming\nparadigm from the area of artificial intelligence---for instantiating objects\nin standard object-oriented programming languages. In particular, we extend\nJava with declarative specifications from which the required objects can be\nautomatically generated using available ASP solver technology.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 13:28:53 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2011 15:31:56 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Oetsch", "Johannes", ""], ["P\u00fchrer", "J\u00f6rg", ""], ["Tompits", "Hans", ""]]}, {"id": "1112.1217", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig and Christian J. Schuler", "title": "Entropy Search for Information-Efficient Global Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary global optimization algorithms are based on local measures of\nutility, rather than a probability measure over location and value of the\noptimum. They thus attempt to collect low function values, not to learn about\nthe optimum. The reason for the absence of probabilistic global optimizers is\nthat the corresponding inference problem is intractable in several ways. This\npaper develops desiderata for probabilistic optimization algorithms, then\npresents a concrete algorithm which addresses each of the computational\nintractabilities with a sequence of approximations and explicitly adresses the\ndecision problem of maximizing information gain from each evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 10:17:31 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Hennig", "Philipp", ""], ["Schuler", "Christian J.", ""]]}, {"id": "1112.1330", "submitter": "Claudius Gros", "authors": "Claudius Gros", "title": "Emotional control - conditio sine qua non for advanced artificial\n  intelligences?", "comments": "Proceedings of PT-AI 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans dispose of two intertwined information processing pathways, cognitive\ninformation processing via neural firing patterns and diffusive volume control\nvia neuromodulation. The cognitive information processing in the brain is\ntraditionally considered to be the prime neural correlate of human\nintelligence, clinical studies indicate that human emotions intrinsically\ncorrelate with the activation of the neuromodulatory system.\n  We examine here the question: Why do humans dispose of the diffusive\nemotional control system? Is this a coincidence, a caprice of nature, perhaps a\nleftover of our genetic heritage, or a necessary aspect of any advanced\nintelligence, being it biological or synthetic? We argue here that emotional\ncontrol is necessary to solve the motivational problem, viz the selection of\nshort-term utility functions, in the context of an environment where\ninformation, computing power and time constitute scarce resources.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 15:59:30 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Gros", "Claudius", ""]]}, {"id": "1112.1489", "submitter": "Wan-Li Chen", "authors": "Wan-Li Chen", "title": "Multi-granular Perspectives on Covering", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covering model provides a general framework for granular computing in that\noverlapping among granules are almost indispensable. For any given covering,\nboth intersection and union of covering blocks containing an element are\nexploited as granules to form granular worlds at different abstraction levels,\nrespectively, and transformations among these different granular worlds are\nalso discussed. As an application of the presented multi-granular perspective\non covering, relational interpretation and axiomization of four types of\ncovering based rough upper approximation operators are investigated, which can\nbe dually applied to lower ones.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 07:11:56 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Chen", "Wan-Li", ""]]}, {"id": "1112.1670", "submitter": "Casey Bennett", "authors": "Casey Bennett, Thomas Doub, April Bragg, Jason Luellen, Christina Van\n  Regenmorter, Jennifer Lockman, Randall Reiserer", "title": "Data Mining Session-Based Patient Reported Outcomes (PROs) in a Mental\n  Health Setting: Toward Data-Driven Clinical Decision Support and Personalized\n  Treatment", "comments": "Keywords- Data Mining; Patient-Reported Outcomes; CDOI;\n  Implementation; Electronic Health Records; Decision Support Systems,\n  Clinical; Theory of Planned Behavior", "journal-ref": "First IEEE International Conference on Healthcare Informatics,\n  Imaging and Systems Biology (HISB). (2011). 229-236", "doi": "10.1109/HISB.2011.20", "report-no": null, "categories": "cs.AI cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CDOI outcome measure - a patient-reported outcome (PRO) instrument\nutilizing direct client feedback - was implemented in a large, real-world\nbehavioral healthcare setting in order to evaluate previous findings from\nsmaller controlled studies. PROs provide an alternative window into treatment\neffectiveness based on client perception and facilitate detection of\nproblems/symptoms for which there is no discernible measure (e.g. pain). The\nprincipal focus of the study was to evaluate the utility of the CDOI for\npredictive modeling of outcomes in a live clinical setting. Implementation\nfactors were also addressed within the framework of the Theory of Planned\nBehavior by linking adoption rates to implementation practices and clinician\nperceptions. The results showed that the CDOI does contain significant capacity\nto predict outcome delta over time based on baseline and early change scores in\na large, real-world clinical setting, as suggested in previous research. The\nimplementation analysis revealed a number of critical factors affecting\nsuccessful implementation and adoption of the CDOI outcome measure, though\nthere was a notable disconnect between clinician intentions and actual\nbehavior. Most importantly, the predictive capacity of the CDOI underscores the\nutility of direct client feedback measures such as PROs and their potential use\nas the basis for next generation clinical decision support tools and\npersonalized treatment approaches.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 19:44:48 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Bennett", "Casey", ""], ["Doub", "Thomas", ""], ["Bragg", "April", ""], ["Luellen", "Jason", ""], ["Van Regenmorter", "Christina", ""], ["Lockman", "Jennifer", ""], ["Reiserer", "Randall", ""]]}, {"id": "1112.1937", "submitter": "Sao Mai Nguyen", "authors": "Sao Mai Nguyen (INRIA Bordeaux - Sud-Ouest), Adrien Baranes (INRIA\n  Bordeaux - Sud-Ouest), Pierre-Yves Oudeyer (INRIA Bordeaux - Sud-Ouest)", "title": "Bootstrapping Intrinsically Motivated Learning with Human Demonstrations", "comments": "IEEE International Conference on Development and Learning, Frankfurt\n  : Germany (2011)", "journal-ref": "2011 IEEE International Conference on Development and Learning\n  (ICDL)", "doi": "10.1109/DEVLRN.2011.6037329", "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the coupling of internally guided learning and social\ninteraction, and more specifically the improvement owing to demonstrations of\nthe learning by intrinsic motivation. We present Socially Guided Intrinsic\nMotivation by Demonstration (SGIM-D), an algorithm for learning in continuous,\nunbounded and non-preset environments. After introducing social learning and\nintrinsic motivation, we describe the design of our algorithm, before showing\nthrough a fishing experiment that SGIM-D efficiently combines the advantages of\nsocial learning and intrinsic motivation to gain a wide repertoire while being\nspecialised in specific subspaces.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2011 20:27:31 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Nguyen", "Sao Mai", "", "INRIA Bordeaux - Sud-Ouest"], ["Baranes", "Adrien", "", "INRIA\n  Bordeaux - Sud-Ouest"], ["Oudeyer", "Pierre-Yves", "", "INRIA Bordeaux - Sud-Ouest"]]}, {"id": "1112.1996", "submitter": "Joris Bierkens", "authors": "Joris Bierkens, Bert Kappen", "title": "KL-learning: Online solution of Kullback-Leibler control problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a stochastic approximation method for the solution of an ergodic\nKullback-Leibler control problem. A Kullback-Leibler control problem is a\nMarkov decision process on a finite state space in which the control cost is\nproportional to a Kullback-Leibler divergence of the controlled transition\nprobabilities with respect to the uncontrolled transition probabilities. The\nalgorithm discussed in this work allows for a sound theoretical analysis using\nthe ODE method. In a numerical experiment the algorithm is shown to be\ncomparable to the power method and the related Z-learning algorithm in terms of\nconvergence speed. It may be used as the basis of a reinforcement learning\nstyle algorithm for Markov decision problems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 01:35:06 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2012 20:36:39 GMT"}], "update_date": "2012-02-17", "authors_parsed": [["Bierkens", "Joris", ""], ["Kappen", "Bert", ""]]}, {"id": "1112.2095", "submitter": "Sao Mai Nguyen", "authors": "Sao Mai Nguyen (INRIA Bordeaux - Sud-Ouest), Masaki Ogino, Minoru\n  Asada", "title": "Real-time face swapping as a tool for understanding infant\n  self-recognition", "comments": null, "journal-ref": "International Conference on Epigenetic Robotics, Glumslov : Sweden\n  (2010)", "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study the preference of infants for contingency of movements and\nfamiliarity of faces during self-recognition task, we built, as an accurate and\ninstantaneous imitator, a real-time face- swapper for videos. We present a\nnon-constraint face-swapper based on 3D visual tracking that achieves real-time\nperformance through parallel computing. Our imitator system is par- ticularly\nsuited for experiments involving children with Autistic Spectrum Disorder who\nare often strongly disturbed by the constraints of other methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 13:19:21 GMT"}], "update_date": "2011-12-12", "authors_parsed": [["Nguyen", "Sao Mai", "", "INRIA Bordeaux - Sud-Ouest"], ["Ogino", "Masaki", ""], ["Asada", "Minoru", ""]]}, {"id": "1112.2113", "submitter": "Varun Raj Kompella", "authors": "Varun Raj Kompella, Matthew Luciw and Juergen Schmidhuber", "title": "Incremental Slow Feature Analysis: Adaptive and Episodic Learning from\n  High-Dimensional Input Streams", "comments": null, "journal-ref": "Neural Computation, 2012, Vol. 24, No. 11, Pages 2994-3024", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow Feature Analysis (SFA) extracts features representing the underlying\ncauses of changes within a temporally coherent high-dimensional raw sensory\ninput signal. Our novel incremental version of SFA (IncSFA) combines\nincremental Principal Components Analysis and Minor Components Analysis. Unlike\nstandard batch-based SFA, IncSFA adapts along with non-stationary environments,\nis amenable to episodic training, is not corrupted by outliers, and is\ncovariance-free. These properties make IncSFA a generally useful unsupervised\npreprocessor for autonomous learning agents and robots. In IncSFA, the CCIPCA\nand MCA updates take the form of Hebbian and anti-Hebbian updating, extending\nthe biological plausibility of SFA. In both single node and deep network\nversions, IncSFA learns to encode its input streams (such as high-dimensional\nvideo) by informative slow features representing meaningful abstract\nenvironmental properties. It can handle cases where batch SFA fails.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 15:01:25 GMT"}], "update_date": "2012-10-11", "authors_parsed": [["Kompella", "Varun Raj", ""], ["Luciw", "Matthew", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1112.2144", "submitter": "Alexandru Godescu Mr", "authors": "Alexandru Godescu", "title": "An Information Theoretic Analysis of Decision in Computer Chess", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basis of the method proposed in this article is the idea that information\nis one of the most important factors in strategic decisions, including\ndecisions in computer chess and other strategy games. The model proposed in\nthis article and the algorithm described are based on the idea of a information\ntheoretic basis of decision in strategy games . The model generalizes and\nprovides a mathematical justification for one of the most popular search\nalgorithms used in leading computer chess programs, the fractional ply scheme.\nHowever, despite its success in leading computer chess applications, until now\nfew has been published about this method. The article creates a fundamental\nbasis for this method in the axioms of information theory, then derives the\nprinciples used in programming the search and describes mathematically the form\nof the coefficients. One of the most important parameters of the fractional ply\nsearch is derived from fundamental principles. Until now this coefficient has\nbeen usually handcrafted or determined from intuitive elements or data mining.\nThere is a deep, information theoretical justification for such a parameter. In\none way the method proposed is a generalization of previous methods. More\nimportant, it shows why the fractional depth ply scheme is so powerful. It is\nbecause the algorithm navigates along the lines where the highest information\ngain is possible. A working and original implementation has been written and\ntested for this algorithm and is provided in the appendix. The article is\nessentially self-contained and gives proper background knowledge and\nreferences. The assumptions are intuitive and in the direction expected and\ndescribed intuitively by great champions of chess.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 16:50:09 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Godescu", "Alexandru", ""]]}, {"id": "1112.2149", "submitter": "Alexandru Godescu Mr", "authors": "Alexandru Godescu", "title": "Information and Search in Computer Chess", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes a model of chess based on information theory. A\nmathematical model of the partial depth scheme is outlined and a formula for\nthe partial depth added for each ply is calculated from the principles of the\nmodel. An implementation of alpha-beta with partial depth is given. The method\nis tested using an experimental strategy having as objective to show the effect\nof allocation of a higher amount of search resources on areas of the search\ntree with higher information. The search proceeds in the direction of lines\nwith higher information gain. The effects on search performance of allocating\nhigher search resources on lines with higher information gain are tested\nexperimentaly and conclusive results are obtained. In order to isolate the\neffects of the partial depth scheme no other heuristic is used.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 17:01:30 GMT"}], "update_date": "2011-12-12", "authors_parsed": [["Godescu", "Alexandru", ""]]}, {"id": "1112.2408", "submitter": "Intisar Al-Mejibli Mrs", "authors": "Intisar Al-Mejibli and Martin Colley", "title": "Maximum Production of Transmission Messages Rate for Service Discovery\n  Protocols", "comments": "20 pages, 8 figures; International Journal of Computer Networks &\n  Communications (IJCNC) Vol.3, No.6, November 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the number of dropped User Datagram Protocol (UDP) messages in a\nnetwork is regarded as a challenge by researchers. This issue represents\nserious problems for many protocols particularly those that depend on sending\nmessages as part of their strategy, such us service discovery protocols. This\npaper proposes and evaluates an algorithm to predict the minimum period of time\nrequired between two or more consecutive messages and suggests the minimum\nqueue sizes for the routers, to manage the traffic and minimise the number of\ndropped messages that has been caused by either congestion or queue overflow or\nboth together. The algorithm has been applied to the Universal Plug and Play\n(UPnP) protocol using ns2 simulator. It was tested when the routers were\nconnected in two configurations; as a centralized and de centralized. The\nmessage length and bandwidth of the links among the routers were taken in the\nconsideration. The result shows Better improvement in number of dropped\nmessages `among the routers.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2011 22:03:21 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Al-Mejibli", "Intisar", ""], ["Colley", "Martin", ""]]}, {"id": "1112.2410", "submitter": "Intisar Al-Mejibli Mrs", "authors": "Intisar Al-Mejibli, Martin Colley Salah Al-Majeed", "title": "Networks Utilization Improvements for Service Discovery Performance", "comments": "20 pages, 22 figures; International Journal on Cloud Computing:\n  Services and Architecture (IJCCSA),Vol.1, No.3, November 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service discovery requests' messages have a vital role in sharing and\nlocating resources in many of service discovery protocols. Sending more\nmessages than a link can handle may cause congestion and loss of messages which\ndramatically influences the performance of these protocols. Re-send the lost\nmessages result in latency and inefficiency in performing the tasks which\nuser(s) require from the connected nodes. This issue become a serious problem\nin two cases: first, when the number of clients which performs a service\ndiscovery request is increasing, as this result in increasing in the number of\nsent discovery messages; second, when the network resources such as bandwidth\ncapacity are consumed by other applications. These two cases lead to network\ncongestion and loss of messages. This paper propose an algorithm to improve the\nservices discovery protocols performance by separating each consecutive burst\nof messages with a specific period of time which calculated regarding the\navailable network resources. It was tested when the routers were connected in\ntwo configurations; decentralised and centralised .In addition, this paper\nexplains the impact of increasing the number of clients and the consumed\nnetwork resources on the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2011 22:40:09 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Al-Mejibli", "Intisar", ""], ["Al-Majeed", "Martin Colley Salah", ""]]}, {"id": "1112.2640", "submitter": "C\\`esar Ferri", "authors": "Jos\\'e Hern\\'andez-Orallo, Peter Flach, C\\`esar Ferri", "title": "Threshold Choice Methods: the Missing Link", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many performance metrics have been introduced for the evaluation of\nclassification performance, with different origins and niches of application:\naccuracy, macro-accuracy, area under the ROC curve, the ROC convex hull, the\nabsolute error, and the Brier score (with its decomposition into refinement and\ncalibration). One way of understanding the relation among some of these metrics\nis the use of variable operating conditions (either in the form of\nmisclassification costs or class proportions). Thus, a metric may correspond to\nsome expected loss over a range of operating conditions. One dimension for the\nanalysis has been precisely the distribution we take for this range of\noperating conditions, leading to some important connections in the area of\nproper scoring rules. However, we show that there is another dimension which\nhas not received attention in the analysis of performance metrics. This new\ndimension is given by the decision rule, which is typically implemented as a\nthreshold choice method when using scoring models. In this paper, we explore\nmany old and new threshold choice methods: fixed, score-uniform, score-driven,\nrate-driven and optimal, among others. By calculating the loss of these methods\nfor a uniform range of operating conditions we get the 0-1 loss, the absolute\nerror, the Brier score (mean squared error), the AUC and the refinement loss\nrespectively. This provides a comprehensive view of performance metrics as well\nas a systematic approach to loss minimisation, namely: take a model, apply\nseveral threshold choice methods consistent with the information which is (and\nwill be) available about the operating condition, and compare their expected\nlosses. In order to assist in this procedure we also derive several connections\nbetween the aforementioned performance metrics, and we highlight the role of\ncalibration in choosing the threshold choice method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 18:03:42 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2012 09:44:33 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""], ["Flach", "Peter", ""], ["Ferri", "C\u00e8sar", ""]]}, {"id": "1112.2679", "submitter": "Tong Zhang", "authors": "Xiao-Tong Yuan and Tong Zhang", "title": "Truncated Power Method for Sparse Eigenvalue Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the sparse eigenvalue problem, which is to extract\ndominant (largest) sparse eigenvectors with at most $k$ non-zero components. We\npropose a simple yet effective solution called truncated power method that can\napproximately solve the underlying nonconvex optimization problem. A strong\nsparse recovery result is proved for the truncated power method, and this\ntheory is our key motivation for developing the new algorithm. The proposed\nmethod is tested on applications such as sparse principal component analysis\nand the densest $k$-subgraph problem. Extensive experiments on several\nsynthetic and real-world large scale datasets demonstrate the competitive\nempirical performance of our method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 20:11:41 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Yuan", "Xiao-Tong", ""], ["Zhang", "Tong", ""]]}, {"id": "1112.2681", "submitter": "Muhammad Islam", "authors": "Muhammad Asiful Islam, C. R. Ramakrishnan, I. V. Ramakrishnan", "title": "Inference in Probabilistic Logic Programs with Continuous Random\n  Variables", "comments": "12 pages. arXiv admin note: substantial text overlap with\n  arXiv:1203.4287", "journal-ref": "Theory and Practice of Logic Programming / Volume12 / Special\n  Issue4-5 / July 2012, pp 505-523", "doi": "10.1017/S1471068412000154", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Logic Programming (PLP), exemplified by Sato and Kameya's\nPRISM, Poole's ICL, Raedt et al's ProbLog and Vennekens et al's LPAD, is aimed\nat combining statistical and logical knowledge representation and inference. A\nkey characteristic of PLP frameworks is that they are conservative extensions\nto non-probabilistic logic programs which have been widely used for knowledge\nrepresentation. PLP frameworks extend traditional logic programming semantics\nto a distribution semantics, where the semantics of a probabilistic logic\nprogram is given in terms of a distribution over possible models of the\nprogram. However, the inference techniques used in these works rely on\nenumerating sets of explanations for a query answer. Consequently, these\nlanguages permit very limited use of random variables with continuous\ndistributions. In this paper, we present a symbolic inference procedure that\nuses constraints and represents sets of explanations without enumeration. This\npermits us to reason over PLPs with Gaussian or Gamma-distributed random\nvariables (in addition to discrete-valued random variables) and linear equality\nconstraints over reals. We develop the inference procedure in the context of\nPRISM; however the procedure's core ideas can be easily applied to other PLP\nlanguages as well. An interesting aspect of our inference procedure is that\nPRISM's query evaluation process becomes a special case in the absence of any\ncontinuous random variables in the program. The symbolic inference procedure\nenables us to reason over complex probabilistic models such as Kalman filters\nand a large subclass of Hybrid Bayesian networks that were hitherto not\npossible in PLP frameworks. (To appear in Theory and Practice of Logic\nProgramming).\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 20:16:55 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2012 04:28:09 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2012 03:24:10 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Islam", "Muhammad Asiful", ""], ["Ramakrishnan", "C. R.", ""], ["Ramakrishnan", "I. V.", ""]]}, {"id": "1112.3115", "submitter": "James Whitacre", "authors": "James M. Whitacre and Sergei P. Atamas", "title": "The Diversity Paradox: How Nature Resolves an Evolutionary Dilemma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.AI q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptation to changing environments is a hallmark of biological systems.\nDiversity in traits is necessary for adaptation and can influence the survival\nof a population faced with novelty. In habitats that remain stable over many\ngenerations, stabilizing selection reduces trait differences within\npopulations, thereby appearing to remove the diversity needed for heritable\nadaptive responses in new environments. Paradoxically, field studies have\ndocumented numerous populations under long periods of stabilizing selection and\nevolutionary stasis that have rapidly evolved under changed environmental\nconditions. In this article, we review how cryptic genetic variation (CGV)\nresolves this diversity paradox by allowing populations in a stable environment\nto gradually accumulate hidden genetic diversity that is revealed as trait\ndifferences when environments change. Instead of being in conflict,\nenvironmental stasis supports CGV accumulation and thus appears to facilitate\nrapid adaptation in new environments as suggested by recent CGV studies.\nSimilarly, degeneracy has been found to support both genetic and non-genetic\nadaptation at many levels of biological organization. Degenerate, as opposed to\ndiverse or redundant, ensembles appear functionally redundant in certain\nenvironmental contexts but functionally diverse in others. CGV and degeneracy\nparadigms for adaptation are integrated in this review, revealing a common set\nof principles that support adaptation at multiple levels of biological\norganization. Though a discussion of simulation studies, molecular-based\nexperimental systems, principles from population genetics, and field\nexperiments, we demonstrate that CGV and degeneracy reflect complementary\ntop-down and bottom-up, respectively, conceptualizations of the same basic\nphenomenon and arguably capture a universal feature of biological adaptive\nprocesses.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 04:47:05 GMT"}], "update_date": "2011-12-15", "authors_parsed": [["Whitacre", "James M.", ""], ["Atamas", "Sergei P.", ""]]}, {"id": "1112.3117", "submitter": "James Whitacre", "authors": "James Whitacre, Axel Bender", "title": "Pervasive Flexibility in Living Technologies through Degeneracy Based\n  Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity to adapt can greatly influence the success of systems that need\nto compensate for damaged parts, learn how to achieve robust performance in new\nenvironments, or exploit novel opportunities that originate from new\ntechnological interfaces or emerging markets. Many of the conditions in which\ntechnology is required to adapt cannot be anticipated during its design stage,\ncreating a significant challenge for the designer. Inspired by the study of a\nrange of biological systems, we propose that degeneracy - the realization of\nmultiple, functionally versatile components with contextually overlapping\nfunctional redundancy - will support adaptation in technologies because it\neffects pervasive flexibility, evolutionary innovation, and homeostatic\nrobustness. We provide examples of degeneracy in a number of rudimentary living\ntechnologies from military socio-technical systems to swarm robotics and we\npresent design principles - including protocols, loose regulatory coupling, and\nfunctional versatility - that allow degeneracy to arise in both biological and\nman-made systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 04:52:18 GMT"}], "update_date": "2011-12-15", "authors_parsed": [["Whitacre", "James", ""], ["Bender", "Axel", ""]]}, {"id": "1112.4031", "submitter": "Tejaswini Hilage Abhijit", "authors": "Tejaswini Hilage, R.V.Kulkarni", "title": "Application of Data Mining Techniques to a Selected Business\n  Organisation with Special Reference to Buying Behaviour", "comments": "16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is a new concept & an exploration and analysis of large data\nsets, in order to discover meaningful patterns and rules. Many organizations\nare now using the data mining techniques to find out meaningful patterns from\nthe database. The present paper studies how data mining techniques can be apply\nto the large database. These data mining techniques give certain behavioral\npattern from the database. The results which come after analysis of the\ndatabase are useful for organization. This paper examines the result after\napplying association rule mining technique, rule induction technique and\nApriori algorithm. These techniques are applied to the database of shopping\nmall. Market basket analysis is performing by the above mentioned techniques\nand some important results are found such as buying behavior.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2011 06:27:52 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Hilage", "Tejaswini", ""], ["Kulkarni", "R. V.", ""]]}, {"id": "1112.4057", "submitter": "Bartlomiej Placzek", "authors": "Bart{\\l}omiej P{\\l}aczek", "title": "Performance Evaluation of Road Traffic Control Using a Fuzzy Cellular\n  Model", "comments": "The final publication is available at http://www.springerlink.com", "journal-ref": "P{\\l}aczek, B., Performance Evaluation of Road Traffic Control\n  Using a Fuzzy Cellular Model. Lecture Notes in Artificial Intelligence 6679.\n  Springer-Verlag, Berlin Heidelberg, pp. 59-66, 2011", "doi": "10.1007/978-3-642-21222-2_8", "report-no": null, "categories": "cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a method is proposed for performance evaluation of road traffic\ncontrol systems. The method is designed to be implemented in an on-line\nsimulation environment, which enables optimisation of adaptive traffic control\nstrategies. Performance measures are computed using a fuzzy cellular traffic\nmodel, formulated as a hybrid system combining cellular automata and fuzzy\ncalculus. Experimental results show that the introduced method allows the\nperformance to be evaluated using imprecise traffic measurements. Moreover, the\nfuzzy definitions of performance measures are convenient for uncertainty\ndetermination in traffic control decisions.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2011 14:23:53 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["P\u0142aczek", "Bart\u0142omiej", ""]]}, {"id": "1112.4628", "submitter": "Habib Shah", "authors": "Habib Shah, Rozaida Ghazali, and Nazri Mohd Nawi", "title": "Using Artificial Bee Colony Algorithm for MLP Training on Earthquake\n  Time Series Data Prediction", "comments": "8 pages,8 figures;\n  http://www.journalofcomputing.org/volume-3-issue-6-june-2011", "journal-ref": "Journal of Computing, 3, 6 (2011), 135-142", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, computer scientists have shown the interest in the study of social\ninsect's behaviour in neural networks area for solving different combinatorial\nand statistical problems. Chief among these is the Artificial Bee Colony (ABC)\nalgorithm. This paper investigates the use of ABC algorithm that simulates the\nintelligent foraging behaviour of a honey bee swarm. Multilayer Perceptron\n(MLP) trained with the standard back propagation algorithm normally utilises\ncomputationally intensive training algorithms. One of the crucial problems with\nthe backpropagation (BP) algorithm is that it can sometimes yield the networks\nwith suboptimal weights because of the presence of many local optima in the\nsolution space. To overcome ABC algorithm used in this work to train MLP\nlearning the complex behaviour of earthquake time series data trained by BP,\nthe performance of MLP-ABC is benchmarked against MLP training with the\nstandard BP. The experimental result shows that MLP-ABC performance is better\nthan MLP-BP for time series data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 09:50:53 GMT"}], "update_date": "2011-12-22", "authors_parsed": [["Shah", "Habib", ""], ["Ghazali", "Rozaida", ""], ["Nawi", "Nazri Mohd", ""]]}, {"id": "1112.5309", "submitter": "Juergen Schmidhuber", "authors": "J\\\"urgen Schmidhuber", "title": "POWERPLAY: Training an Increasingly General Problem Solver by\n  Continually Searching for the Simplest Still Unsolvable Problem", "comments": "21 pages, additional connections to previous work, references to\n  first experiments with POWERPLAY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of computer science focuses on automatically solving given computational\nproblems. I focus on automatically inventing or discovering problems in a way\ninspired by the playful behavior of animals and humans, to train a more and\nmore general problem solver from scratch in an unsupervised fashion. Consider\nthe infinite set of all computable descriptions of tasks with possibly\ncomputable solutions. The novel algorithmic framework POWERPLAY (2011)\ncontinually searches the space of possible pairs of new tasks and modifications\nof the current problem solver, until it finds a more powerful problem solver\nthat provably solves all previously learned tasks plus the new one, while the\nunmodified predecessor does not. Wow-effects are achieved by continually making\npreviously learned skills more efficient such that they require less time and\nspace. New skills may (partially) re-use previously learned skills. POWERPLAY's\nsearch orders candidate pairs of tasks and solver modifications by their\nconditional computational (time & space) complexity, given the stored\nexperience so far. The new task and its corresponding task-solving skill are\nthose first found and validated. The computational costs of validating new\ntasks need not grow with task repertoire size. POWERPLAY's ongoing search for\nnovelty keeps breaking the generalization abilities of its present solver. This\nis related to Goedel's sequence of increasingly powerful formal theories based\non adding formerly unprovable statements to the axioms without affecting\npreviously provable theorems. The continually increasing repertoire of problem\nsolving procedures can be exploited by a parallel search for solutions to\nadditional externally posed tasks. POWERPLAY may be viewed as a greedy but\npractical implementation of basic principles of creativity. A first\nexperimental analysis can be found in separate papers [53,54].\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 13:50:46 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2012 17:22:46 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1112.5370", "submitter": "Arijit  Laha Ph.D.", "authors": "Arijit Laha", "title": "Enhancing Support for Knowledge Works: A relatively unexplored vista of\n  computing research", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let us envision a new class of IT systems, the \"Support Systems for Knowledge\nWorks\" or SSKW. An SSKW can be defined as a system built for providing\ncomprehensive support to human knowledge-workers while performing instances of\ncomplex knowledge-works of a particular type within a particular domain of\nprofessional activities To get an idea what an SSKW-enabled work environment\ncan be like, let us look into a hypothetical scenario that depicts the\ninteraction between a physician and a patient-care SSKW during the activity of\ndiagnosing a patient.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 16:34:41 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Laha", "Arijit", ""]]}, {"id": "1112.5381", "submitter": "Daan Fierens", "authors": "Daan Fierens", "title": "Improving the Efficiency of Approximate Inference for Probabilistic\n  Logical Models by means of Program Specialization", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of performing probabilistic inference with probabilistic\nlogical models. Many algorithms for approximate inference with such models are\nbased on sampling. From a logic programming perspective, sampling boils down to\nrepeatedly calling the same queries on a knowledge base composed of a static\npart and a dynamic part. The larger the static part, the more redundancy there\nis in these repeated calls. This is problematic since inefficient sampling\nyields poor approximations.\n  We show how to apply logic program specialization to make sampling-based\ninference more efficient. We develop an algorithm that specializes the\ndefinitions of the query predicates with respect to the static part of the\nknowledge base. In experiments on real-world data we obtain speedups of up to\nan order of magnitude, and these speedups grow with the data-size.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 17:01:34 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Fierens", "Daan", ""]]}, {"id": "1112.5493", "submitter": "John Scoville", "authors": "John Scoville", "title": "Critical Data Compression", "comments": "99 pages, 31 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to data compression is developed and applied to multimedia\ncontent. This method separates messages into components suitable for both\nlossless coding and 'lossy' or statistical coding techniques, compressing\ncomplex objects by separately encoding signals and noise. This is demonstrated\nby compressing the most significant bits of data exactly, since they are\ntypically redundant and compressible, and either fitting a maximally likely\nnoise function to the residual bits or compressing them using lossy methods.\nUpon decompression, the significant bits are decoded and added to a noise\nfunction, whether sampled from a noise model or decompressed from a lossy code.\nThis results in compressed data similar to the original. For many test images,\na two-part image code using JPEG2000 for lossy coding and PAQ8l for lossless\ncoding produces less mean-squared error than an equal length of JPEG2000.\nComputer-generated images typically compress better using this method than\nthrough direct lossy coding, as do many black and white photographs and most\ncolor photographs at sufficiently high quality levels. Examples applying the\nmethod to audio and video coding are also demonstrated. Since two-part codes\nare efficient for both periodic and chaotic data, concatenations of roughly\nsimilar objects may be encoded efficiently, which leads to improved inference.\nApplications to artificial intelligence are demonstrated, showing that signals\nusing an economical lossless code have a critical level of redundancy which\nleads to better description-based inference than signals which encode either\ninsufficient data or too much detail.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 00:17:46 GMT"}], "update_date": "2011-12-26", "authors_parsed": [["Scoville", "John", ""]]}, {"id": "1112.5505", "submitter": "Nikzad Babaii-Rizvandi", "authors": "Nikzad Babaii Rizvandi, Javid Taheri, Albert Y. Zomaya, Reza Moraveji", "title": "A Study on Using Uncertain Time Series Matching Algorithms in MapReduce\n  Applications", "comments": "12 pages a version has been accepted to journal of \"Concurrency and\n  Computation: Practice and Experience\", available online from the University\n  of Sydney at http://www.nicta.com.au/pub?doc=4744", "journal-ref": null, "doi": null, "report-no": "TR672- University of Sydney", "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study CPU utilization time patterns of several Map-Reduce\napplications. After extracting running patterns of several applications, the\npatterns with their statistical information are saved in a reference database\nto be later used to tweak system parameters to efficiently execute unknown\napplications in future. To achieve this goal, CPU utilization patterns of new\napplications along with its statistical information are compared with the\nalready known ones in the reference database to find/predict their most\nprobable execution patterns. Because of different patterns lengths, the Dynamic\nTime Warping (DTW) is utilized for such comparison; a statistical analysis is\nthen applied to DTWs' outcomes to select the most suitable candidates.\nMoreover, under a hypothesis, another algorithm is proposed to classify\napplications under similar CPU utilization patterns. Three widely used text\nprocessing applications (WordCount, Distributed Grep, and Terasort) and another\napplication (Exim Mainlog parsing) are used to evaluate our hypothesis in\ntweaking system parameters in executing similar applications. Results were very\npromising and showed effectiveness of our approach on 5-node Map-Reduce\nplatform\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 02:38:42 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2012 00:30:11 GMT"}, {"version": "v3", "created": "Fri, 20 Jan 2012 00:28:52 GMT"}, {"version": "v4", "created": "Wed, 13 Jun 2012 03:33:54 GMT"}, {"version": "v5", "created": "Fri, 18 Jan 2013 03:54:34 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Rizvandi", "Nikzad Babaii", ""], ["Taheri", "Javid", ""], ["Zomaya", "Albert Y.", ""], ["Moraveji", "Reza", ""]]}, {"id": "1112.6219", "submitter": "Rafi Muhammad", "authors": "Muhammad Rafi, M. Shahid Shaikh, Amir Farooq", "title": "Document Clustering based on Topic Maps", "comments": null, "journal-ref": "International Journal of Computer Applications 12(1):32-36,\n  December 2010", "doi": "10.5120/1640-2204", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance of document clustering is now widely acknowledged by researchers\nfor better management, smart navigation, efficient filtering, and concise\nsummarization of large collection of documents like World Wide Web (WWW). The\nnext challenge lies in semantically performing clustering based on the semantic\ncontents of the document. The problem of document clustering has two main\ncomponents: (1) to represent the document in such a form that inherently\ncaptures semantics of the text. This may also help to reduce dimensionality of\nthe document, and (2) to define a similarity measure based on the semantic\nrepresentation such that it assigns higher numerical values to document pairs\nwhich have higher semantic relationship. Feature space of the documents can be\nvery challenging for document clustering. A document may contain multiple\ntopics, it may contain a large set of class-independent general-words, and a\nhandful class-specific core-words. With these features in mind, traditional\nagglomerative clustering algorithms, which are based on either Document Vector\nmodel (DVM) or Suffix Tree model (STC), are less efficient in producing results\nwith high cluster quality. This paper introduces a new approach for document\nclustering based on the Topic Map representation of the documents. The document\nis being transformed into a compact form. A similarity measure is proposed\nbased upon the inferred information through topic maps data and structures. The\nsuggested method is implemented using agglomerative hierarchal clustering and\ntested on standard Information retrieval (IR) datasets. The comparative\nexperiment reveals that the proposed approach is effective in improving the\ncluster quality.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 04:15:48 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Rafi", "Muhammad", ""], ["Shaikh", "M. Shahid", ""], ["Farooq", "Amir", ""]]}, {"id": "1112.6222", "submitter": "Rafi Muhammad", "authors": "Muhammad Rafi, M. Maujood, M. M. Fazal, S. M. Ali", "title": "A comparison of two suffix tree-based document clustering algorithms", "comments": "Information and Emerging Technologies (ICIET), 2010 International\n  Conference", "journal-ref": null, "doi": "10.1109/2010.5625688", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document clustering as an unsupervised approach extensively used to navigate,\nfilter, summarize and manage large collection of document repositories like the\nWorld Wide Web (WWW). Recently, focuses in this domain shifted from traditional\nvector based document similarity for clustering to suffix tree based document\nsimilarity, as it offers more semantic representation of the text present in\nthe document. In this paper, we compare and contrast two recently introduced\napproaches to document clustering based on suffix tree data model. The first is\nan Efficient Phrase based document clustering, which extracts phrases from\ndocuments to form compact document representation and uses a similarity measure\nbased on common suffix tree to cluster the documents. The second approach is a\nfrequent word/word meaning sequence based document clustering, it similarly\nextracts the common word sequence from the document and uses the common\nsequence/ common word meaning sequence to perform the compact representation,\nand finally, it uses document clustering approach to cluster the compact\ndocuments. These algorithms are using agglomerative hierarchical document\nclustering to perform the actual clustering step, the difference in these\napproaches are mainly based on extraction of phrases, model representation as a\ncompact document, and the similarity measures used for clustering. This paper\ninvestigates the computational aspect of the two algorithms, and the quality of\nresults they produced.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 04:25:10 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2012 15:40:29 GMT"}], "update_date": "2012-01-11", "authors_parsed": [["Rafi", "Muhammad", ""], ["Maujood", "M.", ""], ["Fazal", "M. M.", ""], ["Ali", "S. M.", ""]]}, {"id": "1112.6371", "submitter": "Odemir Bruno PhD", "authors": "Ricardo Fabbri, Wesley N. Gon\\c{c}alves, Francisco J. P. Lopes, Odemir\n  M. Bruno", "title": "Multi-q Analysis of Image Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.AI cs.CV physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the use of the Tsallis Entropy versus the classic\nBoltzmann-Gibbs-Shannon entropy for classifying image patterns. Given a\ndatabase of 40 pattern classes, the goal is to determine the class of a given\nimage sample. Our experiments show that the Tsallis entropy encoded in a\nfeature vector for different $q$ indices has great advantage over the\nBoltzmann-Gibbs-Shannon entropy for pattern classification, boosting\nrecognition rates by a factor of 3. We discuss the reasons behind this success,\nshedding light on the usefulness of the Tsallis entropy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 18:38:34 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Fabbri", "Ricardo", ""], ["Gon\u00e7alves", "Wesley N.", ""], ["Lopes", "Francisco J. P.", ""], ["Bruno", "Odemir M.", ""]]}]