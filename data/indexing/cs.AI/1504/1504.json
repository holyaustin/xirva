[{"id": "1504.00110", "submitter": "Daniel Lowd", "authors": "Daniel Lowd, Amirmohammad Rooshenas", "title": "The Libra Toolkit for Probabilistic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Libra Toolkit is a collection of algorithms for learning and inference\nwith discrete probabilistic models, including Bayesian networks, Markov\nnetworks, dependency networks, and sum-product networks. Compared to other\ntoolkits, Libra places a greater emphasis on learning the structure of\ntractable models in which exact inference is efficient. It also includes a\nvariety of algorithms for learning graphical models in which inference is\npotentially intractable, and for performing exact and approximate inference.\nLibra is released under a 2-clause BSD license to encourage broad use in\nacademia and industry.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 06:05:40 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Lowd", "Daniel", ""], ["Rooshenas", "Amirmohammad", ""]]}, {"id": "1504.00136", "submitter": "Guangming Lang", "authors": "Guangming Lang", "title": "Knowledge reduction of dynamic covering decision information systems\n  with immigration of more objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical situations, it is of interest to investigate computing\napproximations of sets as an important step of knowledge reduction of dynamic\ncovering decision information systems. In this paper, we present incremental\napproaches to computing the type-1 and type-2 characteristic matrices of\ndynamic coverings whose cardinalities increase with immigration of more\nobjects. We also present the incremental algorithms of computing the second and\nsixth lower and upper approximations of sets in dynamic covering approximation\nspaces.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 08:12:01 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Lang", "Guangming", ""]]}, {"id": "1504.00522", "submitter": "Gian Diego Tipaldi", "authors": "Bahram Behzadian and Pratik Agarwal and Wolfram Burgard and Gian Diego\n  Tipaldi", "title": "Monte Carlo Localization in Hand-Drawn Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot localization is a one of the most important problems in robotics. Most\nof the existing approaches assume that the map of the environment is available\nbeforehand and focus on accurate metrical localization. In this paper, we\naddress the localization problem when the map of the environment is not present\nbeforehand, and the robot relies on a hand-drawn map from a non-expert user. We\naddressed this problem by expressing the robot pose in the pixel coordinate and\nsimultaneously estimate a local deformation of the hand-drawn map. Experiments\nshow that we are able to localize the robot in the correct room with a\nrobustness up to 80%\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 12:25:11 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Behzadian", "Bahram", ""], ["Agarwal", "Pratik", ""], ["Burgard", "Wolfram", ""], ["Tipaldi", "Gian Diego", ""]]}, {"id": "1504.00854", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "Evaluation Evaluation a Monte Carlo study", "comments": "5 pages, 14 Equations, 2 Figures, 1 Table, as submitted to European\n  Conference on Artificial Intelligence (shorter version published with 2\n  pages, 4 Equations, 0 Figures, 1 Table)", "journal-ref": "ECAI 2008, pp.843-844", "doi": null, "report-no": null, "categories": "cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade there has been increasing concern about the biases\nembodied in traditional evaluation methods for Natural Language\nProcessing/Learning, particularly methods borrowed from Information Retrieval.\nWithout knowledge of the Bias and Prevalence of the contingency being tested,\nor equivalently the expectation due to chance, the simple conditional\nprobabilities Recall, Precision and Accuracy are not meaningful as evaluation\nmeasures, either individually or in combinations such as F-factor. The\nexistence of bias in NLP measures leads to the 'improvement' of systems by\nincreasing their bias, such as the practice of improving tagging and parsing\nscores by using most common value (e.g. water is always a Noun) rather than the\nattempting to discover the correct one. The measures Cohen Kappa and Powers\nInformedness are discussed as unbiased alternative to Recall and related to the\npsychologically significant measure DeltaP. In this paper we will analyze both\nbiased and unbiased measures theoretically, characterizing the precise\nrelationship between all these measures as well as evaluating the evaluation\nmeasures themselves empirically using a Monte Carlo simulation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 14:46:29 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1504.01004", "submitter": "Zhen Zhang Dr.", "authors": "Zhen Zhang, Chonghui Guo, Luis Mart\\'inez", "title": "Managing Multi-Granular Linguistic Distribution Assessments in\n  Large-Scale Multi-Attribute Group Decision Making", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistic large-scale group decision making (LGDM) problems are more and\nmore common nowadays. In such problems a large group of decision makers are\ninvolved in the decision process and elicit linguistic information that are\nusually assessed in different linguistic scales with diverse granularity\nbecause of decision makers' distinct knowledge and background. To keep maximum\ninformation in initial stages of the linguistic LGDM problems, the use of\nmulti-granular linguistic distribution assessments seems a suitable choice,\nhowever to manage such multigranular linguistic distribution assessments, it is\nnecessary the development of a new linguistic computational approach. In this\npaper it is proposed a novel computational model based on the use of extended\nlinguistic hierarchies, which not only can be used to operate with\nmulti-granular linguistic distribution assessments, but also can provide\ninterpretable linguistic results to decision makers. Based on this new\nlinguistic computational model, an approach to linguistic large-scale\nmulti-attribute group decision making is proposed and applied to a talent\nselection process in universities.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 10:52:47 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 06:01:06 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Zhang", "Zhen", ""], ["Guo", "Chonghui", ""], ["Mart\u00ednez", "Luis", ""]]}, {"id": "1504.01173", "submitter": "Arthur Choi", "authors": "Arthur Choi and Adnan Darwiche", "title": "Dual Decomposition from the Perspective of Relax, Compensate and then\n  Recover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relax, Compensate and then Recover (RCR) is a paradigm for approximate\ninference in probabilistic graphical models that has previously provided\ntheoretical and practical insights on iterative belief propagation and some of\nits generalizations. In this paper, we characterize the technique of dual\ndecomposition in the terms of RCR, viewing it as a specific way to compensate\nfor relaxed equivalence constraints. Among other insights gathered from this\nperspective, we propose novel heuristics for recovering relaxed equivalence\nconstraints with the goal of incrementally tightening dual decomposition\napproximations, all the way to reaching exact solutions. We also show\nempirically that recovering equivalence constraints can sometimes tighten the\ncorresponding approximation (and obtaining exact results), without increasing\nmuch the complexity of inference.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 23:49:11 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Choi", "Arthur", ""], ["Darwiche", "Adnan", ""]]}, {"id": "1504.01639", "submitter": "Marc Bola\\~nos", "authors": "Marc Bola\\~nos and Petia Radeva", "title": "Ego-Object Discovery", "comments": "9 pages, 13 figures, Submitted to: Image and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelogging devices are spreading faster everyday. This growth can represent\ngreat benefits to develop methods for extraction of meaningful information\nabout the user wearing the device and his/her environment. In this paper, we\npropose a semi-supervised strategy for easily discovering objects relevant to\nthe person wearing a first-person camera. Given an egocentric video/images\nsequence acquired by the camera, our algorithm uses both the appearance\nextracted by means of a convolutional neural network and an object refill\nmethodology that allows to discover objects even in case of small amount of\nobject appearance in the collection of images. An SVM filtering strategy is\napplied to deal with the great part of the False Positive object candidates\nfound by most of the state of the art object detectors. We validate our method\non a new egocentric dataset of 4912 daily images acquired by 4 persons as well\nas on both PASCAL 2012 and MSRC datasets. We obtain for all of them results\nthat largely outperform the state of the art approach. We make public both the\nEDUB dataset and the algorithm code.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 15:23:22 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 09:19:48 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Bola\u00f1os", "Marc", ""], ["Radeva", "Petia", ""]]}, {"id": "1504.01684", "submitter": "Miao Fan", "authors": "Miao Fan, Qiang Zhou, Thomas Fang Zheng and Ralph Grishman", "title": "Large Margin Nearest Neighbor Embedding for Knowledge Representation", "comments": "arXiv admin note: text overlap with arXiv:1503.08155", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional way of storing facts in triplets ({\\it head\\_entity, relation,\ntail\\_entity}), abbreviated as ({\\it h, r, t}), makes the knowledge intuitively\ndisplayed and easily acquired by mankind, but hardly computed or even reasoned\nby AI machines. Inspired by the success in applying {\\it Distributed\nRepresentations} to AI-related fields, recent studies expect to represent each\nentity and relation with a unique low-dimensional embedding, which is different\nfrom the symbolic and atomic framework of displaying knowledge in triplets. In\nthis way, the knowledge computing and reasoning can be essentially facilitated\nby means of a simple {\\it vector calculation}, i.e. ${\\bf h} + {\\bf r} \\approx\n{\\bf t}$. We thus contribute an effective model to learn better embeddings\nsatisfying the formula by pulling the positive tail entities ${\\bf t^{+}}$ to\nget together and close to {\\bf h} + {\\bf r} ({\\it Nearest Neighbor}), and\nsimultaneously pushing the negatives ${\\bf t^{-}}$ away from the positives\n${\\bf t^{+}}$ via keeping a {\\it Large Margin}. We also design a corresponding\nlearning algorithm to efficiently find the optimal solution based on {\\it\nStochastic Gradient Descent} in iterative fashion. Quantitative experiments\nillustrate that our approach can achieve the state-of-the-art performance,\ncompared with several latest methods on some benchmark datasets for two\nclassical applications, i.e. {\\it Link prediction} and {\\it Triplet\nclassification}. Moreover, we analyze the parameter complexities among all the\nevaluated models, and analytical results indicate that our model needs fewer\ncomputational resources on outperforming the other methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 17:50:31 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Fan", "Miao", ""], ["Zhou", "Qiang", ""], ["Zheng", "Thomas Fang", ""], ["Grishman", "Ralph", ""]]}, {"id": "1504.01783", "submitter": "Jos\\'e Bento", "authors": "Jos\\'e Bento, Nate Derbinsky, Charles Mathy, Jonathan S. Yedidia", "title": "Proximal operators for multi-agent path planning", "comments": "See movie at http://youtu.be/gRnsjd_ocxs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of planning collision-free paths for multiple agents\nusing optimization methods known as proximal algorithms. Recently this approach\nwas explored in Bento et al. 2013, which demonstrated its ease of\nparallelization and decentralization, the speed with which the algorithms\ngenerate good quality solutions, and its ability to incorporate different\nproximal operators, each ensuring that paths satisfy a desired property.\nUnfortunately, the operators derived only apply to paths in 2D and require that\nany intermediate waypoints we might want agents to follow be preassigned to\nspecific agents, limiting their range of applicability. In this paper we\nresolve these limitations. We introduce new operators to deal with agents\nmoving in arbitrary dimensions that are faster to compute than their 2D\npredecessors and we introduce landmarks, space-time positions that are\nautomatically assigned to the set of agents under different optimality\ncriteria. Finally, we report the performance of the new operators in several\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 23:49:31 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Bento", "Jos\u00e9", ""], ["Derbinsky", "Nate", ""], ["Mathy", "Charles", ""], ["Yedidia", "Jonathan S.", ""]]}, {"id": "1504.02027", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "The Neutrosophic Entropy and its Five Components", "comments": null, "journal-ref": "Neutrosophic Sets and Systems, Vol.7, 2015,pp. 40-46", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two variants of penta-valued representation for\nneutrosophic entropy. The first is an extension of Kaufmann's formula and the\nsecond is an extension of Kosko's formula.\n  Based on the primary three-valued information represented by the degree of\ntruth, degree of falsity and degree of neutrality there are built some\npenta-valued representations that better highlights some specific features of\nneutrosophic entropy. Thus, we highlight five features of neutrosophic\nuncertainty such as ambiguity, ignorance, contradiction, neutrality and\nsaturation. These five features are supplemented until a seven partition of\nunity by adding two features of neutrosophic certainty such as truth and\nfalsity.\n  The paper also presents the particular forms of neutrosophic entropy obtained\nin the case of bifuzzy representations, intuitionistic fuzzy representations,\nparaconsistent fuzzy representations and finally the case of fuzzy\nrepresentations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 07:06:16 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1504.02059", "submitter": "Hayat Alrefaie", "authors": "Hayat Alrefaie and Allan Ramsay", "title": "Supporting Language Learners with the Meanings Of Closed Class Items", "comments": "12 pages include references, 5 figures and AIAPP 2015 conference in\n  Geneva", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of language learning involves the mastery of countless tasks:\nmaking the constituent sounds of the language being learned, learning the\ngrammatical patterns, and acquiring the requisite vocabulary for reception and\nproduction. While a plethora of computational tools exist to facilitate the\nfirst and second of these tasks, a number of challenges arise with respect to\nenabling the third. This paper describes a tool that has been designed to\nsupport language learners with the challenge of understanding the use of\nclosed-class lexical items. The process of learning the Arabic for office is\n(mktb) is relatively simple and should be possible by means of simple\nrepetition of the word. However, it is much more difficult to learn and\ncorrectly use the Arabic equivalent of the word on. The current paper describes\na mechanism for the delivery of diagnostic information regarding specific\nlexical examples, with the aim of clearly demonstrating why a particular\ntranslation of a given closed-class item may be appropriate in certain\nsituations but not others, thereby helping learners to understand and use the\nterm correctly.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 18:12:07 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Alrefaie", "Hayat", ""], ["Ramsay", "Allan", ""]]}, {"id": "1504.02141", "submitter": "Shehroz Khan", "authors": "Shehroz S. Khan, Michelle E. Karg, Dana Kulic, Jesse Hoey", "title": "Detecting Falls with X-Factor Hidden Markov Models", "comments": "27 pages, 4 figures, 3 tables, Applied Soft Computing, 2017", "journal-ref": "Applied Soft Computing Volume 55, June 2017, Pages 168-177", "doi": "10.1016/j.asoc.2017.01.034", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of falls while performing normal activities of daily living\n(ADL) is important to ensure personal safety and well-being. However, falling\nis a short term activity that occurs infrequently. This poses a challenge to\ntraditional classification algorithms, because there may be very little\ntraining data for falls (or none at all). This paper proposes an approach for\nthe identification of falls using a wearable device in the absence of training\ndata for falls but with plentiful data for normal ADL. We propose three\n`X-Factor' Hidden Markov Model (XHMMs) approaches. The XHMMs model unseen falls\nusing \"inflated\" output covariances (observation models). To estimate the\ninflated covariances, we propose a novel cross validation method to remove\n\"outliers\" from the normal ADL that serve as proxies for the unseen falls and\nallow learning the XHMMs using only normal activities. We tested the proposed\nXHMM approaches on two activity recognition datasets and show high detection\nrates for falls in the absence of fall-specific training data. We show that the\ntraditional method of choosing a threshold based on maximum of negative of\nlog-likelihood to identify unseen falls is ill-posed for this problem. We also\nshow that supervised classification methods perform poorly when very limited\nfall data are available during the training phase.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 22:02:27 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 00:57:15 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2015 23:20:48 GMT"}, {"version": "v4", "created": "Thu, 30 Jun 2016 20:48:13 GMT"}, {"version": "v5", "created": "Fri, 20 Jan 2017 20:18:15 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Khan", "Shehroz S.", ""], ["Karg", "Michelle E.", ""], ["Kulic", "Dana", ""], ["Hoey", "Jesse", ""]]}, {"id": "1504.02150", "submitter": "Chao-Lin Liu", "authors": "Wei-Jie Huang and Chao-Lin Liu", "title": "Exploring Lexical, Syntactic, and Semantic Features for Chinese Textual\n  Entailment in NTCIR RITE Evaluation Tasks", "comments": "20 pages, 1 figure, 26 tables, Journal article in Soft Computing\n  (Spinger). Soft Computing, online. Springer, Germany, 2015", "journal-ref": null, "doi": "10.1007/s00500-015-1629-1", "report-no": null, "categories": "cs.CL cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We computed linguistic information at the lexical, syntactic, and semantic\nlevels for Recognizing Inference in Text (RITE) tasks for both traditional and\nsimplified Chinese in NTCIR-9 and NTCIR-10. Techniques for syntactic parsing,\nnamed-entity recognition, and near synonym recognition were employed, and\nfeatures like counts of common words, statement lengths, negation words, and\nantonyms were considered to judge the entailment relationships of two\nstatements, while we explored both heuristics-based functions and\nmachine-learning approaches. The reported systems showed robustness by\nsimultaneously achieving second positions in the binary-classification subtasks\nfor both simplified and traditional Chinese in NTCIR-10 RITE-2. We conducted\nmore experiments with the test data of NTCIR-9 RITE, with good results. We also\nextended our work to search for better configurations of our classifiers and\ninvestigated contributions of individual features. This extended work showed\ninteresting results and should encourage further discussion.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 22:47:59 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Huang", "Wei-Jie", ""], ["Liu", "Chao-Lin", ""]]}, {"id": "1504.02247", "submitter": "Alexey Melnikov", "authors": "Alexey A. Melnikov, Adi Makmal, Vedran Dunjko and Hans J. Briegel", "title": "Projective simulation with generalization", "comments": "14 pages, 9 figures", "journal-ref": "Sci. Rep. 7, 14430 (2017)", "doi": "10.1038/s41598-017-14740-y", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generalize is an important feature of any intelligent agent.\nNot only because it may allow the agent to cope with large amounts of data, but\nalso because in some environments, an agent with no generalization capabilities\ncannot learn. In this work we outline several criteria for generalization, and\npresent a dynamic and autonomous machinery that enables projective simulation\nagents to meaningfully generalize. Projective simulation, a novel, physical\napproach to artificial intelligence, was recently shown to perform well in\nstandard reinforcement learning problems, with applications in advanced\nrobotics as well as quantum experiments. Both the basic projective simulation\nmodel and the presented generalization machinery are based on very simple\nprinciples. This allows us to provide a full analytical analysis of the agent's\nperformance and to illustrate the benefit the agent gains by generalizing.\nSpecifically, we show that already in basic (but extreme) environments,\nlearning without generalization may be impossible, and demonstrate how the\npresented generalization machinery enables the projective simulation agent to\nlearn.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 10:37:11 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 19:18:40 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Melnikov", "Alexey A.", ""], ["Makmal", "Adi", ""], ["Dunjko", "Vedran", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1504.02255", "submitter": "Aleksey Buzmakov A", "authors": "Aleksey Buzmakov, Elias Egho, Nicolas Jay, Sergei O. Kuznetsov, Amedeo\n  Napoli, Chedy Ra\\\"issi", "title": "On mining complex sequential data by means of FCA and pattern structures", "comments": "An accepted publication in International Journal of General Systems.\n  The paper is created in the wake of the conference on Concept Lattice and\n  their Applications (CLA'2013). 27 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays data sets are available in very complex and heterogeneous ways.\nMining of such data collections is essential to support many real-world\napplications ranging from healthcare to marketing. In this work, we focus on\nthe analysis of \"complex\" sequential data by means of interesting sequential\npatterns. We approach the problem using the elegant mathematical framework of\nFormal Concept Analysis (FCA) and its extension based on \"pattern structures\".\nPattern structures are used for mining complex data (such as sequences or\ngraphs) and are based on a subsumption operation, which in our case is defined\nwith respect to the partial order on sequences. We show how pattern structures\nalong with projections (i.e., a data reduction of sequential structures), are\nable to enumerate more meaningful patterns and increase the computing\nefficiency of the approach. Finally, we show the applicability of the presented\nmethod for discovering and analyzing interesting patient patterns from a French\nhealthcare data set on cancer. The quantitative and qualitative results (with\nannotations and analysis from a physician) are reported in this use case which\nis the main motivation for this work.\n  Keywords: data mining; formal concept analysis; pattern structures;\nprojections; sequences; sequential data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 10:57:53 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Buzmakov", "Aleksey", ""], ["Egho", "Elias", ""], ["Jay", "Nicolas", ""], ["Kuznetsov", "Sergei O.", ""], ["Napoli", "Amedeo", ""], ["Ra\u00efssi", "Chedy", ""]]}, {"id": "1504.02281", "submitter": "Ratlamwala Khatija Yusuf", "authors": "Ahlam Ansari, Mohd Amin Sayyed, Khatija Ratlamwala, Parvin Shaikh", "title": "An Optimized Hybrid Approach for Path Finding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Path finding algorithm addresses problem of finding shortest path from source\nto destination avoiding obstacles. There exist various search algorithms namely\nA*, Dijkstra's and ant colony optimization. Unlike most path finding algorithms\nwhich require destination co-ordinates to compute path, the proposed algorithm\ncomprises of a new method which finds path using backtracking without requiring\ndestination co-ordinates. Moreover, in existing path finding algorithm, the\nnumber of iterations required to find path is large. Hence, to overcome this,\nan algorithm is proposed which reduces number of iterations required to\ntraverse the path. The proposed algorithm is hybrid of backtracking and a new\ntechnique(modified 8- neighbor approach). The proposed algorithm can become\nessential part in location based, network, gaming applications. grid traversal,\nnavigation, gaming applications, mobile robot and Artificial Intelligence.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 12:49:53 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Ansari", "Ahlam", ""], ["Sayyed", "Mohd Amin", ""], ["Ratlamwala", "Khatija", ""], ["Shaikh", "Parvin", ""]]}, {"id": "1504.02358", "submitter": "Alessandro Provetti", "authors": "Carlo Bernava, Giacomo Fiumara, Dario Maggiorini, Alessandro Provetti,\n  Laura Ripamonti", "title": "RDF annotation of Second Life objects: Knowledge Representation meets\n  Social Virtual reality", "comments": "The final publication is available at link.springer.com", "journal-ref": "Computational and Mathematical Organization Theory (2014) Vol. 20,\n  pages 20-35. ISSN 1381-298X", "doi": "10.1007/s10588-012-9148-4", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have designed and implemented an application running inside Second Life\nthat supports user annotation of graphical objects and graphical visualization\nof concept ontologies, thus providing a formal, machine-accessible description\nof objects. As a result, we offer a platform that combines the graphical\nknowledge representation that is expected from a MUVE artifact with the\nsemantic structure given by the Resource Framework Description (RDF)\nrepresentation of information.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 15:52:59 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Bernava", "Carlo", ""], ["Fiumara", "Giacomo", ""], ["Maggiorini", "Dario", ""], ["Provetti", "Alessandro", ""], ["Ripamonti", "Laura", ""]]}, {"id": "1504.02878", "submitter": "Aske Plaat", "authors": "Aske Plaat", "title": "Data Science and Ebola", "comments": "Inaugural lecture Leiden University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Science---Today, everybody and everything produces data. People produce\nlarge amounts of data in social networks and in commercial transactions.\nMedical, corporate, and government databases continue to grow. Sensors continue\nto get cheaper and are increasingly connected, creating an Internet of Things,\nand generating even more data. In every discipline, large, diverse, and rich\ndata sets are emerging, from astrophysics, to the life sciences, to the\nbehavioral sciences, to finance and commerce, to the humanities and to the\narts. In every discipline people want to organize, analyze, optimize and\nunderstand their data to answer questions and to deepen insights. The science\nthat is transforming this ocean of data into a sea of knowledge is called data\nscience. This lecture will discuss how data science has changed the way in\nwhich one of the most visible challenges to public health is handled, the 2014\nEbola outbreak in West Africa.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 14:14:08 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Plaat", "Aske", ""]]}, {"id": "1504.02882", "submitter": "Liu Feng", "authors": "Feng Liu, Yong Shi", "title": "Quantitative Analysis of Whether Machine Intelligence Can Surpass Human\n  Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether the machine intelligence can surpass the human intelligence is a\ncontroversial issue. On the basis of traditional IQ, this article presents the\nUniversal IQ test method suitable for both the machine intelligence and the\nhuman intelligence. With the method, machine and human intelligences were\ndivided into 4 major categories and 15 subcategories. A total of 50 search\nengines across the world and 150 persons at different ages were subject to the\nrelevant test. And then, the Universal IQ ranking list of 2014 for the test\nobjects was obtained.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 14:48:23 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Liu", "Feng", ""], ["Shi", "Yong", ""]]}, {"id": "1504.02930", "submitter": "Mingjie Cai", "authors": "Mingjie Cai", "title": "Knowledge reduction of dynamic covering decision information systems\n  with varying attribute values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge reduction of dynamic covering information systems involves with the\ntime in practical situations. In this paper, we provide incremental approaches\nto computing the type-1 and type-2 characteristic matrices of dynamic coverings\nbecause of varying attribute values. Then we present incremental algorithms of\nconstructing the second and sixth approximations of sets by using\ncharacteristic matrices. We employ experimental results to illustrate that the\nincremental approaches are effective to calculate approximations of sets in\ndynamic covering information systems. Finally, we perform knowledge reduction\nof dynamic covering information systems with the incremental approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 03:40:06 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Cai", "Mingjie", ""]]}, {"id": "1504.03071", "submitter": "Jaeyong Sung", "authors": "Jaeyong Sung, Seok Hyun Jin, Ashutosh Saxena", "title": "Robobarista: Object Part based Transfer of Manipulation Trajectories\n  from Crowd-sourcing in 3D Pointclouds", "comments": "In International Symposium on Robotics Research (ISRR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large variety of objects and appliances in human environments,\nsuch as stoves, coffee dispensers, juice extractors, and so on. It is\nchallenging for a roboticist to program a robot for each of these object types\nand for each of their instantiations. In this work, we present a novel approach\nto manipulation planning based on the idea that many household objects share\nsimilarly-operated object parts. We formulate the manipulation planning as a\nstructured prediction problem and design a deep learning model that can handle\nlarge noise in the manipulation demonstrations and learns features from three\ndifferent modalities: point-clouds, language and trajectory. In order to\ncollect a large number of manipulation demonstrations for different objects, we\ndeveloped a new crowd-sourcing platform called Robobarista. We test our model\non our dataset consisting of 116 objects with 249 parts along with 250 language\ninstructions, for which there are 1225 crowd-sourced manipulation\ndemonstrations. We further show that our robot can even manipulate objects it\nhas never seen before.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 06:25:42 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 23:43:19 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Sung", "Jaeyong", ""], ["Jin", "Seok Hyun", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1504.03303", "submitter": "Eray Ozkural", "authors": "Eray \\\"Ozkural", "title": "Ultimate Intelligence Part II: Physical Measure and Complexity of\n  Intelligence", "comments": "This paper was initially submitted to ALT-2014. We are taking the\n  valuable opinions of the anonymous reviewers into account. Many thanks to\n  Laurent Orseau for his constructive comments on the draft, which inspired\n  this revision. arXiv admin note: substantial text overlap with\n  arXiv:1501.00601 This is a major revision over the last version edited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We continue our analysis of volume and energy measures that are appropriate\nfor quantifying inductive inference systems. We extend logical depth and\nconceptual jump size measures in AIT to stochastic problems, and physical\nmeasures that involve volume and energy. We introduce a graphical model of\ncomputational complexity that we believe to be appropriate for intelligent\nmachines. We show several asymptotic relations between energy, logical depth\nand volume of computation for inductive inference. In particular, we arrive at\na \"black-hole equation\" of inductive inference, which relates energy, volume,\nspace, and algorithmic information for an optimal inductive inference solution.\nWe introduce energy-bounded algorithmic entropy. We briefly apply our ideas to\nthe physical limits of intelligent computation in our universe.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 20:39:14 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 10:39:16 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["\u00d6zkural", "Eray", ""]]}, {"id": "1504.03386", "submitter": "Leopoldo Bertossi", "authors": "Mostafa Milani and Leopoldo Bertossi", "title": "Tractable Query Answering and Optimization for Extensions of\n  Weakly-Sticky Datalog+-", "comments": "To appear in Proc. Alberto Mendelzon WS on Foundations of Data\n  Management (AMW15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a semantic class, weakly-chase-sticky (WChS), and a syntactic\nsubclass, jointly-weakly-sticky (JWS), of Datalog+- programs. Both extend that\nof weakly-sticky (WS) programs, which appear in our applications to data\nquality. For WChS programs we propose a practical, polynomial-time query\nanswering algorithm (QAA). We establish that the two classes are closed under\nmagic-sets rewritings. As a consequence, QAA can be applied to the optimized\nprograms. QAA takes as inputs the program (including the query) and semantic\ninformation about the \"finiteness\" of predicate positions. For the syntactic\nsubclasses JWS and WS of WChS, this additional information is computable.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 23:01:58 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Milani", "Mostafa", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1504.03425", "submitter": "Iftekhar Naim", "authors": "Iftekhar Naim, M. Iftekhar Tanveer, Daniel Gildea, Mohammed (Ehsan)\n  Hoque", "title": "Automated Analysis and Prediction of Job Interview Performance", "comments": "14 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational framework for automatically quantifying verbal and\nnonverbal behaviors in the context of job interviews. The proposed framework is\ntrained by analyzing the videos of 138 interview sessions with 69\ninternship-seeking undergraduates at the Massachusetts Institute of Technology\n(MIT). Our automated analysis includes facial expressions (e.g., smiles, head\ngestures, facial tracking points), language (e.g., word counts, topic\nmodeling), and prosodic information (e.g., pitch, intonation, and pauses) of\nthe interviewees. The ground truth labels are derived by taking a weighted\naverage over the ratings of 9 independent judges. Our framework can\nautomatically predict the ratings for interview traits such as excitement,\nfriendliness, and engagement with correlation coefficients of 0.75 or higher,\nand can quantify the relative importance of prosody, language, and facial\nexpressions. By analyzing the relative feature weights learned by the\nregression models, our framework recommends to speak more fluently, use less\nfiller words, speak as \"we\" (vs. \"I\"), use more unique words, and smile more.\nWe also find that the students who were rated highly while answering the first\ninterview question were also rated highly overall (i.e., first impression\nmatters). Finally, our MIT Interview dataset will be made available to other\nresearchers to further validate and expand our findings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 05:49:26 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Naim", "Iftekhar", "", "Ehsan"], ["Tanveer", "M. Iftekhar", "", "Ehsan"], ["Gildea", "Daniel", "", "Ehsan"], ["Mohammed", "", "", "Ehsan"], ["Hoque", "", ""]]}, {"id": "1504.03451", "submitter": "Song-Ju Kim Dr.", "authors": "Song-Ju Kim, Makoto Naruse and Masashi Aono", "title": "Harnessing Natural Fluctuations: Analogue Computer for Efficient\n  Socially Maximal Decision Making", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each individual handles many tasks of finding the most profitable option from\na set of options that stochastically provide rewards. Our society comprises a\ncollection of such individuals, and the society is expected to maximise the\ntotal rewards, while the individuals compete for common rewards. Such\ncollective decision making is formulated as the `competitive multi-armed bandit\nproblem (CBP)', requiring a huge computational cost. Herein, we demonstrate a\nprototype of an analog computer that efficiently solves CBPs by exploiting the\nphysical dynamics of numerous fluids in coupled cylinders. This device enables\nthe maximisation of the total rewards for the society without paying the\nconventionally required computational cost; this is because the fluids estimate\nthe reward probabilities of the options for the exploitation of past knowledge\nand generate random fluctuations for the exploration of new knowledge. Our\nresults suggest that to optimise the social rewards, the utilisation of\nfluid-derived natural fluctuations is more advantageous than applying\nartificial external fluctuations. Our analog computing scheme is expected to\ntrigger further studies for harnessing the huge computational power of natural\nphenomena for resolving a wide variety of complex problems in modern\ninformation society.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 08:30:27 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Kim", "Song-Ju", ""], ["Naruse", "Makoto", ""], ["Aono", "Masashi", ""]]}, {"id": "1504.03558", "submitter": "Nguyen Minh van", "authors": "Nguyen Van Minh and Le Hoang Son", "title": "Fuzzy approaches to context variable in fuzzy geographically weighted\n  clustering", "comments": "11 pages", "journal-ref": null, "doi": "10.5121/csit.2015.50503", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Fuzzy Geographically Weighted Clustering (FGWC) is considered as a suitable\ntool for the analysis of geo-demographic data that assists the provision and\nplanning of products and services to local people. Context variables were\nattached to FGWC in order to accelerate the computing speed of the algorithm\nand to focus the results on the domain of interests. Nonetheless, the\ndetermination of exact, crisp values of the context variable is a hard task. In\nthis paper, we propose two novel methods using fuzzy approaches for that\ndetermination. A numerical example is given to illustrate the uses of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 10:34:02 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Van Minh", "Nguyen", ""], ["Son", "Le Hoang", ""]]}, {"id": "1504.03592", "submitter": "Louise Dennis Dr", "authors": "Louise A. Dennis, Michael Fisher, Alan F. T. Winfield", "title": "Towards Verifiably Ethical Robot Behaviour", "comments": "Presented at the 1st International Workshop on AI and Ethics, Sunday\n  25th January 2015, Hill Country A, Hyatt Regency Austin. Will appear in the\n  workshop proceedings published by AAAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring that autonomous systems work ethically is both complex and\ndifficult. However, the idea of having an additional `governor' that assesses\noptions the system has, and prunes them to select the most ethical choices is\nwell understood. Recent work has produced such a governor consisting of a\n`consequence engine' that assesses the likely future outcomes of actions then\napplies a Safety/Ethical logic to select actions. Although this is appealing,\nit is impossible to be certain that the most ethical options are actually\ntaken. In this paper we extend and apply a well-known agent verification\napproach to our consequence engine, allowing us to verify the correctness of\nits ethical decision-making.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 15:49:40 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Dennis", "Louise A.", ""], ["Fisher", "Michael", ""], ["Winfield", "Alan F. T.", ""]]}, {"id": "1504.03659", "submitter": "Azad Dehghan Mr", "authors": "Azad Dehghan", "title": "Temporal ordering of clinical events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes a minimalistic set of methods engineered to anchor\nclinical events onto a temporal space. Specifically, we describe methods to\nextract clinical events (e.g., Problems, Treatments and Tests), temporal\nexpressions (i.e., time, date, duration, and frequency), and temporal links\n(e.g., Before, After, Overlap) between events and temporal entities. These\nmethods are developed and validated using high quality datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 18:48:58 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Dehghan", "Azad", ""]]}, {"id": "1504.03874", "submitter": "Thomas Burger", "authors": "Thomas Burger", "title": "Bridging belief function theory to modern machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is a quickly evolving field which now looks really different\nfrom what it was 15 years ago, when classification and clustering were major\nissues. This document proposes several trends to explore the new questions of\nmodern machine learning, with the strong afterthought that the belief function\nframework has a major role to play.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 12:04:58 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Burger", "Thomas", ""]]}, {"id": "1504.04716", "submitter": "Vishal Shukla", "authors": "Vishal Shukla", "title": "Gap Analysis of Natural Language Processing Systems with respect to\n  Linguistic Modality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modality is one of the important components of grammar in linguistics. It\nlets speaker to express attitude towards, or give assessment or potentiality of\nstate of affairs. It implies different senses and thus has different\nperceptions as per the context. This paper presents an account showing the gap\nin the functionality of the current state of art Natural Language Processing\n(NLP) systems. The contextual nature of linguistic modality is studied. In this\npaper, the works and logical approaches employed by Natural Language Processing\nsystems dealing with modality are reviewed. It sees human cognition and\nintelligence as multi-layered approach that can be implemented by intelligent\nsystems for learning. Lastly, current flow of research going on within this\nfield is talked providing futurology.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 13:28:59 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Shukla", "Vishal", ""]]}, {"id": "1504.04802", "submitter": "Ryuta Arisaka", "authors": "Ryuta Arisaka", "title": "Gradual Classical Logic for Attributed Objects - Extended in\n  Re-Presentation", "comments": "arXiv admin note: substantial text overlap with arXiv:1404.6036", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our understanding about things is conceptual. By stating that we reason about\nobjects, it is in fact not the objects but concepts referring to them that we\nmanipulate. Now, so long just as we acknowledge infinitely extending notions\nsuch as space, time, size, colour, etc, - in short, any reasonable quality -\ninto which an object is subjected, it becomes infeasible to affirm atomicity in\nthe concept referring to the object. However, formal/symbolic logics typically\npresume atomic entities upon which other expressions are built. Can we reflect\nour intuition about the concept onto formal/symbolic logics at all? I assure\nthat we can, but the usual perspective about the atomicity needs inspected. In\nthis work, I present gradual logic which materialises the observation that we\ncannot tell apart whether a so-regarded atomic entity is atomic or is just\natomic enough not to be considered non-atomic. The motivation is to capture\ncertain phenomena that naturally occur around concepts with attributes,\nincluding presupposition and contraries. I present logical particulars of the\nlogic, which is then mapped onto formal semantics. Two linguistically\ninteresting semantics will be considered. Decidability is shown.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 07:03:17 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Arisaka", "Ryuta", ""]]}, {"id": "1504.04850", "submitter": "Adway Mitra", "authors": "Adway Mitra", "title": "Exploring Bayesian Models for Multi-level Clustering of Hierarchically\n  Grouped Sequential Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of Bayesian models have been proposed for data that is divided\nhierarchically into groups. These models aim to cluster the data at different\nlevels of grouping, by assigning a mixture component to each datapoint, and a\nmixture distribution to each group. Multi-level clustering is facilitated by\nthe sharing of these components and distributions by the groups. In this paper,\nwe introduce the concept of Degree of Sharing (DoS) for the mixture components\nand distributions, with an aim to analyze and classify various existing models.\nNext we introduce a generalized hierarchical Bayesian model, of which the\nexisting models can be shown to be special cases. Unlike most of these models,\nour model takes into account the sequential nature of the data, and various\nother temporal structures at different levels while assigning mixture\ncomponents and distributions. We show one specialization of this model aimed at\nhierarchical segmentation of news transcripts, and present a Gibbs Sampling\nbased inference algorithm for it. We also show experimentally that the proposed\nmodel outperforms existing models for the same task.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 16:37:46 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Mitra", "Adway", ""]]}, {"id": "1504.04909", "submitter": "Jeff Clune Jeff Clune", "authors": "Jean-Baptiste Mouret, Jeff Clune", "title": "Illuminating search spaces by mapping elites", "comments": "Early draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.RO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many fields use search algorithms, which automatically explore a search space\nto find high-performing solutions: chemists search through the space of\nmolecules to discover new drugs; engineers search for stronger, cheaper, safer\ndesigns, scientists search for models that best explain data, etc. The goal of\nsearch algorithms has traditionally been to return the single\nhighest-performing solution in a search space. Here we describe a new,\nfundamentally different type of algorithm that is more useful because it\nprovides a holistic view of how high-performing solutions are distributed\nthroughout a search space. It creates a map of high-performing solutions at\neach point in a space defined by dimensions of variation that a user gets to\nchoose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites)\nalgorithm illuminates search spaces, allowing researchers to understand how\ninteresting attributes of solutions combine to affect performance, either\npositively or, equally of interest, negatively. For example, a drug company may\nwish to understand how performance changes as the size of molecules and their\ncost-to-produce vary. MAP-Elites produces a large diversity of high-performing,\nyet qualitatively different solutions, which can be more helpful than a single,\nhigh-performing solution. Interestingly, because MAP-Elites explores more of\nthe search space, it also tends to find a better overall solution than\nstate-of-the-art search algorithms. We demonstrate the benefits of this new\nalgorithm in three different problem domains ranging from producing modular\nneural networks to designing simulated and real soft robots. Because MAP-\nElites (1) illuminates the relationship between performance and dimensions of\ninterest in solutions, (2) returns a set of high-performing, yet diverse\nsolutions, and (3) improves finding a single, best solution, it will advance\nscience and engineering.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 01:17:00 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Mouret", "Jean-Baptiste", ""], ["Clune", "Jeff", ""]]}, {"id": "1504.04914", "submitter": "Peng Yang", "authors": "Ke Tang, Peng Yang, Xin Yao", "title": "Negatively Correlated Search", "comments": null, "journal-ref": "IEEE Journal on Selected Areas in Communications, Vol. 34, Issue\n  3, pp. 1-9, March 2016", "doi": "10.1109/JSAC.2016.2525458", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary Algorithms (EAs) have been shown to be powerful tools for\ncomplex optimization problems, which are ubiquitous in both communication and\nbig data analytics. This paper presents a new EA, namely Negatively Correlated\nSearch (NCS), which maintains multiple individual search processes in parallel\nand models the search behaviors of individual search processes as probability\ndistributions. NCS explicitly promotes negatively correlated search behaviors\nby encouraging differences among the probability distributions (search\nbehaviors). By this means, individual search processes share information and\ncooperate with each other to search diverse regions of a search space, which\nmakes NCS a promising method for non-convex optimization. The cooperation\nscheme of NCS could also be regarded as a novel diversity preservation scheme\nthat, different from other existing schemes, directly promotes diversity at the\nlevel of search behaviors rather than merely trying to maintain diversity among\ncandidate solutions. Empirical studies showed that NCS is competitive to\nwell-established search methods in the sense that NCS achieved the best overall\nperformance on 20 multimodal (non-convex) continuous optimization problems. The\nadvantages of NCS over state-of-the-art approaches are also demonstrated with a\ncase study on the synthesis of unequally spaced linear antenna arrays.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 01:51:39 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 02:41:37 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Tang", "Ke", ""], ["Yang", "Peng", ""], ["Yao", "Xin", ""]]}, {"id": "1504.05095", "submitter": "Bassam AlKindy Mr.", "authors": "Bassam AlKindy, Christophe Guyeux, Jean-Fran\\c{c}ois Couchot, Michel\n  Salomon, Christian Parisod, and Jacques M. Bahi", "title": "Hybrid Genetic Algorithm and Lasso Test Approach for Inferring Well\n  Supported Phylogenetic Trees based on Subsets of Chloroplastic Core Genes", "comments": "15 pages, 7 figures, 2nd International Conference on Algorithms for\n  Computational Biology, AlCoB 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of completely sequenced chloroplast genomes increases rapidly\nevery day, leading to the possibility to build large scale phylogenetic trees\nof plant species. Considering a subset of close plant species defined according\nto their chloroplasts, the phylogenetic tree that can be inferred by their core\ngenes is not necessarily well supported, due to the possible occurrence of\n\"problematic\" genes (i.e., homoplasy, incomplete lineage sorting, horizontal\ngene transfers, etc.) which may blur phylogenetic signal. However, a\ntrustworthy phylogenetic tree can still be obtained if the number of\nproblematic genes is low, the problem being to determine the largest subset of\ncore genes that produces the best supported tree. To discard problematic genes\nand due to the overwhelming number of possible combinations, we propose an\nhybrid approach that embeds both genetic algorithms and statistical tests.\nGiven a set of organisms, the result is a pipeline of many stages for the\nproduction of well supported phylogenetic trees. The proposal has been applied\nto different cases of plant families, leading to encouraging results for these\nfamilies.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 15:50:46 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["AlKindy", "Bassam", ""], ["Guyeux", "Christophe", ""], ["Couchot", "Jean-Fran\u00e7ois", ""], ["Salomon", "Michel", ""], ["Parisod", "Christian", ""], ["Bahi", "Jacques M.", ""]]}, {"id": "1504.05122", "submitter": "Reinaldo Augusto Uribe Muriel", "authors": "Reinaldo Uribe Muriel, Fernando Lozando and Charles Anderson", "title": "Optimal Nudging: Solving Average-Reward Semi-Markov Decision Processes\n  as a Minimal Sequence of Cumulative Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel method to solve average-reward semi-Markov\ndecision processes, by reducing them to a minimal sequence of cumulative reward\nproblems. The usual solution methods for this type of problems update the gain\n(optimal average reward) immediately after observing the result of taking an\naction. The alternative introduced, optimal nudging, relies instead on setting\nthe gain to some fixed value, which transitorily makes the problem a\ncumulative-reward task, solving it by any standard reinforcement learning\nmethod, and only then updating the gain in a way that minimizes uncertainty in\na minmax sense. The rule for optimal gain update is derived by exploiting the\ngeometric features of the w-l space, a simple mapping of the space of policies.\nThe total number of cumulative reward tasks that need to be solved is shown to\nbe small. Some experiments are presented to explore the features of the\nalgorithm and to compare its performance with other approaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 16:58:26 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Muriel", "Reinaldo Uribe", ""], ["Lozando", "Fernando", ""], ["Anderson", "Charles", ""]]}, {"id": "1504.05150", "submitter": "Mark Kaminski", "authors": "Mark Kaminski, Bernardo Cuenca Grau", "title": "Computing Horn Rewritings of Description Logics Ontologies", "comments": "15 pages. To appear in IJCAI-15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of rewriting an ontology O1 expressed in a DL L1 into an\nontology O2 in a Horn DL L2 such that O1 and O2 are equisatisfiable when\nextended with an arbitrary dataset. Ontologies that admit such rewritings are\namenable to reasoning techniques ensuring tractability in data complexity.\nAfter showing undecidability whenever L1 extends ALCF, we focus on devising\nefficiently checkable conditions that ensure existence of a Horn rewriting. By\nlifting existing techniques for rewriting Disjunctive Datalog programs into\nplain Datalog to the case of arbitrary first-order programs with function\nsymbols, we identify a class of ontologies that admit Horn rewritings of\npolynomial size. Our experiments indicate that many real-world ontologies\nsatisfy our sufficient conditions and thus admit polynomial Horn rewritings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 18:39:27 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2015 10:59:25 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Kaminski", "Mark", ""], ["Grau", "Bernardo Cuenca", ""]]}, {"id": "1504.05381", "submitter": "Ryuta Arisaka", "authors": "Ryuta Arisaka", "title": "How do you revise your belief set with %$;@*?", "comments": "Corrected the following: 1. In Definition 1, the function I and Assoc\n  were both defined to map into 2^Props x 2^Props, but they should be clearly\n  into 2^{Props x Props}. 2. In Definition 1, one disjunctive case was being\n  omitted. One case (5th item) was inserted to complete the picture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classic AGM belief revision theory, beliefs are static and do not\nchange their own shape. For instance, if p is accepted by a rational agent, it\nwill remain p to the agent. But such rarely happens to us. Often, when we\naccept some information p, what is actually accepted is not the whole p, but\nonly a portion of it; not necessarily because we select the portion but because\np must be perceived. Only the perceived p is accepted; and the perception is\nsubject to what we already believe (know). What may, however, happen to the\nrest of p that initially escaped our attention? In this work we argue that the\ninvisible part is also accepted to the agent, if only unconsciously. Hence some\nparts of p are accepted as visible, while some other parts as latent, beliefs.\nThe division is not static. As the set of beliefs changes, what were hidden may\nbecome visible. We present a perception-based belief theory that incorporates\nlatent beliefs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 10:44:07 GMT"}, {"version": "v2", "created": "Thu, 21 May 2015 11:45:49 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2016 03:29:16 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Arisaka", "Ryuta", ""]]}, {"id": "1504.05411", "submitter": "Daniel Nyga", "authors": "Daniel Nyga and Michael Beetz", "title": "Reasoning about Unmodelled Concepts - Incorporating Class Taxonomies in\n  Probabilistic Relational Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in the application of first-order probabilistic methods is the\nenormous size of graphical models they imply. The size results from the\npossible worlds that can be generated by a domain of objects and relations. One\nof the reasons for this explosion is that so far the approaches do not\nsufficiently exploit the structure and similarity of possible worlds in order\nto encode the models more compactly. We propose fuzzy inference in Markov logic\nnetworks, which enables the use of taxonomic knowledge as a source of imposing\nstructure onto possible worlds. We show that by exploiting this structure,\nprobability distributions can be represented more compactly and that the\nreasoning systems become capable of reasoning about concepts not contained in\nthe probabilistic knowledge base.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 13:04:24 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Nyga", "Daniel", ""], ["Beetz", "Michael", ""]]}, {"id": "1504.05457", "submitter": "Yury Kashnitsky", "authors": "Yury Kashnitsky, Sergei O. Kuznetsov", "title": "Graphlet-based lazy associative graph classification", "comments": "This paper has been withdrawn by the author due to the incomplete set\n  of necessary definitions and experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the graph classification problem and introduces a\nmodification of the lazy associative classification method to efficiently\nhandle intersections of graphs. Graph intersections are approximated with all\ncommon subgraphs up to a fixed size similarly to what is done with graphlet\nkernels. We explain the idea of the algorithm with a toy example and describe\nour experiments with a predictive toxicology dataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 15:12:45 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 20:46:47 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Kashnitsky", "Yury", ""], ["Kuznetsov", "Sergei O.", ""]]}, {"id": "1504.05469", "submitter": "Yury Kashnitsky", "authors": "Yury Kashnitsky", "title": "Visual analytics in FCA-based clustering", "comments": "11 pages, 3 figures, 2 algorithms, 3rd International Conference on\n  Analysis of Images, Social Networks and Texts (AIST'2014). in Supplementary\n  Proceedings of the 3rd International Conference on Analysis of Images, Social\n  Networks and Texts (AIST 2014), Vol. 1197, CEUR-WS.org, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual analytics is a subdomain of data analysis which combines both human\nand machine analytical abilities and is applied mostly in decision-making and\ndata mining tasks. Triclustering, based on Formal Concept Analysis (FCA), was\ndeveloped to detect groups of objects with similar properties under similar\nconditions. It is used in Social Network Analysis (SNA) and is a basis for\ncertain types of recommender systems. The problem of triclustering algorithms\nis that they do not always produce meaningful clusters. This article describes\na specific triclustering algorithm and a prototype of a visual analytics\nplatform for working with obtained clusters. This tool is designed as a testing\nframeworkis and is intended to help an analyst to grasp the results of\ntriclustering and recommender algorithms, and to make decisions on\nmeaningfulness of certain triclusters and recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 15:28:23 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Kashnitsky", "Yury", ""]]}, {"id": "1504.05603", "submitter": "Caspar Oesterheld", "authors": "Caspar Oesterheld", "title": "Formalizing Preference Utilitarianism in Physical World Models", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": "10.1007/s11229-015-0883-1", "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most ethical work is done at a low level of formality. This makes practical\nmoral questions inaccessible to formal and natural sciences and can lead to\nmisunderstandings in ethical discussion. In this paper, we use Bayesian\ninference to introduce a formalization of preference utilitarianism in physical\nworld models, specifically cellular automata. Even though our formalization is\nnot immediately applicable, it is a first step in providing ethics and\nultimately the question of how to \"make the world better\" with a formal basis.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 20:35:54 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 10:37:33 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 15:46:32 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Oesterheld", "Caspar", ""]]}, {"id": "1504.05651", "submitter": "Kun Zhang", "authors": "Kun Zhang, Jiji Zhang, Bernhard Sch\\\"olkopf", "title": "Distinguishing Cause from Effect Based on Exogeneity", "comments": "11 pages, 4 figures, published in Proceedings of the 15th conference\n  on Theoretical Aspects of Rationality and Knowledge (TARK'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in structural equation modeling have produced several\nmethods that can usually distinguish cause from effect in the two-variable\ncase. For that purpose, however, one has to impose substantial structural\nconstraints or smoothness assumptions on the functional causal models. In this\npaper, we consider the problem of determining the causal direction from a\nrelated but different point of view, and propose a new framework for causal\ndirection determination. We show that it is possible to perform causal\ninference based on the condition that the cause is \"exogenous\" for the\nparameters involved in the generating process from the cause to the effect. In\nthis way, we avoid the structural constraints required by the SEM-based\napproaches. In particular, we exploit nonparametric methods to estimate\nmarginal and conditional distributions, and propose a bootstrap-based approach\nto test for the exogeneity condition; the testing results indicate the causal\ndirection between two variables. The proposed method is validated on both\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 04:35:33 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Zhang", "Kun", ""], ["Zhang", "Jiji", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1504.05696", "submitter": "Murray Shanahan", "authors": "Murray Shanahan", "title": "Ascribing Consciousness to Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper critically assesses the anti-functionalist stance on consciousness\nadopted by certain advocates of integrated information theory (IIT), a\ncorollary of which is that human-level artificial intelligence implemented on\nconventional computing hardware is necessarily not conscious. The critique\ndraws on variations of a well-known gradual neuronal replacement thought\nexperiment, as well as bringing out tensions in IIT's treatment of\nself-knowledge. The aim, though, is neither to reject IIT outright nor to\nchampion functionalism in particular. Rather, it is suggested that both ideas\nhave something to offer a scientific understanding of consciousness, as long as\nthey are not dressed up as solutions to illusory metaphysical problems. As for\nhuman-level AI, we must await its development before we can decide whether or\nnot to ascribe consciousness to it.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 08:50:16 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2015 08:40:33 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Shanahan", "Murray", ""]]}, {"id": "1504.05811", "submitter": "Michele Colledanchise", "authors": "Michele Colledanchise, Ramviyas Parasuraman, and Petter \\\"Ogren", "title": "Learning of Behavior Trees for Autonomous Agents", "comments": null, "journal-ref": "IEEE Transactions on Games ( Volume: 11 , Issue: 2 , June 2019 )", "doi": "10.1109/TG.2018.2816806", "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Definition of an accurate system model for Automated Planner (AP) is often\nimpractical, especially for real-world problems. Conversely, off-the-shelf\nplanners fail to scale up and are domain dependent. These drawbacks are\ninherited from conventional transition systems such as Finite State Machines\n(FSMs) that describes the action-plan execution generated by the AP. On the\nother hand, Behavior Trees (BTs) represent a valid alternative to FSMs\npresenting many advantages in terms of modularity, reactiveness, scalability\nand domain-independence. In this paper, we propose a model-free AP framework\nusing Genetic Programming (GP) to derive an optimal BT for an autonomous agent\nto achieve a given goal in unknown (but fully observable) environments. We\nillustrate the proposed framework using experiments conducted with an open\nsource benchmark Mario AI for automated generation of BTs that can play the\ngame character Mario to complete a certain level at various levels of\ndifficulty to include enemies and obstacles.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 14:06:06 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Colledanchise", "Michele", ""], ["Parasuraman", "Ramviyas", ""], ["\u00d6gren", "Petter", ""]]}, {"id": "1504.05846", "submitter": "Peter Nightingale", "authors": "James Caldwell and Ian P. Gent and Peter Nightingale", "title": "Generalized Support and Formal Development of Constraint Propagators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint programming is a family of techniques for solving combinatorial\nproblems, where the problem is modelled as a set of decision variables\n(typically with finite domains) and a set of constraints that express relations\namong the decision variables. One key concept in constraint programming is\npropagation: reasoning on a constraint or set of constraints to derive new\nfacts, typically to remove values from the domains of decision variables.\nSpecialised propagation algorithms (propagators) exist for many classes of\nconstraints.\n  The concept of support is pervasive in the design of propagators.\nTraditionally, when a domain value ceases to have support, it may be removed\nbecause it takes part in no solutions. Arc-consistency algorithms such as\nAC2001 make use of support in the form of a single domain value. GAC algorithms\nsuch as GAC-Schema use a tuple of values to support each literal. We generalize\nthese notions of support in two ways. First, we allow a set of tuples to act as\nsupport. Second, the supported object is generalized from a set of literals\n(GAC-Schema) to an entire constraint or any part of it.\n  We design a methodology for developing correct propagators using generalized\nsupport. A constraint is expressed as a family of support properties, which may\nbe proven correct against the formal semantics of the constraint. Using\nCurry-Howard isomorphism to interpret constructive proofs as programs, we show\nhow to derive correct propagators from the constructive proofs of the support\nproperties. The framework is carefully designed to allow efficient algorithms\nto be produced. Derived algorithms may make use of dynamic literal triggers or\nwatched literals for efficiency. Finally, two case studies of deriving\nefficient algorithms are given.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 15:34:56 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 11:50:53 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Caldwell", "James", ""], ["Gent", "Ian P.", ""], ["Nightingale", "Peter", ""]]}, {"id": "1504.05895", "submitter": "Zolzaya Dashdorj", "authors": "Zolzaya Dashdorj and Stanislav Sobolevsky and Luciano Serafini and\n  Fabrizio Antonelli and Carlo Ratti", "title": "Semantic Enrichment of Mobile Phone Data Records Using Background\n  Knowledge", "comments": "40 pages, 34 figures", "journal-ref": "Knowledge-Based Systems, Volume 143, 2018", "doi": "10.1016/j.knosys.2017.11.038", "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every day, billions of mobile network events (i.e. CDRs) are generated by\ncellular phone operator companies. Latent in this data are inspiring insights\nabout human actions and behaviors, the discovery of which is important because\ncontext-aware applications and services hold the key to user-driven,\nintelligent services, which can enhance our everyday lives such as social and\neconomic development, urban planning, and health prevention. The major\nchallenge in this area is that interpreting such a big stream of data requires\na deep understanding of mobile network events' context through available\nbackground knowledge. This article addresses the issues in context awareness\ngiven heterogeneous and uncertain data of mobile network events missing\nreliable information on the context of this activity. The contribution of this\nresearch is a model from a combination of logical and statistical reasoning\nstandpoints for enabling human activity inference in qualitative terms from\nopen geographical data that aimed at improving the quality of human behaviors\nrecognition tasks from CDRs. We use open geographical data, Openstreetmap\n(OSM), as a proxy for predicting the content of human activity in the area. The\nuser study performed in Trento shows that predicted human activities (top\nlevel) match the survey data with around 93% overall accuracy. The extensive\nvalidation for predicting a more specific economic type of human activity\nperformed in Barcelona, by employing credit card transaction data. The analysis\nidentifies that appropriately normalized data on points of interest (POI) is a\ngood proxy for predicting human economical activities, with 84% accuracy on\naverage. So the model is proven to be efficient for predicting the context of\nhuman activity, when its total level could be efficiently observed from cell\nphone data records, missing contextual information however.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 17:34:53 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Dashdorj", "Zolzaya", ""], ["Sobolevsky", "Stanislav", ""], ["Serafini", "Luciano", ""], ["Antonelli", "Fabrizio", ""], ["Ratti", "Carlo", ""]]}, {"id": "1504.05932", "submitter": "Lirong Xia", "authors": "Erika Mackin, Lirong Xia", "title": "Allocating Indivisible Items in Categorized Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate a general class of allocation problems called categorized domain\nallocation problems (CDAPs), where indivisible items from multiple categories\nare allocated to agents without monetary transfer and each agent gets at least\none item per category.\n  We focus on basic CDAPs, where the number of items in each category is equal\nto the number of agents. We characterize serial dictatorships for basic CDAPs\nby a minimal set of three axiomatic properties: strategy-proofness,\nnon-bossiness, and category-wise neutrality. Then, we propose a natural\nextension of serial dictatorships called categorial sequential allocation\nmechanisms (CSAMs), which allocate the items in multiple rounds: in each round,\nthe active agent chooses an item from a designated category. We fully\ncharacterize the worst-case rank efficiency of CSAMs for optimistic and\npessimistic agents, and provide a bound for strategic agents. We also conduct\nexperiments to compare expected rank efficiency of various CSAMs w.r.t. random\ngenerated data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 19:27:28 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Mackin", "Erika", ""], ["Xia", "Lirong", ""]]}, {"id": "1504.05996", "submitter": "Ehsan Variani", "authors": "Ehsan Variani, Kamel Lahouel, Avner Bar-Hen, Bruno Jedynak", "title": "Non-Adaptive Policies for 20 Questions Target Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of target localization with noise is addressed. The target is a\nsample from a continuous random variable with known distribution and the goal\nis to locate it with minimum mean squared error distortion. The localization\nscheme or policy proceeds by queries, or questions, weather or not the target\nbelongs to some subset as it is addressed in the 20-question framework. These\nsubsets are not constrained to be intervals and the answers to the queries are\nnoisy. While this situation is well studied for adaptive querying, this paper\nis focused on the non adaptive querying policies based on dyadic questions. The\nasymptotic minimum achievable distortion under such policies is derived.\nFurthermore, a policy named the Aurelian1 is exhibited which achieves\nasymptotically this distortion.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 22:05:40 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 00:33:44 GMT"}, {"version": "v3", "created": "Sat, 2 May 2015 00:46:32 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Variani", "Ehsan", ""], ["Lahouel", "Kamel", ""], ["Bar-Hen", "Avner", ""], ["Jedynak", "Bruno", ""]]}, {"id": "1504.06058", "submitter": "Haifeng Xu", "authors": "Haifeng Xu, Albert X. Jiang, Arunesh Sinha, Zinovi Rabinovich, Shaddin\n  Dughmi, Milind Tambe", "title": "Security Games with Information Leakage: Modeling and Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most models of Stackelberg security games assume that the attacker only knows\nthe defender's mixed strategy, but is not able to observe (even partially) the\ninstantiated pure strategy. Such partial observation of the deployed pure\nstrategy -- an issue we refer to as information leakage -- is a significant\nconcern in practical applications. While previous research on patrolling games\nhas considered the attacker's real-time surveillance, our settings, therefore\nmodels and techniques, are fundamentally different. More specifically, after\ndescribing the information leakage model, we start with an LP formulation to\ncompute the defender's optimal strategy in the presence of leakage. Perhaps\nsurprisingly, we show that a key subproblem to solve this LP (more precisely,\nthe defender oracle) is NP-hard even for the simplest of security game models.\nWe then approach the problem from three possible directions: efficient\nalgorithms for restricted cases, approximation algorithms, and heuristic\nalgorithms for sampling that improves upon the status quo. Our experiments\nconfirm the necessity of handling information leakage and the advantage of our\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 06:49:49 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 05:08:52 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Xu", "Haifeng", ""], ["Jiang", "Albert X.", ""], ["Sinha", "Arunesh", ""], ["Rabinovich", "Zinovi", ""], ["Dughmi", "Shaddin", ""], ["Tambe", "Milind", ""]]}, {"id": "1504.06078", "submitter": "Nicolas Turenne", "authors": "Nicolas Turenne, Tien Phan", "title": "x.ent: R Package for Entities and Relations Extraction based on\n  Unsupervised Learning and Document Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction with accurate precision is still a challenge when\nprocessing full text databases. We propose an approach based on cooccurrence\nanalysis in each document for which we used document organization to improve\naccuracy of relation extraction. This approach is implemented in a R package\ncalled \\emph{x.ent}. Another facet of extraction relies on use of extracted\nrelation into a querying system for expert end-users. Two datasets had been\nused. One of them gets interest from specialists of epidemiology in plant\nhealth. For this dataset usage is dedicated to plant-disease exploration\nthrough agricultural information news. An open-data platform exploits exports\nfrom \\emph{x.ent} and is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 08:28:01 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Turenne", "Nicolas", ""], ["Phan", "Tien", ""]]}, {"id": "1504.06158", "submitter": "Pierre Crescenzo", "authors": "Isabelle Mirbel (INRIA Sophia Antipolis / Laboratoire I3S), Pierre\n  Crescenzo (I3S)", "title": "From End-User's Requirements to Web Services Retrieval: A Semantic and\n  Intention-Driven Approach", "comments": "{\\'e}galement rapport de recherche I3S/RR--2010-03--FR in\n  Computational Materials Science (2015). arXiv admin note: substantial text\n  overlap with arXiv:1502.06735", "journal-ref": null, "doi": "10.1007/978-3-642-14319-9_3", "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present SATIS, a framework to derive Web Service\nspecifications from end-user's requirements in order to opera-tionalise\nbusiness processes in the context of a specific application domain. The aim of\nSATIS is to provide to neuroscientists, which are not familiar with computer\nscience, a complete solution to easily find a set of Web Services to implement\nan image processing pipeline. More precisely, our framework offers the\ncapability to capture high-level end-user's requirements in an iterative and\nincremental way and to turn them into queries to retrieve Web Services\ndescription. The whole framework relies on reusable and combinable elements\nwhich can be shared out by a community of users sharing some interest or\nproblems for a given topic. In our approach, we adopt Web semantic languages\nand models as a unified framework to deal with end-user's requirements and Web\nService descriptions in order to take advantage of their reasoning and\ntraceability capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 12:56:44 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Mirbel", "Isabelle", "", "INRIA Sophia Antipolis / Laboratoire I3S"], ["Crescenzo", "Pierre", "", "I3S"]]}, {"id": "1504.06341", "submitter": "Burkhard C. Schipper", "authors": "Burkhard C. Schipper", "title": "Strategic Teaching and Learning in Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that there are uncoupled learning heuristics leading to Nash\nequilibrium in all finite games. Why should players use such learning\nheuristics and where could they come from? We show that there is no uncoupled\nlearning heuristic leading to Nash equilibrium in all finite games that a\nplayer has an incentive to adopt, that would be evolutionary stable or that\ncould \"learn itself\". Rather, a player has an incentive to strategically teach\nsuch a learning opponent in order secure at least the Stackelberg leader\npayoff. The impossibility result remains intact when restricted to the classes\nof generic games, two-player games, potential games, games with strategic\ncomplements or 2x2 games, in which learning is known to be \"nice\". More\ngenerally, it also applies to uncoupled learning heuristics leading to\ncorrelated equilibria, rationalizable outcomes, iterated admissible outcomes,\nor minimal curb sets. A possibility result restricted to \"strategically\ntrivial\" games fails if some generic games outside this class are considered as\nwell.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 20:49:16 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Schipper", "Burkhard C.", ""]]}, {"id": "1504.06366", "submitter": "Sakthithasan Sripirakas", "authors": "Sripirakas Sakthithasan, Russel Pears, Albert Bifet and Bernhard\n  Pfahringer", "title": "Use of Ensembles of Fourier Spectra in Capturing Recurrent Concepts in\n  Data Streams", "comments": "This paper has been accepted for IJCNN 2015 conference, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we apply ensembles of Fourier encoded spectra to capture\nand mine recurring concepts in a data stream environment. Previous research\nshowed that compact versions of Decision Trees can be obtained by applying the\nDiscrete Fourier Transform to accurately capture recurrent concepts in a data\nstream. However, in highly volatile environments where new concepts emerge\noften, the approach of encoding each concept in a separate spectrum is no\nlonger viable due to memory overload and thus in this research we present an\nensemble approach that addresses this problem. Our empirical results on real\nworld data and synthetic data exhibiting varying degrees of recurrence reveal\nthat the ensemble approach outperforms the single spectrum approach in terms of\nclassification accuracy, memory and execution time.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 23:34:39 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Sakthithasan", "Sripirakas", ""], ["Pears", "Russel", ""], ["Bifet", "Albert", ""], ["Pfahringer", "Bernhard", ""]]}, {"id": "1504.06374", "submitter": "Vijay Saraswat", "authors": "Cristina Cornelio and Andrea Loreggia and Vijay Saraswat", "title": "Logical Conditional Preference Theories", "comments": "15 pages, 1 figure, submitted to CP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CP-nets represent the dominant existing framework for expressing qualitative\nconditional preferences between alternatives, and are used in a variety of\nareas including constraint solving. Over the last fifteen years, a significant\nliterature has developed exploring semantics, algorithms, implementation and\nuse of CP-nets. This paper introduces a comprehensive new framework for\nconditional preferences: logical conditional preference theories (LCP\ntheories). To express preferences, the user specifies arbitrary (constraint)\nDatalog programs over a binary ordering relation on outcomes. We show how LCP\ntheories unify and generalize existing conditional preference proposals, and\nleverage the rich semantic, algorithmic and implementation frameworks of\nDatalog.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 02:07:36 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Cornelio", "Cristina", ""], ["Loreggia", "Andrea", ""], ["Saraswat", "Vijay", ""]]}, {"id": "1504.06423", "submitter": "Adish Singla", "authors": "Adish Singla, Eric Horvitz, Pushmeet Kohli, Ryen White, Andreas Krause", "title": "Information Gathering in Networks via Active Exploration", "comments": "Longer version of IJCAI'15 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should we gather information in a network, where each node's visibility\nis limited to its local neighborhood? This problem arises in numerous\nreal-world applications, such as surveying and task routing in social networks,\nteam formation in collaborative networks and experimental design with\ndependency constraints. Often the informativeness of a set of nodes can be\nquantified via a submodular utility function. Existing approaches for\nsubmodular optimization, however, require that the set of all nodes that can be\nselected is known ahead of time, which is often unrealistic. In contrast, we\npropose a novel model where we start our exploration from an initial node, and\nnew nodes become visible and available for selection only once one of their\nneighbors has been chosen. We then present a general algorithm NetExp for this\nproblem, and provide theoretical bounds on its performance dependent on\nstructural properties of the underlying network. We evaluate our methodology on\nvarious simulated problem instances as well as on data collected from social\nquestion answering system deployed within a large enterprise.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 08:41:08 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 15:39:42 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Singla", "Adish", ""], ["Horvitz", "Eric", ""], ["Kohli", "Pushmeet", ""], ["White", "Ryen", ""], ["Krause", "Andreas", ""]]}, {"id": "1504.06529", "submitter": "Dmitriy Zheleznyakov", "authors": "Bernardo Cuenca Grau, Evgeny Kharlamov, Egor V. Kostylev, Dmitriy\n  Zheleznyakov", "title": "Controlled Query Evaluation for Datalog and OWL 2 Profile Ontologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study confidentiality enforcement in ontologies under the Controlled Query\nEvaluation framework, where a policy specifies the sensitive information and a\ncensor ensures that query answers that may compromise the policy are not\nreturned. We focus on censors that ensure confidentiality while maximising\ninformation access, and consider both Datalog and the OWL 2 profiles as\nontology languages.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 14:49:18 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Grau", "Bernardo Cuenca", ""], ["Kharlamov", "Evgeny", ""], ["Kostylev", "Egor V.", ""], ["Zheleznyakov", "Dmitriy", ""]]}, {"id": "1504.06665", "submitter": "Jonathan May", "authors": "Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, Jonathan May", "title": "Using Syntax-Based Machine Translation to Parse English into Abstract\n  Meaning Representation", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parser for Abstract Meaning Representation (AMR). We treat\nEnglish-to-AMR conversion within the framework of string-to-tree, syntax-based\nmachine translation (SBMT). To make this work, we transform the AMR structure\ninto a form suitable for the mechanics of SBMT and useful for modeling. We\nintroduce an AMR-specific language model and add data and features drawn from\nsemantic resources. Our resulting AMR parser improves upon state-of-the-art\nresults by 7 Smatch points.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 23:24:10 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2015 16:36:13 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Pust", "Michael", ""], ["Hermjakob", "Ulf", ""], ["Knight", "Kevin", ""], ["Marcu", "Daniel", ""], ["May", "Jonathan", ""]]}, {"id": "1504.06700", "submitter": "Kedian Mu", "authors": "Kedian Mu and Kewen Wang and Lian Wen", "title": "Preferential Multi-Context Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-context systems (MCS) presented by Brewka and Eiter can be considered\nas a promising way to interlink decentralized and heterogeneous knowledge\ncontexts. In this paper, we propose preferential multi-context systems (PMCS),\nwhich provide a framework for incorporating a total preorder relation over\ncontexts in a multi-context system. In a given PMCS, its contexts are divided\ninto several parts according to the total preorder relation over them,\nmoreover, only information flows from a context to ones of the same part or\nless preferred parts are allowed to occur. As such, the first $l$ preferred\nparts of an PMCS always fully capture the information exchange between contexts\nof these parts, and then compose another meaningful PMCS, termed the\n$l$-section of that PMCS. We generalize the equilibrium semantics for an MCS to\nthe (maximal) $l_{\\leq}$-equilibrium which represents belief states at least\nacceptable for the $l$-section of an PMCS. We also investigate inconsistency\nanalysis in PMCS and related computational complexity issues.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 08:20:37 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Mu", "Kedian", ""], ["Wang", "Kewen", ""], ["Wen", "Lian", ""]]}, {"id": "1504.06825", "submitter": "Patrick O. Glauner", "authors": "Patrick O. Glauner", "title": "Comparison of Training Methods for Deep Neural Networks", "comments": "50 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the difficulties of training neural networks and in\nparticular deep neural networks. It then provides a literature review of\ntraining methods for deep neural networks, with a focus on pre-training. It\nfocuses on Deep Belief Networks composed of Restricted Boltzmann Machines and\nStacked Autoencoders and provides an outreach on further and alternative\napproaches. It also includes related practical recommendations from the\nliterature on training them. In the second part, initial experiments using some\nof the covered methods are performed on two databases. In particular,\nexperiments are performed on the MNIST hand-written digit dataset and on facial\nemotion data from a Kaggle competition. The results are discussed in the\ncontext of results reported in other research papers. An error rate lower than\nthe best contribution to the Kaggle competition is achieved using an optimized\nStacked Autoencoder.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 14:09:17 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Glauner", "Patrick O.", ""]]}, {"id": "1504.06848", "submitter": "David Tolpin", "authors": "David Tolpin, Frank Wood", "title": "Maximum a Posteriori Estimation by Search in Probabilistic Programs", "comments": "To appear in proceedings of SOCS15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approximate search algorithm for fast maximum a posteriori\nprobability estimation in probabilistic programs, which we call Bayesian ascent\nMonte Carlo (BaMC). Probabilistic programs represent probabilistic models with\nvarying number of mutually dependent finite, countable, and continuous random\nvariables. BaMC is an anytime MAP search algorithm applicable to any\ncombination of random variables and dependencies. We compare BaMC to other MAP\nestimation algorithms and show that BaMC is faster and more robust on a range\nof probabilistic models.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 17:23:06 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Tolpin", "David", ""], ["Wood", "Frank", ""]]}, {"id": "1504.06936", "submitter": "Alejandro Metke Jimenez", "authors": "Alejandro Metke-Jimenez, Sarvnaz Karimi", "title": "Concept Extraction to Identify Adverse Drug Reactions in Medical Forums:\n  A Comparison of Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is becoming an increasingly important source of information to\ncomplement traditional pharmacovigilance methods. In order to identify signals\nof potential adverse drug reactions, it is necessary to first identify medical\nconcepts in the social media text. Most of the existing studies use\ndictionary-based methods which are not evaluated independently from the overall\nsignal detection task.\n  We compare different approaches to automatically identify and normalise\nmedical concepts in consumer reviews in medical forums. Specifically, we\nimplement several dictionary-based methods popular in the relevant literature,\nas well as a method we suggest based on a state-of-the-art machine learning\nmethod for entity recognition. MetaMap, a popular biomedical concept extraction\ntool, is used as a baseline. Our evaluations were performed in a controlled\nsetting on a common corpus which is a collection of medical forum posts\nannotated with concepts and linked to controlled vocabularies such as MedDRA\nand SNOMED CT.\n  To our knowledge, our study is the first to systematically examine the effect\nof popular concept extraction methods in the area of signal detection for\nadverse reactions. We show that the choice of algorithm or controlled\nvocabulary has a significant impact on concept extraction, which will impact\nthe overall signal detection process. We also show that our proposed machine\nlearning approach significantly outperforms all the other methods in\nidentification of both adverse reactions and drugs, even when trained with a\nrelatively small set of annotated text.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 05:56:13 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Metke-Jimenez", "Alejandro", ""], ["Karimi", "Sarvnaz", ""]]}, {"id": "1504.07020", "submitter": "Dov Gabbay", "authors": "D. M. Gabbay", "title": "Theory of Semi-Instantiation in Abstract Argumentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study instantiated abstract argumentation frames of the form $(S,R,I)$,\nwhere $(S,R)$ is an abstract argumentation frame and where the arguments $x$ of\n$S$ are instantiated by $I(x)$ as well formed formulas of a well known logic,\nfor example as Boolean formulas or as predicate logic formulas or as modal\nlogic formulas. We use the method of conceptual analysis to derive the\nproperties of our proposed system. We seek to define the notion of complete\nextensions for such systems and provide algorithms for finding such extensions.\nWe further develop a theory of instantiation in the abstract, using the\nframework of Boolean attack formations and of conjunctive and disjunctive\nattacks. We discuss applications and compare critically with the existing\nrelated literature.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 10:48:28 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Gabbay", "D. M.", ""]]}, {"id": "1504.07107", "submitter": "Wenbo Hu", "authors": "Wenbo Hu, Jun Zhu, Bo Zhang", "title": "Fast Sampling for Bayesian Max-Margin Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian max-margin models have shown superiority in various practical\napplications, such as text categorization, collaborative prediction, social\nnetwork link prediction and crowdsourcing, and they conjoin the flexibility of\nBayesian modeling and predictive strengths of max-margin learning. However,\nMonte Carlo sampling for these models still remains challenging, especially for\napplications that involve large-scale datasets. In this paper, we present the\nstochastic subgradient Hamiltonian Monte Carlo (HMC) methods, which are easy to\nimplement and computationally efficient. We show the approximate detailed\nbalance property of subgradient HMC which reveals a natural and validated\ngeneralization of the ordinary HMC. Furthermore, we investigate the variants\nthat use stochastic subsampling and thermostats for better scalability and\nmixing. Using stochastic subgradient Markov Chain Monte Carlo (MCMC), we\nefficiently solve the posterior inference task of various Bayesian max-margin\nmodels and extensive experimental results demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 14:29:40 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 12:28:41 GMT"}, {"version": "v3", "created": "Sat, 9 May 2015 07:26:02 GMT"}, {"version": "v4", "created": "Sat, 20 Jun 2015 12:53:35 GMT"}, {"version": "v5", "created": "Tue, 18 Oct 2016 13:44:30 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Hu", "Wenbo", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1504.07168", "submitter": "Spyros Angelopoulos", "authors": "Spyros Angelopoulos", "title": "Further Connections Between Contract-Scheduling and Ray-Searching\n  Problems", "comments": "Full version of conference paper, to appear in Proceedings of IJCAI\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses two classes of different, yet interrelated optimization\nproblems. The first class of problems involves a robot that must locate a\nhidden target in an environment that consists of a set of concurrent rays. The\nsecond class pertains to the design of interruptible algorithms by means of a\nschedule of contract algorithms. We study several variants of these families of\nproblems, such as searching and scheduling with probabilistic considerations,\nredundancy and fault-tolerance issues, randomized strategies, and trade-offs\nbetween performance and preemptions. For many of these problems we present the\nfirst known results that apply to multi-ray and multi-problem domains. Our\nobjective is to demonstrate that several well-motivated settings can be\naddressed using the same underlying approach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 17:23:39 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Angelopoulos", "Spyros", ""]]}, {"id": "1504.07182", "submitter": "Ji Wu", "authors": "Ji Wu, Miao Li, Chin-Hui Lee", "title": "A Probabilistic Framework for Representing Dialog Systems and\n  Entropy-Based Dialog Management through Dynamic Stochastic State Evolution", "comments": "10 pages, 6 figures, 6 tables,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a probabilistic framework for goal-driven spoken\ndialog systems. A new dynamic stochastic state (DS-state) is then defined to\ncharacterize the goal set of a dialog state at different stages of the dialog\nprocess. Furthermore, an entropy minimization dialog management(EMDM) strategy\nis also proposed to combine with the DS-states to facilitate a robust and\nefficient solution in reaching a user's goals. A Song-On-Demand task, with a\ntotal of 38117 songs and 12 attributes corresponding to each song, is used to\ntest the performance of the proposed approach. In an ideal simulation, assuming\nno errors, the EMDM strategy is the most efficient goal-seeking method among\nall tested approaches, returning the correct song within 3.3 dialog turns on\naverage. Furthermore, in a practical scenario, with top five candidates to\nhandle the unavoidable automatic speech recognition (ASR) and natural language\nunderstanding (NLU) errors, the results show that only 61.7\\% of the dialog\ngoals can be successfully obtained in 6.23 dialog turns on average when random\nquestions are asked by the system, whereas if the proposed DS-states are\nupdated with the top 5 candidates from the SLU output using the proposed EMDM\nstrategy executed at every DS-state, then a 86.7\\% dialog success rate can be\naccomplished effectively within 5.17 dialog turns on average. We also\ndemonstrate that entropy-based DM strategies are more efficient than\nnon-entropy based DM. Moreover, using the goal set distributions in EMDM, the\nresults are better than those without them, such as in sate-of-the-art database\nsummary DM.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 17:55:53 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Wu", "Ji", ""], ["Li", "Miao", ""], ["Lee", "Chin-Hui", ""]]}, {"id": "1504.07302", "submitter": "Yuyin Sun", "authors": "Yuyin Sun, Adish Singla, Dieter Fox, Andreas Krause", "title": "Building Hierarchies of Concepts via Crowdsourcing", "comments": "12 pages, 8 pages of main paper, 4 pages of appendix, IJCAI2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchies of concepts are useful in many applications from navigation to\norganization of objects. Usually, a hierarchy is created in a centralized\nmanner by employing a group of domain experts, a time-consuming and expensive\nprocess. The experts often design one single hierarchy to best explain the\nsemantic relationships among the concepts, and ignore the natural uncertainty\nthat may exist in the process. In this paper, we propose a crowdsourcing system\nto build a hierarchy and furthermore capture the underlying uncertainty. Our\nsystem maintains a distribution over possible hierarchies and actively selects\nquestions to ask using an information gain criterion. We evaluate our\nmethodology on simulated data and on a set of real world application domains.\nExperimental results show that our system is robust to noise, efficient in\npicking questions, cost-effective and builds high quality hierarchies.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 23:14:32 GMT"}, {"version": "v2", "created": "Sun, 3 May 2015 21:42:14 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2015 00:27:21 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Sun", "Yuyin", ""], ["Singla", "Adish", ""], ["Fox", "Dieter", ""], ["Krause", "Andreas", ""]]}, {"id": "1504.07313", "submitter": "Daniel Aranki", "authors": "Daniel Aranki and Ruzena Bajcsy", "title": "Private Disclosure of Information in Health Tele-monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework, called Private Disclosure of Information (PDI),\nwhich is aimed to prevent an adversary from inferring certain sensitive\ninformation about subjects using the data that they disclosed during\ncommunication with an intended recipient. We show cases where it is possible to\nachieve perfect privacy regardless of the adversary's auxiliary knowledge while\npreserving full utility of the information to the intended recipient and\nprovide sufficient conditions for such cases. We also demonstrate the\napplicability of PDI on a real-world data set that simulates a health\ntele-monitoring scenario.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 00:20:50 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Aranki", "Daniel", ""], ["Bajcsy", "Ruzena", ""]]}, {"id": "1504.07324", "submitter": "Piji Li", "authors": "Piji Li, Lidong Bing, Wai Lam, Hang Li and Yi Liao", "title": "Reader-Aware Multi-Document Summarization via Sparse Coding", "comments": "7 pages, 2 figures, accepted as a full paper at IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new MDS paradigm called reader-aware multi-document\nsummarization (RA-MDS). Specifically, a set of reader comments associated with\nthe news reports are also collected. The generated summaries from the reports\nfor the event should be salient according to not only the reports but also the\nreader comments. To tackle this RA-MDS problem, we propose a\nsparse-coding-based method that is able to calculate the salience of the text\nunits by jointly considering news reports and reader comments. Another\nreader-aware characteristic of our framework is to improve linguistic quality\nvia entity rewriting. The rewriting consideration is jointly assessed together\nwith other summarization requirements under a unified optimization model. To\nsupport the generation of compressive summaries via optimization, we explore a\nfiner syntactic unit, namely, noun/verb phrase. In this work, we also generate\na data set for conducting RA-MDS. Extensive experiments on this data set and\nsome classical data sets demonstrate the effectiveness of our proposed\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 01:34:33 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Li", "Piji", ""], ["Bing", "Lidong", ""], ["Lam", "Wai", ""], ["Li", "Hang", ""], ["Liao", "Yi", ""]]}, {"id": "1504.07443", "submitter": "Marie-Laure Mugnier", "authors": "Jean-Fran\\c{c}ois Baget and Meghyn Bienvenu and Marie-Laure Mugnier\n  and Swan Rocher", "title": "Combining Existential Rules and Transitivity: Next Steps", "comments": "This is an extended version, completed with full proofs, of an\n  article appearing in IJCAI'15 - revised version (December 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider existential rules (aka Datalog+) as a formalism for specifying\nontologies. In recent years, many classes of existential rules have been\nexhibited for which conjunctive query (CQ) entailment is decidable. However,\nmost of these classes cannot express transitivity of binary relations, a\nfrequently used modelling construct. In this paper, we address the issue of\nwhether transitivity can be safely combined with decidable classes of\nexistential rules.\n  First, we prove that transitivity is incompatible with one of the simplest\ndecidable classes, namely aGRD (acyclic graph of rule dependencies), which\nclarifies the landscape of `finite expansion sets' of rules.\n  Second, we show that transitivity can be safely added to linear rules (a\nsubclass of guarded rules, which generalizes the description logic DL-Lite-R)\nin the case of atomic CQs, and also for general CQs if we place a minor\nsyntactic restriction on the rule set. This is shown by means of a novel query\nrewriting algorithm that is specially tailored to handle transitivity rules.\n  Third, for the identified decidable cases, we pinpoint the combined and data\ncomplexities of query entailment.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 12:22:48 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 08:49:35 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Baget", "Jean-Fran\u00e7ois", ""], ["Bienvenu", "Meghyn", ""], ["Mugnier", "Marie-Laure", ""], ["Rocher", "Swan", ""]]}, {"id": "1504.07571", "submitter": "Murat Okandan", "authors": "Murat Okandan", "title": "Can Machines Truly Think", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Can machines truly think? This question and its answer have many implications\nthat depend, in large part, on any number of assumptions underlying how the\nissue has been addressed or considered previously. A crucial question, and one\nthat is almost taken for granted, is the starting point for this discussion:\nCan \"thought\" be achieved or emulated by algorithmic procedures?\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 17:08:18 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Okandan", "Murat", ""]]}, {"id": "1504.07877", "submitter": "Amina Kemmar", "authors": "Amina Kemmar, Samir Loudni, Yahia Lebbah, Patrice Boizumault, Thierry\n  Charnois", "title": "Prefix-Projection Global Constraint for Sequential Pattern Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential pattern mining under constraints is a challenging data mining\ntask. Many efficient ad hoc methods have been developed for mining sequential\npatterns, but they are all suffering from a lack of genericity. Recent works\nhave investigated Constraint Programming (CP) methods, but they are not still\neffective because of their encoding. In this paper, we propose a global\nconstraint based on the projected databases principle which remedies to this\ndrawback. Experiments show that our approach clearly outperforms CP approaches\nand competes well with ad hoc methods on large datasets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 14:48:07 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 09:31:49 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Kemmar", "Amina", ""], ["Loudni", "Samir", ""], ["Lebbah", "Yahia", ""], ["Boizumault", "Patrice", ""], ["Charnois", "Thierry", ""]]}, {"id": "1504.08027", "submitter": "Prashanti Manda", "authors": "Prashanti Manda, Fiona McCarthy, Bindu Nanduri, Hui Wang, Susan M.\n  Bridges", "title": "Information-theoretic Interestingness Measures for Cross-Ontology Data\n  Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community annotation of biological entities with concepts from multiple\nbio-ontologies has created large and growing repositories of ontology-based\nannotation data with embedded implicit relationships among orthogonal\nontologies. Development of efficient data mining methods and metrics to mine\nand assess the quality of the mined relationships has not kept pace with the\ngrowth of annotation data. In this study, we present a data mining method that\nuses ontology-guided generalization to discover relationships across ontologies\nalong with a new interestingness metric based on information theory. We apply\nour data mining algorithm and interestingness measures to datasets from the\nGene Expression Database at the Mouse Genome Informatics as a preliminary proof\nof concept to mine relationships between developmental stages in the mouse\nanatomy ontology and Gene Ontology concepts (biological process, molecular\nfunction and cellular component). In addition, we present a comparison of our\ninterestingness metric to four existing metrics. Ontology-based annotation\ndatasets provide a valuable resource for discovery of relationships across\nontologies. The use of efficient data mining methods and appropriate\ninterestingness metrics enables the identification of high quality\nrelationships.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 21:15:46 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 08:58:17 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Manda", "Prashanti", ""], ["McCarthy", "Fiona", ""], ["Nanduri", "Bindu", ""], ["Wang", "Hui", ""], ["Bridges", "Susan M.", ""]]}, {"id": "1504.08050", "submitter": "Shuangyong Song", "authors": "Shuangyong Song and Yao Meng", "title": "Detecting Concept-level Emotion Cause in Microblogging", "comments": "2 pages, 2 figures, to appear on WWW 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Concept-level Emotion Cause Model (CECM), instead\nof the mere word-level models, to discover causes of microblogging users'\ndiversified emotions on specific hot event. A modified topic-supervised biterm\ntopic model is utilized in CECM to detect emotion topics' in event-related\ntweets, and then context-sensitive topical PageRank is utilized to detect\nmeaningful multiword expressions as emotion causes. Experimental results on a\ndataset from Sina Weibo, one of the largest microblogging websites in China,\nshow CECM can better detect emotion causes than baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 00:35:32 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Song", "Shuangyong", ""], ["Meng", "Yao", ""]]}, {"id": "1504.08108", "submitter": "Ario Santoso", "authors": "Diego Calvanese, Marco Montali, Ario Santoso", "title": "Verification of Generalized Inconsistency-Aware Knowledge and Action\n  Bases (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge and Action Bases (KABs) have been put forward as a semantically\nrich representation of a domain, using a DL KB to account for its static\naspects, and actions to evolve its extensional part over time, possibly\nintroducing new objects. Recently, KABs have been extended to manage\ninconsistency, with ad-hoc verification techniques geared towards specific\nsemantics. This work provides a twofold contribution along this line of\nresearch. On the one hand, we enrich KABs with a high-level, compact action\nlanguage inspired by Golog, obtaining so called Golog-KABs (GKABs). On the\nother hand, we introduce a parametric execution semantics for GKABs, so as to\nelegantly accomodate a plethora of inconsistency-aware semantics based on the\nnotion of repair. We then provide several reductions for the verification of\nsophisticated first-order temporal properties over inconsistency-aware GKABs,\nand show that it can be addressed using known techniques, developed for\nstandard KABs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 08:16:55 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 14:12:39 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Calvanese", "Diego", ""], ["Montali", "Marco", ""], ["Santoso", "Ario", ""]]}, {"id": "1504.08241", "submitter": "Alexander Rass", "authors": "Alexander Ra{\\ss}, Manuel Schmitt, Rolf Wanka", "title": "Explanation of Stagnation at Points that are not Local Optima in\n  Particle Swarm Optimization by Potential Analysis", "comments": "Full version of poster on Genetic and Evolutionary Computation\n  Conference (GECCO) 15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Swarm Optimization (PSO) is a nature-inspired meta-heuristic for\nsolving continuous optimization problems. In the literature, the potential of\nthe particles of swarm has been used to show that slightly modified PSO\nguarantees convergence to local optima. Here we show that under specific\ncircumstances the unmodified PSO, even with swarm parameters known (from the\nliterature) to be good, almost surely does not yield convergence to a local\noptimum is provided. This undesirable phenomenon is called stagnation. For this\npurpose, the particles' potential in each dimension is analyzed mathematically.\nAdditionally, some reasonable assumptions on the behavior if the particles'\npotential are made. Depending on the objective function and, interestingly, the\nnumber of particles, the potential in some dimensions may decrease much faster\nthan in other dimensions. Therefore, these dimensions lose relevance, i.e., the\ncontribution of their entries to the decisions about attractor updates becomes\ninsignificant and, with positive probability, they never regain relevance. If\nBrownian Motion is assumed to be an approximation of the time-dependent drop of\npotential, practical, i.e., large values for this probability are calculated.\nFinally, on chosen multidimensional polynomials of degree two, experiments are\nprovided showing that the required circumstances occur quite frequently.\nFurthermore, experiments are provided showing that even when the very simple\nsphere function is processed the described stagnation phenomenon occurs.\nConsequently, unmodified PSO does not converge to any local optimum of the\nchosen functions for tested parameter settings.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 14:28:44 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Ra\u00df", "Alexander", ""], ["Schmitt", "Manuel", ""], ["Wanka", "Rolf", ""]]}, {"id": "1504.08248", "submitter": "Palash Dey", "authors": "Palash Dey, Neeldhara Misra, and Y. Narahari", "title": "Frugal Bribery in Voting", "comments": "Full version has been accepted in Theoretical Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bribery in elections is an important problem in computational social choice\ntheory. However, bribery with money is often illegal in elections. Motivated by\nthis, we introduce the notion of frugal bribery and formulate two new pertinent\ncomputational problems which we call Frugal-bribery and Frugal- $bribery to\ncapture bribery without money in elections. In the proposed model, the briber\nis frugal in nature and this is captured by her inability to bribe votes of a\ncertain kind, namely, non-vulnerable votes. In the Frugal-bribery problem, the\ngoal is to make a certain candidate win the election by changing only\nvulnerable votes. In the Frugal-{dollar}bribery problem, the vulnerable votes\nhave prices and the goal is to make a certain candidate win the election by\nchanging only vulnerable votes, subject to a budget constraint of the briber.\nWe further formulate two natural variants of the Frugal-{dollar}bribery problem\nnamely Uniform-frugal-{dollar}bribery and Nonuniform-frugal-{dollar}bribery\nwhere the prices of the vulnerable votes are, respectively, all the same or\ndifferent.\n  We study the computational complexity of the above problems for unweighted\nand weighted elections for several commonly used voting rules. We observe that,\neven if we have only a small number of candidates, the problems are intractable\nfor all voting rules studied here for weighted elections, with the sole\nexception of the Frugal-bribery problem for the plurality voting rule. In\ncontrast, we have polynomial time algorithms for the Frugal-bribery problem for\nplurality, veto, k-approval, k-veto, and plurality with runoff voting rules for\nunweighted elections. However, the Frugal-{dollar}bribery problem is\nintractable for all the voting rules studied here barring the plurality and the\nveto voting rules for unweighted elections.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 14:42:34 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 16:39:10 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 16:51:02 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Dey", "Palash", ""], ["Misra", "Neeldhara", ""], ["Narahari", "Y.", ""]]}, {"id": "1504.08256", "submitter": "Palash Dey", "authors": "Palash Dey, Neeldhara Misra, and Y. Narahari", "title": "Manipulation is Harder with Incomplete Votes", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Coalitional Manipulation (CM) problem has been studied extensively in the\nliterature for many voting rules. The CM problem, however, has been studied\nonly in the complete information setting, that is, when the manipulators know\nthe votes of the non-manipulators. A more realistic scenario is an incomplete\ninformation setting where the manipulators do not know the exact votes of the\nnon- manipulators but may have some partial knowledge of the votes. In this\npaper, we study a setting where the manipulators know a partial order for each\nvoter that is consistent with the vote of that voter. In this setting, we\nintroduce and study two natural computational problems - (1) Weak Manipulation\n(WM) problem where the manipulators wish to vote in a way that makes their\npreferred candidate win in at least one extension of the partial votes of the\nnon-manipulators; (2) Strong Manipulation (SM) problem where the manipulators\nwish to vote in a way that makes their preferred candidate win in all possible\nextensions of the partial votes of the non-manipulators. We study the\ncomputational complexity of the WM and the SM problems for commonly used voting\nrules such as plurality, veto, k-approval, k-veto, maximin, Copeland, and\nBucklin. Our key finding is that, barring a few exceptions, manipulation\nbecomes a significantly harder problem in the setting of incomplete votes.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 14:51:57 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Dey", "Palash", ""], ["Misra", "Neeldhara", ""], ["Narahari", "Y.", ""]]}]