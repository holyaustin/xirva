[{"id": "1608.00100", "submitter": "Nikos Katzouris", "authors": "Nikos Katzouris, Alexander Artikis, Georgios Paliouras", "title": "Online Learning of Event Definitions", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 1 PDF figure", "journal-ref": "Theory and Practice of Logic Programming 16(5-6), 817-833, 2016", "doi": "10.1017/S1471068416000260", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems for symbolic event recognition infer occurrences of events in time\nusing a set of event definitions in the form of first-order rules. The Event\nCalculus is a temporal logic that has been used as a basis in event recognition\napplications, providing among others, direct connections to machine learning,\nvia Inductive Logic Programming (ILP). We present an ILP system for online\nlearning of Event Calculus theories. To allow for a single-pass learning\nstrategy, we use the Hoeffding bound for evaluating clauses on a subset of the\ninput stream. We employ a decoupling scheme of the Event Calculus axioms during\nthe learning process, that allows to learn each clause in isolation. Moreover,\nwe use abductive-inductive logic programming techniques to handle unobserved\ntarget predicates. We evaluate our approach on an activity recognition\napplication and compare it to a number of batch learning techniques. We obtain\nresults of comparable predicative accuracy with significant speed-ups in\ntraining time. We also outperform hand-crafted rules and match the performance\nof a sound incremental learner that can only operate on noise-free datasets.\nThis paper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 10:44:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Katzouris", "Nikos", ""], ["Artikis", "Alexander", ""], ["Paliouras", "Georgios", ""]]}, {"id": "1608.00139", "submitter": "Taisuke Sato", "authors": "Taisuke Sato", "title": "A Linear Algebraic Approach to Datalog Evaluation", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fundamentally new approach to Datalog evaluation.\nGiven a linear Datalog program DB written using N constants and binary\npredicates, we first translate if-and-only-if completions of clauses in DB into\na set Eq(DB) of matrix equations with a non-linear operation where relations in\nM_DB, the least Herbrand model of DB, are encoded as adjacency matrices. We\nthen translate Eq(DB) into another, but purely linear matrix equations\ntilde_Eq(DB). It is proved that the least solution of tilde_Eq(DB) in the sense\nof matrix ordering is converted to the least solution of Eq(DB) and the latter\ngives M_DB as a set of adjacency matrices. Hence computing the least solution\nof tilde_Eq(DB) is equivalent to computing M_DB specified by DB. For a class of\ntail recursive programs and for some other types of programs, our approach\nachieves O(N^3) time complexity irrespective of the number of variables in a\nclause since only matrix operations costing O(N^3) or less are used.\n  We conducted two experiments that compute the least Herbrand models of linear\nDatalog programs. The first experiment computes transitive closure of\nartificial data and real network data taken from the Koblenz Network\nCollection. The second one compared the proposed approach with the\nstate-of-the-art symbolic systems including two Prolog systems and two ASP\nsystems, in terms of computation time for a transitive closure program and the\nsame generation program. In the experiment, it is observed that our linear\nalgebraic approach runs 10^1 ~ 10^4 times faster than the symbolic systems when\ndata is not sparse. To appear in Theory and Practice of Logic Programming\n(TPLP).\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 16:14:16 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 05:41:58 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Sato", "Taisuke", ""]]}, {"id": "1608.00302", "submitter": "Beishui Liao", "authors": "Beishui Liao and Kang Xu and Huaxin Huang", "title": "Formulating Semantics of Probabilistic Argumentation by Characterizing\n  Subgraphs: Theory and Empirical Results", "comments": "First version submitted to JLC on Feb 12, 2016. This is the final\n  version, accepted by JLC on Nov 28, 2016", "journal-ref": null, "doi": "10.1093/logcom/exx035", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In existing literature, while approximate approaches based on Monte-Carlo\nsimulation technique have been proposed to compute the semantics of\nprobabilistic argumentation, how to improve the efficiency of computation\nwithout using simulation technique is still an open problem. In this paper, we\naddress this problem from the following two perspectives. First, conceptually,\nwe define specific properties to characterize the subgraphs of a PrAG with\nrespect to a given extension, such that the probability of a set of arguments E\nbeing an extension can be defined in terms of these properties, without (or\nwith less) construction of subgraphs. Second, computationally, we take\npreferred semantics as an example, and develop algorithms to evaluate the\nefficiency of our approach. The results show that our approach not only\ndramatically decreases the time for computing p(E^\\sigma), but also has an\nattractive property, which is contrary to that of existing approaches: the\ndenser the edges of a PrAG are or the bigger the size of a given extension E\nis, the more efficient our approach computes p(E^\\sigma). Meanwhile, it is\nshown that under complete and preferred semantics, the problems of determining\np(E^\\sigma) are fixed-parameter tractable.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 02:34:07 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 21:16:47 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Liao", "Beishui", ""], ["Xu", "Kang", ""], ["Huang", "Huaxin", ""]]}, {"id": "1608.00329", "submitter": "Sujatha Das Gollapalli", "authors": "Sujatha Das Gollapalli and Xiao-li Li", "title": "Keyphrase Extraction using Sequential Labeling", "comments": "10 pages including 2 pages of references, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrases efficiently summarize a document's content and are used in various\ndocument processing and retrieval tasks. Several unsupervised techniques and\nclassifiers exist for extracting keyphrases from text documents. Most of these\nmethods operate at a phrase-level and rely on part-of-speech (POS) filters for\ncandidate phrase generation. In addition, they do not directly handle\nkeyphrases of varying lengths. We overcome these modeling shortcomings by\naddressing keyphrase extraction as a sequential labeling task in this paper. We\nexplore a basic set of features commonly used in NLP tasks as well as\npredictions from various unsupervised methods to train our taggers. In addition\nto a more natural modeling for the keyphrase extraction problem, we show that\ntagging models yield significant performance benefits over existing\nstate-of-the-art extraction methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 06:00:22 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 03:07:46 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Gollapalli", "Sujatha Das", ""], ["Li", "Xiao-li", ""]]}, {"id": "1608.00359", "submitter": "Nikolas Hemion", "authors": "Nikolas J. Hemion", "title": "Discovering Latent States for Model Learning: Applying Sensorimotor\n  Contingencies Theory and Predictive Processing to Model Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robots need to be able to adapt to unforeseen situations and to\nacquire new skills through trial and error. Reinforcement learning in principle\noffers a suitable methodological framework for this kind of autonomous\nlearning. However current computational reinforcement learning agents mostly\nlearn each individual skill entirely from scratch. How can we enable artificial\nagents, such as robots, to acquire some form of generic knowledge, which they\ncould leverage for the learning of new skills? This paper argues that, like the\nbrain, the cognitive system of artificial agents has to develop a world model\nto support adaptive behavior and learning. Inspiration is taken from two recent\ndevelopments in the cognitive science literature: predictive processing\ntheories of cognition, and the sensorimotor contingencies theory of perception.\nBased on these, a hypothesis is formulated about what the content of\ninformation might be that is encoded in an internal world model, and how an\nagent could autonomously acquire it. A computational model is described to\nformalize this hypothesis, and is evaluated in a series of simulation\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 09:09:04 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Hemion", "Nikolas J.", ""]]}, {"id": "1608.00627", "submitter": "Shreyansh Daftry", "authors": "Shreyansh Daftry, J. Andrew Bagnell and Martial Hebert", "title": "Learning Transferable Policies for Monocular Reactive MAV Control", "comments": "International Symposium on Experimental Robotics (ISER 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to transfer knowledge gained in previous tasks into new contexts\nis one of the most important mechanisms of human learning. Despite this,\nadapting autonomous behavior to be reused in partially similar settings is\nstill an open problem in current robotics research. In this paper, we take a\nsmall step in this direction and propose a generic framework for learning\ntransferable motion policies. Our goal is to solve a learning problem in a\ntarget domain by utilizing the training data in a different but related source\ndomain. We present this in the context of an autonomous MAV flight using\nmonocular reactive control, and demonstrate the efficacy of our proposed\napproach through extensive real-world flight experiments in outdoor cluttered\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 21:53:04 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Daftry", "Shreyansh", ""], ["Bagnell", "J. Andrew", ""], ["Hebert", "Martial", ""]]}, {"id": "1608.00655", "submitter": "EPTCS", "authors": "Sotiris Moschoyiannis (Department of Computer Science, University of\n  Surrey, UK), Nicholas Elia (Extended-Content Solutions, London, UK),\n  Alexandra S. Penn (Department of Sociology, University of Surrey, UK), David\n  J.B. Lloyd (Department of Mathematics, University of Surrey, UK), Chris\n  Knight (Department of Mathematics, University of Surrey, UK)", "title": "A Web-based Tool for Identifying Strategic Intervention Points in\n  Complex Systems", "comments": "In Proceedings Cassting'16/SynCoP'16, arXiv:1608.00177", "journal-ref": "EPTCS 220, 2016, pp. 39-52", "doi": "10.4204/EPTCS.220.4", "report-no": null, "categories": "cs.SY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steering a complex system towards a desired outcome is a challenging task.\nThe lack of clarity on the system's exact architecture and the often scarce\nscientific data upon which to base the operationalisation of the dynamic rules\nthat underpin the interactions between participant entities are two\ncontributing factors. We describe an analytical approach that builds on Fuzzy\nCognitive Mapping (FCM) to address the latter and represent the system as a\ncomplex network. We apply results from network controllability to address the\nformer and determine minimal control configurations - subsets of factors, or\nsystem levers, which comprise points for strategic intervention in steering the\nsystem. We have implemented the combination of these techniques in an\nanalytical tool that runs in the browser, and generates all minimal control\nconfigurations of a complex network. We demonstrate our approach by reporting\non our experience of working alongside industrial, local-government, and NGO\nstakeholders in the Humber region, UK. Our results are applied to the\ndecision-making process involved in the transition of the region to a bio-based\neconomy.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 00:36:49 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Moschoyiannis", "Sotiris", "", "Department of Computer Science, University of\n  Surrey, UK"], ["Elia", "Nicholas", "", "Extended-Content Solutions, London, UK"], ["Penn", "Alexandra S.", "", "Department of Sociology, University of Surrey, UK"], ["Lloyd", "David J. B.", "", "Department of Mathematics, University of Surrey, UK"], ["Knight", "Chris", "", "Department of Mathematics, University of Surrey, UK"]]}, {"id": "1608.00667", "submitter": "Hong-Min Chu", "authors": "Hong-Min Chu, Hsuan-Tien Lin", "title": "Can Active Learning Experience Be Transferred?", "comments": "10 pages, 8 figs, 4 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is an important machine learning problem in reducing the\nhuman labeling effort. Current active learning strategies are designed from\nhuman knowledge, and are applied on each dataset in an immutable manner. In\nother words, experience about the usefulness of strategies cannot be updated\nand transferred to improve active learning on other datasets. This paper\ninitiates a pioneering study on whether active learning experience can be\ntransferred. We first propose a novel active learning model that linearly\naggregates existing strategies. The linear weights can then be used to\nrepresent the active learning experience. We equip the model with the popular\nlinear upper- confidence-bound (LinUCB) algorithm for contextual bandit to\nupdate the weights. Finally, we extend our model to transfer the experience\nacross datasets with the technique of biased regularization. Empirical studies\ndemonstrate that the learned experience not only is competitive with existing\nstrategies on most single datasets, but also can be transferred across datasets\nto improve the performance on future learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 01:30:25 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Chu", "Hong-Min", ""], ["Lin", "Hsuan-Tien", ""]]}, {"id": "1608.00700", "submitter": "Qifei Wang", "authors": "Qifei Wang", "title": "A Survey of Visual Analysis of Human Motion and Its Applications", "comments": "5 pages, conference paper in VCIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the recent progress in human motion analysis and its\napplications. In the beginning, we reviewed the motion capture systems and the\nrepresentation model of human's motion data. Next, we sketched the advanced\nhuman motion data processing technologies, including motion data filtering,\ntemporal alignment, and segmentation. The following parts overview the\nstate-of-the-art approaches of action recognition and dynamics measuring since\nthese two are the most active research areas in human motion analysis. The last\npart discusses some emerging applications of the human motion analysis in\nhealthcare, human robot interaction, security surveillance, virtual reality and\nanimation. The promising research topics of human motion analysis in the future\nis also summarized in the last part.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 05:50:11 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 05:15:03 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Wang", "Qifei", ""]]}, {"id": "1608.00730", "submitter": "Carmine Dodaro", "authors": "Carmine Dodaro, Philip Gasteiger, Nicola Leone, Benjamin Musitsch,\n  Francesco Ricca, Kostyantyn Shchekotykhin", "title": "Combining Answer Set Programming and Domain Heuristics for Solving Hard\n  Industrial Problems (Application Paper)", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a popular logic programming paradigm that has\nbeen applied for solving a variety of complex problems. Among the most\nchallenging real-world applications of ASP are two industrial problems defined\nby Siemens: the Partner Units Problem (PUP) and the Combined Configuration\nProblem (CCP). The hardest instances of PUP and CCP are out of reach for\nstate-of-the-art ASP solvers. Experiments show that the performance of ASP\nsolvers could be significantly improved by embedding domain-specific\nheuristics, but a proper effective integration of such criteria in\noff-the-shelf ASP implementations is not obvious. In this paper the combination\nof ASP and domain-specific heuristics is studied with the goal of effectively\nsolving real-world problem instances of PUP and CCP. As a byproduct of this\nactivity, the ASP solver WASP was extended with an interface that eases\nembedding new external heuristics in the solver. The evaluation shows that our\ndomain-heuristic-driven ASP solver finds solutions for all the real-world\ninstances of PUP and CCP ever provided by Siemens. This paper is under\nconsideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 08:36:08 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Dodaro", "Carmine", ""], ["Gasteiger", "Philip", ""], ["Leone", "Nicola", ""], ["Musitsch", "Benjamin", ""], ["Ricca", "Francesco", ""], ["Shchekotykhin", "Kostyantyn", ""]]}, {"id": "1608.00737", "submitter": "Nikolas Hemion", "authors": "Nikolas J. Hemion", "title": "Context Discovery for Model Learning in Partially Observable\n  Environments", "comments": "6th Joint IEEE International Conference on Development and Learning\n  and on Epigenetic Robotics (IEEE ICDL-EPIROB 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn a model is essential for the success of autonomous\nagents. Unfortunately, learning a model is difficult in partially observable\nenvironments, where latent environmental factors influence what the agent\nobserves. In the absence of a supervisory training signal, autonomous agents\ntherefore require a mechanism to autonomously discover these environmental\nfactors, or sensorimotor contexts.\n  This paper presents a method to discover sensorimotor contexts in partially\nobservable environments, by constructing a hierarchical transition model. The\nmethod is evaluated in a simulation experiment, in which a robot learns that\ndifferent rooms are characterized by different objects that are found in them.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 08:57:14 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Hemion", "Nikolas J.", ""]]}, {"id": "1608.00810", "submitter": "Manuele Leonelli", "authors": "Manuele Leonelli, Jim Q. Smith", "title": "Directed expected utility networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of statistical graphical models have been defined to represent the\nconditional independences underlying a random vector of interest. Similarly,\nmany different graphs embedding various types of preferential independences, as\nfor example conditional utility independence and generalized additive\nindependence, have more recently started to appear. In this paper we define a\nnew graphical model, called a directed expected utility network, whose edges\ndepict both probabilistic and utility conditional independences. These embed a\nvery flexible class of utility models, much larger than those usually conceived\nin standard influence diagrams. Our graphical representation, and various\ntransformations of the original graph into a tree structure, are then used to\nguide fast routines for the computation of a decision problem's expected\nutilities. We show that our routines generalize those usually utilized in\nstandard influence diagrams' evaluations under much more restrictive\nconditions. We then proceed with the construction of a directed expected\nutility network to support decision makers in the domain of household food\nsecurity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 13:22:49 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 13:16:02 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Leonelli", "Manuele", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1608.00876", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Rong Zhou, Nesreen K. Ahmed", "title": "Relational Similarity Machines", "comments": "MLG16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Relational Similarity Machines (RSM): a fast, accurate,\nand flexible relational learning framework for supervised and semi-supervised\nlearning tasks. Despite the importance of relational learning, most existing\nmethods are hard to adapt to different settings, due to issues with efficiency,\nscalability, accuracy, and flexibility for handling a wide variety of\nclassification problems, data, constraints, and tasks. For instance, many\nexisting methods perform poorly for multi-class classification problems, graphs\nthat are sparsely labeled or network data with low relational autocorrelation.\nIn contrast, the proposed relational learning framework is designed to be (i)\nfast for learning and inference at real-time interactive rates, and (ii)\nflexible for a variety of learning settings (multi-class problems), constraints\n(few labeled instances), and application domains. The experiments demonstrate\nthe effectiveness of RSM for a variety of tasks and data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:48:58 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Zhou", "Rong", ""], ["Ahmed", "Nesreen K.", ""]]}, {"id": "1608.01018", "submitter": "EPTCS", "authors": "Dimitrios Kartsaklis, Martha Lewis, Laura Rimell", "title": "Proceedings of the 2016 Workshop on Semantic Spaces at the Intersection\n  of NLP, Physics and Cognitive Science", "comments": null, "journal-ref": "EPTCS 221, 2016", "doi": "10.4204/EPTCS.221", "report-no": null, "categories": "cs.CL cs.AI math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the Proceedings of the 2016 Workshop on Semantic Spaces\nat the Intersection of NLP, Physics and Cognitive Science (SLPCS 2016), which\nwas held on the 11th of June at the University of Strathclyde, Glasgow, and was\nco-located with Quantum Physics and Logic (QPL 2016). Exploiting the common\nground provided by the concept of a vector space, the workshop brought together\nresearchers working at the intersection of Natural Language Processing (NLP),\ncognitive science, and physics, offering them an appropriate forum for\npresenting their uniquely motivated work and ideas. The interplay between these\nthree disciplines inspired theoretically motivated approaches to the\nunderstanding of how word meanings interact with each other in sentences and\ndiscourse, how diagrammatic reasoning depicts and simplifies this interaction,\nhow language models are determined by input from the world, and how word and\nsentence meanings interact logically. This first edition of the workshop\nconsisted of three invited talks from distinguished speakers (Hans Briegel,\nPeter G\\\"ardenfors, Dominic Widdows) and eight presentations of selected\ncontributed papers. Each submission was refereed by at least three members of\nthe Programme Committee, who delivered detailed and insightful comments and\nsuggestions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 22:28:45 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Kartsaklis", "Dimitrios", ""], ["Lewis", "Martha", ""], ["Rimell", "Laura", ""]]}, {"id": "1608.01039", "submitter": "Nicholas Mattei", "authors": "Nicholas Mattei and Toby Walsh", "title": "Empirical Evaluation of Real World Tournaments", "comments": "2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational Social Choice (ComSoc) is a rapidly developing field at the\nintersection of computer science, economics, social choice, and political\nscience. The study of tournaments is fundamental to ComSoc and many results\nhave been published about tournament solution sets and reasoning in\ntournaments. Theoretical results in ComSoc tend to be worst case and tell us\nlittle about performance in practice. To this end we detail some experiments on\ntournaments using real wold data from soccer and tennis. We make three main\ncontributions to the understanding of tournaments using real world data from\nEnglish Premier League, the German Bundesliga, and the ATP World Tour: (1) we\nfind that the NP-hard question of finding a seeding for which a given team can\nwin a tournament is easily solvable in real world instances, (2) using detailed\nand principled methodology from statistical physics we show that our real world\ndata obeys a log-normal distribution; and (3) leveraging our log-normal\ndistribution result and using robust statistical methods, we show that the\npopular Condorcet Random (CR) tournament model does not generate realistic\ntournament data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 01:13:54 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Mattei", "Nicholas", ""], ["Walsh", "Toby", ""]]}, {"id": "1608.01093", "submitter": "Sarmimala Saikia", "authors": "Ashwin Srinivasan, Gautam Shroff, Lovekesh Vig, Sarmimala Saikia,\n  Puneet Agarwal", "title": "Generation of Near-Optimal Solutions Using ILP-Guided Sampling", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": "TR-EOIS-2016-1", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our interest in this paper is in optimisation problems that are intractable\nto solve by direct numerical optimisation, but nevertheless have significant\namounts of relevant domain-specific knowledge. The category of heuristic search\ntechniques known as estimation of distribution algorithms (EDAs) seek to\nincrementally sample from probability distributions in which optimal (or\nnear-optimal) solutions have increasingly higher probabilities. Can we use\ndomain knowledge to assist the estimation of these distributions? To answer\nthis in the affirmative, we need: (a)a general-purpose technique for the\nincorporation of domain knowledge when constructing models for optimal values;\nand (b)a way of using these models to generate new data samples. Here we\ninvestigate a combination of the use of Inductive Logic Programming (ILP) for\n(a), and standard logic-programming machinery to generate new samples for (b).\nSpecifically, on each iteration of distribution estimation, an ILP engine is\nused to construct a model for good solutions. The resulting theory is then used\nto guide the generation of new data instances, which are now restricted to\nthose derivable using the ILP model in conjunction with the background\nknowledge). We demonstrate the approach on two optimisation problems\n(predicting optimal depth-of-win for the KRK endgame, and job-shop scheduling).\nOur results are promising: (a)On each iteration of distribution estimation,\nsamples obtained with an ILP theory have a substantially greater proportion of\ngood solutions than samples without a theory; and (b)On termination of\ndistribution estimation, samples obtained with an ILP theory contain more\nnear-optimal samples than samples without a theory. Taken together, these\nresults suggest that the use of ILP-constructed theories could be a useful\ntechnique for incorporating complex domain-knowledge into estimation\ndistribution procedures.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 07:23:48 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 06:11:14 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Srinivasan", "Ashwin", ""], ["Shroff", "Gautam", ""], ["Vig", "Lovekesh", ""], ["Saikia", "Sarmimala", ""], ["Agarwal", "Puneet", ""]]}, {"id": "1608.01127", "submitter": "Alban Laflaqui\\`ere Dr", "authors": "Alban Laflaqui\\`ere", "title": "Autonomous Grounding of Visual Field Experience through Sensorimotor\n  Prediction", "comments": "6 pages, 4 figures, ICDL-Epirob 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a developmental framework, autonomous robots need to explore the world and\nlearn how to interact with it. Without an a priori model of the system, this\nopens the challenging problem of having robots master their interface with the\nworld: how to perceive their environment using their sensors, and how to act in\nit using their motors. The sensorimotor approach of perception claims that a\nnaive agent can learn to master this interface by capturing regularities in the\nway its actions transform its sensory inputs. In this paper, we apply such an\napproach to the discovery and mastery of the visual field associated with a\nvisual sensor. A computational model is formalized and applied to a simulated\nsystem to illustrate the approach.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 09:25:35 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Laflaqui\u00e8re", "Alban", ""]]}, {"id": "1608.01212", "submitter": "Sebastian Baumbach", "authors": "Sebastian Baumbach, Frank Wittich, Florian Sachs, Sheraz Ahmed,\n  Andreas Dengel", "title": "A Novel Approach for Data-Driven Automatic Site Recommendation and\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel, generic, and automatic method for data-driven\nsite selection. Site selection is one of the most crucial and important\ndecisions made by any company. Such a decision depends on various factors of\nsites, including socio-economic, geographical, ecological, as well as specific\nrequirements of companies. The existing approaches for site selection (commonly\nused by economists) are manual, subjective, and not scalable, especially to Big\nData. The presented method for site selection is robust, efficient, scalable,\nand is capable of handling challenges emerging in Big Data. To assess the\neffectiveness of the presented method, it is evaluated on real data (collected\nfrom Federal Statistical Office of Germany) of around 200 influencing factors\nwhich are considered by economists for site selection of Supermarkets in\nGermany (Lidl, EDEKA, and NP). Evaluation results show that there is a big\noverlap (86.4 \\%) between the sites of existing supermarkets and the sites\nrecommended by the presented method. In addition, the method also recommends\nmany sites (328) for supermarket where a store should be opened.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 14:58:53 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Baumbach", "Sebastian", ""], ["Wittich", "Frank", ""], ["Sachs", "Florian", ""], ["Ahmed", "Sheraz", ""], ["Dengel", "Andreas", ""]]}, {"id": "1608.01302", "submitter": "Caelan Garrett", "authors": "Caelan Reed Garrett, Leslie Pack Kaelbling, Tomas Lozano-Perez", "title": "Learning to Rank for Synthesizing Planning Heuristics", "comments": null, "journal-ref": "International Joint Conference on Artificial Intelligence (IJCAI)\n  2016", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate learning heuristics for domain-specific planning. Prior work\nframed learning a heuristic as an ordinary regression problem. However, in a\ngreedy best-first search, the ordering of states induced by a heuristic is more\nindicative of the resulting planner's performance than mean squared error.\nThus, we instead frame learning a heuristic as a learning to rank problem which\nwe solve using a RankSVM formulation. Additionally, we introduce new methods\nfor computing features that capture temporal interactions in an approximate\nplan. Our experiments on recent International Planning Competition problems\nshow that the RankSVM learned heuristics outperform both the original\nheuristics and heuristics learned through ordinary regression.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 19:50:39 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Garrett", "Caelan Reed", ""], ["Kaelbling", "Leslie Pack", ""], ["Lozano-Perez", "Tomas", ""]]}, {"id": "1608.01338", "submitter": "Tiantian Gao", "authors": "Tiantian Gao, Paul Fodor and Michael Kifer", "title": "Paraconsistency and Word Puzzles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word puzzles and the problem of their representations in logic languages have\nreceived considerable attention in the last decade (Ponnuru et al. 2004;\nShapiro 2011; Baral and Dzifcak 2012; Schwitter 2013). Of special interest is\nthe problem of generating such representations directly from natural language\n(NL) or controlled natural language (CNL). An interesting variation of this\nproblem, and to the best of our knowledge, scarcely explored variation in this\ncontext, is when the input information is inconsistent. In such situations, the\nexisting encodings of word puzzles produce inconsistent representations and\nbreak down. In this paper, we bring the well-known type of paraconsistent\nlogics, called Annotated Predicate Calculus (APC) (Kifer and Lozinskii 1992),\nto bear on the problem. We introduce a new kind of non-monotonic semantics for\nAPC, called consistency preferred stable models and argue that it makes APC\ninto a suitable platform for dealing with inconsistency in word puzzles and,\nmore generally, in NL sentences. We also devise a number of general principles\nto help the user choose among the different representations of NL sentences,\nwhich might seem equivalent but, in fact, behave differently when inconsistent\ninformation is taken into account. These principles can be incorporated into\nexisting CNL translators, such as Attempto Controlled English (ACE) (Fuchs et\nal. 2008) and PENG Light (White and Schwitter 2009). Finally, we show that APC\nwith the consistency preferred stable model semantics can be equivalently\nembedded in ASP with preferences over stable models, and we use this embedding\nto implement this version of APC in Clingo (Gebser et al. 2011) and its Asprin\nadd-on (Brewka et al. 2015).\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 20:26:20 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 19:21:19 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Gao", "Tiantian", ""], ["Fodor", "Paul", ""], ["Kifer", "Michael", ""]]}, {"id": "1608.01402", "submitter": "EPTCS", "authors": "Josef Bolt (Univesity of Oxford), Bob Coecke (Univesity of Oxford),\n  Fabrizio Genovese (Univesity of Oxford), Martha Lewis (Univesity of Oxford),\n  Daniel Marsden (Univesity of Oxford), Robin Piedeleu (Univesity of Oxford)", "title": "Interacting Conceptual Spaces", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 11-19", "doi": "10.4204/EPTCS.221.2", "report-no": null, "categories": "cs.AI cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose applying the categorical compositional scheme of [6] to conceptual\nspace models of cognition. In order to do this we introduce the category of\nconvex relations as a new setting for categorical compositional semantics,\nemphasizing the convex structure important to conceptual space applications. We\nshow how conceptual spaces for composite types such as adjectives and verbs can\nbe constructed. We illustrate this new model on detailed examples.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 00:36:21 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Bolt", "Josef", "", "Univesity of Oxford"], ["Coecke", "Bob", "", "Univesity of Oxford"], ["Genovese", "Fabrizio", "", "Univesity of Oxford"], ["Lewis", "Martha", "", "Univesity of Oxford"], ["Marsden", "Daniel", "", "Univesity of Oxford"], ["Piedeleu", "Robin", "", "Univesity of Oxford"]]}, {"id": "1608.01404", "submitter": "EPTCS", "authors": "Mehrnoosh Sadrzadeh (Queen Mary University of London)", "title": "Quantifier Scope in Categorical Compositional Distributional Semantics", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 49-57", "doi": "10.4204/EPTCS.221.6", "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work with J. Hedges, we formalised a generalised quantifiers\ntheory of natural language in categorical compositional distributional\nsemantics with the help of bialgebras. In this paper, we show how quantifier\nscope ambiguity can be represented in that setting and how this representation\ncan be generalised to branching quantifiers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 00:36:57 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Sadrzadeh", "Mehrnoosh", "", "Queen Mary University of London"]]}, {"id": "1608.01603", "submitter": "Amelia  Harrison", "authors": "Amelia Harrison and Vladimir Lifschitz", "title": "Stable Models for Infinitary Formulas with Extensional Atoms", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The definition of stable models for propositional formulas with infinite\nconjunctions and disjunctions can be used to describe the semantics of answer\nset programming languages. In this note, we enhance that definition by\nintroducing a distinction between intensional and extensional atoms. The\nsymmetric splitting theorem for first-order formulas is then extended to\ninfinitary formulas and used to reason about infinitary definitions. This note\nis under consideration for publication in Theory and Practice of Logic\nProgramming.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 16:32:57 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Harrison", "Amelia", ""], ["Lifschitz", "Vladimir", ""]]}, {"id": "1608.01604", "submitter": "Andrea Formisano", "authors": "Stefania Costantini and Andrea Formisano", "title": "Query Answering in Resource-Based Answer Set Semantics", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent work we defined resource-based answer set semantics, which is an\nextension to answer set semantics stemming from the study of its relationship\nwith linear logic. In fact, the name of the new semantics comes from the fact\nthat in the linear-logic formulation every literal (including negative ones)\nwere considered as a resource. In this paper, we propose a query-answering\nprocedure reminiscent of Prolog for answer set programs under this extended\nsemantics as an extension of XSB-resolution for logic programs with negation.\nWe prove formal properties of the proposed procedure.\n  Under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 16:38:52 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Costantini", "Stefania", ""], ["Formisano", "Andrea", ""]]}, {"id": "1608.01611", "submitter": "Harits Ar Rosyid", "authors": "Harits Ar Rosyid, Matt Palmerlee, Ke Chen", "title": "Deploying learning materials to game content for serious education game\n  development: A case study", "comments": null, "journal-ref": null, "doi": "10.1016/j.entcom.2018.01.001", "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate goals of serious education games (SEG) are to facilitate\nlearning and maximizing enjoyment during playing SEGs. In SEG development,\nthere are normally two spaces to be taken into account: knowledge space\nregarding learning materials and content space regarding games to be used to\nconvey learning materials. How to deploy the learning materials seamlessly and\neffectively into game content becomes one of the most challenging problems in\nSEG development. Unlike previous work where experts in education have to be\nused heavily, we proposed a novel approach that works toward minimizing the\nefforts of education experts in mapping learning materials to content space.\nFor a proof-of-concept, we apply the proposed approach in developing an SEG\ngame, named \\emph{Chem Dungeon}, as a case study in order to demonstrate the\neffectiveness of our proposed approach. This SEG game has been tested with a\nnumber of users, and the user survey suggests our method works reasonably well.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 16:56:31 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Rosyid", "Harits Ar", ""], ["Palmerlee", "Matt", ""], ["Chen", "Ke", ""]]}, {"id": "1608.01668", "submitter": "Uwe Aickelin", "authors": "Jan Feyereisl and Uwe Aickelin", "title": "Self-Organising Maps in Computer Security", "comments": "pp. 1-30, Computer Security: Intrusion, Detection and Prevention,\n  2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some argue that biologically inspired algorithms are the future of solving\ndifficult problems in computer science. Others strongly believe that the future\nlies in the exploration of mathematical foundations of problems at hand. The\nfield of computer security tends to accept the latter view as a more\nappropriate approach due to its more workable validation and verification\npossibilities. The lack of rigorous scientific practices prevalent in\nbiologically inspired security research does not aid in presenting bio-inspired\nsecurity approaches as a viable way of dealing with complex security problems.\nThis chapter introduces a biologically inspired algorithm, called the Self\nOrganising Map (SOM), that was developed by Teuvo Kohonen in 1981. Since the\nalgorithm's inception it has been scrutinised by the scientific community and\nanalysed in more than 4000 research papers, many of which dealt with various\ncomputer security issues, from anomaly detection, analysis of executables all\nthe way to wireless network monitoring. In this chapter a review of security\nrelated SOM research undertaken in the past is presented and analysed. The\nalgorithm's biological analogies are detailed and the author's view on the\nfuture possibilities of this successful bio-inspired approach are given. The\nSOM algorithm's close relation to a number of vital functions of the human\nbrain and the emergence of multi-core computer architectures are the two main\nreasons behind our assumption that the future of the SOM algorithm and its\nvariations is promising, notably in the field of computer security.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 05:51:24 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Feyereisl", "Jan", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1608.01716", "submitter": "Hiroki Sayama", "authors": "Ali Jazayeri and Hiroki Sayama", "title": "A Polynomial-Time Deterministic Approach to the Traveling Salesperson\n  Problem", "comments": "8 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new polynomial-time deterministic algorithm that produces an\napproximated solution for the traveling salesperson problem. The proposed\nalgorithm ranks cities based on their priorities calculated using a power\nfunction of means and standard deviations of their distances from other cities\nand then connects the cities to their neighbors in the order of their\npriorities. When connecting a city, a neighbor is selected based on their\nneighbors' priorities calculated as another power function that additionally\nincludes their distance from the focal city to be connected. This repeats until\nall the cities are connected into a single loop. The time complexity of the\nproposed algorithm is $O(n^2)$, where $n$ is the number of cities. Numerical\nevaluation shows that, despite its simplicity, the proposed algorithm produces\nshorter tours with less time complexity than other conventional tour\nconstruction heuristics. The proposed algorithm can be used by itself or as an\ninitial tour generator for other more complex heuristic optimization\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 23:18:47 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 15:27:26 GMT"}, {"version": "v3", "created": "Tue, 26 Dec 2017 03:46:44 GMT"}, {"version": "v4", "created": "Thu, 15 Nov 2018 22:42:16 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Jazayeri", "Ali", ""], ["Sayama", "Hiroki", ""]]}, {"id": "1608.01835", "submitter": "Bart Bogaerts", "authors": "Bart Bogaerts and Tomi Janhunen and Shahab Tasharrofi", "title": "Stable-Unstable Semantics: Beyond NP with Normal Logic Programs", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 16 pages,\n  LaTeX, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard answer set programming (ASP) targets at solving search problems from\nthe first level of the polynomial time hierarchy (PH). Tackling search problems\nbeyond NP using ASP is less straightforward. The class of disjunctive logic\nprograms offers the most prominent way of reaching the second level of the PH,\nbut encoding respective hard problems as disjunctive programs typically\nrequires sophisticated techniques such as saturation or meta-interpretation.\nThe application of such techniques easily leads to encodings that are\ninaccessible to non-experts. Furthermore, while disjunctive ASP solvers often\nrely on calls to a (co-)NP oracle, it may be difficult to detect from the input\nprogram where the oracle is being accessed. In other formalisms, such as\nQuantified Boolean Formulas (QBFs), the interface to the underlying oracle is\nmore transparent as it is explicitly recorded in the quantifier prefix of a\nformula. On the other hand, ASP has advantages over QBFs from the modeling\nperspective. The rich high-level languages such as ASP-Core-2 offer a wide\nvariety of primitives that enable concise and natural encodings of search\nproblems. In this paper, we present a novel logic programming--based modeling\nparadigm that combines the best features of ASP and QBFs. We develop so-called\ncombined logic programs in which oracles are directly cast as (normal) logic\nprograms themselves. Recursive incarnations of this construction enable logic\nprogramming on arbitrarily high levels of the PH. We develop a proof-of-concept\nimplementation for our new paradigm.\n  This paper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 11:18:12 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 10:18:17 GMT"}, {"version": "v3", "created": "Mon, 15 Aug 2016 05:25:55 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Bogaerts", "Bart", ""], ["Janhunen", "Tomi", ""], ["Tasharrofi", "Shahab", ""]]}, {"id": "1608.01856", "submitter": "Michael Morak", "authors": "Manuel Bichler, Michael Morak and Stefan Woltran", "title": "The Power of Non-Ground Rules in Answer Set Programming", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer set programming (ASP) is a well-established logic programming language\nthat offers an intuitive, declarative syntax for problem solving. In its\ntraditional application, a fixed ASP program for a given problem is designed\nand the actual instance of the problem is fed into the program as a set of\nfacts. This approach typically results in programs with comparably short and\nsimple rules. However, as is known from complexity analysis, such an approach\nlimits the expressive power of ASP; in fact, an entire NP-check can be encoded\ninto a single large rule body of bounded arity that performs both a guess and a\ncheck within the same rule.\n  Here, we propose a novel paradigm for encoding hard problems in ASP by making\nexplicit use of large rules which depend on the actual instance of the problem.\nWe illustrate how this new encoding paradigm can be used, providing examples of\nproblems from the first, second, and even third level of the polynomial\nhierarchy. As state-of-the-art solvers are tuned towards short rules, rule\ndecomposition is a key technique in the practical realization of our approach.\nWe also provide some preliminary benchmarks which indicate that giving up the\nconvenient way of specifying a fixed program can lead to a significant\nspeed-up.\n  This paper is under consideration for acceptance into TPLP.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 12:26:22 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Bichler", "Manuel", ""], ["Morak", "Michael", ""], ["Woltran", "Stefan", ""]]}, {"id": "1608.01884", "submitter": "Ernest Davis", "authors": "Ernest Davis", "title": "Winograd Schemas and Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Winograd schema is a pair of sentences that differ in a single word and\nthat contain an ambiguous pronoun whose referent is different in the two\nsentences and requires the use of commonsense knowledge or world knowledge to\ndisambiguate. This paper discusses how Winograd schemas and other sentence\npairs could be used as challenges for machine translation using distinctions\nbetween pronouns, such as gender, that appear in the target language but not in\nthe source.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 13:39:08 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 23:12:29 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Davis", "Ernest", ""]]}, {"id": "1608.01946", "submitter": "Mark Law", "authors": "Mark Law, Alessandra Russo, Krysia Broda", "title": "Iterative Learning of Answer Set Programs from Context Dependent\n  Examples", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 22 pages,\n  LaTeX, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several frameworks and systems have been proposed that\nextend Inductive Logic Programming (ILP) to the Answer Set Programming (ASP)\nparadigm. In ILP, examples must all be explained by a hypothesis together with\na given background knowledge. In existing systems, the background knowledge is\nthe same for all examples; however, examples may be context-dependent. This\nmeans that some examples should be explained in the context of some\ninformation, whereas others should be explained in different contexts. In this\npaper, we capture this notion and present a context-dependent extension of the\nLearning from Ordered Answer Sets framework. In this extension, contexts can be\nused to further structure the background knowledge. We then propose a new\niterative algorithm, ILASP2i, which exploits this feature to scale up the\nexisting ILASP2 system to learning tasks with large numbers of examples. We\ndemonstrate the gain in scalability by applying both algorithms to various\nlearning tasks. Our results show that, compared to ILASP2, the newly proposed\nILASP2i system can be two orders of magnitude faster and use two orders of\nmagnitude less memory, whilst preserving the same average accuracy. This paper\nis under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 17:33:23 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Law", "Mark", ""], ["Russo", "Alessandra", ""], ["Broda", "Krysia", ""]]}, {"id": "1608.01961", "submitter": "Mohammad Taher Pilehvar", "authors": "Mohammad Taher Pilehvar and Nigel Collier", "title": "De-Conflated Semantic Representations", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major deficiency of most semantic representation techniques is that they\nusually model a word type as a single point in the semantic space, hence\nconflating all the meanings that the word can have. Addressing this issue by\nlearning distinct representations for individual meanings of words has been the\nsubject of several research studies in the past few years. However, the\ngenerated sense representations are either not linked to any sense inventory or\nare unreliable for infrequent word senses. We propose a technique that tackles\nthese problems by de-conflating the representations of words based on the deep\nknowledge it derives from a semantic network. Our approach provides multiple\nadvantages in comparison to the past work, including its high coverage and the\nability to generate accurate representations even for infrequent word senses.\nWe carry out evaluations on six datasets across two semantic similarity tasks\nand report state-of-the-art results on most of them.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 18:14:19 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Pilehvar", "Mohammad Taher", ""], ["Collier", "Nigel", ""]]}, {"id": "1608.01987", "submitter": "Peter Krafft", "authors": "Peter M. Krafft, Julia Zheng, Wei Pan, Nicol\\'as Della Penna, Yaniv\n  Altshuler, Erez Shmueli, Joshua B. Tenenbaum, Alex Pentland", "title": "Human collective intelligence as distributed Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.GT cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective intelligence is believed to underly the remarkable success of\nhuman society. The formation of accurate shared beliefs is one of the key\ncomponents of human collective intelligence. How are accurate shared beliefs\nformed in groups of fallible individuals? Answering this question requires a\nmultiscale analysis. We must understand both the individual decision mechanisms\npeople use, and the properties and dynamics of those mechanisms in the\naggregate. As of yet, mathematical tools for such an approach have been\nlacking. To address this gap, we introduce a new analytical framework: We\npropose that groups arrive at accurate shared beliefs via distributed Bayesian\ninference. Distributed inference occurs through information processing at the\nindividual level, and yields rational belief formation at the group level. We\ninstantiate this framework in a new model of human social decision-making,\nwhich we validate using a dataset we collected of over 50,000 users of an\nonline social trading platform where investors mimic each others' trades using\nreal money in foreign exchange and other asset markets. We find that in this\nsetting people use a decision mechanism in which popularity is treated as a\nprior distribution for which decisions are best to make. This mechanism is\nboundedly rational at the individual level, but we prove that in the aggregate\nimplements a type of approximate \"Thompson sampling\"---a well-known and highly\neffective single-agent Bayesian machine learning algorithm for sequential\ndecision-making. The perspective of distributed Bayesian inference therefore\nreveals how collective rationality emerges from the boundedly rational decision\nmechanisms people use.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 19:55:57 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Krafft", "Peter M.", ""], ["Zheng", "Julia", ""], ["Pan", "Wei", ""], ["Della Penna", "Nicol\u00e1s", ""], ["Altshuler", "Yaniv", ""], ["Shmueli", "Erez", ""], ["Tenenbaum", "Joshua B.", ""], ["Pentland", "Alex", ""]]}, {"id": "1608.02076", "submitter": "Hao Cheng", "authors": "Hao Cheng and Hao Fang and Xiaodong He and Jianfeng Gao and Li Deng", "title": "Bi-directional Attention with Agreement for Dependency Parsing", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel bi-directional attention model for dependency parsing,\nwhich learns to agree on headword predictions from the forward and backward\nparsing directions. The parsing procedure for each direction is formulated as\nsequentially querying the memory component that stores continuous headword\nembeddings. The proposed parser makes use of {\\it soft} headword embeddings,\nallowing the model to implicitly capture high-order parsing history without\ndramatically increasing the computational complexity. We conduct experiments on\nEnglish, Chinese, and 12 other languages from the CoNLL 2006 shared task,\nshowing that the proposed model achieves state-of-the-art unlabeled attachment\nscores on 6 languages.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 07:16:31 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 08:52:31 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Cheng", "Hao", ""], ["Fang", "Hao", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1608.02082", "submitter": "Daniela Inclezan", "authors": "Daniela Inclezan", "title": "COREALMLIB: An ALM Library Translated from the Component Library", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 figures (2 of which in PDF format)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents COREALMLIB, an ALM library of commonsense knowledge about\ndynamic domains. The library was obtained by translating part of the COMPONENT\nLIBRARY (CLIB) into the modular action language ALM. CLIB consists of general\nreusable and composable commonsense concepts, selected based on a thorough\nstudy of ontological and lexical resources. Our translation targets CLIB states\n(i.e., fluents) and actions. The resulting ALM library contains the\ndescriptions of 123 action classes grouped into 43 reusable modules that are\norganized into a hierarchy. It is made available online and of interest to\nresearchers in the action language, answer-set programming, and natural\nlanguage understanding communities. We believe that our translation has two\nmain advantages over its CLIB counterpart: (i) it specifies axioms about\nactions in a more elaboration tolerant and readable way, and (ii) it can be\nseamlessly integrated with ASP reasoning algorithms (e.g., for planning and\npostdiction). In contrast, axioms are described in CLIB using STRIPS-like\noperators, and CLIB's inference engine cannot handle planning nor postdiction.\nUnder consideration for publication in TPLP.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 08:59:21 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 16:56:18 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Inclezan", "Daniela", ""]]}, {"id": "1608.02158", "submitter": "Adler Perotte", "authors": "Rajesh Ranganath and Adler Perotte and No\\'emie Elhadad and David Blei", "title": "Deep Survival Analysis", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electronic health record (EHR) provides an unprecedented opportunity to\nbuild actionable tools to support physicians at the point of care. In this\npaper, we investigate survival analysis in the context of EHR data. We\nintroduce deep survival analysis, a hierarchical generative approach to\nsurvival analysis. It departs from previous approaches in two primary ways: (1)\nall observations, including covariates, are modeled jointly conditioned on a\nrich latent structure; and (2) the observations are aligned by their failure\ntime, rather than by an arbitrary time zero as in traditional survival\nanalysis. Further, it (3) scalably handles heterogeneous (continuous and\ndiscrete) data types that occur in the EHR. We validate deep survival analysis\nmodel by stratifying patients according to risk of developing coronary heart\ndisease (CHD). Specifically, we study a dataset of 313,000 patients\ncorresponding to 5.5 million months of observations. When compared to the\nclinically validated Framingham CHD risk score, deep survival analysis is\nsignificantly superior in stratifying patients according to their risk.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 22:18:18 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 14:08:02 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Perotte", "Adler", ""], ["Elhadad", "No\u00e9mie", ""], ["Blei", "David", ""]]}, {"id": "1608.02164", "submitter": "Joshua Peterson", "authors": "Joshua C. Peterson, Joshua T. Abbott, Thomas L. Griffiths", "title": "Adapting Deep Network Features to Capture Psychological Representations", "comments": "6 pages, 4 figures, To appear in the Proceedings of the 38th Annual\n  Conference of the Cognitive Science Society, Winner of the Computational\n  Modeling Prize in Perception/Action", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become increasingly successful at solving classic\nperception problems such as object recognition, semantic segmentation, and\nscene understanding, often reaching or surpassing human-level accuracy. This\nsuccess is due in part to the ability of DNNs to learn useful representations\nof high-dimensional inputs, a problem that humans must also solve. We examine\nthe relationship between the representations learned by these networks and\nhuman psychological representations recovered from similarity judgments. We\nfind that deep features learned in service of object classification account for\na significant amount of the variance in human similarity judgments for a set of\nanimal images. However, these features do not capture some qualitative\ndistinctions that are a key part of human representations. To remedy this, we\ndevelop a method for adapting deep features to align with human similarity\njudgments, resulting in image representations that can potentially be used to\nextend the scope of psychological experiments.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 23:49:48 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Peterson", "Joshua C.", ""], ["Abbott", "Joshua T.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1608.02165", "submitter": "Vladislav Voroninski", "authors": "Thomas Goldstein, Paul Hand, Choongbum Lee, Vladislav Voroninski,\n  Stefano Soatto", "title": "ShapeFit and ShapeKick for Robust, Scalable Structure from Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for location recovery from pair-wise directions\nthat leverages an efficient convex program that comes with exact recovery\nguarantees, even in the presence of adversarial outliers. When pairwise\ndirections represent scaled relative positions between pairs of views\n(estimated for instance with epipolar geometry) our method can be used for\nlocation recovery, that is the determination of relative pose up to a single\nunknown scale. For this task, our method yields performance comparable to the\nstate-of-the-art with an order of magnitude speed-up. Our proposed numerical\nframework is flexible in that it accommodates other approaches to location\nrecovery and can be used to speed up other methods. These properties are\ndemonstrated by extensively testing against state-of-the-art methods for\nlocation recovery on 13 large, irregular collections of images of real scenes\nin addition to simulated data with ground truth.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 00:29:53 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Goldstein", "Thomas", ""], ["Hand", "Paul", ""], ["Lee", "Choongbum", ""], ["Voroninski", "Vladislav", ""], ["Soatto", "Stefano", ""]]}, {"id": "1608.02193", "submitter": "Mark Burgess", "authors": "Mark Burgess", "title": "Spacetimes with Semantics (III) - The Structure of Functional Knowledge\n  Representation and Artificial Reasoning", "comments": "122 pages, builiding on parts I and II Minor updates and corrections\n  added to current version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the previously developed concepts of semantic spacetime, I explore the\ninterpretation of knowledge representations, and their structure, as a semantic\nsystem, within the framework of promise theory. By assigning interpretations to\nphenomena, from observers to observed, we may approach a simple description of\nknowledge-based functional systems, with direct practical utility. The focus is\nespecially on the interpretation of concepts, associative knowledge, and\ncontext awareness. The inference seems to be that most if not all of these\nconcepts emerge from purely semantic spacetime properties, which opens the\npossibility for a more generalized understanding of what constitutes a\nlearning, or even `intelligent' system.\n  Some key principles emerge for effective knowledge representation: 1)\nseparation of spacetime scales, 2) the recurrence of four irreducible types of\nassociation, by which intent propagates: aggregation, causation, cooperation,\nand similarity, 3) the need for discrimination of identities (discrete), which\nis assisted by distinguishing timeline simultaneity from sequential events, and\n4) the ability to learn (memory). It is at least plausible that emergent\nknowledge abstraction capabilities have their origin in basic spacetime\nstructures.\n  These notes present a unified view of mostly well-known results; they allow\nus to see information models, knowledge representations, machine learning, and\nsemantic networking (transport and information base) in a common framework. The\nnotion of `smart spaces' thus encompasses artificial systems as well as living\nsystems, across many different scales, e.g. smart cities and organizations.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 08:35:03 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 10:39:58 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 16:36:04 GMT"}, {"version": "v4", "created": "Tue, 1 Aug 2017 11:55:28 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Burgess", "Mark", ""]]}, {"id": "1608.02229", "submitter": "Fernando Corbacho", "authors": "Fernando Corbacho", "title": "Towards the Self-constructive Brain: emergence of adaptive behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive behavior is mainly the result of adaptive brains. We go a step\nbeyond and claim that the brain does not only adapt to its surrounding reality\nbut rather, it builds itself up to constructs its own reality. That is, rather\nthan just trying to passively understand its environment, the brain is the\narchitect of its own reality in an active process where its internal models of\nthe external world frame how its new interactions with the environment are\nassimilated. These internal models represent relevant predictive patterns of\ninteraction all over the different brain structures: perceptual, sensorimotor,\nmotor, etc. The emergence of adaptive behavior arises from this\nself-constructive nature of the brain, based on the following principles of\norganization: self-experimental, self- growing, and self-repairing.\nSelf-experimental, since to ensure survival, the self-constructive brain (SCB)\nis an active machine capable of performing experiments of its own interactions\nwith the environment by mental simulation. Self-growing, since it dynamically\nand incrementally constructs internal structures in order to build a model of\nthe world as it gathers statistics from its interactions with the environment.\nSelf-repairing, since to survive the SCB must also be robust and capable of\nfinding ways to repair parts of previously working structures and hence\nre-construct a previous relevant pattern of activity.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 15:52:28 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Corbacho", "Fernando", ""]]}, {"id": "1608.02287", "submitter": "David Cox", "authors": "David Cox", "title": "Delta Epsilon Alpha Star: A PAC-Admissible Search Algorithm", "comments": "8 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delta Epsilon Alpha Star is a minimal coverage, real-time robotic search\nalgorithm that yields a moderately aggressive search path with minimal\nbacktracking. Search performance is bounded by a placing a combinatorial bound,\nepsilon and delta, on the maximum deviation from the theoretical shortest path\nand the probability at which further deviations can occur. Additionally, we\nformally define the notion of PAC-admissibility -- a relaxed admissibility\ncriteria for algorithms, and show that PAC-admissible algorithms are better\nsuited to robotic search situations than epsilon-admissible or strict\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 00:14:50 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Cox", "David", ""]]}, {"id": "1608.02315", "submitter": "Federico Schl\\\"uter", "authors": "Federico Schl\\\"uter, Yanela Strappa, Diego H. Milone, Facundo Bromberg", "title": "Blankets Joint Posterior score for learning Markov network structures", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.ijar.2017.10.018", "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov networks are extensively used to model complex sequential, spatial,\nand relational interactions in a wide range of fields. By learning the\nstructure of independences of a domain, more accurate joint probability\ndistributions can be obtained for inference tasks or, more directly, for\ninterpreting the most significant relations among the variables. Recently,\nseveral researchers have investigated techniques for automatically learning the\nstructure from data by obtaining the probabilistic maximum-a-posteriori\nstructure given the available data. However, all the approximations proposed\ndecompose the posterior of the whole structure into local sub-problems, by\nassuming that the posteriors of the Markov blankets of all the variables are\nmutually independent. In this work, we propose a scoring function for relaxing\nsuch assumption. The Blankets Joint Posterior score computes the joint\nposterior of structures as a joint distribution of the collection of its Markov\nblankets. Essentially, the whole posterior is obtained by computing the\nposterior of the blanket of each variable as a conditional distribution that\ntakes into account information from other blankets in the network. We show in\nour experimental results that the proposed approximation can improve the sample\ncomplexity of state-of-the-art scores when learning complex networks, where the\nindependence assumption between blanket variables is clearly incorrect.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 04:59:40 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 21:30:40 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Schl\u00fcter", "Federico", ""], ["Strappa", "Yanela", ""], ["Milone", "Diego H.", ""], ["Bromberg", "Facundo", ""]]}, {"id": "1608.02341", "submitter": "Nicola Di Mauro", "authors": "Antonio Vergari and Nicola Di Mauro and Floriana Esposito", "title": "Towards Representation Learning with Tractable Probabilistic Models", "comments": "10 pages, submitted to ECML-PKDD 2016 Doctoral Consortium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic models learned as density estimators can be exploited in\nrepresentation learning beside being toolboxes used to answer inference queries\nonly. However, how to extract useful representations highly depends on the\nparticular model involved. We argue that tractable inference, i.e. inference\nthat can be computed in polynomial time, can enable general schemes to extract\nfeatures from black box models. We plan to investigate how Tractable\nProbabilistic Models (TPMs) can be exploited to generate embeddings by random\nquery evaluations. We devise two experimental designs to assess and compare\ndifferent TPMs as feature extractors in an unsupervised representation learning\nframework. We show some experimental results on standard image datasets by\napplying such a method to Sum-Product Networks and Mixture of Trees as\ntractable models generating embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 07:44:24 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Vergari", "Antonio", ""], ["Di Mauro", "Nicola", ""], ["Esposito", "Floriana", ""]]}, {"id": "1608.02406", "submitter": "Ronald de Haan", "authors": "Ronald de Haan", "title": "Complexity Results for Manipulation, Bribery and Control of the Kemeny\n  Procedure in Judgment Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of several scenarios of strategic\nbehavior for the Kemeny procedure in the setting of judgment aggregation. In\nparticular, we investigate (1) manipulation, where an individual aims to\nachieve a better group outcome by reporting an insincere individual opinion,\n(2) bribery, where an external agent aims to achieve an outcome with certain\nproperties by bribing a number of individuals, and (3) control (by adding or\ndeleting issues), where an external agent aims to achieve an outcome with\ncertain properties by influencing the set of issues in the judgment aggregation\nsituation. We show that determining whether these types of strategic behavior\nare possible (and if so, computing a policy for successful strategic behavior)\nis complete for the second level of the Polynomial Hierarchy. That is, we show\nthat these problems are $\\Sigma^p_2$-complete.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 12:24:05 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["de Haan", "Ronald", ""]]}, {"id": "1608.02441", "submitter": "Matthias Thimm", "authors": "Sarah A. Gaggl, Matthias Thimm", "title": "Proceedings of the Second Summer School on Argumentation: Computational\n  and Linguistic Perspectives (SSA'16)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the thesis abstracts presented at the Second Summer\nSchool on Argumentation: Computational and Linguistic Perspectives (SSA'2016)\nheld on September 8-12 in Potsdam, Germany.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 09:05:32 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Gaggl", "Sarah A.", ""], ["Thimm", "Matthias", ""]]}, {"id": "1608.02450", "submitter": "Daniele Theseider Dupr\\'e", "authors": "Laura Giordano and Daniele Theseider Dupr\\'e", "title": "ASP for Minimal Entailment in a Rational Extension of SROEL", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we exploit Answer Set Programming (ASP) for reasoning in a\nrational extension SROEL-R-T of the low complexity description logic SROEL,\nwhich underlies the OWL EL ontology language. In the extended language, a\ntypicality operator T is allowed to define concepts T(C) (typical C's) under a\nrational semantics. It has been proven that instance checking under rational\nentailment has a polynomial complexity. To strengthen rational entailment, in\nthis paper we consider a minimal model semantics. We show that, for arbitrary\nSROEL-R-T knowledge bases, instance checking under minimal entailment is\n\\Pi^P_2-complete. Relying on a Small Model result, where models correspond to\nanswer sets of a suitable ASP encoding, we exploit Answer Set Preferences (and,\nin particular, the asprin framework) for reasoning under minimal entailment.\nThe paper is under consideration for acceptance in Theory and Practice of Logic\nProgramming.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 14:26:46 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Giordano", "Laura", ""], ["Dupr\u00e9", "Daniele Theseider", ""]]}, {"id": "1608.02644", "submitter": "Daniel Whalen", "authors": "Daniel Whalen", "title": "Holophrasm: a neural Automated Theorem Prover for higher-order logic", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a system for Automated Theorem Proving in higher order logic using\ndeep learning and eschewing hand-constructed features. Holophrasm exploits the\nformalism of the Metamath language and explores partial proof trees using a\nneural-network-augmented bandit algorithm and a sequence-to-sequence model for\naction enumeration. The system proves 14% of its test theorems from Metamath's\nset.mm module.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 22:33:13 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 03:22:11 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Whalen", "Daniel", ""]]}, {"id": "1608.02658", "submitter": "Abbas Shojaee", "authors": "Abbas Shojaee, Isuru Ranasinghe, Alireza Ani", "title": "Revisiting Causality Inference in Memory-less Transition Networks", "comments": "This edition is improved with further details in the discussion\n  section and Figure 1. Other authors will be added in final revision; For\n  feedback, opinions, or questions please contact: abbas.shojaee@gmail.com OR\n  abbas.shojaee@yale.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI nlin.CD physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods exist to infer causal networks from massive volumes of\nobservational data. However, almost all existing methods require a considerable\nlength of time series data to capture cause and effect relationships. In\ncontrast, memory-less transition networks or Markov Chain data, which refers to\none-step transitions to and from an event, have not been explored for causality\ninference even though such data is widely available. We find that causal\nnetwork can be inferred from characteristics of four unique distribution zones\naround each event. We call this Composition of Transitions and show that cause,\neffect, and random events exhibit different behavior in their compositions. We\napplied machine learning models to learn these different behaviors and to infer\ncausality. We name this new method Causality Inference using Composition of\nTransitions (CICT). To evaluate CICT, we used an administrative inpatient\nhealthcare dataset to set up a network of patients transitions between\ndifferent diagnoses. We show that CICT is highly accurate in inferring whether\nthe transition between a pair of events is causal or random and performs well\nin identifying the direction of causality in a bi-directional association.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 23:46:59 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 21:38:17 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 16:33:44 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Shojaee", "Abbas", ""], ["Ranasinghe", "Isuru", ""], ["Ani", "Alireza", ""]]}, {"id": "1608.02659", "submitter": "Mohamed Ali Mahjoub", "authors": "Anis Elbahi, Mohamed Nazih Omri, Mohamed Ali Mahjoub, Kamel Garrouch", "title": "Mouse Movement and Probabilistic Graphical Models Based E-Learning\n  Activity Recognition Improvement Possibilistic Model", "comments": "in AJSE 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically recognizing the e-learning activities is an important task for\nimproving the online learning process. Probabilistic graphical models such as\nhidden Markov models and conditional random fields have been successfully used\nin order to identify a Web users activity. For such models, the sequences of\nobservation are crucial for training and inference processes. Despite the\nefficiency of these probabilistic graphical models in segmenting and labeling\nstochastic sequences, their performance is adversely affected by the imperfect\nquality of data used for the construction of sequences of observation. In this\npaper, a formalism of the possibilistic theory will be used in order to propose\na new approach for observation sequences preparation. The eminent contribution\nof our approach is to evaluate the effect of possibilistic reasoning during the\ngeneration of observation sequences on the effectiveness of hidden Markov\nmodels and conditional random fields models. Using a dataset containing 51 real\nmanipulations related to three types of learners tasks, the preliminary\nexperiments demonstrate that the sequences of observation obtained based on\npossibilistic reasoning significantly improve the performance of hidden Marvov\nmodels and conditional random fields models in the automatic recognition of the\ne-learning activities.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 23:48:19 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Elbahi", "Anis", ""], ["Omri", "Mohamed Nazih", ""], ["Mahjoub", "Mohamed Ali", ""], ["Garrouch", "Kamel", ""]]}, {"id": "1608.02682", "submitter": "Jaroslaw Zola", "authors": "Subhadeep Karan and Jaroslaw Zola", "title": "Exact Structure Learning of Bayesian Networks by Optimal Path Extension", "comments": "Published in the IEEE BigData 2016, this version contains a\n  correction to Figure 1c", "journal-ref": null, "doi": "10.1109/BigData.2016.7840588", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are probabilistic graphical models often used in big data\nanalytics. The problem of exact structure learning is to find a network\nstructure that is optimal under certain scoring criteria. The problem is known\nto be NP-hard and the existing methods are both computationally and memory\nintensive. In this paper, we introduce a new approach for exact structure\nlearning. Our strategy is to leverage relationship between a partial network\nstructure and the remaining variables to constraint the number of ways in which\nthe partial network can be optimally extended. Via experimental results, we\nshow that the method provides up to three times improvement in runtime, and\norders of magnitude reduction in memory consumption over the current best\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 03:07:50 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 04:45:15 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 14:47:03 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Karan", "Subhadeep", ""], ["Zola", "Jaroslaw", ""]]}, {"id": "1608.02693", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt and Carl Schultz", "title": "Deeply Semantic Inductive Spatio-Temporal Learning", "comments": "Accepted for publication at ILP 2016: 26th International Conference\n  on Inductive Logic Programming 4th - 6th September 2016, London. Keywords:\n  Spatio-Temporal Learning; Dynamic Visuo-Spatial Imagery; Declarative Spatial\n  Reasoning; Inductive Logic Programming; AI and Art", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an inductive spatio-temporal learning framework rooted in\ninductive logic programming. With an emphasis on visuo-spatial language, logic,\nand cognition, the framework supports learning with relational spatio-temporal\nfeatures identifiable in a range of domains involving the processing and\ninterpretation of dynamic visuo-spatial imagery. We present a prototypical\nsystem, and an example application in the domain of computing for visual arts\nand computational cognitive science.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 05:48:51 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Schultz", "Carl", ""]]}, {"id": "1608.02717", "submitter": "Mateusz Malinowski", "authors": "Ashkan Mokarian and Mateusz Malinowski and Mario Fritz", "title": "Mean Box Pooling: A Rich Image Representation and Output Embedding for\n  the Visual Madlibs Task", "comments": "Accepted to BMVC'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Mean Box Pooling, a novel visual representation that pools over\nCNN representations of a large number, highly overlapping object proposals. We\nshow that such representation together with nCCA, a successful multimodal\nembedding technique, achieves state-of-the-art performance on the Visual\nMadlibs task. Moreover, inspired by the nCCA's objective function, we extend\nclassical CNN+LSTM approach to train the network by directly maximizing the\nsimilarity between the internal representation of the deep learning\narchitecture and candidate answers. Again, such approach achieves a significant\nimprovement over the prior work that also uses CNN+LSTM approach on Visual\nMadlibs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 08:24:02 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Mokarian", "Ashkan", ""], ["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1608.02763", "submitter": "Konstantin Yakovlev S", "authors": "Konstantin Yakovlev, Anton Andreychuk", "title": "Resolving Spatial-Time Conflicts In A Set Of Any-angle Or\n  Angle-constrained Grid Paths", "comments": "as submitted to the 2nd Workshop on Multi-Agent Path Finding\n  (http://www.andrew.cmu.edu/user/gswagner/workshop/ijcai_2016_multirobot_path_finding.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multi-agent path finding problem (MAPF) for a group of agents\nwhich are allowed to move into arbitrary directions on a 2D square grid. We\nfocus on centralized conflict resolution for independently computed plans. We\npropose an algorithm that eliminates conflicts by using local re-planning and\nintroducing time offsets to the execution of paths by different agents.\nExperimental results show that the algorithm can find high quality\nconflict-free solutions at low computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 11:13:46 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Yakovlev", "Konstantin", ""], ["Andreychuk", "Anton", ""]]}, {"id": "1608.02833", "submitter": "Mundher Al-Shabi", "authors": "Mundher Al-Shabi, Wooi Ping Cheah, Tee Connie", "title": "Facial Expression Recognition Using a Hybrid CNN-SIFT Aggregator", "comments": "To be appear in LNAI", "journal-ref": null, "doi": "10.1007/978-3-319-69456-6_12", "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving an effective facial expression recognition component is important\nfor a successful human-computer interaction system. Nonetheless, recognizing\nfacial expression remains a challenging task. This paper describes a novel\napproach towards facial expression recognition task. The proposed method is\nmotivated by the success of Convolutional Neural Networks (CNN) on the face\nrecognition problem. Unlike other works, we focus on achieving good accuracy\nwhile requiring only a small sample data for training. Scale Invariant Feature\nTransform (SIFT) features are used to increase the performance on small data as\nSIFT does not require extensive training data to generate useful features. In\nthis paper, both Dense SIFT and regular SIFT are studied and compared when\nmerged with CNN features. Moreover, an aggregator of the models is developed.\nThe proposed approach is tested on the FER-2013 and CK+ datasets. Results\ndemonstrate the superiority of CNN with Dense SIFT over conventional CNN and\nCNN with SIFT. The accuracy even increased when all the models are aggregated\nwhich generates state-of-art results on FER-2013 and CK+ datasets, where it\nachieved 73.4% on FER-2013 and 99.1% on CK+.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 15:21:33 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 07:44:59 GMT"}, {"version": "v3", "created": "Sat, 3 Dec 2016 11:21:39 GMT"}, {"version": "v4", "created": "Thu, 22 Dec 2016 09:07:30 GMT"}, {"version": "v5", "created": "Sat, 12 Aug 2017 02:58:13 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Al-Shabi", "Mundher", ""], ["Cheah", "Wooi Ping", ""], ["Connie", "Tee", ""]]}, {"id": "1608.02858", "submitter": "Jan Drchal", "authors": "Jan Mrkos, Jan Drchal, Malcolm Egan, Michal Jakob", "title": "Liftago On-Demand Transport Dataset and Market Formation Algorithm Based\n  on Machine Learning", "comments": "9 pages, 2 figures, supplemental information for a journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document serves as a technical report for the analysis of on-demand\ntransport dataset. Moreover we show how the dataset can be used to develop a\nmarket formation algorithm based on machine learning. Data used in this work\ncomes from Liftago, a Prague based company which connects taxi drivers and\ncustomers through a smartphone app. The dataset is analysed from the\nmachine-learning perspective: we give an overview of features available as well\nas results of feature ranking. Later we propose the SImple Data-driven MArket\nFormation (SIDMAF) algorithm which aims to improve a relevance while connecting\ncustomers with relevant drivers. We compare the heuristics currently used by\nLiftago with SIDMAF using two key performance indicators.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 16:33:03 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Mrkos", "Jan", ""], ["Drchal", "Jan", ""], ["Egan", "Malcolm", ""], ["Jakob", "Michal", ""]]}, {"id": "1608.02971", "submitter": "Karan Budhraja", "authors": "Karan K. Budhraja and Tim Oates", "title": "Neuroevolution-Based Inverse Reinforcement Learning", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Learning from Demonstration is targeted at learning to perform\ntasks based on observed examples. One approach to Learning from Demonstration\nis Inverse Reinforcement Learning, in which actions are observed to infer\nrewards. This work combines a feature based state evaluation approach to\nInverse Reinforcement Learning with neuroevolution, a paradigm for modifying\nneural networks based on their performance on a given task. Neural networks are\nused to learn from a demonstrated expert policy and are evolved to generate a\npolicy similar to the demonstration. The algorithm is discussed and evaluated\nagainst competitive feature-based Inverse Reinforcement Learning approaches. At\nthe cost of execution time, neural networks allow for non-linear combinations\nof features in state evaluations. These valuations may correspond to state\nvalue or state reward. This results in better correspondence to observed\nexamples as opposed to using linear combinations. This work also extends\nexisting work on Bayesian Non-Parametric Feature Construction for Inverse\nReinforcement Learning by using non-linear combinations of intermediate data to\nimprove performance. The algorithm is observed to be specifically suitable for\na linearly solvable non-deterministic Markov Decision Processes in which\nmultiple rewards are sparsely scattered in state space. A conclusive\nperformance hierarchy between evaluated algorithms is presented.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 20:04:40 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Budhraja", "Karan K.", ""], ["Oates", "Tim", ""]]}, {"id": "1608.03000", "submitter": "Nicholas Locascio", "authors": "Nicholas Locascio, Karthik Narasimhan, Eduardo DeLeon, Nate Kushman,\n  Regina Barzilay", "title": "Neural Generation of Regular Expressions from Natural Language with\n  Minimal Domain Knowledge", "comments": "to be published in EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the task of translating natural language queries into\nregular expressions which embody their meaning. In contrast to prior work, the\nproposed neural model does not utilize domain-specific crafting, learning to\ntranslate directly from a parallel corpus. To fully explore the potential of\nneural models, we propose a methodology for collecting a large corpus of\nregular expression, natural language pairs. Our resulting model achieves a\nperformance gain of 19.6% over previous state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 23:05:03 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Locascio", "Nicholas", ""], ["Narasimhan", "Karthik", ""], ["DeLeon", "Eduardo", ""], ["Kushman", "Nate", ""], ["Barzilay", "Regina", ""]]}, {"id": "1608.03026", "submitter": "Lucius Schoenbaum", "authors": "Lucius Schoenbaum", "title": "Towards Visual Type Theory as a Mathematical Tool and Mathematical User\n  Interface", "comments": "19 pages, to appear in Joint Proceedings of the FM4M, MathUI, and\n  ThEdu Workshops, Doctoral Program, and Work in Progress at the Conference on\n  Intelligent Computer Mathematics, Bialystok, Poland 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A visual type theory is a cognitive tool that has much in common with\nlanguage, and may be regarded as an exceptional form of spatial text adjunct. A\nmathematical visual type theory, called NPM, has been under development that\ncan be viewed as an early-stage project in mathematical knowledge management\nand mathematical user interface development. We discuss in greater detail the\nnotion of a visual type theory, report on progress towards a usable\nmathematical visual type theory, and discuss the outlook for future work on\nthis project.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 02:10:40 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Schoenbaum", "Lucius", ""]]}, {"id": "1608.03235", "submitter": "Ulas Bagci", "authors": "Naji Khosravan, Haydar Celik, Baris Turkbey, Ruida Cheng, Evan\n  McCreedy, Matthew McAuliffe, Sandra Bednarova, Elizabeth Jones, Xinjian Chen,\n  Peter L. Choyke, Bradford J. Wood, Ulas Bagci", "title": "Gaze2Segment: A Pilot Study for Integrating Eye-Tracking Technology into\n  Medical Image Segmentation", "comments": "MICCAI-Medical Computer Vision 2016, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study introduced a novel system, called Gaze2Segment, integrating\nbiological and computer vision techniques to support radiologists' reading\nexperience with an automatic image segmentation task. During diagnostic\nassessment of lung CT scans, the radiologists' gaze information were used to\ncreate a visual attention map. This map was then combined with a\ncomputer-derived saliency map, extracted from the gray-scale CT images. The\nvisual attention map was used as an input for indicating roughly the location\nof a object of interest. With computer-derived saliency information, on the\nother hand, we aimed at finding foreground and background cues for the object\nof interest. At the final step, these cues were used to initiate a seed-based\ndelineation process. Segmentation accuracy of the proposed Gaze2Segment was\nfound to be 86% with dice similarity coefficient and 1.45 mm with Hausdorff\ndistance. To the best of our knowledge, Gaze2Segment is the first true\nintegration of eye-tracking technology into a medical image segmentation task\nwithout the need for any further user-interaction.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 16:44:00 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Khosravan", "Naji", ""], ["Celik", "Haydar", ""], ["Turkbey", "Baris", ""], ["Cheng", "Ruida", ""], ["McCreedy", "Evan", ""], ["McAuliffe", "Matthew", ""], ["Bednarova", "Sandra", ""], ["Jones", "Elizabeth", ""], ["Chen", "Xinjian", ""], ["Choyke", "Peter L.", ""], ["Wood", "Bradford J.", ""], ["Bagci", "Ulas", ""]]}, {"id": "1608.03507", "submitter": "Ramin Rahnamoun", "authors": "Ramin Rahnamoun, Reza Rawassizadeh, Arash Maskooki", "title": "Learning Mobile App Usage Routine through Learning Automata", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its conception, smart app market has grown exponentially. Success in\nthe app market depends on many factors among which the quality of the app is a\nsignificant contributor, such as energy use. Nevertheless, smartphones, as a\nsubset of mobile computing devices. inherit the limited power resource\nconstraint. Therefore, there is a challenge of maintaining the resource while\nincreasing the target app quality. This paper introduces Learning Automata (LA)\nas an online learning method to learn and predict the app usage routines of the\nusers. Such prediction can leverage the app cache functionality of the\noperating system and thus (i) decreases app launch time and (ii) preserve\nbattery. Our algorithm, which is an online learning approach, temporally\nupdates and improves the internal states of itself. In particular, it learns\nthe transition probabilities between app launching. Each App launching instance\nupdates the transition probabilities related to that App, and this will result\nin improving the prediction. We benefit from a real-world lifelogging dataset\nand our experimental results show considerable success with respect to the two\nbaseline methods that are used currently for smartphone app prediction\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 15:43:55 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 08:08:35 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Rahnamoun", "Ramin", ""], ["Rawassizadeh", "Reza", ""], ["Maskooki", "Arash", ""]]}, {"id": "1608.03532", "submitter": "L\\'aszl\\'o Gyarmati", "authors": "Laszlo Gyarmati, Rade Stanojevic", "title": "QPass: a Merit-based Evaluation of Soccer Passes", "comments": "2016 ACM KDD Workshop on Large-Scale Sports Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative analysis of soccer players' passing ability focuses on\ndescriptive statistics without considering the players' real contribution to\nthe passing and ball possession strategy of their team. Which player is able to\nhelp the build-up of an attack, or to maintain the possession of the ball? We\nintroduce a novel methodology called QPass to answer questions like these\nquantitatively. Based on the analysis of an entire season, we rank the players\nbased on the intrinsic value of their passes using QPass. We derive an album of\npass trajectories for different gaming styles. Our methodology reveals a quite\ncounterintuitive paradigm: losing the ball possession could lead to better\nchances to win a game.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 12:54:57 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Gyarmati", "Laszlo", ""], ["Stanojevic", "Rade", ""]]}, {"id": "1608.03544", "submitter": "Claudio Gentile", "authors": "Claudio Gentile, Shuai Li, Purushottam Kar, Alexandros Karatzoglou,\n  Evans Etrue, Giovanni Zappella", "title": "On Context-Dependent Clustering of Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a novel cluster-of-bandit algorithm CAB for collaborative\nrecommendation tasks that implements the underlying feedback sharing mechanism\nby estimating the neighborhood of users in a context-dependent manner. CAB\nmakes sharp departures from the state of the art by incorporating collaborative\neffects into inference as well as learning processes in a manner that\nseamlessly interleaving explore-exploit tradeoffs and collaborative steps. We\nprove regret bounds under various assumptions on the data, which exhibit a\ncrisp dependence on the expected number of clusters over the users, a natural\nmeasure of the statistical difficulty of the learning task. Experiments on\nproduction and real-world datasets show that CAB offers significantly increased\nprediction performance against a representative pool of state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 14:13:28 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 17:16:22 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Gentile", "Claudio", ""], ["Li", "Shuai", ""], ["Kar", "Purushottam", ""], ["Karatzoglou", "Alexandros", ""], ["Etrue", "Evans", ""], ["Zappella", "Giovanni", ""]]}, {"id": "1608.03672", "submitter": "Guillermo Leale", "authors": "Guillermo Leale, Ariel Bay\\'a, Diego Milone, Pablo Granitto and\n  Georgina Stegmayer", "title": "Inferring unknown biological function by integration of GO annotations\n  and gene expression data", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing genes with semantic information is an important process\nregarding the description of gene products. In spite that complete genomes of\nmany organisms have been already sequenced, the biological functions of all of\ntheir genes are still unknown. Since experimentally studying the functions of\nthose genes, one by one, would be unfeasible, new computational methods for\ngene functions inference are needed. We present here a novel computational\napproach for inferring biological function for a set of genes with previously\nunknown function, given a set of genes with well-known information. This\napproach is based on the premise that genes with similar behaviour should be\ngrouped together. This is known as the guilt-by-association principle. Thus, it\nis possible to take advantage of clustering techniques to obtain groups of\nunknown genes that are co-clustered with genes that have well-known semantic\ninformation (GO annotations). Meaningful knowledge to infer unknown semantic\ninformation can therefore be provided by these well-known genes. We provide a\nmethod to explore the potential function of new genes according to those\ncurrently annotated. The results obtained indicate that the proposed approach\ncould be a useful and effective tool when used by biologists to guide the\ninference of biological functions for recently discovered genes. Our work sets\nan important landmark in the field of identifying unknown gene functions\nthrough clustering, using an external source of biological input. A simple web\ninterface to this proposal can be found at\nhttp://fich.unl.edu.ar/sinc/webdemo/gamma-am/.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 04:29:14 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Leale", "Guillermo", ""], ["Bay\u00e1", "Ariel", ""], ["Milone", "Diego", ""], ["Granitto", "Pablo", ""], ["Stegmayer", "Georgina", ""]]}, {"id": "1608.03785", "submitter": "Martha Lewis", "authors": "Yaared Al-Mehairi, Bob Coecke, Martha Lewis", "title": "Compositional Distributional Cognition", "comments": "To appear in Quantum Interaction 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We accommodate the Integrated Connectionist/Symbolic Architecture (ICS) of\n[32] within the categorical compositional semantics (CatCo) of [13], forming a\nmodel of categorical compositional cognition (CatCog). This resolves intrinsic\nproblems with ICS such as the fact that representations inhabit an unbounded\nspace and that sentences with differing tree structures cannot be directly\ncompared. We do so in a way that makes the most of the grammatical structure\navailable, in contrast to strategies like circular convolution. Using the CatCo\nmodel also allows us to make use of tools developed for CatCo such as the\nrepresentation of ambiguity and logical reasoning via density matrices,\nstructural meanings for words such as relative pronouns, and addressing over-\nand under-extension, all of which are present in cognitive processes. Moreover\nthe CatCog framework is sufficiently flexible to allow for entirely different\nrepresentations of meaning, such as conceptual spaces. Interestingly, since the\nCatCo model was largely inspired by categorical quantum mechanics, so is\nCatCog.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 13:13:10 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Al-Mehairi", "Yaared", ""], ["Coecke", "Bob", ""], ["Lewis", "Martha", ""]]}, {"id": "1608.03824", "submitter": "Ashley Edwards", "authors": "Ashley Edwards, Charles Isbell, Atsuo Takanishi", "title": "Perceptual Reward Functions", "comments": "Deep Reinforcement Learning: Frontiers and Challenges Workshop, IJCAI\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning problems are often described through rewards that\nindicate if an agent has completed some task. This specification can yield\ndesirable behavior, however many problems are difficult to specify in this\nmanner, as one often needs to know the proper configuration for the agent. When\nhumans are learning to solve tasks, we often learn from visual instructions\ncomposed of images or videos. Such representations motivate our development of\nPerceptual Reward Functions, which provide a mechanism for creating visual task\ndescriptions. We show that this approach allows an agent to learn from rewards\nthat are based on raw pixels rather than internal parameters.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 15:29:05 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Edwards", "Ashley", ""], ["Isbell", "Charles", ""], ["Takanishi", "Atsuo", ""]]}, {"id": "1608.03845", "submitter": "Michael Grey", "authors": "Michael X. Grey, Aaron D. Ames, C. Karen Liu", "title": "Traversing Environments Using Possibility Graphs for Humanoid Robots", "comments": "Submitted to the International Workshop on the Algorithmic\n  Foundations of Robotics (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locomotion for legged robots poses considerable challenges when confronted by\nobstacles and adverse environments. Footstep planners are typically only\ndesigned for one mode of locomotion, but traversing unfavorable environments\nmay require several forms of locomotion to be sequenced together, such as\nwalking, crawling, and jumping. Multi-modal motion planners can be used to\naddress some of these problems, but existing implementations tend to be\ntime-consuming and are limited to quasi-static actions. This paper presents a\nmotion planning method to traverse complex environments using multiple\ncategories of actions. We introduce the concept of the \"Possibility Graph\",\nwhich uses high-level approximations of constraint manifolds to rapidly explore\nthe \"possibility\" of actions, thereby allowing lower-level single-action motion\nplanners to be utilized more efficiently. We show that the Possibility Graph\ncan quickly find paths through several different challenging environments which\nrequire various combinations of actions in order to traverse.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 16:47:01 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Grey", "Michael X.", ""], ["Ames", "Aaron D.", ""], ["Liu", "C. Karen", ""]]}, {"id": "1608.03938", "submitter": "Christopher Thompson", "authors": "Christopher Thompson, Josh Introne, and Clint Young", "title": "Determining Health Utilities through Data Mining of Social Media", "comments": "8 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  'Health utilities' measure patient preferences for perfect health compared to\nspecific unhealthy states, such as asthma, a fractured hip, or colon cancer.\nWhen integrated over time, these estimations are called quality adjusted life\nyears (QALYs). Until now, characterizing health utilities (HUs) required\ndetailed patient interviews or written surveys. While reliable and specific,\nthis data remained costly due to efforts to locate, enlist and coordinate\nparticipants. Thus the scope, context and temporality of diseases examined has\nremained limited.\n  Now that more than a billion people use social media, we propose a novel\nstrategy: use natural language processing to analyze public online\nconversations for signals of the severity of medical conditions and correlate\nthese to known HUs using machine learning. In this work, we filter a dataset\nthat originally contained 2 billion tweets for relevant content on 60 diseases.\nUsing this data, our algorithm successfully distinguished mild from severe\ndiseases, which had previously been categorized only by traditional techniques.\nThis represents progress towards two related applications: first, predicting\nHUs where such information is nonexistent; and second, (where rich HU data\nalready exists) estimating temporal or geographic patterns of disease severity\nthrough data mining.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 04:02:38 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Thompson", "Christopher", ""], ["Introne", "Josh", ""], ["Young", "Clint", ""]]}, {"id": "1608.04042", "submitter": "Arturo Deza", "authors": "Arturo Deza and Miguel P. Eckstein", "title": "Can Peripheral Representations Improve Clutter Metrics on Complex\n  Scenes?", "comments": "Pre-Print to be presented at NIPS 2016 in Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have proposed image-based clutter measures that correlate\nwith human search times and/or eye movements. However, most models do not take\ninto account the fact that the effects of clutter interact with the foveated\nnature of the human visual system: visual clutter further from the fovea has an\nincreasing detrimental influence on perception. Here, we introduce a new\nfoveated clutter model to predict the detrimental effects in target search\nutilizing a forced fixation search task. We use Feature Congestion (Rosenholtz\net al.) as our non foveated clutter model, and we stack a peripheral\narchitecture on top of Feature Congestion for our foveated model. We introduce\nthe Peripheral Integration Feature Congestion (PIFC) coefficient, as a\nfundamental ingredient of our model that modulates clutter as a non-linear gain\ncontingent on eccentricity. We finally show that Foveated Feature Congestion\n(FFC) clutter scores r(44) = -0.82 correlate better with target detection (hit\nrate) than regular Feature Congestion r(44) = -0.19 in forced fixation search.\nThus, our model allows us to enrich clutter perception research by computing\nfixation specific clutter maps. A toolbox for creating peripheral\narchitectures: Piranhas: Peripheral Architectures for Natural, Hybrid and\nArtificial Systems will be made available.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 01:07:29 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Deza", "Arturo", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "1608.04361", "submitter": "Tao Wu", "authors": "Tao Wu, David F. Gleich", "title": "Multi-way Monte Carlo Method for Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Monte Carlo method for solving a linear system of the form $x =\nH x + b$. A sufficient condition for the method to work is $\\| H \\| < 1$, which\ngreatly limits the usability of this method. We improve this condition by\nproposing a new multi-way Markov random walk, which is a generalization of the\nstandard Markov random walk. Under our new framework we prove that the\nnecessary and sufficient condition for our method to work is the spectral\nradius $\\rho(H^{+}) < 1$, which is a weaker requirement than $\\| H \\| < 1$. In\naddition to solving more problems, our new method can work faster than the\nstandard algorithm. In numerical experiments on both synthetic and real world\nmatrices, we demonstrate the effectiveness of our new method.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 18:45:08 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Wu", "Tao", ""], ["Gleich", "David F.", ""]]}, {"id": "1608.04374", "submitter": "Anthony Caterini", "authors": "Anthony L. Caterini, Dong Eui Chang", "title": "A Geometric Framework for Convolutional Neural Networks", "comments": "Added proofs and algorithms that were missing from previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a geometric framework for neural networks is proposed. This\nframework uses the inner product space structure underlying the parameter set\nto perform gradient descent not in a component-based form, but in a\ncoordinate-free manner. Convolutional neural networks are described in this\nframework in a compact form, with the gradients of standard --- and\nhigher-order --- loss functions calculated for each layer of the network. This\napproach can be applied to other network structures and provides a basis on\nwhich to create new networks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 19:38:35 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 17:45:06 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Caterini", "Anthony L.", ""], ["Chang", "Dong Eui", ""]]}, {"id": "1608.04428", "submitter": "Alexander Gaunt", "authors": "Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman,\n  Pushmeet Kohli, Jonathan Taylor, Daniel Tarlow", "title": "TerpreT: A Probabilistic Programming Language for Program Induction", "comments": "50 pages, 20 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study machine learning formulations of inductive program synthesis; given\ninput-output examples, we try to synthesize source code that maps inputs to\ncorresponding outputs. Our aims are to develop new machine learning approaches\nbased on neural networks and graphical models, and to understand the\ncapabilities of machine learning techniques relative to traditional\nalternatives, such as those based on constraint solving from the programming\nlanguages community.\n  Our key contribution is the proposal of TerpreT, a domain-specific language\nfor expressing program synthesis problems. TerpreT is similar to a\nprobabilistic programming language: a model is composed of a specification of a\nprogram representation (declarations of random variables) and an interpreter\ndescribing how programs map inputs to outputs (a model connecting unknowns to\nobservations). The inference task is to observe a set of input-output examples\nand infer the underlying program. TerpreT has two main benefits. First, it\nenables rapid exploration of a range of domains, program representations, and\ninterpreter models. Second, it separates the model specification from the\ninference algorithm, allowing like-to-like comparisons between different\napproaches to inference. From a single TerpreT specification we automatically\nperform inference using four different back-ends. These are based on gradient\ndescent, linear program (LP) relaxations for graphical models, discrete\nsatisfiability solving, and the Sketch program synthesis system.\n  We illustrate the value of TerpreT by developing several interpreter models\nand performing an empirical comparison between alternative inference\nalgorithms. Our key empirical finding is that constraint solvers dominate the\ngradient descent and LP-based formulations. We conclude with suggestions for\nthe machine learning community to make progress on program synthesis.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 22:34:50 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Gaunt", "Alexander L.", ""], ["Brockschmidt", "Marc", ""], ["Singh", "Rishabh", ""], ["Kushman", "Nate", ""], ["Kohli", "Pushmeet", ""], ["Taylor", "Jonathan", ""], ["Tarlow", "Daniel", ""]]}, {"id": "1608.04544", "submitter": "Tom Everitt", "authors": "Tom Everitt, Tor Lattimore, Marcus Hutter", "title": "Free Lunch for Optimisation under the Universal Distribution", "comments": null, "journal-ref": "Proceedings of 2014 IEEE Congress on Evolutionary Computation\n  (CEC), July 6-11, 2014, Beijing, China, pp. 167-174", "doi": "10.1109/CEC.2014.6900546", "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Function optimisation is a major challenge in computer science. The No Free\nLunch theorems state that if all functions with the same histogram are assumed\nto be equally probable then no algorithm outperforms any other in expectation.\nWe argue against the uniform assumption and suggest a universal prior exists\nfor which there is a free lunch, but where no particular class of functions is\nfavoured over another. We also prove upper and lower bounds on the size of the\nfree lunch.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 10:27:37 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Everitt", "Tom", ""], ["Lattimore", "Tor", ""], ["Hutter", "Marcus", ""]]}, {"id": "1608.04672", "submitter": "Kurt Ammon", "authors": "Kurt Ammon", "title": "Informal Physical Reasoning Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question is whether Turing machines can model all reasoning\nprocesses. We introduce an existence principle stating that the perception of\nthe physical existence of any Turing program can serve as a physical causation\nfor the application of any Turing-computable function to this Turing program.\nThe existence principle overcomes the limitation of the outputs of Turing\nmachines to lists, that is, recursively enumerable sets. The principle is\nillustrated by productive partial functions for productive sets such as the set\nof the Goedel numbers of the Turing-computable total functions. The existence\nprinciple and productive functions imply the existence of physical systems\nwhose reasoning processes cannot be modeled by Turing machines. These systems\nare called creative. Creative systems can prove the undecidable formula in\nGoedel's theorem in another formal system which is constructed at a later point\nin time. A hypothesis about creative systems, which is based on computer\nexperiments, is introduced.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 16:51:38 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Ammon", "Kurt", ""]]}, {"id": "1608.04689", "submitter": "Hongyu Guo", "authors": "Martin Renqiang Min, Hongyu Guo, Dongjin Song", "title": "A Shallow High-Order Parametric Approach to Data Visualization and\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicit high-order feature interactions efficiently capture essential\nstructural knowledge about the data of interest and have been used for\nconstructing generative models. We present a supervised discriminative\nHigh-Order Parametric Embedding (HOPE) approach to data visualization and\ncompression. Compared to deep embedding models with complicated deep\narchitectures, HOPE generates more effective high-order feature mapping through\nan embarrassingly simple shallow model. Furthermore, two approaches to\ngenerating a small number of exemplars conveying high-order interactions to\nrepresent large-scale data sets are proposed. These exemplars in combination\nwith the feature mapping learned by HOPE effectively capture essential data\nvariations. Moreover, through HOPE, these exemplars are employed to increase\nthe computational efficiency of kNN classification for fast information\nretrieval by thousands of times. For classification in two-dimensional\nembedding space on MNIST and USPS datasets, our shallow method HOPE with simple\nSigmoid transformations significantly outperforms state-of-the-art supervised\ndeep embedding models based on deep neural networks, and even achieved\nhistorically low test error rate of 0.65% in two-dimensional space on MNIST,\nwhich demonstrates the representational efficiency and power of supervised\nshallow models with high-order feature interactions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 17:54:40 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Min", "Martin Renqiang", ""], ["Guo", "Hongyu", ""], ["Song", "Dongjin", ""]]}, {"id": "1608.04698", "submitter": "Dan Garant", "authors": "Dan Garant, David Jensen", "title": "Evaluating Causal Models by Comparing Interventional Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predominant method for evaluating the quality of causal models is to\nmeasure the graphical accuracy of the learned model structure. We present an\nalternative method for evaluating causal models that directly measures the\naccuracy of estimated interventional distributions. We contrast such\ndistributional measures with structural measures, such as structural Hamming\ndistance and structural intervention distance, showing that structural measures\noften correspond poorly to the accuracy of estimated interventional\ndistributions. We use a number of real and synthetic datasets to illustrate\nvarious scenarios in which structural measures provide misleading results with\nrespect to algorithm selection and parameter tuning, and we recommend that\ndistributional measures become the new standard for evaluating causal models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 18:32:24 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Garant", "Dan", ""], ["Jensen", "David", ""]]}, {"id": "1608.04839", "submitter": "Ghassen Jerfel", "authors": "Ghassen Jerfel, Mehmet E. Basbug, Barbara E. Engelhardt", "title": "Dynamic Collaborative Filtering with Compound Poisson Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based collaborative filtering analyzes user-item interactions to infer\nlatent factors that represent user preferences and item characteristics in\norder to predict future interactions. Most collaborative filtering algorithms\nassume that these latent factors are static, although it has been shown that\nuser preferences and item perceptions drift over time. In this paper, we\npropose a conjugate and numerically stable dynamic matrix factorization (DCPF)\nbased on compound Poisson matrix factorization that models the smoothly\ndrifting latent factors using Gamma-Markov chains. We propose a numerically\nstable Gamma chain construction, and then present a stochastic variational\ninference approach to estimate the parameters of our model. We apply our model\nto time-stamped ratings data sets: Netflix, Yelp, and Last.fm, where DCPF\nachieves a higher predictive accuracy than state-of-the-art static and dynamic\nfactorization models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 02:38:44 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 19:19:24 GMT"}, {"version": "v3", "created": "Tue, 1 Nov 2016 07:23:16 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Jerfel", "Ghassen", ""], ["Basbug", "Mehmet E.", ""], ["Engelhardt", "Barbara E.", ""]]}, {"id": "1608.04846", "submitter": "Po-Hsuan Chen", "authors": "Po-Hsuan Chen, Xia Zhu, Hejia Zhang, Javier S. Turek, Janice Chen,\n  Theodore L. Willke, Uri Hasson, Peter J. Ramadge", "title": "A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the most effective way to aggregate multi-subject fMRI data is a\nlong-standing and challenging problem. It is of increasing interest in\ncontemporary fMRI studies of human cognition due to the scarcity of data per\nsubject and the variability of brain anatomy and functional response across\nsubjects. Recent work on latent factor models shows promising results in this\ntask but this approach does not preserve spatial locality in the brain. We\nexamine two ways to combine the ideas of a factor model and a searchlight based\nanalysis to aggregate multi-subject fMRI data while preserving spatial\nlocality. We first do this directly by combining a recent factor method known\nas a shared response model with searchlight analysis. Then we design a\nmulti-view convolutional autoencoder for the same task. Both approaches\npreserve spatial locality and have competitive or better performance compared\nwith standard searchlight analysis and the shared response model applied across\nthe whole brain. We also report a system design to handle the computational\nchallenge of training the convolutional autoencoder.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 03:49:56 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Chen", "Po-Hsuan", ""], ["Zhu", "Xia", ""], ["Zhang", "Hejia", ""], ["Turek", "Javier S.", ""], ["Chen", "Janice", ""], ["Willke", "Theodore L.", ""], ["Hasson", "Uri", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1608.04868", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi and George Fazekas and Brian McFee and Kyunghyun Cho and\n  Mark Sandler", "title": "Towards Music Captioning: Generating Music Playlist Descriptions", "comments": "2 pages, ISMIR 2016 Late-breaking/session extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Descriptions are often provided along with recommendations to help users'\ndiscovery. Recommending automatically generated music playlists (e.g.\npersonalised playlists) introduces the problem of generating descriptions. In\nthis paper, we propose a method for generating music playlist descriptions,\nwhich is called as music captioning. In the proposed method, audio content\nanalysis and natural language processing are adopted to utilise the information\nof each track.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 06:24:46 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 05:23:51 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["McFee", "Brian", ""], ["Cho", "Kyunghyun", ""], ["Sandler", "Mark", ""]]}, {"id": "1608.04996", "submitter": "Kamyar Azizzadenesheli Ph.D.", "authors": "Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar", "title": "Open Problem: Approximate Planning of POMDPs in the class of Memoryless\n  Policies", "comments": "arXiv admin note: substantial text overlap with arXiv:1602.07764", "journal-ref": "29th Annual Conference on Learning Theory (2016) 1639--1642", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning plays an important role in the broad class of decision theory.\nPlanning has drawn much attention in recent work in the robotics and sequential\ndecision making areas. Recently, Reinforcement Learning (RL), as an\nagent-environment interaction problem, has brought further attention to\nplanning methods. Generally in RL, one can assume a generative model, e.g.\ngraphical models, for the environment, and then the task for the RL agent is to\nlearn the model parameters and find the optimal strategy based on these learnt\nparameters. Based on environment behavior, the agent can assume various types\nof generative models, e.g. Multi Armed Bandit for a static environment, or\nMarkov Decision Process (MDP) for a dynamic environment. The advantage of these\npopular models is their simplicity, which results in tractable methods of\nlearning the parameters and finding the optimal policy. The drawback of these\nmodels is again their simplicity: these models usually underfit and\nunderestimate the actual environment behavior. For example, in robotics, the\nagent usually has noisy observations of the environment inner state and MDP is\nnot a suitable model.\n  More complex models like Partially Observable Markov Decision Process (POMDP)\ncan compensate for this drawback. Fitting this model to the environment, where\nthe partial observation is given to the agent, generally gives dramatic\nperformance improvement, sometimes unbounded improvement, compared to MDP. In\ngeneral, finding the optimal policy for the POMDP model is computationally\nintractable and fully non convex, even for the class of memoryless policies.\nThe open problem is to come up with a method to find an exact or an approximate\noptimal stochastic memoryless policy for POMDP models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 15:20:35 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Azizzadenesheli", "Kamyar", ""], ["Lazaric", "Alessandro", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1608.05046", "submitter": "Long Ouyang", "authors": "Long Ouyang, Michael Henry Tessler, Daniel Ly, Noah Goodman", "title": "Practical optimal experiment design with probabilistic programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists often run experiments to distinguish competing theories. This\nrequires patience, rigor, and ingenuity - there is often a large space of\npossible experiments one could run. But we need not comb this space by hand -\nif we represent our theories as formal models and explicitly declare the space\nof experiments, we can automate the search for good experiments, looking for\nthose with high expected information gain. Here, we present a general and\nprincipled approach to experiment design based on probabilistic programming\nlanguages (PPLs). PPLs offer a clean separation between declaring problems and\nsolving them, which means that the scientist can automate experiment design by\nsimply declaring her model and experiment spaces in the PPL without having to\nworry about the details of calculating information gain. We demonstrate our\nsystem in two case studies drawn from cognitive psychology, where we use it to\ndesign optimal experiments in the domains of sequence prediction and\ncategorization. We find strong empirical validation that our automatically\ndesigned experiments were indeed optimal. We conclude by discussing a number of\ninteresting questions for future research.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 18:59:23 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Ouyang", "Long", ""], ["Tessler", "Michael Henry", ""], ["Ly", "Daniel", ""], ["Goodman", "Noah", ""]]}, {"id": "1608.05151", "submitter": "Harm Van Seijen", "authors": "Harm van Seijen", "title": "Effective Multi-step Temporal-Difference Learning for Non-Linear\n  Function Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-step temporal-difference (TD) learning, where the update targets\ncontain information from multiple time steps ahead, is one of the most popular\nforms of TD learning for linear function approximation. The reason is that\nmulti-step methods often yield substantially better performance than their\nsingle-step counter-parts, due to a lower bias of the update targets. For\nnon-linear function approximation, however, single-step methods appear to be\nthe norm. Part of the reason could be that on many domains the popular\nmulti-step methods TD($\\lambda$) and Sarsa($\\lambda$) do not perform well when\ncombined with non-linear function approximation. In particular, they are very\nsusceptible to divergence of value estimates. In this paper, we identify the\nreason behind this. Furthermore, based on our analysis, we propose a new\nmulti-step TD method for non-linear function approximation that addresses this\nissue. We confirm the effectiveness of our method using two benchmark tasks\nwith neural networks as function approximation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 01:21:27 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["van Seijen", "Harm", ""]]}, {"id": "1608.05288", "submitter": "Ferdinando Fioretto", "authors": "Ferdinando Fioretto, Enrico Pontelli, William Yeoh, Rina Dechter", "title": "Accelerating Exact and Approximate Inference for (Distributed) Discrete\n  Optimization with GPUs", "comments": null, "journal-ref": "Constraints (2018) 23: 1", "doi": "10.1007/s10601-017-9274-1", "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete optimization is a central problem in artificial intelligence. The\noptimization of the aggregated cost of a network of cost functions arises in a\nvariety of problems including (W)CSP, DCOP, as well as optimization in\nstochastic variants such as the tasks of finding the most probable explanation\n(MPE) in belief networks. Inference-based algorithms are powerful techniques\nfor solving discrete optimization problems, which can be used independently or\nin combination with other techniques. However, their applicability is often\nlimited by their compute intensive nature and their space requirements. This\npaper proposes the design and implementation of a novel inference-based\ntechnique, which exploits modern massively parallel architectures, such as\nthose found in Graphical Processing Units (GPUs), to speed up the resolution of\nexact and approximated inference-based algorithms for discrete optimization.\nThe paper studies the proposed algorithm in both centralized and distributed\noptimization contexts. The paper demonstrates that the use of GPUs provides\nsignificant advantages in terms of runtime and scalability, achieving up to two\norders of magnitude in speedups and showing a considerable reduction in\nexecution time (up to 345 times faster) with respect to a sequential version.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 15:14:37 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 02:14:06 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Fioretto", "Ferdinando", ""], ["Pontelli", "Enrico", ""], ["Yeoh", "William", ""], ["Dechter", "Rina", ""]]}, {"id": "1608.05347", "submitter": "Feras Saad", "authors": "Feras Saad, Vikash Mansinghka", "title": "Probabilistic Data Analysis with Probabilistic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic techniques are central to data analysis, but different\napproaches can be difficult to apply, combine, and compare. This paper\nintroduces composable generative population models (CGPMs), a computational\nabstraction that extends directed graphical models and can be used to describe\nand compose a broad class of probabilistic data analysis techniques. Examples\ninclude hierarchical Bayesian models, multivariate kernel methods,\ndiscriminative machine learning, clustering algorithms, dimensionality\nreduction, and arbitrary probabilistic programs. We also demonstrate the\nintegration of CGPMs into BayesDB, a probabilistic programming platform that\ncan express data analysis tasks using a modeling language and a structured\nquery language. The practical value is illustrated in two ways. First, CGPMs\nare used in an analysis that identifies satellite data records which probably\nviolate Kepler's Third Law, by composing causal probabilistic programs with\nnon-parametric Bayes in under 50 lines of probabilistic code. Second, for\nseveral representative data analysis tasks, we report on lines of code and\naccuracy measurements of various CGPMs, plus comparisons with standard baseline\nsolutions from Python and MATLAB libraries.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 17:47:53 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Saad", "Feras", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1608.05485", "submitter": "Iman Roozbeh", "authors": "Iman Roozbeh, Melih Ozlen, John W. Hearne", "title": "A heuristic scheme for the Cooperative Team Orienteering Problem with\n  Time Windows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cooperative Orienteering Problem with Time Windows (COPTW)is a class of\nproblems with some important applications and yet has received relatively\nlittle attention. In the COPTW a certain number of team members are required to\ncollect the associated reward from each customer simultaneously and\ncooperatively. This requirement to have one or more team members simultaneously\navailable at a vertex to collect the reward, poses a challenging OR task. Exact\nmethods are not able to handle large scale instances of the COPTW and no\nheuristic schemes have been developed for this problem so far. In this paper, a\nnew modification to the classical Clarke and Wright saving heuristic is\nproposed to handle this problem. A new benchmark set generated by adding the\nresource requirement attribute to the existing benchmarks. The heuristic\nalgorithm followed by boosting operators achieves optimal solutions for 64.5%\nof instances for which the optimal results are known. The proposed solution\napproach attains an optimality gap of 2.61% for the same instances and solves\nbenchmarks with realistic size within short computational times.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 03:36:18 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Roozbeh", "Iman", ""], ["Ozlen", "Melih", ""], ["Hearne", "John W.", ""]]}, {"id": "1608.05513", "submitter": "Sagar Gandhi", "authors": "Shraddha Deshmukh, Sagar Gandhi, Pratap Sanap and Vivek Kulkarni", "title": "Data Centroid Based Multi-Level Fuzzy Min-Max Neural Network", "comments": "This paper has been withdrawn by the author due to crucial evidence\n  that the similar work has already been published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a multi-level fuzzy min max neural network (MLF) was proposed,\nwhich improves the classification accuracy by handling an overlapped region\n(area of confusion) with the help of a tree structure. In this brief, an\nextension of MLF is proposed which defines a new boundary region, where the\npreviously proposed methods mark decisions with less confidence and hence\nmisclassification is more frequent. A methodology to classify patterns more\naccurately is presented. Our work enhances the testing procedure by means of\ndata centroids. We exhibit an illustrative example, clearly highlighting the\nadvantage of our approach. Results on standard datasets are also presented to\nevidentially prove a consistent improvement in the classification rate.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 07:05:33 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 08:09:40 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Deshmukh", "Shraddha", ""], ["Gandhi", "Sagar", ""], ["Sanap", "Pratap", ""], ["Kulkarni", "Vivek", ""]]}, {"id": "1608.05609", "submitter": "Bart Bogaerts", "authors": "Joachim Jansen, Jo Devriendt, Bart Bogaerts, Gerda Janssens, Marc\n  Denecker", "title": "Implementing a Relevance Tracker Module", "comments": "Paper presented at the 9th Workshop on Answer Set Programming and\n  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PC(ID) extends propositional logic with inductive definitions: rule sets\nunder the well-founded semantics. Recently, a notion of relevance was\nintroduced for this language. This notion determines the set of undecided\nliterals that can still influence the satisfiability of a PC(ID) formula in a\ngiven partial assignment. The idea is that the PC(ID) solver can make decisions\nonly on relevant literals without losing soundness and thus safely ignore\nirrelevant literals.\n  One important insight that the relevance of a literal is completely\ndetermined by the current solver state. During search, the solver state changes\nhave an effect on the relevance of literals. In this paper, we discuss an\nincremental, lightweight implementation of a relevance tracker module that can\nbe added to and interact with an out-of-the-box SAT(ID) solver.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 14:19:21 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Jansen", "Joachim", ""], ["Devriendt", "Jo", ""], ["Bogaerts", "Bart", ""], ["Janssens", "Gerda", ""], ["Denecker", "Marc", ""]]}, {"id": "1608.05675", "submitter": "Michael Morak", "authors": "Manuel Bichler, Michael Morak and Stefan Woltran", "title": "lpopt: A Rule Optimization Tool for Answer Set Programming", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534), 14 pages, LaTeX, 2\n  figures", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/40", "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art answer set programming (ASP) solvers rely on a program\ncalled a grounder to convert non-ground programs containing variables into\nvariable-free, propositional programs. The size of this grounding depends\nheavily on the size of the non-ground rules, and thus, reducing the size of\nsuch rules is a promising approach to improve solving performance. To this end,\nin this paper we announce lpopt, a tool that decomposes large logic programming\nrules into smaller rules that are easier to handle for current solvers. The\ntool is specifically tailored to handle the standard syntax of the ASP language\n(ASP-Core) and makes it easier for users to write efficient and intuitive ASP\nprograms, which would otherwise often require significant hand-tuning by expert\nASP engineers. It is based on an idea proposed by Morak and Woltran (2012) that\nwe extend significantly in order to handle the full ASP syntax, including\ncomplex constructs like aggregates, weak constraints, and arithmetic\nexpressions. We present the algorithm, the theoretical foundations on how to\ntreat these constructs, as well as an experimental evaluation showing the\nviability of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 17:20:03 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 07:59:54 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Bichler", "Manuel", ""], ["Morak", "Michael", ""], ["Woltran", "Stefan", ""]]}, {"id": "1608.05694", "submitter": "Vladislav Kovchegov B", "authors": "Vladislav B Kovchegov", "title": "The languages of actions, formal grammars and qualitive modeling of\n  companies", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss methods of using the language of actions, formal\nlanguages, and grammars for qualitative conceptual linguistic modeling of\ncompanies as technological and human institutions. The main problem following\nthe discussion is the problem to find and describe a language structure for\nexternal and internal flow of information of companies. We anticipate that the\nlanguage structure of external and internal base flows determine the structure\nof companies. In the structure modeling of an abstract industrial company an\ninternal base flow of information is constructed as certain flow of words\ncomposed on the theoretical parts-processes-actions language. The language of\nprocedures is found for an external base flow of information for an insurance\ncompany. The formal stochastic grammar for the language of procedures is found\nby statistical methods and is used in understanding the tendencies of the\nhealth care industry. We present the model of human communications as a random\nwalk on the semantic tree\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 18:50:21 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Kovchegov", "Vladislav B", ""]]}, {"id": "1608.05701", "submitter": "Eric Rice", "authors": "Eric Rice, Robin Petering, Jaih Craddock, Amanda Yoshioka-Maxwell,\n  Amulya Yadav, and Milind Tambe", "title": "Pilot Testing an Artificial Intelligence Algorithm That Selects Homeless\n  Youth Peer Leaders Who Promote HIV Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective. To pilot test an artificial intelligence (AI) algorithm that\nselects peer change agents (PCA) to disseminate HIV testing messaging in a\npopulation of homeless youth. Methods. We recruited and assessed 62 youth at\nbaseline, 1 month (n = 48), and 3 months (n = 38). A Facebook app collected\npreliminary social network data. Eleven PCAs selected by AI attended a 1-day\ntraining and 7 weekly booster sessions. Mixed-effects models with random\neffects were used to assess change over time. Results. Significant change over\ntime was observed in past 6-month HIV testing (57.9%, 82.4%, 76.3%; p < .05)\nbut not condom use (63.9%, 65.7%, 65.8%). Most youth reported speaking to a PCA\nabout HIV prevention (72.0% at 1 month, 61.5% at 3 months). Conclusions. AI is\na promising avenue for implementing PCA models for homeless youth. Increasing\nrates of regular HIV testing is critical to HIV prevention and linking homeless\nyouth to treatment.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 19:28:55 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Rice", "Eric", ""], ["Petering", "Robin", ""], ["Craddock", "Jaih", ""], ["Yoshioka-Maxwell", "Amanda", ""], ["Yadav", "Amulya", ""], ["Tambe", "Milind", ""]]}, {"id": "1608.05745", "submitter": "Edward Choi", "authors": "Edward Choi, Mohammad Taha Bahadori, Joshua A. Kulas, Andy Schuetz,\n  Walter F. Stewart, Jimeng Sun", "title": "RETAIN: An Interpretable Predictive Model for Healthcare using Reverse\n  Time Attention Mechanism", "comments": "Accepted at Neural Information Processing Systems (NIPS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy and interpretability are two dominant features of successful\npredictive models. Typically, a choice must be made in favor of complex black\nbox models such as recurrent neural networks (RNN) for accuracy versus less\naccurate but more interpretable traditional models such as logistic regression.\nThis tradeoff poses challenges in medicine where both accuracy and\ninterpretability are important. We addressed this challenge by developing the\nREverse Time AttentIoN model (RETAIN) for application to Electronic Health\nRecords (EHR) data. RETAIN achieves high accuracy while remaining clinically\ninterpretable and is based on a two-level neural attention model that detects\ninfluential past visits and significant clinical variables within those visits\n(e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR\ndata in a reverse time order so that recent clinical visits are likely to\nreceive higher attention. RETAIN was tested on a large health system EHR\ndataset with 14 million visits completed by 263K patients over an 8 year period\nand demonstrated predictive accuracy and computational scalability comparable\nto state-of-the-art methods such as RNN, and ease of interpretability\ncomparable to traditional models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 21:54:46 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 06:03:43 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2016 19:45:03 GMT"}, {"version": "v4", "created": "Sun, 26 Feb 2017 15:13:31 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Choi", "Edward", ""], ["Bahadori", "Mohammad Taha", ""], ["Kulas", "Joshua A.", ""], ["Schuetz", "Andy", ""], ["Stewart", "Walter F.", ""], ["Sun", "Jimeng", ""]]}, {"id": "1608.05763", "submitter": "Arun Nampally", "authors": "Arun Nampally and C. R. Ramakrishnan", "title": "Inference in Probabilistic Logic Programs using Lifted Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of lifted inference in the context of\nPrism-like probabilistic logic programming languages. Traditional inference in\nsuch languages involves the construction of an explanation graph for the query\nand computing probabilities over this graph. When evaluating queries over\nprobabilistic logic programs with a large number of instances of random\nvariables, traditional methods treat each instance separately. For many\nprograms and queries, we observe that explanations can be summarized into\nsubstantially more compact structures, which we call lifted explanation graphs.\nIn this paper, we define lifted explanation graphs and operations over them. In\ncontrast to existing lifted inference techniques, our method for constructing\nlifted explanations naturally generalizes existing methods for constructing\nexplanation graphs. To compute probability of query answers, we solve\nrecurrences generated from the lifted graphs. We show examples where the use of\nour technique reduces the asymptotic complexity of inference.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 00:37:20 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Nampally", "Arun", ""], ["Ramakrishnan", "C. R.", ""]]}, {"id": "1608.05852", "submitter": "Jifan Chen", "authors": "Jifan Chen, Kan Chen, Xipeng Qiu, Qi Zhang, Xuanjing Huang, Zheng\n  Zhang", "title": "Learning Word Embeddings from Intrinsic and Extrinsic Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While word embeddings are currently predominant for natural language\nprocessing, most of existing models learn them solely from their contexts.\nHowever, these context-based word embeddings are limited since not all words'\nmeaning can be learned based on only context. Moreover, it is also difficult to\nlearn the representation of the rare words due to data sparsity problem. In\nthis work, we address these issues by learning the representations of words by\nintegrating their intrinsic (descriptive) and extrinsic (contextual)\ninformation. To prove the effectiveness of our model, we evaluate it on four\ntasks, including word similarity, reverse dictionaries,Wiki link prediction,\nand document classification. Experiment results show that our model is powerful\nin both word and document modeling.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 17:34:38 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Chen", "Jifan", ""], ["Chen", "Kan", ""], ["Qiu", "Xipeng", ""], ["Zhang", "Qi", ""], ["Huang", "Xuanjing", ""], ["Zhang", "Zheng", ""]]}, {"id": "1608.05864", "submitter": "Ahmad Masoud Dr", "authors": "Ahmad A. Masoud", "title": "A Hybrid, PDE-ODE Control Strategy for Intercepting an Intelligent,\n  well-informed Target in a Stationary, Cluttered Environment", "comments": "22 pages, 20 figures, Journal paper", "journal-ref": "Applied Mathematical Sciences, HIKARI Ltd, Vol. 1, 2007, No. 48,\n  2345-2371", "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [1,2] a new class of intelligent controllers that can semantically embed\nan agent in a spatial context constraining its behavior in a goal-oriented\nmanner was suggested. A controller of such a class can guide an agent in a\nstationary unknown environment to a fixed target zone along an obstacle-free\ntrajectory. Here, an extension is suggested that would enable the interception\nof an intelligent target that is maneuvering to evade capture amidst stationary\nclutter (i.e. the target zone is moving). This is achieved by forcing the\ndifferential properties of the potential field used to induce the control\naction to satisfy the wave equation. Background of the problem, theoretical\ndevelopments, as well as, proofs of the ability of the modified control to\nintercept the target along an obstacle-free trajectory are supplied. Simulation\nresults are also provided.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 19:02:19 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Masoud", "Ahmad A.", ""]]}, {"id": "1608.05921", "submitter": "Dongwoo Kim", "authors": "Dongwoo Kim, Lexing Xie, Cheng Soon Ong", "title": "Probabilistic Knowledge Graph Construction: Compositional and\n  Incremental Approaches", "comments": "The 25th ACM International Conference on Information and Knowledge\n  Management (CIKM 2016)", "journal-ref": null, "doi": "10.1145/2983323.2983677", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph construction consists of two tasks: extracting information\nfrom external resources (knowledge population) and inferring missing\ninformation through a statistical analysis on the extracted information\n(knowledge completion). In many cases, insufficient external resources in the\nknowledge population hinder the subsequent statistical inference. The gap\nbetween these two processes can be reduced by an incremental population\napproach. We propose a new probabilistic knowledge graph factorisation method\nthat benefits from the path structure of existing knowledge (e.g. syllogism)\nand enables a common modelling approach to be used for both incremental\npopulation and knowledge completion tasks. More specifically, the probabilistic\nformulation allows us to develop an incremental population algorithm that\ntrades off exploitation-exploration. Experiments on three benchmark datasets\nshow that the balanced exploitation-exploration helps the incremental\npopulation, and the additional path structure helps to predict missing\ninformation in knowledge completion.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 11:49:53 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 04:52:33 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Kim", "Dongwoo", ""], ["Xie", "Lexing", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1608.06010", "submitter": "Yun Wang", "authors": "Yun Wang, Xu Chen and Peter J. Ramadge", "title": "Feedback-Controlled Sequential Lasso Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to solve lasso problems when the dictionary does not fit into\navailable memory is to first screen the dictionary to remove unneeded features.\nPrior research has shown that sequential screening methods offer the greatest\npromise in this endeavor. Most existing work on sequential screening targets\nthe context of tuning parameter selection, where one screens and solves a\nsequence of $N$ lasso problems with a fixed grid of geometrically spaced\nregularization parameters. In contrast, we focus on the scenario where a target\nregularization parameter has already been chosen via cross-validated model\nselection, and we then need to solve many lasso instances using this fixed\nvalue. In this context, we propose and explore a feedback controlled sequential\nscreening scheme. Feedback is used at each iteration to select the next problem\nto be solved. This allows the sequence of problems to be adapted to the\ninstance presented and the number of intermediate problems to be automatically\nselected. We demonstrate our feedback scheme using several datasets including a\ndictionary of approximate size 100,000 by 300,000.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 23:40:56 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 22:52:30 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Wang", "Yun", ""], ["Chen", "Xu", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1608.06014", "submitter": "Yun Wang", "authors": "Yun Wang and Peter J. Ramadge", "title": "The Symmetry of a Simple Optimization Problem in Lasso Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently dictionary screening has been proposed as an effective way to\nimprove the computational efficiency of solving the lasso problem, which is one\nof the most commonly used method for learning sparse representations. To\naddress today's ever increasing large dataset, effective screening relies on a\ntight region bound on the solution to the dual lasso. Typical region bounds are\nin the form of an intersection of a sphere and multiple half spaces. One way to\ntighten the region bound is using more half spaces, which however, adds to the\noverhead of solving the high dimensional optimization problem in lasso\nscreening. This paper reveals the interesting property that the optimization\nproblem only depends on the projection of features onto the subspace spanned by\nthe normals of the half spaces. This property converts an optimization problem\nin high dimension to much lower dimension, and thus sheds light on reducing the\ncomputation overhead of lasso screening based on tighter region bounds.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 23:48:43 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 22:05:24 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Wang", "Yun", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1608.06154", "submitter": "Pankaj Malhotra Mr.", "authors": "Pankaj Malhotra, Vishnu TV, Anusha Ramakrishnan, Gaurangi Anand,\n  Lovekesh Vig, Puneet Agarwal, Gautam Shroff", "title": "Multi-Sensor Prognostics using an Unsupervised Health Index based on\n  LSTM Encoder-Decoder", "comments": "Presented at 1st ACM SIGKDD Workshop on Machine Learning for\n  Prognostics and Health Management, San Francisco, CA, USA, 2016. 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches for estimation of Remaining Useful Life (RUL) of a machine,\nusing its operational sensor data, make assumptions about how a system degrades\nor a fault evolves, e.g., exponential degradation. However, in many domains\ndegradation may not follow a pattern. We propose a Long Short Term Memory based\nEncoder-Decoder (LSTM-ED) scheme to obtain an unsupervised health index (HI)\nfor a system using multi-sensor time-series data. LSTM-ED is trained to\nreconstruct the time-series corresponding to healthy state of a system. The\nreconstruction error is used to compute HI which is then used for RUL\nestimation. We evaluate our approach on publicly available Turbofan Engine and\nMilling Machine datasets. We also present results on a real-world industry\ndataset from a pulverizer mill where we find significant correlation between\nLSTM-ED based HI and maintenance costs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 12:59:31 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Malhotra", "Pankaj", ""], ["TV", "Vishnu", ""], ["Ramakrishnan", "Anusha", ""], ["Anand", "Gaurangi", ""], ["Vig", "Lovekesh", ""], ["Agarwal", "Puneet", ""], ["Shroff", "Gautam", ""]]}, {"id": "1608.06175", "submitter": "Andrej Gajduk", "authors": "Andrej Gajduk", "title": "Effectiveness of greedily collecting items in open world games", "comments": "3 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Pokemon Go sent millions on the quest of collecting virtual monsters,\nan important question has been on the minds of many people: Is going after the\nclosest item first a time-and-cost-effective way to play? Here, we show that\nthis is in fact a good strategy which performs on average only 7% worse than\nthe best possible solution in terms of the total distance traveled to gather\nall the items. Even when accounting for errors due to the inability of people\nto accurately measure distances by eye, the performance only goes down to 16%\nof the optimal solution.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 20:43:56 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Gajduk", "Andrej", ""]]}, {"id": "1608.06349", "submitter": "Don Perlis", "authors": "Don Perlis", "title": "Five dimensions of reasoning in the wild", "comments": "minor typos corrected from AAAI version, Proceedings (Blue-Sky track)\n  AAAI-2016, Phoenix AZ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning does not work well when done in isolation from its significance,\nboth to the needs and interests of an agent and with respect to the wider\nworld. Moreover, those issues may best be handled with a new sort of data\nstructure that goes beyond the knowledge base and incorporates aspects of\nperceptual knowledge and even more, in which a kind of anticipatory action may\nbe key.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 00:40:27 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Perlis", "Don", ""]]}, {"id": "1608.06403", "submitter": "Sougata Chaudhuri", "authors": "Sougata Chaudhuri and Ambuj Tewari", "title": "Phased Exploration with Greedy Exploitation in Stochastic Combinatorial\n  Partial Monitoring Games", "comments": "Appearing in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial monitoring games are repeated games where the learner receives\nfeedback that might be different from adversary's move or even the reward\ngained by the learner. Recently, a general model of combinatorial partial\nmonitoring (CPM) games was proposed \\cite{lincombinatorial2014}, where the\nlearner's action space can be exponentially large and adversary samples its\nmoves from a bounded, continuous space, according to a fixed distribution. The\npaper gave a confidence bound based algorithm (GCB) that achieves\n$O(T^{2/3}\\log T)$ distribution independent and $O(\\log T)$ distribution\ndependent regret bounds. The implementation of their algorithm depends on two\nseparate offline oracles and the distribution dependent regret additionally\nrequires existence of a unique optimal action for the learner. Adopting their\nCPM model, our first contribution is a Phased Exploration with Greedy\nExploitation (PEGE) algorithmic framework for the problem. Different algorithms\nwithin the framework achieve $O(T^{2/3}\\sqrt{\\log T})$ distribution independent\nand $O(\\log^2 T)$ distribution dependent regret respectively. Crucially, our\nframework needs only the simpler \"argmax\" oracle from GCB and the distribution\ndependent regret does not require existence of a unique optimal action. Our\nsecond contribution is another algorithm, PEGE2, which combines gap estimation\nwith a PEGE algorithm, to achieve an $O(\\log T)$ regret bound, matching the GCB\nguarantee but removing the dependence on size of the learner's action space.\nHowever, like GCB, PEGE2 requires access to both offline oracles and the\nexistence of a unique optimal action. Finally, we discuss how our algorithm can\nbe efficiently applied to a CPM problem of practical interest: namely, online\nranking with feedback at the top.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 07:14:18 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Chaudhuri", "Sougata", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1608.06651", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Maarten de Rijke, Marcel Worring", "title": "Unsupervised, Efficient and Semantic Expertise Retrieval", "comments": "WWW2016, Proceedings of the 25th International Conference on World\n  Wide Web. 2016", "journal-ref": null, "doi": "10.1145/2872427.2882974", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an unsupervised discriminative model for the task of retrieving\nexperts in online document collections. We exclusively employ textual evidence\nand avoid explicit feature engineering by learning distributed word\nrepresentations in an unsupervised way. We compare our model to\nstate-of-the-art unsupervised statistical vector space and probabilistic\ngenerative approaches. Our proposed log-linear model achieves the retrieval\nperformance levels of state-of-the-art document-centric methods with the low\ninference cost of so-called profile-centric approaches. It yields a\nstatistically significant improved ranking over vector space and generative\nmodels in most cases, matching the performance of supervised methods on various\nbenchmarks. That is, by using solely text we can do as well as methods that\nwork with external evidence and/or relevance feedback. A contrastive analysis\nof rankings produced by discriminative and generative approaches shows that\nthey have complementary strengths due to the ability of the unsupervised\ndiscriminative model to perform semantic matching.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 20:55:09 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 04:57:54 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Van Gysel", "Christophe", ""], ["de Rijke", "Maarten", ""], ["Worring", "Marcel", ""]]}, {"id": "1608.06787", "submitter": "Natasha Alechina", "authors": "Natasha Alechina, Mehdi Dastani, and Brian Logan", "title": "Expressibility of norms in temporal logic", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note we address the issue of expressing norms (such as\nobligations and prohibitions) in temporal logic. In particular, we address the\nargument from [Governatori 2015] that norms cannot be expressed in Linear Time\nTemporal Logic (LTL).\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 12:01:36 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Alechina", "Natasha", ""], ["Dastani", "Mehdi", ""], ["Logan", "Brian", ""]]}, {"id": "1608.06845", "submitter": "Salisu Abdulrahman", "authors": "Salisu Mamman Abdulrahman, Pavel Brazdil", "title": "Effect of Incomplete Meta-dataset on Average Ranking Method", "comments": "8 pages, two figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the simplest metalearning methods is the average ranking method. This\nmethod uses metadata in the form of test results of a given set of algorithms\non given set of datasets and calculates an average rank for each algorithm. The\nranks are used to construct the average ranking. We investigate the problem of\nhow the process of generating the average ranking is affected by incomplete\nmetadata including fewer test results. This issue is relevant, because if we\ncould show that incomplete metadata does not affect the final results much, we\ncould explore it in future design. We could simply conduct fewer tests and save\nthus computation time. In this paper we describe an upgraded average ranking\nmethod that is capable of dealing with incomplete metadata. Our results show\nthat the proposed method is relatively robust to omission in test results in\nthe meta datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 14:44:33 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Abdulrahman", "Salisu Mamman", ""], ["Brazdil", "Pavel", ""]]}, {"id": "1608.06910", "submitter": "Patrick Kahl", "authors": "Patrick Thor Kahl, Anthony P. Leclerc, Tran Cao Son", "title": "A Parallel Memory-efficient Epistemic Logic Program Solver: Harder,\n  Better, Faster", "comments": "Paper presented at the 9th Workshop on Answer Set Programming and\n  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the practical use of answer set programming (ASP) has grown with the\ndevelopment of efficient solvers, we expect a growing interest in extensions of\nASP as their semantics stabilize and solvers supporting them mature. Epistemic\nSpecifications, which adds modal operators K and M to the language of ASP, is\none such extension. We call a program in this language an epistemic logic\nprogram (ELP). Solvers have thus far been practical for only the simplest ELPs\ndue to exponential growth of the search space. We describe a solver that is\nable to solve harder problems better (e.g., without exponentially-growing\nmemory needs w.r.t. K and M occurrences) and faster than any other known ELP\nsolver.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 18:18:08 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 16:25:52 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Kahl", "Patrick Thor", ""], ["Leclerc", "Anthony P.", ""], ["Son", "Tran Cao", ""]]}, {"id": "1608.06954", "submitter": "Hiroyuki Kasai", "authors": "Hiromi Narimatsu and Hiroyuki Kasai", "title": "State Duration and Interval Modeling in Hidden Semi-Markov Model for\n  Sequential Data Analysis", "comments": null, "journal-ref": "Annals of Mathematics and Artificial Intelligence, vol.81, Issue\n  3-4, pp.377-403, 2017", "doi": "10.1007/s10472-017-9561-y", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential data modeling and analysis have become indispensable tools for\nanalyzing sequential data, such as time-series data, because larger amounts of\nsensed event data have become available. These methods capture the sequential\nstructure of data of interest, such as input-output relations and correlation\namong datasets. However, because most studies in this area are specialized or\nlimited to their respective applications, rigorous requirement analysis of such\nmodels has not been undertaken from a general perspective. Therefore, we\nparticularly examine the structure of sequential data, and extract the\nnecessity of `state duration' and `state interval' of events for efficient and\nrich representation of sequential data. Specifically addressing the hidden\nsemi-Markov model (HSMM) that represents such state duration inside a model, we\nattempt to add representational capability of a state interval of events onto\nHSMM. To this end, we propose two extended models: an interval state hidden\nsemi-Markov model (IS-HSMM) to express the length of a state interval with a\nspecial state node designated as \"interval state node\"; and an interval length\nprobability hidden semi-Markov model (ILP-HSMM) which represents the length of\nthe state interval with a new probabilistic parameter \"interval length\nprobability.\" Exhaustive simulations have revealed superior performance of the\nproposed models in comparison with HSMM. These proposed models are the first\nreported extensions of HMM to support state interval representation as well as\nstate duration representation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 20:11:14 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 23:05:06 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Narimatsu", "Hiromi", ""], ["Kasai", "Hiroyuki", ""]]}, {"id": "1608.07001", "submitter": "Yangtao Wang", "authors": "Yangtao Wang, Lihui Chen, Xiaoli Li", "title": "Incremental Minimax Optimization based Fuzzy Clustering for Large\n  Multi-view Data", "comments": "32 pages, 1 figures, submitted to Fuzzy Sets and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental clustering approaches have been proposed for handling large data\nwhen given data set is too large to be stored. The key idea of these approaches\nis to find representatives to represent each cluster in each data chunk and\nfinal data analysis is carried out based on those identified representatives\nfrom all the chunks. However, most of the incremental approaches are used for\nsingle view data. As large multi-view data generated from multiple sources\nbecomes prevalent nowadays, there is a need for incremental clustering\napproaches to handle both large and multi-view data. In this paper we propose a\nnew incremental clustering approach called incremental minimax optimization\nbased fuzzy clustering (IminimaxFCM) to handle large multi-view data. In\nIminimaxFCM, representatives with multiple views are identified to represent\neach cluster by integrating multiple complementary views using minimax\noptimization. The detailed problem formulation, updating rules derivation, and\nthe in-depth analysis of the proposed IminimaxFCM are provided. Experimental\nstudies on several real world multi-view data sets have been conducted. We\nobserved that IminimaxFCM outperforms related incremental fuzzy clustering in\nterms of clustering accuracy, demonstrating the great potential of IminimaxFCM\nfor large multi-view data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 01:56:20 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Wang", "Yangtao", ""], ["Chen", "Lihui", ""], ["Li", "Xiaoli", ""]]}, {"id": "1608.07005", "submitter": "Yangtao Wang", "authors": "Yangtao Wang, Lihui Chen", "title": "Multi-View Fuzzy Clustering with Minimax Optimization for Effective\n  Clustering of Data from Multiple Sources", "comments": "34 pages, submitted to Expert Systems with Applications. arXiv admin\n  note: text overlap with arXiv:1608.07001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view data clustering refers to categorizing a data set by making good\nuse of related information from multiple representations of the data. It\nbecomes important nowadays because more and more data can be collected in a\nvariety of ways, in different settings and from different sources, so each data\nset can be represented by different sets of features to form different views of\nit. Many approaches have been proposed to improve clustering performance by\nexploring and integrating heterogeneous information underlying different views.\nIn this paper, we propose a new multi-view fuzzy clustering approach called\nMinimaxFCM by using minimax optimization based on well-known Fuzzy c means. In\nMinimaxFCM the consensus clustering results are generated based on minimax\noptimization in which the maximum disagreements of different weighted views are\nminimized. Moreover, the weight of each view can be learned automatically in\nthe clustering process. In addition, there is only one parameter to be set\nbesides the fuzzifier. The detailed problem formulation, updating rules\nderivation, and the in-depth analysis of the proposed MinimaxFCM are provided\nhere. Experimental studies on nine multi-view data sets including real world\nimage and document data sets have been conducted. We observed that MinimaxFCM\noutperforms related multi-view clustering approaches in terms of clustering\naccuracy, demonstrating the great potential of MinimaxFCM for multi-view data\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 02:15:37 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Wang", "Yangtao", ""], ["Chen", "Lihui", ""]]}, {"id": "1608.07068", "submitter": "Kuo-Hao Zeng", "authors": "Kuo-Hao Zeng and Tseng-Hung Chen and Juan Carlos Niebles and Min Sun", "title": "Title Generation for User Generated Videos", "comments": "14 pages, 4 figures, ECCV2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great video title describes the most salient event compactly and captures\nthe viewer's attention. In contrast, video captioning tends to generate\nsentences that describe the video as a whole. Although generating a video title\nautomatically is a very useful task, it is much less addressed than video\ncaptioning. We address video title generation for the first time by proposing\ntwo methods that extend state-of-the-art video captioners to this new task.\nFirst, we make video captioners highlight sensitive by priming them with a\nhighlight detector. Our framework allows for jointly training a model for title\ngeneration and video highlight localization. Second, we induce high sentence\ndiversity in video captioners, so that the generated titles are also diverse\nand catchy. This means that a large number of sentences might be required to\nlearn the sentence structure of titles. Hence, we propose a novel sentence\naugmentation method to train a captioner with additional sentence-only examples\nthat come without corresponding videos. We collected a large-scale Video Titles\nin the Wild (VTW) dataset of 18100 automatically crawled user-generated videos\nand titles. On VTW, our methods consistently improve title prediction accuracy,\nand achieve the best performance in both automatic and human evaluation.\nFinally, our sentence augmentation method also outperforms the baselines on the\nM-VAD dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 09:49:23 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 17:36:13 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Zeng", "Kuo-Hao", ""], ["Chen", "Tseng-Hung", ""], ["Niebles", "Juan Carlos", ""], ["Sun", "Min", ""]]}, {"id": "1608.07117", "submitter": "Marwin Segler", "authors": "Marwin H.S. Segler, Mark P. Waller", "title": "Modelling Chemical Reasoning to Predict Reactions", "comments": "17 pages, 8 figures", "journal-ref": "Chem. Eur. J. 2017, 23, 6118-6128", "doi": "10.1002/chem.201604556", "report-no": null, "categories": "cs.AI physics.chem-ph q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to reason beyond established knowledge allows Organic Chemists to\nsolve synthetic problems and to invent novel transformations. Here, we propose\na model which mimics chemical reasoning and formalises reaction prediction as\nfinding missing links in a knowledge graph. We have constructed a knowledge\ngraph containing 14.4 million molecules and 8.2 million binary reactions, which\nrepresents the bulk of all chemical reactions ever published in the scientific\nliterature. Our model outperforms a rule-based expert system in the reaction\nprediction task for 180,000 randomly selected binary reactions. We show that\nour data-driven model generalises even beyond known reaction types, and is thus\ncapable of effectively (re-) discovering novel transformations (even including\ntransition-metal catalysed reactions). Our model enables computers to infer\nhypotheses about reactivity and reactions by only considering the intrinsic\nlocal structure of the graph, and because each single reaction prediction is\ntypically achieved in a sub-second time frame, our model can be used as a\nhigh-throughput generator of reaction hypotheses for reaction discovery.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 12:45:20 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Segler", "Marwin H. S.", ""], ["Waller", "Mark P.", ""]]}, {"id": "1608.07187", "submitter": "Aylin Caliskan", "authors": "Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan", "title": "Semantics derived automatically from language corpora contain human-like\n  biases", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": "10.1126/science.aal4230", "report-no": null, "categories": "cs.AI cs.CL cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial intelligence and machine learning are in a period of astounding\ngrowth. However, there are concerns that these technologies may be used, either\nwith or without intention, to perpetuate the prejudice and unfairness that\nunfortunately characterizes many human institutions. Here we show for the first\ntime that human-like semantic biases result from the application of standard\nmachine learning to ordinary language---the same sort of language humans are\nexposed to every day. We replicate a spectrum of standard human biases as\nexposed by the Implicit Association Test and other well-known psychological\nstudies. We replicate these using a widely used, purely statistical\nmachine-learning model---namely, the GloVe word embedding---trained on a corpus\nof text from the Web. Our results indicate that language itself contains\nrecoverable and accurate imprints of our historic biases, whether these are\nmorally neutral as towards insects or flowers, problematic as towards race or\ngender, or even simply veridical, reflecting the {\\em status quo} for the\ndistribution of gender with respect to careers or first names. These\nregularities are captured by machine learning along with the rest of semantics.\nIn addition to our empirical findings concerning language, we also contribute\nnew methods for evaluating bias in text, the Word Embedding Association Test\n(WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results\nhave implications not only for AI and machine learning, but also for the fields\nof psychology, sociology, and human ethics, since they raise the possibility\nthat mere exposure to everyday language can account for the biases we replicate\nhere.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 15:07:17 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 18:23:06 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 19:03:45 GMT"}, {"version": "v4", "created": "Thu, 25 May 2017 17:50:31 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Caliskan", "Aylin", ""], ["Bryson", "Joanna J.", ""], ["Narayanan", "Arvind", ""]]}, {"id": "1608.07223", "submitter": "J. Quetzalcoatl Toledo-Marin", "authors": "J. Quetzalc\\'oatl Toledo-Mar\\'in, Rogelio D\\'iaz-M\\'endez, Marcelo del\n  Castillo Mussot", "title": "Is a good offensive always the best defense?", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A checkers-like model game with a simplified set of rules is studied through\nextensive simulations of agents with different expertise and strategies. The\nintroduction of complementary strategies, in a quite general way, provides a\ntool to mimic the basic ingredients of a wide scope of real games. We find that\nonly for the player having the higher offensive expertise (the dominant player\n), maximizing the offensive always increases the probability to win. For the\nnon-dominant player, interestingly, a complete minimization of the offensive\nbecomes the best way to win in many situations, depending on the relative\nvalues of the defense expertise. Further simulations on the interplay of\ndefense expertise were done separately, in the context of a fully-offensive\nscenario, offering a starting point for analytical treatments. In particular,\nwe established that in this scenario the total number of moves is defined only\nby the player with the lower defensive expertise. We believe that these results\nstand for a first step towards a new way to improve decisions-making in a large\nnumber of zero-sum real games.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 15:31:36 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Toledo-Mar\u00edn", "J. Quetzalc\u00f3atl", ""], ["D\u00edaz-M\u00e9ndez", "Rogelio", ""], ["Mussot", "Marcelo del Castillo", ""]]}, {"id": "1608.07225", "submitter": "Joanna Tomasik", "authors": "Pierre Berg\\'e, Kaourintin Le Guiban, Arpad Rimmel, Joanna Tomasik", "title": "On Simulated Annealing Dedicated to Maximin Latin Hypercube Designs", "comments": "extended version of ACM GECCO 2016 paper entitled \"Search Space\n  Exploration and an Optimization Criterion for Hard Design Problems\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of our research was to enhance local search heuristics used to\nconstruct Latin Hypercube Designs. First, we introduce the \\textit{1D-move}\nperturbation to improve the space exploration performed by these algorithms.\nSecond, we propose a new evaluation function $\\psi_{p,\\sigma}$ specifically\ntargeting the Maximin criterion.\n  Exhaustive series of experiments with Simulated Annealing, which we used as a\ntypically well-behaving local search heuristics, confirm that our goal was\nreached as the result we obtained surpasses the best scores reported in the\nliterature. Furthermore, the $\\psi_{p,\\sigma}$ function seems very promising\nfor a wide spectrum of optimization problems through the Maximin criterion.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 14:55:43 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Berg\u00e9", "Pierre", ""], ["Guiban", "Kaourintin Le", ""], ["Rimmel", "Arpad", ""], ["Tomasik", "Joanna", ""]]}, {"id": "1608.07253", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Maarten de Rijke, Evangelos Kanoulas", "title": "Learning Latent Vector Spaces for Product Search", "comments": "CIKM2016, Proceedings of the 25th ACM International Conference on\n  Information and Knowledge Management. 2016", "journal-ref": null, "doi": "10.1145/2983323.2983702", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel latent vector space model that jointly learns the latent\nrepresentations of words, e-commerce products and a mapping between the two\nwithout the need for explicit annotations. The power of the model lies in its\nability to directly model the discriminative relation between products and a\nparticular word. We compare our method to existing latent vector space models\n(LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank\nsetting. Our latent vector space model achieves its enhanced performance as it\nlearns better product representations. Furthermore, the mapping from words to\nproducts and the representations of words benefit directly from the errors\npropagated back from the product representations during parameter estimation.\nWe provide an in-depth analysis of the performance of our model and analyze the\nstructure of the learned representations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 18:57:50 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Van Gysel", "Christophe", ""], ["de Rijke", "Maarten", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "1608.07398", "submitter": "EPTCS", "authors": "Gregor G\\\"ossler (INRIA, France), Oleg Sokolsky", "title": "Proceedings First Workshop on Causal Reasoning for Embedded and\n  safety-critical Systems Technologies", "comments": null, "journal-ref": "EPTCS 224, 2016", "doi": "10.4204/EPTCS.224", "report-no": null, "categories": "cs.LO cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal approaches for automated causality analysis, fault localization,\nexplanation of events, accountability and blaming have been proposed\nindependently by several communities --- in particular, AI, concurrency,\nmodel-based diagnosis, formal methods. Work on these topics has significantly\ngained speed during the last years. The goals of CREST are to bring together\nand foster exchange between researchers from the different communities, and to\npresent and discuss recent advances and new ideas in the field.\n  The workshop program consisted of a set of invited and contributed\npresentations that illustrate different techniques for, and applications of,\ncausality analysis and fault localization.\n  The program was anchored by two keynote talks. The keynote by Hana Chockler\n(King's College) provided a broad perspective on the application of causal\nreasoning based on Halpern and Pearl's definitions of actual causality to a\nvariety of application domains ranging from formal verification to legal\nreasoning. The keynote by Chao Wang (Virginia Tech) concentrated on\nconstraint-based analysis techniques for debugging and verifying concurrent\nprograms.\n  Workshop papers deal with compositional causality analysis and a wide\nspectrum of application for causal reasoning, such as debugging of\nprobabilistic models, accountability and responsibility, hazard analysis in\npractice based on Lewis' counterfactuals, and fault localization and repair.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 09:13:22 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["G\u00f6ssler", "Gregor", "", "INRIA, France"], ["Sokolsky", "Oleg", ""]]}, {"id": "1608.07440", "submitter": "Cinzia Di Giusto", "authors": "Franck Delaplace (IBISC), Cinzia Di Giusto, Jean-Louis Giavitto\n  (Repmus), Hanna Klaudel (IBISC)", "title": "Activity Networks with Delays An application to toxicity analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ANDy , Activity Networks with Delays, is a discrete time framework aimed at\nthe qualitative modelling of time-dependent activities. The modular and concise\nsyntax makes ANDy suitable for an easy and natural modelling of time-dependent\nbiological systems (i.e., regulatory pathways). Activities involve entities\nplaying the role of activators, inhibitors or products of biochemical network\noperation. Activities may have given duration, i.e., the time required to\nobtain results. An entity may represent an object (e.g., an agent, a\nbiochemical species or a family of thereof) with a local attribute, a state\ndenoting its level (e.g., concentration, strength). Entities levels may change\nas a result of an activity or may decay gradually as time passes by. The\nsemantics of ANDy is formally given via high-level Petri nets ensuring this way\nsome modularity. As main results we show that ANDy systems have finite state\nrepresentations even for potentially infinite processes and it well adapts to\nthe modelling of toxic behaviours. As an illustration, we present a\nclassification of toxicity properties and give some hints on how they can be\nverified with existing tools on ANDy systems. A small case study on blood\nglucose regulation is provided to exemplify the ANDy framework and the toxicity\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 12:41:43 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Delaplace", "Franck", "", "IBISC"], ["Di Giusto", "Cinzia", "", "Repmus"], ["Giavitto", "Jean-Louis", "", "Repmus"], ["Klaudel", "Hanna", "", "IBISC"]]}, {"id": "1608.07441", "submitter": "Maxime Bucher", "authors": "Maxime Bucher (Palaiseau), St\\'ephane Herbin (Palaiseau), Fr\\'ed\\'eric\n  Jurie", "title": "Hard Negative Mining for Metric Learning Based Zero-Shot Classification", "comments": null, "journal-ref": "ECCV 16 WS TASK-CV: Transferring and Adapting Source Knowledge in\n  Computer Vision, Oct 2016, Amsterdam, Netherlands. ECCV 16 WS TASK-CV:\n  Transferring and Adapting Source Knowledge in Computer Vision", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot learning has been shown to be an efficient strategy for domain\nadaptation. In this context, this paper builds on the recent work of Bucher et\nal. [1], which proposed an approach to solve Zero-Shot classification problems\n(ZSC) by introducing a novel metric learning based objective function. This\nobjective function allows to learn an optimal embedding of the attributes\njointly with a measure of similarity between images and attributes. This paper\nextends their approach by proposing several schemes to control the generation\nof the negative pairs, resulting in a significant improvement of the\nperformance and giving above state-of-the-art results on three challenging ZSC\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 12:42:43 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Bucher", "Maxime", "", "Palaiseau"], ["Herbin", "St\u00e9phane", "", "Palaiseau"], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1608.07639", "submitter": "Yuval Atzmon", "authors": "Yuval Atzmon, Jonathan Berant, Vahid Kezami, Amir Globerson and Gal\n  Chechik", "title": "Learning to generalize to new compositions in image understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks have recently been used for learning to describe\nimages using natural language. However, it has been observed that these models\ngeneralize poorly to scenes that were not observed during training, possibly\ndepending too strongly on the statistics of the text in the training data. Here\nwe propose to describe images using short structured representations, aiming to\ncapture the crux of a description. These structured representations allow us to\ntease-out and evaluate separately two types of generalization: standard\ngeneralization to new images with similar scenes, and generalization to new\ncombinations of known entities. We compare two learning approaches on the\nMS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,\nAttend and Tell), and a simple structured prediction model on top of a deep\nnetwork. We find that the structured model generalizes to new compositions\nsubstantially better than the LSTM, ~7 times the accuracy of predicting\nstructured representations. By providing a concrete method to quantify\ngeneralization for unseen combinations, we argue that structured\nrepresentations and compositional splits are a useful benchmark for image\ncaptioning, and advocate compositional models that capture linguistic and\nvisual structure.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 00:34:00 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Atzmon", "Yuval", ""], ["Berant", "Jonathan", ""], ["Kezami", "Vahid", ""], ["Globerson", "Amir", ""], ["Chechik", "Gal", ""]]}, {"id": "1608.07685", "submitter": "Han Xiao", "authors": "Han Xiao, Minlie Huang, Xiaoyan Zhu", "title": "KSR: A Semantic Representation of Knowledge Graph within a Novel\n  Unsupervised Paradigm", "comments": "submitting to IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge representation is a long-history topic in AI, which is very\nimportant. A variety of models have been proposed for knowledge graph\nembedding, which projects symbolic entities and relations into continuous\nvector space. However, most related methods merely focus on the data-fitting of\nknowledge graph, and ignore the interpretable semantic expression. Thus,\ntraditional embedding methods are not friendly for applications that require\nsemantic analysis, such as question answering and entity retrieval. To this\nend, this paper proposes a semantic representation method for knowledge graph\n\\textbf{(KSR)}, which imposes a two-level hierarchical generative process that\nglobally extracts many aspects and then locally assigns a specific category in\neach aspect for every triple. Since both aspects and categories are\nsemantics-relevant, the collection of categories in each aspect is treated as\nthe semantic representation of this triple. Extensive experiments show that our\nmodel outperforms other state-of-the-art baselines substantially.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 09:53:38 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 02:48:01 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 02:06:16 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 09:59:23 GMT"}, {"version": "v5", "created": "Fri, 1 Dec 2017 01:54:44 GMT"}, {"version": "v6", "created": "Sat, 16 Dec 2017 11:20:48 GMT"}, {"version": "v7", "created": "Fri, 11 May 2018 04:16:05 GMT"}, {"version": "v8", "created": "Wed, 1 Apr 2020 03:14:54 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Xiao", "Han", ""], ["Huang", "Minlie", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1608.07734", "submitter": "Tameem Adel", "authors": "Tameem Adel, Cassio P. de Campos", "title": "Learning Bayesian Networks with Incomplete Data by Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new algorithms for learning Bayesian networks from data with\nmissing values using a data augmentation approach. An exact Bayesian network\nlearning algorithm is obtained by recasting the problem into a standard\nBayesian network learning problem without missing data. To the best of our\nknowledge, this is the first exact algorithm for this problem. As expected, the\nexact algorithm does not scale to large domains. We build on the exact method\nto create an approximate algorithm using a hill-climbing technique. This\nalgorithm scales to large domains so long as a suitable standard structure\nlearning method for complete data is available. We perform a wide range of\nexperiments to demonstrate the benefits of learning Bayesian networks with such\nnew approach.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 18:41:47 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2016 01:50:25 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Adel", "Tameem", ""], ["de Campos", "Cassio P.", ""]]}, {"id": "1608.07764", "submitter": "Russell K. Standish", "authors": "Russell K. Standish", "title": "The Movie Graph Argument Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we reexamine the Movie Graph Argument, which demonstrates a\nbasic incompatibility between computationalism and materialism. We discover\nthat the incompatibility is only manifest in singular classical-like universes.\nIf we accept that we live in a Multiverse, then the incompatibility goes away,\nbut in that case another line of argument shows that with computationalism, the\nfundamental, or primitive materiality has no causal influence on what is\nobserved, which must must be derivable from basic arithmetic properties.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 04:18:39 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Standish", "Russell K.", ""]]}, {"id": "1608.07793", "submitter": "Zhongqi Lu", "authors": "Zhongqi Lu, Qiang Yang", "title": "Partially Observable Markov Decision Process for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the \"Recurrent Deterioration\" (RD) phenomenon observed in online\nrecommender systems. The RD phenomenon is reflected by the trend of performance\ndegradation when the recommendation model is always trained based on users'\nfeedbacks of the previous recommendations. There are several reasons for the\nrecommender systems to encounter the RD phenomenon, including the lack of\nnegative training data and the evolution of users' interests, etc. Motivated to\ntackle the problems causing the RD phenomenon, we propose the POMDP-Rec\nframework, which is a neural-optimized Partially Observable Markov Decision\nProcess algorithm for recommender systems. We show that the POMDP-Rec framework\neffectively uses the accumulated historical data from real-world recommender\nsystems and automatically achieves comparable results with those models\nfine-tuned exhaustively by domain exports on public datasets.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 09:42:52 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 15:41:02 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Lu", "Zhongqi", ""], ["Yang", "Qiang", ""]]}, {"id": "1608.07846", "submitter": "Henry Kim", "authors": "Henry M. Kim, Jackie Ho Nam Cheung, Marek Laskowski, Iryna Gel", "title": "Data Analytics using Ontologies of Management Theories: Towards\n  Implementing 'From Theory to Practice'", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore how computational ontologies can be impactful vis-a-vis the\ndeveloping discipline of \"data science.\" We posit an approach wherein\nmanagement theories are represented as formal axioms, and then applied to draw\ninferences about data that reside in corporate databases. That is, management\ntheories would be implemented as rules within a data analytics engine. We\ndemonstrate a case study development of such an ontology by formally\nrepresenting an accounting theory in First-Order Logic. Though quite\npreliminary, the idea that an information technology, namely ontologies, can\npotentially actualize the academic cliche, \"From Theory to Practice,\" and be\napplicable to the burgeoning domain of data analytics is novel and exciting.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 19:51:31 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Kim", "Henry M.", ""], ["Cheung", "Jackie Ho Nam", ""], ["Laskowski", "Marek", ""], ["Gel", "Iryna", ""]]}, {"id": "1608.07879", "submitter": "EPTCS", "authors": "Hana Chockler (King's College London)", "title": "Causality and Responsibility for Formal Verification and Beyond", "comments": "In Proceedings CREST 2016, arXiv:1608.07398. Invited paper", "journal-ref": "EPTCS 224, 2016, pp. 1-8", "doi": "10.4204/EPTCS.224.1", "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of actual causality, defined by Halpern and Pearl, and its\nquantitative measure - the degree of responsibility - was shown to be extremely\nuseful in various areas of computer science due to a good match between the\nresults it produces and our intuition. In this paper, I describe the\napplications of causality to formal verification, namely, explanation of\ncounterexamples, refinement of coverage metrics, and symbolic trajectory\nevaluation. I also briefly discuss recent applications of causality to legal\nreasoning.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 01:35:46 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Chockler", "Hana", "", "King's College London"]]}, {"id": "1608.07905", "submitter": "Shuohang Wang", "authors": "Shuohang Wang and Jing Jiang", "title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "comments": "11 pages; 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine comprehension of text is an important problem in natural language\nprocessing. A recently released dataset, the Stanford Question Answering\nDataset (SQuAD), offers a large number of real questions and their answers\ncreated by humans through crowdsourcing. SQuAD provides a challenging testbed\nfor evaluating machine comprehension algorithms, partly because compared with\nprevious datasets, in SQuAD the answers do not come from a small set of\ncandidate answers and they have variable lengths. We propose an end-to-end\nneural architecture for the task. The architecture is based on match-LSTM, a\nmodel we proposed previously for textual entailment, and Pointer Net, a\nsequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the\noutput tokens to be from the input sequences. We propose two ways of using\nPointer Net for our task. Our experiments show that both of our two models\nsubstantially outperform the best results obtained by Rajpurkar et al.(2016)\nusing logistic regression and manually crafted features.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 03:42:50 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 03:39:40 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Wang", "Shuohang", ""], ["Jiang", "Jing", ""]]}, {"id": "1608.08015", "submitter": "Charles Prud'homme", "authors": "Charles Prud'homme, Xavier Lorca and Narendra Jussien", "title": "Event Selection Rules to Compute Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": "15/1/INFO", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanations have been introduced in the previous century. Their interest in\nreducing the search space is no longer questioned. Yet, their efficient\nimplementation into CSP solver is still a challenge. In this paper, we\nintroduce ESeR, an Event Selection Rules algorithm that filters events\ngenerated during propagation. This dynamic selection enables an efficient\ncomputation of explanations for intelligent backtracking al- gorithms. We show\nthe effectiveness of our approach on the instances of the last three MiniZinc\nchallenges\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 12:07:04 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Prud'homme", "Charles", ""], ["Lorca", "Xavier", ""], ["Jussien", "Narendra", ""]]}, {"id": "1608.08028", "submitter": "Joris Mooij", "authors": "Paul K. Rubenstein, Stephan Bongers, Bernhard Schoelkopf, Joris M.\n  Mooij", "title": "From Deterministic ODEs to Dynamic Structural Causal Models", "comments": "Accepted for publication in Conference on Uncertainy in Artificial\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural Causal Models are widely used in causal modelling, but how they\nrelate to other modelling tools is poorly understood. In this paper we provide\na novel perspective on the relationship between Ordinary Differential Equations\nand Structural Causal Models. We show how, under certain conditions, the\nasymptotic behaviour of an Ordinary Differential Equation under non-constant\ninterventions can be modelled using Dynamic Structural Causal Models. In\ncontrast to earlier work, we study not only the effect of interventions on\nequilibrium states; rather, we model asymptotic behaviour that is dynamic under\ninterventions that vary in time, and include as a special case the study of\nstatic equilibria.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 12:43:42 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 10:05:49 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Rubenstein", "Paul K.", ""], ["Bongers", "Stephan", ""], ["Schoelkopf", "Bernhard", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1608.08033", "submitter": "Van Hung Le", "authors": "Van Hung Le", "title": "Fuzzy Logic in Narrow Sense with Hedges", "comments": "10 pages, International Journal of Computer Science & Information\n  Technology (IJCSIT) Vol 8, No 3, June 2016", "journal-ref": null, "doi": "10.5121/ijcsit.2016.8310", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical logic has a serious limitation in that it cannot cope with the\nissues of vagueness and uncertainty into which fall most modes of human\nreasoning. In order to provide a foundation for human knowledge representation\nand reasoning in the presence of vagueness, imprecision, and uncertainty, fuzzy\nlogic should have the ability to deal with linguistic hedges, which play a very\nimportant role in the modification of fuzzy predicates. In this paper, we\nextend fuzzy logic in narrow sense with graded syntax, introduced by Novak et\nal., with many hedge connectives. In one case, each hedge does not have any\ndual one. In the other case, each hedge can have its own dual one. The\nresulting logics are shown to also have the Pavelka-style completeness\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 12:55:15 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Le", "Van Hung", ""]]}, {"id": "1608.08072", "submitter": "Leslie Sikos Ph.D.", "authors": "Leslie F. Sikos", "title": "A Novel Approach to Multimedia Ontology Engineering for Automated\n  Reasoning over Audiovisual LOD Datasets", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-49381-6_1", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia reasoning, which is suitable for, among others, multimedia content\nanalysis and high-level video scene interpretation, relies on the formal and\ncomprehensive conceptualization of the represented knowledge domain. However,\nmost multimedia ontologies are not exhaustive in terms of role definitions, and\ndo not incorporate complex role inclusions and role interdependencies. In fact,\nmost multimedia ontologies do not have a role box at all, and implement only a\nbasic subset of the available logical constructors. Consequently, their\napplication in multimedia reasoning is limited. To address the above issues,\nVidOnt, the very first multimedia ontology with SROIQ(D) expressivity and a\nDL-safe ruleset has been introduced for next-generation multimedia reasoning.\nIn contrast to the common practice, the formal grounding has been set in one of\nthe most expressive description logics, and the ontology validated with\nindustry-leading reasoners, namely HermiT and FaCT++. This paper also presents\nbest practices for developing multimedia ontologies, based on my ontology\nengineering approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 05:53:07 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Sikos", "Leslie F.", ""]]}, {"id": "1608.08144", "submitter": "Vladimir Lifschitz", "authors": "Vladimir Lifschitz", "title": "Achievements in Answer Set Programming", "comments": "Revised version of a paper published in Theory and Practice of Logic\n  Programming", "journal-ref": "Theory and Practice of Logic Programming, Vol. 17, 2017", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an approach to the methodology of answer set programming\n(ASP) that can facilitate the design of encodings that are easy to understand\nand provably correct. Under this approach, after appending a rule or a small\ngroup of rules to the emerging program we include a comment that states what\nhas been \"achieved\" so far. This strategy allows us to set out our\nunderstanding of the design of the program by describing the roles of small\nparts of the program in a mathematically precise way.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 16:59:43 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 01:06:05 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Lifschitz", "Vladimir", ""]]}, {"id": "1608.08176", "submitter": "Amritanshu Agrawal", "authors": "Amritanshu Agrawal, Wei Fu, Tim Menzies", "title": "What is Wrong with Topic Modeling? (and How to Fix it Using Search-based\n  Software Engineering)", "comments": "15 pages + 2 page references. Accepted to IST", "journal-ref": "Information and Software Technology Journal, 2018", "doi": "10.1016/j.infsof.2018.02.005", "report-no": null, "categories": "cs.SE cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Topic modeling finds human-readable structures in unstructured\ntextual data. A widely used topic modeler is Latent Dirichlet allocation. When\nrun on different datasets, LDA suffers from \"order effects\" i.e. different\ntopics are generated if the order of training data is shuffled. Such order\neffects introduce a systematic error for any study. This error can relate to\nmisleading results;specifically, inaccurate topic descriptions and a reduction\nin the efficacy of text mining classification results. Objective: To provide a\nmethod in which distributions generated by LDA are more stable and can be used\nfor further analysis. Method: We use LDADE, a search-based software engineering\ntool that tunes LDA's parameters using DE (Differential Evolution). LDADE is\nevaluated on data from a programmer information exchange site (Stackoverflow),\ntitle and abstract text of thousands ofSoftware Engineering (SE) papers, and\nsoftware defect reports from NASA. Results were collected across different\nimplementations of LDA (Python+Scikit-Learn, Scala+Spark); across different\nplatforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using\nGibbs sampling). Results were scored via topic stability and text mining\nclassification accuracy. Results: In all treatments: (i) standard LDA exhibits\nvery large topic instability; (ii) LDADE's tunings dramatically reduce cluster\ninstability; (iii) LDADE also leads to improved performances for supervised as\nwell as unsupervised learning. Conclusion: Due to topic instability, using\nstandard LDA with its \"off-the-shelf\" settings should now be depreciated. Also,\nin future, we should require SE papers that use LDA to test and (if needed)\nmitigate LDA topic instability. Finally, LDADE is a candidate technology for\neffectively and efficiently reducing that instability.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 18:45:00 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 01:19:06 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 04:49:42 GMT"}, {"version": "v4", "created": "Tue, 20 Feb 2018 17:26:51 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Agrawal", "Amritanshu", ""], ["Fu", "Wei", ""], ["Menzies", "Tim", ""]]}, {"id": "1608.08188", "submitter": "Danna Gurari", "authors": "Danna Gurari and Kristen Grauman", "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) systems are emerging from a desire to empower\nusers to ask any natural language question about visual content and receive a\nvalid answer in response. However, close examination of the VQA problem reveals\nan unavoidable, entangled problem that multiple humans may or may not always\nagree on a single answer to a visual question. We train a model to\nautomatically predict from a visual question whether a crowd would agree on a\nsingle answer. We then propose how to exploit this system in a novel\napplication to efficiently allocate human effort to collect answers to visual\nquestions. Specifically, we propose a crowdsourcing system that automatically\nsolicits fewer human responses when answer agreement is expected and more human\nresponses when answer disagreement is expected. Our system improves upon\nexisting crowdsourcing systems, typically eliminating at least 20% of human\neffort with no loss to the information collected from the crowd.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 19:24:25 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Gurari", "Danna", ""], ["Grauman", "Kristen", ""]]}, {"id": "1608.08252", "submitter": "Fabrizio Maria Maggi", "authors": "Hoang Nguyen, Marlon Dumas, Marcello La Rosa, Fabrizio Maria Maggi,\n  Suriadi Suriadi", "title": "Business Process Deviance Mining: Review and Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business process deviance refers to the phenomenon whereby a subset of the\nexecutions of a business process deviate, in a negative or positive way, with\nrespect to its expected or desirable outcomes. Deviant executions of a business\nprocess include those that violate compliance rules, or executions that\nundershoot or exceed performance targets. Deviance mining is concerned with\nuncovering the reasons for deviant executions by analyzing business process\nevent logs. This article provides a systematic review and comparative\nevaluation of deviance mining approaches based on a family of data mining\ntechniques known as sequence classification. Using real-life logs from multiple\ndomains, we evaluate a range of feature types and classification methods in\nterms of their ability to accurately discriminate between normal and deviant\nexecutions of a process. We also analyze the interestingness of the rule sets\nextracted using different methods. We observe that feature sets extracted using\npattern mining techniques only slightly outperform simpler feature sets based\non counts of individual activity occurrences in a trace.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 21:14:01 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Nguyen", "Hoang", ""], ["Dumas", "Marlon", ""], ["La Rosa", "Marcello", ""], ["Maggi", "Fabrizio Maria", ""], ["Suriadi", "Suriadi", ""]]}, {"id": "1608.08262", "submitter": "Yuanlin Zhang", "authors": "Michael Gelfond and Yuanlin Zhang", "title": "Vicious Circle Principle and Formation of Sets in ASP Based Languages", "comments": "Paper presented at the 9th Workshop on Answer Set Programming and\n  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper continues the investigation of Poincare and Russel's Vicious Circle\nPrinciple (VCP) in the context of the design of logic programming languages\nwith sets. We expand previously introduced language Alog with aggregates by\nallowing infinite sets and several additional set related constructs useful for\nknowledge representation and teaching. In addition, we propose an alternative\nformalization of the original VCP and incorporate it into the semantics of new\nlanguage, Slog+, which allows more liberal construction of sets and their use\nin programming rules. We show that, for programs without disjunction and\ninfinite sets, the formal semantics of aggregates in Slog+ coincides with that\nof several other known languages. Their intuitive and formal semantics,\nhowever, are based on quite different ideas and seem to be more involved than\nthat of Slog+.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 21:58:07 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Gelfond", "Michael", ""], ["Zhang", "Yuanlin", ""]]}, {"id": "1608.08292", "submitter": "Shantanu Chakraborty D.Eng.", "authors": "Shantanu Chakraborty and Toshiya Okabe", "title": "Robust Energy Storage Scheduling for Imbalance Reduction of\n  Strategically Formed Energy Balancing Groups", "comments": null, "journal-ref": "Energy, Volume 114, 1 November 2016, Pages 405-417, ISSN 0360-5442", "doi": "10.1016/j.energy.2016.07.170.", "report-no": null, "categories": "cs.SY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imbalance (on-line energy gap between contracted supply and actual demand,\nand associated cost) reduction is going to be a crucial service for a Power\nProducer and Supplier (PPS) in the deregulated energy market. PPS requires\nforward market interactions to procure energy as precisely as possible in order\nto reduce imbalance energy. This paper presents, 1) (off-line) an effective\ndemand aggregation based strategy for creating a number of balancing groups\nthat leads to higher predictability of group-wise aggregated demand, 2)\n(on-line) a robust energy storage scheduling that minimizes the imbalance for a\nparticular balancing group considering the demand prediction uncertainty. The\ngroup formation is performed by a Probabilistic Programming approach using\nBayesian Markov Chain Monte Carlo (MCMC) method after applied on the historical\ndemand statistics. Apart from the group formation, the aggregation strategy\n(with the help of Bayesian Inference) also clears out the upper-limit of the\nrequired storage capacity for a formed group, fraction of which is to be\nutilized in on-line operation. For on-line operation, a robust energy storage\nscheduling method is proposed that minimizes expected imbalance energy and cost\n(a non-linear function of imbalance energy) while incorporating the demand\nuncertainty of a particular group. The proposed methods are applied on the real\napartment buildings' demand data in Tokyo, Japan. Simulation results are\npresented to verify the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 00:59:07 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Chakraborty", "Shantanu", ""], ["Okabe", "Toshiya", ""]]}, {"id": "1608.08435", "submitter": "Rajasekar Venkatesan", "authors": "Rajasekar Venkatesan, Meng Joo Er", "title": "Multi-Label Classification Method Based on Extreme Learning Machines", "comments": "6 pages, 7 figures, 7 tables, ICARCV", "journal-ref": null, "doi": "10.1109/ICARCV.2014.7064375", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an Extreme Learning Machine (ELM) based technique for\nMulti-label classification problems is proposed and discussed. In multi-label\nclassification, each of the input data samples belongs to one or more than one\nclass labels. The traditional binary and multi-class classification problems\nare the subset of the multi-label problem with the number of labels\ncorresponding to each sample limited to one. The proposed ELM based multi-label\nclassification technique is evaluated with six different benchmark multi-label\ndatasets from different domains such as multimedia, text and biology. A\ndetailed comparison of the results is made by comparing the proposed method\nwith the results from nine state of the arts techniques for five different\nevaluation metrics. The nine methods are chosen from different categories of\nmulti-label methods. The comparative results shows that the proposed Extreme\nLearning Machine based multi-label classification technique is a better\nalternative than the existing state of the art methods for multi-label\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 13:08:06 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Venkatesan", "Rajasekar", ""], ["Er", "Meng Joo", ""]]}, {"id": "1608.08447", "submitter": "Bart Bogaerts", "authors": "Jo Devriendt and Bart Bogaerts", "title": "BreakID: Static Symmetry Breaking for ASP (System Description)", "comments": "Paper presented at the 9th Workshop on Answer Set Programming and\n  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry breaking has been proven to be an efficient preprocessing technique\nfor satisfiability solving (SAT). In this paper, we port the state-of-the-art\nSAT symmetry breaker BreakID to answer set programming (ASP). The result is a\nlightweight tool that can be plugged in between the grounding and the solving\nphases that are common when modelling in ASP. We compare our tool with sbass,\nthe current state-of-the-art symmetry breaker for ASP.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 13:47:41 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Devriendt", "Jo", ""], ["Bogaerts", "Bart", ""]]}, {"id": "1608.08472", "submitter": "Marcel Wild", "authors": "Marcel Wild", "title": "ALLSAT compressed with wildcards: From CNF's to orthogonal DNF's by\n  imposing the clauses one by one", "comments": "To appear in The Computer Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel technique for converting a Boolean CNF into an orthogonal\nDNF, aka exclusive sum of products. Our method (which will be pitted against a\nhardwired command from Mathematica) zooms in on the models of the CNF by\nimposing its clauses one by one. Clausal Imposition invites parallelization,\nand wildcards beyond the common don't-care symbol compress the output. The\nmethod is most efficient for few but large clauses. Generalizing clauses one\ncan in fact impose superclauses. By definition, superclauses are obtained from\nclauses by substituting each positive litereal by an arbitrary conjunction of\npositive literals.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 14:32:41 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 15:57:49 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 14:48:43 GMT"}, {"version": "v4", "created": "Sat, 3 Oct 2020 09:46:04 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wild", "Marcel", ""]]}, {"id": "1608.08497", "submitter": "Uwe Aickelin", "authors": "Simon Miller, Christian Wagner, Uwe Aickelin, Jonathan M. Garibaldi", "title": "Modelling Cyber-Security Experts' Decision Making Processes using\n  Aggregation Operators", "comments": "Computers and Security, Volume 62, September 2016, Pages 229-245", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important role carried out by cyber-security experts is the assessment of\nproposed computer systems, during their design stage. This task is fraught with\ndifficulties and uncertainty, making the knowledge provided by human experts\nessential for successful assessment. Today, the increasing number of\nprogressively complex systems has led to an urgent need to produce tools that\nsupport the expert-led process of system-security assessment. In this research,\nwe use weighted averages (WAs) and ordered weighted averages (OWAs) with\nevolutionary algorithms (EAs) to create aggregation operators that model parts\nof the assessment process. We show how individual overall ratings for security\ncomponents can be produced from ratings of their characteristics, and how these\nindividual overall ratings can be aggregated to produce overall rankings of\npotential attacks on a system. As well as the identification of salient attacks\nand weak points in a prospective system, the proposed method also highlights\nwhich factors and security components contribute most to a component's\ndifficulty and attack ranking respectively. A real world scenario is used in\nwhich experts were asked to rank a set of technical attacks, and to answer a\nseries of questions about the security components that are the subject of the\nattacks. The work shows how finding good aggregation operators, and identifying\nimportant components and factors of a cyber-security problem can be automated.\nThe resulting operators have the potential for use as decision aids for systems\ndesigners and cyber-security experts, increasing the amount of assessment that\ncan be achieved with the limited resources available.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 15:21:42 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Miller", "Simon", ""], ["Wagner", "Christian", ""], ["Aickelin", "Uwe", ""], ["Garibaldi", "Jonathan M.", ""]]}, {"id": "1608.08515", "submitter": "Ivana Balazevic", "authors": "Ivana Balazevic, Mikio Braun, Klaus-Robert M\\\"uller", "title": "Language Detection For Short Text Messages In Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the constant growth of the World Wide Web and the number of documents in\ndifferent languages accordingly, the need for reliable language detection tools\nhas increased as well. Platforms such as Twitter with predominantly short texts\nare becoming important information resources, which additionally imposes the\nneed for short texts language detection algorithms. In this paper, we show how\nincorporating personalized user-specific information into the language\ndetection algorithm leads to an important improvement of detection results. To\nchoose the best algorithm for language detection for short text messages, we\ninvestigate several machine learning approaches. These approaches include the\nuse of the well-known classifiers such as SVM and logistic regression, a\ndictionary based approach, and a probabilistic model based on modified\nKneser-Ney smoothing. Furthermore, the extension of the probabilistic model to\ninclude additional user-specific information such as evidence accumulation per\nuser and user interface language is explored, with the goal of improving the\nclassification performance. The proposed approaches are evaluated on randomly\ncollected Twitter data containing Latin as well as non-Latin alphabet languages\nand the quality of the obtained results is compared, followed by the selection\nof the best performing algorithm. This algorithm is then evaluated against two\nalready existing general language detection tools: Chromium Compact Language\nDetector 2 (CLD2) and langid, where our method significantly outperforms the\nresults achieved by both of the mentioned methods. Additionally, a preview of\nbenefits and possible applications of having a reliable language detection\nalgorithm is given.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 15:43:52 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Balazevic", "Ivana", ""], ["Braun", "Mikio", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1608.08517", "submitter": "Haifeng Zhang", "authors": "Haifeng Zhang and Yevgeniy Vorobeychik", "title": "Empirically Grounded Agent-Based Models of Innovation Diffusion: A\n  Critical Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Innovation diffusion has been studied extensively in a variety of\ndisciplines, including sociology, economics, marketing, ecology, and computer\nscience. Traditional literature on innovation diffusion has been dominated by\nmodels of aggregate behavior and trends. However, the agent-based modeling\n(ABM) paradigm is gaining popularity as it captures agent heterogeneity and\nenables fine-grained modeling of interactions mediated by social and geographic\nnetworks. While most ABM work on innovation diffusion is theoretical,\nempirically grounded models are increasingly important, particularly in guiding\npolicy decisions. We present a critical review of empirically grounded\nagent-based models of innovation diffusion, developing a categorization of this\nresearch based on types of agent models as well as applications. By connecting\nthe modeling methodologies in the fields of information and innovation\ndiffusion, we suggest that the maximum likelihood estimation framework widely\nused in the former is a promising paradigm for calibration of agent-based\nmodels for innovation diffusion. Although many advances have been made to\nstandardize ABM methodology, we identify four major issues in model calibration\nand validation, and suggest potential solutions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 15:45:13 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 11:55:52 GMT"}, {"version": "v3", "created": "Thu, 22 Sep 2016 15:34:27 GMT"}, {"version": "v4", "created": "Thu, 25 May 2017 13:53:45 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Zhang", "Haifeng", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1608.08589", "submitter": "Nan Li", "authors": "Nan Li, Dave Oyler, Mengxuan Zhang, Yildiray Yildiz, Ilya Kolmanovsky,\n  Anouck Girard", "title": "Game-Theoretic Modeling of Driver and Vehicle Interactions for\n  Verification and Validation of Autonomous Vehicle Control Systems", "comments": "13 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving has been the subject of increased interest in recent years\nboth in industry and in academia. Serious efforts are being pursued to address\nlegal, technical and logistical problems and make autonomous cars a viable\noption for everyday transportation. One significant challenge is the time and\neffort required for the verification and validation of the decision and control\nalgorithms employed in these vehicles to ensure a safe and comfortable driving\nexperience. Hundreds of thousands of miles of driving tests are required to\nachieve a well calibrated control system that is capable of operating an\nautonomous vehicle in an uncertain traffic environment where multiple\ninteractions between vehicles and drivers simultaneously occur. Traffic\nsimulators where these interactions can be modeled and represented with\nreasonable fidelity can help decrease the time and effort necessary for the\ndevelopment of the autonomous driving control algorithms by providing a venue\nwhere acceptable initial control calibrations can be achieved quickly and\nsafely before actual road tests. In this paper, we present a game theoretic\ntraffic model that can be used to 1) test and compare various autonomous\nvehicle decision and control systems and 2) calibrate the parameters of an\nexisting control system. We demonstrate two example case studies, where, in the\nfirst case, we test and quantitatively compare two autonomous vehicle control\nsystems in terms of their safety and performance, and, in the second case, we\noptimize the parameters of an autonomous vehicle control system, utilizing the\nproposed traffic model and simulation environment.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 18:25:35 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Li", "Nan", ""], ["Oyler", "Dave", ""], ["Zhang", "Mengxuan", ""], ["Yildiz", "Yildiray", ""], ["Kolmanovsky", "Ilya", ""], ["Girard", "Anouck", ""]]}, {"id": "1608.08614", "submitter": "Minyoung Huh", "authors": "Minyoung Huh, Pulkit Agrawal, Alexei A. Efros", "title": "What makes ImageNet good for transfer learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The tremendous success of ImageNet-trained deep features on a wide range of\ntransfer tasks begs the question: what are the properties of the ImageNet\ndataset that are critical for learning good, general-purpose features? This\nwork provides an empirical investigation of various facets of this question: Is\nmore pre-training data always better? How does feature quality depend on the\nnumber of training examples per class? Does adding more object classes improve\nperformance? For the same data budget, how should the data be split into\nclasses? Is fine-grained recognition necessary for learning good features?\nGiven the same number of training classes, is it better to have coarse classes\nor fine-grained classes? Which is better: more classes or more examples per\nclass? To answer these and related questions, we pre-trained CNN features on\nvarious subsets of the ImageNet dataset and evaluated transfer performance on\nPASCAL detection, PASCAL action classification, and SUN scene classification\ntasks. Our overall findings suggest that most changes in the choice of\npre-training data long thought to be critical do not significantly affect\ntransfer performance.? Given the same number of training classes, is it better\nto have coarse classes or fine-grained classes? Which is better: more classes\nor more examples per class?\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 19:45:09 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 13:37:06 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Huh", "Minyoung", ""], ["Agrawal", "Pulkit", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1608.08716", "submitter": "Aishwarya Agrawal", "authors": "C. Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret\n  Mitchell, Dhruv Batra, Devi Parikh", "title": "Measuring Machine Intelligence Through Visual Question Answering", "comments": "AI Magazine, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machines have become more intelligent, there has been a renewed interest\nin methods for measuring their intelligence. A common approach is to propose\ntasks for which a human excels, but one which machines find difficult. However,\nan ideal task should also be easy to evaluate and not be easily gameable. We\nbegin with a case study exploring the recently popular task of image captioning\nand its limitations as a task for measuring machine intelligence. An\nalternative and more promising task is Visual Question Answering that tests a\nmachine's ability to reason about language and vision. We describe a dataset\nunprecedented in size created for the task that contains over 760,000 human\ngenerated questions about images. Using around 10 million human generated\nanswers, machines may be easily evaluated.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 02:56:00 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Zitnick", "C. Lawrence", ""], ["Agrawal", "Aishwarya", ""], ["Antol", "Stanislaw", ""], ["Mitchell", "Margaret", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1608.08724", "submitter": "Christopher Lin", "authors": "Christopher H. Lin, Mausam, Daniel S. Weld", "title": "A Programming Language With a POMDP Inside", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present POAPS, a novel planning system for defining Partially Observable\nMarkov Decision Processes (POMDPs) that abstracts away from POMDP details for\nthe benefit of non-expert practitioners. POAPS includes an expressive adaptive\nprogramming language based on Lisp that has constructs for choice points that\ncan be dynamically optimized. Non-experts can use our language to write\nadaptive programs that have partially observable components without needing to\nspecify belief/hidden states or reason about probabilities. POAPS is also a\ncompiler that defines and performs the transformation of any program written in\nour language into a POMDP with control knowledge. We demonstrate the generality\nand power of POAPS in the rapidly growing domain of human computation by\ndescribing its expressiveness and simplicity by writing several POAPS programs\nfor common crowdsourcing tasks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 04:25:45 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Lin", "Christopher H.", ""], ["Mausam", "", ""], ["Weld", "Daniel S.", ""]]}, {"id": "1608.08749", "submitter": "Christophe Guyeux", "authors": "Bassam AlKindy, Bashar Al-Nuaimi, Christophe Guyeux, Jean-Fran\\c{c}ois\n  Couchot, Michel Salomon, Reem Alsrraj, Laurent Philippe", "title": "Binary Particle Swarm Optimization versus Hybrid Genetic Algorithm for\n  Inferring Well Supported Phylogenetic Trees", "comments": null, "journal-ref": "Lecture Notes in Bioinformatics LNBI series, 9874, 165--179, 2016", "doi": null, "report-no": null, "categories": "cs.AI q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of completely sequenced chloroplast genomes increases rapidly\nevery day, leading to the possibility to build large-scale phylogenetic trees\nof plant species. Considering a subset of close plant species defined according\nto their chloroplasts, the phylogenetic tree that can be inferred by their core\ngenes is not necessarily well supported, due to the possible occurrence of\nproblematic genes (i.e., homoplasy, incomplete lineage sorting, horizontal gene\ntransfers, etc.) which may blur the phylogenetic signal. However, a trustworthy\nphylogenetic tree can still be obtained provided such a number of blurring\ngenes is reduced. The problem is thus to determine the largest subset of core\ngenes that produces the best-supported tree. To discard problematic genes and\ndue to the overwhelming number of possible combinations, this article focuses\non how to extract the largest subset of sequences in order to obtain the most\nsupported species tree. Due to computational complexity, a distributed Binary\nParticle Swarm Optimization (BPSO) is proposed in sequential and distributed\nfashions. Obtained results from both versions of the BPSO are compared with\nthose computed using an hybrid approach embedding both genetic algorithms and\nstatistical tests. The proposal has been applied to different cases of plant\nfamilies, leading to encouraging results for these families.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 07:13:16 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["AlKindy", "Bassam", ""], ["Al-Nuaimi", "Bashar", ""], ["Guyeux", "Christophe", ""], ["Couchot", "Jean-Fran\u00e7ois", ""], ["Salomon", "Michel", ""], ["Alsrraj", "Reem", ""], ["Philippe", "Laurent", ""]]}, {"id": "1608.08898", "submitter": "Rajasekar Venkatesan", "authors": "Meng Joo Er, Rajasekar Venkatesan and Ning Wang", "title": "A High Speed Multi-label Classifier based on Extreme Learning Machines", "comments": "12 pages, 2 figures, 10 tables", "journal-ref": null, "doi": "10.1007/978-3-319-28373-9_37", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a high speed neural network classifier based on extreme\nlearning machines for multi-label classification problem is proposed and\ndis-cussed. Multi-label classification is a superset of traditional binary and\nmulti-class classification problems. The proposed work extends the extreme\nlearning machine technique to adapt to the multi-label problems. As opposed to\nthe single-label problem, both the number of labels the sample belongs to, and\neach of those target labels are to be identified for multi-label classification\nresulting in in-creased complexity. The proposed high speed multi-label\nclassifier is applied to six benchmark datasets comprising of different\napplication areas such as multi-media, text and biology. The training time and\ntesting time of the classifier are compared with those of the state-of-the-arts\nmethods. Experimental studies show that for all the six datasets, our proposed\ntechnique have faster execution speed and better performance, thereby\noutperforming all the existing multi-label clas-sification methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 14:56:12 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Er", "Meng Joo", ""], ["Venkatesan", "Rajasekar", ""], ["Wang", "Ning", ""]]}, {"id": "1608.08905", "submitter": "Rajasekar Venkatesan", "authors": "Rajasekar Venkatesan, Meng Joo Er, Shiqian Wu, Mahardhika Pratama", "title": "A Novel Online Real-time Classifier for Multi-label Data Streams", "comments": "8 pages, 7 tables, 3 figures. arXiv admin note: text overlap with\n  arXiv:1609.00086", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel extreme learning machine based online multi-label\nclassifier for real-time data streams is proposed. Multi-label classification\nis one of the actively researched machine learning paradigm that has gained\nmuch attention in the recent years due to its rapidly increasing real world\napplications. In contrast to traditional binary and multi-class classification,\nmulti-label classification involves association of each of the input samples\nwith a set of target labels simultaneously. There are no real-time online\nneural network based multi-label classifier available in the literature. In\nthis paper, we exploit the inherent nature of high speed exhibited by the\nextreme learning machines to develop a novel online real-time classifier for\nmulti-label data streams. The developed classifier is experimented with\ndatasets from different application domains for consistency, performance and\nspeed. The experimental studies show that the proposed method outperforms the\nexisting state-of-the-art techniques in terms of speed and accuracy and can\nclassify multi-label data streams in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 15:14:06 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Venkatesan", "Rajasekar", ""], ["Er", "Meng Joo", ""], ["Wu", "Shiqian", ""], ["Pratama", "Mahardhika", ""]]}, {"id": "1608.08927", "submitter": "Payam Siyari", "authors": "Payam Siyari and Matthias Gall\\'e", "title": "The Generalized Smallest Grammar Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Smallest Grammar Problem -- the problem of finding the smallest\ncontext-free grammar that generates exactly one given sequence -- has never\nbeen successfully applied to grammatical inference. We investigate the reasons\nand propose an extended formulation that seeks to minimize non-recursive\ngrammars, instead of straight-line programs. In addition, we provide very\nefficient algorithms that approximate the minimization problem of this class of\ngrammars. Our empirical evaluation shows that we are able to find smaller\nmodels than the current best approximations to the Smallest Grammar Problem on\nstandard benchmarks, and that the inferred rules capture much better the\nsyntactic structure of natural language.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 16:23:07 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Siyari", "Payam", ""], ["Gall\u00e9", "Matthias", ""]]}, {"id": "1608.08956", "submitter": "Matthias Van Der Hallen", "authors": "Matthias van der Hallen, Sergey Paramonov, Michael Leuschel, Gerda\n  Janssens", "title": "Knowledge Representation Analysis of Graph Mining", "comments": "Paper presented at the 9th Workshop on Answer Set Programming and\n  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems, especially those with a composite structure, can naturally be\nexpressed in higher order logic. From a KR perspective modeling these problems\nin an intuitive way is a challenging task. In this paper we study the graph\nmining problem as an example of a higher order problem. In short, this problem\nasks us to find a graph that frequently occurs as a subgraph among a set of\nexample graphs. We start from the problem's mathematical definition to solve it\nin three state-of-the-art specification systems. For IDP and ASP, which have no\nnative support for higher order logic, we propose the use of encoding\ntechniques such as the disjoint union technique and the saturation technique.\nProB benefits from the higher order support for sets. We compare the\nperformance of the three approaches to get an idea of the overhead of the\nhigher order support.\n  We propose higher-order language extensions for IDP-like specification\nlanguages and discuss what kind of solver support is needed. Native higher\norder shifts the burden of rewriting specifications using encoding techniques\nfrom the user to the solver itself.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:23:58 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["van der Hallen", "Matthias", ""], ["Paramonov", "Sergey", ""], ["Leuschel", "Michael", ""], ["Janssens", "Gerda", ""]]}, {"id": "1608.08974", "submitter": "Yash Goyal", "authors": "Yash Goyal, Akrit Mohapatra, Devi Parikh, Dhruv Batra", "title": "Towards Transparent AI Systems: Interpreting Visual Question Answering\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown striking progress and obtained\nstate-of-the-art results in many AI research fields in the recent years.\nHowever, it is often unsatisfying to not know why they predict what they do. In\nthis paper, we address the problem of interpreting Visual Question Answering\n(VQA) models. Specifically, we are interested in finding what part of the input\n(pixels in images or words in questions) the VQA model focuses on while\nanswering the question. To tackle this problem, we use two visualization\ntechniques -- guided backpropagation and occlusion -- to find important words\nin the question and important regions in the image. We then present qualitative\nand quantitative analyses of these importance maps. We found that even without\nexplicit attention mechanisms, VQA models may sometimes be implicitly attending\nto relevant regions in the image, and often to appropriate words in the\nquestion.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 18:11:29 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 19:51:06 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Goyal", "Yash", ""], ["Mohapatra", "Akrit", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}]