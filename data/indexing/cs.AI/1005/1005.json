[{"id": "1005.0080", "submitter": "Xiaoyu Chen", "authors": "Xiaoyu Chen", "title": "Electronic Geometry Textbook: A Geometric Textbook Knowledge Management\n  System", "comments": "To appear in The 9th International Conference on Mathematical\n  Knowledge Management: MKM 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Geometry Textbook is a knowledge management system that manages\ngeometric textbook knowledge to enable users to construct and share dynamic\ngeometry textbooks interactively and efficiently. Based on a knowledge base\norganizing and storing the knowledge represented in specific languages, the\nsystem implements interfaces for maintaining the data representing that\nknowledge as well as relations among those data, for automatically generating\nreadable documents for viewing or printing, and for automatically discovering\nthe relations among knowledge data. An interface has been developed for users\nto create geometry textbooks with automatic checking, in real time, of the\nconsistency of the structure of each resulting textbook. By integrating an\nexternal geometric theorem prover and an external dynamic geometry software\npackage, the system offers the facilities for automatically proving theorems\nand generating dynamic figures in the created textbooks. This paper provides a\ncomprehensive account of the current version of Electronic Geometry Textbook.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2010 15:06:32 GMT"}], "update_date": "2010-05-04", "authors_parsed": [["Chen", "Xiaoyu", ""]]}, {"id": "1005.0089", "submitter": "Lars Kotthoff", "authors": "Tom Kelsey and Lars Kotthoff", "title": "The Exact Closest String Problem as a Constraint Satisfaction Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report (to our knowledge) the first evaluation of Constraint Satisfaction\nas a computational framework for solving closest string problems. We show that\ncareful consideration of symbol occurrences can provide search heuristics that\nprovide several orders of magnitude speedup at and above the optimal distance.\nWe also report (to our knowledge) the first analysis and evaluation -- using\nany technique -- of the computational difficulties involved in the\nidentification of all closest strings for a given input set. We describe\nalgorithms for web-scale distributed solution of closest string problems, both\npurely based on AI backtrack search and also hybrid numeric-AI methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2010 16:00:59 GMT"}], "update_date": "2010-05-04", "authors_parsed": [["Kelsey", "Tom", ""], ["Kotthoff", "Lars", ""]]}, {"id": "1005.0104", "submitter": "Rahul Gupta", "authors": "Rahul Gupta, Sunita Sarawagi", "title": "Joint Structured Models for Extraction from Overlapping Sources", "comments": null, "journal-ref": null, "doi": "10.1145/1935826.1935868", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of jointly training structured models for extraction\nfrom sources whose instances enjoy partial overlap. This has important\napplications like user-driven ad-hoc information extraction on the web. Such\napplications present new challenges in terms of the number of sources and their\narbitrary pattern of overlap not seen by earlier collective training schemes\napplied on two sources. We present an agreement-based learning framework and\nalternatives within it to trade-off tractability, robustness to noise, and\nextent of agreement. We provide a principled scheme to discover low-noise\nagreement sets in unlabeled data across the sources. Through extensive\nexperiments over 58 real datasets, we establish that our method of additively\nrewarding agreement over maximal segments of text provides the best trade-offs,\nand also scores over alternatives such as collective inference, staged\ntraining, and multi-view learning.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2010 20:55:23 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Gupta", "Rahul", ""], ["Sarawagi", "Sunita", ""]]}, {"id": "1005.0125", "submitter": "Dotan Di Castro", "authors": "Dotan Di Castro and Shie Mannor", "title": "Adaptive Bases for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reinforcement learning using function\napproximation, where the approximating basis can change dynamically while\ninteracting with the environment. A motivation for such an approach is\nmaximizing the value function fitness to the problem faced. Three errors are\nconsidered: approximation square error, Bellman residual, and projected Bellman\nresidual. Algorithms under the actor-critic framework are presented, and shown\nto converge. The advantage of such an adaptive basis is demonstrated in\nsimulations.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2010 06:40:21 GMT"}], "update_date": "2010-05-04", "authors_parsed": [["Di Castro", "Dotan", ""], ["Mannor", "Shie", ""]]}, {"id": "1005.0530", "submitter": "Mohak Shah", "authors": "Mohak Shah, Mario Marchand and Jacques Corbeil", "title": "Feature Selection with Conjunctions of Decision Stumps and Learning from\n  Microarray Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the objectives of designing feature selection learning algorithms is\nto obtain classifiers that depend on a small number of attributes and have\nverifiable future performance guarantees. There are few, if any, approaches\nthat successfully address the two goals simultaneously. Performance guarantees\nbecome crucial for tasks such as microarray data analysis due to very small\nsample sizes resulting in limited empirical evaluation. To the best of our\nknowledge, such algorithms that give theoretical bounds on the future\nperformance have not been proposed so far in the context of the classification\nof gene expression data. In this work, we investigate the premise of learning a\nconjunction (or disjunction) of decision stumps in Occam's Razor, Sample\nCompression, and PAC-Bayes learning settings for identifying a small subset of\nattributes that can be used to perform reliable classification tasks. We apply\nthe proposed approaches for gene identification from DNA microarray data and\ncompare our results to those of well known successful approaches proposed for\nthe task. We show that our algorithm not only finds hypotheses with much\nsmaller number of genes while giving competitive classification accuracy but\nalso have tight risk guarantees on future performance unlike other approaches.\nThe proposed approaches are general and extensible in terms of both designing\nnovel algorithms and application to other domains.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2010 14:01:10 GMT"}], "update_date": "2010-05-05", "authors_parsed": [["Shah", "Mohak", ""], ["Marchand", "Mario", ""], ["Corbeil", "Jacques", ""]]}, {"id": "1005.0605", "submitter": "Vladimir Gavrikov L", "authors": "Vladimir L. Gavrikov, Rem G. Khlebopros", "title": "An approach to visualize the course of solving of a research task in\n  humans", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A technique to study the dynamics of solving of a research task is suggested.\nThe research task was based on specially developed software Right- Wrong\nResponder (RWR), with the participants having to reveal the response logic of\nthe program. The participants interacted with the program in the form of a\nsemi-binary dialogue, which implies the feedback responses of only two kinds -\n\"right\" or \"wrong\". The technique has been applied to a small pilot group of\nvolunteer participants. Some of them have successfully solved the task\n(solvers) and some have not (non-solvers). In the beginning of the work, the\nsolvers did more wrong moves than non-solvers, and they did less wrong moves\ncloser to the finish of the work. A phase portrait of the work both in solvers\nand non-solvers showed definite cycles that may correspond to sequences of\npartially true hypotheses that may be formulated by the participants during the\nsolving of the task.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 11:00:24 GMT"}], "update_date": "2010-05-05", "authors_parsed": [["Gavrikov", "Vladimir L.", ""], ["Khlebopros", "Rem G.", ""]]}, {"id": "1005.0608", "submitter": "Kurt Ammon", "authors": "Kurt Ammon", "title": "Informal Concepts in Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper constructively proves the existence of an effective procedure\ngenerating a computable (total) function that is not contained in any given\neffectively enumerable set of such functions. The proof implies the existence\nof machines that process informal concepts such as computable (total) functions\nbeyond the limits of any given Turing machine or formal system, that is, these\nmachines can, in a certain sense, \"compute\" function values beyond these\nlimits. We call these machines creative. We argue that any \"intelligent\"\nmachine should be capable of processing informal concepts such as computable\n(total) functions, that is, it should be creative. Finally, we introduce\nhypotheses on creative machines which were developed on the basis of\ntheoretical investigations and experiments with computer programs. The\nhypotheses say that machine intelligence is the execution of a self-developing\nprocedure starting from any universal programming language and any input.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2010 19:00:37 GMT"}], "update_date": "2010-05-05", "authors_parsed": [["Ammon", "Kurt", ""]]}, {"id": "1005.0707", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff", "title": "The Production of Probabilistic Entropy in Structure/Action Contingency\n  Relations", "comments": null, "journal-ref": "Journal of Social and Evolutionary Systems 18 (1995) 339-356", "doi": null, "report-no": null, "categories": "cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Luhmann (1984) defined society as a communication system which is\nstructurally coupled to, but not an aggregate of, human action systems. The\ncommunication system is then considered as self-organizing (\"autopoietic\"), as\nare human actors. Communication systems can be studied by using Shannon's\n(1948) mathematical theory of communication. The update of a network by action\nat one of the local nodes is then a well-known problem in artificial\nintelligence (Pearl 1988). By combining these various theories, a general\nalgorithm for probabilistic structure/action contingency can be derived. The\nconsequences of this contingency for each system, its consequences for their\nfurther histories, and the stabilization on each side by counterbalancing\nmechanisms are discussed, in both mathematical and theoretical terms. An\nempirical example is elaborated.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2010 09:19:59 GMT"}], "update_date": "2010-05-06", "authors_parsed": [["Leydesdorff", "Loet", ""]]}, {"id": "1005.0749", "submitter": "Jonathan Heras", "authors": "Jonathan Heras and Vico Pascual and Ana Romero and Julio Rubio", "title": "Integrating multiple sources to answer questions in Algebraic Topology", "comments": "To appear in The 9th International Conference on Mathematical\n  Knowledge Management: MKM 2010", "journal-ref": "Lectures Notes in Artificial Intelligence, 6167: 331-335, 2010", "doi": null, "report-no": null, "categories": "cs.SC cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper an evolution of a tool from a user interface for a\nconcrete Computer Algebra system for Algebraic Topology (the Kenzo system), to\na front-end allowing the interoperability among different sources for\ncomputation and deduction. The architecture allows the system not only to\ninterface several systems, but also to make them cooperate in shared\ncalculations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 07:46:10 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Heras", "Jonathan", ""], ["Pascual", "Vico", ""], ["Romero", "Ana", ""], ["Rubio", "Julio", ""]]}, {"id": "1005.0896", "submitter": "Jean Dezert", "authors": "Jean-Marc Tacnet (UR ETGR), Mireille Batton-Hubert (ENSM-SE), Jean\n  Dezert (ONERA)", "title": "A two-step fusion process for multi-criteria decision applied to natural\n  hazards in mountains", "comments": null, "journal-ref": "Workshop on the Theory of Belief Functions, April 1- 2, 2010\n  Brest, France, Brest : France (2010)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mountain river torrents and snow avalanches generate human and material\ndamages with dramatic consequences. Knowledge about natural phenomenona is\noften lacking and expertise is required for decision and risk management\npurposes using multi-disciplinary quantitative or qualitative approaches.\nExpertise is considered as a decision process based on imperfect information\ncoming from more or less reliable and conflicting sources. A methodology mixing\nthe Analytic Hierarchy Process (AHP), a multi-criteria aid-decision method, and\ninformation fusion using Belief Function Theory is described. Fuzzy Sets and\nPossibilities theories allow to transform quantitative and qualitative criteria\ninto a common frame of discernment for decision in Dempster-Shafer Theory (DST\n) and Dezert-Smarandache Theory (DSmT) contexts. Main issues consist in basic\nbelief assignments elicitation, conflict identification and management, fusion\nrule choices, results validation but also in specific needs to make a\ndifference between importance and reliability and uncertainty in the fusion\nprocess.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2010 06:32:59 GMT"}], "update_date": "2011-01-20", "authors_parsed": [["Tacnet", "Jean-Marc", "", "UR ETGR"], ["Batton-Hubert", "Mireille", "", "ENSM-SE"], ["Dezert", "Jean", "", "ONERA"]]}, {"id": "1005.0917", "submitter": "Christoph Schwarzweller", "authors": "Agnieszka Rowinska-Schwarzweller and Christoph Schwarzweller", "title": "On Building a Knowledge Base for Stability Theory", "comments": "To appear in The 9th International Conference on Mathematical\n  Knowledge Management: MKM 2010", "journal-ref": "Lecture Notes in Computer Science, 2010, Volume 6167, Intelligent\n  Computer Mathematics, Pages 427-439", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of mathematical knowledge has been formalized and stored in\nrepositories by now: different mathematical theorems and theories have been\ntaken into consideration and included in mathematical repositories.\nApplications more distant from pure mathematics, however --- though based on\nthese theories --- often need more detailed knowledge about the underlying\ntheories. In this paper we present an example Mizar formalization from the area\nof electrical engineering focusing on stability theory which is based on\ncomplex analysis. We discuss what kind of special knowledge is necessary here\nand which amount of this knowledge is included in existing repositories.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2010 07:59:52 GMT"}], "update_date": "2010-09-22", "authors_parsed": [["Rowinska-Schwarzweller", "Agnieszka", ""], ["Schwarzweller", "Christoph", ""]]}, {"id": "1005.0957", "submitter": "Rdv Ijcsis", "authors": "S. Karpagachelvi, M.Arthanari, M. Sivakumar", "title": "ECG Feature Extraction Techniques - A Survey Approach", "comments": "IEEE Publication format, International Journal of Computer Science\n  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  ECG Feature Extraction plays a significant role in diagnosing most of the\ncardiac diseases. One cardiac cycle in an ECG signal consists of the P-QRS-T\nwaves. This feature extraction scheme determines the amplitudes and intervals\nin the ECG signal for subsequent analysis. The amplitudes and intervals value\nof P-QRS-T segment determines the functioning of heart of every human.\nRecently, numerous research and techniques have been developed for analyzing\nthe ECG signal. The proposed schemes were mostly based on Fuzzy Logic Methods,\nArtificial Neural Networks (ANN), Genetic Algorithm (GA), Support Vector\nMachines (SVM), and other Signal Analysis techniques. All these techniques and\nalgorithms have their advantages and limitations. This proposed paper discusses\nvarious techniques and transformations proposed earlier in literature for\nextracting feature from an ECG signal. In addition this paper also provides a\ncomparative study of various methods proposed by researchers in extracting the\nfeature from ECG signal.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2010 09:54:44 GMT"}], "update_date": "2010-05-07", "authors_parsed": [["Karpagachelvi", "S.", ""], ["Arthanari", "M.", ""], ["Sivakumar", "M.", ""]]}, {"id": "1005.1475", "submitter": "Luca Saiu", "authors": "Jean-Vincent Loddo and Luca Saiu", "title": "How to correctly prune tropical trees", "comments": "To appear in \"Artificial Intelligence and Symbolic Computation,\n  2010\".", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM cs.GT cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present tropical games, a generalization of combinatorial min-max games\nbased on tropical algebras. Our model breaks the traditional symmetry of\nrational zero-sum games where players have exactly opposed goals (min vs. max),\nis more widely applicable than min-max and also supports a form of pruning,\ndespite it being less effective than alpha-beta. Actually, min-max games may be\nseen as particular cases where both the game and its dual are tropical: when\nthe dual of a tropical game is also tropical, the power of alpha-beta is\ncompletely recovered. We formally develop the model and prove that the tropical\npruning strategy is correct, then conclude by showing how the problem of\napproximated parsing can be modeled as a tropical game, profiting from pruning.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2010 09:06:26 GMT"}, {"version": "v2", "created": "Tue, 11 May 2010 06:35:47 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Loddo", "Jean-Vincent", ""], ["Saiu", "Luca", ""]]}, {"id": "1005.1518", "submitter": "Liane Gabora", "authors": "Liane Gabora", "title": "Recognizability of Individual Creative Style Within and Across Domains:\n  Preliminary Studies", "comments": "6 pages, submitted to Annual Meeting of the Cognitive Science\n  Society. August 11-14, 2010, Portland, Oregon", "journal-ref": "In S. Ohlsson & R. Catrambone (Eds.), Proceedings of the 32nd\n  Annual Meeting of the Cognitive Science Society (pp. 2350-2355). Austin, TX:\n  Cognitive Science Society. (2010)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is hypothesized that creativity arises from the self-mending capacity of\nan internal model of the world, or worldview. The uniquely honed worldview of a\ncreative individual results in a distinctive style that is recognizable within\nand across domains. It is further hypothesized that creativity is domaingeneral\nin the sense that there exist multiple avenues by which the distinctiveness of\none's worldview can be expressed. These hypotheses were tested using art\nstudents and creative writing students. Art students guessed significantly\nabove chance both which painting was done by which of five famous artists, and\nwhich artwork was done by which of their peers. Similarly, creative writing\nstudents guessed significantly above chance both which passage was written by\nwhich of five famous writers, and which passage was written by which of their\npeers. These findings support the hypothesis that creative style is\nrecognizable. Moreover, creative writing students guessed significantly above\nchance which of their peers produced particular works of art, supporting the\nhypothesis that creative style is recognizable not just within but across\ndomains.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2010 12:30:36 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 00:41:40 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 20:58:02 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 19:56:00 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Gabora", "Liane", ""]]}, {"id": "1005.1567", "submitter": "Francesco Scarcello", "authors": "Gianluigi Greco and Francesco Scarcello", "title": "On The Power of Tree Projections: Structural Tractability of Enumerating\n  CSP Solutions", "comments": null, "journal-ref": "Constraints 18(1): 38-74 (2013)", "doi": "10.1007/s10601-012-9129-8", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of deciding whether CSP instances admit solutions has been deeply\nstudied in the literature, and several structural tractability results have\nbeen derived so far. However, constraint satisfaction comes in practice as a\ncomputation problem where the focus is either on finding one solution, or on\nenumerating all solutions, possibly projected to some given set of output\nvariables. The paper investigates the structural tractability of the problem of\nenumerating (possibly projected) solutions, where tractability means here\ncomputable with polynomial delay (WPD), since in general exponentially many\nsolutions may be computed. A general framework based on the notion of tree\nprojection of hypergraphs is considered, which generalizes all known\ndecomposition methods. Tractability results have been obtained both for classes\nof structures where output variables are part of their specification, and for\nclasses of structures where computability WPD must be ensured for any possible\nset of output variables. These results are shown to be tight, by exhibiting\ndichotomies for classes of structures having bounded arity and where the tree\ndecomposition method is considered.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2010 14:46:29 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2010 14:05:01 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Greco", "Gianluigi", ""], ["Scarcello", "Francesco", ""]]}, {"id": "1005.1684", "submitter": "John Scoville", "authors": "John Scoville", "title": "On Macroscopic Complexity and Perceptual Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.MM cs.SD math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical limits of 'lossy' data compression algorithms are considered.\nThe complexity of an object as seen by a macroscopic observer is the size of\nthe perceptual code which discards all information that can be lost without\naltering the perception of the specified observer. The complexity of this\nmacroscopically observed state is the simplest description of any microstate\ncomprising that macrostate. Inference and pattern recognition based on\nmacrostate rather than microstate complexities will take advantage of the\ncomplexity of the macroscopic observer to ignore irrelevant noise.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2010 22:41:10 GMT"}, {"version": "v10", "created": "Tue, 14 Jun 2011 18:27:34 GMT"}, {"version": "v11", "created": "Mon, 4 Jul 2011 23:01:29 GMT"}, {"version": "v12", "created": "Thu, 7 Jul 2011 02:14:22 GMT"}, {"version": "v2", "created": "Mon, 24 May 2010 02:41:23 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2010 06:29:43 GMT"}, {"version": "v4", "created": "Wed, 22 Sep 2010 03:51:59 GMT"}, {"version": "v5", "created": "Thu, 23 Sep 2010 01:10:18 GMT"}, {"version": "v6", "created": "Thu, 7 Oct 2010 23:23:37 GMT"}, {"version": "v7", "created": "Wed, 20 Oct 2010 18:40:28 GMT"}, {"version": "v8", "created": "Tue, 26 Apr 2011 22:50:28 GMT"}, {"version": "v9", "created": "Tue, 10 May 2011 11:46:04 GMT"}], "update_date": "2011-07-08", "authors_parsed": [["Scoville", "John", ""]]}, {"id": "1005.1716", "submitter": "Christian Drescher", "authors": "Christian Drescher and Martin Gebser and Benjamin Kaufmann and Torsten\n  Schaub", "title": "Heuristics in Conflict Resolution", "comments": null, "journal-ref": "Proceedings of the Twelfth International Workshop on Nonmonotonic\n  Reasoning (2008) 141-149", "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern solvers for Boolean Satisfiability (SAT) and Answer Set Programming\n(ASP) are based on sophisticated Boolean constraint solving techniques. In both\nareas, conflict-driven learning and related techniques constitute key features\nwhose application is enabled by conflict analysis. Although various conflict\nanalysis schemes have been proposed, implemented, and studied both\ntheoretically and practically in the SAT area, the heuristic aspects involved\nin conflict analysis have not yet received much attention. Assuming a fixed\nconflict analysis scheme, we address the open question of how to identify\n\"good'' reasons for conflicts, and we investigate several heuristics for\nconflict analysis in ASP solving. To our knowledge, a systematic study like\nours has not yet been performed in the SAT area, thus, it might be beneficial\nfor both the field of ASP as well as the one of SAT solving.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2010 05:32:37 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Drescher", "Christian", ""], ["Gebser", "Martin", ""], ["Kaufmann", "Benjamin", ""], ["Schaub", "Torsten", ""]]}, {"id": "1005.1860", "submitter": "Gavin Taylor", "authors": "Marek Petrik, Gavin Taylor, Ron Parr, Shlomo Zilberstein", "title": "Feature Selection Using Regularization in Approximate Linear Programs\n  for Markov Decision Processes", "comments": "Technical report corresponding to the ICML2010 submission of the same\n  name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate dynamic programming has been used successfully in a large variety\nof domains, but it relies on a small set of provided approximation features to\ncalculate solutions reliably. Large and rich sets of features can cause\nexisting algorithms to overfit because of a limited number of samples. We\naddress this shortcoming using $L_1$ regularization in approximate linear\nprogramming. Because the proposed method can automatically select the\nappropriate richness of features, its performance does not degrade with an\nincreasing number of features. These results rely on new and stronger sampling\nbounds for regularized approximate linear programs. We also propose a\ncomputationally efficient homotopy method. The empirical evaluation of the\napproach shows that the proposed method performs well on simple MDPs and\nstandard benchmark problems.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2010 15:24:36 GMT"}, {"version": "v2", "created": "Thu, 20 May 2010 14:19:17 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Petrik", "Marek", ""], ["Taylor", "Gavin", ""], ["Parr", "Ron", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1005.1934", "submitter": "Michael Wick", "authors": "Michael Wick, Andrew McCallum, Gerome Miklau", "title": "Scalable Probabilistic Databases with Factor Graphs and MCMC", "comments": "Submitted to VLDB 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic databases play a crucial role in the management and\nunderstanding of uncertain data. However, incorporating probabilities into the\nsemantics of incomplete databases has posed many challenges, forcing systems to\nsacrifice modeling power, scalability, or restrict the class of relational\nalgebra formula under which they are closed. We propose an alternative approach\nwhere the underlying relational database always represents a single world, and\nan external factor graph encodes a distribution over possible worlds; Markov\nchain Monte Carlo (MCMC) inference is then used to recover this uncertainty to\na desired level of fidelity. Our approach allows the efficient evaluation of\narbitrary queries over probabilistic databases with arbitrary dependencies\nexpressed by graphical models with structure that changes during inference.\nMCMC sampling provides efficiency by hypothesizing {\\em modifications} to\npossible worlds rather than generating entire worlds from scratch. Queries are\nthen run over the portions of the world that change, avoiding the onerous cost\nof running full queries over each sampled world. A significant innovation of\nthis work is the connection between MCMC sampling and materialized view\nmaintenance techniques: we find empirically that using view maintenance\ntechniques is several orders of magnitude faster than naively querying each\nsampled world. We also demonstrate our system's ability to answer relational\nqueries with aggregation, and demonstrate additional scalability through the\nuse of parallelization.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2010 20:06:57 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Wick", "Michael", ""], ["McCallum", "Andrew", ""], ["Miklau", "Gerome", ""]]}, {"id": "1005.2303", "submitter": "Andrew Adamatzky", "authors": "Jeff Jones and Andrew Adamatzky", "title": "Towards Physarum Binary Adders", "comments": "Biosystems (2010), in press. Please download final version of the\n  paper from the Publishers's site", "journal-ref": "Biosystems Volume 101, Issue 1, July 2010, Pages 51-58", "doi": "10.1016/j.biosystems.2010.04.005", "report-no": null, "categories": "nlin.PS cs.AI physics.bio-ph q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plasmodium of \\emph{Physarum polycephalum} is a single cell visible by\nunaided eye. The plasmodium's foraging behaviour is interpreted in terms of\ncomputation. Input data is a configuration of nutrients, result of computation\nis a network of plasmodium's cytoplasmic tubes spanning sources of nutrients.\nTsuda et al (2004) experimentally demonstrated that basic logical gates can be\nimplemented in foraging behaviour of the plasmodium. We simplify the original\ndesigns of the gates and show --- in computer models --- that the plasmodium is\ncapable for computation of two-input two-output gate $<x, y> \\to <xy, x+y>$ and\nthree-input two-output $<x, y, z> \\to < \\bar{x}yz, x+y+z>$. We assemble the\ngates in a binary one-bit adder and demonstrate validity of the design using\ncomputer simulation.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2010 11:53:09 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Jones", "Jeff", ""], ["Adamatzky", "Andrew", ""]]}, {"id": "1005.2815", "submitter": "Marc Schoenauer", "authors": "Miguel Nicolau (INRIA Saclay - Ile de France, LRI), Marc Schoenauer\n  (INRIA Saclay - Ile de France, LRI), W. Banzhaf", "title": "Evolving Genes to Balance a Pole", "comments": null, "journal-ref": "EUropean Conference on Genetic Programming, Istanbul : Turkey\n  (2010)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss how to use a Genetic Regulatory Network as an evolutionary\nrepresentation to solve a typical GP reinforcement problem, the pole balancing.\nThe network is a modified version of an Artificial Regulatory Network proposed\na few years ago, and the task could be solved only by finding a proper way of\nconnecting inputs and outputs to the network. We show that the representation\nis able to generalize well over the problem domain, and discuss the performance\nof different models of this kind.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2010 06:44:44 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Nicolau", "Miguel", "", "INRIA Saclay - Ile de France, LRI"], ["Schoenauer", "Marc", "", "INRIA Saclay - Ile de France, LRI"], ["Banzhaf", "W.", ""]]}, {"id": "1005.3502", "submitter": "Lars Kotthoff", "authors": "Lars Kotthoff and Ian Gent and Ian Miguel", "title": "Using machine learning to make constraint solver implementation\n  decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs to solve so-called constraint problems are complex pieces of\nsoftware which require many design decisions to be made more or less\narbitrarily by the implementer. These decisions affect the performance of the\nfinished solver significantly. Once a design decision has been made, it cannot\neasily be reversed, although a different decision may be more appropriate for a\nparticular problem.\n  We investigate using machine learning to make these decisions automatically\ndepending on the problem to solve with the alldifferent constraint as an\nexample. Our system is capable of making non-trivial, multi-level decisions\nthat improve over always making a default choice.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2010 17:53:43 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Kotthoff", "Lars", ""], ["Gent", "Ian", ""], ["Miguel", "Ian", ""]]}, {"id": "1005.4025", "submitter": "William Jackson", "authors": "Siddharths Sankar Biswas", "title": "A Soft Computing Model for Physicians' Decision Process", "comments": "http://www.journalofcomputing.org", "journal-ref": "Journal of Computing, Volume 2, Issue 5, May 2010", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the author presents a kind of Soft Computing Technique, mainly\nan application of fuzzy set theory of Prof. Zadeh [16], on a problem of Medical\nExperts Systems. The choosen problem is on design of a physician's decision\nmodel which can take crisp as well as fuzzy data as input, unlike the\ntraditional models. The author presents a mathematical model based on fuzzy set\ntheory for physician aided evaluation of a complete representation of\ninformation emanating from the initial interview including patient past\nhistory, present symptoms, and signs observed upon physical examination and\nresults of clinical and diagnostic tests.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2010 17:40:38 GMT"}], "update_date": "2010-05-24", "authors_parsed": [["Biswas", "Siddharths Sankar", ""]]}, {"id": "1005.4032", "submitter": "Debotosh Bhattacharjee", "authors": "Sandhya Arora, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar\n  Basu, and Mahantapas Kundu", "title": "Combining Multiple Feature Extraction Techniques for Handwritten\n  Devnagari Character Recognition", "comments": "6 pages, 8-10 December 2008", "journal-ref": "ICIIS 2008", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an OCR for Handwritten Devnagari Characters. Basic\nsymbols are recognized by neural classifier. We have used four feature\nextraction techniques namely, intersection, shadow feature, chain code\nhistogram and straight line fitting features. Shadow features are computed\nglobally for character image while intersection features, chain code histogram\nfeatures and line fitting features are computed by dividing the character image\ninto different segments. Weighted majority voting technique is used for\ncombining the classification decision obtained from four Multi Layer\nPerceptron(MLP) based classifier. On experimentation with a dataset of 4900\nsamples the overall recognition rate observed is 92.80% as we considered top\nfive choices results. This method is compared with other recent methods for\nHandwritten Devnagari Character Recognition and it has been observed that this\napproach has better success rate than other methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2010 17:57:50 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Arora", "Sandhya", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1005.4159", "submitter": "Andrew Lin", "authors": "Andrew Lin", "title": "The Complexity of Manipulating $k$-Approval Elections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in computational social choice theory is the complexity\nof undesirable behavior among agents, such as control, manipulation, and\nbribery in election systems. These kinds of voting strategies are often\ntempting at the individual level but disastrous for the agents as a whole.\nCreating election systems where the determination of such strategies is\ndifficult is thus an important goal.\n  An interesting set of elections is that of scoring protocols. Previous work\nin this area has demonstrated the complexity of misuse in cases involving a\nfixed number of candidates, and of specific election systems on unbounded\nnumber of candidates such as Borda. In contrast, we take the first step in\ngeneralizing the results of computational complexity of election misuse to\ncases of infinitely many scoring protocols on an unbounded number of\ncandidates. Interesting families of systems include $k$-approval and $k$-veto\nelections, in which voters distinguish $k$ candidates from the candidate set.\n  Our main result is to partition the problems of these families based on their\ncomplexity. We do so by showing they are polynomial-time computable, NP-hard,\nor polynomial-time equivalent to another problem of interest. We also\ndemonstrate a surprising connection between manipulation in election systems\nand some graph theory problems.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2010 00:04:11 GMT"}, {"version": "v2", "created": "Thu, 27 May 2010 14:14:44 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2012 05:02:18 GMT"}], "update_date": "2012-04-20", "authors_parsed": [["Lin", "Andrew", ""]]}, {"id": "1005.4272", "submitter": "Chriss Romy", "authors": "G. Arutchelvan, S. K. Srivatsa, R. Jagannathan", "title": "Inaccuracy Minimization by Partioning Fuzzy Data Sets - Validation of\n  Analystical Methodology", "comments": "IEEE Publication format, International Journal of Computer Science\n  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In the last two decades, a number of methods have been proposed for\nforecasting based on fuzzy time series. Most of the fuzzy time series methods\nare presented for forecasting of car road accidents. However, the forecasting\naccuracy rates of the existing methods are not good enough. In this paper, we\ncompared our proposed new method of fuzzy time series forecasting with existing\nmethods. Our method is based on means based partitioning of the historical data\nof car road accidents. The proposed method belongs to the kth order and\ntime-variant methods. The proposed method can get the best forecasting accuracy\nrate for forecasting the car road accidents than the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2010 07:50:55 GMT"}], "update_date": "2010-05-25", "authors_parsed": [["Arutchelvan", "G.", ""], ["Srivatsa", "S. K.", ""], ["Jagannathan", "R.", ""]]}, {"id": "1005.4298", "submitter": "Sameer Singh", "authors": "Sameer Singh and Michael Wick and Andrew McCallum", "title": "Distantly Labeling Data for Large Scale Cross-Document Coreference", "comments": "16 pages, submitted to ECML 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-document coreference, the problem of resolving entity mentions across\nmulti-document collections, is crucial to automated knowledge base construction\nand data mining tasks. However, the scarcity of large labeled data sets has\nhindered supervised machine learning research for this task. In this paper we\ndevelop and demonstrate an approach based on ``distantly-labeling'' a data set\nfrom which we can train a discriminative cross-document coreference model. In\nparticular we build a dataset of more than a million people mentions extracted\nfrom 3.5 years of New York Times articles, leverage Wikipedia for distant\nlabeling with a generative model (and measure the reliability of such\nlabeling); then we train and evaluate a conditional random field coreference\nmodel that has factors on cross-document entities as well as mention-pairs.\nThis coreference model obtains high accuracy in resolving mentions and entities\nthat are not present in the training data, indicating applicability to\nnon-Wikipedia data. Given the large amount of data, our work is also an\nexercise demonstrating the scalability of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2010 10:35:50 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Singh", "Sameer", ""], ["Wick", "Michael", ""], ["McCallum", "Andrew", ""]]}, {"id": "1005.4446", "submitter": "Martyn Amos", "authors": "Jack Coldridge and Martyn Amos", "title": "Genetic algorithms and the art of Zen", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": "2010/5/1", "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel genetic algorithm (GA) solution to a simple\nyet challenging commercial puzzle game known as the Zen Puzzle Garden (ZPG). We\ndescribe the game in detail, before presenting a suitable encoding scheme and\nfitness function for candidate solutions. We then compare the performance of\nthe genetic algorithm with that of the A* algorithm. Our results show that the\nGA is competitive with informed search in terms of solution quality, and\nsignificantly out-performs it in terms of computational resource requirements.\nWe conclude with a brief discussion of the implications of our findings for\ngame solving and other \"real world\" problems.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2010 22:15:57 GMT"}], "update_date": "2010-05-26", "authors_parsed": [["Coldridge", "Jack", ""], ["Amos", "Martyn", ""]]}, {"id": "1005.4447", "submitter": "Alexander Lyaletski", "authors": "Alexander Lyaletski (1), Konstantin Verchinine (2) ((1) Kiev National\n  Taras Shevchenko University, (2) Math-Info Department, Paris 12 University)", "title": "Evidence Algorithm and System for Automated Deduction: A Retrospective\n  View", "comments": "To appear in The 9th International Conference on Mathematical\n  Knowledge Management: MKM 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A research project aimed at the development of an automated theorem proving\nsystem was started in Kiev (Ukraine) in early 1960s. The mastermind of the\nproject, Academician V.Glushkov, baptized it \"Evidence Algorithm\", EA. The work\non the project lasted, off and on, more than 40 years. In the framework of the\nproject, the Russian and English versions of the System for Automated\nDeduction, SAD, were constructed. They may be already seen as powerful\ntheorem-proving assistants.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2010 22:18:29 GMT"}], "update_date": "2010-05-26", "authors_parsed": [["Lyaletski", "Alexander", ""], ["Verchinine", "Konstantin", ""]]}, {"id": "1005.4496", "submitter": "Secretary Aircc Journal", "authors": "Dewan Md. Farid(1), Nouria Harbi(1), and Mohammad Zahidur Rahman(2),\n  ((1)University Lumiere Lyon 2 - France, (2)Jahangirnagar University,\n  Bangladesh)", "title": "Combining Naive Bayes and Decision Tree for Adaptive Intrusion Detection", "comments": "14 Pages, IJNSA", "journal-ref": "International Journal of Network Security & Its Applications 2.2\n  (2010) 12-25", "doi": "10.5121/ijnsa.2010.2202", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, a new learning algorithm for adaptive network intrusion\ndetection using naive Bayesian classifier and decision tree is presented, which\nperforms balance detections and keeps false positives at acceptable level for\ndifferent types of network attacks, and eliminates redundant attributes as well\nas contradictory examples from training data that make the detection model\ncomplex. The proposed algorithm also addresses some difficulties of data mining\nsuch as handling continuous attribute, dealing with missing attribute values,\nand reducing noise in training data. Due to the large volumes of security audit\ndata as well as the complex and dynamic properties of intrusion behaviours,\nseveral data miningbased intrusion detection techniques have been applied to\nnetwork-based traffic data and host-based data in the last decades. However,\nthere remain various issues needed to be examined towards current intrusion\ndetection systems (IDS). We tested the performance of our proposed algorithm\nwith existing learning algorithms by employing on the KDD99 benchmark intrusion\ndetection dataset. The experimental results prove that the proposed algorithm\nachieved high detection rates (DR) and significant reduce false positives (FP)\nfor different types of network intrusions using limited computational\nresources.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2010 07:47:00 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Farid", "Dewan Md.", ""], ["Harbi", "Nouria", ""], ["Rahman", "Mohammad Zahidur", ""]]}, {"id": "1005.4592", "submitter": "Josef Urban", "authors": "Josef Urban and Geoff Sutcliffe", "title": "Automated Reasoning and Presentation Support for Formalizing Mathematics\n  in Mizar", "comments": "To appear in 10th International Conference on. Artificial\n  Intelligence and Symbolic Computation AISC 2010", "journal-ref": "Intelligent Computer Mathematics 2010, LNCS 6167, pp. 132-146", "doi": "10.1007/978-3-642-14128-7_12", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a combination of several automated reasoning and proof\npresentation tools with the Mizar system for formalization of mathematics. The\ncombination forms an online service called MizAR, similar to the SystemOnTPTP\nservice for first-order automated reasoning. The main differences to\nSystemOnTPTP are the use of the Mizar language that is oriented towards human\nmathematicians (rather than the pure first-order logic used in SystemOnTPTP),\nand setting the service in the context of the large Mizar Mathematical Library\nof previous theorems,definitions, and proofs (rather than the isolated problems\nthat are solved in SystemOnTPTP). These differences poses new challenges and\nnew opportunities for automated reasoning and for proof presentation tools.\nThis paper describes the overall structure of MizAR, and presents the automated\nreasoning systems and proof presentation tools that are combined to make MizAR\na useful mathematical service.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2010 14:49:03 GMT"}], "update_date": "2011-07-27", "authors_parsed": [["Urban", "Josef", ""], ["Sutcliffe", "Geoff", ""]]}, {"id": "1005.4963", "submitter": "Anon Plangprasopchok", "authors": "Anon Plangprasopchok, Kristina Lerman, Lise Getoor", "title": "Integrating Structured Metadata with Relational Affinity Propagation", "comments": "6 Pages, To appear at AAAI Workshop on Statistical Relational AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured and semi-structured data describing entities, taxonomies and\nontologies appears in many domains. There is a huge interest in integrating\nstructured information from multiple sources; however integrating structured\ndata to infer complex common structures is a difficult task because the\nintegration must aggregate similar structures while avoiding structural\ninconsistencies that may appear when the data is combined. In this work, we\nstudy the integration of structured social metadata: shallow personal\nhierarchies specified by many individual users on the SocialWeb, and focus on\ninferring a collection of integrated, consistent taxonomies. We frame this task\nas an optimization problem with structural constraints. We propose a new\ninference algorithm, which we refer to as Relational Affinity Propagation (RAP)\nthat extends affinity propagation (Frey and Dueck 2007) by introducing\nstructural constraints. We validate the approach on a real-world social media\ndataset, collected from the photosharing website Flickr. Our empirical results\nshow that our proposed approach is able to construct deeper and denser\nstructures compared to an approach using only the standard affinity propagation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2010 23:13:05 GMT"}], "update_date": "2010-05-28", "authors_parsed": [["Plangprasopchok", "Anon", ""], ["Lerman", "Kristina", ""], ["Getoor", "Lise", ""]]}, {"id": "1005.4989", "submitter": "Evgeny Chutchev", "authors": "Evgeny Chutchev", "title": "A Formalization of the Turing Test", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper offers a mathematical formalization of the Turing test. This\nformalization makes it possible to establish the conditions under which some\nTuring machine will pass the Turing test and the conditions under which every\nTuring machine (or every Turing machine of the special class) will fail the\nTuring test.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2010 05:59:56 GMT"}], "update_date": "2010-05-28", "authors_parsed": [["Chutchev", "Evgeny", ""]]}, {"id": "1005.5114", "submitter": "Anon Plangprasopchok", "authors": "Anon Plangprasopchok, Kristina Lerman, Lise Getoor", "title": "Growing a Tree in the Forest: Constructing Folksonomies by Integrating\n  Structured Metadata", "comments": "10 pages, To appear in the Proceedings of ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining(KDD) 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many social Web sites allow users to annotate the content with descriptive\nmetadata, such as tags, and more recently to organize content hierarchically.\nThese types of structured metadata provide valuable evidence for learning how a\ncommunity organizes knowledge. For instance, we can aggregate many personal\nhierarchies into a common taxonomy, also known as a folksonomy, that will aid\nusers in visualizing and browsing social content, and also to help them in\norganizing their own content. However, learning from social metadata presents\nseveral challenges, since it is sparse, shallow, ambiguous, noisy, and\ninconsistent. We describe an approach to folksonomy learning based on\nrelational clustering, which exploits structured metadata contained in personal\nhierarchies. Our approach clusters similar hierarchies using their structure\nand tag statistics, then incrementally weaves them into a deeper, bushier tree.\nWe study folksonomy learning using social metadata extracted from the\nphoto-sharing site Flickr, and demonstrate that the proposed approach addresses\nthe challenges. Moreover, comparing to previous work, the approach produces\nlarger, more accurate folksonomies, and in addition, scales better.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2010 16:46:04 GMT"}], "update_date": "2010-05-28", "authors_parsed": [["Plangprasopchok", "Anon", ""], ["Lerman", "Kristina", ""], ["Getoor", "Lise", ""]]}, {"id": "1005.5124", "submitter": "Manfred Kerber", "authors": "Manfred Kerber", "title": "Proofs, proofs, proofs, and proofs", "comments": "10 pages, To appear in The 9th International Conference on\n  Mathematical Knowledge Management: MKM 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In logic there is a clear concept of what constitutes a proof and what not. A\nproof is essentially defined as a finite sequence of formulae which are either\naxioms or derived by proof rules from formulae earlier in the sequence.\nSociologically, however, it is more difficult to say what should constitute a\nproof and what not. In this paper we will look at different forms of proofs and\ntry to clarify the concept of proof in the wider meaning of the term. This has\nimplications on how proofs should be represented formally.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2010 17:21:06 GMT"}], "update_date": "2010-05-28", "authors_parsed": [["Kerber", "Manfred", ""]]}, {"id": "1005.5253", "submitter": "Sergio Guadarrama", "authors": "Sergio Guadarrama (1) and David P. Pancho (1) ((1) European Centre for\n  Soft Computing)", "title": "Using Soft Constraints To Learn Semantic Models Of Descriptions Of\n  Shapes", "comments": "8 pages, 8 figures, WCCI'10 Conference", "journal-ref": null, "doi": null, "report-no": "FSC 2009-22", "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contribution of this paper is to provide a semantic model (using soft\nconstraints) of the words used by web-users to describe objects in a language\ngame; a game in which one user describes a selected object of those composing\nthe scene, and another user has to guess which object has been described. The\ngiven description needs to be non ambiguous and accurate enough to allow other\nusers to guess the described shape correctly.\n  To build these semantic models the descriptions need to be analyzed to\nextract the syntax and words' classes used. We have modeled the meaning of\nthese descriptions using soft constraints as a way for grounding the meaning.\n  The descriptions generated by the system took into account the context of the\nobject to avoid ambiguous descriptions, and allowed users to guess the\ndescribed object correctly 72% of the times.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2010 09:41:50 GMT"}], "update_date": "2010-05-31", "authors_parsed": [["Guadarrama", "Sergio", ""], ["Pancho", "David P.", ""]]}, {"id": "1005.5268", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "An Empirical Study of the Manipulability of Single Transferable Voting", "comments": "To appear in Proceedings of the 19th European Conference on\n  Artificial Intelligence (ECAI 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voting is a simple mechanism to combine together the preferences of multiple\nagents. Agents may try to manipulate the result of voting by mis-reporting\ntheir preferences. One barrier that might exist to such manipulation is\ncomputational complexity. In particular, it has been shown that it is NP-hard\nto compute how to manipulate a number of different voting rules. However,\nNP-hardness only bounds the worst-case complexity. Recent theoretical results\nsuggest that manipulation may often be easy in practice. In this paper, we\nstudy empirically the manipulability of single transferable voting (STV) to\ndetermine if computational complexity is really a barrier to manipulation. STV\nwas one of the first voting rules shown to be NP-hard. It also appears one of\nthe harder voting rules to manipulate. We sample a number of distributions of\nvotes including uniform and real world elections. In almost every election in\nour experiments, it was easy to compute how a single agent could manipulate the\nelection or to prove that manipulation by a single agent was impossible.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2010 11:11:56 GMT"}], "update_date": "2012-04-18", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "1005.5270", "submitter": "Toby Walsh", "authors": "George Katsirelos and Toby Walsh", "title": "Symmetries of Symmetry Breaking Constraints", "comments": "To appear in Proceedings of the 19th European Conference on\n  Artificial Intelligence (ECAI 2010). Revises workshop paper that appears at\n  SymCon 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry is an important feature of many constraint programs. We show that\nany problem symmetry acting on a set of symmetry breaking constraints can be\nused to break symmetry. Different symmetries pick out different solutions in\neach symmetry class. This simple but powerful idea can be used in a number of\ndifferent ways. We describe one application within model restarts, a search\ntechnique designed to reduce the conflict between symmetry breaking and the\nbranching heuristic. In model restarts, we restart search periodically with a\nrandom symmetry of the symmetry breaking constraints. Experimental results show\nthat this symmetry breaking technique is effective in practice on some standard\nbenchmark problems.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2010 11:22:29 GMT"}], "update_date": "2010-05-31", "authors_parsed": [["Katsirelos", "George", ""], ["Walsh", "Toby", ""]]}, {"id": "1005.5448", "submitter": "Shailesh Kumar Jagadeesan", "authors": "Shailesh Kumar and Shrisha Rao", "title": "Failover in cellular automata", "comments": "16 pages, 15 figures and associated video at\n  http://dl.dropbox.com/u/7553694/failover_demo.avi and simulation at\n  http://dl.dropbox.com/u/7553694/failover_simulation.jar", "journal-ref": "Physics Procedia, vol. 22, pp. 557--564, 2011", "doi": "10.1016/j.phpro.2011.11.086", "report-no": null, "categories": "cs.AI nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cellular automata (CA) configuration is constructed that exhibits emergent\nfailover. The configuration is based on standard Game of Life rules. Gliders\nand glider-guns form the core messaging structure in the configuration. The\nblinker is represented as the basic computational unit, and it is shown how it\ncan be recreated in case of a failure. Stateless failover using primary-backup\nmechanism is demonstrated. The details of the CA components used in the\nconfiguration and its working are described, and a simulation of the complete\nconfiguration is also presented.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2010 12:17:09 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Kumar", "Shailesh", ""], ["Rao", "Shrisha", ""]]}, {"id": "1005.5556", "submitter": "Ridwan Al Iqbal", "authors": "Ridwan Al Iqbal", "title": "Empirical learning aided by weak domain knowledge in the form of feature\n  importance", "comments": "9 pages, 1 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard hybrid learners that use domain knowledge require stronger knowledge\nthat is hard and expensive to acquire. However, weaker domain knowledge can\nbenefit from prior knowledge while being cost effective. Weak knowledge in the\nform of feature relative importance (FRI) is presented and explained. Feature\nrelative importance is a real valued approximation of a feature's importance\nprovided by experts. Advantage of using this knowledge is demonstrated by IANN,\na modified multilayer neural network algorithm. IANN is a very simple\nmodification of standard neural network algorithm but attains significant\nperformance gains. Experimental results in the field of molecular biology show\nhigher performance over other empirical learning algorithms including standard\nbackpropagation and support vector machines. IANN performance is even\ncomparable to a theory refinement system KBANN that uses stronger domain\nknowledge. This shows Feature relative importance can improve performance of\nexisting empirical learning algorithms significantly with minimal effort.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2010 19:28:01 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2010 14:45:54 GMT"}], "update_date": "2010-06-04", "authors_parsed": [["Iqbal", "Ridwan Al", ""]]}]