[{"id": "1603.00423", "submitter": "Phong Le", "authors": "Phong Le and Willem Zuidema", "title": "Quantifying the vanishing gradient and long distance dependency problem\n  in recursive neural networks and recursive LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive neural networks (RNN) and their recently proposed extension\nrecursive long short term memory networks (RLSTM) are models that compute\nrepresentations for sentences, by recursively combining word embeddings\naccording to an externally provided parse tree. Both models thus, unlike\nrecurrent networks, explicitly make use of the hierarchical structure of a\nsentence. In this paper, we demonstrate that RNNs nevertheless suffer from the\nvanishing gradient and long distance dependency problem, and that RLSTMs\ngreatly improve over RNN's on these problems. We present an artificial learning\ntask that allows us to quantify the severity of these problems for both models.\nWe further show that a ratio of gradients (at the root node and a focal leaf\nnode) is highly indicative of the success of backpropagation at optimizing the\nrelevant weights low in the tree. This paper thus provides an explanation for\nexisting, superior results of RLSTMs on tasks such as sentiment analysis, and\nsuggests that the benefits of including hierarchical structure and of including\nLSTM-style gating are complementary.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 19:45:25 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Le", "Phong", ""], ["Zuidema", "Willem", ""]]}, {"id": "1603.00448", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Sergey Levine, Pieter Abbeel", "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy\n  Optimization", "comments": "International Conference on Machine Learning (ICML), 2016, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning can acquire complex behaviors from high-level\nspecifications. However, defining a cost function that can be optimized\neffectively and encodes the correct task is challenging in practice. We explore\nhow inverse optimal control (IOC) can be used to learn behaviors from\ndemonstrations, with applications to torque control of high-dimensional robotic\nsystems. Our method addresses two key challenges in inverse optimal control:\nfirst, the need for informative features and effective regularization to impose\nstructure on the cost, and second, the difficulty of learning the cost function\nunder unknown dynamics for high-dimensional continuous systems. To address the\nformer challenge, we present an algorithm capable of learning arbitrary\nnonlinear cost functions, such as neural networks, without meticulous feature\nengineering. To address the latter challenge, we formulate an efficient\nsample-based approximation for MaxEnt IOC. We evaluate our method on a series\nof simulated tasks and real-world robotic manipulation problems, demonstrating\nsubstantial improvement over prior methods both in terms of task complexity and\nsample efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 20:35:56 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 07:30:36 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 16:53:46 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Finn", "Chelsea", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1603.00709", "submitter": "Mouna Ben Ishak", "authors": "Mouna Ben Ishak (LARODEC), Rajani Chulyadyo (LINA), Philippe Leray\n  (LINA)", "title": "Probabilistic Relational Model Benchmark Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The validation of any database mining methodology goes through an evaluation\nprocess where benchmarks availability is essential. In this paper, we aim to\nrandomly generate relational database benchmarks that allow to check\nprobabilistic dependencies among the attributes. We are particularly interested\nin Probabilistic Relational Models (PRMs), which extend Bayesian Networks (BNs)\nto a relational data mining context and enable effective and robust reasoning\nover relational data. Even though a panoply of works have focused, separately ,\non the generation of random Bayesian networks and relational databases, no work\nhas been identified for PRMs on that track. This paper provides an algorithmic\napproach for generating random PRMs from scratch to fill this gap. The proposed\nmethod allows to generate PRMs as well as synthetic relational data from a\nrandomly generated relational schema and a random set of probabilistic\ndependencies. This can be of interest not only for machine learning researchers\nto evaluate their proposals in a common framework, but also for databases\ndesigners to evaluate the effectiveness of the components of a database\nmanagement system.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 13:46:31 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Ishak", "Mouna Ben", "", "LARODEC"], ["Chulyadyo", "Rajani", "", "LINA"], ["Leray", "Philippe", "", "LINA"]]}, {"id": "1603.00748", "submitter": "Shixiang Gu", "authors": "Shixiang Gu and Timothy Lillicrap and Ilya Sutskever and Sergey Levine", "title": "Continuous Deep Q-Learning with Model-based Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free reinforcement learning has been successfully applied to a range of\nchallenging problems, and has recently been extended to handle large neural\nnetwork policies and value functions. However, the sample complexity of\nmodel-free algorithms, particularly when using high-dimensional function\napproximators, tends to limit their applicability to physical systems. In this\npaper, we explore algorithms and representations to reduce the sample\ncomplexity of deep reinforcement learning for continuous control tasks. We\npropose two complementary techniques for improving the efficiency of such\nalgorithms. First, we derive a continuous variant of the Q-learning algorithm,\nwhich we call normalized adantage functions (NAF), as an alternative to the\nmore commonly used policy gradient and actor-critic methods. NAF representation\nallows us to apply Q-learning with experience replay to continuous tasks, and\nsubstantially improves performance on a set of simulated robotic control tasks.\nTo further improve the efficiency of our approach, we explore the use of\nlearned models for accelerating model-free reinforcement learning. We show that\niteratively refitted local linear models are especially effective for this, and\ndemonstrate substantially faster learning on domains where such models are\napplicable.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 15:28:25 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Gu", "Shixiang", ""], ["Lillicrap", "Timothy", ""], ["Sutskever", "Ilya", ""], ["Levine", "Sergey", ""]]}, {"id": "1603.00772", "submitter": "Azad Naik", "authors": "Azad Naik, Huzefa Rangwala", "title": "Filter based Taxonomy Modification for Improving Hierarchical\n  Classification", "comments": "The conference version of the paper is submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Classification (HC) is a supervised learning problem where\nunlabeled instances are classified into a taxonomy of classes. Several methods\nthat utilize the hierarchical structure have been developed to improve the HC\nperformance. However, in most cases apriori defined hierarchical structure by\ndomain experts is inconsistent; as a consequence performance improvement is not\nnoticeable in comparison to flat classification methods. We propose a scalable\ndata-driven filter based rewiring approach to modify an expert-defined\nhierarchy. Experimental comparisons of top-down HC with our modified hierarchy,\non a wide range of datasets shows classification performance improvement over\nthe baseline hierarchy (i:e:, defined by expert), clustered hierarchy and\nflattening based hierarchy modification approaches. In comparison to existing\nrewiring approaches, our developed method (rewHier) is computationally\nefficient, enabling it to scale to datasets with large numbers of classes,\ninstances and features. We also show that our modified hierarchy leads to\nimproved classification performance for classes with few training samples in\ncomparison to flat and state-of-the-art HC approaches.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 16:14:49 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 06:41:42 GMT"}, {"version": "v3", "created": "Sat, 15 Oct 2016 06:21:54 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Naik", "Azad", ""], ["Rangwala", "Huzefa", ""]]}, {"id": "1603.00788", "submitter": "Alp Kucukelbir", "authors": "Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M.\n  Blei", "title": "Automatic Differentiation Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling is iterative. A scientist posits a simple model, fits\nit to her data, refines it according to her analysis, and repeats. However,\nfitting complex models to large data is a bottleneck in this process. Deriving\nalgorithms for new models can be both mathematically and computationally\nchallenging, which makes it difficult to efficiently cycle through the steps.\nTo this end, we develop automatic differentiation variational inference (ADVI).\nUsing our method, the scientist only provides a probabilistic model and a\ndataset, nothing else. ADVI automatically derives an efficient variational\ninference algorithm, freeing the scientist to refine and explore many models.\nADVI supports a broad class of models-no conjugacy assumptions are required. We\nstudy ADVI across ten different models and apply it to a dataset with millions\nof observations. ADVI is integrated into Stan, a probabilistic programming\nsystem; it is available for immediate use.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 16:43:15 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Kucukelbir", "Alp", ""], ["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Gelman", "Andrew", ""], ["Blei", "David M.", ""]]}, {"id": "1603.00806", "submitter": "Florian Strub", "authors": "Florian Strub (SEQUEL, CRIStAL), Jeremie Mary (CRIStAL, SEQUEL),\n  Romaric Gaudel (LIFL)", "title": "Hybrid Collaborative Filtering with Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering aims at exploiting the feedback of users to provide\npersonalised recommendations. Such algorithms look for latent variables in a\nlarge sparse matrix of ratings. They can be enhanced by adding side information\nto tackle the well-known cold start problem. While Neu-ral Networks have\ntremendous success in image and speech recognition, they have received less\nattention in Collaborative Filtering. This is all the more surprising that\nNeural Networks are able to discover latent variables in large and\nheterogeneous datasets. In this paper, we introduce a Collaborative Filtering\nNeural network architecture aka CFN which computes a non-linear Matrix\nFactorization from sparse rating inputs and side information. We show\nexperimentally on the MovieLens and Douban dataset that CFN outper-forms the\nstate of the art and benefits from side information. We provide an\nimplementation of the algorithm as a reusable plugin for Torch, a popular\nNeural Network framework.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 17:48:25 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 19:18:09 GMT"}, {"version": "v3", "created": "Tue, 19 Jul 2016 08:10:08 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Strub", "Florian", "", "SEQUEL, CRIStAL"], ["Mary", "Jeremie", "", "CRIStAL, SEQUEL"], ["Gaudel", "Romaric", "", "LIFL"]]}, {"id": "1603.00964", "submitter": "Zhen Zeng", "authors": "Zhen Zeng, Benjamin Kuipers", "title": "Object Manipulation Learning by Imitation", "comments": "A more detailed report compared to previous versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to enable robot to learn object manipulation by imitation. Given\nexternal observations of demonstrations on object manipulations, we believe\nthat two underlying problems to address in learning by imitation is 1) segment\na given demonstration into skills that can be individually learned and reused,\nand 2) formulate the correct RL (Reinforcement Learning) problem that only\nconsiders the relevant aspects of each skill so that the policy for each skill\ncan be effectively learned. Previous works made certain progress in this\ndirection, but none has taken private information into account. The public\ninformation is the information that is available in the external observations\nof demonstration, and the private information is the information that are only\navailable to the agent that executes the actions, such as tactile sensations.\nOur contribution is that we provide a method for the robot to automatically\nsegment the demonstration of object manipulations into multiple skills, and\nformulate the correct RL problem for each skill, and automatically decide\nwhether the private information is an important aspect of each skill based on\ninteraction with the world. Our experiment shows that our robot learns to pick\nup a block, and stack it onto another block by imitating an observed\ndemonstration. The evaluation is based on 1) whether the demonstration is\nreasonably segmented, 2) whether the correct RL problems are formulated, 3) and\nwhether a good policy is learned.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 03:49:02 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 19:20:03 GMT"}, {"version": "v3", "created": "Sun, 19 Nov 2017 01:33:43 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Zeng", "Zhen", ""], ["Kuipers", "Benjamin", ""]]}, {"id": "1603.01006", "submitter": "Manuel Marin-Jimenez", "authors": "F.M. Castro and M.J. Marin-Jimenez and N. Guil and N. Perez de la\n  Blanca", "title": "Automatic learning of gait signatures for people identification", "comments": "Proof of concept paper. Technical report on the use of ConvNets (CNN)\n  for gait recognition. Data and code:\n  http://www.uco.es/~in1majim/research/cnngaitof.html", "journal-ref": null, "doi": null, "report-no": "2016-03", "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work targets people identification in video based on the way they walk\n(i.e. gait). While classical methods typically derive gait signatures from\nsequences of binary silhouettes, in this work we explore the use of\nconvolutional neural networks (CNN) for learning high-level descriptors from\nlow-level motion features (i.e. optical flow components). We carry out a\nthorough experimental evaluation of the proposed CNN architecture on the\nchallenging TUM-GAID dataset. The experimental results indicate that using\nspatio-temporal cuboids of optical flow as input data for CNN allows to obtain\nstate-of-the-art results on the gait task with an image resolution eight times\nlower than the previously reported results (i.e. 80x60 pixels).\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 08:07:14 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 16:07:07 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Castro", "F. M.", ""], ["Marin-Jimenez", "M. J.", ""], ["Guil", "N.", ""], ["de la Blanca", "N. Perez", ""]]}, {"id": "1603.01067", "submitter": "Itir Onal Ertugrul", "authors": "Itir Onal, Mete Ozay, Eda Mizrak, Ilke Oztekin, Fatos T. Yarman Vural", "title": "Modeling the Sequence of Brain Volumes by Local Mesh Models for Brain\n  Decoding", "comments": "13 pages, 10 figures, submitted to JSTSP Special Issue on Advanced\n  Signal Processing in Brain Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We represent the sequence of fMRI (Functional Magnetic Resonance Imaging)\nbrain volumes recorded during a cognitive stimulus by a graph which consists of\na set of local meshes. The corresponding cognitive process, encoded in the\nbrain, is then represented by these meshes each of which is estimated assuming\na linear relationship among the voxel time series in a predefined locality.\nFirst, we define the concept of locality in two neighborhood systems, namely,\nthe spatial and functional neighborhoods. Then, we construct spatially and\nfunctionally local meshes around each voxel, called seed voxel, by connecting\nit either to its spatial or functional p-nearest neighbors. The mesh formed\naround a voxel is a directed sub-graph with a star topology, where the\ndirection of the edges is taken towards the seed voxel at the center of the\nmesh. We represent the time series recorded at each seed voxel in terms of\nlinear combination of the time series of its p-nearest neighbors in the mesh.\nThe relationships between a seed voxel and its neighbors are represented by the\nedge weights of each mesh, and are estimated by solving a linear regression\nequation. The estimated mesh edge weights lead to a better representation of\ninformation in the brain for encoding and decoding of the cognitive tasks. We\ntest our model on a visual object recognition and emotional memory retrieval\nexperiments using Support Vector Machines that are trained using the mesh edge\nweights as features. In the experimental analysis, we observe that the edge\nweights of the spatial and functional meshes perform better than the\nstate-of-the-art brain decoding models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 12:06:00 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Onal", "Itir", ""], ["Ozay", "Mete", ""], ["Mizrak", "Eda", ""], ["Oztekin", "Ilke", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1603.01121", "submitter": "Johannes Heinrich", "authors": "Johannes Heinrich, David Silver", "title": "Deep Reinforcement Learning from Self-Play in Imperfect-Information\n  Games", "comments": "updated version, incorporating conference feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications can be described as large-scale games of\nimperfect information. To deal with these challenging domains, prior work has\nfocused on computing Nash equilibria in a handcrafted abstraction of the\ndomain. In this paper we introduce the first scalable end-to-end approach to\nlearning approximate Nash equilibria without prior domain knowledge. Our method\ncombines fictitious self-play with deep reinforcement learning. When applied to\nLeduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium,\nwhereas common reinforcement learning methods diverged. In Limit Texas Holdem,\na poker game of real-world scale, NFSP learnt a strategy that approached the\nperformance of state-of-the-art, superhuman algorithms based on significant\ndomain expertise.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 15:01:54 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 15:28:30 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Heinrich", "Johannes", ""], ["Silver", "David", ""]]}, {"id": "1603.01182", "submitter": "Filipe Alves Neto Verri", "authors": "Filipe Alves Neto Verri, Paulo Roberto Urio, Liang Zhao", "title": "Network Unfolding Map by Edge Dynamics Modeling", "comments": "Published version in http://ieeexplore.ieee.org/document/7762202/", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, vol.\n  29, no. 2, pp. 405-418, Feb. 2018. doi: 10.1109/TNNLS.2016.2626341", "doi": "10.1109/TNNLS.2016.2626341", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of collective dynamics in neural networks is a mechanism of the\nanimal and human brain for information processing. In this paper, we develop a\ncomputational technique using distributed processing elements in a complex\nnetwork, which are called particles, to solve semi-supervised learning\nproblems. Three actions govern the particles' dynamics: generation, walking,\nand absorption. Labeled vertices generate new particles that compete against\nrival particles for edge domination. Active particles randomly walk in the\nnetwork until they are absorbed by either a rival vertex or an edge currently\ndominated by rival particles. The result from the model evolution consists of\nsets of edges arranged by the label dominance. Each set tends to form a\nconnected subnetwork to represent a data class. Although the intrinsic dynamics\nof the model is a stochastic one, we prove there exists a deterministic version\nwith largely reduced computational complexity; specifically, with linear\ngrowth. Furthermore, the edge domination process corresponds to an unfolding\nmap in such way that edges \"stretch\" and \"shrink\" according to the vertex-edge\ndynamics. Consequently, the unfolding effect summarizes the relevant\nrelationships between vertices and the uncovered data classes. The proposed\nmodel captures important details of connectivity patterns over the vertex-edge\ndynamics evolution, in contrast to previous approaches which focused on only\nvertex or only edge dynamics. Computer simulations reveal that the new model\ncan identify nonlinear features in both real and artificial data, including\nboundaries between distinct classes and overlapping structures of data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 17:11:23 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 12:02:21 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Verri", "Filipe Alves Neto", ""], ["Urio", "Paulo Roberto", ""], ["Zhao", "Liang", ""]]}, {"id": "1603.01228", "submitter": "Zolt\\'an Kov\\'acs", "authors": "Zolt\\'an Kov\\'acs, Csilla S\\'olyom-Gecse", "title": "GeoGebra Tools with Proof Capabilities", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report about significant enhancements of the complex algebraic geometry\ntheorem proving subsystem in GeoGebra for automated proofs in Euclidean\ngeometry, concerning the extension of numerous GeoGebra tools with proof\ncapabilities. As a result, a number of elementary theorems can be proven by\nusing GeoGebra's intuitive user interface on various computer architectures\nincluding native Java and web based systems with JavaScript. We also provide a\ntest suite for benchmarking our results with 200 test cases.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 19:29:08 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Kov\u00e1cs", "Zolt\u00e1n", ""], ["S\u00f3lyom-Gecse", "Csilla", ""]]}, {"id": "1603.01250", "submitter": "Yani Ioannou", "authors": "Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie\n  Shotton, Matthew Brown, and Antonio Criminisi", "title": "Decision Forests, Convolutional Networks and the Models in-Between", "comments": "Microsoft Research Technical Report", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2015-58", "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the connections between two state of the art\nclassifiers: decision forests (DFs, including decision jungles) and\nconvolutional neural networks (CNNs). Decision forests are computationally\nefficient thanks to their conditional computation property (computation is\nconfined to only a small region of the tree, the nodes along a single branch).\nCNNs achieve state of the art accuracy, thanks to their representation learning\ncapabilities. We present a systematic analysis of how to fuse conditional\ncomputation with representation learning and achieve a continuum of hybrid\nmodels with different ratios of accuracy vs. efficiency. We call this new\nfamily of hybrid models conditional networks. Conditional networks can be\nthought of as: i) decision trees augmented with data transformation operators,\nor ii) CNNs, with block-diagonal sparse weight matrices, and explicit data\nrouting functions. Experimental validation is performed on the common task of\nimage classification on both the CIFAR and Imagenet datasets. Compared to state\nof the art CNNs, our hybrid models yield the same accuracy with a fraction of\nthe compute cost and much smaller number of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 20:41:47 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Ioannou", "Yani", ""], ["Robertson", "Duncan", ""], ["Zikic", "Darko", ""], ["Kontschieder", "Peter", ""], ["Shotton", "Jamie", ""], ["Brown", "Matthew", ""], ["Criminisi", "Antonio", ""]]}, {"id": "1603.01312", "submitter": "Rob Fergus", "authors": "Adam Lerer, Sam Gross and Rob Fergus", "title": "Learning Physical Intuition of Block Towers by Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wooden blocks are a common toy for infants, allowing them to develop motor\nskills and gain intuition about the physical behavior of the world. In this\npaper, we explore the ability of deep feed-forward models to learn such\nintuitive physics. Using a 3D game engine, we create small towers of wooden\nblocks whose stability is randomized and render them collapsing (or remaining\nupright). This data allows us to train large convolutional network models which\ncan accurately predict the outcome, as well as estimating the block\ntrajectories. The models are also able to generalize in two important ways: (i)\nto new physical scenarios, e.g. towers with an additional block and (ii) to\nimages of real wooden blocks, where it obtains a performance comparable to\nhuman subjects.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 22:59:35 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Lerer", "Adam", ""], ["Gross", "Sam", ""], ["Fergus", "Rob", ""]]}, {"id": "1603.01488", "submitter": "EPTCS", "authors": "Adrien Basso-Blandin (LIP, ENS Lyon), Walter Fontana (Harvard Medical\n  School), Russ Harmer (CNRS & LIP, ENS Lyon)", "title": "A knowledge representation meta-model for rule-based modelling of\n  signalling networks", "comments": "In Proceedings DCM 2015, arXiv:1603.00536", "journal-ref": "EPTCS 204, 2016, pp. 47-59", "doi": "10.4204/EPTCS.204.5", "report-no": null, "categories": "cs.AI q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of cellular signalling pathways and their deregulation in disease\nstates, such as cancer, is a large and extremely complex task. Indeed, these\nsystems involve many parts and processes but are studied piecewise and their\nliteratures and data are consequently fragmented, distributed and sometimes--at\nleast apparently--inconsistent. This makes it extremely difficult to build\nsignificant explanatory models with the result that effects in these systems\nthat are brought about by many interacting factors are poorly understood.\n  The rule-based approach to modelling has shown some promise for the\nrepresentation of the highly combinatorial systems typically found in\nsignalling where many of the proteins are composed of multiple binding domains,\ncapable of simultaneous interactions, and/or peptide motifs controlled by\npost-translational modifications. However, the rule-based approach requires\nhighly detailed information about the precise conditions for each and every\ninteraction which is rarely available from any one single source. Rather, these\nconditions must be painstakingly inferred and curated, by hand, from\ninformation contained in many papers--each of which contains only part of the\nstory.\n  In this paper, we introduce a graph-based meta-model, attuned to the\nrepresentation of cellular signalling networks, which aims to ease this massive\ncognitive burden on the rule-based curation process. This meta-model is a\ngeneralization of that used by Kappa and BNGL which allows for the flexible\nrepresentation of knowledge at various levels of granularity. In particular, it\nallows us to deal with information which has either too little, or too much,\ndetail with respect to the strict rule-based meta-model. Our approach provides\na basis for the gradual aggregation of fragmented biological knowledge\nextracted from the literature into an instance of the meta-model from which we\ncan define an automated translation into executable Kappa programs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 05:34:00 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Basso-Blandin", "Adrien", "", "LIP, ENS Lyon"], ["Fontana", "Walter", "", "Harvard Medical\n  School"], ["Harmer", "Russ", "", "CNRS & LIP, ENS Lyon"]]}, {"id": "1603.01524", "submitter": "Ilan Nehama", "authors": "Ilan Nehama", "title": "Analyzing Games with Ambiguous Player Types using the ${\\rm MINthenMAX}$\n  Decision Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many common interactive scenarios, participants lack information about\nother participants, and specifically about the preferences of other\nparticipants. In this work, we model an extreme case of incomplete information,\nwhich we term games with type ambiguity, where a participant lacks even\ninformation enabling him to form a belief on the preferences of others. Under\ntype ambiguity, one cannot analyze the scenario using the commonly used\nBayesian framework, and therefore he needs to model the participants using a\ndifferent decision model.\n  In this work, we present the ${\\rm MINthenMAX}$ decision model under\nambiguity. This model is a refinement of Wald's MiniMax principle, which we\nshow to be too coarse for games with type ambiguity. We characterize ${\\rm\nMINthenMAX}$ as the finest refinement of the MiniMax principle that satisfies\nthree properties we claim are necessary for games with type ambiguity. This\nprior-less approach we present her also follows the common practice in computer\nscience of worst-case analysis.\n  Finally, we define and analyze the corresponding equilibrium concept assuming\nall players follow ${\\rm MINthenMAX}$. We demonstrate this equilibrium by\napplying it to two common economic scenarios: coordination games and bilateral\ntrade. We show that in both scenarios, an equilibrium in pure strategies always\nexists and we analyze the equilibria.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 16:29:02 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 19:00:58 GMT"}, {"version": "v3", "created": "Thu, 3 Nov 2016 18:51:47 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Nehama", "Ilan", ""]]}, {"id": "1603.01570", "submitter": "Chainarong Amornbunchornvej", "authors": "Chainarong Amornbunchornvej, Ivan Brugere, Ariana Strandburg-Peshkin,\n  Damien Farine, Margaret C. Crofoot, Tanya Y. Berger-Wolf", "title": "Coordination Event Detection and Initiator Identification in Time Series\n  Data", "comments": "This is the final peer-reviewed version before publishing of the\n  previous pre-print entitled \"FLICA: A Framework for Leader Identification in\n  Coordinated Activity\"", "journal-ref": "ACM Transactions on Knowledge Discovery from Data (TKDD), 12(5),\n  53 (2018)", "doi": "10.1145/3201406", "report-no": null, "categories": "cs.SI cs.AI cs.MA econ.EM physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior initiation is a form of leadership and is an important aspect of\nsocial organization that affects the processes of group formation, dynamics,\nand decision-making in human societies and other social animal species. In this\nwork, we formalize the \"Coordination Initiator Inference Problem\" and propose a\nsimple yet powerful framework for extracting periods of coordinated activity\nand determining individuals who initiated this coordination, based solely on\nthe activity of individuals within a group during those periods. The proposed\napproach, given arbitrary individual time series, automatically (1) identifies\ntimes of coordinated group activity, (2) determines the identities of\ninitiators of those activities, and (3) classifies the likely mechanism by\nwhich the group coordination occurred, all of which are novel computational\ntasks. We demonstrate our framework on both simulated and real-world data:\ntrajectories tracking of animals as well as stock market data. Our method is\ncompetitive with existing global leadership inference methods but provides the\nfirst approaches for local leadership and coordination mechanism\nclassification. Our results are consistent with ground-truthed biological data\nand the framework finds many known events in financial data which are not\notherwise reflected in the aggregate NASDAQ index. Our method is easily\ngeneralizable to any coordinated time-series data from interacting entities.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 19:04:17 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 16:07:09 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Amornbunchornvej", "Chainarong", ""], ["Brugere", "Ivan", ""], ["Strandburg-Peshkin", "Ariana", ""], ["Farine", "Damien", ""], ["Crofoot", "Margaret C.", ""], ["Berger-Wolf", "Tanya Y.", ""]]}, {"id": "1603.01581", "submitter": "Philipp Geiger", "authors": "Philipp Geiger, Lucian Carata, Bernhard Schoelkopf", "title": "Causal inference for data-driven debugging and decision making in cloud\n  computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing involves complex technical and economical systems and\ninteractions. This brings about various challenges, two of which are: (1)\ndebugging and control to optimize the performance of computing systems, with\nthe help of sandbox experiments, and (2) privacy-preserving prediction of the\ncost of ``spot'' resources for decision making of cloud clients. In this paper,\nwe formalize debugging by counterfactual probabilities and control by\npost-(soft-)interventional probabilities. We prove that counterfactuals can\napproximately be calculated from a ``stochastic'' graphical causal model (while\nthey are originally defined only for ``deterministic'' functional causal\nmodels), and based on this sketch a data-driven approach to address problem\n(1). To address problem (2), we formalize bidding by post-(soft-)interventional\nprobabilities and present a simple mathematical result on approximate\nintegration of ``incomplete'' conditional probability distributions. We show\nhow this can be used by cloud clients to trade off privacy against\npredictability of the outcome of their bidding actions in a toy scenario. We\nreport experiments on simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 19:28:13 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 12:09:23 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 09:14:54 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 15:50:17 GMT"}, {"version": "v5", "created": "Thu, 18 Apr 2019 13:45:27 GMT"}, {"version": "v6", "created": "Mon, 10 Jun 2019 11:53:17 GMT"}, {"version": "v7", "created": "Sat, 25 Jan 2020 07:37:15 GMT"}, {"version": "v8", "created": "Tue, 10 Mar 2020 09:58:37 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Geiger", "Philipp", ""], ["Carata", "Lucian", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1603.01595", "submitter": "Hussam Hamdan", "authors": "Hussam Hamdan, Patrice Bellot, Frederic Bechet", "title": "Sentiment Analysis in Scholarly Book Reviews", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  So far different studies have tackled the sentiment analysis in several\ndomains such as restaurant and movie reviews. But, this problem has not been\nstudied in scholarly book reviews which is different in terms of review style\nand size. In this paper, we propose to combine different features in order to\nbe presented to a supervised classifiers which extract the opinion target\nexpressions and detect their polarities in scholarly book reviews. We construct\na labeled corpus for training and evaluating our methods in French book\nreviews. We also evaluate them on English restaurant reviews in order to\nmeasure their robustness across the domains and languages. The evaluation shows\nthat our methods are enough robust for English restaurant reviews and French\nbook reviews.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 20:04:31 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Hamdan", "Hussam", ""], ["Bellot", "Patrice", ""], ["Bechet", "Frederic", ""]]}, {"id": "1603.01722", "submitter": "Paolo Pareti Mr.", "authors": "Paolo Pareti, Ewan Klein, Adam Barker", "title": "A Linked Data Scalability Challenge: Concept Reuse Leads to Semantic\n  Decay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing amount of available Linked Data resources is laying the\nfoundations for more advanced Semantic Web applications. One of their main\nlimitations, however, remains the general low level of data quality. In this\npaper we focus on a measure of quality which is negatively affected by the\nincrease of the available resources. We propose a measure of semantic richness\nof Linked Data concepts and we demonstrate our hypothesis that the more a\nconcept is reused, the less semantically rich it becomes. This is a significant\nscalability issue, as one of the core aspects of Linked Data is the propagation\nof semantic information on the Web by reusing common terms. We prove our\nhypothesis with respect to our measure of semantic richness and we validate our\nmodel empirically. Finally, we suggest possible future directions to address\nthis scalability problem.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 12:50:22 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Pareti", "Paolo", ""], ["Klein", "Ewan", ""], ["Barker", "Adam", ""]]}, {"id": "1603.01770", "submitter": "Joseph Corneli", "authors": "Maximos Kaliakatsos-Papakostas, Roberto Confalonieri, Joseph Corneli,\n  Asterios Zacharakis and Emilios Cambouropoulos", "title": "An Argument-based Creative Assistant for Harmonic Blending", "comments": "8 pp; submitted to 7th International Conference on Computational\n  Creativity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conceptual blending is a powerful tool for computational creativity where,\nfor example, the properties of two harmonic spaces may be combined in a\nconsistent manner to produce a novel harmonic space. However, deciding about\nthe importance of property features in the input spaces and evaluating the\nresults of conceptual blending is a nontrivial task. In the specific case of\nmusical harmony, defining the salient features of chord transitions and\nevaluating invented harmonic spaces requires deep musicological background\nknowledge. In this paper, we propose a creative tool that helps musicologists\nto evaluate and to enhance harmonic innovation. This tool allows a music expert\nto specify arguments over given transition properties. These arguments are then\nconsidered by the system when defining combinations of features in an\nidiom-blending process. A music expert can assess whether the new harmonic\nidiom makes musicological sense and re-adjust the arguments (selection of\nfeatures) to explore alternative blends that can potentially produce better\nharmonic spaces. We conclude with a discussion of future work that would\nfurther automate the harmonisation process.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 00:06:09 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Kaliakatsos-Papakostas", "Maximos", ""], ["Confalonieri", "Roberto", ""], ["Corneli", "Joseph", ""], ["Zacharakis", "Asterios", ""], ["Cambouropoulos", "Emilios", ""]]}, {"id": "1603.01840", "submitter": "Gal Dalal", "authors": "Gal Dalal, Elad Gilboa, Shie Mannor", "title": "Hierarchical Decision Making In Electricity Grid Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power grid is a complex and vital system that necessitates careful\nreliability management. Managing the grid is a difficult problem with multiple\ntime scales of decision making and stochastic behavior due to renewable energy\ngenerations, variable demand and unplanned outages. Solving this problem in the\nface of uncertainty requires a new methodology with tractable algorithms. In\nthis work, we introduce a new model for hierarchical decision making in complex\nsystems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., a\nlevel of abstraction, for real-time power grid reliability. We devise an\nalgorithm that alternates between slow time-scale policy improvement, and fast\ntime-scale value function approximation. We compare our results to prevailing\nheuristics, and show the strength of our method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 16:30:34 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Dalal", "Gal", ""], ["Gilboa", "Elad", ""], ["Mannor", "Shie", ""]]}, {"id": "1603.01882", "submitter": "Robert Zinkov", "authors": "Robert Zinkov, Chung-chieh Shan", "title": "Composing inference algorithms as program transformations", "comments": "10 pages, 5 figures. To appear in Proceedings of the 33rd Conference\n  on Uncertainty in Artificial Intelligence (UAI2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic inference procedures are usually coded painstakingly from\nscratch, for each target model and each inference algorithm. We reduce this\neffort by generating inference procedures from models automatically. We make\nthis code generation modular by decomposing inference algorithms into reusable\nprogram-to-program transformations. These transformations perform exact\ninference as well as generate probabilistic programs that compute expectations,\ndensities, and MCMC samples. The resulting inference procedures are about as\naccurate and fast as other probabilistic programming systems on real-world\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 21:30:10 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 16:01:42 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Zinkov", "Robert", ""], ["Shan", "Chung-chieh", ""]]}, {"id": "1603.02028", "submitter": "Sylvain Chevallier", "authors": "Hugo Martin, Sylvain Chevallier and Eric Monacelli", "title": "Adaptive Visualisation System for Construction Building Information\n  Models Using Saliency", "comments": "10 pages, 5 figures, to be submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building Information Modeling (BIM) is a recent construction process based on\na 3D model, containing every component related to the building achievement.\nArchitects, structure engineers, method engineers, and others participant to\nthe building process work on this model through the design-to-construction\ncycle. The high complexity and the large amount of information included in\nthese models raise several issues, delaying its wide adoption in the industrial\nworld. One of the most important is the visualization: professionals have\ndifficulties to find out the relevant information for their job. Actual\nsolutions suffer from two limitations: the BIM models information are processed\nmanually and insignificant information are simply hidden, leading to\ninconsistencies in the building model. This paper describes a system relying on\nan ontological representation of the building information to label\nautomatically the building elements. Depending on the user's department, the\nvisualization is modified according to these labels by automatically adjusting\nthe colors and image properties based on a saliency model. The proposed\nsaliency model incorporates several adaptations to fit the specificities of\narchitectural images.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 12:25:33 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Martin", "Hugo", ""], ["Chevallier", "Sylvain", ""], ["Monacelli", "Eric", ""]]}, {"id": "1603.02038", "submitter": "Ruben Martinez-Cantin", "authors": "Jos\\'e Nogueira, Ruben Martinez-Cantin, Alexandre Bernardino and\n  Lorenzo Jamone", "title": "Unscented Bayesian Optimization for Safe Robot Grasping", "comments": "conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the robot grasp optimization problem of unknown objects\nconsidering uncertainty in the input space. Grasping unknown objects can be\nachieved by using a trial and error exploration strategy. Bayesian optimization\nis a sample efficient optimization algorithm that is especially suitable for\nthis setups as it actively reduces the number of trials for learning about the\nfunction to optimize. In fact, this active object exploration is the same\nstrategy that infants do to learn optimal grasps. One problem that arises while\nlearning grasping policies is that some configurations of grasp parameters may\nbe very sensitive to error in the relative pose between the object and robot\nend-effector. We call these configurations unsafe because small errors during\ngrasp execution may turn good grasps into bad grasps. Therefore, to reduce the\nrisk of grasp failure, grasps should be planned in safe areas. We propose a new\nalgorithm, Unscented Bayesian optimization that is able to perform sample\nefficient optimization while taking into consideration input noise to find safe\noptima. The contribution of Unscented Bayesian optimization is twofold as if\nprovides a new decision process that drives exploration to safe regions and a\nnew selection procedure that chooses the optimal in terms of its safety without\nextra analysis or computational cost. Both contributions are rooted on the\nstrong theory behind the unscented transformation, a popular nonlinear\napproximation method. We show its advantages with respect to the classical\nBayesian optimization both in synthetic problems and in realistic robot grasp\nsimulations. The results highlights that our method achieves optimal and robust\ngrasping policies after few trials while the selected grasps remain in safe\nregions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 12:51:43 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Nogueira", "Jos\u00e9", ""], ["Martinez-Cantin", "Ruben", ""], ["Bernardino", "Alexandre", ""], ["Jamone", "Lorenzo", ""]]}, {"id": "1603.02041", "submitter": "Diana Borsa", "authors": "Diana Borsa and Thore Graepel and John Shawe-Taylor", "title": "Learning Shared Representations in Multi-task Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a paradigm in multi-task reinforcement learning (MT-RL) in\nwhich an agent is placed in an environment and needs to learn to perform a\nseries of tasks, within this space. Since the environment does not change,\nthere is potentially a lot of common ground amongst tasks and learning to solve\nthem individually seems extremely wasteful. In this paper, we explicitly model\nand learn this shared structure as it arises in the state-action value space.\nWe will show how one can jointly learn optimal value-functions by modifying the\npopular Value-Iteration and Policy-Iteration procedures to accommodate this\nshared representation assumption and leverage the power of multi-task\nsupervised learning. Finally, we demonstrate that the proposed model and\ntraining procedures, are able to infer good value functions, even under low\nsamples regimes. In addition to data efficiency, we will show in our analysis,\nthat learning abstractions of the state space jointly across tasks leads to\nmore robust, transferable representations with the potential for better\ngeneralization. this shared representation assumption and leverage the power of\nmulti-task supervised learning. Finally, we demonstrate that the proposed model\nand training procedures, are able to infer good value functions, even under low\nsamples regimes. In addition to data efficiency, we will show in our analysis,\nthat learning abstractions of the state space jointly across tasks leads to\nmore robust, transferable representations with the potential for better\ngeneralization.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 13:03:30 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Borsa", "Diana", ""], ["Graepel", "Thore", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1603.02199", "submitter": "Sergey Levine", "authors": "Sergey Levine, Peter Pastor, Alex Krizhevsky, Deirdre Quillen", "title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning\n  and Large-Scale Data Collection", "comments": "This is an extended version of \"Learning Hand-Eye Coordination for\n  Robotic Grasping with Large-Scale Data Collection,\" ISER 2016. Draft modified\n  to correct typo in Algorithm 1 and add a link to the publicly available\n  dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a learning-based approach to hand-eye coordination for robotic\ngrasping from monocular images. To learn hand-eye coordination for grasping, we\ntrained a large convolutional neural network to predict the probability that\ntask-space motion of the gripper will result in successful grasps, using only\nmonocular camera images and independently of camera calibration or the current\nrobot pose. This requires the network to observe the spatial relationship\nbetween the gripper and objects in the scene, thus learning hand-eye\ncoordination. We then use this network to servo the gripper in real time to\nachieve successful grasps. To train our network, we collected over 800,000\ngrasp attempts over the course of two months, using between 6 and 14 robotic\nmanipulators at any given time, with differences in camera placement and\nhardware. Our experimental evaluation demonstrates that our method achieves\neffective real-time control, can successfully grasp novel objects, and corrects\nmistakes by continuous servoing.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 18:53:00 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 23:01:46 GMT"}, {"version": "v3", "created": "Sat, 2 Apr 2016 23:50:24 GMT"}, {"version": "v4", "created": "Sun, 28 Aug 2016 23:32:37 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Levine", "Sergey", ""], ["Pastor", "Peter", ""], ["Krizhevsky", "Alex", ""], ["Quillen", "Deirdre", ""]]}, {"id": "1603.02208", "submitter": "Wen Shen", "authors": "Wen Shen, Cristina V. Lopes and Jacob W. Crandall", "title": "An Online Mechanism for Ridesharing in Autonomous Mobility-on-Demand\n  Systems", "comments": null, "journal-ref": "Proceedings of the 25th International Joint Conference on\n  Artificial Intelligence (IJCAI 2016) pp. 475-481", "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With proper management, Autonomous Mobility-on-Demand (AMoD) systems have\ngreat potential to satisfy the transport demands of urban populations by\nproviding safe, convenient, and affordable ridesharing services. Meanwhile,\nsuch systems can substantially decrease private car ownership and use, and thus\nsignificantly reduce traffic congestion, energy consumption, and carbon\nemissions. To achieve this objective, an AMoD system requires private\ninformation about the demand from passengers. However, due to\nself-interestedness, passengers are unlikely to cooperate with the service\nproviders in this regard. Therefore, an online mechanism is desirable if it\nincentivizes passengers to truthfully report their actual demand. For the\npurpose of promoting ridesharing, we hereby introduce a posted-price,\nintegrated online ridesharing mechanism (IORS) that satisfies desirable\nproperties such as ex-post incentive compatibility, individual rationality, and\nbudget-balance. Numerical results indicate the competitiveness of IORS compared\nwith two benchmarks, namely the optimal assignment and an offline,\nauction-based mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 19:10:46 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 20:37:03 GMT"}, {"version": "v3", "created": "Thu, 2 Mar 2017 01:18:26 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Shen", "Wen", ""], ["Lopes", "Cristina V.", ""], ["Crandall", "Jacob W.", ""]]}, {"id": "1603.02626", "submitter": "Nicolas Gillis", "authors": "Olivier Sobrie and Nicolas Gillis and Vincent Mousseau and Marc Pirlot", "title": "UTA-poly and UTA-splines: additive value functions with polynomial\n  marginals", "comments": "30 pages, 16 figures, 4 tables. No major changes since the first\n  version (few typos, adding references, discussions)", "journal-ref": "European Journal of Operational Research 264, pp. 405-418, 2018", "doi": "10.1016/j.ejor.2017.03.021", "report-no": null, "categories": "math.OC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive utility function models are widely used in multiple criteria\ndecision analysis. In such models, a numerical value is associated to each\nalternative involved in the decision problem. It is computed by aggregating the\nscores of the alternative on the different criteria of the decision problem.\nThe score of an alternative is determined by a marginal value function that\nevolves monotonically as a function of the performance of the alternative on\nthis criterion. Determining the shape of the marginals is not easy for a\ndecision maker. It is easier for him/her to make statements such as\n\"alternative $a$ is preferred to $b$\". In order to help the decision maker, UTA\ndisaggregation procedures use linear programming to approximate the marginals\nby piecewise linear functions based only on such statements. In this paper, we\npropose to infer polynomials and splines instead of piecewise linear functions\nfor the marginals. In this aim, we use semidefinite programming instead of\nlinear programming. We illustrate this new elicitation method and present some\nexperimental results.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 17:49:03 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 13:23:07 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Sobrie", "Olivier", ""], ["Gillis", "Nicolas", ""], ["Mousseau", "Vincent", ""], ["Pirlot", "Marc", ""]]}, {"id": "1603.02738", "submitter": "Matthew Guzdial", "authors": "Matthew Guzdial and Mark Riedl", "title": "Learning to Blend Computer Game Levels", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to generate novel computer game levels that blend\ndifferent game concepts in an unsupervised fashion. Our primary contribution is\nan analogical reasoning process to construct blends between level design models\nlearned from gameplay videos. The models represent probabilistic relationships\nbetween elements in the game. An analogical reasoning process maps features\nbetween two models to produce blended models that can then generate new level\nchunks. As a proof-of-concept we train our system on the classic platformer\ngame Super Mario Bros. due to its highly-regarded and well understood level\ndesign. We evaluate the extent to which the models represent stylistic level\ndesign knowledge and demonstrate the ability of our system to explain levels\nthat were blended by human expert designers.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 23:19:50 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Guzdial", "Matthew", ""], ["Riedl", "Mark", ""]]}, {"id": "1603.02740", "submitter": "Johan Ugander", "authors": "Stephen Ragain, Johan Ugander", "title": "Pairwise Choice Markov Chains", "comments": "Advances in Neural Information Processing Systems (NIPS) 29, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As datasets capturing human choices grow in richness and scale --\nparticularly in online domains -- there is an increasing need for choice models\nthat escape traditional choice-theoretic axioms such as regularity, stochastic\ntransitivity, and Luce's choice axiom. In this work we introduce the Pairwise\nChoice Markov Chain (PCMC) model of discrete choice, an inferentially tractable\nmodel that does not assume any of the above axioms while still satisfying the\nfoundational axiom of uniform expansion, a considerably weaker assumption than\nLuce's choice axiom. We show that the PCMC model significantly outperforms the\nMultinomial Logit (MNL) model in prediction tasks on both synthetic and\nempirical datasets known to exhibit violations of Luce's axiom. Our analysis\nalso synthesizes several recent observations connecting the Multinomial Logit\nmodel and Markov chains; the PCMC model retains the Multinomial Logit model as\na special case.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 23:47:03 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 06:54:59 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 18:38:48 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 05:00:26 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ragain", "Stephen", ""], ["Ugander", "Johan", ""]]}, {"id": "1603.02776", "submitter": "Yang Liu", "authors": "Yang Liu, Sujian Li, Xiaodong Zhang and Zhifang Sui", "title": "Implicit Discourse Relation Classification via Multi-Task Neural\n  Networks", "comments": "This is the pre-print version of a paper accepted by AAAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without discourse connectives, classifying implicit discourse relations is a\nchallenging task and a bottleneck for building a practical discourse parser.\nPrevious research usually makes use of one kind of discourse framework such as\nPDTB or RST to improve the classification performance on discourse relations.\nActually, under different discourse annotation frameworks, there exist multiple\ncorpora which have internal connections. To exploit the combination of\ndifferent discourse corpora, we design related discourse classification tasks\nspecific to a corpus, and propose a novel Convolutional Neural Network embedded\nmulti-task learning system to synthesize these tasks by learning both unique\nand shared representations for each task. The experimental results on the PDTB\nimplicit discourse relation classification task demonstrate that our model\nachieves significant gains over baseline systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 03:13:37 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Liu", "Yang", ""], ["Li", "Sujian", ""], ["Zhang", "Xiaodong", ""], ["Sui", "Zhifang", ""]]}, {"id": "1603.03007", "submitter": "Alexander Tchitchigin", "authors": "Alexander Tchitchigin, Max Talanov, Larisa Safina, Manuel Mazzara", "title": "Robot Dream", "comments": "keywords: robotics, spiking neural networks, artificial emotions,\n  affective computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this position paper we present a novel approach to neurobiologically\nplausible implementation of emotional reactions and behaviors for real-time\nautonomous robotic systems. The working metaphor we use is the \"day\" and\n\"night\" phases of mammalian life. During the \"day\" phase a robotic system\nstores the inbound information and is controlled by a light-weight rule-based\nsystem in real time. In contrast to that, during the \"night\" phase the stored\ninformation is been transferred to the supercomputing system to update the\nrealistic neural network: emotional and behavioral strategies.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 19:14:27 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Tchitchigin", "Alexander", ""], ["Talanov", "Max", ""], ["Safina", "Larisa", ""], ["Mazzara", "Manuel", ""]]}, {"id": "1603.03112", "submitter": "Lifu Huang", "authors": "Lifu Huang, Jonathan May, Xiaoman Pan, Heng Ji", "title": "Building a Fine-Grained Entity Typing System Overnight for a New X (X =\n  Language, Domain, Genre)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent research has shown great progress on fine-grained entity typing. Most\nexisting methods require pre-defining a set of types and training a multi-class\nclassifier from a large labeled data set based on multi-level linguistic\nfeatures. They are thus limited to certain domains, genres and languages. In\nthis paper, we propose a novel unsupervised entity typing framework by\ncombining symbolic and distributional semantics. We start from learning general\nembeddings for each entity mention, compose the embeddings of specific contexts\nusing linguistic structures, link the mention to knowledge bases and learn its\nrelated knowledge representations. Then we develop a novel joint hierarchical\nclustering and linking algorithm to type all mentions using these\nrepresentations. This framework doesn't rely on any annotated data, predefined\ntyping schema, or hand-crafted features, therefore it can be quickly adapted to\na new domain, genre and language. Furthermore, it has great flexibility at\nincorporating linguistic structures (e.g., Abstract Meaning Representation\n(AMR), dependency relations) to improve specific context representation.\nExperiments on genres (news and discussion forum) show comparable performance\nwith state-of-the-art supervised typing systems trained from a large amount of\nlabeled data. Results on various languages (English, Chinese, Japanese, Hausa,\nand Yoruba) and domains (general and biomedical) demonstrate the portability of\nour framework.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 00:33:28 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Huang", "Lifu", ""], ["May", "Jonathan", ""], ["Pan", "Xiaoman", ""], ["Ji", "Heng", ""]]}, {"id": "1603.03181", "submitter": "Nabil Hossain", "authors": "Nabil Hossain, Tianran Hu, Roghayeh Feizi, Ann Marie White, Jiebo Luo\n  and Henry Kautz", "title": "Inferring Fine-grained Details on User Activities and Home Location from\n  Social Media: Detecting Drinking-While-Tweeting Patterns in Communities", "comments": "12 pages, 7 figures, 4-page poster version accepted at ICWSM 2016,\n  alcohol dataset and keywords available in:\n  cs.rochester.edu/u/nhossain/icwsm-16-data.zip", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearly all previous work on geo-locating latent states and activities from\nsocial media confounds general discussions about activities, self-reports of\nusers participating in those activities at times in the past or future, and\nself-reports made at the immediate time and place the activity occurs.\nActivities, such as alcohol consumption, may occur at different places and\ntypes of places, and it is important not only to detect the local regions where\nthese activities occur, but also to analyze the degree of participation in them\nby local residents. In this paper, we develop new machine learning based\nmethods for fine-grained localization of activities and home locations from\nTwitter data. We apply these methods to discover and compare alcohol\nconsumption patterns in a large urban area, New York City, and a more suburban\nand rural area, Monroe County. We find positive correlations between the rate\nof alcohol consumption reported among a community's Twitter users and the\ndensity of alcohol outlets, demonstrating that the degree of correlation varies\nsignificantly between urban and suburban areas. While our experiments are\nfocused on alcohol use, our methods for locating homes and distinguishing\ntemporally-specific self-reports are applicable to a broad range of behaviors\nand latent states.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 08:32:34 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Hossain", "Nabil", ""], ["Hu", "Tianran", ""], ["Feizi", "Roghayeh", ""], ["White", "Ann Marie", ""], ["Luo", "Jiebo", ""], ["Kautz", "Henry", ""]]}, {"id": "1603.03251", "submitter": "Tayeb Lemlouma", "authors": "Zaineb Liouane (ENIM - MONASTIR), Tayeb Lemlouma (IRISA-D2), Philippe\n  Roose, Fr\\'ed\\'eric Weis (TACOMA), Messaoud Hassani (ENIM - MONASTIR)", "title": "A Markovian-based Approach for Daily Living Activities Recognition", "comments": "The International Conference on Sensor Networks (SENSORNETS'16), Feb\n  2016, rome, Italy. Proceedings of theInternational Conference on Sensor\n  Networks (SENSORNETS 2016), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing the activities of daily living plays an important role in\nhealthcare. It is necessary to use an adapted model to simulate the human\nbehavior in a domestic space to monitor the patient harmonically and to\nintervene in the necessary time. In this paper, we tackle this problem using\nthe hierarchical hidden Markov model for representing and recognizing complex\nindoor activities. We propose a new grammar, called \"Home By Room Activities\nLanguage\", to facilitate the complexity of human scenarios and consider the\nabnormal activities.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 13:09:57 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Liouane", "Zaineb", "", "ENIM - MONASTIR"], ["Lemlouma", "Tayeb", "", "IRISA-D2"], ["Roose", "Philippe", "", "TACOMA"], ["Weis", "Fr\u00e9d\u00e9ric", "", "TACOMA"], ["Hassani", "Messaoud", "", "ENIM - MONASTIR"]]}, {"id": "1603.03267", "submitter": "Vicen\\c{c} G\\'omez Cerd\\`a", "authors": "Anders Jonsson, Vicen\\c{c} G\\'omez", "title": "Hierarchical Linearly-Solvable Markov Decision Problems", "comments": "11 pages, 6 figures, 26th International Conference on Automated\n  Planning and Scheduling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hierarchical reinforcement learning framework that formulates\neach task in the hierarchy as a special type of Markov decision process for\nwhich the Bellman equation is linear and has analytical solution. Problems of\nthis type, called linearly-solvable MDPs (LMDPs) have interesting properties\nthat can be exploited in a hierarchical setting, such as efficient learning of\nthe optimal value function or task compositionality. The proposed hierarchical\napproach can also be seen as a novel alternative to solving LMDPs with large\nstate spaces. We derive a hierarchical version of the so-called Z-learning\nalgorithm that learns different tasks simultaneously and show empirically that\nit significantly outperforms the state-of-the-art learning methods in two\nclassical hierarchical reinforcement learning domains: the taxi domain and an\nautonomous guided vehicle task.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 13:50:31 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Jonsson", "Anders", ""], ["G\u00f3mez", "Vicen\u00e7", ""]]}, {"id": "1603.03491", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried and Qingyun Sun", "title": "Bayesian Opponent Exploitation in Imperfect-Information Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two fundamental problems in computational game theory are computing a Nash\nequilibrium and learning to exploit opponents given observations of their play\n(opponent exploitation). The latter is perhaps even more important than the\nformer: Nash equilibrium does not have a compelling theoretical justification\nin game classes other than two-player zero-sum, and for all games one can\npotentially do better by exploiting perceived weaknesses of the opponent than\nby following a static equilibrium strategy throughout the match. The natural\nsetting for opponent exploitation is the Bayesian setting where we have a prior\nmodel that is integrated with observations to create a posterior opponent model\nthat we respond to. The most natural, and a well-studied prior distribution is\nthe Dirichlet distribution. An exact polynomial-time algorithm is known for\nbest-responding to the posterior distribution for an opponent assuming a\nDirichlet prior with multinomial sampling in normal-form games; however, for\nimperfect-information games the best known algorithm is based on approximating\nan infinite integral without theoretical guarantees. We present the first exact\nalgorithm for a natural class of imperfect-information games. We demonstrate\nthat our algorithm runs quickly in practice and outperforms the best prior\napproaches. We also present an algorithm for the uniform prior setting.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 23:50:51 GMT"}, {"version": "v2", "created": "Sat, 17 Sep 2016 19:35:22 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 06:23:30 GMT"}, {"version": "v4", "created": "Mon, 13 Feb 2017 22:04:34 GMT"}, {"version": "v5", "created": "Wed, 27 Jun 2018 02:35:11 GMT"}, {"version": "v6", "created": "Thu, 28 Jun 2018 00:55:09 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Ganzfried", "Sam", ""], ["Sun", "Qingyun", ""]]}, {"id": "1603.03511", "submitter": "Yi Zhou Dr.", "authors": "Yi Zhou", "title": "A Set Theoretic Approach for Knowledge Representation: the\n  Representation Part", "comments": "This paper targets an ambitious goal to rebuild a foundation of\n  knowledge representation based on set theory rather than classical logic. Any\n  comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a set theoretic approach for knowledge\nrepresentation. While the syntax of an application domain is captured by set\ntheoretic constructs including individuals, concepts and operators, knowledge\nis formalized by equality assertions. We first present a primitive form that\nuses minimal assumed knowledge and constructs. Then, assuming naive set theory,\nwe extend it by definitions, which are special kinds of knowledge.\nInterestingly, we show that the primitive form is expressive enough to define\nlogic operators, not only propositional connectives but also quantifiers.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 03:22:12 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Zhou", "Yi", ""]]}, {"id": "1603.03515", "submitter": "Lin Chen", "authors": "Lin Chen, Hamed Hassani, Amin Karbasi", "title": "Near-Optimal Active Learning of Halfspaces via Query Synthesis in the\n  Noisy Setting", "comments": "Accepted by AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of actively learning a linear\nclassifier through query synthesis where the learner can construct artificial\nqueries in order to estimate the true decision boundaries. This problem has\nrecently gained a lot of interest in automated science and adversarial reverse\nengineering for which only heuristic algorithms are known. In such\napplications, queries can be constructed de novo to elicit information (e.g.,\nautomated science) or to evade detection with minimal cost (e.g., adversarial\nreverse engineering). We develop a general framework, called dimension coupling\n(DC), that 1) reduces a d-dimensional learning problem to d-1 low dimensional\nsub-problems, 2) solves each sub-problem efficiently, 3) appropriately\naggregates the results and outputs a linear classifier, and 4) provides a\ntheoretical guarantee for all possible schemes of aggregation. The proposed\nmethod is proved resilient to noise. We show that the DC framework avoids the\ncurse of dimensionality: its computational complexity scales linearly with the\ndimension. Moreover, we show that the query complexity of DC is near optimal\n(within a constant factor of the optimum algorithm). To further support our\ntheoretical analysis, we compare the performance of DC with the existing work.\nWe observe that DC consistently outperforms the prior arts in terms of query\ncomplexity while often running orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 04:18:48 GMT"}, {"version": "v2", "created": "Sat, 12 Nov 2016 17:39:47 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Chen", "Lin", ""], ["Hassani", "Hamed", ""], ["Karbasi", "Amin", ""]]}, {"id": "1603.03518", "submitter": "Peng Yang", "authors": "Peng Yang, Ke Tang, Xin Yao", "title": "High-dimensional Black-box Optimization via Divide and Approximate\n  Conquer", "comments": "7 pages, 2 figures, conference", "journal-ref": "IEEE Transactions on Evolutionary Computation, 2018, 22(1):\n  143-156", "doi": "10.1109/TEVC.2017.2672689", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divide and Conquer (DC) is conceptually well suited to high-dimensional\noptimization by decomposing a problem into multiple small-scale sub-problems.\nHowever, appealing performance can be seldom observed when the sub-problems are\ninterdependent. This paper suggests that the major difficulty of tackling\ninterdependent sub-problems lies in the precise evaluation of a partial\nsolution (to a sub-problem), which can be overwhelmingly costly and thus makes\nsub-problems non-trivial to conquer. Thus, we propose an approximation\napproach, named Divide and Approximate Conquer (DAC), which reduces the cost of\npartial solution evaluation from exponential time to polynomial time.\nMeanwhile, the convergence to the global optimum (of the original problem) is\nstill guaranteed. The effectiveness of DAC is demonstrated empirically on two\nsets of non-separable high-dimensional problems.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 04:50:59 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 02:06:09 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Yang", "Peng", ""], ["Tang", "Ke", ""], ["Yao", "Xin", ""]]}, {"id": "1603.03729", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Penta and Hexa Valued Representation of Neutrosophic Information", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.1.2667.1762", "report-no": "IT.1.3.2016", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from the primary representation of neutrosophic information, namely\nthe degree of truth, degree of indeterminacy and degree of falsity, we define a\nnuanced representation in a penta valued fuzzy space, described by the index of\ntruth, index of falsity, index of ignorance, index of contradiction and index\nof hesitation. Also, it was constructed an associated penta valued logic and\nthen using this logic, it was defined for the proposed penta valued structure\nthe following operators: union, intersection, negation, complement and dual.\nThen, the penta valued representation is extended to a hexa valued one, adding\nthe sixth component, namely the index of ambiguity.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 04:18:38 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1603.03795", "submitter": "Vanessa Volz", "authors": "Vanessa Volz, G\\\"unter Rudolph, Boris Naujoks", "title": "Demonstrating the Feasibility of Automatic Game Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game balancing is an important part of the (computer) game design process, in\nwhich designers adapt a game prototype so that the resulting gameplay is as\nentertaining as possible. In industry, the evaluation of a game is often based\non costly playtests with human players. It suggests itself to automate this\nprocess using surrogate models for the prediction of gameplay and outcome. In\nthis paper, the feasibility of automatic balancing using simulation- and\ndeck-based objectives is investigated for the card game top trumps.\nAdditionally, the necessity of a multi-objective approach is asserted by a\ncomparison with the only known (single-objective) method. We apply a\nmulti-objective evolutionary algorithm to obtain decks that optimise\nobjectives, e.g. win rate and average number of tricks, developed to express\nthe fairness and the excitement of a game of top trumps. The results are\ncompared with decks from published top trumps decks using simulation-based\nobjectives. The possibility to generate decks better or at least as good as\ndecks from published top trumps decks in terms of these objectives is\ndemonstrated. Our results indicate that automatic balancing with the presented\napproach is feasible even for more complex games such as real-time strategy\ngames.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 21:36:27 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Volz", "Vanessa", ""], ["Rudolph", "G\u00fcnter", ""], ["Naujoks", "Boris", ""]]}, {"id": "1603.03814", "submitter": "Mohamed El Halaby", "authors": "Mohamed El Halaby", "title": "Solving MaxSAT by Successive Calls to a SAT Solver", "comments": "Survey, 46 pages", "journal-ref": null, "doi": "10.1007/978-3-319-56994-9_31", "report-no": null, "categories": "cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Maximum Satisfiability (MaxSAT) problem is the problem of finding a truth\nassignment that maximizes the number of satisfied clauses of a given Boolean\nformula in Conjunctive Normal Form (CNF). Many exact solvers for MaxSAT have\nbeen developed during recent years, and many of them were presented in the\nwell-known SAT conference. Algorithms for MaxSAT generally fall into two\ncategories: (1) branch and bound algorithms and (2) algorithms that use\nsuccessive calls to a SAT solver (SAT- based), which this paper in on. In\npractical problems, SAT-based algorithms have been shown to be more efficient.\nThis paper provides an experimental investigation to compare the performance of\nrecent SAT-based and branch and bound algorithms on the benchmarks of the\nMaxSAT Evaluations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 22:54:28 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Halaby", "Mohamed El", ""]]}, {"id": "1603.03827", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt", "title": "Sequential Short-Text Classification with Recurrent and Convolutional\n  Neural Networks", "comments": "Accepted as a conference paper at NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for short-text classification. However, many short texts\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\nand most existing ANN-based systems do not leverage the preceding short texts\nwhen classifying a subsequent one. In this work, we present a model based on\nrecurrent neural networks and convolutional neural networks that incorporates\nthe preceding short texts. Our model achieves state-of-the-art results on three\ndifferent datasets for dialog act prediction.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 00:02:51 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""]]}, {"id": "1603.03833", "submitter": "Rouhollah Rahmatizadeh", "authors": "Rouhollah Rahmatizadeh, Pooya Abolghasemi, Aman Behal, Ladislau\n  B\\\"ol\\\"oni", "title": "From virtual demonstration to real-world manipulation using LSTM and MDN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots assisting the disabled or elderly must perform complex manipulation\ntasks and must adapt to the home environment and preferences of their user.\nLearning from demonstration is a promising choice, that would allow the\nnon-technical user to teach the robot different tasks. However, collecting\ndemonstrations in the home environment of a disabled user is time consuming,\ndisruptive to the comfort of the user, and presents safety challenges. It would\nbe desirable to perform the demonstrations in a virtual environment. In this\npaper we describe a solution to the challenging problem of behavior transfer\nfrom virtual demonstration to a physical robot. The virtual demonstrations are\nused to train a deep neural network based controller, which is using a Long\nShort Term Memory (LSTM) recurrent neural network to generate trajectories. The\ntraining process uses a Mixture Density Network (MDN) to calculate an error\nsignal suitable for the multimodal nature of demonstrations. The controller\nlearned in the virtual environment is transferred to a physical robot (a\nRethink Robotics Baxter). An off-the-shelf vision component is used to\nsubstitute for geometric knowledge available in the simulation and an inverse\nkinematics module is used to allow the Baxter to enact the trajectory. Our\nexperimental studies validate the three contributions of the paper: (1) the\ncontroller learned from virtual demonstrations can be used to successfully\nperform the manipulation tasks on a physical robot, (2) the LSTM+MDN\narchitectural choice outperforms other choices, such as the use of feedforward\nnetworks and mean-squared error based training signals and (3) allowing\nimperfect demonstrations in the training set also allows the controller to\nlearn how to correct its manipulation mistakes.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 00:47:38 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 23:56:19 GMT"}, {"version": "v3", "created": "Wed, 15 Nov 2017 20:31:07 GMT"}, {"version": "v4", "created": "Wed, 22 Nov 2017 02:44:36 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Rahmatizadeh", "Rouhollah", ""], ["Abolghasemi", "Pooya", ""], ["Behal", "Aman", ""], ["B\u00f6l\u00f6ni", "Ladislau", ""]]}, {"id": "1603.03884", "submitter": "Torsten Schaub", "authors": "Martin Gebser and Roland Kaminski and Torsten Schaub", "title": "Grounding Recursive Aggregates: Preliminary Report", "comments": "21 pages, 7 figures, preliminary version appeared at GTTV'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problem solving in Answer Set Programming consists of two steps, a first\ngrounding phase, systematically replacing all variables by terms, and a second\nsolving phase computing the stable models of the obtained ground program. An\nintricate part of both phases is the treatment of aggregates, which are popular\nlanguage constructs that allow for expressing properties over sets. In this\npaper, we elaborate upon the treatment of aggregates during grounding in Gringo\nseries 4. Consequently, our approach is applicable to grounding based on\nsemi-naive database evaluation techniques. In particular, we provide a series\nof algorithms detailing the treatment of recursive aggregates and illustrate\nthis by a running example.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 10:22:13 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Gebser", "Martin", ""], ["Kaminski", "Roland", ""], ["Schaub", "Torsten", ""]]}, {"id": "1603.03980", "submitter": "Ravi Ganti", "authors": "Nikhil Rao, Ravi Ganti, Laura Balzano, Rebecca Willett, Robert Nowak", "title": "On Learning High Dimensional Structured Single Index Models", "comments": "7 pages, 3 tables, 1 Figure, substantial text overlap with\n  arXiv:1506.08910; Accepted for publication at AAAI 2017; added new\n  experimental results comparing our method to a single layer neural network", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Index Models (SIMs) are simple yet flexible semi-parametric models for\nmachine learning, where the response variable is modeled as a monotonic\nfunction of a linear combination of features. Estimation in this context\nrequires learning both the feature weights and the nonlinear function that\nrelates features to observations. While methods have been described to learn\nSIMs in the low dimensional regime, a method that can efficiently learn SIMs in\nhigh dimensions, and under general structural assumptions, has not been\nforthcoming. In this paper, we propose computationally efficient algorithms for\nSIM inference in high dimensions with structural constraints. Our general\napproach specializes to sparsity, group sparsity, and low-rank assumptions\namong others. Experiments show that the proposed method enjoys superior\npredictive performance when compared to generalized linear models, and achieves\nresults comparable to or better than single layer feedforward neural networks\nwith significantly less computational cost.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 01:53:40 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 22:55:39 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Rao", "Nikhil", ""], ["Ganti", "Ravi", ""], ["Balzano", "Laura", ""], ["Willett", "Rebecca", ""], ["Nowak", "Robert", ""]]}, {"id": "1603.04068", "submitter": "Benjamin McCamish", "authors": "Ben McCamish, Vahid Ghadakchi, Arash Termehchy and Behrouz Touri", "title": "A Signaling Game Approach to Databases Querying and Interaction", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As most users do not precisely know the structure and/or the content of\ndatabases, their queries do not exactly reflect their information needs. The\ndatabase management systems (DBMS) may interact with users and use their\nfeedback on the returned results to learn the information needs behind their\nqueries. Current query interfaces assume that users do not learn and modify the\nway way they express their information needs in form of queries during their\ninteraction with the DBMS. Using a real-world interaction workload, we show\nthat users learn and modify how to express their information needs during their\ninteractions with the DBMS and their learning is accurately modeled by a\nwell-known reinforcement learning mechanism. As current data interaction\nsystems assume that users do not modify their strategies, they cannot discover\nthe information needs behind users' queries effectively. We model the\ninteraction between users and DBMS as a game with identical interest between\ntwo rational agents whose goal is to establish a common language for\nrepresenting information needs in form of queries. We propose a reinforcement\nlearning method that learns and answers the information needs behind queries\nand adapts to the changes in users' strategies and prove that it improves the\neffectiveness of answering queries stochastically speaking. We propose two\nefficient implementation of this method over large relational databases. Our\nextensive empirical studies over real-world query workloads indicate that our\nalgorithms are efficient and effective.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 19:28:22 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 14:18:52 GMT"}, {"version": "v3", "created": "Sat, 27 May 2017 22:04:56 GMT"}, {"version": "v4", "created": "Thu, 22 Jun 2017 17:04:16 GMT"}, {"version": "v5", "created": "Fri, 4 May 2018 21:33:26 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["McCamish", "Ben", ""], ["Ghadakchi", "Vahid", ""], ["Termehchy", "Arash", ""], ["Touri", "Behrouz", ""]]}, {"id": "1603.04110", "submitter": "Seyed Morteza Mousavi Barroudi", "authors": "Seyed Morteza Mousavi, Aaron Harwood, Shanika Karunasekera, Mojtaba\n  Maghrebi", "title": "Geometry of Interest (GOI): Spatio-Temporal Destination Extraction and\n  Partitioning in GPS Trajectory Data", "comments": "A version of this technical report has been submitted to the Springer\n  Journal of Ambient Intelligence and Humanized Computing and it is under\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays large amounts of GPS trajectory data is being continuously collected\nby GPS-enabled devices such as vehicles navigation systems and mobile phones.\nGPS trajectory data is useful for applications such as traffic management,\nlocation forecasting, and itinerary planning. Such applications often need to\nextract the time-stamped Sequence of Visited Locations (SVLs) of the mobile\nobjects. The nearest neighbor query (NNQ) is the most applied method for\nlabeling the visited locations based on the IDs of the POIs in the process of\nSVL generation. NNQ in some scenarios is not accurate enough. To improve the\nquality of the extracted SVLs, instead of using NNQ, we label the visited\nlocations as the IDs of the POIs which geometrically intersect with the GPS\nobservations. Intersection operator requires the accurate geometry of the\npoints of interest which we refer to them as the Geometries of Interest (GOIs).\nIn some application domains (e.g. movement trajectories of animals), adequate\ninformation about the POIs and their GOIs may not be available a priori, or\nthey may not be publicly accessible and, therefore, they need to be derived\nfrom GPS trajectory data. In this paper we propose a novel method for\nestimating the POIs and their GOIs, which consists of three phases: (i)\nextracting the geometries of the stay regions; (ii) constructing the geometry\nof destination regions based on the extracted stay regions; and (iii)\nconstructing the GOIs based on the geometries of the destination regions. Using\nthe geometric similarity to known GOIs as the major evaluation criterion, the\nexperiments we performed using long-term GPS trajectory data show that our\nmethod outperforms the existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 01:52:28 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 20:24:07 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Mousavi", "Seyed Morteza", ""], ["Harwood", "Aaron", ""], ["Karunasekera", "Shanika", ""], ["Maghrebi", "Mojtaba", ""]]}, {"id": "1603.04118", "submitter": "Ravi Ganti", "authors": "Aniruddha Bhargava, Ravi Ganti, Robert Nowak", "title": "Active Algorithms For Preference Learning Problems with Multiple\n  Populations", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we model the problem of learning preferences of a population as\nan active learning problem. We propose an algorithm can adaptively choose pairs\nof items to show to users coming from a heterogeneous population, and use the\nobtained reward to decide which pair of items to show next. We provide\ncomputationally efficient algorithms with provable sample complexity guarantees\nfor this problem in both the noiseless and noisy cases. In the process of\nestablishing sample complexity guarantees for our algorithms, we establish new\nresults using a Nystr{\\\"o}m-like method which can be of independent interest.\nWe supplement our theoretical results with experimental comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 03:08:24 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 16:48:58 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Bhargava", "Aniruddha", ""], ["Ganti", "Ravi", ""], ["Nowak", "Robert", ""]]}, {"id": "1603.04119", "submitter": "Alekh Agarwal", "authors": "David Abel, Alekh Agarwal, Fernando Diaz, Akshay Krishnamurthy, Robert\n  E. Schapire", "title": "Exploratory Gradient Boosting for Reinforcement Learning in Complex\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional observations and complex real-world dynamics present major\nchallenges in reinforcement learning for both function approximation and\nexploration. We address both of these challenges with two complementary\ntechniques: First, we develop a gradient-boosting style, non-parametric\nfunction approximator for learning on $Q$-function residuals. And second, we\npropose an exploration strategy inspired by the principles of state abstraction\nand information acquisition under uncertainty. We demonstrate the empirical\neffectiveness of these techniques, first, as a preliminary check, on two\nstandard tasks (Blackjack and $n$-Chain), and then on two much larger and more\nrealistic tasks with high-dimensional observation spaces. Specifically, we\nintroduce two benchmarks built within the game Minecraft where the observations\nare pixel arrays of the agent's visual field. A combination of our two\nalgorithmic techniques performs competitively on the standard\nreinforcement-learning tasks while consistently and substantially outperforming\nbaselines on the two tasks with high-dimensional observation spaces. The new\nfunction approximator, exploration strategy, and evaluation benchmarks are each\nof independent interest in the pursuit of reinforcement-learning methods that\nscale to real-world domains.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 03:16:25 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Abel", "David", ""], ["Agarwal", "Alekh", ""], ["Diaz", "Fernando", ""], ["Krishnamurthy", "Akshay", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1603.04259", "submitter": "Oren Barkan", "authors": "Oren Barkan and Noam Koenigstein", "title": "Item2Vec: Neural Item Embedding for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Collaborative Filtering (CF) algorithms are item-based in the sense that\nthey analyze item-item relations in order to produce item similarities.\nRecently, several works in the field of Natural Language Processing (NLP)\nsuggested to learn a latent representation of words using neural embedding\nalgorithms. Among them, the Skip-gram with Negative Sampling (SGNS), also known\nas word2vec, was shown to provide state-of-the-art results on various\nlinguistics tasks. In this paper, we show that item-based CF can be cast in the\nsame framework of neural word embedding. Inspired by SGNS, we describe a method\nwe name item2vec for item-based CF that produces embedding for items in a\nlatent space. The method is capable of inferring item-item relations even when\nuser information is not available. We present experimental results that\ndemonstrate the effectiveness of the item2vec method and show it is competitive\nwith SVD.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 13:37:03 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 13:45:53 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 20:37:53 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Barkan", "Oren", ""], ["Koenigstein", "Noam", ""]]}, {"id": "1603.04319", "submitter": "Jalal Etesami", "authors": "Jalal Etesami, Negar Kiyavash, Kun Zhang, Kushagra Singhal", "title": "Learning Network of Multivariate Hawkes Processes: A Time Series\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the influence structure of multiple time series data is of great\ninterest to many disciplines. This paper studies the problem of recovering the\ncausal structure in network of multivariate linear Hawkes processes. In such\nprocesses, the occurrence of an event in one process affects the probability of\noccurrence of new events in some other processes. Thus, a natural notion of\ncausality exists between such processes captured by the support of the\nexcitation matrix. We show that the resulting causal influence network is\nequivalent to the Directed Information graph (DIG) of the processes, which\nencodes the causal factorization of the joint distribution of the processes.\nFurthermore, we present an algorithm for learning the support of excitation\nmatrix (or equivalently the DIG). The performance of the algorithm is evaluated\non synthesized multivariate Hawkes networks as well as a stock market and\nMemeTracker real-world dataset.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 16:08:26 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Etesami", "Jalal", ""], ["Kiyavash", "Negar", ""], ["Zhang", "Kun", ""], ["Singhal", "Kushagra", ""]]}, {"id": "1603.04402", "submitter": "Abhishek Sharma", "authors": "Abhishek Sharma, Michael Witbrock, Keith Goolsbey", "title": "Controlling Search in Very large Commonsense Knowledge Bases: A Machine\n  Learning Approach", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very large commonsense knowledge bases (KBs) often have thousands to millions\nof axioms, of which relatively few are relevant for answering any given query.\nA large number of irrelevant axioms can easily overwhelm resolution-based\ntheorem provers. Therefore, methods that help the reasoner identify useful\ninference paths form an essential part of large-scale reasoning systems. In\nthis paper, we describe two ordering heuristics for optimization of reasoning\nin such systems. First, we discuss how decision trees can be used to select\ninference steps that are more likely to succeed. Second, we identify a small\nset of problem instance features that suffice to guide searches away from\nintractable regions of the search space. We show the efficacy of these\ntechniques via experiments on thousands of queries from the Cyc KB. Results\nshow that these methods lead to an order of magnitude reduction in inference\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 19:20:36 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Sharma", "Abhishek", ""], ["Witbrock", "Michael", ""], ["Goolsbey", "Keith", ""]]}, {"id": "1603.04466", "submitter": "L. Elisa Celis", "authors": "L. Elisa Celis, Peter M. Krafft, Nathan Kobe", "title": "Sequential Voting Promotes Collective Discovery in Social Recommendation\n  Systems", "comments": "To be published in the 10th International AAAI Conference on Web and\n  Social Media (ICWSM) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.HC cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One goal of online social recommendation systems is to harness the wisdom of\ncrowds in order to identify high quality content. Yet the sequential voting\nmechanisms that are commonly used by these systems are at odds with existing\ntheoretical and empirical literature on optimal aggregation. This literature\nsuggests that sequential voting will promote herding---the tendency for\nindividuals to copy the decisions of others around them---and hence lead to\nsuboptimal content recommendation. Is there a problem with our practice, or a\nproblem with our theory? Previous attempts at answering this question have been\nlimited by a lack of objective measurements of content quality. Quality is\ntypically defined endogenously as the popularity of content in absence of\nsocial influence. The flaw of this metric is its presupposition that the\npreferences of the crowd are aligned with underlying quality. Domains in which\ncontent quality can be defined exogenously and measured objectively are thus\nneeded in order to better assess the design choices of social recommendation\nsystems. In this work, we look to the domain of education, where content\nquality can be measured via how well students are able to learn from the\nmaterial presented to them. Through a behavioral experiment involving a\nsimulated massive open online course (MOOC) run on Amazon Mechanical Turk, we\nshow that sequential voting systems can surface better content than systems\nthat elicit independent votes.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 20:48:43 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Celis", "L. Elisa", ""], ["Krafft", "Peter M.", ""], ["Kobe", "Nathan", ""]]}, {"id": "1603.04535", "submitter": "Ke Yan", "authors": "Ke Yan, Lu Kou, and David Zhang", "title": "Learning Domain-Invariant Subspace using Domain Features and\n  Independence Maximization", "comments": "Accepted", "journal-ref": null, "doi": "10.1109/TCYB.2016.2633306", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation algorithms are useful when the distributions of the\ntraining and the test data are different. In this paper, we focus on the\nproblem of instrumental variation and time-varying drift in the field of\nsensors and measurement, which can be viewed as discrete and continuous\ndistributional change in the feature space. We propose maximum independence\ndomain adaptation (MIDA) and semi-supervised MIDA (SMIDA) to address this\nproblem. Domain features are first defined to describe the background\ninformation of a sample, such as the device label and acquisition time. Then,\nMIDA learns a subspace which has maximum independence with the domain features,\nso as to reduce the inter-domain discrepancy in distributions. A feature\naugmentation strategy is also designed to project samples according to their\nbackgrounds so as to improve the adaptation. The proposed algorithms are\nflexible and fast. Their effectiveness is verified by experiments on synthetic\ndatasets and four real-world ones on sensors, measurement, and computer vision.\nThey can greatly enhance the practicability of sensor systems, as well as\nextend the application scope of existing domain adaptation algorithms by\nuniformly handling different kinds of distributional change.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 02:56:22 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 01:39:22 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Yan", "Ke", ""], ["Kou", "Lu", ""], ["Zhang", "David", ""]]}, {"id": "1603.04586", "submitter": "Mikko Lauri", "authors": "Mikko Lauri, Risto Ritala", "title": "Optimal Sensing via Multi-armed Bandit Relaxations in Mixed\n  Observability Domains", "comments": "6 pages, 2 figures", "journal-ref": "Proc. IEEE Intl. Conf. on Robotics and Automation (ICRA), pp.\n  4807-4812, 2015", "doi": "10.1109/ICRA.2015.7139867", "report-no": null, "categories": "cs.AI cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential decision making under uncertainty is studied in a mixed\nobservability domain. The goal is to maximize the amount of information\nobtained on a partially observable stochastic process under constraints imposed\nby a fully observable internal state. An upper bound for the optimal value\nfunction is derived by relaxing constraints. We identify conditions under which\nthe relaxed problem is a multi-armed bandit whose optimal policy is easily\ncomputable. The upper bound is applied to prune the search space in the\noriginal problem, and the effect on solution quality is assessed via simulation\nexperiments. Empirical results show effective pruning of the search space in a\ntarget monitoring domain.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 08:12:52 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Lauri", "Mikko", ""], ["Ritala", "Risto", ""]]}, {"id": "1603.05106", "submitter": "Shakir Mohamed", "authors": "Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor,\n  Daan Wierstra", "title": "One-Shot Generalization in Deep Generative Models", "comments": "8pgs, 1pg references, 1pg appendix, In Proceedings of the 33rd\n  International Conference on Machine Learning, JMLR: W&CP volume 48, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have an impressive ability to reason about new concepts and\nexperiences from just a single example. In particular, humans have an ability\nfor one-shot generalization: an ability to encounter a new concept, understand\nits structure, and then be able to generate compelling alternative variations\nof the concept. We develop machine learning systems with this important\ncapacity by developing new deep generative models, models that combine the\nrepresentational power of deep learning with the inferential power of Bayesian\nreasoning. We develop a class of sequential generative models that are built on\nthe principles of feedback and attention. These two characteristics lead to\ngenerative models that are among the state-of-the art in density estimation and\nimage generation. We demonstrate the one-shot generalization ability of our\nmodels using three tasks: unconditional sampling, generating new exemplars of a\ngiven concept, and generating new exemplars of a family of concepts. In all\ncases our models are able to generate compelling and diverse samples---having\nseen new examples just once---providing an important class of general-purpose\nmodels for one-shot machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 14:10:00 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 12:57:19 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""], ["Danihelka", "Ivo", ""], ["Gregor", "Karol", ""], ["Wierstra", "Daan", ""]]}, {"id": "1603.05145", "submitter": "Qiyang Zhao", "authors": "Qiyang Zhao, Lewis D Griffin", "title": "Suppressing the Unusual: towards Robust CNNs using Symmetric Activation\n  Functions", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deep Convolutional Neural Networks (CNN) make incorrect predictions on\nadversarial samples obtained by imperceptible perturbations of clean samples.\nWe hypothesize that this is caused by a failure to suppress unusual signals\nwithin network layers. As remedy we propose the use of Symmetric Activation\nFunctions (SAF) in non-linear signal transducer units. These units suppress\nsignals of exceptional magnitude. We prove that SAF networks can perform\nclassification tasks to arbitrary precision in a simplified situation. In\npractice, rather than use SAFs alone, we add them into CNNs to improve their\nrobustness. The modified CNNs can be easily trained using popular strategies\nwith the moderate training load. Our experiments on MNIST and CIFAR-10 show\nthat the modified CNNs perform similarly to plain ones on clean samples, and\nare remarkably more robust against adversarial and nonsense samples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 15:35:07 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Zhao", "Qiyang", ""], ["Griffin", "Lewis D", ""]]}, {"id": "1603.05314", "submitter": "Te-Hsuan Chen", "authors": "Te-Hsuan Chen and Ju-Yi Lu", "title": "Hardware Acceleration for Boolean Satisfiability Solver by Applying\n  Belief Propagation Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean satisfiability (SAT) has an extensive application domain in computer\nscience, especially in electronic design automation applications. Circuit\nsynthesis, optimization, and verification problems can be solved by\ntransforming original problems to SAT problems. However, the SAT problem is\nknown as NP-complete, which means there is no efficient method to solve it.\nTherefore, an efficient SAT solver to enhance the performance is always\ndesired. We propose a hardware acceleration method for SAT problems. By\nsurveying the properties of SAT problems and the decoding of low-density\nparity-check (LDPC) codes, a special class of error-correcting codes, we\ndiscover that both of them are constraint satisfaction problems. The belief\npropagation algorithm has been successfully applied to the decoding of LDPC,\nand the corresponding decoder hardware designs are extensively studied.\nTherefore, we proposed a belief propagation based algorithm to solve SAT\nproblems. With this algorithm, the SAT solver can be accelerated by hardware. A\nsoftware simulator is implemented to verify the proposed algorithm and the\nperformance improvement is estimated. Our experiment results show that time\ncomplexity does not increase with the size of SAT problems and the proposed\nmethod can achieve at least 30x speedup compared to MiniSat.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 23:57:27 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Chen", "Te-Hsuan", ""], ["Lu", "Ju-Yi", ""]]}, {"id": "1603.05474", "submitter": "Jiaolong Yang Dr.", "authors": "Jiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen,\n  Hongdong Li, Gang Hua", "title": "Neural Aggregation Network for Video Face Recognition", "comments": "Post CVPR2017 version with minor typo fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Neural Aggregation Network (NAN) for video face\nrecognition. The network takes a face video or face image set of a person with\na variable number of face images as its input, and produces a compact,\nfixed-dimension feature representation for recognition. The whole network is\ncomposed of two modules. The feature embedding module is a deep Convolutional\nNeural Network (CNN) which maps each face image to a feature vector. The\naggregation module consists of two attention blocks which adaptively aggregate\nthe feature vectors to form a single feature inside the convex hull spanned by\nthem. Due to the attention mechanism, the aggregation is invariant to the image\norder. Our NAN is trained with a standard classification or verification loss\nwithout any extra supervision signal, and we found that it automatically learns\nto advocate high-quality face images while repelling low-quality ones such as\nblurred, occluded and improperly exposed faces. The experiments on IJB-A,\nYouTube Face, Celebrity-1000 video face recognition benchmarks show that it\nconsistently outperforms naive aggregation methods and achieves the\nstate-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 13:30:45 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 12:38:10 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 06:02:06 GMT"}, {"version": "v4", "created": "Wed, 2 Aug 2017 08:08:14 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Yang", "Jiaolong", ""], ["Ren", "Peiran", ""], ["Zhang", "Dongqing", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""], ["Li", "Hongdong", ""], ["Hua", "Gang", ""]]}, {"id": "1603.05594", "submitter": "Enmei Tu", "authors": "Enmei Tu, Nikola Kasabov and Jie Yang", "title": "Mapping Temporal Variables into the NeuCube for Improved Pattern\n  Recognition, Predictive Modelling and Understanding of Stream Data", "comments": "Accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for an optimized mapping of temporal\nvariables, describing a temporal stream data, into the recently proposed\nNeuCube spiking neural network architecture. This optimized mapping extends the\nuse of the NeuCube, which was initially designed for spatiotemporal brain data,\nto work on arbitrary stream data and to achieve a better accuracy of temporal\npattern recognition, a better and earlier event prediction and a better\nunderstanding of complex temporal stream data through visualization of the\nNeuCube connectivity. The effect of the new mapping is demonstrated on three\nbench mark problems. The first one is early prediction of patient sleep stage\nevent from temporal physiological data. The second one is pattern recognition\nof dynamic temporal patterns of traffic in the Bay Area of California and the\nlast one is the Challenge 2012 contest data set. In all cases the use of the\nproposed mapping leads to an improved accuracy of pattern recognition and event\nprediction and a better understanding of the data when compared to traditional\nmachine learning techniques or spiking neural network reservoirs with arbitrary\nmapping of the variables.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 17:58:48 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Tu", "Enmei", ""], ["Kasabov", "Nikola", ""], ["Yang", "Jie", ""]]}, {"id": "1603.05670", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist, Peter Sarlin", "title": "Bank distress in the news: Describing events through deep learning", "comments": "Forthcoming in Neurocomputing. arXiv admin note: substantial text\n  overlap with arXiv:1507.07870 [in version 1]", "journal-ref": "Neurocomputing, 264, 2017", "doi": "10.1016/j.neucom.2016.12.11", "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.NE q-fin.CP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While many models are purposed for detecting the occurrence of significant\nevents in financial systems, the task of providing qualitative detail on the\ndevelopments is not usually as well automated. We present a deep learning\napproach for detecting relevant discussion in text and extracting natural\nlanguage descriptions of events. Supervised by only a small set of event\ninformation, comprising entity names and dates, the model is leveraged by\nunsupervised learning of semantic vector representations on extensive text\ndata. We demonstrate applicability to the study of financial risk based on news\n(6.6M articles), particularly bank distress and government interventions (243\nevents), where indices can signal the level of bank-stress-related reporting at\nthe entity level, or aggregated at national or European level, while being\ncoupled with explanations. Thus, we exemplify how text, as timely, widely\navailable and descriptive data, can serve as a useful complementary source of\ninformation for financial and systemic risk analytics.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 20:06:27 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 23:24:49 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Sarlin", "Peter", ""]]}, {"id": "1603.05959", "submitter": "Konstantinos Kamnitsas", "authors": "Konstantinos Kamnitsas, Christian Ledig, Virginia F.J. Newcombe,\n  Joanna P. Simpson, Andrew D. Kane, David K. Menon, Daniel Rueckert, Ben\n  Glocker", "title": "Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain\n  Lesion Segmentation", "comments": "This version was accepted in the journal Medical Image Analysis\n  (MedIA)", "journal-ref": null, "doi": "10.1016/j.media.2016.10.004", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dual pathway, 11-layers deep, three-dimensional Convolutional\nNeural Network for the challenging task of brain lesion segmentation. The\ndevised architecture is the result of an in-depth analysis of the limitations\nof current networks proposed for similar applications. To overcome the\ncomputational burden of processing 3D medical scans, we have devised an\nefficient and effective dense training scheme which joins the processing of\nadjacent image patches into one pass through the network while automatically\nadapting to the inherent class imbalance present in the data. Further, we\nanalyze the development of deeper, thus more discriminative 3D CNNs. In order\nto incorporate both local and larger contextual information, we employ a dual\npathway architecture that processes the input images at multiple scales\nsimultaneously. For post-processing of the network's soft segmentation, we use\na 3D fully connected Conditional Random Field which effectively removes false\npositives. Our pipeline is extensively evaluated on three challenging tasks of\nlesion segmentation in multi-channel MRI patient data with traumatic brain\ninjuries, brain tumors, and ischemic stroke. We improve on the state-of-the-art\nfor all three applications, with top ranking performance on the public\nbenchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient,\nwhich allows its adoption in a variety of research and clinical settings. The\nsource code of our implementation is made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 19:07:01 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 19:59:22 GMT"}, {"version": "v3", "created": "Sun, 8 Jan 2017 13:55:35 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Kamnitsas", "Konstantinos", ""], ["Ledig", "Christian", ""], ["Newcombe", "Virginia F. J.", ""], ["Simpson", "Joanna P.", ""], ["Kane", "Andrew D.", ""], ["Menon", "David K.", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""]]}, {"id": "1603.06015", "submitter": "Grigorios Chrysos", "authors": "Grigorios G. Chrysos, Epameinondas Antonakos, Patrick Snape, Akshay\n  Asthana and Stefanos Zafeiriou", "title": "A Comprehensive Performance Evaluation of Deformable Face Tracking\n  \"In-the-Wild\"", "comments": "E. Antonakos and P. Snape contributed equally and have joint second\n  authorship", "journal-ref": null, "doi": "10.1007/s11263-017-0999-5", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, technologies such as face detection, facial landmark localisation\nand face recognition and verification have matured enough to provide effective\nand efficient solutions for imagery captured under arbitrary conditions\n(referred to as \"in-the-wild\"). This is partially attributed to the fact that\ncomprehensive \"in-the-wild\" benchmarks have been developed for face detection,\nlandmark localisation and recognition/verification. A very important technology\nthat has not been thoroughly evaluated yet is deformable face tracking\n\"in-the-wild\". Until now, the performance has mainly been assessed\nqualitatively by visually assessing the result of a deformable face tracking\ntechnology on short videos. In this paper, we perform the first, to the best of\nour knowledge, thorough evaluation of state-of-the-art deformable face tracking\npipelines using the recently introduced 300VW benchmark. We evaluate many\ndifferent architectures focusing mainly on the task of on-line deformable face\ntracking. In particular, we compare the following general strategies: (a)\ngeneric face detection plus generic facial landmark localisation, (b) generic\nmodel free tracking plus generic facial landmark localisation, as well as (c)\nhybrid approaches using state-of-the-art face detection, model free tracking\nand facial landmark localisation technologies. Our evaluation reveals future\navenues for further research on the topic.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 23:17:01 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 22:23:40 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Chrysos", "Grigorios G.", ""], ["Antonakos", "Epameinondas", ""], ["Snape", "Patrick", ""], ["Asthana", "Akshay", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1603.06059", "submitter": "Nasrin Mostafazadeh", "authors": "Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell,\n  Xiaodong He, Lucy Vanderwende", "title": "Generating Natural Questions About an Image", "comments": "Proceedings of the 54th Annual Meeting of the Association for\n  Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an explosion of work in the vision & language community during\nthe past few years from image captioning to video transcription, and answering\nquestions about images. These tasks have focused on literal descriptions of the\nimage. To move beyond the literal, we choose to explore how questions about an\nimage are often directed at commonsense inference and the abstract events\nevoked by objects in the image. In this paper, we introduce the novel task of\nVisual Question Generation (VQG), where the system is tasked with asking a\nnatural and engaging question when shown an image. We provide three datasets\nwhich cover a variety of images from object-centric to event-centric, with\nconsiderably more abstract training data than provided to state-of-the-art\ncaptioning systems thus far. We train and test several generative and retrieval\nmodels to tackle the task of VQG. Evaluation results show that while such\nmodels ask reasonable questions for a variety of images, there is still a wide\ngap with human performance which motivates further work on connecting images\nwith commonsense knowledge and pragmatics. Our proposed task offers a new\nchallenge to the community which we hope furthers interest in exploring deeper\nconnections between vision & language.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 07:27:15 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 06:54:58 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 01:20:49 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Mostafazadeh", "Nasrin", ""], ["Misra", "Ishan", ""], ["Devlin", "Jacob", ""], ["Mitchell", "Margaret", ""], ["He", "Xiaodong", ""], ["Vanderwende", "Lucy", ""]]}, {"id": "1603.06125", "submitter": "Joshua Brul\\'e", "authors": "Joshua Brul\\'e", "title": "The Computational Power of Dynamic Bayesian Networks", "comments": null, "journal-ref": "Proceedings of the 4th International Workshop on Artificial\n  Intelligence and Cognition co-located with the Joint Multi-Conference on\n  Human-Level Artificial Intelligence (HLAI 2016) 158-166", "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the computational power of constant size, dynamic\nBayesian networks. Although discrete dynamic Bayesian networks are no more\npowerful than hidden Markov models, dynamic Bayesian networks with continuous\nrandom variables and discrete children of continuous parents are capable of\nperforming Turing-complete computation. With modified versions of existing\nalgorithms for belief propagation, such a simulation can be carried out in real\ntime. This result suggests that dynamic Bayesian networks may be more powerful\nthan previously considered. Relationships to causal models and recurrent neural\nnetworks are also discussed.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 18:30:02 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Brul\u00e9", "Joshua", ""]]}, {"id": "1603.06127", "submitter": "Petr Baudi\\v{s}", "authors": "Petr Baudi\\v{s}, Jan Pichl, Tom\\'a\\v{s} Vysko\\v{c}il, Jan \\v{S}ediv\\'y", "title": "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension", "comments": "submitted as paper to CoNLL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review the task of Sentence Pair Scoring, popular in the literature in\nvarious forms - viewed as Answer Sentence Selection, Semantic Text Scoring,\nNext Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a\ncomponent of Memory Networks.\n  We argue that all such tasks are similar from the model perspective and\npropose new baselines by comparing the performance of common IR metrics and\npopular convolutional, recurrent and attention-based neural models across many\nSentence Pair Scoring tasks and datasets. We discuss the problem of evaluating\nrandomized models, propose a statistically grounded methodology, and attempt to\nimprove comparisons by releasing new datasets that are much harder than some of\nthe currently used well explored benchmarks. We introduce a unified open source\nsoftware framework with easily pluggable models and tasks, which enables us to\nexperiment with multi-task reusability of trained sentence model. We set a new\nstate-of-art in performance on the Ubuntu Dialogue dataset.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 18:35:26 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 03:10:26 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 22:17:36 GMT"}, {"version": "v4", "created": "Tue, 17 May 2016 14:08:38 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Baudi\u0161", "Petr", ""], ["Pichl", "Jan", ""], ["Vysko\u010dil", "Tom\u00e1\u0161", ""], ["\u0160ediv\u00fd", "Jan", ""]]}, {"id": "1603.06129", "submitter": "Rishabh Singh", "authors": "Sahil Bhatia and Rishabh Singh", "title": "Automated Correction for Syntax Errors in Programming Assignments using\n  Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for automatically generating repair feedback for syntax\nerrors for introductory programming problems. Syntax errors constitute one of\nthe largest classes of errors (34%) in our dataset of student submissions\nobtained from a MOOC course on edX. The previous techniques for generating\nautomated feed- back on programming assignments have focused on functional\ncorrectness and style considerations of student programs. These techniques\nanalyze the program AST of the program and then perform some dynamic and\nsymbolic analyses to compute repair feedback. Unfortunately, it is not possible\nto generate ASTs for student pro- grams with syntax errors and therefore the\nprevious feedback techniques are not applicable in repairing syntax errors.\n  We present a technique for providing feedback on syntax errors that uses\nRecurrent neural networks (RNNs) to model syntactically valid token sequences.\nOur approach is inspired from the recent work on learning language models from\nBig Code (large code corpus). For a given programming assignment, we first\nlearn an RNN to model all valid token sequences using the set of syntactically\ncorrect student submissions. Then, for a student submission with syntax errors,\nwe query the learnt RNN model with the prefix to- ken sequence to predict token\nsequences that can fix the error by either replacing or inserting the predicted\ntoken sequence at the error location. We evaluate our technique on over 14, 000\nstudent submissions with syntax errors. Our technique can completely re- pair\n31.69% (4501/14203) of submissions with syntax errors and in addition partially\ncorrect 6.39% (908/14203) of the submissions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 18:43:28 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Bhatia", "Sahil", ""], ["Singh", "Rishabh", ""]]}, {"id": "1603.06141", "submitter": "Joshua Brul\\'e", "authors": "Joshua Brul\\'e, Kevin Engel, Nick Fung, Isaac Julien", "title": "Evolving Shepherding Behavior with Genetic Programming Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply genetic programming techniques to the `shepherding' problem, in\nwhich a group of one type of animal (sheep dogs) attempts to control the\nmovements of a second group of animals (sheep) obeying flocking behavior. Our\ngenetic programming algorithm evolves an expression tree that governs the\nmovements of each dog. The operands of the tree are hand-selected features of\nthe simulation environment that may allow the dogs to herd the sheep\neffectively. The algorithm uses tournament-style selection, crossover\nreproduction, and a point mutation. We find that the evolved solutions\ngeneralize well and outperform a (naive) human-designed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 20:36:44 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Brul\u00e9", "Joshua", ""], ["Engel", "Kevin", ""], ["Fung", "Nick", ""], ["Julien", "Isaac", ""]]}, {"id": "1603.06143", "submitter": "Daniel Ritchie", "authors": "Daniel Ritchie, Anna Thomas, Pat Hanrahan, Noah D. Goodman", "title": "Neurally-Guided Procedural Models: Amortized Inference for Procedural\n  Graphics Programs using Neural Networks", "comments": null, "journal-ref": "Neural Information Processing Systems (NIPS 2016)", "doi": null, "report-no": null, "categories": "cs.GR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic inference algorithms such as Sequential Monte Carlo (SMC)\nprovide powerful tools for constraining procedural models in computer graphics,\nbut they require many samples to produce desirable results. In this paper, we\nshow how to create procedural models which learn how to satisfy constraints. We\naugment procedural models with neural networks which control how the model\nmakes random choices based on the output it has generated thus far. We call\nsuch models neurally-guided procedural models. As a pre-computation, we train\nthese models to maximize the likelihood of example outputs generated via SMC.\nThey are then used as efficient SMC importance samplers, generating\nhigh-quality results with very few samples. We evaluate our method on\nL-system-like models with image-based constraints. Given a desired quality\nthreshold, neurally-guided models can generate satisfactory results up to 10x\nfaster than unguided models.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 20:58:47 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 20:10:09 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Ritchie", "Daniel", ""], ["Thomas", "Anna", ""], ["Hanrahan", "Pat", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1603.06212", "submitter": "Randal Olson", "authors": "Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, Jason H. Moore", "title": "Evaluation of a Tree-based Pipeline Optimization Tool for Automating\n  Data Science", "comments": "8 pages, 5 figures, preprint to appear in GECCO 2016, edits not yet\n  made from reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the field of data science continues to grow, there will be an\never-increasing demand for tools that make machine learning accessible to\nnon-experts. In this paper, we introduce the concept of tree-based pipeline\noptimization for automating one of the most tedious parts of machine\nlearning---pipeline design. We implement an open source Tree-based Pipeline\nOptimization Tool (TPOT) in Python and demonstrate its effectiveness on a\nseries of simulated and real-world benchmark data sets. In particular, we show\nthat TPOT can design machine learning pipelines that provide a significant\nimprovement over a basic machine learning analysis while requiring little to no\ninput nor prior knowledge from the user. We also address the tendency for TPOT\nto design overly complex pipelines by integrating Pareto optimization, which\nproduces compact pipelines without sacrificing classification accuracy. As\nsuch, this work represents an important step toward fully automating machine\nlearning pipeline design.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 13:32:27 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Olson", "Randal S.", ""], ["Bartley", "Nathan", ""], ["Urbanowicz", "Ryan J.", ""], ["Moore", "Jason H.", ""]]}, {"id": "1603.06217", "submitter": "Aliakbar Safilian", "authors": "Masoud Safilian and S. Mehdi Tashakkori and Sepehr Eghbali and\n  Aliakbar Safilian", "title": "An Approximation Approach for Solving the Subpath Planning Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subpath planning problem is a branch of the path planning problem, which\nhas widespread applications in automated manufacturing process as well as\nvehicle and robot navigation. This problem is to find the shortest path or tour\nsubject for travelling a set of given subpaths. The current approaches for\ndealing with the subpath planning problem are all based on meta-heuristic\napproaches. It is well-known that meta-heuristic based approaches have several\ndeficiencies. To address them, we propose a novel approximation algorithm in\nthe O(n^3) time complexity class, which guarantees to solve any subpath\nplanning problem instance with the fixed ratio bound of 2. Also, the formal\nproofs of the claims, our empirical evaluation shows that our approximation\nmethod acts much better than a state-of-the-art method, both in result and\nexecution time.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 14:22:26 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Safilian", "Masoud", ""], ["Tashakkori", "S. Mehdi", ""], ["Eghbali", "Sepehr", ""], ["Safilian", "Aliakbar", ""]]}, {"id": "1603.06288", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff\n  Schneider, Barnabas Poczos", "title": "Multi-fidelity Gaussian Process Bandit Optimisation", "comments": "Preliminary version appeared at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific and engineering applications, we are tasked with the\nmaximisation of an expensive to evaluate black box function $f$. Traditional\nsettings for this problem assume just the availability of this single function.\nHowever, in many cases, cheap approximations to $f$ may be obtainable. For\nexample, the expensive real world behaviour of a robot can be approximated by a\ncheap computer simulation. We can use these approximations to eliminate low\nfunction value regions cheaply and use the expensive evaluations of $f$ in a\nsmall but promising region and speedily identify the optimum. We formalise this\ntask as a \\emph{multi-fidelity} bandit problem where the target function and\nits approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a\nnovel method based on upper confidence bound techniques. In our theoretical\nanalysis we demonstrate that it exhibits precisely the above behaviour, and\nachieves better regret than strategies which ignore multi-fidelity information.\nEmpirically, MF-GP-UCB outperforms such naive strategies and other\nmulti-fidelity methods on several synthetic and real experiments.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 22:58:43 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 00:29:30 GMT"}, {"version": "v3", "created": "Sat, 4 Aug 2018 01:25:57 GMT"}, {"version": "v4", "created": "Fri, 15 Mar 2019 18:05:28 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Dasarathy", "Gautam", ""], ["Oliva", "Junier B.", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1603.06318", "submitter": "Zhiting Hu", "authors": "Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, Eric Xing", "title": "Harnessing Deep Neural Networks with Logic Rules", "comments": "Fix typos in appendix. ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining deep neural networks with structured logic rules is desirable to\nharness flexibility and reduce uninterpretability of the neural models. We\npropose a general framework capable of enhancing various types of neural\nnetworks (e.g., CNNs and RNNs) with declarative first-order logic rules.\nSpecifically, we develop an iterative distillation method that transfers the\nstructured information of logic rules into the weights of neural networks. We\ndeploy the framework on a CNN for sentiment analysis, and an RNN for named\nentity recognition. With a few highly intuitive rules, we obtain substantial\nimprovements and achieve state-of-the-art or comparable results to previous\nbest-performing systems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 03:33:20 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 05:28:21 GMT"}, {"version": "v3", "created": "Tue, 19 Jul 2016 23:30:48 GMT"}, {"version": "v4", "created": "Tue, 15 Nov 2016 21:41:21 GMT"}, {"version": "v5", "created": "Tue, 26 Mar 2019 05:16:10 GMT"}, {"version": "v6", "created": "Sat, 8 Aug 2020 07:38:00 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Hu", "Zhiting", ""], ["Ma", "Xuezhe", ""], ["Liu", "Zhengzhong", ""], ["Hovy", "Eduard", ""], ["Xing", "Eric", ""]]}, {"id": "1603.06393", "submitter": "Jiatao Gu", "authors": "Jiatao Gu, Zhengdong Lu, Hang Li and Victor O.K. Li", "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "comments": "10 pages, 5 figures, accepted by ACL2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address an important problem in sequence-to-sequence (Seq2Seq) learning\nreferred to as copying, in which certain segments in the input sequence are\nselectively replicated in the output sequence. A similar phenomenon is\nobservable in human language communication. For example, humans tend to repeat\nentity names or even long phrases in conversation. The challenge with regard to\ncopying in Seq2Seq is that new machinery is needed to decide when to perform\nthe operation. In this paper, we incorporate copying into neural network-based\nSeq2Seq learning and propose a new model called CopyNet with encoder-decoder\nstructure. CopyNet can nicely integrate the regular way of word generation in\nthe decoder with the new copying mechanism which can choose sub-sequences in\nthe input sequence and put them at proper places in the output sequence. Our\nempirical study on both synthetic data sets and real world data sets\ndemonstrates the efficacy of CopyNet. For example, CopyNet can outperform\nregular RNN-based model with remarkable margins on text summarization tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 11:35:08 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 03:33:58 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2016 13:53:21 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Gu", "Jiatao", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1603.06459", "submitter": "Nguyen Thi Thanh Dang", "authors": "Nguyen Thi Thanh Dang, Patrick De Causmaecker", "title": "Characterization of neighborhood behaviours in a multi-neighborhood\n  local search algorithm", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-neighborhood local search algorithm with a large number\nof possible neighborhoods. Each neighborhood is accompanied by a weight value\nwhich represents the probability of being chosen at each iteration. These\nweights are fixed before the algorithm runs, and are considered as parameters\nof the algorithm. Given a set of instances, off-line tuning of the algorithm's\nparameters can be done by automated algorithm configuration tools (e.g., SMAC).\nHowever, the large number of neighborhoods can make the tuning expensive and\ndifficult even when the number of parameters has been reduced by some\nintuition. In this work, we propose a systematic method to characterize each\nneighborhood's behaviours, representing them as a feature vector, and using\ncluster analysis to form similar groups of neighborhoods. The novelty of our\ncharacterization method is the ability of reflecting changes of behaviours\naccording to hardness of different solution quality regions. We show that using\nneighborhood clusters instead of individual neighborhoods helps to reduce the\nparameter configuration space without misleading the search of the tuning\nprocedure. Moreover, this method is problem-independent and potentially can be\napplied in similar contexts.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 12:38:32 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Dang", "Nguyen Thi Thanh", ""], ["De Causmaecker", "Patrick", ""]]}, {"id": "1603.06485", "submitter": "Lisa Posch", "authors": "Lisa Posch and Philipp Schaer and Arnim Bleier and Markus Strohmaier", "title": "A System for Probabilistic Linking of Thesauri and Classification\n  Systems", "comments": null, "journal-ref": "KI - K\\\"unstliche Intelligenz, 2015", "doi": "10.1007/s13218-015-0413-9", "report-no": null, "categories": "cs.AI cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a system which creates and visualizes probabilistic\nsemantic links between concepts in a thesaurus and classes in a classification\nsystem. For creating the links, we build on the Polylingual Labeled Topic Model\n(PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class in\nthe classification system by using information from the natural language text\nof documents, their assigned thesaurus descriptors and their designated\nclasses. The links are then presented to users of the system in an interactive\nvisualization, providing them with an automatically generated overview of the\nrelations between the thesaurus and the classification system.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 16:34:13 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Posch", "Lisa", ""], ["Schaer", "Philipp", ""], ["Bleier", "Arnim", ""], ["Strohmaier", "Markus", ""]]}, {"id": "1603.06554", "submitter": "Mohamed Amer", "authors": "Timothy J. Shields, Mohamed R. Amer, Max Ehrlich, Amir Tamrakar", "title": "Action-Affect Classification and Morphing using Multi-Task\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Most recent work focused on affect from facial expressions, and not as much\non body. This work focuses on body affect analysis. Affect does not occur in\nisolation. Humans usually couple affect with an action in natural interactions;\nfor example, a person could be talking and smiling. Recognizing body affect in\nsequences requires efficient algorithms to capture both the micro movements\nthat differentiate between happy and sad and the macro variations between\ndifferent actions. We depart from traditional approaches for time-series data\nanalytics by proposing a multi-task learning model that learns a shared\nrepresentation that is well-suited for action-affect classification as well as\ngeneration. For this paper we choose Conditional Restricted Boltzmann Machines\nto be our building block. We propose a new model that enhances the CRBM model\nwith a factored multi-task component to become Multi-Task Conditional\nRestricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two\npublicly available datasets, the Body Affect dataset and the Tower Game\ndataset, and show superior classification performance improvement over the\nstate-of-the-art, as well as the generative abilities of our model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 19:38:07 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Shields", "Timothy J.", ""], ["Amer", "Mohamed R.", ""], ["Ehrlich", "Max", ""], ["Tamrakar", "Amir", ""]]}, {"id": "1603.06677", "submitter": "Percy Liang", "authors": "Percy Liang", "title": "Learning Executable Semantic Parsers for Natural Language Understanding", "comments": "Accepted to the Communications of the ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For building question answering systems and natural language interfaces,\nsemantic parsing has emerged as an important and powerful paradigm. Semantic\nparsers map natural language into logical forms, the classic representation for\nmany important linguistic phenomena. The modern twist is that we are interested\nin learning semantic parsers from data, which introduces a new layer of\nstatistical and computational issues. This article lays out the components of a\nstatistical semantic parser, highlighting the key challenges. We will see that\nsemantic parsing is a rich fusion of the logical and the statistical world, and\nthat this fusion will play an integral role in the future of natural language\nunderstanding systems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 05:07:16 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Liang", "Percy", ""]]}, {"id": "1603.06807", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Alberto Garc\\'ia-Dur\\'an, Caglar Gulcehre, Sungjin\n  Ahn, Sarath Chandar, Aaron Courville, Yoshua Bengio", "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M\n  Factoid Question-Answer Corpus", "comments": "13 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, large-scale supervised learning corpora have enabled\nmachine learning researchers to make substantial advances. However, to this\ndate, there are no large-scale question-answer corpora available. In this paper\nwe present the 30M Factoid Question-Answer Corpus, an enormous question answer\npair corpus produced by applying a novel neural network architecture on the\nknowledge base Freebase to transduce facts into natural language questions. The\nproduced question answer pairs are evaluated both by human evaluators and using\nautomatic evaluation metrics, including well-established machine translation\nand sentence similarity metrics. Across all evaluation criteria the\nquestion-generation model outperforms the competing template-based baseline.\nFurthermore, when presented to human evaluators, the generated questions appear\ncomparable in quality to real human-generated questions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 14:25:16 GMT"}, {"version": "v2", "created": "Sun, 29 May 2016 20:00:20 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Garc\u00eda-Dur\u00e1n", "Alberto", ""], ["Gulcehre", "Caglar", ""], ["Ahn", "Sungjin", ""], ["Chandar", "Sarath", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1603.06881", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Sivaraman Balakrishnan, Martin J. Wainwright", "title": "Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of\n  Pairwise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study methods for aggregating pairwise comparison data in order to\nestimate outcome probabilities for future comparisons among a collection of n\nitems. Working within a flexible framework that imposes only a form of strong\nstochastic transitivity (SST), we introduce an adaptivity index defined by the\nindifference sets of the pairwise comparison probabilities. In addition to\nmeasuring the usual worst-case risk of an estimator, this adaptivity index also\ncaptures the extent to which the estimator adapts to instance-specific\ndifficulty relative to an oracle estimator. We prove three main results that\ninvolve this adaptivity index and different algorithms. First, we propose a\nthree-step estimator termed Count-Randomize-Least squares (CRL), and show that\nit has adaptivity index upper bounded as $\\sqrt{n}$ up to logarithmic factors.\nWe then show that that conditional on the hardness of planted clique, no\ncomputationally efficient estimator can achieve an adaptivity index smaller\nthan $\\sqrt{n}$. Second, we show that a regularized least squares estimator can\nachieve a poly-logarithmic adaptivity index, thereby demonstrating a\n$\\sqrt{n}$-gap between optimal and computationally achievable adaptivity.\nFinally, we prove that the standard least squares estimator, which is known to\nbe optimally adaptive in several closely related problems, fails to adapt in\nthe context of estimating pairwise probabilities.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 17:28:08 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Shah", "Nihar B.", ""], ["Balakrishnan", "Sivaraman", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1603.07029", "submitter": "Michael Wiser", "authors": "Michael J Wiser, Louise S Mead, James J Smith, Robert T Pennock", "title": "Comparing Human and Automated Evaluation of Open-Ended Student Responses\n  to Questions of Evolution", "comments": "Submitted to ALife 2016", "journal-ref": "Artificial Life XV: Proceedings of the Fifteenth International\n  Conference on Artificial life. pp. 116 - 122. MIT Press. 2016", "doi": "10.7551/978-0-262-33936-0-ch025", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Written responses can provide a wealth of data in understanding student\nreasoning on a topic. Yet they are time- and labor-intensive to score,\nrequiring many instructors to forego them except as limited parts of summative\nassessments at the end of a unit or course. Recent developments in Machine\nLearning (ML) have produced computational methods of scoring written responses\nfor the presence or absence of specific concepts. Here, we compare the scores\nfrom one particular ML program -- EvoGrader -- to human scoring of responses to\nstructurally- and content-similar questions that are distinct from the ones the\nprogram was trained on. We find that there is substantial inter-rater\nreliability between the human and ML scoring. However, sufficient systematic\ndifferences remain between the human and ML scoring that we advise only using\nthe ML scoring for formative, rather than summative, assessment of student\nreasoning.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 23:36:02 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Wiser", "Michael J", ""], ["Mead", "Louise S", ""], ["Smith", "James J", ""], ["Pennock", "Robert T", ""]]}, {"id": "1603.07051", "submitter": "Mohamed El Yafrani", "authors": "Mohamed El Yafrani and Bela\\\"id Ahiod", "title": "Cosolver2B: An Efficient Local Search Heuristic for the Travelling Thief\n  Problem", "comments": "12th ACS/IEEE International Conference on Computer Systems and\n  Applications (AICCSA) 2015. November 17-20, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world problems are very difficult to optimize. However, many researchers\nhave been solving benchmark problems that have been extensively investigated\nfor the last decades even if they have very few direct applications. The\nTraveling Thief Problem (TTP) is a NP-hard optimization problem that aims to\nprovide a more realistic model. TTP targets particularly routing problem under\npacking/loading constraints which can be found in supply chain management and\ntransportation. In this paper, TTP is presented and formulated mathematically.\nA combined local search algorithm is proposed and compared with Random Local\nSearch (RLS) and Evolutionary Algorithm (EA). The obtained results are quite\npromising since new better solutions were found.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 02:30:35 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Yafrani", "Mohamed El", ""], ["Ahiod", "Bela\u00efd", ""]]}, {"id": "1603.07292", "submitter": "Shayak Sen", "authors": "Aleksandar Chakarov, Aditya Nori, Sriram Rajamani, Shayak Sen, and\n  Deepak Vijaykeerthy", "title": "Debugging Machine Learning Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike traditional programs (such as operating systems or word processors)\nwhich have large amounts of code, machine learning tasks use programs with\nrelatively small amounts of code (written in machine learning libraries), but\nvoluminous amounts of data. Just like developers of traditional programs debug\nerrors in their code, developers of machine learning tasks debug and fix errors\nin their data. However, algorithms and tools for debugging and fixing errors in\ndata are less common, when compared to their counterparts for detecting and\nfixing errors in code. In this paper, we consider classification tasks where\nerrors in training data lead to misclassifications in test points, and propose\nan automated method to find the root causes of such misclassifications. Our\nroot cause analysis is based on Pearl's theory of causation, and uses Pearl's\nPS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,\nencodes the computation of PS as a probabilistic program, and uses recent work\non probabilistic programs and transformations on probabilistic programs (along\nwith gray-box models of machine learning algorithms) to efficiently compute PS.\nPsi is able to identify root causes of data errors in interesting data sets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 18:30:37 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Chakarov", "Aleksandar", ""], ["Nori", "Aditya", ""], ["Rajamani", "Sriram", ""], ["Sen", "Shayak", ""], ["Vijaykeerthy", "Deepak", ""]]}, {"id": "1603.07294", "submitter": "James Foulds", "authors": "James Foulds, Joseph Geumlek, Max Welling, Kamalika Chaudhuri", "title": "On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis", "comments": "Updated to match the accepted UAI version. Generalized the ARE result\n  and included a more detailed proof. Improved some figures, etc", "journal-ref": "Proceedings of the 32nd Conference on Uncertainty in Artificial\n  Intelligence (UAI), 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference has great promise for the privacy-preserving analysis of\nsensitive data, as posterior sampling automatically preserves differential\nprivacy, an algorithmic notion of data privacy, under certain conditions\n(Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample\n(OPS) approach elegantly provides privacy \"for free,\" it is data inefficient in\nthe sense of asymptotic relative efficiency (ARE). We show that a simple\nalternative based on the Laplace mechanism, the workhorse of differential\nprivacy, is as asymptotically efficient as non-private posterior inference,\nunder general assumptions. This technique also has practical advantages\nincluding efficient use of the privacy budget for MCMC. We demonstrate the\npracticality of our approach on a time-series analysis of sensitive military\nrecords from the Afghanistan and Iraq wars disclosed by the Wikileaks\norganization.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 18:31:05 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 00:00:10 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Foulds", "James", ""], ["Geumlek", "Joseph", ""], ["Welling", "Max", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1603.07396", "submitter": "Aniruddha Kembhavi", "authors": "Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh\n  Hajishirzi, Ali Farhadi", "title": "A Diagram Is Worth A Dozen Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagrams are common tools for representing complex concepts, relationships\nand events, often when it would be difficult to portray the same information\nwith natural images. Understanding natural images has been extensively studied\nin computer vision, while diagram understanding has received little attention.\nIn this paper, we study the problem of diagram interpretation and reasoning,\nthe challenging task of identifying the structure of a diagram and the\nsemantics of its constituents and their relationships. We introduce Diagram\nParse Graphs (DPG) as our representation to model the structure of diagrams. We\ndefine syntactic parsing of diagrams as learning to infer DPGs for diagrams and\nstudy semantic interpretation and reasoning of diagrams in the context of\ndiagram question answering. We devise an LSTM-based method for syntactic\nparsing of diagrams and introduce a DPG-based attention model for diagram\nquestion answering. We compile a new dataset of diagrams with exhaustive\nannotations of constituents and relationships for over 5,000 diagrams and\n15,000 questions and answers. Our results show the significance of our models\nfor syntactic parsing and question answering in diagrams using DPGs.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 00:02:58 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Kembhavi", "Aniruddha", ""], ["Salvato", "Mike", ""], ["Kolve", "Eric", ""], ["Seo", "Minjoon", ""], ["Hajishirzi", "Hannaneh", ""], ["Farhadi", "Ali", ""]]}, {"id": "1603.07417", "submitter": "Stephen Makonin", "authors": "Md. Zulfiquar Ali Bhotto, Stephen Makonin, Ivan V. Bajic", "title": "Load Disaggregation Based on Aided Linear Integer Programming", "comments": null, "journal-ref": null, "doi": "10.1109/TCSII.2016.2603479", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Load disaggregation based on aided linear integer programming (ALIP) is\nproposed. We start with a conventional linear integer programming (IP) based\ndisaggregation and enhance it in several ways. The enhancements include\nadditional constraints, correction based on a state diagram, median filtering,\nand linear programming-based refinement. With the aid of these enhancements,\nthe performance of IP-based disaggregation is significantly improved. The\nproposed ALIP system relies only on the instantaneous load samples instead of\nwaveform signatures, and hence does not crucially depend on high sampling\nfrequency. Experimental results show that the proposed ALIP system performs\nbetter than the conventional IP-based load disaggregation system.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 02:54:45 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 00:27:59 GMT"}, {"version": "v3", "created": "Tue, 30 Aug 2016 16:32:57 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Bhotto", "Md. Zulfiquar Ali", ""], ["Makonin", "Stephen", ""], ["Bajic", "Ivan V.", ""]]}, {"id": "1603.07442", "submitter": "Donggeun Yoo", "authors": "Donggeun Yoo, Namil Kim, Sunggyun Park, Anthony S. Paek, In So Kweon", "title": "Pixel-Level Domain Transfer", "comments": "Published in ECCV 2016. Code and dataset available at dgyoo.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an image-conditional image generation model. The model transfers\nan input domain to a target domain in semantic level, and generates the target\nimage in pixel level. To generate realistic target images, we employ the\nreal/fake-discriminator as in Generative Adversarial Nets, but also introduce a\nnovel domain-discriminator to make the generated image relevant to the input\nimage. We verify our model through a challenging task of generating a piece of\nclothing from an input image of a dressed person. We present a high quality\nclothing dataset containing the two domains, and succeed in demonstrating\ndecent results.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 05:20:59 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 01:20:33 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 13:17:40 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Yoo", "Donggeun", ""], ["Kim", "Namil", ""], ["Park", "Sunggyun", ""], ["Paek", "Anthony S.", ""], ["Kweon", "In So", ""]]}, {"id": "1603.07453", "submitter": "Bruno Woltzenlogel Paleo", "authors": "Bruno Woltzenlogel Paleo", "title": "An Expressive Probabilistic Temporal Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper argues that a combined treatment of probabilities, time and\nactions is essential for an appropriate logical account of the notion of\nprobability; and, based on this intuition, describes an expressive\nprobabilistic temporal logic for reasoning about actions with uncertain\noutcomes. The logic is modal and higher-order: modalities annotated by actions\nare used to express possibility and necessity of propositions in the next\nstates resulting from the actions, and a higher-order function is needed to\nexpress the probability operator. The proposed logic is shown to be an adequate\nextension of classical mathematical probability theory, and its expressiveness\nis illustrated through the formalization of the Monty Hall problem.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 07:00:33 GMT"}, {"version": "v2", "created": "Sun, 8 Oct 2017 02:02:49 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Paleo", "Bruno Woltzenlogel", ""]]}, {"id": "1603.07704", "submitter": "Quan Liu", "authors": "Quan Liu, Hui Jiang, Andrew Evdokimov, Zhen-Hua Ling, Xiaodan Zhu, Si\n  Wei, Yu Hu", "title": "Probabilistic Reasoning via Deep Learning: Neural Association Models", "comments": "Probabilistic reasoning, Winograd Schema Challenge, Deep learning,\n  Neural Networks, Distributed Representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new deep learning approach, called neural\nassociation model (NAM), for probabilistic reasoning in artificial\nintelligence. We propose to use neural networks to model association between\nany two events in a domain. Neural networks take one event as input and compute\na conditional probability of the other event to model how likely these two\nevents are to be associated. The actual meaning of the conditional\nprobabilities varies between applications and depends on how the models are\ntrained. In this work, as two case studies, we have investigated two NAM\nstructures, namely deep neural networks (DNN) and relation-modulated neural\nnets (RMNN), on several probabilistic reasoning tasks in AI, including\nrecognizing textual entailment, triple classification in multi-relational\nknowledge bases and commonsense reasoning. Experimental results on several\npopular datasets derived from WordNet, FreeBase and ConceptNet have all\ndemonstrated that both DNNs and RMNNs perform equally well and they can\nsignificantly outperform the conventional methods available for these reasoning\ntasks. Moreover, compared with DNNs, RMNNs are superior in knowledge transfer,\nwhere a pre-trained model can be quickly extended to an unseen relation after\nobserving only a few training samples. To further prove the effectiveness of\nthe proposed models, in this work, we have applied NAMs to solving challenging\nWinograd Schema (WS) problems. Experiments conducted on a set of WS problems\nprove that the proposed models have the potential for commonsense reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 18:54:18 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 14:31:17 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Liu", "Quan", ""], ["Jiang", "Hui", ""], ["Evdokimov", "Andrew", ""], ["Ling", "Zhen-Hua", ""], ["Zhu", "Xiaodan", ""], ["Wei", "Si", ""], ["Hu", "Yu", ""]]}, {"id": "1603.07810", "submitter": "Andreas Veit", "authors": "Andreas Veit, Serge Belongie, Theofanis Karaletsos", "title": "Conditional Similarity Networks", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What makes images similar? To measure the similarity between images, they are\ntypically embedded in a feature-vector space, in which their distance preserve\nthe relative dissimilarity. However, when learning such similarity embeddings\nthe simplifying assumption is commonly made that images are only compared to\none unique measure of similarity. A main reason for this is that contradicting\nnotions of similarities cannot be captured in a single space. To address this\nshortcoming, we propose Conditional Similarity Networks (CSNs) that learn\nembeddings differentiated into semantically distinct subspaces that capture the\ndifferent notions of similarities. CSNs jointly learn a disentangled embedding\nwhere features for different similarities are encoded in separate dimensions as\nwell as masks that select and reweight relevant dimensions to induce a subspace\nthat encodes a specific similarity notion. We show that our approach learns\ninterpretable image representations with visually relevant semantic subspaces.\nFurther, when evaluating on triplet questions from multiple similarity notions\nour model even outperforms the accuracy obtained by training individual\nspecialized networks for each notion separately.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 02:52:02 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 12:41:01 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 15:18:21 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Veit", "Andreas", ""], ["Belongie", "Serge", ""], ["Karaletsos", "Theofanis", ""]]}, {"id": "1603.07886", "submitter": "Shanlin Zhong", "authors": "Peijie Yin, Hong Qiao, Wei Wu, Lu Qi, YinLin Li, Shanlin Zhong, Bo\n  Zhang", "title": "A Novel Biologically Mechanism-Based Visual Cognition Model--Automatic\n  Extraction of Semantics, Formation of Integrated Concepts and Re-selection\n  Features for Ambiguity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration between biology and information science benefits both fields.\nMany related models have been proposed, such as computational visual cognition\nmodels, computational motor control models, integrations of both and so on. In\ngeneral, the robustness and precision of recognition is one of the key problems\nfor object recognition models.\n  In this paper, inspired by features of human recognition process and their\nbiological mechanisms, a new integrated and dynamic framework is proposed to\nmimic the semantic extraction, concept formation and feature re-selection in\nhuman visual processing. The main contributions of the proposed model are as\nfollows:\n  (1) Semantic feature extraction: Local semantic features are learnt from\nepisodic features that are extracted from raw images through a deep neural\nnetwork;\n  (2) Integrated concept formation: Concepts are formed with local semantic\ninformation and structural information learnt through network.\n  (3) Feature re-selection: When ambiguity is detected during recognition\nprocess, distinctive features according to the difference between ambiguous\ncandidates are re-selected for recognition.\n  Experimental results on hand-written digits and facial shape dataset show\nthat, compared with other methods, the new proposed model exhibits higher\nrobustness and precision for visual recognition, especially in the condition\nwhen input samples are smantic ambiguous. Meanwhile, the introduced biological\nmechanisms further strengthen the interaction between neuroscience and\ninformation science.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 11:47:16 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Yin", "Peijie", ""], ["Qiao", "Hong", ""], ["Wu", "Wei", ""], ["Qi", "Lu", ""], ["Li", "YinLin", ""], ["Zhong", "Shanlin", ""], ["Zhang", "Bo", ""]]}, {"id": "1603.08023", "submitter": "Ryan Lowe T.", "authors": "Chia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent\n  Charlin, Joelle Pineau", "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of\n  Unsupervised Evaluation Metrics for Dialogue Response Generation", "comments": "First 4 authors had equal contribution. 13 pages, 5 tables, 6\n  figures. EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate evaluation metrics for dialogue response generation systems\nwhere supervised labels, such as task completion, are not available. Recent\nworks in response generation have adopted metrics from machine translation to\ncompare a model's generated response to a single target response. We show that\nthese metrics correlate very weakly with human judgements in the non-technical\nTwitter domain, and not at all in the technical Ubuntu domain. We provide\nquantitative and qualitative results highlighting specific weaknesses in\nexisting metrics, and provide recommendations for future development of better\nautomatic evaluation metrics for dialogue systems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 20:32:21 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 18:28:32 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Liu", "Chia-Wei", ""], ["Lowe", "Ryan", ""], ["Serban", "Iulian V.", ""], ["Noseworthy", "Michael", ""], ["Charlin", "Laurent", ""], ["Pineau", "Joelle", ""]]}, {"id": "1603.08079", "submitter": "Andrei Barbu", "authors": "Yevgeni Berzak and Andrei Barbu and Daniel Harari and Boris Katz and\n  Shimon Ullman", "title": "Do You See What I Mean? Visual Resolution of Linguistic Ambiguities", "comments": "EMNLP 2015", "journal-ref": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), 2015, pages 1477--1487", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding language goes hand in hand with the ability to integrate\ncomplex contextual information obtained via perception. In this work, we\npresent a novel task for grounded language understanding: disambiguating a\nsentence given a visual scene which depicts one of the possible interpretations\nof that sentence. To this end, we introduce a new multimodal corpus containing\nambiguous sentences, representing a wide range of syntactic, semantic and\ndiscourse ambiguities, coupled with videos that visualize the different\ninterpretations for each sentence. We address this task by extending a vision\nmodel which determines if a sentence is depicted by a video. We demonstrate how\nsuch a model can be adjusted to recognize different interpretations of the same\nunderlying sentence, allowing to disambiguate sentences in a unified fashion\nacross the different ambiguity types.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 06:49:33 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Berzak", "Yevgeni", ""], ["Barbu", "Andrei", ""], ["Harari", "Daniel", ""], ["Katz", "Boris", ""], ["Ullman", "Shimon", ""]]}, {"id": "1603.08253", "submitter": "Devon Merrill", "authors": "Devon Merrill", "title": "Negative Learning Rates and P-Learning", "comments": "Embarrassingly poor manuscript with many errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of training a differentiable function approximator for a\nregression task using negative examples. We effect this training using negative\nlearning rates. We also show how this method can be used to perform direct\npolicy learning in a reinforcement learning setting.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 20:02:13 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 11:10:33 GMT"}, {"version": "v3", "created": "Sun, 17 Jun 2018 16:42:04 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Merrill", "Devon", ""]]}, {"id": "1603.08262", "submitter": "Kamil Rocki", "authors": "Kamil Rocki", "title": "Towards Machine Intelligence", "comments": "10 pages, submitted to AGI-16. arXiv admin note: substantial text\n  overlap with arXiv:1512.01926", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a theory of a single general-purpose learning algorithm which\ncould explain the principles of its operation. This theory assumes that the\nbrain has some initial rough architecture, a small library of simple innate\ncircuits which are prewired at birth and proposes that all significant mental\nalgorithms can be learned. Given current understanding and observations, this\npaper reviews and lists the ingredients of such an algorithm from both\narchitectural and functional perspectives.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 22:01:59 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Rocki", "Kamil", ""]]}, {"id": "1603.08507", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue,\n  Bernt Schiele, Trevor Darrell", "title": "Generating Visual Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clearly explaining a rationale for a classification decision to an end-user\ncan be as important as the decision itself. Existing approaches for deep visual\nrecognition are generally opaque and do not output any justification text;\ncontemporary vision-language models can describe image content but fail to take\ninto account class-discriminative image aspects which justify visual\npredictions. We propose a new model that focuses on the discriminating\nproperties of the visible object, jointly predicts a class label, and explains\nwhy the predicted label is appropriate for the image. We propose a novel loss\nfunction based on sampling and reinforcement learning that learns to generate\nsentences that realize a global sentence property, such as class specificity.\nOur results on a fine-grained bird species classification dataset show that our\nmodel is able to generate explanations which are not only consistent with an\nimage but also more discriminative than descriptions produced by existing\ncaptioning methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 19:54:12 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Akata", "Zeynep", ""], ["Rohrbach", "Marcus", ""], ["Donahue", "Jeff", ""], ["Schiele", "Bernt", ""], ["Darrell", "Trevor", ""]]}, {"id": "1603.08561", "submitter": "Ishan Misra", "authors": "Ishan Misra and C. Lawrence Zitnick and Martial Hebert", "title": "Shuffle and Learn: Unsupervised Learning using Temporal Order\n  Verification", "comments": "Accepted at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach for learning a visual representation\nfrom the raw spatiotemporal signals in videos. Our representation is learned\nwithout supervision from semantic labels. We formulate our method as an\nunsupervised sequential verification task, i.e., we determine whether a\nsequence of frames from a video is in the correct temporal order. With this\nsimple task and no semantic labels, we learn a powerful visual representation\nusing a Convolutional Neural Network (CNN). The representation contains\ncomplementary information to that learned from supervised image datasets like\nImageNet. Qualitative results show that our method captures information that is\ntemporally varying, such as human pose. When used as pre-training for action\nrecognition, our method gives significant gains over learning without external\ndata on benchmark datasets like UCF101 and HMDB51. To demonstrate its\nsensitivity to human pose, we show results for pose estimation on the FLIC and\nMPII datasets that are competitive, or better than approaches using\nsignificantly more supervision. Our method can be combined with supervised\nrepresentations to provide an additional boost in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 21:00:43 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 17:26:01 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Misra", "Ishan", ""], ["Zitnick", "C. Lawrence", ""], ["Hebert", "Martial", ""]]}, {"id": "1603.08714", "submitter": "Kristijonas \\v{C}yras", "authors": "Kristijonas Cyras, Francesca Toni", "title": "Properties of ABA+ for Non-Monotonic Reasoning", "comments": "This is a revised version of the paper presented at the workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate properties of ABA+, a formalism that extends the well studied\nstructured argumentation formalism Assumption-Based Argumentation (ABA) with a\npreference handling mechanism. In particular, we establish desirable properties\nthat ABA+ semantics exhibit. These pave way to the satisfaction by ABA+ of some\n(arguably) desirable principles of preference handling in argumentation and\nnonmonotonic reasoning, as well as non-monotonic inference properties of ABA+\nunder various semantics.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 10:37:38 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 15:59:11 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 12:04:57 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Cyras", "Kristijonas", ""], ["Toni", "Francesca", ""]]}, {"id": "1603.08776", "submitter": "Nikolaus Hansen", "authors": "Nikolaus Hansen (Inria), Tea Tusar (Inria), Olaf Mersmann, Anne Auger\n  (Inria), Dimo Brockhoff (Inria)", "title": "COCO: The Experimental Procedure", "comments": "ArXiv e-prints, arXiv:1603.08776", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a budget-free experimental setup and procedure for benchmarking\nnumericaloptimization algorithms in a black-box scenario. This procedure can be\napplied with the COCO benchmarking platform. We describe initialization of and\ninput to the algorithm and touch upon therelevance of termination and restarts.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 14:10:14 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 11:58:22 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Hansen", "Nikolaus", "", "Inria"], ["Tusar", "Tea", "", "Inria"], ["Mersmann", "Olaf", "", "Inria"], ["Auger", "Anne", "", "Inria"], ["Brockhoff", "Dimo", "", "Inria"]]}, {"id": "1603.08785", "submitter": "Nikolaus Hansen", "authors": "Nikolaus Hansen (RANDOPT), Anne Auger (RANDOPT), Raymond Ros (TAO),\n  Olaf Mersmann (TU), Tea Tu\\v{s}ar (IJS), Dimo Brockhoff (RANDOPT)", "title": "COCO: A Platform for Comparing Continuous Optimizers in a Black-Box\n  Setting", "comments": "Optimization Methods and Software, Taylor & Francis, In press,\n  pp.1-31", "journal-ref": null, "doi": "10.1080/10556788.2020.1808977", "report-no": null, "categories": "cs.AI cs.MS cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce COCO, an open source platform for Comparing Continuous\nOptimizers in a black-box setting. COCO aims at automatizing the tedious and\nrepetitive task of benchmarking numerical optimization algorithms to the\ngreatest possible extent. The platform and the underlying methodology allow to\nbenchmark in the same framework deterministic and stochastic solvers for both\nsingle and multiobjective optimization. We present the rationales behind the\n(decade-long) development of the platform as a general proposition for\nguidelines towards better benchmarking. We detail underlying fundamental\nconcepts of COCO such as the definition of a problem as a function instance,\nthe underlying idea of instances, the use of target values, and runtime defined\nby the number of function calls as the central performance measure. Finally, we\ngive a quick overview of the basic code structure and the currently available\ntest suites.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 14:18:52 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 06:27:09 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 15:19:31 GMT"}, {"version": "v4", "created": "Wed, 9 Sep 2020 14:41:57 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Hansen", "Nikolaus", "", "RANDOPT"], ["Auger", "Anne", "", "RANDOPT"], ["Ros", "Raymond", "", "TAO"], ["Mersmann", "Olaf", "", "TU"], ["Tu\u0161ar", "Tea", "", "IJS"], ["Brockhoff", "Dimo", "", "RANDOPT"]]}, {"id": "1603.08789", "submitter": "Jean-Guy Mailly", "authors": "Jean-Guy Mailly", "title": "Using Enthymemes to Fill the Gap between Logical Argumentation and\n  Revision of Abstract Argumentation Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a preliminary work on an approach to fill the gap\nbetween logic-based argumentation and the numerous approaches to tackle the\ndynamics of abstract argumentation frameworks. Our idea is that, even when\narguments and attacks are defined by means of a logical belief base, there may\nbe some uncertainty about how accurate is the content of an argument, and so\nthe presence (or absence) of attacks concerning it. We use enthymemes to\nillustrate this notion of uncertainty of arguments and attacks. Indeed, as\nargued in the literature, real arguments are often enthymemes instead of\ncompletely specified deductive arguments. This means that some parts of the\npair (support, claim) may be missing because they are supposed to belong to\nsome \"common knowledge\", and then should be deduced by the agent which receives\nthe enthymeme. But the perception that agents have of the common knowledge may\nbe wrong, and then a first agent may state an enthymeme that her opponent is\nnot able to decode in an accurate way. It is likely that the decoding of the\nenthymeme by the agent leads to mistaken attacks between this new argument and\nthe existing ones. In this case, the agent can receive some information about\nattacks or arguments acceptance statuses which disagree with her argumentation\nframework. We exemplify a way to incorporate this new piece of information by\nmeans of existing works on the dynamics of abstract argumentation frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 14:29:00 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Mailly", "Jean-Guy", ""]]}, {"id": "1603.08869", "submitter": "Tiancheng Zhao", "authors": "Tiancheng Zhao, Mohammad Gowayyed", "title": "Algorithms for Batch Hierarchical Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hierarchical Reinforcement Learning (HRL) exploits temporal abstraction to\nsolve large Markov Decision Processes (MDP) and provide transferable subtask\npolicies. In this paper, we introduce an off-policy HRL algorithm: Hierarchical\nQ-value Iteration (HQI). We show that it is possible to effectively learn\nrecursive optimal policies for any valid hierarchical decomposition of the\noriginal MDP, given a fixed dataset collected from a flat stochastic behavioral\npolicy. We first formally prove the convergence of the algorithm for tabular\nMDP. Then our experiments on the Taxi domain show that HQI converges faster\nthan a flat Q-value Iteration and enjoys easy state abstraction. Also, we\ndemonstrate that our algorithm is able to learn optimal policies for different\nhierarchical structures from the same fixed dataset, which enables model\ncomparison without recollecting data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 18:17:17 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Zhao", "Tiancheng", ""], ["Gowayyed", "Mohammad", ""]]}, {"id": "1603.08976", "submitter": "Zachary Friggstad", "authors": "Zachary Friggstad, Mohsen Rezapour, and Mohammad R. Salavatipour", "title": "Local Search Yields a PTAS for k-Means in Doubling Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most well known and ubiquitous clustering problem encountered in nearly\nevery branch of science is undoubtedly $k$-means: given a set of data points\nand a parameter $k$, select $k$ centres and partition the data points into $k$\nclusters around these centres so that the sum of squares of distances of the\npoints to their cluster centre is minimized. Typically these data points lie\n$\\mathbb{R}^d$ for some $d\\geq 2$.\n  $k$-means and the first algorithms for it were introduced in the 1950's.\nSince then, hundreds of papers have studied this problem and many algorithms\nhave been proposed for it. The most commonly used algorithm is known as\nLloyd-Forgy, which is also referred to as \"the\" $k$-means algorithm, and\nvarious extensions of it often work very well in practice. However, they may\nproduce solutions whose cost is arbitrarily large compared to the optimum\nsolution. Kanungo et al. [2004] analyzed a simple local search heuristic to get\na polynomial-time algorithm with approximation ratio $9+\\epsilon$ for any fixed\n$\\epsilon>0$ for $k$-means in Euclidean space.\n  Finding an algorithm with a better approximation guarantee has remained one\nof the biggest open questions in this area, in particular whether one can get a\ntrue PTAS for fixed dimension Euclidean space. We settle this problem by\nshowing that a simple local search algorithm provides a PTAS for $k$-means in\n$\\mathbb{R}^d$ for any fixed $d$. More precisely, for any error parameter\n$\\epsilon>0$, the local search algorithm that considers swaps of up to\n$\\rho=d^{O(d)}\\cdot{\\epsilon}^{-O(d/\\epsilon)}$ centres at a time finds a\nsolution using exactly $k$ centres whose cost is at most a\n$(1+\\epsilon)$-factor greater than the optimum.\n  Finally, we provide the first demonstration that local search yields a PTAS\nfor the uncapacitated facility location problem and $k$-median with non-uniform\nopening costs in doubling metrics.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 21:41:55 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 20:00:19 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Friggstad", "Zachary", ""], ["Rezapour", "Mohsen", ""], ["Salavatipour", "Mohammad R.", ""]]}, {"id": "1603.08988", "submitter": "Yusuf Bugra Erol", "authors": "Yusuf Bugra Erol, Yi Wu, Lei Li, Stuart Russell", "title": "Towards Practical Bayesian Parameter and State Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint state and parameter estimation is a core problem for dynamic Bayesian\nnetworks. Although modern probabilistic inference toolkits make it relatively\neasy to specify large and practically relevant probabilistic models, the silver\nbullet---an efficient and general online inference algorithm for such\nproblems---remains elusive, forcing users to write special-purpose code for\neach application. We propose a novel blackbox algorithm -- a hybrid of particle\nfiltering for state variables and assumed density filtering for parameter\nvariables. It has following advantages: (a) it is efficient due to its online\nnature, and (b) it is applicable to both discrete and continuous parameter\nspaces . On a variety of toy and real models, our system is able to generate\nmore accurate results within a fixed computation budget. This preliminary\nevidence indicates that the proposed approach is likely to be of practical use.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 22:41:17 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Erol", "Yusuf Bugra", ""], ["Wu", "Yi", ""], ["Li", "Lei", ""], ["Russell", "Stuart", ""]]}, {"id": "1603.09029", "submitter": "Nguyen Viet Cuong", "authors": "Nguyen Viet Cuong, Huan Xu", "title": "Adaptive Maximization of Pointwise Submodular Functions With Budget\n  Constraint", "comments": "This paper was published at the 30th Conference on Neural Information\n  Processing Systems (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the worst-case adaptive optimization problem with budget constraint\nthat is useful for modeling various practical applications in artificial\nintelligence and machine learning. We investigate the near-optimality of greedy\nalgorithms for this problem with both modular and non-modular cost functions.\nIn both cases, we prove that two simple greedy algorithms are not near-optimal\nbut the best between them is near-optimal if the utility function satisfies\npointwise submodularity and pointwise cost-sensitive submodularity\nrespectively. This implies a combined algorithm that is near-optimal with\nrespect to the optimal algorithm that uses half of the budget. We discuss\napplications of our theoretical results and also report experiments comparing\nthe greedy algorithms on the active learning problem.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 03:27:41 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 21:08:53 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Cuong", "Nguyen Viet", ""], ["Xu", "Huan", ""]]}, {"id": "1603.09051", "submitter": "Rahul Aralikatte", "authors": "Rahul Aralikatte and G Srinivasaraghavan", "title": "Phoenix: A Self-Optimizing Chess Engine", "comments": "Accepted in CICN 2015. Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the advent of computers, many tasks which required humans to spend a\nlot of time and energy have been trivialized by the computers' ability to\nperform repetitive tasks extremely quickly. Playing chess is one such task. It\nwas one of the first games which was `solved' using AI. With the advent of deep\nlearning, chess playing agents can surpass human ability with relative ease.\nHowever algorithms using deep learning must learn millions of parameters. This\nwork looks at the game of chess through the lens of genetic algorithms. We\ntrain a genetic player from scratch using only a handful of learnable\nparameters. We use Multi-Niche Crowding to optimize positional Value Tables\n(PVTs) which are used extensively in chess engines to evaluate the goodness of\na position. With a very simple setup and after only 1000 generations of\nevolution, the player reaches the level of an International Master.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 06:41:04 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 04:53:19 GMT"}, {"version": "v3", "created": "Wed, 16 Aug 2017 14:32:55 GMT"}, {"version": "v4", "created": "Sun, 20 Aug 2017 11:25:43 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Aralikatte", "Rahul", ""], ["Srinivasaraghavan", "G", ""]]}, {"id": "1603.09194", "submitter": "\\\"Ozg\\\"ur L\\\"utf\\\"u \\\"Oz\\c{c}ep", "authors": "\\\"Ozg\\\"ur L\\\"utf\\\"u \\\"Oz\\c{c}ep", "title": "Iterated Ontology Revision by Reinterpretation", "comments": "10 pages, 1 figure, to be published in Proceedings of the 16th\n  International Workshop on Non-Monotonic Reasoning (NMR'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterated applications of belief change operators are essential for different\nscenarios such as that of ontology evolution where new information is not\npresented at once but only in piecemeal fashion within a sequence. I discuss\niterated applications of so called reinterpretation operators that trace\nconflicts between ontologies back to the ambiguous of symbols and that provide\nconflict resolution strategies with bridging axioms. The discussion centers on\nadaptations of the classical iteration postulates according to Darwiche and\nPearl. The main result of the paper is that reinterpretation operators fulfill\nthe postulates for sequences containing only atomic triggers. For complex\ntriggers, a fulfillment is not guaranteed and indeed there are different\nreasons for the different postulates why they should not be fulfilled in the\nparticular scenario of ontology revision with well developed ontologies.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 13:50:13 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["\u00d6z\u00e7ep", "\u00d6zg\u00fcr L\u00fctf\u00fc", ""]]}, {"id": "1603.09405", "submitter": "Peng Li", "authors": "Peng Li and Heng Huang", "title": "Enhancing Sentence Relation Modeling with Auxiliary Character-level\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based approaches for sentence relation modeling automatically\ngenerate hidden matching features from raw sentence pairs. However, the quality\nof matching feature representation may not be satisfied due to complex semantic\nrelations such as entailment or contradiction. To address this challenge, we\npropose a new deep neural network architecture that jointly leverage\npre-trained word embedding and auxiliary character embedding to learn sentence\nmeanings. The two kinds of word sequence representations as inputs into\nmulti-layer bidirectional LSTM to learn enhanced sentence representation. After\nthat, we construct matching features followed by another temporal CNN to learn\nhigh-level hidden matching feature representations. Experimental results\ndemonstrate that our approach consistently outperforms the existing methods on\nstandard evaluation datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 22:39:59 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Li", "Peng", ""], ["Huang", "Heng", ""]]}, {"id": "1603.09429", "submitter": "Aaron Hunter", "authors": "Aaron Hunter", "title": "Ordinal Conditional Functions for Nearly Counterfactual Revision", "comments": "7 pages, 1 figure, presented at the International Workshop on\n  Non-monotonic Reasoning 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in belief revision involving conditional statements where\nthe antecedent is almost certainly false. To represent such problems, we use\nOrdinal Conditional Functions that may take infinite values. We model belief\nchange in this context through simple arithmetical operations that allow us to\ncapture the intuition that certain antecedents can not be validated by any\nnumber of observations. We frame our approach as a form of finite belief\nimprovement, and we propose a model of conditional belief revision in which\nonly the \"right\" hypothetical levels of implausibility are revised.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 00:48:40 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Hunter", "Aaron", ""]]}, {"id": "1603.09465", "submitter": "Zhiqiang Zhuang", "authors": "Zhiqiang Zhuang, James Delgrande, Abhaya Nayak, Abdul Sattar", "title": "A New Approach for Revising Logic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief revision has been studied mainly with respect to background logics\nthat are monotonic in character. In this paper we study belief revision when\nthe underlying logic is non-monotonic instead--an inherently interesting\nproblem that is under explored. In particular, we will focus on the revision of\na body of beliefs that is represented as a logic program under the answer set\nsemantics, while the new information is also similarly represented as a logic\nprogram. Our approach is driven by the observation that unlike in a monotonic\nsetting where, when necessary, consistency in a revised body of beliefs is\nmaintained by jettisoning some old beliefs, in a non-monotonic setting\nconsistency can be restored by adding new beliefs as well. We will define a\nsyntactic revision function and subsequently provide representation theorem for\ncharacterising it.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 06:27:16 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Zhuang", "Zhiqiang", ""], ["Delgrande", "James", ""], ["Nayak", "Abhaya", ""], ["Sattar", "Abdul", ""]]}, {"id": "1603.09488", "submitter": "Andrey Luxemburg", "authors": "Andrey Luxemburg", "title": "Building the Signature of Set Theory Using the MathSem Program", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge representation is a popular research field in IT. As mathematical\nknowledge is most formalized, its representation is important and interesting.\nMathematical knowledge consists of various mathematical theories. In this paper\nwe consider a deductive system that derives mathematical notions, axioms and\ntheorems. All these notions, axioms and theorems can be considered as the part\nof elementary set theory. This theory will be represented as a semantic net.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 08:41:22 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Luxemburg", "Andrey", ""]]}, {"id": "1603.09495", "submitter": "Zeynep G\\\"ozen Saribatur", "authors": "Zeynep G. Saribatur, Thomas Eiter", "title": "Reactive Policies with Planning for Action Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a representation in a high-level transition system for policies\nthat express a reactive behavior for the agent. We consider a target decision\ncomponent that figures out what to do next and an (online) planning capability\nto compute the plans needed to reach these targets. Our representation allows\none to analyze the flow of executing the given reactive policy, and to\ndetermine whether it works as expected. Additionally, the flexibility of the\nrepresentation opens a range of possibilities for designing behaviors.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 09:05:28 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Saribatur", "Zeynep G.", ""], ["Eiter", "Thomas", ""]]}, {"id": "1603.09502", "submitter": "Thomas Linsbichler", "authors": "Ringo Baumann, Thomas Linsbichler and Stefan Woltran", "title": "Verifiability of Argumentation Semantics", "comments": "Contribution to the 16h International Workshop on Non-Monotonic\n  Reasoning, 2016, Cape Town", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dung's abstract argumentation theory is a widely used formalism to model\nconflicting information and to draw conclusions in such situations. Hereby, the\nknowledge is represented by so-called argumentation frameworks (AFs) and the\nreasoning is done via semantics extracting acceptable sets. All reasonable\nsemantics are based on the notion of conflict-freeness which means that\narguments are only jointly acceptable when they are not linked within the AF.\nIn this paper, we study the question which information on top of conflict-free\nsets is needed to compute extensions of a semantics at hand. We introduce a\nhierarchy of so-called verification classes specifying the required amount of\ninformation. We show that well-known standard semantics are exactly verifiable\nthrough a certain such class. Our framework also gives a means to study\nsemantics lying inbetween known semantics, thus contributing to a more abstract\nunderstanding of the different features argumentation semantics offer.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 09:29:02 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Baumann", "Ringo", ""], ["Linsbichler", "Thomas", ""], ["Woltran", "Stefan", ""]]}, {"id": "1603.09511", "submitter": "Jean-Guy Mailly", "authors": "Adrian Haret, Jean-Guy Mailly, Stefan Woltran", "title": "Distributing Knowledge into Simple Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the behavior of belief change operators for fragments of\nclassical logic has received increasing interest over the last years. Results\nin this direction are mainly concerned with adapting representation theorems.\nHowever, fragment-driven belief change also leads to novel research questions.\nIn this paper we propose the concept of belief distribution, which can be\nunderstood as the reverse task of merging. More specifically, we are interested\nin the following question: given an arbitrary knowledge base $K$ and some\nmerging operator $\\Delta$, can we find a profile $E$ and a constraint $\\mu$,\nboth from a given fragment of classical logic, such that $\\Delta_\\mu(E)$ yields\na result equivalent to $K$? In other words, we are interested in seeing if $K$\ncan be distributed into knowledge bases of simpler structure, such that the\ntask of merging allows for a reconstruction of the original knowledge. Our\ninitial results show that merging based on drastic distance allows for an easy\ndistribution of knowledge, while the power of distribution for operators based\non Hamming distance relies heavily on the fragment of choice.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 09:59:02 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Haret", "Adrian", ""], ["Mailly", "Jean-Guy", ""], ["Woltran", "Stefan", ""]]}, {"id": "1603.09545", "submitter": "Thomas Linsbichler", "authors": "Thomas Linsbichler, J\\\"org P\\\"uhrer and Hannes Strass", "title": "Characterizing Realizability in Abstract Argumentation", "comments": "Contribution to the 16h International Workshop on Non-Monotonic\n  Reasoning, 2016, Cape Town", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realizability for knowledge representation formalisms studies the following\nquestion: given a semantics and a set of interpretations, is there a knowledge\nbase whose semantics coincides exactly with the given interpretation set? We\nintroduce a general framework for analyzing realizability in abstract\ndialectical frameworks (ADFs) and various of its subclasses. In particular, the\nframework applies to Dung argumentation frameworks, SETAFs by Nielsen and\nParsons, and bipolar ADFs. We present a uniform characterization method for the\nadmissible, complete, preferred and model/stable semantics. We employ this\nmethod to devise an algorithm that decides realizability for the mentioned\nformalisms and semantics; moreover the algorithm allows for constructing a\ndesired knowledge base whenever one exists. The algorithm is built in a modular\nway and thus easily extensible to new formalisms and semantics. We have also\nimplemented our approach in answer set programming, and used the implementation\nto obtain several novel results on the relative expressiveness of the\nabovementioned formalisms.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 12:05:34 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Linsbichler", "Thomas", ""], ["P\u00fchrer", "J\u00f6rg", ""], ["Strass", "Hannes", ""]]}, {"id": "1603.09727", "submitter": "Ziang Xie", "authors": "Ziang Xie, Anand Avati, Naveen Arivazhagan, Dan Jurafsky, Andrew Y. Ng", "title": "Neural Language Correction with Character-Based Attention", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language correction has the potential to help language learners\nimprove their writing skills. While approaches with separate classifiers for\ndifferent error types have high precision, they do not flexibly handle errors\nsuch as redundancy or non-idiomatic phrasing. On the other hand, word and\nphrase-based machine translation methods are not designed to cope with\northographic errors, and have recently been outpaced by neural models.\nMotivated by these issues, we present a neural network-based approach to\nlanguage correction. The core component of our method is an encoder-decoder\nrecurrent neural network with an attention mechanism. By operating at the\ncharacter level, the network avoids the problem of out-of-vocabulary words. We\nillustrate the flexibility of our approach on dataset of noisy, user-generated\ntext collected from an English learner forum. When combined with a language\nmodel, our method achieves a state-of-the-art $F_{0.5}$-score on the CoNLL 2014\nShared Task. We further demonstrate that training the network on additional\ndata with synthesized errors can improve performance.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 19:16:54 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Xie", "Ziang", ""], ["Avati", "Anand", ""], ["Arivazhagan", "Naveen", ""], ["Jurafsky", "Dan", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1603.09728", "submitter": "Shafi'i Muhammad Abdulhamid Mr", "authors": "Shafii Muhammad Abdulhamid, Muhammad Shafie Abd Latiff, Syed Hamid\n  Hussain Madni, Osho Oluwafemi", "title": "A Survey of League Championship Algorithm: Prospects and Challenges", "comments": "10 pages, 2 figures, 2 tables, Indian Journal of Science and\n  Technology, 2015", "journal-ref": null, "doi": "10.17485/ijst/2015/v8iS3/60476", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The League Championship Algorithm (LCA) is sport-inspired optimization\nalgorithm that was introduced by Ali Husseinzadeh Kashan in the year 2009. It\nhas since drawn enormous interest among the researchers because of its\npotential efficiency in solving many optimization problems and real-world\napplications. The LCA has also shown great potentials in solving\nnon-deterministic polynomial time (NP-complete) problems. This survey presents\na brief synopsis of the LCA literatures in peer-reviewed journals, conferences\nand book chapters. These research articles are then categorized according to\nindexing in the major academic databases (Web of Science, Scopus, IEEE Xplore\nand the Google Scholar). The analysis was also done to explore the prospects\nand the challenges of the algorithm and its acceptability among researchers.\nThis systematic categorization can be used as a basis for future studies.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 10:09:11 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Abdulhamid", "Shafii Muhammad", ""], ["Latiff", "Muhammad Shafie Abd", ""], ["Madni", "Syed Hamid Hussain", ""], ["Oluwafemi", "Osho", ""]]}]