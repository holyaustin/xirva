[{"id": "0906.0052", "submitter": "Brian Tomasik", "authors": "Brian Tomasik", "title": "A Minimum Description Length Approach to Multitask Feature Selection", "comments": "29 pages, 3 figures, undergraduate thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Many regression problems involve not one but several response variables\n(y's). Often the responses are suspected to share a common underlying\nstructure, in which case it may be advantageous to share information across\nthem; this is known as multitask learning. As a special case, we can use\nmultiple responses to better identify shared predictive features -- a project\nwe might call multitask feature selection.\n  This thesis is organized as follows. Section 1 introduces feature selection\nfor regression, focusing on ell_0 regularization methods and their\ninterpretation within a Minimum Description Length (MDL) framework. Section 2\nproposes a novel extension of MDL feature selection to the multitask setting.\nThe approach, called the \"Multiple Inclusion Criterion\" (MIC), is designed to\nborrow information across regression tasks by more easily selecting features\nthat are associated with multiple responses. We show in experiments on\nsynthetic and real biological data sets that MIC can reduce prediction error in\nsettings where features are at least partially shared across responses. Section\n3 surveys hypothesis testing by regression with a single response, focusing on\nthe parallel between the standard Bonferroni correction and an MDL approach.\nMirroring the ideas in Section 2, Section 4 proposes a novel MIC approach to\nhypothesis testing with multiple responses and shows that on synthetic data\nwith significant sharing of features across responses, MIC sometimes\noutperforms standard FDR-controlling methods in terms of finding true positives\nfor a given level of false positives. Section 5 concludes.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2009 03:41:37 GMT"}], "update_date": "2009-06-02", "authors_parsed": [["Tomasik", "Brian", ""]]}, {"id": "0906.0311", "submitter": "Christophe Paoli", "authors": "Christophe Paoli, Cyril Voyant, Marc Muselli, Marie-Laure Nivet", "title": "Solar radiation forecasting using ad-hoc time series preprocessing and\n  neural networks", "comments": "14 pages, 8 figures, 2009 International Conference on Intelligent\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NA physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an application of neural networks in the renewable\nenergy domain. We have developed a methodology for the daily prediction of\nglobal solar radiation on a horizontal surface. We use an ad-hoc time series\npreprocessing and a Multi-Layer Perceptron (MLP) in order to predict solar\nradiation at daily horizon. First results are promising with nRMSE < 21% and\nRMSE < 998 Wh/m2. Our optimized MLP presents prediction similar to or even\nbetter than conventional methods such as ARIMA techniques, Bayesian inference,\nMarkov chains and k-Nearest-Neighbors approximators. Moreover we found that our\ndata preprocessing approach can reduce significantly forecasting errors.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2009 16:02:10 GMT"}], "update_date": "2009-06-02", "authors_parsed": [["Paoli", "Christophe", ""], ["Voyant", "Cyril", ""], ["Muselli", "Marc", ""], ["Nivet", "Marie-Laure", ""]]}, {"id": "0906.0885", "submitter": "Yongxin Tong", "authors": "Yongxin Tong, Li Zhao, Dan Yu, Shilong Ma, Ke Xu", "title": "Mining Compressed Repetitive Gapped Sequential Patterns Efficiently", "comments": "19 pages, 7 figures", "journal-ref": "The 5th International Conference on Advanced Data Mining and\n  Applications (ADMA2009)", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent sequential patterns from sequence databases has been a\ncentral research topic in data mining and various efficient mining sequential\npatterns algorithms have been proposed and studied. Recently, in many problem\ndomains (e.g, program execution traces), a novel sequential pattern mining\nresearch, called mining repetitive gapped sequential patterns, has attracted\nthe attention of many researchers, considering not only the repetition of\nsequential pattern in different sequences but also the repetition within a\nsequence is more meaningful than the general sequential pattern mining which\nonly captures occurrences in different sequences. However, the number of\nrepetitive gapped sequential patterns generated by even these closed mining\nalgorithms may be too large to understand for users, especially when support\nthreshold is low. In this paper, we propose and study the problem of\ncompressing repetitive gapped sequential patterns. Inspired by the ideas of\nsummarizing frequent itemsets, RPglobal, we develop an algorithm, CRGSgrow\n(Compressing Repetitive Gapped Sequential pattern grow), including an efficient\npruning strategy, SyncScan, and an efficient representative pattern checking\nscheme, -dominate sequential pattern checking. The CRGSgrow is a two-step\napproach: in the first step, we obtain all closed repetitive sequential\npatterns as the candidate set of representative repetitive sequential patterns,\nand at the same time get the most of representative repetitive sequential\npatterns; in the second step, we only spend a little time in finding the\nremaining the representative patterns from the candidate set. An empirical\nstudy with both real and synthetic data sets clearly shows that the CRGSgrow\nhas good performance.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2009 11:17:34 GMT"}], "update_date": "2009-06-05", "authors_parsed": [["Tong", "Yongxin", ""], ["Zhao", "Li", ""], ["Yu", "Dan", ""], ["Ma", "Shilong", ""], ["Xu", "Ke", ""]]}, {"id": "0906.1182", "submitter": "Paolo Mancarella", "authors": "P. Mancarella, G. Terreni, F. Sadri, F. Toni, U. Endriss", "title": "The CIFF Proof Procedure for Abductive Logic Programming with\n  Constraints: Theory, Implementation and Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the CIFF proof procedure for abductive logic programming with\nconstraints, and we prove its correctness. CIFF is an extension of the IFF\nproof procedure for abductive logic programming, relaxing the original\nrestrictions over variable quantification (allowedness conditions) and\nincorporating a constraint solver to deal with numerical constraints as in\nconstraint logic programming. Finally, we describe the CIFF system, comparing\nit with state of the art abductive systems and answer set solvers and showing\nhow to use it to program some applications. (To appear in Theory and Practice\nof Logic Programming - TPLP).\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2009 16:13:23 GMT"}], "update_date": "2009-06-08", "authors_parsed": [["Mancarella", "P.", ""], ["Terreni", "G.", ""], ["Sadri", "F.", ""], ["Toni", "F.", ""], ["Endriss", "U.", ""]]}, {"id": "0906.1593", "submitter": "Didehvar Farzaad", "authors": "Farzad Didehvar", "title": "On Defining 'I' \"I logy\"", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Could we define I? Throughout this article we give a negative answer to this\nquestion. More exactly, we show that there is no definition for I in a certain\nway. But this negative answer depends on our definition of definability. Here,\nwe try to consider sufficient generalized definition of definability. In the\nmiddle of paper a paradox will arise which makes us to modify the way we use\nthe concept of property and definability.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2009 04:34:54 GMT"}], "update_date": "2009-06-10", "authors_parsed": [["Didehvar", "Farzad", ""]]}, {"id": "0906.1673", "submitter": "Victor Odumuyiwa", "authors": "Bolanle Oladejo (LORIA), Adenike Osofisan (LORIA), Victor Odumuyiwa\n  (LORIA)", "title": "Knowledge Management in Economic Intelligence with Reasoning on Temporal\n  Attributes", "comments": null, "journal-ref": "VSST 2009, S\\'eminaire on Veille Strat\\'egique Scientifique et\n  Technologique (2009)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People have to make important decisions within a time frame. Hence, it is\nimperative to employ means or strategy to aid effective decision making.\nConsequently, Economic Intelligence (EI) has emerged as a field to aid\nstrategic and timely decision making in an organization. In the course of\nattaining this goal: it is indispensable to be more optimistic towards\nprovision for conservation of intellectual resource invested into the process\nof decision making. This intellectual resource is nothing else but the\nknowledge of the actors as well as that of the various processes for effecting\ndecision making. Knowledge has been recognized as a strategic economic resource\nfor enhancing productivity and a key for innovation in any organization or\ncommunity. Thus, its adequate management with cognizance of its temporal\nproperties is highly indispensable. Temporal properties of knowledge refer to\nthe date and time (known as timestamp) such knowledge is created as well as the\nduration or interval between related knowledge. This paper focuses on the needs\nfor a user-centered knowledge management approach as well as exploitation of\nassociated temporal properties. Our perspective of knowledge is with respect to\ndecision-problems projects in EI. Our hypothesis is that the possibility of\nreasoning about temporal properties in exploitation of knowledge in EI projects\nshould foster timely decision making through generation of useful inferences\nfrom available and reusable knowledge for a new project.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2009 09:33:16 GMT"}], "update_date": "2009-06-10", "authors_parsed": [["Oladejo", "Bolanle", "", "LORIA"], ["Osofisan", "Adenike", "", "LORIA"], ["Odumuyiwa", "Victor", "", "LORIA"]]}, {"id": "0906.1694", "submitter": "Nikolaj Glazunov", "authors": "Nikolaj Glazunov", "title": "Toward a Category Theory Design of Ontological Knowledge Bases", "comments": "10 pages, Preliminary results to International Joint Conference on\n  Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K\n  2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I discuss (ontologies_and_ontological_knowledge_bases /\nformal_methods_and_theories) duality and its category theory extensions as a\nstep toward a solution to Knowledge-Based Systems Theory. In particular I focus\non the example of the design of elements of ontologies and ontological\nknowledge bases of next three electronic courses: Foundations of Research\nActivities, Virtual Modeling of Complex Systems and Introduction to String\nTheory.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2009 11:03:41 GMT"}], "update_date": "2009-06-10", "authors_parsed": [["Glazunov", "Nikolaj", ""]]}, {"id": "0906.1713", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Feature Reinforcement Learning: Part I: Unstructured MDPs", "comments": "24 LaTeX pages, 5 diagrams", "journal-ref": "Journal of Artificial General Intelligence, 1 (2009) pages 3-24", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General-purpose, intelligent, learning agents cycle through sequences of\nobservations, actions, and rewards that are complex, uncertain, unknown, and\nnon-Markovian. On the other hand, reinforcement learning is well-developed for\nsmall finite state Markov decision processes (MDPs). Up to now, extracting the\nright state representations out of bare observations, that is, reducing the\ngeneral agent setup to the MDP framework, is an art that involves significant\neffort by designers. The primary goal of this work is to automate the reduction\nprocess and thereby significantly expand the scope of many existing\nreinforcement learning algorithms and the agents that employ them. Before we\ncan think of mechanizing this search for suitable MDPs, we need a formal\nobjective criterion. The main contribution of this article is to develop such a\ncriterion. I also integrate the various parts into one learning algorithm.\nExtensions to more realistic dynamic Bayesian networks are developed in Part\nII. The role of POMDPs is also considered there.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2009 12:50:29 GMT"}], "update_date": "2009-12-30", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "0906.1814", "submitter": "Renqiang Min", "authors": "Martin Renqiang Min, David A. Stanley, Zineng Yuan, Anthony Bonner,\n  and Zhaolei Zhang", "title": "Large-Margin kNN Classification Using a Deep Encoder Network", "comments": "13 pages (preliminary version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  KNN is one of the most popular classification methods, but it often fails to\nwork well with inappropriate choice of distance metric or due to the presence\nof numerous class-irrelevant features. Linear feature transformation methods\nhave been widely applied to extract class-relevant information to improve kNN\nclassification, which is very limited in many applications. Kernels have been\nused to learn powerful non-linear feature transformations, but these methods\nfail to scale to large datasets. In this paper, we present a scalable\nnon-linear feature mapping method based on a deep neural network pretrained\nwith restricted boltzmann machines for improving kNN classification in a\nlarge-margin framework, which we call DNet-kNN. DNet-kNN can be used for both\nclassification and for supervised dimensionality reduction. The experimental\nresults on two benchmark handwritten digit datasets show that DNet-kNN has much\nbetter performance than large-margin kNN using a linear mapping and kNN based\non a deep autoencoder pretrained with retricted boltzmann machines.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2009 20:06:45 GMT"}], "update_date": "2009-06-11", "authors_parsed": [["Min", "Martin Renqiang", ""], ["Stanley", "David A.", ""], ["Yuan", "Zineng", ""], ["Bonner", "Anthony", ""], ["Zhang", "Zhaolei", ""]]}, {"id": "0906.1842", "submitter": "Arash Shaban-Nejad", "authors": "Arash Shaban-Nejad, Olga Ormandjieva, Mohamad Kassab, Volker Haarslev", "title": "Managing Requirement Volatility in an Ontology-Driven Clinical LIMS\n  Using Category Theory. International Journal of Telemedicine and Applications", "comments": "36 Pages, 16 Figures", "journal-ref": "International Journal of Telemedicine and Applications, Volume\n  2009, Article ID 917826, 14 pages, PubMed ID: 19343191", "doi": "10.1155/2009/917826", "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Requirement volatility is an issue in software engineering in general, and in\nWeb-based clinical applications in particular, which often originates from an\nincomplete knowledge of the domain of interest. With advances in the health\nscience, many features and functionalities need to be added to, or removed\nfrom, existing software applications in the biomedical domain. At the same\ntime, the increasing complexity of biomedical systems makes them more difficult\nto understand, and consequently it is more difficult to define their\nrequirements, which contributes considerably to their volatility. In this\npaper, we present a novel agent-based approach for analyzing and managing\nvolatile and dynamic requirements in an ontology-driven laboratory information\nmanagement system (LIMS) designed for Web-based case reporting in medical\nmycology. The proposed framework is empowered with ontologies and formalized\nusing category theory to provide a deep and common understanding of the\nfunctional and nonfunctional requirement hierarchies and their interrelations,\nand to trace the effects of a change on the conceptual framework.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2009 00:44:20 GMT"}], "update_date": "2009-06-11", "authors_parsed": [["Shaban-Nejad", "Arash", ""], ["Ormandjieva", "Olga", ""], ["Kassab", "Mohamad", ""], ["Haarslev", "Volker", ""]]}, {"id": "0906.1845", "submitter": "Serguei Mokhov", "authors": "Serguei A. Mokhov", "title": "Towards Improving Validation, Verification, Crash Investigations, and\n  Event Reconstruction of Flight-Critical Systems with Self-Forensics", "comments": "10 pages; a white discussion paper submitted in response to NASA's\n  RFI NNH09ZEA001L at\n  http://prod.nais.nasa.gov/cgi-bin/eps/synopsis.cgi?acqid=134490", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel concept of self-forensics to complement the\nstandard autonomic self-CHOP properties of the self-managed systems, to be\nspecified in the Forensic Lucid language. We argue that self-forensics, with\nthe forensics taken out of the cybercrime domain, is applicable to\n\"self-dissection\" for the purpose of verification of autonomous software and\nhardware systems of flight-critical systems for automated incident and anomaly\nanalysis and event reconstruction by the engineering teams in a variety of\nincident scenarios during design and testing as well as actual flight data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2009 06:30:48 GMT"}], "update_date": "2009-06-11", "authors_parsed": [["Mokhov", "Serguei A.", ""]]}, {"id": "0906.1980", "submitter": "Aram Galstyan", "authors": "Armen Allahverdyan, Aram Galstyan", "title": "On Maximum a Posteriori Estimation of Hidden Markov Processes", "comments": "9 pages, to appear in the Proceedings of the 25th Conference on\n  Uncertainty in Artificial Intelligence (UAI-2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cond-mat.stat-mech cs.IT math.IT physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretical analysis of Maximum a Posteriori (MAP) sequence\nestimation for binary symmetric hidden Markov processes. We reduce the MAP\nestimation to the energy minimization of an appropriately defined Ising spin\nmodel, and focus on the performance of MAP as characterized by its accuracy and\nthe number of solutions corresponding to a typical observed sequence. It is\nshown that for a finite range of sufficiently low noise levels, the solution is\nuniquely related to the observed sequence, while the accuracy degrades linearly\nwith increasing the noise strength. For intermediate noise values, the accuracy\nis nearly noise-independent, but now there are exponentially many solutions to\nthe estimation problem, which is reflected in non-zero ground-state entropy for\nthe Ising model. Finally, for even larger noise intensities, the number of\nsolutions reduces again, but the accuracy is poor. It is shown that these\nregimes are different thermodynamic phases of the Ising model that are related\nto each other via first-order phase transitions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2009 17:23:12 GMT"}], "update_date": "2009-06-11", "authors_parsed": [["Allahverdyan", "Armen", ""], ["Galstyan", "Aram", ""]]}, {"id": "0906.2154", "submitter": "Giorgi Japaridze", "authors": "Giorgi Japaridze (School of Computer Science and Technology, Shandong\n  University, Department of Co)", "title": "From formulas to cirquents in computability logic", "comments": "LMCS 7 (2:1) 2011", "journal-ref": "Logical Methods in Computer Science, Volume 7, Issue 2 (April 21,\n  2011) lmcs:1121", "doi": "10.2168/LMCS-7(2:1)2011", "report-no": null, "categories": "cs.LO cs.AI cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computability logic (CoL) (see http://www.cis.upenn.edu/~giorgi/cl.html) is a\nrecently introduced semantical platform and ambitious program for redeveloping\nlogic as a formal theory of computability, as opposed to the formal theory of\ntruth that logic has more traditionally been. Its expressions represent\ninteractive computational tasks seen as games played by a machine against the\nenvironment, and \"truth\" is understood as existence of an algorithmic winning\nstrategy. With logical operators standing for operations on games, the\nformalism of CoL is open-ended, and has already undergone series of extensions.\nThis article extends the expressive power of CoL in a qualitatively new way,\ngeneralizing formulas (to which the earlier languages of CoL were limited) to\ncircuit-style structures termed cirquents. The latter, unlike formulas, are\nable to account for subgame/subtask sharing between different parts of the\noverall game/task. Among the many advantages offered by this ability is that it\nallows us to capture, refine and generalize the well known\nindependence-friendly logic which, after the present leap forward, naturally\nbecomes a conservative fragment of CoL, just as classical logic had been known\nto be a conservative fragment of the formula-based version of CoL. Technically,\nthis paper is self-contained, and can be read without any prior familiarity\nwith CoL.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2009 16:34:51 GMT"}, {"version": "v2", "created": "Tue, 17 Aug 2010 10:16:42 GMT"}, {"version": "v3", "created": "Sat, 1 Jan 2011 22:31:55 GMT"}, {"version": "v4", "created": "Wed, 20 Apr 2011 16:44:25 GMT"}, {"version": "v5", "created": "Fri, 22 Apr 2011 07:31:58 GMT"}, {"version": "v6", "created": "Mon, 25 Apr 2011 21:46:55 GMT"}, {"version": "v7", "created": "Wed, 27 Apr 2011 16:46:20 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Japaridze", "Giorgi", "", "School of Computer Science and Technology, Shandong\n  University, Department of Co"]]}, {"id": "0906.2228", "submitter": "Hans Tompits", "authors": "David Pearce, Hans Tompits, Stefan Woltran", "title": "Characterising equilibrium logic and nested logic programs: Reductions\n  and complexity", "comments": null, "journal-ref": "Theory and Practice of Logic Programming (2009), 9 : 565-616", "doi": "10.1017/S147106840999010X", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equilibrium logic is an approach to nonmonotonic reasoning that extends the\nstable-model and answer-set semantics for logic programs. In particular, it\nincludes the general case of nested logic programs, where arbitrary Boolean\ncombinations are permitted in heads and bodies of rules, as special kinds of\ntheories. In this paper, we present polynomial reductions of the main reasoning\ntasks associated with equilibrium logic and nested logic programs into\nquantified propositional logic, an extension of classical propositional logic\nwhere quantifications over atomic formulas are permitted. We provide reductions\nnot only for decision problems, but also for the central semantical concepts of\nequilibrium logic and nested logic programs. In particular, our encodings map a\ngiven decision problem into some formula such that the latter is valid\nprecisely in case the former holds. The basic tasks we deal with here are the\nconsistency problem, brave reasoning, and skeptical reasoning. Additionally, we\nalso provide encodings for testing equivalence of theories or programs under\ndifferent notions of equivalence, viz. ordinary, strong, and uniform\nequivalence. For all considered reasoning tasks, we analyse their computational\ncomplexity and give strict complexity bounds.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2009 23:24:43 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2009 17:47:12 GMT"}], "update_date": "2009-12-30", "authors_parsed": [["Pearce", "David", ""], ["Tompits", "Hans", ""], ["Woltran", "Stefan", ""]]}, {"id": "0906.2274", "submitter": "D\\v{z}enan Zuki\\'c", "authors": "D\\v{z}enan Zuki\\'c, Christof Rezk-Salama, Andreas Kolb", "title": "A Neural Network Classifier of Volume Datasets", "comments": "10 pages, 10 figures, 1 table, 3IA conference http://3ia.teiath.gr/", "journal-ref": "International Conference on Computer Graphics and Artificial\n  Intelligence, Proceedings (2009) 53-62", "doi": null, "report-no": null, "categories": "cs.GR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the art visualization techniques must be tailored to the\nspecific type of dataset, its modality (CT, MRI, etc.), the recorded object or\nanatomical region (head, spine, abdomen, etc.) and other parameters related to\nthe data acquisition process. While parts of the information (imaging modality\nand acquisition sequence) may be obtained from the meta-data stored with the\nvolume scan, there is important information which is not stored explicitly\n(anatomical region, tracing compound). Also, meta-data might be incomplete,\ninappropriate or simply missing.\n  This paper presents a novel and simple method of determining the type of\ndataset from previously defined categories. 2D histograms based on intensity\nand gradient magnitude of datasets are used as input to a neural network, which\nclassifies it into one of several categories it was trained with. The proposed\nmethod is an important building block for visualization systems to be used\nautonomously by non-experts. The method has been tested on 80 datasets, divided\ninto 3 classes and a \"rest\" class.\n  A significant result is the ability of the system to classify datasets into a\nspecific class after being trained with only one dataset of that class. Other\nadvantages of the method are its easy implementation and its high computational\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2009 11:17:05 GMT"}], "update_date": "2009-06-15", "authors_parsed": [["Zuki\u0107", "D\u017eenan", ""], ["Rezk-Salama", "Christof", ""], ["Kolb", "Andreas", ""]]}, {"id": "0906.2459", "submitter": "Vit Niennattrakul", "authors": "Vit Niennattrakul, Pongsakorn Ruengronghirunya, Chotirat Ann\n  Ratanamahatana", "title": "Exact Indexing for Massive Time Series Databases under Time Warping\n  Distance", "comments": "Submitted to Data Mining and Knowledge Discovery (DMKD). 33 pages, 19\n  figures, and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among many existing distance measures for time series data, Dynamic Time\nWarping (DTW) distance has been recognized as one of the most accurate and\nsuitable distance measures due to its flexibility in sequence alignment.\nHowever, DTW distance calculation is computationally intensive. Especially in\nvery large time series databases, sequential scan through the entire database\nis definitely impractical, even with random access that exploits some index\nstructures since high dimensionality of time series data incurs extremely high\nI/O cost. More specifically, a sequential structure consumes high CPU but low\nI/O costs, while an index structure requires low CPU but high I/O costs. In\nthis work, we therefore propose a novel indexed sequential structure called\nTWIST (Time Warping in Indexed Sequential sTructure) which benefits from both\nsequential access and index structure. When a query sequence is issued, TWIST\ncalculates lower bounding distances between a group of candidate sequences and\nthe query sequence, and then identifies the data access order in advance, hence\nreducing a great number of both sequential and random accesses. Impressively,\nour indexed sequential structure achieves significant speedup in a querying\nprocess by a few orders of magnitude. In addition, our method shows superiority\nover existing rival methods in terms of query processing time, number of page\naccesses, and storage requirement with no false dismissal guaranteed.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2009 09:07:05 GMT"}], "update_date": "2009-06-16", "authors_parsed": [["Niennattrakul", "Vit", ""], ["Ruengronghirunya", "Pongsakorn", ""], ["Ratanamahatana", "Chotirat Ann", ""]]}, {"id": "0906.2824", "submitter": "Carlos Gershenson", "authors": "Carlos Gershenson", "title": "What Does Artificial Life Tell Us About Death?", "comments": "5 pages", "journal-ref": "International Journal of Artificial Life Research 2(3):1-5. 2011", "doi": null, "report-no": null, "categories": "cs.AI cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short philosophical essay\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2009 23:47:28 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Gershenson", "Carlos", ""]]}, {"id": "0906.3036", "submitter": "Gilles Champenois", "authors": "Gilles Champenois", "title": "Mnesors for automatic control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mnesors are defined as elements of a semimodule over the min-plus integers.\nThis two-sorted structure is able to merge graduation properties of vectors and\nidempotent properties of boolean numbers, which makes it appropriate for hybrid\nsystems. We apply it to the control of an inverted pendulum and design a full\nlogical controller, that is, without the usual algebra of real numbers.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2009 22:05:57 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2009 21:19:23 GMT"}], "update_date": "2009-10-09", "authors_parsed": [["Champenois", "Gilles", ""]]}, {"id": "0906.3149", "submitter": "David Tolpin", "authors": "David Tolpin, Solomon Eyal Shimony", "title": "Semi-Myopic Sensing Plans for Value Optimization", "comments": "9 pages, 4 figures, presented at BISFAI 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following sequential decision problem. Given a set of items\nof unknown utility, we need to select one of as high a utility as possible\n(``the selection problem''). Measurements (possibly noisy) of item values prior\nto selection are allowed, at a known cost. The goal is to optimize the overall\nsequential decision process of measurements and selection.\n  Value of information (VOI) is a well-known scheme for selecting measurements,\nbut the intractability of the problem typically leads to using myopic VOI\nestimates. In the selection problem, myopic VOI frequently badly underestimates\nthe value of information, leading to inferior sensing plans. We relax the\nstrict myopic assumption into a scheme we term semi-myopic, providing a\nspectrum of methods that can improve the performance of sensing plans. In\nparticular, we propose the efficiently computable method of ``blinkered'' VOI,\nand examine theoretical bounds for special cases. Empirical evaluation of\n``blinkered'' VOI in the selection problem with normally distributed item\nvalues shows that is performs much better than pure myopic VOI.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2009 11:45:40 GMT"}], "update_date": "2009-06-18", "authors_parsed": [["Tolpin", "David", ""], ["Shimony", "Solomon Eyal", ""]]}, {"id": "0906.3461", "submitter": "Martin Drozda", "authors": "Martin Drozda, Sven Schaust, Helena Szczerbicka", "title": "AIS for Misbehavior Detection in Wireless Sensor Networks: Performance\n  and Design Principles", "comments": "16 pages, 20 figures, a full version of our IEEE CEC 2007 paper", "journal-ref": null, "doi": "10.1109/CEC.2007.4424955", "report-no": null, "categories": "cs.NI cs.AI cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sensor network is a collection of wireless devices that are able to monitor\nphysical or environmental conditions. These devices (nodes) are expected to\noperate autonomously, be battery powered and have very limited computational\ncapabilities. This makes the task of protecting a sensor network against\nmisbehavior or possible malfunction a challenging problem. In this document we\ndiscuss performance of Artificial immune systems (AIS) when used as the\nmechanism for detecting misbehavior.\n  We show that (i) mechanism of the AIS have to be carefully applied in order\nto avoid security weaknesses, (ii) the choice of genes and their interaction\nhave a profound influence on the performance of the AIS, (iii) randomly created\ndetectors do not comply with limitations imposed by communications protocols\nand (iv) the data traffic pattern seems not to impact significantly the overall\nperformance.\n  We identified a specific MAC layer based gene that showed to be especially\nuseful for detection; genes measure a network's performance from a node's\nviewpoint. Furthermore, we identified an interesting complementarity property\nof genes; this property exploits the local nature of sensor networks and moves\nthe burden of excessive communication from normally behaving nodes to\nmisbehaving nodes. These results have a direct impact on the design of AIS for\nsensor networks and on engineering of sensor networks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2009 15:31:29 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Drozda", "Martin", ""], ["Schaust", "Sven", ""], ["Szczerbicka", "Helena", ""]]}, {"id": "0906.3722", "submitter": "Nidhal Bouaynaya", "authors": "Nidhal Bouaynaya, Jerzy Zielinski and Dan Schonfeld", "title": "Two-Dimensional ARMA Modeling for Breast Cancer Detection and\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model-based computer-aided diagnosis (CAD) system for tumor\ndetection and classification (cancerous v.s. benign) in breast images.\nSpecifically, we show that (x-ray, ultrasound and MRI) images can be accurately\nmodeled by two-dimensional autoregressive-moving average (ARMA) random fields.\nWe derive a two-stage Yule-Walker Least-Squares estimates of the model\nparameters, which are subsequently used as the basis for statistical inference\nand biophysical interpretation of the breast image. We use a k-means classifier\nto segment the breast image into three regions: healthy tissue, benign tumor,\nand cancerous tumor. Our simulation results on ultrasound breast images\nillustrate the power of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2009 17:56:16 GMT"}], "update_date": "2009-06-22", "authors_parsed": [["Bouaynaya", "Nidhal", ""], ["Zielinski", "Jerzy", ""], ["Schonfeld", "Dan", ""]]}, {"id": "0906.3815", "submitter": "Wlodzimierz Drabent", "authors": "W. Drabent, J. Maluszynski", "title": "Hybrid Rules with Well-Founded Semantics", "comments": null, "journal-ref": "Knowledge and Information Systems, 25:137-168, 2010", "doi": "10.1007/s10115-010-0300-5", "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general framework is proposed for integration of rules and external first\norder theories. It is based on the well-founded semantics of normal logic\nprograms and inspired by ideas of Constraint Logic Programming (CLP) and\nconstructive negation for logic programs. Hybrid rules are normal clauses\nextended with constraints in the bodies; constraints are certain formulae in\nthe language of the external theory. A hybrid program is a pair of a set of\nhybrid rules and an external theory. Instances of the framework are obtained by\nspecifying the class of external theories, and the class of constraints. An\nexample instance is integration of (non-disjunctive) Datalog with ontologies\nformalized as description logics.\n  The paper defines a declarative semantics of hybrid programs and a\ngoal-driven formal operational semantics. The latter can be seen as a\ngeneralization of SLS-resolution. It provides a basis for hybrid\nimplementations combining Prolog with constraint solvers. Soundness of the\noperational semantics is proven. Sufficient conditions for decidability of the\ndeclarative semantics, and for completeness of the operational semantics are\ngiven.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2009 16:09:24 GMT"}], "update_date": "2010-12-08", "authors_parsed": [["Drabent", "W.", ""], ["Maluszynski", "J.", ""]]}, {"id": "0906.3926", "submitter": "EPTCS", "authors": "Stefano Bistarelli, Francesco Santini", "title": "Soft Constraints for Quality Aspects in Service Oriented Architectures", "comments": null, "journal-ref": "EPTCS 2, 2009, pp. 51-65", "doi": "10.4204/EPTCS.2.5", "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of Soft Constraints as a natural way to model Service\nOriented Architecture. In the framework, constraints are used to model\ncomponents and connectors and constraint aggregation is used to represent their\ninteractions. The \"quality of a service\" is measured and considered when\nperforming queries to service providers. Some examples consist in the levels of\ncost, performance and availability required by clients. In our framework, the\nQoS scores are represented by the softness level of the constraint and the\nmeasure of complex (web) services is computed by combining the levels of the\ncomponents.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2009 06:37:02 GMT"}], "update_date": "2009-06-23", "authors_parsed": [["Bistarelli", "Stefano", ""], ["Santini", "Francesco", ""]]}, {"id": "0906.4044", "submitter": "Don Conry", "authors": "Don Conry, Yehuda Koren, Naren Ramakrishnan", "title": "Recommender Systems for the Conference Paper Assignment Problem", "comments": "8 pages, 5 figures, submitted to the ACM Conference on Recommender\n  Systems 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conference paper assignment, i.e., the task of assigning paper submissions to\nreviewers, presents multi-faceted issues for recommender systems research.\nBesides the traditional goal of predicting `who likes what?', a conference\nmanagement system must take into account aspects such as: reviewer capacity\nconstraints, adequate numbers of reviews for papers, expertise modeling,\nconflicts of interest, and an overall distribution of assignments that balances\nreviewer preferences with conference objectives. Among these, issues of\nmodeling preferences and tastes in reviewing have traditionally been studied\nseparately from the optimization of paper-reviewer assignment. In this paper,\nwe present an integrated study of both these aspects. First, due to the paucity\nof data per reviewer or per paper (relative to other recommender systems\napplications) we show how we can integrate multiple sources of information to\nlearn paper-reviewer preference models. Second, our models are evaluated not\njust in terms of prediction accuracy but in terms of the end-assignment\nquality. Using a linear programming-based assignment optimization formulation,\nwe show how our approach better explores the space of unsupplied assignments to\nmaximize the overall affinities of papers assigned to reviewers. We demonstrate\nour results on real reviewer preference data from the IEEE ICDM 2007\nconference.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2009 17:26:24 GMT"}], "update_date": "2009-06-23", "authors_parsed": [["Conry", "Don", ""], ["Koren", "Yehuda", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "0906.4096", "submitter": "Naveen Ashish", "authors": "Naveen Ashish, Dmitri Kalashnikov, Sharad Mehrotra and Nalini\n  Venkatasubramanian", "title": "An Event Based Approach To Situational Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Many application domains require representing interrelated real-world\nactivities and/or evolving physical phenomena. In the crisis response domain,\nfor instance, one may be interested in representing the state of the unfolding\ncrisis (e.g., forest fire), the progress of the response activities such as\nevacuation and traffic control, and the state of the crisis site(s). Such a\nsituation representation can then be used to support a multitude of\napplications including situation monitoring, analysis, and planning. In this\npaper, we make a case for an event based representation of situations where\nevents are defined to be domain-specific significant occurrences in space and\ntime. We argue that events offer a unifying and powerful abstraction to\nbuilding situational awareness applications. We identify challenges in building\nan Event Management System (EMS) for which traditional data and knowledge\nmanagement systems prove to be limited and suggest possible directions and\ntechnologies to address the challenges.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2009 19:40:12 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2009 20:04:32 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Ashish", "Naveen", ""], ["Kalashnikov", "Dmitri", ""], ["Mehrotra", "Sharad", ""], ["Venkatasubramanian", "Nalini", ""]]}, {"id": "0906.4228", "submitter": "Michael Meier", "authors": "Michael Meier, Michael Schmidt, Georg Lausen", "title": "On Chase Termination Beyond Stratification", "comments": "Technical Report of VLDB 2009 conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the termination problem of the chase algorithm, a central tool in\nvarious database problems such as the constraint implication problem,\nConjunctive Query optimization, rewriting queries using views, data exchange,\nand data integration. The basic idea of the chase is, given a database instance\nand a set of constraints as input, to fix constraint violations in the database\ninstance. It is well-known that, for an arbitrary set of constraints, the chase\ndoes not necessarily terminate (in general, it is even undecidable if it does\nor not). Addressing this issue, we review the limitations of existing\nsufficient termination conditions for the chase and develop new techniques that\nallow us to establish weaker sufficient conditions. In particular, we introduce\ntwo novel termination conditions called safety and inductive restriction, and\nuse them to define the so-called T-hierarchy of termination conditions. We then\nstudy the interrelations of our termination conditions with previous conditions\nand the complexity of checking our conditions. This analysis leads to an\nalgorithm that checks membership in a level of the T-hierarchy and accounts for\nthe complexity of termination conditions. As another contribution, we study the\nproblem of data-dependent chase termination and present sufficient termination\nconditions w.r.t. fixed instances. They might guarantee termination although\nthe chase does not terminate in the general case. As an application of our\ntechniques beyond those already mentioned, we transfer our results into the\nfield of query answering over knowledge bases where the chase on the underlying\ndatabase may not terminate, making existing algorithms applicable to broader\nclasses of constraints.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2009 11:46:43 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2009 14:38:04 GMT"}], "update_date": "2009-09-17", "authors_parsed": [["Meier", "Michael", ""], ["Schmidt", "Michael", ""], ["Lausen", "Georg", ""]]}, {"id": "0906.4316", "submitter": "Joseph Y. Halpern", "authors": "Lawrence Blume, David Easley, and Joseph Y. Halpern", "title": "Constructive Decision Theory", "comments": "A preliminary version of paper, with the title \"Redoing the\n  Foundations of Decision Theory\" appeared in the Tenth International\n  Conference on Principles of Knowledge Representation and Reasoning. The paper\n  will appear in \"Journal of Economic Theory\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most contemporary approaches to decision making, a decision problem is\ndescribed by a sets of states and set of outcomes, and a rich set of acts,\nwhich are functions from states to outcomes over which the decision maker (DM)\nhas preferences. Most interesting decision problems, however, do not come with\na state space and an outcome space. Indeed, in complex problems it is often far\nfrom clear what the state and outcome spaces would be. We present an\nalternative foundation for decision making, in which the primitive objects of\nchoice are syntactic programs. A representation theorem is proved in the spirit\nof standard representation theorems, showing that if the DM's preference\nrelation on objects of choice satisfies appropriate axioms, then there exist a\nset S of states, a set O of outcomes, a way of interpreting the objects of\nchoice as functions from S to O, a probability on S, and a utility function on\nO, such that the DM prefers choice a to choice b if and only if the expected\nutility of a is higher than that of b. Thus, the state space and outcome space\nare subjective, just like the probability and utility; they are not part of the\ndescription of the problem. In principle, a modeler can test for SEU behavior\nwithout having access to states or outcomes. We illustrate the power of our\napproach by showing that it can capture decision makers who are subject to\nframing effects.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2009 18:22:45 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 16:28:50 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Blume", "Lawrence", ""], ["Easley", "David", ""], ["Halpern", "Joseph Y.", ""]]}, {"id": "0906.4321", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern and Leandro Rego", "title": "Reasoning About Knowledge of Unawareness Revisited", "comments": "In Proceedings of Twelfth Conference on Theoretical Aspects of\n  Rationality and Knowledge, 2009, pp. 166-173", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In earlier work, we proposed a logic that extends the Logic of General\nAwareness of Fagin and Halpern [1988] by allowing quantification over primitive\npropositions. This makes it possible to express the fact that an agent knows\nthat there are some facts of which he is unaware. In that logic, it is not\npossible to model an agent who is uncertain about whether he is aware of all\nformulas. To overcome this problem, we keep the syntax of the earlier paper,\nbut allow models where, with each world, a possibly different language is\nassociated. We provide a sound and complete axiomatization for this logic and\nshow that, under natural assumptions, the quantifier-free fragment of the logic\nis characterized by exactly the same axioms as the logic of Heifetz, Meier, and\nSchipper [2008].\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2009 17:34:16 GMT"}], "update_date": "2009-06-24", "authors_parsed": [["Halpern", "Joseph Y.", ""], ["Rego", "Leandro", ""]]}, {"id": "0906.4326", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern and Rafael Pass", "title": "A Logical Characterization of Iterated Admissibility", "comments": "In Proceedings of Twelfth Conference on Theoretical Aspects of\n  Rationality and Knowledge, 2009, pp. 146-155", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brandenburger, Friedenberg, and Keisler provide an epistemic characterization\nof iterated admissibility (i.e., iterated deletion of weakly dominated\nstrategies) where uncertainty is represented using LPSs (lexicographic\nprobability sequences). Their characterization holds in a rich structure called\na complete structure, where all types are possible. Here, a logical\ncharaacterization of iterated admisibility is given that involves only standard\nprobability and holds in all structures, not just complete structures. A\nstronger notion of strong admissibility is then defined. Roughly speaking,\nstrong admissibility is meant to capture the intuition that \"all the agent\nknows\" is that the other agents satisfy the appropriate rationality\nassumptions. Strong admissibility makes it possible to relate admissibility,\ncanonical structures (as typically considered in completeness proofs in modal\nlogic), complete structures, and the notion of ``all I know''.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2009 18:04:05 GMT"}], "update_date": "2009-06-24", "authors_parsed": [["Halpern", "Joseph Y.", ""], ["Pass", "Rafael", ""]]}, {"id": "0906.4332", "submitter": "Joseph Y. Halpern", "authors": "Adam J. Grove and Joseph Y. Halpern", "title": "Updating Sets of Probabilities", "comments": "In Proceedings of the Fourteenth Conference on Uncertainty in AI,\n  1998, pp. 173-182", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several well-known justifications for conditioning as the\nappropriate method for updating a single probability measure, given an\nobservation. However, there is a significant body of work arguing for sets of\nprobability measures, rather than single measures, as a more realistic model of\nuncertainty. Conditioning still makes sense in this context--we can simply\ncondition each measure in the set individually, then combine the results--and,\nindeed, it seems to be the preferred updating procedure in the literature. But\nhow justified is conditioning in this richer setting? Here we show, by\nconsidering an axiomatic account of conditioning given by van Fraassen, that\nthe single-measure and sets-of-measures cases are very different. We show that\nvan Fraassen's axiomatization for the former case is nowhere near sufficient\nfor updating sets of measures. We give a considerably longer (and not as\ncompelling) list of axioms that together force conditioning in this setting,\nand describe other update methods that are allowed once any of these axioms is\ndropped.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2009 19:34:47 GMT"}, {"version": "v2", "created": "Sun, 10 Aug 2014 01:47:19 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Grove", "Adam J.", ""], ["Halpern", "Joseph Y.", ""]]}, {"id": "0906.4982", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov, Sergei O. Kuznetsov", "title": "Concept-based Recommendations for Internet Advertisement", "comments": "D.I.Ignatov, S.O. Kuznetsov. Concept-based Recommendations for\n  Internet Advertisement//In proceedings of The Sixth International Conference\n  Concept Lattices and Their Applications (CLA'08), Olomouc, Czech Republic,\n  2008 ISBN 978-80-244-2111-7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting terms that can be interesting to the advertiser is\nconsidered. If a company has already bought some advertising terms which\ndescribe certain services, it is reasonable to find out the terms bought by\ncompeting companies. A part of them can be recommended as future advertising\nterms to the company. The goal of this work is to propose better interpretable\nrecommendations based on FCA and association rules.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2009 17:26:05 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Ignatov", "Dmitry I.", ""], ["Kuznetsov", "Sergei O.", ""]]}, {"id": "0906.5038", "submitter": "R Doomun", "authors": "Huma Naeem, Asif Masood, Mukhtar Hussain, Shoab A. Khan", "title": "A Novel Two-Stage Dynamic Decision Support based Optimal Threat\n  Evaluation and Defensive Resource Scheduling Algorithm for Multi Air-borne\n  threats", "comments": "8 Pages, International Journal of Computer Science and Information\n  Security, IJCSIS", "journal-ref": "IJCSIS June 2009 Issue, Vol.2, No. 1", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel two-stage flexible dynamic decision support based\noptimal threat evaluation and defensive resource scheduling algorithm for\nmulti-target air-borne threats. The algorithm provides flexibility and\noptimality by swapping between two objective functions, i.e. the preferential\nand subtractive defense strategies as and when required. To further enhance the\nsolution quality, it outlines and divides the critical parameters used in\nThreat Evaluation and Weapon Assignment (TEWA) into three broad categories\n(Triggering, Scheduling and Ranking parameters). Proposed algorithm uses a\nvariant of many-to-many Stable Marriage Algorithm (SMA) to solve Threat\nEvaluation (TE) and Weapon Assignment (WA) problem. In TE stage, Threat Ranking\nand Threat-Asset pairing is done. Stage two is based on a new flexible dynamic\nweapon scheduling algorithm, allowing multiple engagements using\nshoot-look-shoot strategy, to compute near-optimal solution for a range of\nscenarios. Analysis part of this paper presents the strengths and weaknesses of\nthe proposed algorithm over an alternative greedy algorithm as applied to\ndifferent offline scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2009 04:24:59 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Naeem", "Huma", ""], ["Masood", "Asif", ""], ["Hussain", "Mukhtar", ""], ["Khan", "Shoab A.", ""]]}, {"id": "0906.5040", "submitter": "Chendong Li", "authors": "Chendong Li", "title": "Towards the Patterns of Hard CSPs with Association Rule Mining", "comments": "10 pages, 3 figures, submitted to ICDM'09", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hardness of finite domain Constraint Satisfaction Problems (CSPs) is a\nvery important research area in Constraint Programming (CP) community. However,\nthis problem has not yet attracted much attention from the researchers in the\nassociation rule mining community. As a popular data mining technique,\nassociation rule mining has an extremely wide application area and it has\nalready been successfully applied to many interdisciplines. In this paper, we\nstudy the association rule mining techniques and propose a cascaded approach to\nextract the interesting patterns of the hard CSPs. As far as we know, this\nproblem is investigated with the data mining techniques for the first time.\nSpecifically, we generate the random CSPs and collect their characteristics by\nsolving all the CSP instances, and then apply the data mining techniques on the\ndata set and further to discover the interesting patterns of the hardness of\nthe randomly generated CSPs\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2009 05:11:48 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Li", "Chendong", ""]]}, {"id": "0906.5119", "submitter": "Arnaud Martin", "authors": "Arnaud Martin (E3I2), Christophe Osswald (E3I2), Jean Dezert (ONERA),\n  Florentin Smarandache (UNM)", "title": "General combination rules for qualitative and quantitative beliefs", "comments": null, "journal-ref": "Journal of Advances in Information Fusion 3, 2 (2008) 67-89", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Martin and Osswald \\cite{Martin07} have recently proposed many\ngeneralizations of combination rules on quantitative beliefs in order to manage\nthe conflict and to consider the specificity of the responses of the experts.\nSince the experts express themselves usually in natural language with\nlinguistic labels, Smarandache and Dezert \\cite{Li07} have introduced a\nmathematical framework for dealing directly also with qualitative beliefs. In\nthis paper we recall some element of our previous works and propose the new\ncombination rules, developed for the fusion of both qualitative or quantitative\nbeliefs.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2009 08:09:04 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Martin", "Arnaud", "", "E3I2"], ["Osswald", "Christophe", "", "E3I2"], ["Dezert", "Jean", "", "ONERA"], ["Smarandache", "Florentin", "", "UNM"]]}, {"id": "0906.5120", "submitter": "Arnaud Martin", "authors": "Jean Dezert (ONERA), Arnaud Martin (E3I2), Florentin Smarandache (UNM)", "title": "Comments on \"A new combination of evidence based on compromise\" by K.\n  Yamada", "comments": null, "journal-ref": "Fuzzy Sets and Systems 160, 6 (2009) 853-855", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comments on ``A new combination of evidence based on compromise'' by K.\nYamada\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2009 08:10:33 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Dezert", "Jean", "", "ONERA"], ["Martin", "Arnaud", "", "E3I2"], ["Smarandache", "Florentin", "", "UNM"]]}, {"id": "0906.5148", "submitter": "Tijl De Bie", "authors": "Tijl De Bie", "title": "Explicit probabilistic models for databases and networks", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in data mining and related areas has highlighted the importance\nof the statistical assessment of data mining results. Crucial to this endeavour\nis the choice of a non-trivial null model for the data, to which the found\npatterns can be contrasted. The most influential null models proposed so far\nare defined in terms of invariants of the null distribution. Such null models\ncan be used by computation intensive randomization approaches in estimating the\nstatistical significance of data mining results.\n  Here, we introduce a methodology to construct non-trivial probabilistic\nmodels based on the maximum entropy (MaxEnt) principle. We show how MaxEnt\nmodels allow for the natural incorporation of prior information. Furthermore,\nthey satisfy a number of desirable properties of previously introduced\nrandomization approaches. Lastly, they also have the benefit that they can be\nrepresented explicitly. We argue that our approach can be used for a variety of\ndata types. However, for concreteness, we have chosen to demonstrate it in\nparticular for databases and networks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2009 07:55:41 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["De Bie", "Tijl", ""]]}, {"id": "0906.5233", "submitter": "Toby Walsh", "authors": "George Katsirelos, Sebastian Maneth, Nina Narodytska, Toby Walsh", "title": "Restricted Global Grammar Constraints", "comments": "Proceedings of the 15th International Conference on Principles and\n  Practice of Constraint Programming, Lisbon, Portugal. September 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the global GRAMMAR constraint over restricted classes of\ncontext free grammars like deterministic and unambiguous context-free grammars.\nWe show that detecting disentailment for the GRAMMAR constraint in these cases\nis as hard as parsing an unrestricted context free grammar.We also consider the\nclass of linear grammars and give a propagator that runs in quadratic time.\nFinally, to demonstrate the use of linear grammars, we show that a weighted\nlinear GRAMMAR constraint can efficiently encode the EDITDISTANCE constraint,\nand a conjunction of the EDITDISTANCE constraint and the REGULAR constraint\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2009 09:23:39 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Katsirelos", "George", ""], ["Maneth", "Sebastian", ""], ["Narodytska", "Nina", ""], ["Walsh", "Toby", ""]]}, {"id": "0906.5485", "submitter": "Markus Ojala", "authors": "Markus Ojala, Gemma C. Garriga, Aristides Gionis, Heikki Mannila", "title": "Query Significance in Databases via Randomizations", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sorts of structured data are commonly stored in a multi-relational\nformat of interrelated tables. Under this relational model, exploratory data\nanalysis can be done by using relational queries. As an example, in the\nInternet Movie Database (IMDb) a query can be used to check whether the average\nrank of action movies is higher than the average rank of drama movies.\n  We consider the problem of assessing whether the results returned by such a\nquery are statistically significant or just a random artifact of the structure\nin the data. Our approach is based on randomizing the tables occurring in the\nqueries and repeating the original query on the randomized tables. It turns out\nthat there is no unique way of randomizing in multi-relational data. We propose\nseveral randomization techniques, study their properties, and show how to find\nout which queries or hypotheses about our data result in statistically\nsignificant information. We give results on real and generated data and show\nhow the significance of some queries vary between different randomizations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2009 14:08:19 GMT"}], "update_date": "2009-07-01", "authors_parsed": [["Ojala", "Markus", ""], ["Garriga", "Gemma C.", ""], ["Gionis", "Aristides", ""], ["Mannila", "Heikki", ""]]}]