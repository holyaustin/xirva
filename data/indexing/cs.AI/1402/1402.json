[{"id": "1402.0052", "submitter": "David Gamarnik", "authors": "David Gamarnik, Madhu Sudan", "title": "Performance of the Survey Propagation-guided decimation algorithm for\n  the random NAE-K-SAT problem", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cond-mat.stat-mech cs.AI cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Survey Propagation-guided decimation algorithm fails to find\nsatisfying assignments on random instances of the \"Not-All-Equal-$K$-SAT\"\nproblem if the number of message passing iterations is bounded by a constant\nindependent of the size of the instance and the clause-to-variable ratio is\nabove $(1+o_K(1)){2^{K-1}\\over K}\\log^2 K$ for sufficiently large $K$. Our\nanalysis in fact applies to a broad class of algorithms described as\n\"sequential local algorithms\". Such algorithms iteratively set variables based\non some local information and then recurse on the reduced instance. Survey\nPropagation-guided as well as Belief Propagation-guided decimation algorithms -\ntwo widely studied message passing based algorithms, fall under this category\nof algorithms provided the number of message passing iterations is bounded by a\nconstant. Another well-known algorithm falling into this category is the Unit\nClause algorithm. Our work constitutes the first rigorous analysis of the\nperformance of the SP-guided decimation algorithm.\n  The approach underlying our paper is based on an intricate geometry of the\nsolution space of random NAE-$K$-SAT problem. We show that above the\n$(1+o_K(1)){2^{K-1}\\over K}\\log^2 K$ threshold, the overlap structure of\n$m$-tuples of satisfying assignments exhibit a certain clustering behavior\nexpressed in the form of constraints on distances between the $m$ assignments,\nfor appropriately chosen $m$. We further show that if a sequential local\nalgorithm succeeds in finding a satisfying assignment with probability bounded\naway from zero, then one can construct an $m$-tuple of solutions violating\nthese constraints, thus leading to a contradiction. Along with (citation), this\nresult is the first work which directly links the clustering property of random\nconstraint satisfaction problems to the computational hardness of finding\nsatisfying assignments.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2014 05:05:12 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 02:07:36 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Gamarnik", "David", ""], ["Sudan", "Madhu", ""]]}, {"id": "1402.0402", "submitter": "Ben Strasser", "authors": "Julian Dibbelt, Ben Strasser, Dorothea Wagner", "title": "Customizable Contraction Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of quickly computing shortest paths in weighted\ngraphs given auxiliary data derived in an expensive preprocessing phase. By\nadding a fast weight-customization phase, we extend Contraction Hierarchies by\nGeisberger et al to support the three-phase workflow introduced by Delling et\nal. Our Customizable Contraction Hierarchies use nested dissection orders as\nsuggested by Bauer et al. We provide an in-depth experimental analysis on large\nroad and game maps that clearly shows that Customizable Contraction Hierarchies\nare a very practicable solution in scenarios where edge weights often change.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 15:38:27 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 12:29:17 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2014 09:37:36 GMT"}, {"version": "v4", "created": "Fri, 27 Jun 2014 08:18:28 GMT"}, {"version": "v5", "created": "Fri, 21 Aug 2015 11:25:25 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Dibbelt", "Julian", ""], ["Strasser", "Ben", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1402.0557", "submitter": "Eric Huang", "authors": "Eric Huang, Richard E. Korf", "title": "Optimal Rectangle Packing: An Absolute Placement Approach", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  47-87, 2013", "doi": "10.1613/jair.3735", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding all enclosing rectangles of minimum area\nthat can contain a given set of rectangles without overlap. Our rectangle\npacker chooses the x-coordinates of all the rectangles before any of the\ny-coordinates. We then transform the problem into a perfect-packing problem\nwith no empty space by adding additional rectangles. To determine the\ny-coordinates, we branch on the different rectangles that can be placed in each\nempty position. Our packer allows us to extend the known solutions for a\nconsecutive-square benchmark from 27 to 32 squares. We also introduce three new\nbenchmarks, avoiding properties that make a benchmark easy, such as rectangles\nwith shared dimensions. Our third benchmark consists of rectangles of\nincreasingly high precision. To pack them efficiently, we limit the rectangles\ncoordinates and the bounding box dimensions to the set of subset sums of the\nrectangles dimensions. Overall, our algorithms represent the current\nstate-of-the-art for this problem, outperforming other algorithms by orders of\nmagnitude, depending on the benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:33:30 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Huang", "Eric", ""], ["Korf", "Richard E.", ""]]}, {"id": "1402.0558", "submitter": "Sebastian Ordyniak", "authors": "Sebastian Ordyniak, Stefan Szeider", "title": "Parameterized Complexity Results for Exact Bayesian Network Structure\n  Learning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  263-302, 2013", "doi": "10.1613/jair.3744", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network structure learning is the notoriously difficult problem of\ndiscovering a Bayesian network that optimally represents a given set of\ntraining data. In this paper we study the computational worst-case complexity\nof exact Bayesian network structure learning under graph theoretic restrictions\non the (directed) super-structure. The super-structure is an undirected graph\nthat contains as subgraphs the skeletons of solution networks. We introduce the\ndirected super-structure as a natural generalization of its undirected\ncounterpart. Our results apply to several variants of score-based Bayesian\nnetwork structure learning where the score of a network decomposes into local\nscores of its nodes. Results: We show that exact Bayesian network structure\nlearning can be carried out in non-uniform polynomial time if the\nsuper-structure has bounded treewidth, and in linear time if in addition the\nsuper-structure has bounded maximum degree. Furthermore, we show that if the\ndirected super-structure is acyclic, then exact Bayesian network structure\nlearning can be carried out in quadratic time. We complement these positive\nresults with a number of hardness results. We show that both restrictions\n(treewidth and degree) are essential and cannot be dropped without loosing\nuniform polynomial time tractability (subject to a complexity-theoretic\nassumption). Similarly, exact Bayesian network structure learning remains\nNP-hard for \"almost acyclic\" directed super-structures. Furthermore, we show\nthat the restrictions remain essential if we do not search for a globally\noptimal network but aim to improve a given network by means of at most k arc\nadditions, arc deletions, or arc reversals (k-neighborhood local search).\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:33:50 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1402.0559", "submitter": "Peter Nightingale", "authors": "Peter Nightingale, Ian Philip Gent, Christopher Jefferson, Ian Miguel", "title": "Short and Long Supports for Constraint Propagation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  1-45, 2013", "doi": "10.1613/jair.3749", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Special-purpose constraint propagation algorithms frequently make implicit\nuse of short supports -- by examining a subset of the variables, they can infer\nsupport (a justification that a variable-value pair may still form part of an\nassignment that satisfies the constraint) for all other variables and values\nand save substantial work -- but short supports have not been studied in their\nown right. The two main contributions of this paper are the identification of\nshort supports as important for constraint propagation, and the introduction of\nHaggisGAC, an efficient and effective general purpose propagation algorithm for\nexploiting short supports. Given the complexity of HaggisGAC, we present it as\nan optimised version of a simpler algorithm ShortGAC. Although experiments\ndemonstrate the efficiency of ShortGAC compared with other general-purpose\npropagation algorithms where a compact set of short supports is available, we\nshow theoretically and experimentally that HaggisGAC is even better. We also\nfind that HaggisGAC performs better than GAC-Schema on full-length supports. We\nalso introduce a variant algorithm HaggisGAC-Stable, which is adapted to avoid\nwork on backtracking and in some cases can be faster and have significant\nreductions in memory use. All the proposed algorithms are excellent for\npropagating disjunctions of constraints. In all experiments with disjunctions\nwe found our algorithms to be faster than Constructive Or and GAC-Schema by at\nleast an order of magnitude, and up to three orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:34:04 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Nightingale", "Peter", ""], ["Gent", "Ian Philip", ""], ["Jefferson", "Christopher", ""], ["Miguel", "Ian", ""]]}, {"id": "1402.0560", "submitter": "Javier Garcia", "authors": "Javier Garcia, Fernando Fernandez", "title": "Safe Exploration of State and Action Spaces in Reinforcement Learning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  515-564, 2012", "doi": "10.1613/jair.3761", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the important problem of safe exploration in\nreinforcement learning. While reinforcement learning is well-suited to domains\nwith complex transition dynamics and high-dimensional state-action spaces, an\nadditional challenge is posed by the need for safe and efficient exploration.\nTraditional exploration techniques are not particularly useful for solving\ndangerous tasks, where the trial and error process may lead to the selection of\nactions whose execution in some states may result in damage to the learning\nsystem (or any other system). Consequently, when an agent begins an interaction\nwith a dangerous and high-dimensional state-action space, an important question\narises; namely, that of how to avoid (or at least minimize) damage caused by\nthe exploration of the state-action space. We introduce the PI-SRL algorithm\nwhich safely improves suboptimal albeit robust behaviors for continuous state\nand action control tasks and which efficiently learns from the experience\ngained from the environment. We evaluate the proposed method in four complex\ntasks: automatic car parking, pole-balancing, helicopter hovering, and business\nmanagement.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:34:25 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Garcia", "Javier", ""], ["Fernandez", "Fernando", ""]]}, {"id": "1402.0561", "submitter": "Gert de Cooman", "authors": "Gert de Cooman, Enrique Miranda", "title": "Irrelevant and independent natural extension for sets of desirable\n  gambles", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  601-640, 2012", "doi": "10.1613/jair.3770", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The results in this paper add useful tools to the theory of sets of desirable\ngambles, a growing toolbox for reasoning with partial probability assessments.\nWe investigate how to combine a number of marginal coherent sets of desirable\ngambles into a joint set using the properties of epistemic irrelevance and\nindependence. We provide formulas for the smallest such joint, called their\nindependent natural extension, and study its main properties. The independent\nnatural extension of maximal coherent sets of desirable gambles allows us to\ndefine the strong product of sets of desirable gambles. Finally, we explore an\neasy way to generalise these results to also apply for the conditional versions\nof epistemic irrelevance and independence. Having such a set of tools that are\neasily implemented in computer programs is clearly beneficial to fields, like\nAI, with a clear interest in coherent reasoning under uncertainty using general\nand robust uncertainty models that require no full specification.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:34:40 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["de Cooman", "Gert", ""], ["Miranda", "Enrique", ""]]}, {"id": "1402.0564", "submitter": "Amanda Jane Coles", "authors": "Amanda Jane Coles, Andrew Ian Coles, Maria Fox, Derek Long", "title": "A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in\n  Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  343-412, 2013", "doi": "10.1613/jair.3788", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the use of metric fluents is fundamental to many practical planning\nproblems, the study of heuristics to support fully automated planners working\nwith these fluents remains relatively unexplored. The most widely used\nheuristic is the relaxation of metric fluents into interval-valued variables\n--- an idea first proposed a decade ago. Other heuristics depend on domain\nencodings that supply additional information about fluents, such as capacity\nconstraints or other resource-related annotations. A particular challenge to\nthese approaches is in handling interactions between metric fluents that\nrepresent exchange, such as the transformation of quantities of raw materials\ninto quantities of processed goods, or trading of money for materials. The\nusual relaxation of metric fluents is often very poor in these situations,\nsince it does not recognise that resources, once spent, are no longer available\nto be spent again. We present a heuristic for numeric planning problems\nbuilding on the propositional relaxed planning graph, but using a mathematical\nprogram for numeric reasoning. We define a class of producer--consumer planning\nproblems and demonstrate how the numeric constraints in these can be modelled\nin a mixed integer program (MIP). This MIP is then combined with a metric\nRelaxed Planning Graph (RPG) heuristic to produce an integrated hybrid\nheuristic. The MIP tracks resource use more accurately than the usual\nrelaxation, but relaxes the ordering of actions, while the RPG captures the\ncausal propositional aspects of the problem. We discuss how these two\ncomponents interact to produce a single unified heuristic and go on to explore\nhow further numeric features of planning problems can be integrated into the\nMIP. We show that encoding a limited subset of the propositional problem to\naugment the MIP can yield more accurate guidance, partly by exploiting\nstructure such as propositional landmarks and propositional resources. Our\nresults show that the use of this heuristic enhances scalability on problems\nwhere numeric resource interaction is key in finding a solution.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:35:19 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Coles", "Amanda Jane", ""], ["Coles", "Andrew Ian", ""], ["Fox", "Maria", ""], ["Long", "Derek", ""]]}, {"id": "1402.0565", "submitter": "Nima Taghipour", "authors": "Nima Taghipour, Daan Fierens, Jesse Davis, Hendrik Blockeel", "title": "Lifted Variable Elimination: Decoupling the Operators from the\n  Constraint Language", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 47, pages\n  393-439, 2013", "doi": "10.1613/jair.3793", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifted probabilistic inference algorithms exploit regularities in the\nstructure of graphical models to perform inference more efficiently. More\nspecifically, they identify groups of interchangeable variables and perform\ninference once per group, as opposed to once per variable. The groups are\ndefined by means of constraints, so the flexibility of the grouping is\ndetermined by the expressivity of the constraint language. Existing approaches\nfor exact lifted inference use specific languages for (in)equality constraints,\nwhich often have limited expressivity. In this article, we decouple lifted\ninference from the constraint language. We define operators for lifted\ninference in terms of relational algebra operators, so that they operate on the\nsemantic level (the constraints extension) rather than on the syntactic level,\nmaking them language-independent. As a result, lifted inference can be\nperformed using more powerful constraint languages, which provide more\nopportunities for lifting. We empirically demonstrate that this can improve\ninference efficiency by orders of magnitude, allowing exact inference where\nuntil now only approximate inference was feasible.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:35:39 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Taghipour", "Nima", ""], ["Fierens", "Daan", ""], ["Davis", "Jesse", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1402.0566", "submitter": "Frans  Adriaan Oliehoek", "authors": "Frans Adriaan Oliehoek, Matthijs T.J. Spaan, Christopher Amato, Shimon\n  Whiteson", "title": "Incremental Clustering and Expansion for Faster Optimal Planning in\n  Dec-POMDPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  449-509, 2013", "doi": "10.1613/jair.3804", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the state-of-the-art in optimal solution methods for\ndecentralized partially observable Markov decision processes (Dec-POMDPs),\nwhich are general models for collaborative multiagent planning under\nuncertainty. Building off the generalized multiagent A* (GMAA*) algorithm,\nwhich reduces the problem to a tree of one-shot collaborative Bayesian games\n(CBGs), we describe several advances that greatly expand the range of\nDec-POMDPs that can be solved optimally. First, we introduce lossless\nincremental clustering of the CBGs solved by GMAA*, which achieves exponential\nspeedups without sacrificing optimality. Second, we introduce incremental\nexpansion of nodes in the GMAA* search tree, which avoids the need to expand\nall children, the number of which is in the worst case doubly exponential in\nthe nodes depth. This is particularly beneficial when little clustering is\npossible. In addition, we introduce new hybrid heuristic representations that\nare more compact and thereby enable the solution of larger Dec-POMDPs. We\nprovide theoretical guarantees that, when a suitable heuristic is used, both\nincremental clustering and incremental expansion yield algorithms that are both\ncomplete and search equivalent. Finally, we present extensive empirical results\ndemonstrating that GMAA*-ICE, an algorithm that synthesizes these advances, can\noptimally solve Dec-POMDPs of unprecedented size.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:35:59 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Oliehoek", "Frans Adriaan", ""], ["Spaan", "Matthijs T. J.", ""], ["Amato", "Christopher", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1402.0568", "submitter": "Amit Metodi", "authors": "Amit Metodi, Michael Codish, Peter James Stuckey", "title": "Boolean Equi-propagation for Concise and Efficient SAT Encodings of\n  Combinatorial Problems", "comments": "arXiv admin note: text overlap with arXiv:1206.3883", "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  303-341, 2013", "doi": "10.1613/jair.3809", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to propagation-based SAT encoding of combinatorial\nproblems, Boolean equi-propagation, where constraints are modeled as Boolean\nfunctions which propagate information about equalities between Boolean\nliterals. This information is then applied to simplify the CNF encoding of the\nconstraints. A key factor is that considering only a small fragment of a\nconstraint model at one time enables us to apply stronger, and even complete,\nreasoning to detect equivalent literals in that fragment. Once detected,\nequivalences apply to simplify the entire constraint model and facilitate\nfurther reasoning on other fragments. Equi-propagation in combination with\npartial evaluation and constraint simplification provide the foundation for a\npowerful approach to SAT-based finite domain constraint solving. We introduce a\ntool called BEE (Ben-Gurion Equi-propagation Encoder) based on these ideas and\ndemonstrate for a variety of benchmarks that our approach leads to a\nconsiderable reduction in the size of CNF encodings and subsequent speed-ups in\nSAT solving times.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:36:36 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Metodi", "Amit", ""], ["Codish", "Michael", ""], ["Stuckey", "Peter James", ""]]}, {"id": "1402.0569", "submitter": "Babak Bagheri Hariri", "authors": "Babak Bagheri Hariri, Diego Calvanese, Marco Montali, Giuseppe De\n  Giacomo, Riccardo De Masellis, Paolo Felli", "title": "Description Logic Knowledge and Action Bases", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  651-686, 2013", "doi": "10.1613/jair.3826", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Description logic Knowledge and Action Bases (KAB) are a mechanism for\nproviding both a semantically rich representation of the information on the\ndomain of interest in terms of a description logic knowledge base and actions\nto change such information over time, possibly introducing new objects. We\nresort to a variant of DL-Lite where the unique name assumption is not enforced\nand where equality between objects may be asserted and inferred. Actions are\nspecified as sets of conditional effects, where conditions are based on\nepistemic queries over the knowledge base (TBox and ABox), and effects are\nexpressed in terms of new ABoxes. In this setting, we address verification of\ntemporal properties expressed in a variant of first-order mu-calculus with\nquantification across states. Notably, we show decidability of verification,\nunder a suitable restriction inspired by the notion of weak acyclicity in data\nexchange.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:36:58 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Hariri", "Babak Bagheri", ""], ["Calvanese", "Diego", ""], ["Montali", "Marco", ""], ["De Giacomo", "Giuseppe", ""], ["De Masellis", "Riccardo", ""], ["Felli", "Paolo", ""]]}, {"id": "1402.0571", "submitter": "Gerald Tesauro", "authors": "Gerald Tesauro, David C. Gondek, Jonathan Lenchner, James Fan, John M.\n  Prager", "title": "Analysis of Watson's Strategies for Playing Jeopardy!", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 47, pages\n  205-251, 2013", "doi": "10.1613/jair.3834", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major advances in Question Answering technology were needed for IBM Watson to\nplay Jeopardy! at championship level -- the show requires rapid-fire answers to\nchallenging natural language questions, broad general knowledge, high\nprecision, and accurate confidence estimates. In addition, Jeopardy! features\nfour types of decision making carrying great strategic importance: (1) Daily\nDouble wagering; (2) Final Jeopardy wagering; (3) selecting the next square\nwhen in control of the board; (4) deciding whether to attempt to answer, i.e.,\n\"buzz in.\" Using sophisticated strategies for these decisions, that properly\naccount for the game state and future event probabilities, can significantly\nboost a players overall chances to win, when compared with simple \"rule of\nthumb\" strategies. This article presents our approach to developing Watsons\ngame-playing strategies, comprising development of a faithful simulation model,\nand then using learning and Monte-Carlo methods within the simulator to\noptimize Watsons strategic decision-making. After giving a detailed description\nof each of our game-strategy algorithms, we then focus in particular on\nvalidating the accuracy of the simulators predictions, and documenting\nperformance improvements using our methods. Quantitative performance benefits\nare shown with respect to both simple heuristic strategies, and actual human\ncontestant performance in historical episodes. We further extend our analysis\nof human play to derive a number of valuable and counterintuitive examples\nillustrating how human contestants may improve their performance on the show.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:37:44 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Tesauro", "Gerald", ""], ["Gondek", "David C.", ""], ["Lenchner", "Jonathan", ""], ["Fan", "James", ""], ["Prager", "John M.", ""]]}, {"id": "1402.0573", "submitter": "Srdjan Vesic", "authors": "Srdjan Vesic", "title": "Identifying the Class of Maxi-Consistent Operators in Argumentation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 47, pages\n  71-93, 2013", "doi": "10.1613/jair.3860", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dung's abstract argumentation theory can be seen as a general framework for\nnon-monotonic reasoning. An important question is then: what is the class of\nlogics that can be subsumed as instantiations of this theory? The goal of this\npaper is to identify and study the large class of logic-based instantiations of\nDung's theory which correspond to the maxi-consistent operator, i.e. to the\nfunction which returns maximal consistent subsets of an inconsistent knowledge\nbase. In other words, we study the class of instantiations where very extension\nof the argumentation system corresponds to exactly one maximal consistent\nsubset of the knowledge base. We show that an attack relation belonging to this\nclass must be conflict-dependent, must not be valid, must not be\nconflict-complete, must not be symmetric etc. Then, we show that some attack\nrelations serve as lower or upper bounds of the class (e.g. if an attack\nrelation contains canonical undercut then it is not a member of this class). By\nusing our results, we show for all existing attack relations whether or not\nthey belong to this class. We also define new attack relations which are\nmembers of this class. Finally, we interpret our results and discuss more\ngeneral questions, like: what is the added value of argumentation in such a\nsetting? We believe that this work is a first step towards achieving our\nlong-term goal, which is to better understand the role of argumentation and,\nparticularly, the expressivity of logic-based instantiations of Dung-style\nargumentation frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:38:48 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Vesic", "Srdjan", ""]]}, {"id": "1402.0574", "submitter": "Kira Radinsky", "authors": "Kira Radinsky, Sagie Davidovich, Shaul Markovitch", "title": "Learning to Predict from Textual Data", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  641-684, 2012", "doi": "10.1613/jair.3865", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a current news event, we tackle the problem of generating plausible\npredictions of future events it might cause. We present a new methodology for\nmodeling and predicting such future news events using machine learning and data\nmining techniques. Our Pundit algorithm generalizes examples of causality pairs\nto infer a causality predictor. To obtain precisely labeled causality examples,\nwe mine 150 years of news articles and apply semantic natural language modeling\ntechniques to headlines containing certain predefined causality patterns. For\ngeneralization, the model uses a vast number of world knowledge ontologies.\nEmpirical evaluation on real news articles shows that our Pundit algorithm\nperforms as well as non-expert humans.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:39:12 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Radinsky", "Kira", ""], ["Davidovich", "Sagie", ""], ["Markovitch", "Shaul", ""]]}, {"id": "1402.0575", "submitter": "Diego Calvanese", "authors": "Diego Calvanese, Magdalena Ortiz, Mantas Simkus, Giorgio Stefanoni", "title": "Reasoning about Explanations for Negative Query Answers in DL-Lite", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 48, pages\n  635-669, 2013", "doi": "10.1613/jair.3870", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to meet usability requirements, most logic-based applications\nprovide explanation facilities for reasoning services. This holds also for\nDescription Logics, where research has focused on the explanation of both TBox\nreasoning and, more recently, query answering. Besides explaining the presence\nof a tuple in a query answer, it is important to explain also why a given tuple\nis missing. We address the latter problem for instance and conjunctive query\nanswering over DL-Lite ontologies by adopting abductive reasoning; that is, we\nlook for additions to the ABox that force a given tuple to be in the result. As\nreasoning tasks we consider existence and recognition of an explanation, and\nrelevance and necessity of a given assertion for an explanation. We\ncharacterize the computational complexity of these problems for arbitrary,\nsubset minimal, and cardinality minimal explanations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:39:28 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Calvanese", "Diego", ""], ["Ortiz", "Magdalena", ""], ["Simkus", "Mantas", ""], ["Stefanoni", "Giorgio", ""]]}, {"id": "1402.0576", "submitter": "Ilianna  Kollia", "authors": "Ilianna Kollia, Birte Glimm", "title": "Optimizing SPARQL Query Answering over OWL Ontologies", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 48, pages\n  253-303, 2013", "doi": "10.1613/jair.3872", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SPARQL query language is currently being extended by the World Wide Web\nConsortium (W3C) with so-called entailment regimes. An entailment regime\ndefines how queries are evaluated under more expressive semantics than SPARQLs\nstandard simple entailment, which is based on subgraph matching. The queries\nare very expressive since variables can occur within complex concepts and can\nalso bind to concept or role names. In this paper, we describe a sound and\ncomplete algorithm for the OWL Direct Semantics entailment regime. We further\npropose several novel optimizations such as strategies for determining a good\nquery execution order, query rewriting techniques, and show how specialized OWL\nreasoning tasks and the concept and role hierarchy can be used to reduce the\nquery execution time. For determining a good execution order, we propose a\ncost-based model, where the costs are based on information about the instances\nof concepts and roles that are extracted from a model abstraction built by an\nOWL reasoner. We present two ordering strategies: a static and a dynamic one.\nFor the dynamic case, we improve the performance by exploiting an individual\nclustering approach that allows for computing the cost functions based on one\nindividual sample from a cluster. We provide a prototypical implementation and\nevaluate the efficiency of the proposed optimizations. Our experimental study\nshows that the static ordering usually outperforms the dynamic one when\naccurate statistics are available. This changes, however, when the statistics\nare less accurate, e.g., due to nondeterministic reasoning decisions. For\nqueries that go beyond conjunctive instance queries we observe an improvement\nof up to three orders of magnitude due to the proposed optimizations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:39:51 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Kollia", "Ilianna", ""], ["Glimm", "Birte", ""]]}, {"id": "1402.0579", "submitter": "Masahiro Ono", "authors": "Masahiro Ono, Brian C. Williams, L. Blackmore", "title": "Probabilistic Planning for Continuous Dynamic Systems under Bounded Risk", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  511-577, 2013", "doi": "10.1613/jair.3893", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a model-based planner called the Probabilistic Sulu\nPlanner or the p-Sulu Planner, which controls stochastic systems in a goal\ndirected manner within user-specified risk bounds. The objective of the p-Sulu\nPlanner is to allow users to command continuous, stochastic systems, such as\nunmanned aerial and space vehicles, in a manner that is both intuitive and\nsafe. To this end, we first develop a new plan representation called a\nchance-constrained qualitative state plan (CCQSP), through which users can\nspecify the desired evolution of the plant state as well as the acceptable\nlevel of risk. An example of a CCQSP statement is go to A through B within 30\nminutes, with less than 0.001% probability of failure.\" We then develop the\np-Sulu Planner, which can tractably solve a CCQSP planning problem. In order to\nenable CCQSP planning, we develop the following two capabilities in this paper:\n1) risk-sensitive planning with risk bounds, and 2) goal-directed planning in a\ncontinuous domain with temporal constraints. The first capability is to ensures\nthat the probability of failure is bounded. The second capability is essential\nfor the planner to solve problems with a continuous state space such as vehicle\npath planning. We demonstrate the capabilities of the p-Sulu Planner by\nsimulations on two real-world scenarios: the path planning and scheduling of a\npersonal aerial vehicle as well as the space rendezvous of an autonomous cargo\nspacecraft.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:41:20 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Ono", "Masahiro", ""], ["Williams", "Brian C.", ""], ["Blackmore", "L.", ""]]}, {"id": "1402.0581", "submitter": "Neal  Andrew Snooke", "authors": "Neal Andrew Snooke, Mark H Lee", "title": "Qualitative Order of Magnitude Energy-Flow-Based Failure Modes and\n  Effects Analysis", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  413-447, 2013", "doi": "10.1613/jair.3898", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a structured power and energy-flow-based qualitative\nmodelling approach that is applicable to a variety of system types including\nelectrical and fluid flow. The modelling is split into two parts. Power flow is\na global phenomenon and is therefore naturally represented and analysed by a\nnetwork comprised of the relevant structural elements from the components of a\nsystem. The power flow analysis is a platform for higher-level behaviour\nprediction of energy related aspects using local component behaviour models to\ncapture a state-based representation with a global time. The primary\napplication is Failure Modes and Effects Analysis (FMEA) and a form of\nexaggeration reasoning is used, combined with an order of magnitude\nrepresentation to derive the worst case failure modes. The novel aspects of the\nwork are an order of magnitude(OM) qualitative network analyser to represent\nany power domain and topology, including multiple power sources, a feature that\nwas not required for earlier specialised electrical versions of the approach.\nSecondly, the representation of generalised energy related behaviour as\nstate-based local models is presented as a modelling strategy that can be more\nvivid and intuitive for a range of topologically complex applications than\nqualitative equation-based representations.The two-level modelling strategy\nallows the broad system behaviour coverage of qualitative simulation to be\nexploited for the FMEA task, while limiting the difficulties of qualitative\nambiguity explanation that can arise from abstracted numerical models. We have\nused the method to support an automated FMEA system with examples of an\naircraft fuel system and domestic a heating system discussed in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:41:55 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Snooke", "Neal Andrew", ""], ["Lee", "Mark H", ""]]}, {"id": "1402.0582", "submitter": "Maliheh Aramon Bajestani", "authors": "Maliheh Aramon Bajestani, J. Christopher Beck", "title": "Scheduling a Dynamic Aircraft Repair Shop with Limited Repair Resources", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 47, pages\n  35-70, 2013", "doi": "10.1613/jair.3902", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a dynamic repair shop scheduling problem in the context of\nmilitary aircraft fleet management where the goal is to maintain a full\ncomplement of aircraft over the long-term. A number of flights, each with a\nrequirement for a specific number and type of aircraft, are already scheduled\nover a long horizon. We need to assign aircraft to flights and schedule repair\nactivities while considering the flights requirements, repair capacity, and\naircraft failures. The number of aircraft awaiting repair dynamically changes\nover time due to failures and it is therefore necessary to rebuild the repair\nschedule online. To solve the problem, we view the dynamic repair shop as\nsuccessive static repair scheduling sub-problems over shorter time periods. We\npropose a complete approach based on the logic-based Benders decomposition to\nsolve the static sub-problems, and design different rescheduling policies to\nschedule the dynamic repair shop. Computational experiments demonstrate that\nthe Benders model is able to find and prove optimal solutions on average four\ntimes faster than a mixed integer programming model. The rescheduling approach\nhaving both aspects of scheduling over a longer horizon and quickly adjusting\nthe schedule increases aircraft available in the long term by 10% compared to\nthe approaches having either one of the aspects alone.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:42:10 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Bajestani", "Maliheh Aramon", ""], ["Beck", "J. Christopher", ""]]}, {"id": "1402.0584", "submitter": "Shaowei Cai", "authors": "Shaowei Cai, Kaile Su, Chuan Luo, Abdul Sattar", "title": "NuMVC: An Efficient Local Search Algorithm for Minimum Vertex Cover", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  687-716, 2013", "doi": "10.1613/jair.3907", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Vertex Cover (MVC) problem is a prominent NP-hard combinatorial\noptimization problem of great importance in both theory and application. Local\nsearch has proved successful for this problem. However, there are two main\ndrawbacks in state-of-the-art MVC local search algorithms. First, they select a\npair of vertices to exchange simultaneously, which is time-consuming. Secondly,\nalthough using edge weighting techniques to diversify the search, these\nalgorithms lack mechanisms for decreasing the weights. To address these issues,\nwe propose two new strategies: two-stage exchange and edge weighting with\nforgetting. The two-stage exchange strategy selects two vertices to exchange\nseparately and performs the exchange in two stages. The strategy of edge\nweighting with forgetting not only increases weights of uncovered edges, but\nalso decreases some weights for each edge periodically. These two strategies\nare used in designing a new MVC local search algorithm, which is referred to as\nNuMVC. We conduct extensive experimental studies on the standard benchmarks,\nnamely DIMACS and BHOSLIB. The experiment comparing NuMVC with state-of-the-art\nheuristic algorithms show that NuMVC is at least competitive with the nearest\ncompetitor namely PLS on the DIMACS benchmark, and clearly dominates all\ncompetitors on the BHOSLIB benchmark. Also, experimental results indicate that\nNuMVC finds an optimal solution much faster than the current best exact\nalgorithm for Maximum Clique on random instances as well as some structured\nones. Moreover, we study the effectiveness of the two strategies and the\nrun-time behaviour through experimental analysis.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:42:48 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Cai", "Shaowei", ""], ["Su", "Kaile", ""], ["Luo", "Chuan", ""], ["Sattar", "Abdul", ""]]}, {"id": "1402.0585", "submitter": "Jose David Fernandez", "authors": "Jose David Fernandez, Francisco Vico", "title": "AI Methods in Algorithmic Composition: A Comprehensive Survey", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 48, pages\n  513-582, 2013", "doi": "10.1613/jair.3908", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic composition is the partial or total automation of the process of\nmusic composition by using computers. Since the 1950s, different computational\ntechniques related to Artificial Intelligence have been used for algorithmic\ncomposition, including grammatical representations, probabilistic methods,\nneural networks, symbolic rule-based systems, constraint programming and\nevolutionary algorithms. This survey aims to be a comprehensive account of\nresearch on algorithmic composition, presenting a thorough view of the field\nfor researchers in Artificial Intelligence.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:43:06 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Fernandez", "Jose David", ""], ["Vico", "Francisco", ""]]}, {"id": "1402.0587", "submitter": "Tal Grinshpoun", "authors": "Tal Grinshpoun, Alon Grubshtein, Roie Zivan, Arnon Netzer, Amnon\n  Meisels", "title": "Asymmetric Distributed Constraint Optimization Problems", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 47, pages\n  613-647, 2013", "doi": "10.1613/jair.3945", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Constraint Optimization (DCOP) is a powerful framework for\nrepresenting and solving distributed combinatorial problems, where the\nvariables of the problem are owned by different agents. Many multi-agent\nproblems include constraints that produce different gains (or costs) for the\nparticipating agents. Asymmetric gains of constrained agents cannot be\nnaturally represented by the standard DCOP model. The present paper proposes a\ngeneral framework for Asymmetric DCOPs (ADCOPs). In ADCOPs different agents may\nhave different valuations for constraints that they are involved in. The new\nframework bridges the gap between multi-agent problems which tend to have\nasymmetric structure and the standard symmetric DCOP model. The benefits of the\nproposed model over previous attempts to generalize the DCOP model are\ndiscussed and evaluated. Innovative algorithms that apply to the special\nproperties of the proposed ADCOP model are presented in detail. These include\ncomplete algorithms that have a substantial advantage in terms of runtime and\nnetwork load over existing algorithms (for standard DCOPs) which use\nalternative representations. Moreover, standard incomplete algorithms (i.e.,\nlocal search algorithms) are inapplicable to the existing DCOP representations\nof asymmetric constraints and when they are applied to the new ADCOP framework\nthey often fail to converge to a local optimum and yield poor results. The\nlocal search algorithms proposed in the present paper converge to high quality\nsolutions. The experimental evidence that is presented reveals that the\nproposed local search algorithms for ADCOPs achieve high quality solutions\nwhile preserving a high level of privacy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:43:59 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Grinshpoun", "Tal", ""], ["Grubshtein", "Alon", ""], ["Zivan", "Roie", ""], ["Netzer", "Arnon", ""], ["Meisels", "Amnon", ""]]}, {"id": "1402.0588", "submitter": "Christer B\\\"ackstr\\\"om", "authors": "Christer B\\\"ackstr\\\"om, Peter Jonsson", "title": "A Refined View of Causal Graphs and Component Sizes: SP-Closed Graph\n  Classes and Beyond", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 47, pages\n  575-611, 2013", "doi": "10.1613/jair.3968", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal graph of a planning instance is an important tool for planning\nboth in practice and in theory. The theoretical studies of causal graphs have\nlargely analysed the computational complexity of planning for instances where\nthe causal graph has a certain structure, often in combination with other\nparameters like the domain size of the variables. Chen and Gimand#233;nez\nignored even the structure and considered only the size of the weakly connected\ncomponents. They proved that planning is tractable if the components are\nbounded by a constant and otherwise intractable. Their intractability result\nwas, however, conditioned by an assumption from parameterised complexity theory\nthat has no known useful relationship with the standard complexity classes. We\napproach the same problem from the perspective of standard complexity classes,\nand prove that planning is NP-hard for classes with unbounded components under\nan additional restriction we refer to as SP-closed. We then argue that most\nNP-hardness theorems for causal graphs are difficult to apply and, thus, prove\na more general result; even if the component sizes grow slowly and the class is\nnot densely populated with graphs, planning still cannot be tractable unless\nthe polynomial hierachy collapses. Both these results still hold when\nrestricted to the class of acyclic causal graphs. We finally give a partial\ncharacterization of the borderline between NP-hard and NP-intermediate classes,\ngiving further insight into the problem.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:44:34 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["B\u00e4ckstr\u00f6m", "Christer", ""], ["Jonsson", "Peter", ""]]}, {"id": "1402.0589", "submitter": "Thomas L\\'eaut\\'e", "authors": "Thomas Leaute, Boi Faltings", "title": "Protecting Privacy through Distributed Computation in Multi-agent\n  Decision Making", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 47, pages\n  649-695, 2013", "doi": "10.1613/jair.3983", "report-no": null, "categories": "cs.AI cs.CR cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As large-scale theft of data from corporate servers is becoming increasingly\ncommon, it becomes interesting to examine alternatives to the paradigm of\ncentralizing sensitive data into large databases. Instead, one could use\ncryptography and distributed computation so that sensitive data can be supplied\nand processed in encrypted form, and only the final result is made known. In\nthis paper, we examine how such a paradigm can be used to implement constraint\nsatisfaction, a technique that can solve a broad class of AI problems such as\nresource allocation, planning, scheduling, and diagnosis. Most previous work on\nprivacy in constraint satisfaction only attempted to protect specific types of\ninformation, in particular the feasibility of particular combinations of\ndecisions. We formalize and extend these restricted notions of privacy by\nintroducing four types of private information, including the feasibility of\ndecisions and the final decisions made, but also the identities of the\nparticipants and the topology of the problem. We present distributed algorithms\nthat allow computing solutions to constraint satisfaction problems while\nmaintaining these four types of privacy. We formally prove the privacy\nproperties of these algorithms, and show experiments that compare their\nrespective performance on benchmark problems.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:44:50 GMT"}, {"version": "v2", "created": "Sat, 12 Jul 2014 22:20:22 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Leaute", "Thomas", ""], ["Faltings", "Boi", ""]]}, {"id": "1402.0590", "submitter": "Diederik Marijn Roijers", "authors": "Diederik Marijn Roijers, Peter Vamplew, Shimon Whiteson, Richard\n  Dazeley", "title": "A Survey of Multi-Objective Sequential Decision-Making", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 48, pages\n  67-113, 2013", "doi": "10.1613/jair.3987", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential decision-making problems with multiple objectives arise naturally\nin practice and pose unique challenges for research in decision-theoretic\nplanning and learning, which has largely focused on single-objective settings.\nThis article surveys algorithms designed for sequential decision-making\nproblems with multiple objectives. Though there is a growing body of literature\non this subject, little of it makes explicit under what circumstances special\nmethods are needed to solve multi-objective problems. Therefore, we identify\nthree distinct scenarios in which converting such a problem to a\nsingle-objective one is impossible, infeasible, or undesirable. Furthermore, we\npropose a taxonomy that classifies multi-objective methods according to the\napplicable scenario, the nature of the scalarization function (which projects\nmulti-objective values to scalar ones), and the type of policies considered. We\nshow how these factors determine the nature of an optimal solution, which can\nbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, we\nsurvey the literature on multi-objective methods for planning and learning.\nFinally, we discuss key applications of such methods and outline opportunities\nfor future work.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:45:08 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Roijers", "Diederik Marijn", ""], ["Vamplew", "Peter", ""], ["Whiteson", "Shimon", ""], ["Dazeley", "Richard", ""]]}, {"id": "1402.0591", "submitter": "Paulo Roberto Costa", "authors": "Paulo Roberto Costa, Lu\\'is Miguel Botelho", "title": "Learning by Observation of Agent Software Images", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 47, pages\n  313-349, 2013", "doi": "10.1613/jair.3989", "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning by observation can be of key importance whenever agents sharing\nsimilar features want to learn from each other. This paper presents an agent\narchitecture that enables software agents to learn by direct observation of the\nactions executed by expert agents while they are performing a task. This is\npossible because the proposed architecture displays information that is\nessential for observation, making it possible for software agents to observe\neach other. The agent architecture supports a learning process that covers all\naspects of learning by observation, such as discovering and observing experts,\nlearning from the observed data, applying the acquired knowledge and evaluating\nthe agents progress. The evaluation provides control over the decision to\nobtain new knowledge or apply the acquired knowledge to new problems. We\ncombine two methods for learning from the observed information. The first one,\nthe recall method, uses the sequence on which the actions were observed to\nsolve new problems. The second one, the classification method, categorizes the\ninformation in the observed data and determines to which set of categories the\nnew problems belong. Results show that agents are able to learn in conditions\nwhere common supervised learning algorithms fail, such as when agents do not\nknow the results of their actions a priori or when not all the effects of the\nactions are visible. The results also show that our approach provides better\nresults than other learning methods since it requires shorter learning periods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:45:26 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Costa", "Paulo Roberto", ""], ["Botelho", "Lu\u00eds Miguel", ""]]}, {"id": "1402.0635", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy, Zheng Wen", "title": "Generalization and Exploration via Randomized Value Functions", "comments": "arXiv admin note: text overlap with arXiv:1307.4847", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose randomized least-squares value iteration (RLSVI) -- a new\nreinforcement learning algorithm designed to explore and generalize efficiently\nvia linearly parameterized value functions. We explain why versions of\nleast-squares value iteration that use Boltzmann or epsilon-greedy exploration\ncan be highly inefficient, and we present computational results that\ndemonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish\nan upper bound on the expected regret of RLSVI that demonstrates\nnear-optimality in a tabula rasa learning context. More broadly, our results\nsuggest that randomized value functions offer a promising approach to tackling\na critical challenge in reinforcement learning: synthesizing efficient\nexploration and effective generalization.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 06:41:59 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 23:11:02 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2016 10:20:11 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""], ["Wen", "Zheng", ""]]}, {"id": "1402.0672", "submitter": "Roshan Ragel", "authors": "A. K. B. Karunathilake, B. M. D. Balasuriya and R. G. Ragel", "title": "User Friendly Line CAPTCHAs", "comments": "6 pages", "journal-ref": "Industrial and Information Systems (ICIIS), 2009 International\n  Conference on , pp.210,215, 28-31 Dec. 2009", "doi": "10.1109/ICIINFS.2009.5429864", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CAPTCHAs or reverse Turing tests are real-time assessments used by programs\n(or computers) to tell humans and machines apart. This is achieved by assigning\nand assessing hard AI problems that could only be solved easily by human but\nnot by machines. Applications of such assessments range from stopping spammers\nfrom automatically filling online forms to preventing hackers from performing\ndictionary attack. Today, the race between makers and breakers of CAPTCHAs is\nat a juncture, where the CAPTCHAs proposed are not even answerable by humans.\nWe consider such CAPTCHAs as non user friendly. In this paper, we propose a\nnovel technique for reverse Turing test - we call it the Line CAPTCHAs - that\nmainly focuses on user friendliness while not compromising the security aspect\nthat is expected to be provided by such a system.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 09:39:32 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Karunathilake", "A. K. B.", ""], ["Balasuriya", "B. M. D.", ""], ["Ragel", "R. G.", ""]]}, {"id": "1402.1361", "submitter": "Jean-Guillaume Fages", "authors": "Jean-Guillaume Fages, Gilles Chabert and Charles Prud'homme", "title": "Combining finite and continuous solvers", "comments": "Presented at Workshop TRICS in conference CP'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining efficiency with reliability within CP systems is one of the main\nconcerns of CP developers. This paper presents a simple and efficient way to\nconnect Choco and Ibex, two CP solvers respectively specialised on finite and\ncontinuous domains. This enables to take advantage of the most recent advances\nof the continuous community within Choco while saving development and\nmaintenance resources, hence ensuring a better software quality.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 14:21:26 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Fages", "Jean-Guillaume", ""], ["Chabert", "Gilles", ""], ["Prud'homme", "Charles", ""]]}, {"id": "1402.1500", "submitter": "Eran Shaham Mr.", "authors": "Eran Shaham, David Sarne, Boaz Ben-Moshe", "title": "Co-clustering of Fuzzy Lagged Data", "comments": "Under consideration for publication in Knowledge and Information\n  Systems. The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s10115-014-0758-7", "journal-ref": null, "doi": "10.1007/s10115-014-0758-7", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper focuses on mining patterns that are characterized by a fuzzy lagged\nrelationship between the data objects forming them. Such a regulatory mechanism\nis quite common in real life settings. It appears in a variety of fields:\nfinance, gene expression, neuroscience, crowds and collective movements are but\na limited list of examples. Mining such patterns not only helps in\nunderstanding the relationship between objects in the domain, but assists in\nforecasting their future behavior. For most interesting variants of this\nproblem, finding an optimal fuzzy lagged co-cluster is an NP-complete problem.\nWe thus present a polynomial-time Monte-Carlo approximation algorithm for\nmining fuzzy lagged co-clusters. We prove that for any data matrix, the\nalgorithm mines a fuzzy lagged co-cluster with fixed probability, which\nencompasses the optimal fuzzy lagged co-cluster by a maximum 2 ratio columns\noverhead and completely no rows overhead. Moreover, the algorithm handles\nnoise, anti-correlations, missing values and overlapping patterns. The\nalgorithm was extensively evaluated using both artificial and real datasets.\nThe results not only corroborate the ability of the algorithm to efficiently\nmine relevant and accurate fuzzy lagged co-clusters, but also illustrate the\nimportance of including the fuzziness in the lagged-pattern model.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 21:02:16 GMT"}, {"version": "v2", "created": "Thu, 15 May 2014 12:01:08 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Shaham", "Eran", ""], ["Sarne", "David", ""], ["Ben-Moshe", "Boaz", ""]]}, {"id": "1402.1757", "submitter": "Ron Rouhani", "authors": "Tao Mao, Laura Ray", "title": "Frequency-Based Patrolling with Heterogeneous Agents and Limited\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates multi-agent frequencybased patrolling of\nintersecting, circle graphs under conditions where graph nodes have non-uniform\nvisitation requirements and agents have limited ability to communicate. The\ntask is modeled as a partially observable Markov decision process, and a\nreinforcement learning solution is developed. Each agent generates its own\npolicy from Markov chains, and policies are exchanged only when agents occupy\nthe same or adjacent nodes. This constraint on policy exchange models sparse\ncommunication conditions over large, unstructured environments. Empirical\nresults provide perspectives on convergence properties, agent cooperation, and\ngeneralization of learned patrolling policies to new instances of the task. The\nemergent behavior indicates learned coordination strategies between\nheterogeneous agents for patrolling large, unstructured regions as well as the\nability to generalize to dynamic variation in node visitation requirements.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 20:51:00 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Mao", "Tao", ""], ["Ray", "Laura", ""]]}, {"id": "1402.1921", "submitter": "Zhenhua Wang", "authors": "Qinfeng Shi, Mark Reid, Tiberio Caetano, Anton van den Hengel and\n  Zhenhua Wang", "title": "A Hybrid Loss for Multiclass and Structured Prediction", "comments": "12 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1009.3346", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hybrid loss for multiclass and structured prediction\nproblems that is a convex combination of a log loss for Conditional Random\nFields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs).\nWe provide a sufficient condition for when the hybrid loss is Fisher consistent\nfor classification. This condition depends on a measure of dominance between\nlabels--specifically, the gap between the probabilities of the best label and\nthe second best label. We also prove Fisher consistency is necessary for\nparametric consistency when learning models such as CRFs. We demonstrate\nempirically that the hybrid loss typically performs least as well as--and often\nbetter than--both of its constituent losses on a variety of tasks, such as\nhuman action recognition. In doing so we also provide an empirical comparison\nof the efficacy of probabilistic and margin based approaches to multiclass and\nstructured prediction.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 06:47:17 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Shi", "Qinfeng", ""], ["Reid", "Mark", ""], ["Caetano", "Tiberio", ""], ["Hengel", "Anton van den", ""], ["Wang", "Zhenhua", ""]]}, {"id": "1402.1956", "submitter": "Lakhdar Sais", "authors": "Said Jabbour and Jerry Lonlac and Lakhdar Sais and Yakoub Salhi", "title": "Revisiting the Learned Clauses Database Reduction Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit an important issue of CDCL-based SAT solvers,\nnamely the learned clauses database management policies. Our motivation takes\nits source from a simple observation on the remarkable performances of both\nrandom and size-bounded reduction strategies. We first derive a simple\nreduction strategy, called Size-Bounded Randomized strategy (in short SBR),\nthat combines maintaing short clauses (of size bounded by k), while deleting\nrandomly clauses of size greater than k. The resulting strategy outperform the\nstate-of-the-art, namely the LBD based one, on SAT instances taken from the\nlast SAT competition. Reinforced by the interest of keeping short clauses, we\npropose several new dynamic variants, and we discuss their performances.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 15:14:24 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Jabbour", "Said", ""], ["Lonlac", "Jerry", ""], ["Sais", "Lakhdar", ""], ["Salhi", "Yakoub", ""]]}, {"id": "1402.1958", "submitter": "Arthur Guez", "authors": "Arthur Guez, David Silver, Peter Dayan", "title": "Better Optimism By Bayes: Adaptive Planning with Rich Models", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational costs of inference and planning have confined Bayesian\nmodel-based reinforcement learning to one of two dismal fates: powerful\nBayes-adaptive planning but only for simplistic models, or powerful, Bayesian\nnon-parametric models but using simple, myopic planning strategies such as\nThompson sampling. We ask whether it is feasible and truly beneficial to\ncombine rich probabilistic models with a closer approximation to fully Bayesian\nplanning. First, we use a collection of counterexamples to show formal problems\nwith the over-optimism inherent in Thompson sampling. Then we leverage\nstate-of-the-art techniques in efficient Bayes-adaptive planning and\nnon-parametric Bayesian methods to perform qualitatively better than both\nexisting conventional algorithms and Thompson sampling on two contextual\nbandit-like problems.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 15:38:57 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Guez", "Arthur", ""], ["Silver", "David", ""], ["Dayan", "Peter", ""]]}, {"id": "1402.1986", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Recommandation mobile, sensible au contexte de contenus \\'evolutifs:\n  Contextuel-E-Greedy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce in this paper an algorithm named Contextuel-E-Greedy that\ntackles the dynamicity of the user's content. It is based on dynamic\nexploration/exploitation tradeoff and can adaptively balance the two aspects by\ndeciding which situation is most relevant for exploration or exploitation. The\nexperimental results demonstrate that our algorithm outperforms surveyed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 20:28:55 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1402.2300", "submitter": "Aaron Karper", "authors": "Aaron Karper", "title": "Feature and Variable Selection in Classification", "comments": "Part of master seminar in document analysis held by Marcus\n  Eichenberger-Liwicki", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The amount of information in the form of features and variables avail- able\nto machine learning algorithms is ever increasing. This can lead to classifiers\nthat are prone to overfitting in high dimensions, high di- mensional models do\nnot lend themselves to interpretable results, and the CPU and memory resources\nnecessary to run on high-dimensional datasets severly limit the applications of\nthe approaches. Variable and feature selection aim to remedy this by finding a\nsubset of features that in some way captures the information provided best. In\nthis paper we present the general methodology and highlight some specific\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 21:05:58 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Karper", "Aaron", ""]]}, {"id": "1402.2359", "submitter": "Josef Urban", "authors": "Cezary Kaliszyk, Josef Urban, Ji\\v{r}\\'i Vysko\\v{c}il", "title": "Machine Learner for Automated Reasoning 0.4 and 0.5", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learner for Automated Reasoning (MaLARea) is a learning and reasoning\nsystem for proving in large formal libraries where thousands of theorems are\navailable when attacking a new conjecture, and a large number of related\nproblems and proofs can be used to learn specific theorem-proving knowledge.\nThe last version of the system has by a large margin won the 2013 CASC LTB\ncompetition. This paper describes the motivation behind the methods used in\nMaLARea, discusses the general approach and the issues arising in evaluation of\nsuch system, and describes the Mizar@Turing100 and CASC'24 versions of MaLARea.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 03:42:00 GMT"}, {"version": "v2", "created": "Wed, 28 May 2014 13:51:17 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""], ["Vysko\u010dil", "Ji\u0159\u00ed", ""]]}, {"id": "1402.2871", "submitter": "Christopher Amato", "authors": "Christopher Amato and George D. Konidaris and Gabriel Cruz and\n  Christopher A. Maynor and Jonathan P. How and Leslie P. Kaelbling", "title": "Planning for Decentralized Control of Multiple Robots Under Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a probabilistic framework for synthesizing control policies for\ngeneral multi-robot systems, given environment and sensor models and a cost\nfunction. Decentralized, partially observable Markov decision processes\n(Dec-POMDPs) are a general model of decision processes where a team of agents\nmust cooperate to optimize some objective (specified by a shared reward or cost\nfunction) in the presence of uncertainty, but where communication limitations\nmean that the agents cannot share their state, so execution must proceed in a\ndecentralized fashion. While Dec-POMDPs are typically intractable to solve for\nreal-world problems, recent research on the use of macro-actions in Dec-POMDPs\nhas significantly increased the size of problem that can be practically solved\nas a Dec-POMDP. We describe this general model, and show how, in contrast to\nmost existing methods that are specialized to a particular problem class, it\ncan synthesize control policies that use whatever opportunities for\ncoordination are present in the problem, while balancing off uncertainty in\noutcomes, sensor information, and information about other agents. We use three\nvariations on a warehouse task to show that a single planner of this type can\ngenerate cooperative behavior using task allocation, direct communication, and\nsignaling, as appropriate.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 16:13:16 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Amato", "Christopher", ""], ["Konidaris", "George D.", ""], ["Cruz", "Gabriel", ""], ["Maynor", "Christopher A.", ""], ["How", "Jonathan P.", ""], ["Kaelbling", "Leslie P.", ""]]}, {"id": "1402.2959", "submitter": "Sebastien Verel", "authors": "Gabriela Ochoa, S\\'ebastien Verel (LISIC), Fabio Daolio (ISI), Marco\n  Tomassini (ISI)", "title": "Local Optima Networks: A New Model of Combinatorial Fitness Landscapes", "comments": null, "journal-ref": "Recent Advances in the Theory and Application of Fitness\n  Landscapes, Hendrik Richter, Andries Engelbrecht (Ed.) (2014) 233-262", "doi": "10.1007/978-3-642-41888-4_9", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter overviews a recently introduced network-based model of\ncombinatorial landscapes: Local Optima Networks (LON). The model compresses the\ninformation given by the whole search space into a smaller mathematical object\nthat is a graph having as vertices the local optima and as edges the possible\nweighted transitions between them. Two definitions of edges have been proposed:\nbasin-transition and escape-edges, which capture relevant topological features\nof the underlying search spaces. This network model brings a new set of metrics\nto characterize the structure of combinatorial landscapes, those associated\nwith the science of complex networks. These metrics are described, and results\nare presented of local optima network extraction and analysis for two selected\ncombinatorial landscapes: NK landscapes and the quadratic assignment problem.\nNetwork features are found to correlate with and even predict the performance\nof heuristic search algorithms operating on these problems.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 20:21:54 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Ochoa", "Gabriela", "", "LISIC"], ["Verel", "S\u00e9bastien", "", "LISIC"], ["Daolio", "Fabio", "", "ISI"], ["Tomassini", "Marco", "", "ISI"]]}, {"id": "1402.3044", "submitter": "Piotr Skowron", "authors": "Piotr Skowron and Piotr Faliszewski and Jerome Lang", "title": "Finding a Collective Set of Items: From Proportional Multirepresentation\n  to Group Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem: There is a set of items (e.g., movies) and\na group of agents (e.g., passengers on a plane); each agent has some intrinsic\nutility for each of the items. Our goal is to pick a set of $K$ items that\nmaximize the total derived utility of all the agents (i.e., in our example we\nare to pick $K$ movies that we put on the plane's entertainment system).\nHowever, the actual utility that an agent derives from a given item is only a\nfraction of its intrinsic one, and this fraction depends on how the agent ranks\nthe item among the chosen, available, ones. We provide a formal specification\nof the model and provide concrete examples and settings where it is applicable.\nWe show that the problem is hard in general, but we show a number of\ntractability results for its natural special cases.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 07:14:28 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 19:51:00 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Skowron", "Piotr", ""], ["Faliszewski", "Piotr", ""], ["Lang", "Jerome", ""]]}, {"id": "1402.3096", "submitter": "Irfan Deli research", "authors": "Irfan Deli and Naim \\c{C}a\\u{g}man", "title": "Relations on FP-Soft Sets Applied to Decision Making Problems", "comments": "soft applications", "journal-ref": "Journal of New Theory 3 (2015) 98-107", "doi": null, "report-no": null, "categories": "math.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we first define relations on the fuzzy parametrized soft sets\nand study their properties. We also give a decision making method based on\nthese relations. In approximate reasoning, relations on the fuzzy parametrized\nsoft sets have shown to be of a primordial importance. Finally, the method is\nsuccessfully applied to a problems that contain uncertainties.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 11:39:27 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Deli", "Irfan", ""], ["\u00c7a\u011fman", "Naim", ""]]}, {"id": "1402.3490", "submitter": "Xinyang Deng", "authors": "Xinyang Deng, Yong Deng", "title": "D numbers theory: a generalization of Dempster-Shafer theory", "comments": "This paper has been withdrawn by the authors due to a crucial error\n  of the combination rule", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dempster-Shafer theory is widely applied to uncertainty modelling and\nknowledge reasoning due to its ability of expressing uncertain information.\nHowever, some conditions, such as exclusiveness hypothesis and completeness\nconstraint, limit its development and application to a large extend. To\novercome these shortcomings in Dempster-Shafer theory and enhance its\ncapability of representing uncertain information, a novel theory called D\nnumbers theory is systematically proposed in this paper. Within the proposed\ntheory, uncertain information is expressed by D numbers, reasoning and\nsynthesization of information are implemented by D numbers combination rule.\nThe proposed D numbers theory is an generalization of Dempster-Shafer theory,\nwhich inherits the advantage of Dempster-Shafer theory and strengthens its\ncapability of uncertainty modelling.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 15:15:26 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 15:47:48 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Deng", "Xinyang", ""], ["Deng", "Yong", ""]]}, {"id": "1402.3578", "submitter": "Cezary Kaliszyk", "authors": "Cezary Kaliszyk and Josef Urban", "title": "Learning-assisted Theorem Proving with Millions of Lemmas", "comments": "journal version of arXiv:1310.2797 (which was submitted to LPAR\n  conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large formal mathematical libraries consist of millions of atomic inference\nsteps that give rise to a corresponding number of proved statements (lemmas).\nAnalogously to the informal mathematical practice, only a tiny fraction of such\nstatements is named and re-used in later proofs by formal mathematicians. In\nthis work, we suggest and implement criteria defining the estimated usefulness\nof the HOL Light lemmas for proving further theorems. We use these criteria to\nmine the large inference graph of the lemmas in the HOL Light and Flyspeck\nlibraries, adding up to millions of the best lemmas to the pool of statements\nthat can be re-used in later proofs. We show that in combination with\nlearning-based relevance filtering, such methods significantly strengthen\nautomated theorem proving of new conjectures over large formal mathematical\nlibraries such as Flyspeck.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 03:08:02 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""]]}, {"id": "1402.3613", "submitter": "Pavel Janovsk\\'y", "authors": "Pavel Janovsk\\'y and Michal \\v{C}\\'ap and Ji\\v{r}\\'i Vok\\v{r}\\'inek", "title": "Finding Coordinated Paths for Multiple Holonomic Agents in 2-d Polygonal\n  Environment", "comments": "Proceedings of the 13th International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Avoiding collisions is one of the vital tasks for systems of autonomous\nmobile agents. We focus on the problem of finding continuous coordinated paths\nfor multiple mobile disc agents in a 2-d environment with polygonal obstacles.\nThe problem is PSPACE-hard, with the state space growing exponentially in the\nnumber of agents. Therefore, the state of the art methods include mainly\nreactive techniques and sampling-based iterative algorithms.\n  We compare the performance of a widely-used reactive method ORCA with three\nvariants of a popular planning algorithm RRT* applied to multi-agent path\nplanning and find that an algorithm combining reactive collision avoidance and\nRRT* planning, which we call ORCA-RRT* can be used to solve instances that are\nout of the reach of either of the techniques. We experimentally show that: 1)\nthe reactive part of the algorithm can efficiently solve many multi-agent path\nfinding problems involving large number of agents, for which RRT* algorithm is\noften unable to find a solution in limited time and 2) the planning component\nof the algorithm is able to solve many instances containing local minima, where\nreactive techniques typically fail.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 22:09:44 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Janovsk\u00fd", "Pavel", ""], ["\u010c\u00e1p", "Michal", ""], ["Vok\u0159\u00ednek", "Ji\u0159\u00ed", ""]]}, {"id": "1402.3664", "submitter": "Xinyang Deng", "authors": "Xinyang Deng, Yong Hu, Felix Chan, Sankaran Mahadevan, Yong Deng", "title": "Parameter estimation based on interval-valued belief structures", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation based on uncertain data represented as belief structures\nis one of the latest problems in the Dempster-Shafer theory. In this paper, a\nnovel method is proposed for the parameter estimation in the case where belief\nstructures are uncertain and represented as interval-valued belief structures.\nWithin our proposed method, the maximization of likelihood criterion and\nminimization of estimated parameter's uncertainty are taken into consideration\nsimultaneously. As an illustration, the proposed method is employed to estimate\nparameters for deterministic and uncertain belief structures, which\ndemonstrates its effectiveness and versatility.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2014 08:07:49 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Deng", "Xinyang", ""], ["Hu", "Yong", ""], ["Chan", "Felix", ""], ["Mahadevan", "Sankaran", ""], ["Deng", "Yong", ""]]}, {"id": "1402.4157", "submitter": "Jan-Peter Calliess", "authors": "Jan-Peter Calliess, Michael Osborne, Stephen Roberts", "title": "Conservative collision prediction and avoidance for stochastic\n  trajectories in continuous time and space", "comments": "This preprint is an extended version of a conference paper that is to\n  appear in \\textit{Proceedings of the 13th International Conference on\n  Autonomous Agents and Multiagent Systems (AAMAS 2014)}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing work in multi-agent collision prediction and avoidance typically\nassumes discrete-time trajectories with Gaussian uncertainty or that are\ncompletely deterministic. We propose an approach that allows detection of\ncollisions even between continuous, stochastic trajectories with the only\nrestriction that means and variances can be computed. To this end, we employ\nprobabilistic bounds to derive criterion functions whose negative sign provably\nis indicative of probable collisions. For criterion functions that are\nLipschitz, an algorithm is provided to rapidly find negative values or prove\ntheir absence. We propose an iterative policy-search approach that avoids prior\ndiscretisations and yields collision-free trajectories with adjustably high\ncertainty. We test our method with both fixed-priority and auction-based\nprotocols for coordinating the iterative planning process. Results are provided\nin collision-avoidance simulations of feedback controlled plants.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 21:46:26 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 15:38:42 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Calliess", "Jan-Peter", ""], ["Osborne", "Michael", ""], ["Roberts", "Stephen", ""]]}, {"id": "1402.4303", "submitter": "Christian Geist", "authors": "Christian Geist", "title": "Finding Preference Profiles of Condorcet Dimension $k$ via SAT", "comments": "Corrected typos, updated references, and added conclusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Condorcet winning sets are a set-valued generalization of the well-known\nconcept of a Condorcet winner. As supersets of Condorcet winning sets are\nalways Condorcet winning sets themselves, an interesting property of preference\nprofiles is the size of the smallest Condorcet winning set they admit. This\nsmallest size is called the Condorcet dimension of a preference profile. Since\nlittle is known about profiles that have a certain Condorcet dimension, we show\nin this paper how the problem of finding a preference profile that has a given\nCondorcet dimension can be encoded as a satisfiability problem and solved by a\nSAT solver. Initial results include a minimal example of a preference profile\nof Condorcet dimension 3, improving previously known examples both in terms of\nthe number of agents as well as alternatives. Due to the high complexity of\nsuch problems it remains open whether a preference profile of Condorcet\ndimension 4 exists.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 11:31:08 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 17:01:37 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Geist", "Christian", ""]]}, {"id": "1402.4306", "submitter": "Amar Shah", "authors": "Amar Shah, Andrew Gordon Wilson and Zoubin Ghahramani", "title": "Student-t Processes as Alternatives to Gaussian Processes", "comments": "13 pages, 6 figures, 1 table. To appear in \"The Seventeenth\n  International Conference on Artificial Intelligence and Statistics (AISTATS),\n  2014.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the Student-t process as an alternative to the Gaussian\nprocess as a nonparametric prior over functions. We derive closed form\nexpressions for the marginal likelihood and predictive distribution of a\nStudent-t process, by integrating away an inverse Wishart process prior over\nthe covariance kernel of a Gaussian process model. We show surprising\nequivalences between different hierarchical Gaussian process models leading to\nStudent-t processes, and derive a new sampling scheme for the inverse Wishart\nprocess, which helps elucidate these equivalences. Overall, we show that a\nStudent-t process can retain the attractive properties of a Gaussian process --\na nonparametric representation, analytic marginal and predictive distributions,\nand easy model selection through covariance kernels -- but has enhanced\nflexibility, and predictive covariances that, unlike a Gaussian process,\nexplicitly depend on the values of training observations. We verify empirically\nthat a Student-t process is especially useful in situations where there are\nchanges in covariance structure, or in applications like Bayesian optimization,\nwhere accurate predictive covariances are critical for good performance. These\nadvantages come at no additional computational cost over Gaussian processes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 11:47:38 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 10:49:16 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Shah", "Amar", ""], ["Wilson", "Andrew Gordon", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1402.4413", "submitter": "Marijn Heule", "authors": "Shai Haim and Marijn Heule", "title": "Towards Ultra Rapid Restarts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe a trend regarding restart strategies used in SAT solvers. A few\nyears ago, most state-of-the-art solvers restarted on average after a few\nthousands of backtracks. Currently, restarting after a dozen backtracks results\nin much better performance. The main reason for this trend is that heuristics\nand data structures have become more restart-friendly. We expect further\ncontinuation of this trend, so future SAT solvers will restart even more\nrapidly. Additionally, we present experimental results to support our\nobservations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 17:39:39 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Haim", "Shai", ""], ["Heule", "Marijn", ""]]}, {"id": "1402.4455", "submitter": "Marijn Heule", "authors": "Sid Mijnders and Boris de Wilde and Marijn Heule", "title": "Symbiosis of Search and Heuristics for Random 3-SAT", "comments": "Proceedings of the Third International Workshop on Logic and Search\n  (LaSh 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When combined properly, search techniques can reveal the full potential of\nsophisticated branching heuristics. We demonstrate this observation on the\nwell-known class of random 3-SAT formulae. First, a new branching heuristic is\npresented, which generalizes existing work on this class. Much smaller search\ntrees can be constructed by using this heuristic. Second, we introduce a\nvariant of discrepancy search, called ALDS. Theoretical and practical evidence\nsupport that ALDS traverses the search tree in a near-optimal order when\ncombined with the new heuristic. Both techniques, search and heuristic, have\nbeen implemented in the look-ahead solver march. The SAT 2009 competition\nresults show that march is by far the strongest complete solver on random k-SAT\nformulae.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 19:59:58 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Mijnders", "Sid", ""], ["de Wilde", "Boris", ""], ["Heule", "Marijn", ""]]}, {"id": "1402.4465", "submitter": "Marijn Heule", "authors": "Peter van der Tak and Marijn J.H. Heule and Armin Biere", "title": "Concurrent Cube-and-Conquer", "comments": "Third International Workshop on Pragmatics of SAT (PoS 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work introduced the cube-and-conquer technique to solve hard SAT\ninstances. It partitions the search space into cubes using a lookahead solver.\nEach cube is tackled by a conflict-driven clause learning (CDCL) solver.\nCrucial for strong performance is the cutoff heuristic that decides when to\nswitch from lookahead to CDCL. Yet, this offline heuristic is far from ideal.\nIn this paper, we present a novel hybrid solver that applies the cube and\nconquer steps simultaneously. A lookahead and a CDCL solver work together on\neach cube, while communication is restricted to synchronization. Our concurrent\ncube-and-conquer solver can solve many instances faster than pure lookahead,\npure CDCL and offline cube-and-conquer, and can abort early in favor of a pure\nCDCL search if an instance is not suitable for cube-and-conquer techniques.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 20:39:30 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["van der Tak", "Peter", ""], ["Heule", "Marijn J. H.", ""], ["Biere", "Armin", ""]]}, {"id": "1402.4525", "submitter": "Saminda Abeyruwan", "authors": "Saminda Abeyruwan and Andreas Seekircher and Ubbo Visser", "title": "Off-Policy General Value Functions to Represent Dynamic Role Assignments\n  in RoboCup 3D Soccer Simulation", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Collecting and maintaining accurate world knowledge in a dynamic, complex,\nadversarial, and stochastic environment such as the RoboCup 3D Soccer\nSimulation is a challenging task. Knowledge should be learned in real-time with\ntime constraints. We use recently introduced Off-Policy Gradient Descent\nalgorithms within Reinforcement Learning that illustrate learnable knowledge\nrepresentations for dynamic role assignments. The results show that the agents\nhave learned competitive policies against the top teams from the RoboCup 2012\ncompetitions for three vs three, five vs five, and seven vs seven agents. We\nhave explicitly used subsets of agents to identify the dynamics and the\nsemantics for which the agents learn to maximize their performance measures,\nand to gather knowledge about different objectives, so that all agents\nparticipate effectively and efficiently within the group.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 23:01:13 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Abeyruwan", "Saminda", ""], ["Seekircher", "Andreas", ""], ["Visser", "Ubbo", ""]]}, {"id": "1402.4542", "submitter": "Chunguo Li", "authors": "Chun-Guo Li, Xing Mei, Bao-Gang Hu", "title": "Unsupervised Ranking of Multi-Attribute Objects Based on Principal\n  Curves", "comments": "This paper has 14 pages and 9 figures. The paper has submitted to\n  IEEE Transactions on Knowledge and Data Engineering (TKDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised ranking faces one critical challenge in evaluation applications,\nthat is, no ground truth is available. When PageRank and its variants show a\ngood solution in related subjects, they are applicable only for ranking from\nlink-structure data. In this work, we focus on unsupervised ranking from\nmulti-attribute data which is also common in evaluation tasks. To overcome the\nchallenge, we propose five essential meta-rules for the design and assessment\nof unsupervised ranking approaches: scale and translation invariance, strict\nmonotonicity, linear/nonlinear capacities, smoothness, and explicitness of\nparameter size. These meta-rules are regarded as high level knowledge for\nunsupervised ranking tasks. Inspired by the works in [8] and [14], we propose a\nranking principal curve (RPC) model, which learns a one-dimensional manifold\nfunction to perform unsupervised ranking tasks on multi-attribute observations.\nFurthermore, the RPC is modeled to be a cubic B\\'ezier curve with control\npoints restricted in the interior of a hypercube, thereby complying with all\nthe five meta-rules to infer a reasonable ranking list. With control points as\nthe model parameters, one is able to understand the learned manifold and to\ninterpret the ranking list semantically. Numerical experiments of the presented\nRPC model are conducted on two open datasets of different ranking applications.\nIn comparison with the state-of-the-art approaches, the new model is able to\nshow more reasonable ranking lists.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 01:29:14 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Li", "Chun-Guo", ""], ["Mei", "Xing", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "1402.4699", "submitter": "Shujia Liu", "authors": "Shujia Liu", "title": "A Powerful Genetic Algorithm for Traveling Salesman Problem", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a powerful genetic algorithm(GA) to solve the traveling\nsalesman problem (TSP). To construct a powerful GA, I use edge swapping(ES)\nwith a local search procedure to determine good combinations of building blocks\nof parent solutions for generating even better offspring solutions.\nExperimental results on well studied TSP benchmarks demonstrate that the\nproposed GA is competitive in finding very high quality solutions on instances\nwith up to 16,862 cities.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 15:33:35 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Liu", "Shujia", ""]]}, {"id": "1402.4741", "submitter": "Julio Lemos", "authors": "Julio Lemos", "title": "A normative account of defeasible and probabilistic inference", "comments": null, "journal-ref": "Intuitio 6 (2), 2013, pp. 211-219", "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide more evidence for the contention that logical\nconsequence should be understood in normative terms. Hartry Field and John\nMacFarlane covered the classical case. We extend their work, examining what it\nmeans for an agent to be obliged to infer a conclusion when faced with\nuncertain information or reasoning within a non-monotonic, defeasible, logical\nframework (which allows e. g. for inference to be drawn from premises\nconsidered true unless evidence to the contrary is presented).\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 17:52:00 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Lemos", "Julio", ""]]}, {"id": "1402.4834", "submitter": "Mir Ehsan Hesam Sadati", "authors": "Mir Ehsan Hesam Sadati, Jamshid Bagherzadeh Mohasefi", "title": "The Application of Imperialist Competitive Algorithm for Fuzzy Random\n  Portfolio Selection Problem", "comments": "5 pages, 2 tables, Published with International Journal of Computer\n  Applications (IJCA)", "journal-ref": "International Journal of Computer Applications 79(9):10-14,\n  October 2013", "doi": "10.5120/13767-1618", "report-no": null, "categories": "math.OC cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents an implementation of the Imperialist Competitive\nAlgorithm (ICA) for solving the fuzzy random portfolio selection problem where\nthe asset returns are represented by fuzzy random variables. Portfolio\nOptimization is an important research field in modern finance. By using the\nnecessity-based model, fuzzy random variables reformulate to the linear\nprogramming and ICA will be designed to find the optimum solution. To show the\nefficiency of the proposed method, a numerical example illustrates the whole\nidea on implementation of ICA for fuzzy random portfolio selection problem.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 21:37:17 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Sadati", "Mir Ehsan Hesam", ""], ["Mohasefi", "Jamshid Bagherzadeh", ""]]}, {"id": "1402.4914", "submitter": "Vikash Mansinghka", "authors": "Vikash Mansinghka and Eric Jonas", "title": "Building fast Bayesian computing machines out of intentionally\n  stochastic, digital parts", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.AR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain interprets ambiguous sensory information faster and more reliably\nthan modern computers, using neurons that are slower and less reliable than\nlogic gates. But Bayesian inference, which underpins many computational models\nof perception and cognition, appears computationally challenging even given\nmodern transistor speeds and energy budgets. The computational principles and\nstructures needed to narrow this gap are unknown. Here we show how to build\nfast Bayesian computing machines using intentionally stochastic, digital parts,\nnarrowing this efficiency gap by multiple orders of magnitude. We find that by\nconnecting stochastic digital components according to simple mathematical\nrules, one can build massively parallel, low precision circuits that solve\nBayesian inference problems and are compatible with the Poisson firing\nstatistics of cortical neurons. We evaluate circuits for depth and motion\nperception, perceptual learning and causal reasoning, each performing inference\nover 10,000+ latent variables in real time - a 1,000x speed advantage over\ncommodity microprocessors. These results suggest a new role for randomness in\nthe engineering and reverse-engineering of intelligent computation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 07:17:03 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Mansinghka", "Vikash", ""], ["Jonas", "Eric", ""]]}, {"id": "1402.5034", "submitter": "Lucas Paletta", "authors": "Sigal Sina, Sarit Kraus, Avi Rosenfeld", "title": "Using the Crowd to Generate Content for Scenario-Based Serious-Games", "comments": null, "journal-ref": null, "doi": null, "report-no": "IDGEI/2014/03", "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, scenario-based serious-games have become a main tool for\nlearning new skills and capabilities. An important factor in the development of\nsuch systems is the overhead in time, cost and human resources to manually\ncreate the content for these scenarios. We focus on how to create content for\nscenarios in medical, military, commerce and gaming applications where\nmaintaining the integrity and coherence of the content is integral for the\nsystem's success. To do so, we present an automatic method for generating\ncontent about everyday activities through combining computer science techniques\nwith the crowd. We use the crowd in three basic ways: to capture a database of\nscenarios of everyday activities, to generate a database of likely replacements\nfor specific events within that scenario, and to evaluate the resulting\nscenarios. We found that the generated scenarios were rated as reliable and\nconsistent by the crowd when compared to the scenarios that were originally\ncaptured. We also compared the generated scenarios to those created by\ntraditional planning techniques. We found that both methods were equally\neffective in generated reliable and consistent scenarios, yet the main\nadvantages of our approach is that the content we generate is more varied and\nmuch easier to create. We have begun integrating this approach within a\nscenario-based training application for novice investigators within the law\nenforcement departments to improve their questioning skills.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 15:33:03 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Sina", "Sigal", ""], ["Kraus", "Sarit", ""], ["Rosenfeld", "Avi", ""]]}, {"id": "1402.5037", "submitter": "Lucas Paletta", "authors": "Ian Dunwell, Panagiotis Petridis, Petros Lameras, Maurice Hendrix, and\n  Stella Doukianou, Mark Gaved", "title": "Assessing the Reach and Impact of Game-Based Learning Approaches to\n  Cultural Competency and Behavioural Change", "comments": null, "journal-ref": null, "doi": null, "report-no": "IDGEI/2014/08", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As digital games continue to be explored as solutions to educational and\nbehavioural challenges, the need for evaluation methodologies which support\nboth the unique nature of the format and the need for comparison with other\napproaches continues to increase. In this workshop paper, a range of challenges\nare described related specifically to the case of cultural learning using\ndigital games, in terms of how it may best be assessed, understood, and\nsustained through an iterative process supported by research. An evaluation\nframework is proposed, identifying metrics for reach and impact and their\nassociated challenges, as well as presenting ethical considerations and the\nmeans to utilize evaluation outcomes within an iterative cycle, and to provide\nfeedback to learners. Presenting as a case study a serious game from the Mobile\nAssistance for Social Inclusion and Empowerment of Immigrants with Persuasive\nLearning Technologies and Social Networks (MASELTOV) project, the use of the\nframework in the context of an integrative project is discussed, with emphasis\non the need to view game-based learning as a blended component of the cultural\nlearning process, rather than a standalone solution. The particular case of\nmobile gaming is also considered within this case study, providing a platform\nby which to deliver and update content in response to evaluation outcomes.\nDiscussion reflects upon the general challenges related to the assessment of\ncultural learning, and behavioural change in more general terms, suggesting\nfuture work should address the need to provide sustainable, research-driven\nplatforms for game-based learning content.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 15:37:23 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Dunwell", "Ian", ""], ["Petridis", "Panagiotis", ""], ["Lameras", "Petros", ""], ["Hendrix", "Maurice", ""], ["Doukianou", "Stella", ""], ["Gaved", "Mark", ""]]}, {"id": "1402.5039", "submitter": "Lucas Paletta", "authors": "Haza\u007fel Jones, Nicolas Sabouret, Ionut Damian, Tobias Baur, Elisabeth\n  Andr\\'e, Ka\\'ska Porayska-Pomsta and Paola Rizzo", "title": "Interpreting social cues to generate credible affective reactions of\n  virtual job interviewers", "comments": null, "journal-ref": null, "doi": null, "report-no": "IDGEI/2014/06", "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a mechanism of generating credible affective\nreactions in a virtual recruiter during an interaction with a user. This is\ndone using communicative performance computation based on the behaviours of the\nuser as detected by a recognition module. The proposed software pipeline is\npart of the TARDIS system which aims to aid young job seekers in acquiring job\ninterview related social skills. In this context, our system enables the\nvirtual recruiter to realistically adapt and react to the user in real-time.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 15:38:43 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2014 11:04:53 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Jones", "Haza\u007fel", ""], ["Sabouret", "Nicolas", ""], ["Damian", "Ionut", ""], ["Baur", "Tobias", ""], ["Andr\u00e9", "Elisabeth", ""], ["Porayska-Pomsta", "Ka\u015bka", ""], ["Rizzo", "Paola", ""]]}, {"id": "1402.5043", "submitter": "Lucas Paletta", "authors": "Marwen Belkaid, Nicolas Sabouret", "title": "A logical model of Theory of Mind for virtual agents in the context of\n  job interview simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": "IDGEI/2014/10", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Job interview simulation with a virtual agents aims at improving people's\nsocial skills and supporting professional inclusion. In such simulators, the\nvirtual agent must be capable of representing and reasoning about the user's\nmental state based on social cues that inform the system about his/her affects\nand social attitude. In this paper, we propose a formal model of Theory of Mind\n(ToM) for virtual agent in the context of human-agent interaction that focuses\non the affective dimension. It relies on a hybrid ToM that combines the two\nmajor paradigms of the domain. Our framework is based on modal logic and\ninference rules about the mental states, emotions and social relations of both\nactors. Finally, we present preliminary results regarding the impact of such a\nmodel on natural interaction in the context of job interviews simulation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 15:40:08 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Belkaid", "Marwen", ""], ["Sabouret", "Nicolas", ""]]}, {"id": "1402.5045", "submitter": "Lucas Paletta", "authors": "Nicolas Sabouret, Haza\\\"el Jones, Magalie Ochs, Mathieu Chollet,\n  Catherine Pelachaud", "title": "Expressing social attitudes in virtual agents for social training games", "comments": null, "journal-ref": null, "doi": null, "report-no": "IDGEI/2014/11", "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of virtual agents in social coaching has increased rapidly in the\nlast decade. In order to train the user in different situations than can occur\nin real life, the virtual agent should be able to express different social\nattitudes. In this paper, we propose a model of social attitudes that enables a\nvirtual agent to reason on the appropriate social attitude to express during\nthe interaction with a user given the course of the interaction, but also the\nemotions, mood and personality of the agent. Moreover, the model enables the\nvirtual agent to display its social attitude through its non-verbal behaviour.\nThe proposed model has been developed in the context of job interview\nsimulation. The methodology used to develop such a model combined a theoretical\nand an empirical approach. Indeed, the model is based both on the literature in\nHuman and Social Sciences on social attitudes but also on the analysis of an\naudiovisual corpus of job interviews and on post-hoc interviews with the\nrecruiters on their expressed attitudes during the job interview.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 15:41:26 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Sabouret", "Nicolas", ""], ["Jones", "Haza\u00ebl", ""], ["Ochs", "Magalie", ""], ["Chollet", "Mathieu", ""], ["Pelachaud", "Catherine", ""]]}, {"id": "1402.5161", "submitter": "Roberto Rossi", "authors": "Roberto Rossi and Steven Prestwich and S. Armagan Tarim", "title": "Statistical Constraints", "comments": null, "journal-ref": "Proceedings of the 21st European Conference on Artificial\n  Intelligence, ECAI 2014, August 18-22, 2014, Prague, Czech Republic, IOS\n  Press, Volume 263, pp. 777-782, 2014", "doi": "10.3233/978-1-61499-419-0-777", "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce statistical constraints, a declarative modelling tool that links\nstatistics and constraint programming. We discuss two statistical constraints\nand some associated filtering algorithms. Finally, we illustrate applications\nto standard problems encountered in statistics and to a novel inspection\nscheduling problem in which the aim is to find inspection plans with desirable\nstatistical properties.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 22:29:59 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2014 21:38:16 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2014 20:16:41 GMT"}, {"version": "v4", "created": "Sun, 1 Jun 2014 14:39:30 GMT"}, {"version": "v5", "created": "Thu, 14 Aug 2014 21:33:04 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Rossi", "Roberto", ""], ["Prestwich", "Steven", ""], ["Tarim", "S. Armagan", ""]]}, {"id": "1402.5205", "submitter": "Thilagavathi Jayamurugan", "authors": "D. Thilagavathi, Antony Selvadoss Thanamani", "title": "A Survey on Dynamic Job Scheduling in Grid Environment Based on\n  Heuristic Algorithms", "comments": "6 Pages, 1 FiguerE, \"Published with International Journal of Computer\n  Trends and Technology (IJCTT)\". arXiv admin note: contains excessive text\n  overlap with other internet sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational Grids are a new trend in distributed computing systems. They\nallow the sharing of geographically distributed resources in an efficient way,\nextending the boundaries of what we perceive as distributed computing. Various\nsciences can benefit from the use of grids to solve CPU-intensive problems,\ncreating potential benefits to the entire society. Job scheduling is an\nintegrated part of parallel and distributed computing. It allows selecting\ncorrect match of resource for a particular job and thus increases the job\nthroughput and utilization of resources. Job should be scheduled in an\nautomatic way to make the system more reliable, accessible and less sensitive\nto subsystem failures. This paper provides a survey on various heuristic\nalgorithms, used for scheduling in grid.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 05:01:24 GMT"}], "update_date": "2014-08-24", "authors_parsed": [["Thilagavathi", "D.", ""], ["Thanamani", "Antony Selvadoss", ""]]}, {"id": "1402.5358", "submitter": "J\\'anos P\\'anovics", "authors": "Tam\\'as K\\'adek and J\\'anos P\\'anovics", "title": "Extended Breadth-First Search Algorithm", "comments": "5 pages, 1 figure, 1 table", "journal-ref": "International Journal of Computer Science Issues, Volume 10, Issue\n  6, No 2, ISSN (Print): 1694-0814, ISSN (Online): 1694-0784, November 2013,\n  pp. 78-82", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of artificial intelligence is to provide representation techniques\nfor describing problems, as well as search algorithms that can be used to\nanswer our questions. A widespread and elaborated model is state-space\nrepresentation, which, however, has some shortcomings. Classical search\nalgorithms are not applicable in practice when the state space contains even\nonly a few tens of thousands of states. We can give remedy to this problem by\ndefining some kind of heuristic knowledge. In case of classical state-space\nrepresentation, heuristic must be defined so that it qualifies an arbitrary\nstate based on its \"goodness,\" which is obviously not trivial. In our paper, we\nintroduce an algorithm that gives us the ability to handle huge state spaces\nand to use a heuristic concept which is easier to embed into search algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 17:21:52 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["K\u00e1dek", "Tam\u00e1s", ""], ["P\u00e1novics", "J\u00e1nos", ""]]}, {"id": "1402.5379", "submitter": "Eray Ozkural", "authors": "Eray \\\"Ozkural", "title": "What Is It Like to Be a Brain Simulation?", "comments": "10 pages, draft of conference paper published in AGI 2012, also\n  accepted to AISB 2012 but it was too late to arrange travel, unfortunately;\n  Artificial General Intelligence, 5th International Conference, AGI 2012,\n  Oxford, UK, December 8-11, 2012. Proceedings", "journal-ref": null, "doi": "10.1007/978-3-642-35506-6_24", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We frame the question of what kind of subjective experience a brain\nsimulation would have in contrast to a biological brain. We discuss the brain\nprosthesis thought experiment. We evaluate how the experience of the brain\nsimulation might differ from the biological, according to a number of\nhypotheses about experience and the properties of simulation. Then, we identify\nfiner questions relating to the original inquiry, and answer them from both a\ngeneral physicalist, and panexperientialist perspective.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2014 17:19:53 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["\u00d6zkural", "Eray", ""]]}, {"id": "1402.5380", "submitter": "Eray Ozkural", "authors": "Eray \\\"Ozkural", "title": "Godseed: Benevolent or Malevolent?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is hypothesized by some thinkers that benign looking AI objectives may\nresult in powerful AI drives that may pose an existential risk to human\nsociety. We analyze this scenario and find the underlying assumptions to be\nunlikely. We examine the alternative scenario of what happens when universal\ngoals that are not human-centric are used for designing AI agents. We follow a\ndesign approach that tries to exclude malevolent motivations from AI agents,\nhowever, we see that objectives that seem benevolent may pose significant risk.\nWe consider the following meta-rules: preserve and pervade life and culture,\nmaximize the number of free minds, maximize intelligence, maximize wisdom,\nmaximize energy production, behave like human, seek pleasure, accelerate\nevolution, survive, maximize control, and maximize capital. We also discuss\nvarious solution approaches for benevolent behavior including selfless goals,\nhybrid designs, Darwinism, universal constraints, semi-autonomy, and\ngeneralization of robot laws. A \"prime directive\" for AI may help in\nformulating an encompassing constraint for avoiding malicious behavior. We\nhypothesize that social instincts for autonomous robots may be effective such\nas attachment learning. We mention multiple beneficial scenarios for an\nadvanced semi-autonomous AGI agent in the near future including space\nexploration, automation of industries, state functions, and cities. We conclude\nthat a beneficial AI agent with intelligence beyond human-level is possible and\nhas many practical use cases.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2014 17:35:53 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 21:43:51 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["\u00d6zkural", "Eray", ""]]}, {"id": "1402.5436", "submitter": "Alessandro Provetti", "authors": "Gianpaolo Brignoli, Stefania Costantini, Ottavio D'Antona, Alessandro\n  Provetti", "title": "Characterizing and computing stable models of logic programs: The\n  non-stratified case", "comments": "Proceedings of the Conference on Information Technology. Bhubaneswar,\n  India, 1999. https://sites.google.com/site/citconference/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stable Logic Programming (SLP) is an emergent, alternative style of logic\nprogramming: each solution to a problem is represented by a stable model of a\ndeductive database/function-free logic program encoding the problem itself.\nSeveral implementations now exist for stable logic programming, and their\nperformance is rapidly improving. To make SLP generally applicable, it should\nbe possible to check for consistency (i.e., existence of stable models) of the\ninput program before attempting to answer queries. In the literature, only\nrather strong sufficient conditions have been proposed for consistency, e.g.,\nstratification. This paper extends these results in several directions. First,\nthe syntactic features of programs, viz. cyclic negative dependencies,\naffecting the existence of stable models are characterized, and their relevance\nis discussed. Next, a new graph representation of logic programs, the Extended\nDependency Graph (EDG), is introduced, which conveys enough information for\nreasoning about stable models (while the traditional Dependency Graph does\nnot). Finally, we show that the problem of the existence of stable models can\nbe reformulated in terms of coloring of the EDG.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 22:17:39 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Brignoli", "Gianpaolo", ""], ["Costantini", "Stefania", ""], ["D'Antona", "Ottavio", ""], ["Provetti", "Alessandro", ""]]}, {"id": "1402.5458", "submitter": "Sindhu Kutty", "authors": "Jacob Abernethy, Sindhu Kutty, S\\'ebastien Lahaie, Rahul Sami", "title": "Information Aggregation in Exponential Family Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the design of prediction market mechanisms known as automated\nmarket makers. We show that we can design these mechanisms via the mold of\n\\emph{exponential family distributions}, a popular and well-studied probability\ndistribution template used in statistics. We give a full development of this\nrelationship and explore a range of benefits. We draw connections between the\ninformation aggregation of market prices and the belief aggregation of learning\nagents that rely on exponential family distributions. We develop a very natural\nanalysis of the market behavior as well as the price equilibrium under the\nassumption that the traders exhibit risk aversion according to exponential\nutility. We also consider similar aspects under alternative models, such as\nwhen traders are budget constrained.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 01:11:05 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Abernethy", "Jacob", ""], ["Kutty", "Sindhu", ""], ["Lahaie", "S\u00e9bastien", ""], ["Sami", "Rahul", ""]]}, {"id": "1402.5593", "submitter": "Rustam Tagiew", "authors": "Rustam Tagiew and Dmitry I. Ignatov", "title": "Reciprocity in Gift-Exchange-Games", "comments": "6 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an analysis of data from a gift-exchange-game experiment.\nThe experiment was described in `The Impact of Social Comparisons on\nReciprocity' by G\\\"achter et al. 2012. Since this paper uses state-of-art data\nscience techniques, the results provide a different point of view on the\nproblem. As already shown in relevant literature from experimental economics,\nhuman decisions deviate from rational payoff maximization. The average gift\nrate was $31$%. Gift rate was under no conditions zero. Further, we derive some\nspecial findings and calculate their significance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 10:07:59 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Tagiew", "Rustam", ""], ["Ignatov", "Dmitry I.", ""]]}, {"id": "1402.5684", "submitter": "Orhan Firat", "authors": "Orhan Firat and Mete Ozay and Ilke Oztekin and Fatos T. Yarman Vural", "title": "Discriminative Functional Connectivity Measures for Brain Decoding", "comments": "This paper has been withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical learning model for classifying cognitive processes\nbased on distributed patterns of neural activation in the brain, acquired via\nfunctional magnetic resonance imaging (fMRI). In the proposed learning method,\nlocal meshes are formed around each voxel. The distance between voxels in the\nmesh is determined by using a functional neighbourhood concept. In order to\ndefine the functional neighbourhood, the similarities between the time series\nrecorded for voxels are measured and functional connectivity matrices are\nconstructed. Then, the local mesh for each voxel is formed by including the\nfunctionally closest neighbouring voxels in the mesh. The relationship between\nthe voxels within a mesh is estimated by using a linear regression model. These\nrelationship vectors, called Functional Connectivity aware Local Relational\nFeatures (FC-LRF) are then used to train a statistical learning machine. The\nproposed method was tested on a recognition memory experiment, including data\npertaining to encoding and retrieval of words belonging to ten different\nsemantic categories. Two popular classifiers, namely k-nearest neighbour (k-nn)\nand Support Vector Machine (SVM), are trained in order to predict the semantic\ncategory of the item being retrieved, based on activation patterns during\nencoding. The classification performance of the Functional Mesh Learning model,\nwhich range in 62%-71% is superior to the classical multi-voxel pattern\nanalysis (MVPA) methods, which range in 40%-48%, for ten semantic categories.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 22:01:11 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2014 19:02:07 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Firat", "Orhan", ""], ["Ozay", "Mete", ""], ["Oztekin", "Ilke", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1402.5830", "submitter": "Luca Vassio Mr", "authors": "Enrico Ampellio and Luca Vassio", "title": "A hybrid swarm-based algorithm for single-objective optimization\n  problems involving high-cost analyses", "comments": "19 pages, 4 figures, Springer Swarm Intelligence", "journal-ref": "Swarm Intelligence 10, 99-121 (2016)", "doi": "10.1007/s11721-016-0121-6", "report-no": null, "categories": "math.OC cs.AI cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many technical fields, single-objective optimization procedures in\ncontinuous domains involve expensive numerical simulations. In this context, an\nimprovement of the Artificial Bee Colony (ABC) algorithm, called the Artificial\nsuper-Bee enhanced Colony (AsBeC), is presented. AsBeC is designed to provide\nfast convergence speed, high solution accuracy and robust performance over a\nwide range of problems. It implements enhancements of the ABC structure and\nhybridizations with interpolation strategies. The latter are inspired by the\nquadratic trust region approach for local investigation and by an efficient\nglobal optimizer for separable problems. Each modification and their combined\neffects are studied with appropriate metrics on a numerical benchmark, which is\nalso used for comparing AsBeC with some effective ABC variants and other\nderivative-free algorithms. In addition, the presented algorithm is validated\non two recent benchmarks adopted for competitions in international conferences.\nResults show remarkable competitiveness and robustness for AsBeC.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 14:06:52 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 14:55:37 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ampellio", "Enrico", ""], ["Vassio", "Luca", ""]]}, {"id": "1402.5878", "submitter": "Lucas Paletta", "authors": "Alexandra Cetto, Michael Netter, G\\\"unther Pernul, Christian\n  Richthammer, Moritz Riesner, Christian Roth, Johannes S\\\"anger", "title": "Friend Inspector: A Serious Game to Enhance Privacy Awareness in Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "IDGEI/2014/01", "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, many users of Social Network Sites are insufficiently aware of who\ncan see their shared personal items. Nonetheless, most approaches focus on\nenhancing privacy in Social Networks through improved privacy settings,\nneglecting the fact that privacy awareness is a prerequisite for privacy\ncontrol. Social Network users first need to know about privacy issues before\nbeing able to make adjustments. In this paper, we introduce Friend Inspector, a\nserious game that allows its users to playfully increase their privacy\nawareness on Facebook. Since its launch, Friend Inspector has attracted a\nsignificant number of visitors, emphasising the need for better tools to\nunderstand privacy settings on Social Networks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 15:26:15 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Cetto", "Alexandra", ""], ["Netter", "Michael", ""], ["Pernul", "G\u00fcnther", ""], ["Richthammer", "Christian", ""], ["Riesner", "Moritz", ""], ["Roth", "Christian", ""], ["S\u00e4nger", "Johannes", ""]]}, {"id": "1402.5886", "submitter": "Shervin Javdani", "authors": "Shervin Javdani, Yuxin Chen, Amin Karbasi, Andreas Krause, J. Andrew\n  Bagnell, Siddhartha Srinivasa", "title": "Near Optimal Bayesian Active Learning for Decision Making", "comments": "Extended version of work appearing in the International conference on\n  Artificial Intelligence and Statistics (AISTATS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should we gather information to make effective decisions? We address\nBayesian active learning and experimental design problems, where we\nsequentially select tests to reduce uncertainty about a set of hypotheses.\nInstead of minimizing uncertainty per se, we consider a set of overlapping\ndecision regions of these hypotheses. Our goal is to drive uncertainty into a\nsingle decision region as quickly as possible.\n  We identify necessary and sufficient conditions for correctly identifying a\ndecision region that contains all hypotheses consistent with observations. We\ndevelop a novel Hyperedge Cutting (HEC) algorithm for this problem, and prove\nthat is competitive with the intractable optimal policy. Our efficient\nimplementation of the algorithm relies on computing subsets of the complete\nhomogeneous symmetric polynomials. Finally, we demonstrate its effectiveness on\ntwo practical applications: approximate comparison-based learning and active\nlocalization using a robot manipulator.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 16:59:21 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Javdani", "Shervin", ""], ["Chen", "Yuxin", ""], ["Karbasi", "Amin", ""], ["Krause", "Andreas", ""], ["Bagnell", "J. Andrew", ""], ["Srinivasa", "Siddhartha", ""]]}, {"id": "1402.5988", "submitter": "Nikos Katzouris", "authors": "Nikos Katzouris, Alexander Artikis, George Paliouras", "title": "Incremental Learning of Event Definitions with Inductive Logic\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event recognition systems rely on properly engineered knowledge bases of\nevent definitions to infer occurrences of events in time. The manual\ndevelopment of such knowledge is a tedious and error-prone task, thus\nevent-based applications may benefit from automated knowledge construction\ntechniques, such as Inductive Logic Programming (ILP), which combines machine\nlearning with the declarative and formal semantics of First-Order Logic.\nHowever, learning temporal logical formalisms, which are typically utilized by\nlogic-based Event Recognition systems is a challenging task, which most ILP\nsystems cannot fully undertake. In addition, event-based data is usually\nmassive and collected at different times and under various circumstances.\nIdeally, systems that learn from temporal data should be able to operate in an\nincremental mode, that is, revise prior constructed knowledge in the face of\nnew evidence. Most ILP systems are batch learners, in the sense that in order\nto account for new evidence they have no alternative but to forget past\nknowledge and learn from scratch. Given the increased inherent complexity of\nILP and the volumes of real-life temporal data, this results to algorithms that\nscale poorly. In this work we present an incremental method for learning and\nrevising event-based knowledge, in the form of Event Calculus programs. The\nproposed algorithm relies on abductive-inductive learning and comprises a\nscalable clause refinement methodology, based on a compressive summarization of\nclause coverage in a stream of examples. We present an empirical evaluation of\nour approach on real and synthetic data from activity recognition and city\ntransport applications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 21:22:51 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 13:24:29 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Katzouris", "Nikos", ""], ["Artikis", "Alexander", ""], ["Paliouras", "George", ""]]}, {"id": "1402.5991", "submitter": "Issac Shams", "authors": "Issac Shams, Saeede Ajorlou, Kai Yang", "title": "A predictive analytics approach to reducing avoidable hospital\n  readmission", "comments": "30 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hospital readmission has become a critical metric of quality and cost of\nhealthcare. Medicare anticipates that nearly $17 billion is paid out on the 20%\nof patients who are readmitted within 30 days of discharge. Although several\ninterventions such as transition care management and discharge reengineering\nhave been practiced in recent years, the effectiveness and sustainability\ndepends on how well they can identify and target patients at high risk of\nrehospitalization. Based on the literature, most current risk prediction models\nfail to reach an acceptable accuracy level; none of them considers patient's\nhistory of readmission and impacts of patient attribute changes over time; and\nthey often do not discriminate between planned and unnecessary readmissions.\nTackling such drawbacks, we develop a new readmission metric based on\nadministrative data that can identify potentially avoidable readmissions from\nall other types of readmission. We further propose a tree based classification\nmethod to estimate the predicted probability of readmission that can directly\nincorporate patient's history of readmission and risk factors changes over\ntime. The proposed methods are validated with 2011-12 Veterans Health\nAdministration data from inpatients hospitalized for heart failure, acute\nmyocardial infarction, pneumonia, or chronic obstructive pulmonary disease in\nthe State of Michigan. Results shows improved discrimination power compared to\nthe literature (c-statistics>80%) and good calibration.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 21:37:25 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2014 23:42:51 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Shams", "Issac", ""], ["Ajorlou", "Saeede", ""], ["Yang", "Kai", ""]]}, {"id": "1402.6028", "submitter": "Volodymyr Kuleshov", "authors": "Volodymyr Kuleshov and Doina Precup", "title": "Algorithms for multi-armed bandit problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many algorithms for the multi-armed bandit problem are\nwell-understood theoretically, empirical confirmation of their effectiveness is\ngenerally scarce. This paper presents a thorough empirical study of the most\npopular multi-armed bandit algorithms. Three important observations can be made\nfrom our results. Firstly, simple heuristics such as epsilon-greedy and\nBoltzmann exploration outperform theoretically sound algorithms on most\nsettings by a significant margin. Secondly, the performance of most algorithms\nvaries dramatically with the parameters of the bandit problem. Our study\nidentifies for each algorithm the settings where it performs well, and the\nsettings where it performs poorly. Thirdly, the algorithms' performance\nrelative each to other is affected only by the number of bandit arms and the\nvariance of the rewards. This finding may guide the design of subsequent\nempirical evaluations. In the second part of the paper, we turn our attention\nto an important area of application of bandit algorithms: clinical trials.\nAlthough the design of clinical trials has been one of the principal practical\nproblems motivating research on multi-armed bandits, bandit algorithms have\nnever been evaluated as potential treatment allocation strategies. Using data\nfrom a real study, we simulate the outcome that a 2001-2002 clinical trial\nwould have had if bandit algorithms had been used to allocate patients to\ntreatments. We find that an adaptive trial would have successfully treated at\nleast 50% more patients, while significantly reducing the number of adverse\neffects and increasing patient retention. At the end of the trial, the best\ntreatment could have still been identified with a high level of statistical\nconfidence. Our findings demonstrate that bandit algorithms are attractive\nalternatives to current adaptive treatment allocation strategies.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 01:34:43 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Kuleshov", "Volodymyr", ""], ["Precup", "Doina", ""]]}, {"id": "1402.6077", "submitter": "Zhi-Hua Zhou", "authors": "Wang-Zhou Dai and Zhi-Hua Zhou", "title": "Inductive Logic Boosting", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a surge of interest in Probabilistic Logic Programming\n(PLP) and Statistical Relational Learning (SRL) models that combine logic with\nprobabilities. Structure learning of these systems is an intersection area of\nInductive Logic Programming (ILP) and statistical learning (SL). However, ILP\ncannot deal with probabilities, SL cannot model relational hypothesis. The\nbiggest challenge of integrating these two machine learning frameworks is how\nto estimate the probability of a logic clause only from the observation of\ngrounded logic atoms. Many current methods models a joint probability by\nrepresenting clause as graphical model and literals as vertices in it. This\nmodel is still too complicate and only can be approximate by pseudo-likelihood.\nWe propose Inductive Logic Boosting framework to transform the relational\ndataset into a feature-based dataset, induces logic rules by boosting Problog\nRule Trees and relaxes the independence constraint of pseudo-likelihood.\nExperimental evaluation on benchmark datasets demonstrates that the AUC-PR and\nAUC-ROC value of ILP learned rules are higher than current state-of-the-art SRL\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 07:53:49 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Dai", "Wang-Zhou", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1402.6109", "submitter": "Sebastian Ordyniak", "authors": "Eun Jung Kim, Sebastian Ordyniak, Stefan Szeider", "title": "The Complexity of Repairing, Adjusting, and Aggregating of Extensions in\n  Abstract Argumentation", "comments": null, "journal-ref": "Proc. TAFA 2013, pp. 158-175, Springer LNCS", "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of problems that arise in abstract\nargumentation in the context of dynamic argumentation, minimal change, and\naggregation. In particular, we consider the following problems where always an\nargumentation framework F and a small positive integer k are given.\n  - The Repair problem asks whether a given set of arguments can be modified\ninto an extension by at most k elementary changes (i.e., the extension is of\ndistance k from the given set).\n  - The Adjust problem asks whether a given extension can be modified by at\nmost k elementary changes into an extension that contains a specified argument.\n  - The Center problem asks whether, given two extensions of distance k,\nwhether there is a \"center\" extension that is a distance at most (k-1) from\nboth given extensions.\n  We study these problems in the framework of parameterized complexity, and\ntake the distance k as the parameter. Our results covers several different\nsemantics, including admissible, complete, preferred, semi-stable and stable\nsemantics.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 09:45:23 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Kim", "Eun Jung", ""], ["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1402.6208", "submitter": "Thomas Lansdall-Welfare", "authors": "Ilias Flaounas, Thomas Lansdall-Welfare, Panagiota Antonakaki, Nello\n  Cristianini", "title": "The Anatomy of a Modular System for Media Content Analysis", "comments": "Updated to include previously missing figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent systems for the annotation of media content are increasingly\nbeing used for the automation of parts of social science research. In this\ndomain the problem of integrating various Artificial Intelligence (AI)\nalgorithms into a single intelligent system arises spontaneously. As part of\nour ongoing effort in automating media content analysis for the social\nsciences, we have built a modular system by combining multiple AI modules into\na flexible framework in which they can cooperate in complex tasks. Our system\ncombines data gathering, machine translation, topic classification, extraction\nand annotation of entities and social networks, as well as many other tasks\nthat have been perfected over the past years of AI research. Over the last few\nyears, it has allowed us to realise a series of scientific studies over a vast\nrange of applications including comparative studies between news outlets and\nmedia content in different countries, modelling of user preferences, and\nmonitoring public mood. The framework is flexible and allows the design and\nimplementation of modular agents, where simple modules cooperate in the\nannotation of a large dataset without central coordination.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 15:45:16 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 16:25:07 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Flaounas", "Ilias", ""], ["Lansdall-Welfare", "Thomas", ""], ["Antonakaki", "Panagiota", ""], ["Cristianini", "Nello", ""]]}, {"id": "1402.6485", "submitter": "Martin Vatshelle", "authors": "Sigve Hortemo S{\\ae}ther, Jan Arne Telle, Martin Vatshelle", "title": "Solving MaxSAT and #SAT on structured CNF formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a structural parameter of CNF formulas and use it to\nidentify instances of weighted MaxSAT and #SAT that can be solved in polynomial\ntime. Given a CNF formula we say that a set of clauses is precisely satisfiable\nif there is some complete assignment satisfying these clauses only. Let the\nps-value of the formula be the number of precisely satisfiable sets of clauses.\nApplying the notion of branch decompositions to CNF formulas and using ps-value\nas cut function, we define the ps-width of a formula. For a formula given with\na decomposition of polynomial ps-width we show dynamic programming algorithms\nsolving weighted MaxSAT and #SAT in polynomial time. Combining with results of\n'Belmonte and Vatshelle, Graph classes with structured neighborhoods and\nalgorithmic applications, Theor. Comput. Sci. 511: 54-65 (2013)' we get\npolynomial-time algorithms solving weighted MaxSAT and #SAT for some classes of\nstructured CNF formulas. For example, we get $O(m^2(m + n)s)$ algorithms for\nformulas $F$ of $m$ clauses and $n$ variables and size $s$, if $F$ has a linear\nordering of the variables and clauses such that for any variable $x$ occurring\nin clause $C$, if $x$ appears before $C$ then any variable between them also\noccurs in $C$, and if $C$ appears before $x$ then $x$ occurs also in any clause\nbetween them. Note that the class of incidence graphs of such formulas do not\nhave bounded clique-width.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 10:48:36 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["S\u00e6ther", "Sigve Hortemo", ""], ["Telle", "Jan Arne", ""], ["Vatshelle", "Martin", ""]]}, {"id": "1402.6556", "submitter": "Csaba P\\u{a}tca\\c{s}", "authors": "Csaba Patcas and Attila Bartha", "title": "Evolutionary solving of the debts' clearing problem", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The debts' clearing problem is about clearing all the debts in a group of n\nentities (persons, companies etc.) using a minimal number of money transaction\noperations. The problem is known to be NP-hard in the strong sense. As for many\nintractable problems, techniques from the field of artificial intelligence are\nuseful in finding solutions close to optimum for large inputs. An evolutionary\nalgorithm for solving the debts' clearing problem is proposed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 14:39:57 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Patcas", "Csaba", ""], ["Bartha", "Attila", ""]]}, {"id": "1402.6560", "submitter": "Jesus Cerquides", "authors": "Jordi Roca-Lacostena, Jesus Cerquides", "title": "Even more generic solution construction in Valuation-Based Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valuation algebras abstract a large number of formalisms for automated\nreasoning and enable the definition of generic inference procedures. Many of\nthese formalisms provide some notions of solutions. Typical examples are\nsatisfying assignments in constraint systems, models in logics or solutions to\nlinear equation systems.\n  Recently, formal requirements for the presence of solutions and a generic\nalgorithm for solution construction based on the results of a previously\nexecuted inference scheme have been proposed in the literature. Unfortunately,\nthe formalization of Pouly and Kohlas relies on a theorem for which we provide\na counter example. In spite of that, the mainline of the theory described is\ncorrect, although some of the necessary conditions to apply some of the\nalgorithms have to be revised. To fix the theory, we generalize some of their\ndefinitions and provide correct sufficient conditions for the algorithms. As a\nresult, we get a more general and corrected version of the already existing\ntheory.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 14:51:57 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Roca-Lacostena", "Jordi", ""], ["Cerquides", "Jesus", ""]]}, {"id": "1402.6663", "submitter": "Pierre De Loor", "authors": "Pierre De Loor, Kristen Manach and Jacques Tisseau", "title": "Enaction-Based Artificial Intelligence: Toward Coevolution with Humans\n  in the Loop", "comments": null, "journal-ref": "Minds and Machine, num 19, pp 319-343, 2009", "doi": "10.1007/s11023-009-9165-3", "report-no": null, "categories": "cs.AI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the links between the enaction paradigm and\nartificial intelligence. Enaction is considered a metaphor for artificial\nintelligence, as a number of the notions which it deals with are deemed\nincompatible with the phenomenal field of the virtual. After explaining this\nstance, we shall review previous works regarding this issue in terms of\nartifical life and robotics. We shall focus on the lack of recognition of\nco-evolution at the heart of these approaches. We propose to explicitly\nintegrate the evolution of the environment into our approach in order to refine\nthe ontogenesis of the artificial system, and to compare it with the enaction\nparadigm. The growing complexity of the ontogenetic mechanisms to be activated\ncan therefore be compensated by an interactive guidance system emanating from\nthe environment. This proposition does not however resolve that of the\nrelevance of the meaning created by the machine (sense-making). Such\nreflections lead us to integrate human interaction into this environment in\norder to construct relevant meaning in terms of participative artificial\nintelligence. This raises a number of questions with regards to setting up an\nenactive interaction. The article concludes by exploring a number of issues,\nthereby enabling us to associate current approaches with the principles of\nmorphogenesis, guidance, the phenomenology of interactions and the use of\nminimal enactive interfaces in setting up experiments which will deal with the\nproblem of artificial intelligence in a variety of enaction-based ways.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 20:10:39 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["De Loor", "Pierre", ""], ["Manach", "Kristen", ""], ["Tisseau", "Jacques", ""]]}, {"id": "1402.6763", "submitter": "Alan Malek", "authors": "Yasin Abbasi-Yadkori, Peter L. Bartlett, Alan Malek", "title": "Linear Programming for Large-Scale Markov Decision Problems", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of controlling a Markov decision process (MDP) with a\nlarge state space, so as to minimize average cost. Since it is intractable to\ncompete with the optimal policy for large scale problems, we pursue the more\nmodest goal of competing with a low-dimensional family of policies. We use the\ndual linear programming formulation of the MDP average cost problem, in which\nthe variable is a stationary distribution over state-action pairs, and we\nconsider a neighborhood of a low-dimensional subset of the set of stationary\ndistributions (defined in terms of state-action features) as the comparison\nclass. We propose two techniques, one based on stochastic convex optimization,\nand one based on constraint sampling. In both cases, we give bounds that show\nthat the performance of our algorithms approaches the best achievable by any\npolicy in the comparison class. Most importantly, these results depend on the\nsize of the comparison class, but not on the size of the state space.\nPreliminary experiments show the effectiveness of the proposed algorithms in a\nqueuing application.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 01:43:38 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Abbasi-Yadkori", "Yasin", ""], ["Bartlett", "Peter L.", ""], ["Malek", "Alan", ""]]}, {"id": "1402.6785", "submitter": "EPTCS", "authors": "Gal Katz (Bar Ilan University), Doron Peled (Bar Ilan University)", "title": "Synthesis of Parametric Programs using Genetic Programming and Model\n  Checking", "comments": "In Proceedings INFINITY 2013, arXiv:1402.6610", "journal-ref": "EPTCS 140, 2014, pp. 70-84", "doi": "10.4204/EPTCS.140.5", "report-no": null, "categories": "cs.SE cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal methods apply algorithms based on mathematical principles to enhance\nthe reliability of systems. It would only be natural to try to progress from\nverification, model checking or testing a system against its formal\nspecification into constructing it automatically. Classical algorithmic\nsynthesis theory provides interesting algorithms but also alarming high\ncomplexity and undecidability results. The use of genetic programming, in\ncombination with model checking and testing, provides a powerful heuristic to\nsynthesize programs. The method is not completely automatic, as it is fine\ntuned by a user that sets up the specification and parameters. It also does not\nguarantee to always succeed and converge towards a solution that satisfies all\nthe required properties. However, we applied it successfully on quite\nnontrivial examples and managed to find solutions to hard programming\nchallenges, as well as to improve and to correct code. We describe here several\nversions of our method for synthesizing sequential and concurrent systems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 03:45:20 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Katz", "Gal", "", "Bar Ilan University"], ["Peled", "Doron", "", "Bar Ilan University"]]}, {"id": "1402.7122", "submitter": "Anonymous Anonymous Mr.", "authors": "Meghyn Bienvenu, Diego Calvanese, Magdalena Ortiz, Mantas Simkus", "title": "Nested Regular Path Queries in Description Logics", "comments": "added Figure 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-way regular path queries (2RPQs) have received increased attention\nrecently due to their ability to relate pairs of objects by flexibly navigating\ngraph-structured data. They are present in property paths in SPARQL 1.1, the\nnew standard RDF query language, and in the XML query language XPath. In line\nwith XPath, we consider the extension of 2RPQs with nesting, which allows one\nto require that objects along a path satisfy complex conditions, in turn\nexpressed through (nested) 2RPQs. We study the computational complexity of\nanswering nested 2RPQs and conjunctions thereof (CN2RPQs) in the presence of\ndomain knowledge expressed in description logics (DLs). We establish tight\ncomplexity bounds in data and combined complexity for a variety of DLs, ranging\nfrom lightweight DLs (DL-Lite, EL) up to highly expressive ones. Interestingly,\nwe are able to show that adding nesting to (C)2RPQs does not affect worst-case\ndata complexity of query answering for any of the considered DLs. However, in\nthe case of lightweight DLs, adding nesting to 2RPQs leads to a surprising jump\nin combined complexity, from P-complete to Exp-complete.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 02:52:57 GMT"}, {"version": "v2", "created": "Tue, 4 Mar 2014 18:18:00 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Bienvenu", "Meghyn", ""], ["Calvanese", "Diego", ""], ["Ortiz", "Magdalena", ""], ["Simkus", "Mantas", ""]]}, {"id": "1402.7276", "submitter": "Vaishak Belle", "authors": "Vaishak Belle, Hector Levesque", "title": "Robot Location Estimation in the Situation Calculus", "comments": "Appears in Proceedings of the Eleventh International Symposium on\n  Logical Formalizations on Commonsense Reasoning, Cyprus, May 27-29, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location estimation is a fundamental sensing task in robotic applications,\nwhere the world is uncertain, and sensors and effectors are noisy. Most systems\nmake various assumptions about the dependencies between state variables, and\nespecially about how these dependencies change as a result of actions. Building\non a general framework by Bacchus, Halpern and Levesque for reasoning about\ndegrees of belief in the situation calculus, and a recent extension to it for\ncontinuous domains, in this paper we illustrate location estimation in the\npresence of a rich theory of actions using an example. We also show that while\nactions might affect prior distributions in nonstandard ways, suitable\nposterior beliefs are nonetheless entailed as a side-effect of the overall\nspecification.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 15:12:17 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Belle", "Vaishak", ""], ["Levesque", "Hector", ""]]}]