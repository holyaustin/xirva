[{"id": "1711.00043", "submitter": "Guillaume Lample", "authors": "Guillaume Lample, Alexis Conneau, Ludovic Denoyer, Marc'Aurelio\n  Ranzato", "title": "Unsupervised Machine Translation Using Monolingual Corpora Only", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation has recently achieved impressive performance thanks to\nrecent advances in deep learning and the availability of large-scale parallel\ncorpora. There have been numerous attempts to extend these successes to\nlow-resource language pairs, yet requiring tens of thousands of parallel\nsentences. In this work, we take this research direction to the extreme and\ninvestigate whether it is possible to learn to translate even without any\nparallel data. We propose a model that takes sentences from monolingual corpora\nin two different languages and maps them into the same latent space. By\nlearning to reconstruct in both languages from this shared feature space, the\nmodel effectively learns to translate without using any labeled data. We\ndemonstrate our model on two widely used datasets and two language pairs,\nreporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French\ndatasets, without using even a single parallel sentence at training time.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 18:31:11 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 13:30:28 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Lample", "Guillaume", ""], ["Conneau", "Alexis", ""], ["Denoyer", "Ludovic", ""], ["Ranzato", "Marc'Aurelio", ""]]}, {"id": "1711.00054", "submitter": "Lei Lin", "authors": "Zhenhua Zhang, Lei Lin", "title": "Abnormal Spatial-Temporal Pattern Analysis for Niagara Frontier Border\n  Wait Times", "comments": "submitted to ITS World Congress 2017 Montreal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Border crossing delays cause problems like huge economics loss and heavy\nenvironmental pollutions. To understand more about the nature of border\ncrossing delay, this study applies a dictionary-based compression algorithm to\nprocess the historical Niagara Frontier border wait times data. It can identify\nthe abnormal spatial-temporal patterns for both passenger vehicles and trucks\nat three bridges connecting US and Canada. Furthermore, it provides a\nquantitate anomaly score to rank the wait times patterns across the three\nbridges for each vehicle type and each direction. By analyzing the top three\nmost abnormal patterns, we find that there are at least two factors\ncontributing the anomaly of the patterns. The weekends and holidays may cause\nunusual heave congestions at the three bridges at the same time, and the\nfreight transportation demand may be uneven from Canada to the USA at Peace\nBridge and Lewiston-Queenston Bridge, which may lead to a high anomaly score.\nBy calculating the frequency of the top 5% abnormal patterns by hour of the\nday, the results show that for cars from the USA to Canada, the frequency of\nabnormal waiting time patterns is the highest during noon while for trucks in\nthe same direction, it is the highest during the afternoon peak hours. For\nCanada to US direction, the frequency of abnormal border wait time patterns for\nboth cars and trucks reaches to the peak during the afternoon. The analysis of\nabnormal spatial-temporal wait times patterns is promising to improve the\nborder crossing management\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 18:53:26 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Zhang", "Zhenhua", ""], ["Lin", "Lei", ""]]}, {"id": "1711.00066", "submitter": "Konrad Zolna", "authors": "Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio", "title": "Fraternal Dropout", "comments": "Accepted to ICLR 2018. Extended appendix. Added official GitHub code\n  for replication: https://github.com/kondiz/fraternal-dropout . Added\n  references. Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are important class of architectures among\nneural networks useful for language modeling and sequential prediction.\nHowever, optimizing RNNs is known to be harder compared to feed-forward neural\nnetworks. A number of techniques have been proposed in literature to address\nthis problem. In this paper we propose a simple technique called fraternal\ndropout that takes advantage of dropout to achieve this goal. Specifically, we\npropose to train two identical copies of an RNN (that share parameters) with\ndifferent dropout masks while minimizing the difference between their\n(pre-softmax) predictions. In this way our regularization encourages the\nrepresentations of RNNs to be invariant to dropout mask, thus being robust. We\nshow that our regularization term is upper bounded by the expectation-linear\ndropout objective which has been shown to address the gap due to the difference\nbetween the train and inference phases of dropout. We evaluate our model and\nachieve state-of-the-art results in sequence modeling tasks on two benchmark\ndatasets - Penn Treebank and Wikitext-2. We also show that our approach leads\nto performance improvement by a significant margin in image captioning\n(Microsoft COCO) and semi-supervised (CIFAR-10) tasks.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 19:32:45 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 16:40:34 GMT"}, {"version": "v3", "created": "Wed, 31 Jan 2018 00:12:47 GMT"}, {"version": "v4", "created": "Wed, 28 Mar 2018 15:50:58 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Zolna", "Konrad", ""], ["Arpit", "Devansh", ""], ["Suhubdy", "Dendi", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1711.00106", "submitter": "Victor Zhong", "authors": "Caiming Xiong, Victor Zhong, Richard Socher", "title": "DCN+: Mixed Objective and Deep Residual Coattention for Question\n  Answering", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional models for question answering optimize using cross entropy loss,\nwhich encourages exact answers at the cost of penalizing nearby or overlapping\nanswers that are sometimes equally accurate. We propose a mixed objective that\ncombines cross entropy loss with self-critical policy learning. The objective\nuses rewards derived from word overlap to solve the misalignment between\nevaluation metric and optimization objective. In addition to the mixed\nobjective, we improve dynamic coattention networks (DCN) with a deep residual\ncoattention encoder that is inspired by recent work in deep self-attention and\nresidual networks. Our proposals improve model performance across question\ntypes and input lengths, especially for long questions that requires the\nability to capture long-term dependencies. On the Stanford Question Answering\nDataset, our model achieves state-of-the-art results with 75.1% exact match\naccuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy\nand 86.0% F1.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 20:53:42 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 20:56:56 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Xiong", "Caiming", ""], ["Zhong", "Victor", ""], ["Socher", "Richard", ""]]}, {"id": "1711.00107", "submitter": "James Goldfarb", "authors": "James W Goldfarb", "title": "Separation of Water and Fat Magnetic Resonance Imaging Signals Using\n  Deep Learning with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: A new method for magnetic resonance (MR) imaging water-fat\nseparation using a convolutional neural network (ConvNet) and deep learning\n(DL) is presented. Feasibility of the method with complex and magnitude images\nis demonstrated with a series of patient studies and accuracy of predicted\nquantitative values is analyzed.\n  Methods: Water-fat separation of 1200 gradient-echo acquisitions from 90\nimaging sessions (normal, acute and chronic myocardial infarction) was\nperformed using a conventional model based method with modeling of R2* and\noff-resonance and a multi-peak fat spectrum. A U-Net convolutional neural\nnetwork for calculation of water-only, fat-only, R2* and off-resonance images\nwas trained with 900 gradient-echo Multiple and single-echo complex and\nmagnitude input data algorithms were studied and compared to conventional\nextended echo modeling.\n  Results: The U-Net ConvNet was easily trained and provided water-fat\nseparation results visually comparable to conventional methods. Myocardial fat\ndeposition in chronic myocardial infarction and intramyocardial hemorrhage in\nacute myocardial infarction were well visualized in the DL results. Predicted\nvalues for R2*, off-resonance, water and fat signal intensities were well\ncorrelated with conventional model based water fat separation (R2>=0.97,\np<0.001). DL images had a 14% higher signal-to-noise ratio (p<0.001) when\ncompared to the conventional method.\n  Conclusion: Deep learning utilizing ConvNets is a feasible method for MR\nwater-fat separationimaging with complex, magnitude and single echo image data.\nA trained U-Net can be efficiently used for MR water-fat separation, providing\nresults comparable to conventional model based methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 17:36:36 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Goldfarb", "James W", ""]]}, {"id": "1711.00108", "submitter": "Elliot Meyerson", "authors": "Elliot Meyerson and Risto Miikkulainen", "title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer\n  Ordering", "comments": "14 pages (main paper: 10 pages). Published as a conference paper at\n  ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep multitask learning (MTL) approaches align layers shared between\ntasks in a parallel ordering. Such an organization significantly constricts the\ntypes of shared structure that can be learned. The necessity of parallel\nordering for deep MTL is first tested by comparing it with permuted ordering of\nshared layers. The results indicate that a flexible ordering can enable more\neffective sharing, thus motivating the development of a soft ordering approach,\nwhich learns how shared layers are applied in different ways for different\ntasks. Deep MTL with soft ordering outperforms parallel ordering methods across\na series of domains. These results suggest that the power of deep MTL comes\nfrom learning highly general building blocks that can be assembled to meet the\ndemands of each task.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 20:55:06 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 02:05:34 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Meyerson", "Elliot", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "1711.00129", "submitter": "Xiao Li", "authors": "Xiao Li, Yao Ma and Calin Belta", "title": "Automata-Guided Hierarchical Reinforcement Learning for Skill\n  Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skills learned through (deep) reinforcement learning often generalizes poorly\nacross domains and re-training is necessary when presented with a new task. We\npresent a framework that combines techniques in \\textit{formal methods} with\n\\textit{reinforcement learning} (RL). The methods we provide allows for\nconvenient specification of tasks with logical expressions, learns hierarchical\npolicies (meta-controller and low-level controllers) with well-defined\nintrinsic rewards, and construct new skills from existing ones with little to\nno additional exploration. We evaluate the proposed methods in a simple grid\nworld simulation as well as a more complicated kitchen environment in AI2Thor\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 22:21:02 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 01:38:04 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Li", "Xiao", ""], ["Ma", "Yao", ""], ["Belta", "Calin", ""]]}, {"id": "1711.00137", "submitter": "Jacob Schreiber", "authors": "Jacob Schreiber", "title": "Pomegranate: fast and flexible probabilistic modeling in python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present pomegranate, an open source machine learning package for\nprobabilistic modeling in Python. Probabilistic modeling encompasses a wide\nrange of methods that explicitly describe uncertainty using probability\ndistributions. Three widely used probabilistic models implemented in\npomegranate are general mixture models, hidden Markov models, and Bayesian\nnetworks. A primary focus of pomegranate is to abstract away the complexities\nof training models from their definition. This allows users to focus on\nspecifying the correct model for their application instead of being limited by\ntheir understanding of the underlying algorithms. An aspect of this focus\ninvolves the collection of additive sufficient statistics from data sets as a\nstrategy for training models. This approach trivially enables many useful\nlearning strategies, such as out-of-core learning, minibatch learning, and\nsemi-supervised learning, without requiring the user to consider how to\npartition data or modify the algorithms to handle these tasks themselves.\npomegranate is written in Cython to speed up calculations and releases the\nglobal interpreter lock to allow for built-in multithreaded parallelism, making\nit competitive with---or outperform---other implementations of similar\nalgorithms. This paper presents an overview of the design choices in\npomegranate, and how they have enabled complex features to be supported by\nsimple code.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 22:53:20 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 23:43:16 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Schreiber", "Jacob", ""]]}, {"id": "1711.00138", "submitter": "Sam Greydanus", "authors": "Sam Greydanus, Anurag Koul, Jonathan Dodge, Alan Fern", "title": "Visualizing and Understanding Atari Agents", "comments": "ICML 2018 conference paper. Code:\n  https://github.com/greydanus/visualize_atari Blog:\n  https://greydanus.github.io/2017/11/01/visualize-atari/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep reinforcement learning (deep RL) agents are effective at\nmaximizing rewards, it is often unclear what strategies they use to do so. In\nthis paper, we take a step toward explaining deep RL agents through a case\nstudy using Atari 2600 environments. In particular, we focus on using saliency\nmaps to understand how an agent learns and executes a policy. We introduce a\nmethod for generating useful saliency maps and use it to show 1) what strong\nagents attend to, 2) whether agents are making decisions for the right or wrong\nreasons, and 3) how agents evolve during learning. We also test our method on\nnon-expert human subjects and find that it improves their ability to reason\nabout these agents. Overall, our results show that saliency information can\nprovide significant insight into an RL agent's decisions and learning behavior.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 23:03:17 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 19:35:42 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 21:34:02 GMT"}, {"version": "v4", "created": "Fri, 23 Mar 2018 00:37:12 GMT"}, {"version": "v5", "created": "Mon, 10 Sep 2018 18:42:40 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Greydanus", "Sam", ""], ["Koul", "Anurag", ""], ["Dodge", "Jonathan", ""], ["Fern", "Alan", ""]]}, {"id": "1711.00150", "submitter": "Anna Korhonen", "authors": "Yiding Lu, Yufan Guo, Anna Korhonen", "title": "Erratum: Link prediction in drug-target interactions network using\n  similarity indices", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: In silico drug-target interaction (DTI) prediction plays an\nintegral role in drug repositioning: the discovery of new uses for existing\ndrugs. One popular method of drug repositioning is network-based DTI\nprediction, which uses complex network theory to predict DTIs from a\ndrug-target network. Currently, most network-based DTI prediction is based on\nmachine learning methods such as Restricted Boltzmann Machines (RBM) or Support\nVector Machines (SVM). These methods require additional information about the\ncharacteristics of drugs, targets and DTIs, such as chemical structure, genome\nsequence, binding types, causes of interactions, etc., and do not perform\nsatisfactorily when such information is unavailable. We propose a new,\nalternative method for DTI prediction that makes use of only network topology\ninformation attempting to solve this problem.\n  Results: We compare our method for DTI prediction against the well-known RBM\napproach. We show that when applied to the MATADOR database, our approach based\non node neighborhoods yield higher precision for high-ranking predictions than\nRBM when no information regarding DTI types is available.\n  Conclusion: This demonstrates that approaches purely based on network\ntopology provide a more suitable approach to DTI prediction in the many\nreal-life situations where little or no prior knowledge is available about the\ncharacteristics of drugs, targets, or their interactions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 00:21:48 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Lu", "Yiding", ""], ["Guo", "Yufan", ""], ["Korhonen", "Anna", ""]]}, {"id": "1711.00267", "submitter": "Wenbin Li", "authors": "Wenbin Li, Jeannette Bohg, Mario Fritz", "title": "Acquiring Target Stacking Skills by Goal-Parameterized Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding physical phenomena is a key component of human intelligence and\nenables physical interaction with previously unseen environments. In this\npaper, we study how an artificial agent can autonomously acquire this intuition\nthrough interaction with the environment. We created a synthetic block stacking\nenvironment with physics simulation in which the agent can learn a policy\nend-to-end through trial and error. Thereby, we bypass to explicitly model\nphysical knowledge within the policy. We are specifically interested in tasks\nthat require the agent to reach a given goal state that may be different for\nevery new trial. To this end, we propose a deep reinforcement learning\nframework that learns policies which are parametrized by a goal. We validated\nthe model on a toy example navigating in a grid world with different target\npositions and in a block stacking task with different target structures of the\nfinal tower. In contrast to prior work, our policies show better generalization\nacross different goals.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 10:04:29 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 11:38:17 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Li", "Wenbin", ""], ["Bohg", "Jeannette", ""], ["Fritz", "Mario", ""]]}, {"id": "1711.00350", "submitter": "Brenden Lake", "authors": "Brenden M. Lake and Marco Baroni", "title": "Generalization without systematicity: On the compositional skills of\n  sequence-to-sequence recurrent networks", "comments": "Published at the 35th International Conference on Machine Learning\n  (ICML 2018)", "journal-ref": "Lake, B. M. and Baroni, M. (2018). Generalization without\n  systematicity: On the compositional skills of sequence-to-sequence recurrent\n  networks. International Conference on Machine Learning (ICML)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can understand and produce new utterances effortlessly, thanks to\ntheir compositional skills. Once a person learns the meaning of a new verb\n\"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing\nand dax.\" In this paper, we introduce the SCAN domain, consisting of a set of\nsimple compositional navigation commands paired with the corresponding action\nsequences. We then test the zero-shot generalization capabilities of a variety\nof recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence\nmethods. We find that RNNs can make successful zero-shot generalizations when\nthe differences between training and test commands are small, so that they can\napply \"mix-and-match\" strategies to solve the task. However, when\ngeneralization requires systematic compositional skills (as in the \"dax\"\nexample above), RNNs fail spectacularly. We conclude with a proof-of-concept\nexperiment in neural machine translation, suggesting that lack of systematicity\nmight be partially responsible for neural networks' notorious training data\nthirst.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 01:50:02 GMT"}, {"version": "v2", "created": "Sun, 11 Feb 2018 21:55:39 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 20:52:51 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Lake", "Brenden M.", ""], ["Baroni", "Marco", ""]]}, {"id": "1711.00363", "submitter": "Andrew Critch PhD", "authors": "Andrew Critch and Stuart Russell", "title": "Servant of Many Masters: Shifting priorities in Pareto-optimal\n  sequential decision-making", "comments": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:1701.01302", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often argued that an agent making decisions on behalf of two or more\nprincipals who have different utility functions should adopt a {\\em\nPareto-optimal} policy, i.e., a policy that cannot be improved upon for one\nagent without making sacrifices for another. A famous theorem of Harsanyi shows\nthat, when the principals have a common prior on the outcome distributions of\nall policies, a Pareto-optimal policy for the agent is one that maximizes a\nfixed, weighted linear combination of the principals' utilities.\n  In this paper, we show that Harsanyi's theorem does not hold for principals\nwith different priors, and derive a more precise generalization which does\nhold, which constitutes our main result. In this more general case, the\nrelative weight given to each principal's utility should evolve over time\naccording to how well the agent's observations conform with that principal's\nprior. The result has implications for the design of contracts, treaties, joint\nventures, and robots.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 05:09:13 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Critch", "Andrew", ""], ["Russell", "Stuart", ""]]}, {"id": "1711.00399", "submitter": "Brent Mittelstadt", "authors": "Sandra Wachter, Brent Mittelstadt, Chris Russell", "title": "Counterfactual Explanations without Opening the Black Box: Automated\n  Decisions and the GDPR", "comments": null, "journal-ref": "Harvard Journal of Law & Technology, 2018", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been much discussion of the right to explanation in the EU General\nData Protection Regulation, and its existence, merits, and disadvantages.\nImplementing a right to explanation that opens the black box of algorithmic\ndecision-making faces major legal and technical barriers. Explaining the\nfunctionality of complex algorithmic decision-making systems and their\nrationale in specific cases is a technically challenging problem. Some\nexplanations may offer little meaningful information to data subjects, raising\nquestions around their value. Explanations of automated decisions need not\nhinge on the general public understanding how algorithmic systems function.\nEven though such interpretability is of great importance and should be pursued,\nexplanations can, in principle, be offered without opening the black box.\nLooking at explanations as a means to help a data subject act rather than\nmerely understand, one could gauge the scope and content of explanations\naccording to the specific goal or action they are intended to support. From the\nperspective of individuals affected by automated decision-making, we propose\nthree aims for explanations: (1) to inform and help the individual understand\nwhy a particular decision was reached, (2) to provide grounds to contest the\ndecision if the outcome is undesired, and (3) to understand what would need to\nchange in order to receive a desired result in the future, based on the current\ndecision-making model. We assess how each of these goals finds support in the\nGDPR. We suggest data controllers should offer a particular type of\nexplanation, unconditional counterfactual explanations, to support these three\naims. These counterfactual explanations describe the smallest change to the\nworld that can be made to obtain a desirable outcome, or to arrive at the\nclosest possible world, without needing to explain the internal logic of the\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 15:39:23 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 12:26:47 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 11:43:46 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Wachter", "Sandra", ""], ["Mittelstadt", "Brent", ""], ["Russell", "Chris", ""]]}, {"id": "1711.00400", "submitter": "Richard Combes", "authors": "Richard Combes, Stefan Magureanu and Alexandre Proutiere", "title": "Minimal Exploration in Structured Stochastic Bandits", "comments": "13 pages, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and addresses a wide class of stochastic bandit\nproblems where the function mapping the arm to the corresponding reward\nexhibits some known structural properties. Most existing structures (e.g.\nlinear, Lipschitz, unimodal, combinatorial, dueling, ...) are covered by our\nframework. We derive an asymptotic instance-specific regret lower bound for\nthese problems, and develop OSSB, an algorithm whose regret matches this\nfundamental limit. OSSB is not based on the classical principle of \"optimism in\nthe face of uncertainty\" or on Thompson sampling, and rather aims at matching\nthe minimal exploration rates of sub-optimal arms as characterized in the\nderivation of the regret lower bound. We illustrate the efficiency of OSSB\nusing numerical experiments in the case of the linear bandit problem and show\nthat OSSB outperforms existing algorithms, including Thompson sampling.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 15:40:26 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Combes", "Richard", ""], ["Magureanu", "Stefan", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1711.00404", "submitter": "Julia Ling", "authors": "Julia Ling, Maxwell Hutchinson, Erin Antono, Brian DeCost, Elizabeth\n  A. Holm, Bryce Meredig", "title": "Building Data-driven Models with Microstructural Images: Generalization\n  and Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cond-mat.mtrl-sci", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data-driven methods rise in popularity in materials science applications,\na key question is how these machine learning models can be used to understand\nmicrostructure. Given the importance of process-structure-property relations\nthroughout materials science, it seems logical that models that can leverage\nmicrostructural data would be more capable of predicting property information.\nWhile there have been some recent attempts to use convolutional neural networks\nto understand microstructural images, these early studies have focused only on\nwhich featurizations yield the highest machine learning model accuracy for a\nsingle data set. This paper explores the use of convolutional neural networks\nfor classifying microstructure with a more holistic set of objectives in mind:\ngeneralization between data sets, number of features required, and\ninterpretability.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 15:45:51 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Ling", "Julia", ""], ["Hutchinson", "Maxwell", ""], ["Antono", "Erin", ""], ["DeCost", "Brian", ""], ["Holm", "Elizabeth A.", ""], ["Meredig", "Bryce", ""]]}, {"id": "1711.00455", "submitter": "Rudy Bunel", "authors": "Rudy Bunel, Ilker Turkaslan, Philip H.S. Torr, Pushmeet Kohli, M.\n  Pawan Kumar", "title": "A Unified View of Piecewise Linear Neural Network Verification", "comments": "Updated version of \"Piecewise Linear Neural Network verification: A\n  comparative study\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of Deep Learning and its potential use in many safety-critical\napplications has motivated research on formal verification of Neural Network\n(NN) models. Despite the reputation of learned NN models to behave as black\nboxes and the theoretical hardness of proving their properties, researchers\nhave been successful in verifying some classes of models by exploiting their\npiecewise linear structure and taking insights from formal methods such as\nSatisifiability Modulo Theory. These methods are however still far from scaling\nto realistic neural networks. To facilitate progress on this crucial area, we\nmake two key contributions. First, we present a unified framework that\nencompasses previous methods. This analysis results in the identification of\nnew methods that combine the strengths of multiple existing approaches,\naccomplishing a speedup of two orders of magnitude compared to the previous\nstate of the art. Second, we propose a new data set of benchmarks which\nincludes a collection of previously released testcases. We use the benchmark to\nprovide the first experimental comparison of existing algorithms and identify\nthe factors impacting the hardness of verification problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 17:42:12 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 09:58:39 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 10:37:06 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Bunel", "Rudy", ""], ["Turkaslan", "Ilker", ""], ["Torr", "Philip H. S.", ""], ["Kohli", "Pushmeet", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1711.00462", "submitter": "Tshilidzi Marwala", "authors": "Satyakama Paul, Madhur Hasija and Tshilidzi Marwala", "title": "Early prediction of the duration of protests using probabilistic Latent\n  Dirichlet Allocation and Decision Trees", "comments": "This paper is to appear in the 4th IEEE Latin American Conference on\n  Computational Intelligence LA-CCI. This paper was written by Satyakama and\n  Madhur and supervised by Tshilidzi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protests and agitations are an integral part of every democratic civil\nsociety. In recent years, South Africa has seen a large increase in its\nprotests. The objective of this paper is to provide an early prediction of the\nduration of protests from its free flowing English text description. Free\nflowing descriptions of the protests help us in capturing its various nuances\nsuch as multiple causes, courses of actions etc. Next we use a combination of\nunsupervised learning (topic modeling) and supervised learning (decision trees)\nto predict the duration of the protests. Our results show a high degree (close\nto 90%) of accuracy in early prediction of the duration of protests.We expect\nthe work to help police and other security services in planning and managing\ntheir resources in better handling protests in future.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 06:03:09 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Paul", "Satyakama", ""], ["Hasija", "Madhur", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "1711.00530", "submitter": "Zhongxiang Wang", "authors": "Ali Shafahi, Zhongxiang Wang, Ali Haghani", "title": "School bus routing by maximizing trip compatibility", "comments": "The final version of this paper will be published in Transportation\n  Research Record: Journal of the Transportation Research Board, No. 2667. The\n  publication index can be found at\n  https://pubsindex.trb.org/view/2017/C/1437820", "journal-ref": null, "doi": "10.3141/2667-03", "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  School bus planning is usually divided into routing and scheduling due to the\ncomplexity of solving them concurrently. However, the separation between these\ntwo steps may lead to worse solutions with higher overall costs than that from\nsolving them together. When finding the minimal number of trips in the routing\nproblem, neglecting the importance of trip compatibility may increase the\nnumber of buses actually needed in the scheduling problem. This paper proposes\na new formulation for the multi-school homogeneous fleet routing problem that\nmaximizes trip compatibility while minimizing total travel time. This\nincorporates the trip compatibility for the scheduling problem in the routing\nproblem. Since the problem is inherently just a routing problem, finding a good\nsolution is not cumbersome. To compare the performance of the model with\ntraditional routing problems, we generate eight mid-size data sets. Through\nimporting the generated trips of the routing problems into the bus scheduling\n(blocking) problem, it is shown that the proposed model uses up to 13% fewer\nbuses than the common traditional routing models.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 20:27:47 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 22:51:22 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Shafahi", "Ali", ""], ["Wang", "Zhongxiang", ""], ["Haghani", "Ali", ""]]}, {"id": "1711.00532", "submitter": "Zhongxiang Wang", "authors": "Zhongxiang Wang, Ali Shafahi, Ali Haghani", "title": "SCDA: School Compatibility Decomposition Algorithm for Solving the\n  Multi-School Bus Routing and Scheduling Problem", "comments": "This paper was accepted for presentation at TRB Annual Meeting 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safely serving the school transportation demand with the minimum number of\nbuses is one of the highest financial goals of school transportation directors.\nTo achieve that objective, a good and efficient way to solve the routing and\nscheduling problem is required. Due to the growth of the computing power, the\nspotlight has been shed on solving the combined problem of the school bus\nrouting and scheduling problem. We show that an integrated multi-school bus\nrouting and scheduling can be formulated with the help of trip compatibility. A\nnovel decomposition algorithm is proposed to solve the integrated model. The\nmerit of this integrated model and the decomposition method is that with the\nconsideration of the trip compatibility, the interrelationship between the\nrouting and scheduling sub-problems will not be lost in the process of\ndecomposition. Results show the proposed decomposed problem could provide the\nsolutions using the same number of buses as the integrated model in much\nshorter time (as little as 0.6%) and that the proposed method can save up to\n26% number of buses from existing research.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 20:29:15 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 22:47:28 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 14:12:23 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Wang", "Zhongxiang", ""], ["Shafahi", "Ali", ""], ["Haghani", "Ali", ""]]}, {"id": "1711.00536", "submitter": "Rossano Schifanella", "authors": "Luca M. Aiello, Rossano Schifanella, Miriam Redi, Stacey Svetlichnaya,\n  Frank Liu, Simon Osindero", "title": "Beautiful and damned. Combined effect of content quality and social ties\n  on user engagement", "comments": "13 pages, 12 figures, final version published in IEEE Transactions on\n  Knowledge and Data Engineering (Volume: PP, Issue: 99)", "journal-ref": null, "doi": "10.1109/TKDE.2017.2747552", "report-no": null, "categories": "cs.SI cs.AI cs.CV cs.MM physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User participation in online communities is driven by the intertwinement of\nthe social network structure with the crowd-generated content that flows along\nits links. These aspects are rarely explored jointly and at scale. By looking\nat how users generate and access pictures of varying beauty on Flickr, we\ninvestigate how the production of quality impacts the dynamics of online social\nsystems. We develop a deep learning computer vision model to score images\naccording to their aesthetic value and we validate its output through\ncrowdsourcing. By applying it to over 15B Flickr photos, we study for the first\ntime how image beauty is distributed over a large-scale social system.\nBeautiful images are evenly distributed in the network, although only a small\ncore of people get social recognition for them. To study the impact of exposure\nto quality on user engagement, we set up matching experiments aimed at\ndetecting causality from observational data. Exposure to beauty is\ndouble-edged: following people who produce high-quality content increases one's\nprobability of uploading better photos; however, an excessive imbalance between\nthe quality generated by a user and the user's neighbors leads to a decline in\nengagement. Our analysis has practical implications for improving link\nrecommender systems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 20:48:30 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Aiello", "Luca M.", ""], ["Schifanella", "Rossano", ""], ["Redi", "Miriam", ""], ["Svetlichnaya", "Stacey", ""], ["Liu", "Frank", ""], ["Osindero", "Simon", ""]]}, {"id": "1711.00549", "submitter": "Anjishnu Kumar", "authors": "Anjishnu Kumar, Arpit Gupta, Julian Chan, Sam Tucker, Bjorn\n  Hoffmeister, Markus Dreyer, Stanislav Peshterliev, Ankur Gandhe, Denis\n  Filiminov, Ariya Rastrow, Christian Monson and Agnika Kumar", "title": "Just ASK: Building an Architecture for Extensible Self-Service Spoken\n  Language Understanding", "comments": "Published at the 1st Workshop on Conversational AI at NIPS 2017\n  (NIPS-WCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the design of the machine learning architecture that\nunderlies the Alexa Skills Kit (ASK) a large scale Spoken Language\nUnderstanding (SLU) Software Development Kit (SDK) that enables developers to\nextend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the\ninfrastructure powers over 25,000 skills deployed through the ASK, as well as\nAWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability\nand a rapid iteration cycle for third party developers. It imposes inductive\nbiases that allow it to learn robust SLU models from extremely small and sparse\ndatasets and, in doing so, removes significant barriers to entry for software\ndevelopers and dialogue systems researchers.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 22:10:11 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 09:19:37 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 00:37:00 GMT"}, {"version": "v4", "created": "Fri, 2 Mar 2018 13:58:04 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Kumar", "Anjishnu", ""], ["Gupta", "Arpit", ""], ["Chan", "Julian", ""], ["Tucker", "Sam", ""], ["Hoffmeister", "Bjorn", ""], ["Dreyer", "Markus", ""], ["Peshterliev", "Stanislav", ""], ["Gandhe", "Ankur", ""], ["Filiminov", "Denis", ""], ["Rastrow", "Ariya", ""], ["Monson", "Christian", ""], ["Kumar", "Agnika", ""]]}, {"id": "1711.00694", "submitter": "Smitha Milli", "authors": "Smitha Milli, Pieter Abbeel, Igor Mordatch", "title": "Interpretable and Pedagogical Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teachers intentionally pick the most informative examples to show their\nstudents. However, if the teacher and student are neural networks, the examples\nthat the teacher network learns to give, although effective at teaching the\nstudent, are typically uninterpretable. We show that training the student and\nteacher iteratively, rather than jointly, can produce interpretable teaching\nstrategies. We evaluate interpretability by (1) measuring the similarity of the\nteacher's emergent strategies to intuitive strategies in each domain and (2)\nconducting human experiments to evaluate how effective the teacher's strategies\nare at teaching humans. We show that the teacher network learns to select or\ngenerate interpretable, pedagogical examples to teach rule-based,\nprobabilistic, boolean, and hierarchical concepts.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 11:40:08 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 15:41:23 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Milli", "Smitha", ""], ["Abbeel", "Pieter", ""], ["Mordatch", "Igor", ""]]}, {"id": "1711.00698", "submitter": "Benoit Girard", "authors": "Guillaume Viejo (ISIR), Beno\\^it Girard (ISIR), Emmanuel Procyk, Mehdi\n  Khamassi (ISIR)", "title": "Adaptive coordination of working-memory and reinforcement learning in\n  non-human primates performing a trial-and-error problem solving task", "comments": "Behavioural Brain Research, Elsevier, 2017", "journal-ref": null, "doi": "10.1016/j.bbr.2017.09.030", "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accumulating evidence suggest that human behavior in trial-and-error learning\ntasks based on decisions between discrete actions may involve a combination of\nreinforcement learning (RL) and working-memory (WM). While the understanding of\nbrain activity at stake in this type of tasks often involve the comparison with\nnon-human primate neurophysiological results, it is not clear whether monkeys\nuse similar combined RL and WM processes to solve these tasks. Here we analyzed\nthe behavior of five monkeys with computational models combining RL and WM. Our\nmodel-based analysis approach enables to not only fit trial-by-trial choices\nbut also transient slowdowns in reaction times, indicative of WM use. We found\nthat the behavior of the five monkeys was better explained in terms of a\ncombination of RL and WM despite inter-individual differences. The same\ncoordination dynamics we used in a previous study in humans best explained the\nbehavior of some monkeys while the behavior of others showed the opposite\npattern, revealing a possible different dynamics of WM process. We further\nanalyzed different variants of the tested models to open a discussion on how\nthe long pretraining in these tasks may have favored particular coordination\ndynamics between RL and WM. This points towards either inter-species\ndifferences or protocol differences which could be further tested in humans.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 11:53:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Viejo", "Guillaume", "", "ISIR"], ["Girard", "Beno\u00eet", "", "ISIR"], ["Procyk", "Emmanuel", "", "ISIR"], ["Khamassi", "Mehdi", "", "ISIR"]]}, {"id": "1711.00740", "submitter": "Miltiadis Allamanis", "authors": "Miltiadis Allamanis, Marc Brockschmidt, Mahmoud Khademi", "title": "Learning to Represent Programs with Graphs", "comments": "Published in ICLR 2018. arXiv admin note: text overlap with\n  arXiv:1705.07867", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning tasks on source code (i.e., formal languages) have been considered\nrecently, but most work has tried to transfer natural language methods and does\nnot capitalize on the unique opportunities offered by code's known syntax. For\nexample, long-range dependencies induced by using the same variable or function\nin distant locations are often not considered. We propose to use graphs to\nrepresent both the syntactic and semantic structure of code and use graph-based\ndeep learning methods to learn to reason over program structures.\n  In this work, we present how to construct graphs from source code and how to\nscale Gated Graph Neural Networks training to such large graphs. We evaluate\nour method on two tasks: VarNaming, in which a network attempts to predict the\nname of a variable given its usage, and VarMisuse, in which the network learns\nto reason about selecting the correct variable that should be used at a given\nprogram location. Our comparison to methods that use less structured program\nrepresentations shows the advantages of modeling known structure, and suggests\nthat our models learn to infer meaningful names and to solve the VarMisuse task\nin many cases. Additionally, our testing showed that VarMisuse identifies a\nnumber of bugs in mature open-source projects.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 09:48:06 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 11:59:03 GMT"}, {"version": "v3", "created": "Fri, 4 May 2018 20:30:53 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Allamanis", "Miltiadis", ""], ["Brockschmidt", "Marc", ""], ["Khademi", "Mahmoud", ""]]}, {"id": "1711.00804", "submitter": "Ankit Parag Shah", "authors": "Rohan Badlani, Ankit Shah, Benjamin Elizalde, Anurag Kumar, Bhiksha\n  Raj", "title": "Framework for evaluation of sound event detection in web videos", "comments": "Camera Ready Version of Paper accepted at International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP) 2018. First two Authors -\n  Rohan Badlani and Ankit Shah contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.IR eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The largest source of sound events is web videos. Most videos lack sound\nevent labels at segment level, however, a significant number of them do respond\nto text queries, from a match found using metadata by search engines. In this\npaper we explore the extent to which a search query can be used as the true\nlabel for detection of sound events in videos. We present a framework for\nlarge-scale sound event recognition on web videos. The framework crawls videos\nusing search queries corresponding to 78 sound event labels drawn from three\ndatasets. The datasets are used to train three classifiers, and we obtain a\nprediction on 3.7 million web video segments. We evaluated performance using\nthe search query as true label and compare it with human labeling. Both types\nof ground truth exhibited close performance, to within 10%, and similar\nperformance trend with increasing number of evaluated segments. Hence, our\nexperiments show potential for using search query as a preliminary true label\nfor sound event recognition in web videos.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 16:32:23 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 15:05:37 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Badlani", "Rohan", ""], ["Shah", "Ankit", ""], ["Elizalde", "Benjamin", ""], ["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1711.00832", "submitter": "Marc Lanctot", "authors": "Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou,\n  Karl Tuyls, Julien Perolat, David Silver, Thore Graepel", "title": "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning", "comments": "Camera-ready copy of NIPS 2017 paper, including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve general intelligence, agents must learn how to interact with\nothers in a shared environment: this is the challenge of multiagent\nreinforcement learning (MARL). The simplest form is independent reinforcement\nlearning (InRL), where each agent treats its experience as part of its\n(non-stationary) environment. In this paper, we first observe that policies\nlearned using InRL can overfit to the other agents' policies during training,\nfailing to sufficiently generalize during execution. We introduce a new metric,\njoint-policy correlation, to quantify this effect. We describe an algorithm for\ngeneral MARL, based on approximate best responses to mixtures of policies\ngenerated using deep reinforcement learning, and empirical game-theoretic\nanalysis to compute meta-strategies for policy selection. The algorithm\ngeneralizes previous ones such as InRL, iterated best response, double oracle,\nand fictitious play. Then, we present a scalable implementation which reduces\nthe memory requirement using decoupled meta-solvers. Finally, we demonstrate\nthe generality of the resulting policies in two partially observable settings:\ngridworld coordination games and poker.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 17:34:24 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 12:38:37 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Lanctot", "Marc", ""], ["Zambaldi", "Vinicius", ""], ["Gruslys", "Audrunas", ""], ["Lazaridou", "Angeliki", ""], ["Tuyls", "Karl", ""], ["Perolat", "Julien", ""], ["Silver", "David", ""], ["Graepel", "Thore", ""]]}, {"id": "1711.00848", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Prasanna Sattigeri, Avinash Balakrishnan", "title": "Variational Inference of Disentangled Latent Concepts from Unlabeled\n  Observations", "comments": "ICLR 2018 Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentangled representations, where the higher level data generative factors\nare reflected in disjoint latent dimensions, offer several benefits such as\nease of deriving invariant representations, transferability to other tasks,\ninterpretability, etc. We consider the problem of unsupervised learning of\ndisentangled representations from large pool of unlabeled observations, and\npropose a variational inference based approach to infer disentangled latent\nfactors. We introduce a regularizer on the expectation of the approximate\nposterior over observed data that encourages the disentanglement. We also\npropose a new disentanglement metric which is better aligned with the\nqualitative disentanglement observed in the decoder's output. We empirically\nobserve significant improvement over existing methods in terms of both\ndisentanglement and data likelihood (reconstruction quality).\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 17:57:43 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 21:29:36 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 19:25:22 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sattigeri", "Prasanna", ""], ["Balakrishnan", "Avinash", ""]]}, {"id": "1711.00851", "submitter": "Eric Wong", "authors": "Eric Wong, J. Zico Kolter", "title": "Provable defenses against adversarial examples via the convex outer\n  adversarial polytope", "comments": "ICML final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to learn deep ReLU-based classifiers that are provably\nrobust against norm-bounded adversarial perturbations on the training data. For\npreviously unseen examples, the approach is guaranteed to detect all\nadversarial examples, though it may flag some non-adversarial examples as well.\nThe basic idea is to consider a convex outer approximation of the set of\nactivations reachable through a norm-bounded perturbation, and we develop a\nrobust optimization procedure that minimizes the worst case loss over this\nouter region (via a linear program). Crucially, we show that the dual problem\nto this linear program can be represented itself as a deep network similar to\nthe backpropagation network, leading to very efficient optimization approaches\nthat produce guaranteed bounds on the robust loss. The end result is that by\nexecuting a few more forward and backward passes through a slightly modified\nversion of the original network (though possibly with much larger batch sizes),\nwe can learn a classifier that is provably robust to any norm-bounded\nadversarial attack. We illustrate the approach on a number of tasks to train\nclassifiers with robust adversarial guarantees (e.g. for MNIST, we produce a\nconvolutional classifier that provably has less than 5.8% test error for any\nadversarial attack with bounded $\\ell_\\infty$ norm less than $\\epsilon = 0.1$),\nand code for all experiments in the paper is available at\nhttps://github.com/locuslab/convex_adversarial.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 17:59:24 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 00:41:56 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 19:04:49 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Wong", "Eric", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1711.00909", "submitter": "Robert Woodward", "authors": "Robert J. Woodward and Berthe Y. Choueiry", "title": "Weight-Based Variable Ordering in the Context of High-Level\n  Consistencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dom/wdeg is one of the best performing heuristics for dynamic variable\nordering in backtrack search [Boussemart et al., 2004]. As originally defined,\nthis heuristic increments the weight of the constraint that causes a domain\nwipeout (i.e., a dead-end) when enforcing arc consistency during search. \"The\nprocess of weighting constraints with dom/wdeg is not defined when more than\none constraint lead to a domain wipeout [Vion et al., 2011].\" In this paper, we\ninvestigate how weights should be updated in the context of two high-level\nconsistencies, namely, singleton (POAC) and relational consistencies (RNIC). We\npropose, analyze, and empirically evaluate several strategies for updating the\nweights. We statistically compare the proposed strategies and conclude with our\nrecommendations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 19:55:18 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Woodward", "Robert J.", ""], ["Choueiry", "Berthe Y.", ""]]}, {"id": "1711.00956", "submitter": "Chao Qian", "authors": "Chao Qian, Chao Bian, Wu Jiang, Ke Tang", "title": "Running Time Analysis of the (1+1)-EA for OneMax and LeadingOnes under\n  Bit-wise Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world optimization problems, the objective function evaluation\nis subject to noise, and we cannot obtain the exact objective value.\nEvolutionary algorithms (EAs), a type of general-purpose randomized\noptimization algorithm, have shown able to solve noisy optimization problems\nwell. However, previous theoretical analyses of EAs mainly focused on\nnoise-free optimization, which makes the theoretical understanding largely\ninsufficient. Meanwhile, the few existing theoretical studies under noise often\nconsidered the one-bit noise model, which flips a randomly chosen bit of a\nsolution before evaluation; while in many realistic applications, several bits\nof a solution can be changed simultaneously. In this paper, we study a natural\nextension of one-bit noise, the bit-wise noise model, which independently flips\neach bit of a solution with some probability. We analyze the running time of\nthe (1+1)-EA solving OneMax and LeadingOnes under bit-wise noise for the first\ntime, and derive the ranges of the noise level for polynomial and\nsuper-polynomial running time bounds. The analysis on LeadingOnes under\nbit-wise noise can be easily transferred to one-bit noise, and improves the\npreviously known results. Since our analysis discloses that the (1+1)-EA can be\nefficient only under low noise levels, we also study whether the sampling\nstrategy can bring robustness to noise. We prove that using sampling can\nsignificantly increase the largest noise level allowing a polynomial running\ntime, that is, sampling is robust to noise.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 22:00:21 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Qian", "Chao", ""], ["Bian", "Chao", ""], ["Jiang", "Wu", ""], ["Tang", "Ke", ""]]}, {"id": "1711.01024", "submitter": "Bernhard Scholz", "authors": "Wasuwee Sodsong, Bernhard Scholz, Sanjay Chawla", "title": "SPARK: Static Program Analysis Reasoning and Retrieving Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program analysis is a technique to reason about programs without executing\nthem, and it has various applications in compilers, integrated development\nenvironments, and security. In this work, we present a machine learning\npipeline that induces a security analyzer for programs by example. The security\nanalyzer determines whether a program is either secure or insecure based on\nsymbolic rules that were deduced by our machine learning pipeline. The machine\npipeline is two-staged consisting of a Recurrent Neural Networks (RNN) and an\nExtractor that converts an RNN to symbolic rules.\n  To evaluate the quality of the learned symbolic rules, we propose a\nsampling-based similarity measurement between two infinite regular languages.\nWe conduct a case study using real-world data. In this work, we discuss the\nlimitations of existing techniques and possible improvements in the future. The\nresults show that with sufficient training data and a fair distribution of\nprogram paths it is feasible to deducing symbolic security rules for the\nOpenJDK library with millions lines of code.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 04:28:58 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Sodsong", "Wasuwee", ""], ["Scholz", "Bernhard", ""], ["Chawla", "Sanjay", ""]]}, {"id": "1711.01125", "submitter": "Jianlei Yang", "authors": "Xiaotao Jia, Jianlei Yang, Zhaohao Wang, Yiran Chen, Hai (Helen) Li\n  and Weisheng Zhao", "title": "Spintronics based Stochastic Computing for Efficient Bayesian Inference\n  System", "comments": "accepted by ASPDAC 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bayesian inference is an effective approach for solving statistical learning\nproblems especially with uncertainty and incompleteness. However, inference\nefficiencies are physically limited by the bottlenecks of conventional\ncomputing platforms. In this paper, an emerging Bayesian inference system is\nproposed by exploiting spintronics based stochastic computing. A stochastic\nbitstream generator is realized as the kernel components by leveraging the\ninherent randomness of spintronics devices. The proposed system is evaluated by\ntypical applications of data fusion and Bayesian belief networks. Simulation\nresults indicate that the proposed approach could achieve significant\nimprovement on inference efficiencies in terms of power consumption and\ninference speed.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 12:23:59 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Jia", "Xiaotao", "", "Helen"], ["Yang", "Jianlei", "", "Helen"], ["Wang", "Zhaohao", "", "Helen"], ["Chen", "Yiran", "", "Helen"], ["Hai", "", "", "Helen"], ["Li", "", ""], ["Zhao", "Weisheng", ""]]}, {"id": "1711.01134", "submitter": "Finale Doshi-Velez", "authors": "Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam\n  Gershman, David O'Brien, Kate Scott, Stuart Schieber, James Waldo, David\n  Weinberger, Adrian Weller, and Alexandra Wood", "title": "Accountability of AI Under the Law: The Role of Explanation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of systems using artificial intelligence or \"AI\" has brought\nincreasing attention to how those systems should be regulated. The choice of\nhow to regulate AI systems will require care. AI systems have the potential to\nsynthesize large amounts of data, allowing for greater levels of\npersonalization and precision than ever before---applications range from\nclinical decision support to autonomous driving and predictive policing. That\nsaid, there exist legitimate concerns about the intentional and unintentional\nnegative consequences of AI systems. There are many ways to hold AI systems\naccountable. In this work, we focus on one: explanation. Questions about a\nlegal right to explanation from AI systems was recently debated in the EU\nGeneral Data Protection Regulation, and thus thinking carefully about when and\nhow explanation from AI systems might improve accountability is timely. In this\nwork, we review contexts in which explanation is currently required under the\nlaw, and then list the technical considerations that must be considered if we\ndesired AI systems that could provide kinds of explanations that are currently\nrequired of humans.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 12:54:51 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 02:06:17 GMT"}, {"version": "v3", "created": "Fri, 20 Dec 2019 20:03:56 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Doshi-Velez", "Finale", ""], ["Kortz", "Mason", ""], ["Budish", "Ryan", ""], ["Bavitz", "Chris", ""], ["Gershman", "Sam", ""], ["O'Brien", "David", ""], ["Scott", "Kate", ""], ["Schieber", "Stuart", ""], ["Waldo", "James", ""], ["Weinberger", "David", ""], ["Weller", "Adrian", ""], ["Wood", "Alexandra", ""]]}, {"id": "1711.01244", "submitter": "Ron Amit", "authors": "Ron Amit and Ron Meir", "title": "Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory", "comments": "Accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In meta-learning an agent extracts knowledge from observed tasks, aiming to\nfacilitate learning of novel future tasks. Under the assumption that future\ntasks are 'related' to previous tasks, the accumulated knowledge should be\nlearned in a way which captures the common structure across learned tasks,\nwhile allowing the learner sufficient flexibility to adapt to novel aspects of\nnew tasks. We present a framework for meta-learning that is based on\ngeneralization error bounds, allowing us to extend various PAC-Bayes bounds to\nmeta-learning. Learning takes place through the construction of a distribution\nover hypotheses based on the observed tasks, and its utilization for learning a\nnew task. Thus, prior knowledge is incorporated through setting an\nexperience-dependent prior for novel tasks. We develop a gradient-based\nalgorithm which minimizes an objective function derived from the bounds and\ndemonstrate its effectiveness numerically with deep neural networks. In\naddition to establishing the improved performance available through\nmeta-learning, we demonstrate the intuitive way by which prior information is\nmanifested at different levels of the network.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 17:14:14 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 19:38:13 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 05:26:27 GMT"}, {"version": "v4", "created": "Thu, 17 May 2018 18:33:49 GMT"}, {"version": "v5", "created": "Tue, 22 May 2018 08:43:18 GMT"}, {"version": "v6", "created": "Fri, 8 Jun 2018 09:34:51 GMT"}, {"version": "v7", "created": "Tue, 31 Jul 2018 09:44:24 GMT"}, {"version": "v8", "created": "Mon, 20 May 2019 10:29:06 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Amit", "Ron", ""], ["Meir", "Ron", ""]]}, {"id": "1711.01283", "submitter": "Tommaso Soru", "authors": "Tommaso Soru, Diego Esteves, Edgard Marx, Axel-Cyrille Ngonga Ngomo", "title": "Mandolin: A Knowledge Discovery Framework for the Web of Data", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Logic Networks join probabilistic modeling with first-order logic and\nhave been shown to integrate well with the Semantic Web foundations. While\nseveral approaches have been devised to tackle the subproblems of rule mining,\ngrounding, and inference, no comprehensive workflow has been proposed so far.\nIn this paper, we fill this gap by introducing a framework called Mandolin,\nwhich implements a workflow for knowledge discovery specifically on RDF\ndatasets. Our framework imports knowledge from referenced graphs, creates\nsimilarity relationships among similar literals, and relies on state-of-the-art\ntechniques for rule mining, grounding, and inference computation. We show that\nour best configuration scales well and achieves at least comparable results\nwith respect to other statistical-relational-learning algorithms on link\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 18:04:06 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Soru", "Tommaso", ""], ["Esteves", "Diego", ""], ["Marx", "Edgard", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1711.01287", "submitter": "Niek Tax", "authors": "Niek Tax, Natalia Sidorova, Wil M. P. van der Aalst", "title": "Discovering More Precise Process Models from Event Logs by Filtering Out\n  Chaotic Activities", "comments": null, "journal-ref": "Journal of Intelligent Information Systems, (2018), 1-33", "doi": "10.1007/s10844-018-0507-6", "report-no": null, "categories": "cs.DB cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process Discovery is concerned with the automatic generation of a process\nmodel that describes a business process from execution data of that business\nprocess. Real life event logs can contain chaotic activities. These activities\nare independent of the state of the process and can, therefore, happen at\nrather arbitrary points in time. We show that the presence of such chaotic\nactivities in an event log heavily impacts the quality of the process models\nthat can be discovered with process discovery techniques. The current modus\noperandi for filtering activities from event logs is to simply filter out\ninfrequent activities. We show that frequency-based filtering of activities\ndoes not solve the problems that are caused by chaotic activities. Moreover, we\npropose a novel technique to filter out chaotic activities from event logs. We\nevaluate this technique on a collection of seventeen real-life event logs that\noriginate from both the business process management domain and the smart home\nenvironment domain. As demonstrated, the developed activity filtering methods\nenable the discovery of process models that are more behaviorally specific\ncompared to process models that are discovered using standard frequency-based\nfiltering.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 18:13:36 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Tax", "Niek", ""], ["Sidorova", "Natalia", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1711.01353", "submitter": "Shyamal Vaderia", "authors": "Saurabh Raje, Shyamal Vaderia, Neil Wilson, Rudrakh Panigrahi", "title": "Decentralised firewall for malware detection", "comments": "To be published in \"2017 International Conference on Advances in\n  Computing, Communication and Control (ICAC3)\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design and development of a decentralized firewall\nsystem powered by a novel malware detection engine. The firewall is built using\nblockchain technology. The detection engine aims to classify Portable\nExecutable (PE) files as malicious or benign. File classification is carried\nout using a deep belief neural network (DBN) as the detection engine. Our\napproach is to model the files as grayscale images and use the DBN to classify\nthose images into the aforementioned two classes. An extensive data set of\n10,000 files is used to train the DBN. Validation is carried out using 4,000\nfiles previously unexposed to the network. The final result of whether to allow\nor block a file is obtained by arriving at a proof of work based consensus in\nthe blockchain network.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 22:49:49 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Raje", "Saurabh", ""], ["Vaderia", "Shyamal", ""], ["Wilson", "Neil", ""], ["Panigrahi", "Rudrakh", ""]]}, {"id": "1711.01391", "submitter": "Beomjoon Kim", "authors": "Beomjoon Kim, Leslie Pack Kaelbling, Tomas Lozano-Perez", "title": "Guiding the search in continuous state-action spaces by learning an\n  action sampling distribution from off-target samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In robotics, it is essential to be able to plan efficiently in\nhigh-dimensional continuous state-action spaces for long horizons. For such\ncomplex planning problems, unguided uniform sampling of actions until a path to\na goal is found is hopelessly inefficient, and gradient-based approaches often\nfall short when the optimization manifold of a given problem is not smooth. In\nthis paper we present an approach that guides the search of a state-space\nplanner, such as A*, by learning an action-sampling distribution that can\ngeneralize across different instances of a planning problem. The motivation is\nthat, unlike typical learning approaches for planning for continuous action\nspace that estimate a policy, an estimated action sampler is more robust to\nerror since it has a planner to fall back on. We use a Generative Adversarial\nNetwork (GAN), and address an important issue: search experience consists of a\nrelatively large number of actions that are not on a solution path and a\nrelatively small number of actions that actually are on a solution path. We\nintroduce a new technique, based on an importance-ratio estimation method, for\nusing samples from a non-target distribution to make GAN learning more\ndata-efficient. We provide theoretical guarantees and empirical evaluation in\nthree challenging continuous robot planning problems to illustrate the\neffectiveness of our algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 04:10:05 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Kim", "Beomjoon", ""], ["Kaelbling", "Leslie Pack", ""], ["Lozano-Perez", "Tomas", ""]]}, {"id": "1711.01431", "submitter": "Johan Loeckx", "authors": "Johan Loeckx", "title": "The Case for Meta-Cognitive Machine Learning: On Model Entropy and\n  Concept Formation in Deep Learning", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is usually defined in behaviourist terms, where external\nvalidation is the primary mechanism of learning. In this paper, I argue for a\nmore holistic interpretation in which finding more probable, efficient and\nabstract representations is as central to learning as performance. In other\nwords, machine learning should be extended with strategies to reason over its\nown learning process, leading to so-called meta-cognitive machine learning. As\nsuch, the de facto definition of machine learning should be reformulated in\nthese intrinsically multi-objective terms, taking into account not only the\ntask performance but also internal learning objectives. To this end, we suggest\na \"model entropy function\" to be defined that quantifies the efficiency of the\ninternal learning processes. It is conjured that the minimization of this model\nentropy leads to concept formation. Besides philosophical aspects, some initial\nillustrations are included to support the claims.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 12:54:35 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Loeckx", "Johan", ""]]}, {"id": "1711.01436", "submitter": "Magdalena Fuchs", "authors": "Magdalena Fuchs, Manuel Zimmer, Radu Grosu, Ramin M. Hasani", "title": "Searching for Biophysically Realistic Parameters for Dynamic Neuron\n  Models by Genetic Algorithms from Calcium Imaging Recording", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual Neurons in the nervous systems exploit various dynamics. To\ncapture these dynamics for single neurons, we tune the parameters of an\nelectrophysiological model of nerve cells, to fit experimental data obtained by\ncalcium imaging. A search for the biophysical parameters of this model is\nperformed by means of a genetic algorithm, where the model neuron is exposed to\na predefined input current representing overall inputs from other parts of the\nnervous system. The algorithm is then constrained for keeping the ion-channel\ncurrents within reasonable ranges, while producing the best fit to a calcium\nimaging time series of the AVA interneuron, from the brain of the soil-worm, C.\nelegans. Our settings enable us to project a set of biophysical parameters to\nthe the neuron kinetics observed in neuronal imaging.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 13:43:43 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Fuchs", "Magdalena", ""], ["Zimmer", "Manuel", ""], ["Grosu", "Radu", ""], ["Hasani", "Ramin M.", ""]]}, {"id": "1711.01468", "submitter": "Konstantinos Kamnitsas", "authors": "Konstantinos Kamnitsas, Wenjia Bai, Enzo Ferrante, Steven McDonagh,\n  Matthew Sinclair, Nick Pawlowski, Martin Rajchl, Matthew Lee, Bernhard Kainz,\n  Daniel Rueckert, Ben Glocker", "title": "Ensembles of Multiple Models and Architectures for Robust Brain Tumour\n  Segmentation", "comments": "The method won the 1st-place in the Brain Tumour Segmentation (BRATS)\n  2017 competition (segmentation task)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches such as convolutional neural nets have consistently\noutperformed previous methods on challenging tasks such as dense, semantic\nsegmentation. However, the various proposed networks perform differently, with\nbehaviour largely influenced by architectural choices and training settings.\nThis paper explores Ensembles of Multiple Models and Architectures (EMMA) for\nrobust performance through aggregation of predictions from a wide range of\nmethods. The approach reduces the influence of the meta-parameters of\nindividual models and the risk of overfitting the configuration to a particular\ndatabase. EMMA can be seen as an unbiased, generic deep learning model which is\nshown to yield excellent performance, winning the first position in the BRATS\n2017 competition among 50+ participating teams.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 17:43:07 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Kamnitsas", "Konstantinos", ""], ["Bai", "Wenjia", ""], ["Ferrante", "Enzo", ""], ["McDonagh", "Steven", ""], ["Sinclair", "Matthew", ""], ["Pawlowski", "Nick", ""], ["Rajchl", "Martin", ""], ["Lee", "Matthew", ""], ["Kainz", "Bernhard", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""]]}, {"id": "1711.01503", "submitter": "Richard Liaw", "authors": "Richard Liaw, Sanjay Krishnan, Animesh Garg, Daniel Crankshaw, Joseph\n  E. Gonzalez, Ken Goldberg", "title": "Composing Meta-Policies for Autonomous Driving Using Hierarchical Deep\n  Reinforcement Learning", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rather than learning new control policies for each new task, it is possible,\nwhen tasks share some structure, to compose a \"meta-policy\" from previously\nlearned policies. This paper reports results from experiments using Deep\nReinforcement Learning on a continuous-state, discrete-action autonomous\ndriving simulator. We explore how Deep Neural Networks can represent\nmeta-policies that switch among a set of previously learned policies,\nspecifically in settings where the dynamics of a new scenario are composed of a\nmixture of previously learned dynamics and where the state observation is\npossibly corrupted by sensing noise. We also report the results of experiments\nvarying dynamics mixes, distractor policies, magnitudes/distributions of\nsensing noise, and obstacles. In a fully observed experiment, the meta-policy\nlearning algorithm achieves 2.6x the reward achieved by the next best policy\ncomposition technique with 80% less exploration. In a partially observed\nexperiment, the meta-policy learning algorithm converges after 50 iterations\nwhile a direct application of RL fails to converge even after 200 iterations.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 22:37:25 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Liaw", "Richard", ""], ["Krishnan", "Sanjay", ""], ["Garg", "Animesh", ""], ["Crankshaw", "Daniel", ""], ["Gonzalez", "Joseph E.", ""], ["Goldberg", "Ken", ""]]}, {"id": "1711.01518", "submitter": "Rivindu Perera", "authors": "Rivindu Perera, Parma Nand, Boris Bacic, Wen-Hsin Yang, Kazuhiro Seki,\n  and Radek Burget", "title": "Semantic Web Today: From Oil Rigs to Panama Papers", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next leap on the internet has already started as Semantic Web. At its\ncore, Semantic Web transforms the document oriented web to a data oriented web\nenriched with semantics embedded as metadata. This change in perspective\ntowards the web offers numerous benefits for vast amount of data intensive\nindustries that are bound to the web and its related applications. The\nindustries are diverse as they range from Oil & Gas exploration to the\ninvestigative journalism, and everything in between. This paper discusses eight\ndifferent industries which currently reap the benefits of Semantic Web. The\npaper also offers a future outlook into Semantic Web applications and discusses\nthe areas in which Semantic Web would play a key role in the future.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 01:52:17 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Perera", "Rivindu", ""], ["Nand", "Parma", ""], ["Bacic", "Boris", ""], ["Yang", "Wen-Hsin", ""], ["Seki", "Kazuhiro", ""], ["Burget", "Radek", ""]]}, {"id": "1711.01519", "submitter": "Zahra Khatami", "authors": "Zahra Khatami and Lukas Troska and Hartmut Kaiser and J. Ramanujam and\n  Adrian Serio", "title": "HPX Smart Executors", "comments": "In Proceedings of ESPM2'17: Third International Workshop on Extreme\n  Scale Programming Models and Middleware, Denver, CO, USA, November\n  12-17,,2017 (ESPM2'17), 8 pages", "journal-ref": null, "doi": "10.1145/3152041.3152084", "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of many parallel applications depends on loop-level\nparallelism. However, manually parallelizing all loops may result in degrading\nparallel performance, as some of them cannot scale desirably to a large number\nof threads. In addition, the overheads of manually tuning loop parameters might\nprevent an application from reaching its maximum parallel performance. We\nillustrate how machine learning techniques can be applied to address these\nchallenges. In this research, we develop a framework that is able to\nautomatically capture the static and dynamic information of a loop. Moreover,\nwe advocate a novel method by introducing HPX smart executors for determining\nthe execution policy, chunk size, and prefetching distance of an HPX loop to\nachieve higher possible performance by feeding static information captured\nduring compilation and runtime-based dynamic information to our learning model.\nOur evaluated execution results show that using these smart executors can speed\nup the HPX execution process by around 12%-35% for the Matrix Multiplication,\nStream and $2D$ Stencil benchmarks compared to setting their HPX loop's\nexecution policy/parameters manually or using HPX auto-parallelization\ntechniques.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 02:11:07 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Khatami", "Zahra", ""], ["Troska", "Lukas", ""], ["Kaiser", "Hartmut", ""], ["Ramanujam", "J.", ""], ["Serio", "Adrian", ""]]}, {"id": "1711.01530", "submitter": "James Stokes", "authors": "Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin and James Stokes", "title": "Fisher-Rao Metric, Geometry, and Complexity of Neural Networks", "comments": "To appear in the proceedings of the 22nd International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2019", "journal-ref": "The 22nd International Conference on Artificial Intelligence and\n  Statistics 89 (2019) 888-896", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relationship between geometry and capacity measures for deep\nneural networks from an invariance viewpoint. We introduce a new notion of\ncapacity --- the Fisher-Rao norm --- that possesses desirable invariance\nproperties and is motivated by Information Geometry. We discover an analytical\ncharacterization of the new capacity measure, through which we establish\nnorm-comparison inequalities and further show that the new measure serves as an\numbrella for several existing norm-based complexity measures. We discuss upper\nbounds on the generalization error induced by the proposed measure. Extensive\nnumerical experiments on CIFAR-10 support our theoretical findings. Our\ntheoretical analysis rests on a key structural lemma about partial derivatives\nof multi-layer rectifier networks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 04:32:59 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 21:27:30 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liang", "Tengyuan", ""], ["Poggio", "Tomaso", ""], ["Rakhlin", "Alexander", ""], ["Stokes", "James", ""]]}, {"id": "1711.01569", "submitter": "Markus Dumke", "authors": "Markus Dumke", "title": "Double Q($\\sigma$) and Q($\\sigma, \\lambda$): Unifying Reinforcement\n  Learning Control Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal-difference (TD) learning is an important field in reinforcement\nlearning. Sarsa and Q-Learning are among the most used TD algorithms. The\nQ($\\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper\nextends the Q($\\sigma$) algorithm to an online multi-step algorithm Q($\\sigma,\n\\lambda$) using eligibility traces and introduces Double Q($\\sigma$) as the\nextension of Q($\\sigma$) to double learning. Experiments suggest that the new\nQ($\\sigma, \\lambda$) algorithm can outperform the classical TD control methods\nSarsa($\\lambda$), Q($\\lambda$) and Q($\\sigma$).\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 12:05:31 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Dumke", "Markus", ""]]}, {"id": "1711.01577", "submitter": "Zhen He", "authors": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber", "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence\n  Learning", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is a popular approach to boosting the ability\nof Recurrent Neural Networks to store longer term temporal information. The\ncapacity of an LSTM network can be increased by widening and adding layers.\nHowever, usually the former introduces additional parameters, while the latter\nincreases the runtime. As an alternative we propose the Tensorized LSTM in\nwhich the hidden states are represented by tensors and updated via a\ncross-layer convolution. By increasing the tensor size, the network can be\nwidened efficiently without additional parameters since the parameters are\nshared across different locations in the tensor; by delaying the output, the\nnetwork can be deepened implicitly with little additional runtime since deep\ncomputations for each timestep are merged into temporal computations of the\nsequence. Experiments conducted on five challenging sequence learning tasks\nshow the potential of the proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 12:30:35 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 10:41:28 GMT"}, {"version": "v3", "created": "Wed, 13 Dec 2017 02:14:14 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["He", "Zhen", ""], ["Gao", "Shaobing", ""], ["Xiao", "Liang", ""], ["Liu", "Daxue", ""], ["He", "Hangen", ""], ["Barber", "David", ""]]}, {"id": "1711.01634", "submitter": "Maarten Grachten", "authors": "Maarten Grachten and Carlos Eduardo Cancino Chac\\'on", "title": "Strategies for Conceptual Change in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A remarkable feature of human beings is their capacity for creative\nbehaviour, referring to their ability to react to problems in ways that are\nnovel, surprising, and useful. Transformational creativity is a form of\ncreativity where the creative behaviour is induced by a transformation of the\nactor's conceptual space, that is, the representational system with which the\nactor interprets its environment. In this report, we focus on ways of adapting\nsystems of learned representations as they switch from performing one task to\nperforming another. We describe an experimental comparison of multiple\nstrategies for adaptation of learned features, and evaluate how effectively\neach of these strategies realizes the adaptation, in terms of the amount of\ntraining, and in terms of their ability to cope with restricted availability of\ntraining data. We show, among other things, that across handwritten digits,\nnatural images, and classical music, adaptive strategies are systematically\nmore effective than a baseline method that starts learning from scratch.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 18:31:26 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 08:54:37 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Grachten", "Maarten", ""], ["Chac\u00f3n", "Carlos Eduardo Cancino", ""]]}, {"id": "1711.01694", "submitter": "Shubham Toshniwal", "authors": "Shubham Toshniwal, Tara N. Sainath, Ron J. Weiss, Bo Li, Pedro Moreno,\n  Eugene Weinstein, Kanishka Rao", "title": "Multilingual Speech Recognition With A Single End-To-End Model", "comments": "Accepted in ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a conventional automatic speech recognition (ASR) system to support\nmultiple languages is challenging because the sub-word unit, lexicon and word\ninventories are typically language specific. In contrast, sequence-to-sequence\nmodels are well suited for multilingual ASR because they encapsulate an\nacoustic, pronunciation and language model jointly in a single network. In this\nwork we present a single sequence-to-sequence ASR model trained on 9 different\nIndian languages, which have very little overlap in their scripts.\nSpecifically, we take a union of language-specific grapheme sets and train a\ngrapheme-based sequence-to-sequence model jointly on data from all languages.\nWe find that this model, which is not explicitly given any information about\nlanguage identity, improves recognition performance by 21% relative compared to\nanalogous sequence-to-sequence models trained on each language individually. By\nmodifying the model to accept a language identifier as an additional input\nfeature, we further improve performance by an additional 7% relative and\neliminate confusion between different languages.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 01:55:45 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 08:59:27 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Toshniwal", "Shubham", ""], ["Sainath", "Tara N.", ""], ["Weiss", "Ron J.", ""], ["Li", "Bo", ""], ["Moreno", "Pedro", ""], ["Weinstein", "Eugene", ""], ["Rao", "Kanishka", ""]]}, {"id": "1711.01703", "submitter": "Oliver Obst", "authors": "Olivia Michael and Oliver Obst and Falk Schmidsberger and Frieder\n  Stolzenburg", "title": "RoboCupSimData: A RoboCup soccer research dataset", "comments": "6 pages; https://bitbucket.org/oliverobst/robocupsimdata", "journal-ref": "In Dirk Holz, Katie Genter, Maarouf Saad, and Oskar von Stryk,\n  editors, RoboCup 2018: Robot Soccer World Cup XXII. RoboCup International\n  Symposium, LNAI 11374, pages 230-237, Montr\\'eal, Canada, 2019. Springer\n  Nature Switzerland", "doi": "10.1007/978-3-030-27544-0_19", "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RoboCup is an international scientific robot competition in which teams of\nmultiple robots compete against each other. Its different leagues provide many\nsources of robotics data, that can be used for further analysis and application\nof machine learning. This paper describes a large dataset from games of some of\nthe top teams (from 2016 and 2017) in RoboCup Soccer Simulation League (2D),\nwhere teams of 11 robots (agents) compete against each other. Overall, we used\n10 different teams to play each other, resulting in 45 unique pairings. For\neach pairing, we ran 25 matches (of 10mins), leading to 1125 matches or more\nthan 180 hours of game play. The generated CSV files are 17GB of data (zipped),\nor 229GB (unzipped). The dataset is unique in the sense that it contains both\nthe ground truth data (global, complete, noise-free information of all objects\non the field), as well as the noisy, local and incomplete percepts of each\nrobot. These data are made available as CSV files, as well as in the original\nsoccer simulator formats.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 03:09:38 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Michael", "Olivia", ""], ["Obst", "Oliver", ""], ["Schmidsberger", "Falk", ""], ["Stolzenburg", "Frieder", ""]]}, {"id": "1711.01711", "submitter": "Hector Zenil", "authors": "Hector Zenil, Liliana Badillo, Santiago Hern\\'andez-Orozco, Francisco\n  Hern\\'andez-Quiroz", "title": "Coding-theorem Like Behaviour and Emergence of the Universal\n  Distribution from Resource-bounded Algorithmic Probability", "comments": "27 pages main text, 39 pages including supplement. Online complexity\n  calculator: http://complexitycalculator.com/", "journal-ref": "International Journal of Parallel, Emergent and Distributed\n  Systems, DOI: 10.1080/17445760.2018.1448932", "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previously referred to as `miraculous' in the scientific literature because\nof its powerful properties and its wide application as optimal solution to the\nproblem of induction/inference, (approximations to) Algorithmic Probability\n(AP) and the associated Universal Distribution are (or should be) of the\ngreatest importance in science. Here we investigate the emergence, the rates of\nemergence and convergence, and the Coding-theorem like behaviour of AP in\nTuring-subuniversal models of computation. We investigate empirical\ndistributions of computing models in the Chomsky hierarchy. We introduce\nmeasures of algorithmic probability and algorithmic complexity based upon\nresource-bounded computation, in contrast to previously thoroughly investigated\ndistributions produced from the output distribution of Turing machines. This\napproach allows for numerical approximations to algorithmic\n(Kolmogorov-Chaitin) complexity-based estimations at each of the levels of a\ncomputational hierarchy. We demonstrate that all these estimations are\ncorrelated in rank and that they converge both in rank and values as a function\nof computational power, despite fundamental differences between computational\nmodels. In the context of natural processes that operate below the Turing\nuniversal level because of finite resources and physical degradation, the\ninvestigation of natural biases stemming from algorithmic rules may shed light\non the distribution of outcomes. We show that up to 60\\% of the\nsimplicity/complexity bias in distributions produced even by the weakest of the\ncomputational models can be accounted for by Algorithmic Probability in its\napproximation to the Universal Distribution.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 03:37:46 GMT"}, {"version": "v10", "created": "Fri, 23 Mar 2018 14:00:21 GMT"}, {"version": "v11", "created": "Fri, 13 Apr 2018 11:25:47 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 22:41:46 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 17:04:49 GMT"}, {"version": "v4", "created": "Fri, 10 Nov 2017 13:18:51 GMT"}, {"version": "v5", "created": "Tue, 14 Nov 2017 00:13:49 GMT"}, {"version": "v6", "created": "Wed, 15 Nov 2017 01:56:57 GMT"}, {"version": "v7", "created": "Thu, 16 Nov 2017 16:28:52 GMT"}, {"version": "v8", "created": "Thu, 1 Feb 2018 21:18:20 GMT"}, {"version": "v9", "created": "Sat, 24 Feb 2018 15:35:00 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Zenil", "Hector", ""], ["Badillo", "Liliana", ""], ["Hern\u00e1ndez-Orozco", "Santiago", ""], ["Hern\u00e1ndez-Quiroz", "Francisco", ""]]}, {"id": "1711.01744", "submitter": "Minh Trung Le", "authors": "Trung Le, Tu Dinh Nguyen, Dinh Phung", "title": "KGAN: How to Break The Minimax Game in GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) were intuitively and attractively\nexplained under the perspective of game theory, wherein two involving parties\nare a discriminator and a generator. In this game, the task of the\ndiscriminator is to discriminate the real and generated (i.e., fake) data,\nwhilst the task of the generator is to generate the fake data that maximally\nconfuses the discriminator. In this paper, we propose a new viewpoint for GANs,\nwhich is termed as the minimizing general loss viewpoint. This viewpoint shows\na connection between the general loss of a classification problem regarding a\nconvex loss function and a f-divergence between the true and fake data\ndistributions. Mathematically, we proposed a setting for the classification\nproblem of the true and fake data, wherein we can prove that the general loss\nof this classification problem is exactly the negative f-divergence for a\ncertain convex function f. This allows us to interpret the problem of learning\nthe generator for dismissing the f-divergence between the true and fake data\ndistributions as that of maximizing the general loss which is equivalent to the\nmin-max problem in GAN if the Logistic loss is used in the classification\nproblem. However, this viewpoint strengthens GANs in two ways. First, it allows\nus to employ any convex loss function for the discriminator. Second, it\nsuggests that rather than limiting ourselves in NN-based discriminators, we can\nalternatively utilize other powerful families. Bearing this viewpoint, we then\npropose using the kernel-based family for discriminators. This family has two\nappealing features: i) a powerful capacity in classifying non-linear nature\ndata and ii) being convex in the feature space. Using the convexity of this\nfamily, we can further develop Fenchel duality to equivalently transform the\nmax-min problem to the max-max dual problem.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 06:33:01 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Le", "Trung", ""], ["Nguyen", "Tu Dinh", ""], ["Phung", "Dinh", ""]]}, {"id": "1711.01754", "submitter": "Ju-Hong Lee", "authors": "Ju-Hong Lee, Moon-Ju Kang, Bumghi Choi", "title": "Learning Solving Procedure for Artificial Neural Network", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is expected that progress toward true artificial intelligence will be\nachieved through the emergence of a system that integrates representation\nlearning and complex reasoning (LeCun et al. 2015). In response to this\nprediction, research has been conducted on implementing the symbolic reasoning\nof a von Neumann computer in an artificial neural network (Graves et al. 2016;\nGraves et al. 2014; Reed et al. 2015). However, these studies have many\nlimitations in realizing neural-symbolic integration (Jaeger. 2016). Here, we\npresent a new learning paradigm: a learning solving procedure (LSP) that learns\nthe procedure for solving complex problems. This is not accomplished merely by\nlearning input-output data, but by learning algorithms through a solving\nprocedure that obtains the output as a sequence of tasks for a given input\nproblem. The LSP neural network system not only learns simple problems of\naddition and multiplication, but also the algorithms of complicated problems,\nsuch as complex arithmetic expression, sorting, and Hanoi Tower. To realize\nthis, the LSP neural network structure consists of a deep neural network and\nlong short-term memory, which are recursively combined. Through\nexperimentation, we demonstrate the efficiency and scalability of LSP and its\nvalidity as a mechanism of complex reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 07:28:10 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Lee", "Ju-Hong", ""], ["Kang", "Moon-Ju", ""], ["Choi", "Bumghi", ""]]}, {"id": "1711.01843", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Eric Dimla, Edwin Lughofer, Witold Pedrycz, Tegoeh\n  Tjahjowidowo", "title": "Online Tool Condition Monitoring Based on Parsimonious Ensemble+", "comments": "this paper has been published by IEEE Transactions on Cybernetics", "journal-ref": "IEEE Transactions on Cybernetics, 2018", "doi": "10.1109/TCYB.2018.2871120", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate diagnosis of tool wear in metal turning process remains an open\nchallenge for both scientists and industrial practitioners because of\ninhomogeneities in workpiece material, nonstationary machining settings to suit\nproduction requirements, and nonlinear relations between measured variables and\ntool wear. Common methodologies for tool condition monitoring still rely on\nbatch approaches which cannot cope with a fast sampling rate of metal cutting\nprocess. Furthermore they require a retraining process to be completed from\nscratch when dealing with a new set of machining parameters. This paper\npresents an online tool condition monitoring approach based on Parsimonious\nEnsemble+, pENsemble+. The unique feature of pENsemble+ lies in its highly\nflexible principle where both ensemble structure and base-classifier structure\ncan automatically grow and shrink on the fly based on the characteristics of\ndata streams. Moreover, the online feature selection scenario is integrated to\nactively sample relevant input attributes. The paper presents advancement of a\nnewly developed ensemble learning algorithm, pENsemble+, where online active\nlearning scenario is incorporated to reduce operator labelling effort. The\nensemble merging scenario is proposed which allows reduction of ensemble\ncomplexity while retaining its diversity. Experimental studies utilising\nreal-world manufacturing data streams and comparisons with well known\nalgorithms were carried out. Furthermore, the efficacy of pENsemble was\nexamined using benchmark concept drift data streams. It has been found that\npENsemble+ incurs low structural complexity and results in a significant\nreduction of operator labelling effort.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 11:31:46 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 21:12:37 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Dimla", "Eric", ""], ["Lughofer", "Edwin", ""], ["Pedrycz", "Witold", ""], ["Tjahjowidowo", "Tegoeh", ""]]}, {"id": "1711.01927", "submitter": "Robert B. Allen", "authors": "Robert B. Allen, Eunsang Yang, and Tatsawan Timakum", "title": "A Foundry of Human Activities and Infrastructures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct representation knowledgebases can enhance and even provide an\nalternative to document-centered digital libraries. Here we consider realist\nsemantic modeling of everyday activities and infrastructures in such\nknowledgebases. Because we want to integrate a wide variety of topics, a\ncollection of ontologies (a foundry) and a range of other knowledge resources\nare needed. We first consider modeling the routine procedures that support\nhuman activities and technologies. Next, we examine the interactions of\ntechnologies with aspects of social organization. Then, we consider approaches\nand issues for developing and validating explanations of the relationships\namong various entities.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 10:14:45 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Allen", "Robert B.", ""], ["Yang", "Eunsang", ""], ["Timakum", "Tatsawan", ""]]}, {"id": "1711.02012", "submitter": "Rahul Aralikatte", "authors": "Senthil Mani, Neelamadhav Gantayat, Rahul Aralikatte, Monika Gupta,\n  Sampath Dechu, Anush Sankaran, Shreya Khare, Barry Mitchell, Hemamalini\n  Subramanian, Hema Venkatarangan", "title": "Hi, how can I help you?: Automating enterprise IT support help desks", "comments": "To appear in IAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering is one of the primary challenges of natural language\nunderstanding. In realizing such a system, providing complex long answers to\nquestions is a challenging task as opposed to factoid answering as the former\nneeds context disambiguation. The different methods explored in the literature\ncan be broadly classified into three categories namely: 1) classification\nbased, 2) knowledge graph based and 3) retrieval based. Individually, none of\nthem address the need of an enterprise wide assistance system for an IT support\nand maintenance domain. In this domain the variance of answers is large ranging\nfrom factoid to structured operating procedures; the knowledge is present\nacross heterogeneous data sources like application specific documentation,\nticket management systems and any single technique for a general purpose\nassistance is unable to scale for such a landscape. To address this, we have\nbuilt a cognitive platform with capabilities adopted for this domain. Further,\nwe have built a general purpose question answering system leveraging the\nplatform that can be instantiated for multiple products, technologies in the\nsupport domain. The system uses a novel hybrid answering model that\norchestrates across a deep learning classifier, a knowledge graph based context\ndisambiguation module and a sophisticated bag-of-words search system. This\norchestration performs context switching for a provided question and also does\na smooth hand-off of the question to a human expert if none of the automated\ntechniques can provide a confident answer. This system has been deployed across\n675 internal enterprise IT support and maintenance projects.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 20:04:06 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Mani", "Senthil", ""], ["Gantayat", "Neelamadhav", ""], ["Aralikatte", "Rahul", ""], ["Gupta", "Monika", ""], ["Dechu", "Sampath", ""], ["Sankaran", "Anush", ""], ["Khare", "Shreya", ""], ["Mitchell", "Barry", ""], ["Subramanian", "Hemamalini", ""], ["Venkatarangan", "Hema", ""]]}, {"id": "1711.02013", "submitter": "Yikang Shen", "authors": "Yikang Shen, Zhouhan Lin, Chin-Wei Huang, Aaron Courville", "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon", "comments": "16 pages, 5 figures, ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural language model capable of unsupervised syntactic\nstructure induction. The model leverages the structure information to form\nbetter semantic representations and better language modeling. Standard\nrecurrent neural networks are limited by their structure and fail to\nefficiently use syntactic information. On the other hand, tree-structured\nrecursive networks usually require additional structural supervision at the\ncost of human expert annotation. In this paper, We propose a novel neural\nlanguage model, called the Parsing-Reading-Predict Networks (PRPN), that can\nsimultaneously induce the syntactic structure from unannotated sentences and\nleverage the inferred structure to learn a better language model. In our model,\nthe gradient can be directly back-propagated from the language model loss into\nthe neural parsing network. Experiments show that the proposed model can\ndiscover the underlying syntactic structure and achieve state-of-the-art\nperformance on word/character-level language model tasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 23:02:52 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 04:48:35 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Shen", "Yikang", ""], ["Lin", "Zhouhan", ""], ["Huang", "Chin-Wei", ""], ["Courville", "Aaron", ""]]}, {"id": "1711.02017", "submitter": "Xiaoliang Dai", "authors": "Xiaoliang Dai, Hongxu Yin, Niraj K. Jha", "title": "NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have begun to have a pervasive impact on various\napplications of machine learning. However, the problem of finding an optimal\nDNN architecture for large applications is challenging. Common approaches go\nfor deeper and larger DNN architectures but may incur substantial redundancy.\nTo address these problems, we introduce a network growth algorithm that\ncomplements network pruning to learn both weights and compact DNN architectures\nduring training. We propose a DNN synthesis tool (NeST) that combines both\nmethods to automate the generation of compact and accurate DNNs. NeST starts\nwith a randomly initialized sparse network called the seed architecture. It\niteratively tunes the architecture with gradient-based growth and\nmagnitude-based pruning of neurons and connections. Our experimental results\nshow that NeST yields accurate, yet very compact DNNs, with a wide range of\nseed architecture selection. For the LeNet-300-100 (LeNet-5) architecture, we\nreduce network parameters by 70.2x (74.3x) and floating-point operations\n(FLOPs) by 79.4x (43.7x). For the AlexNet and VGG-16 architectures, we reduce\nnetwork parameters (FLOPs) by 15.7x (4.6x) and 30.2x (8.6x), respectively.\nNeST's grow-and-prune paradigm delivers significant additional parameter and\nFLOPs reduction relative to pruning-only methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 17:03:39 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 18:45:01 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 04:04:09 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Dai", "Xiaoliang", ""], ["Yin", "Hongxu", ""], ["Jha", "Niraj K.", ""]]}, {"id": "1711.02114", "submitter": "Thiago Serra", "authors": "Thiago Serra and Christian Tjandraatmadja and Srikumar Ramalingam", "title": "Bounding and Counting Linear Regions of Deep Neural Networks", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the complexity of deep neural networks (DNN) that represent\npiecewise linear (PWL) functions. In particular, we study the number of linear\nregions, i.e. pieces, that a PWL function represented by a DNN can attain, both\ntheoretically and empirically. We present (i) tighter upper and lower bounds\nfor the maximum number of linear regions on rectifier networks, which are exact\nfor inputs of dimension one; (ii) a first upper bound for multi-layer maxout\nnetworks; and (iii) a first method to perform exact enumeration or counting of\nthe number of regions by modeling the DNN with a mixed-integer linear\nformulation. These bounds come from leveraging the dimension of the space\ndefining each linear region. The results also indicate that a deep rectifier\nnetwork can only have more linear regions than every shallow counterpart with\nsame number of neurons if that number exceeds the dimension of the input.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:06:12 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 13:18:46 GMT"}, {"version": "v3", "created": "Sat, 9 Jun 2018 10:30:13 GMT"}, {"version": "v4", "created": "Sun, 16 Sep 2018 01:36:41 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Serra", "Thiago", ""], ["Tjandraatmadja", "Christian", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1711.02132", "submitter": "Karim Ahmed", "authors": "Karim Ahmed, Nitish Shirish Keskar, Richard Socher", "title": "Weighted Transformer Network for Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art results on neural machine translation often use attentional\nsequence-to-sequence models with some form of convolution or recursion. Vaswani\net al. (2017) propose a new architecture that avoids recurrence and convolution\ncompletely. Instead, it uses only self-attention and feed-forward layers. While\nthe proposed architecture achieves state-of-the-art results on several machine\ntranslation tasks, it requires a large number of parameters and training\niterations to converge. We propose Weighted Transformer, a Transformer with\nmodified attention layers, that not only outperforms the baseline network in\nBLEU score but also converges 15-40% faster. Specifically, we replace the\nmulti-head attention by multiple self-attention branches that the model learns\nto combine during the training process. Our model improves the state-of-the-art\nperformance by 0.5 BLEU points on the WMT 2014 English-to-German translation\ntask and by 0.4 on the English-to-French translation task.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:35:00 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Ahmed", "Karim", ""], ["Keskar", "Nitish Shirish", ""], ["Socher", "Richard", ""]]}, {"id": "1711.02159", "submitter": "Anirban Roychowdhury", "authors": "Anirban Roychowdhury and Srinivasan Parthasarathy", "title": "Adaptive Bayesian Sampling with Monte Carlo EM", "comments": "In Proc. 30th Advances in Neural Information Processing Systems\n  (NIPS), 2017 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel technique for learning the mass matrices in samplers\nobtained from discretized dynamics that preserve some energy function. Existing\nadaptive samplers use Riemannian preconditioning techniques, where the mass\nmatrices are functions of the parameters being sampled. This leads to\nsignificant complexities in the energy reformulations and resultant dynamics,\noften leading to implicit systems of equations and requiring inversion of\nhigh-dimensional matrices in the leapfrog steps. Our approach provides a\nsimpler alternative, by using existing dynamics in the sampling step of a Monte\nCarlo EM framework, and learning the mass matrices in the M step with a novel\nonline technique. We also propose a way to adaptively set the number of samples\ngathered in the E step, using sampling error estimates from the leapfrog\ndynamics. Along with a novel stochastic sampler based on Nos\\'{e}-Poincar\\'{e}\ndynamics, we use this framework with standard Hamiltonian Monte Carlo (HMC) as\nwell as newer stochastic algorithms such as SGHMC and SGNHT, and show strong\nperformance on synthetic and real high-dimensional sampling scenarios; we\nachieve sampling accuracies comparable to Riemannian samplers while being\nsignificantly faster.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 20:23:24 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Roychowdhury", "Anirban", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1711.02195", "submitter": "Hunter Lang", "authors": "Hunter Lang, David Sontag, Aravindan Vijayaraghavan", "title": "Optimality of Approximate Inference Algorithms on Stable Instances", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate algorithms for structured prediction problems---such as LP\nrelaxations and the popular alpha-expansion algorithm (Boykov et al.\n2001)---typically far exceed their theoretical performance guarantees on\nreal-world instances. These algorithms often find solutions that are very close\nto optimal. The goal of this paper is to partially explain the performance of\nalpha-expansion and an LP relaxation algorithm on MAP inference in\nFerromagnetic Potts models (FPMs). Our main results give stability conditions\nunder which these two algorithms provably recover the optimal MAP solution.\nThese theoretical results complement numerous empirical observations of good\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 22:14:34 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 16:02:44 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Lang", "Hunter", ""], ["Sontag", "David", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1711.02231", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang, Chen Fang, Zhaowen Wang, Julian McAuley", "title": "Visually-Aware Fashion Recommendation and Design with Generative Image\n  Models", "comments": "10 pages, 6 figures. Accepted by ICDM'17 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building effective recommender systems for domains like fashion is\nchallenging due to the high level of subjectivity and the semantic complexity\nof the features involved (i.e., fashion styles). Recent work has shown that\napproaches to `visual' recommendation (e.g.~clothing, art, etc.) can be made\nmore accurate by incorporating visual signals directly into the recommendation\nobjective, using `off-the-shelf' feature representations derived from deep\nnetworks. Here, we seek to extend this contribution by showing that\nrecommendation performance can be significantly improved by learning `fashion\naware' image representations directly, i.e., by training the image\nrepresentation (from the pixel level) and the recommender system jointly; this\ncontribution is related to recent work using Siamese CNNs, though we are able\nto show improvements over state-of-the-art recommendation techniques such as\nBPR and variants that make use of pre-trained visual features. Furthermore, we\nshow that our model can be used \\emph{generatively}, i.e., given a user and a\nproduct category, we can generate new images (i.e., clothing items) that are\nmost consistent with their personal taste. This represents a first step towards\nbuilding systems that go beyond recommending existing items from a product\ncorpus, but which can be used to suggest styles and aid the design of new\nproducts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 00:17:51 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["Fang", "Chen", ""], ["Wang", "Zhaowen", ""], ["McAuley", "Julian", ""]]}, {"id": "1711.02301", "submitter": "Maithra Raghu", "authors": "Maithra Raghu, Alex Irpan, Jacob Andreas, Robert Kleinberg, Quoc V.\n  Le, Jon Kleinberg", "title": "Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?", "comments": "Accepted to ICML 2018, code opensourced at:\n  https://github.com/rubai5/ESS_Game", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has achieved many recent successes, but our\nunderstanding of its strengths and limitations is hampered by the lack of rich\nenvironments in which we can fully characterize optimal behavior, and\ncorrespondingly diagnose individual actions against such a characterization.\nHere we consider a family of combinatorial games, arising from work of Erdos,\nSelfridge, and Spencer, and we propose their use as environments for evaluating\nand comparing different approaches to reinforcement learning. These games have\na number of appealing features: they are challenging for current learning\napproaches, but they form (i) a low-dimensional, simply parametrized\nenvironment where (ii) there is a linear closed form solution for optimal\nbehavior from any state, and (iii) the difficulty of the game can be tuned by\nchanging environment parameters in an interpretable way. We use these\nErdos-Selfridge-Spencer games not only to compare different algorithms, but\ntest for generalization, make comparisons to supervised learning, analyse\nmultiagent play, and even develop a self play algorithm. Code can be found at:\nhttps://github.com/rubai5/ESS_Game\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 06:16:56 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 00:51:17 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 21:05:00 GMT"}, {"version": "v4", "created": "Wed, 27 Jun 2018 03:24:25 GMT"}, {"version": "v5", "created": "Fri, 29 Jun 2018 00:18:48 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Raghu", "Maithra", ""], ["Irpan", "Alex", ""], ["Andreas", "Jacob", ""], ["Kleinberg", "Robert", ""], ["Le", "Quoc V.", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1711.02309", "submitter": "Vatsal Sharan", "authors": "Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant", "title": "Learning Overcomplete HMMs", "comments": "Added acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning overcomplete HMMs---those that have many\nhidden states but a small output alphabet. Despite having significant practical\nimportance, such HMMs are poorly understood with no known positive or negative\nresults for efficient learning. In this paper, we present several new\nresults---both positive and negative---which help define the boundaries between\nthe tractable and intractable settings. Specifically, we show positive results\nfor a large subclass of HMMs whose transition matrices are sparse,\nwell-conditioned, and have small probability mass on short cycles. On the other\nhand, we show that learning is impossible given only a polynomial number of\nsamples for HMMs with a small output alphabet and whose transition matrices are\nrandom regular graphs with large degree. We also discuss these results in the\ncontext of learning HMMs which can capture long-term dependencies.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 06:55:03 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 01:49:33 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Sharan", "Vatsal", ""], ["Kakade", "Sham", ""], ["Liang", "Percy", ""], ["Valiant", "Gregory", ""]]}, {"id": "1711.02326", "submitter": "Nan Rosemary Ke", "authors": "Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas,\n  Laurent Charlin, Chris Pal, Yoshua Bengio", "title": "Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major drawback of backpropagation through time (BPTT) is the difficulty of\nlearning long-term dependencies, coming from having to propagate credit\ninformation backwards through every single step of the forward computation.\nThis makes BPTT both computationally impractical and biologically implausible.\nFor this reason, full backpropagation through time is rarely used on long\nsequences, and truncated backpropagation through time is used as a heuristic.\nHowever, this usually leads to biased estimates of the gradient in which longer\nterm dependencies are ignored. Addressing this issue, we propose an alternative\nalgorithm, Sparse Attentive Backtracking, which might also be related to\nprinciples used by brains to learn long-term dependencies. Sparse Attentive\nBacktracking learns an attention mechanism over the hidden states of the past\nand selectively backpropagates through paths with high attention weights. This\nallows the model to learn long term dependencies while only backtracking for a\nsmall number of time steps, not just from the recent past but also from\nattended relevant past states.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 07:52:12 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Ke", "Nan Rosemary", ""], ["Goyal", "Anirudh", ""], ["Bilaniuk", "Olexa", ""], ["Binas", "Jonathan", ""], ["Charlin", "Laurent", ""], ["Pal", "Chris", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1711.02368", "submitter": "Masato Asahara", "authors": "Masato Asahara and Ryohei Fujimaki", "title": "Distributed Bayesian Piecewise Sparse Linear Models", "comments": "Short version of this paper will be published in IEEE BigData 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of interpretability of machine learning models has been\nincreasing due to emerging enterprise predictive analytics, threat of data\nprivacy, accountability of artificial intelligence in society, and so on.\nPiecewise linear models have been actively studied to achieve both accuracy and\ninterpretability. They often produce competitive accuracy against\nstate-of-the-art non-linear methods. In addition, their representations (i.e.,\nrule-based segmentation plus sparse linear formula) are often preferred by\ndomain experts. A disadvantage of such models, however, is high computational\ncost for simultaneous determinations of the number of \"pieces\" and cardinality\nof each linear predictor, which has restricted their applicability to\nmiddle-scale data sets. This paper proposes a distributed factorized asymptotic\nBayesian (FAB) inference of learning piece-wise sparse linear models on\ndistributed memory architectures. The distributed FAB inference solves the\nsimultaneous model selection issue without communicating $O(N)$ data where N is\nthe number of training samples and achieves linear scale-out against the number\nof CPU cores. Experimental results demonstrate that the distributed FAB\ninference achieves high prediction accuracy and performance scalability with\nboth synthetic and benchmark data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 10:05:31 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Asahara", "Masato", ""], ["Fujimaki", "Ryohei", ""]]}, {"id": "1711.02515", "submitter": "Yatao A. Bian", "authors": "An Bian, Kfir Y. Levy, Andreas Krause, Joachim M. Buhmann", "title": "Continuous DR-submodular Maximization: Structure and Algorithms", "comments": "Published in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DR-submodular continuous functions are important objectives with wide\nreal-world applications spanning MAP inference in determinantal point processes\n(DPPs), and mean-field inference for probabilistic submodular models, amongst\nothers. DR-submodularity captures a subclass of non-convex functions that\nenables both exact minimization and approximate maximization in polynomial\ntime.\n  In this work we study the problem of maximizing non-monotone DR-submodular\ncontinuous functions under general down-closed convex constraints. We start by\ninvestigating geometric properties that underlie such objectives, e.g., a\nstrong relation between (approximately) stationary points and global optimum is\nproved. These properties are then used to devise two optimization algorithms\nwith provable guarantees. Concretely, we first devise a \"two-phase\" algorithm\nwith $1/4$ approximation guarantee. This algorithm allows the use of existing\nmethods for finding (approximately) stationary points as a subroutine, thus,\nharnessing recent progress in non-convex optimization. Then we present a\nnon-monotone Frank-Wolfe variant with $1/e$ approximation guarantee and\nsublinear convergence rate. Finally, we extend our approach to a broader class\nof generalized DR-submodular continuous functions, which captures a wider\nspectrum of applications. Our theoretical findings are validated on synthetic\nand real-world problem instances.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 01:07:56 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 20:38:16 GMT"}, {"version": "v3", "created": "Sat, 16 Dec 2017 18:39:02 GMT"}, {"version": "v4", "created": "Fri, 24 May 2019 16:14:41 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Bian", "An", ""], ["Levy", "Kfir Y.", ""], ["Krause", "Andreas", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1711.02741", "submitter": "Kuan Fang", "authors": "Kuan Fang, Yu Xiang, Xiaocheng Li, Silvio Savarese", "title": "Recurrent Autoregressive Networks for Online Multi-Object Tracking", "comments": "10 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenge of online multi-object tracking is to reliably associate\nobject trajectories with detections in each video frame based on their tracking\nhistory. In this work, we propose the Recurrent Autoregressive Network (RAN), a\ntemporal generative modeling framework to characterize the appearance and\nmotion dynamics of multiple objects over time. The RAN couples an external\nmemory and an internal memory. The external memory explicitly stores previous\ninputs of each trajectory in a time window, while the internal memory learns to\nsummarize long-term tracking history and associate detections by processing the\nexternal memory. We conduct experiments on the MOT 2015 and 2016 datasets to\ndemonstrate the robustness of our tracking method in highly crowded and\noccluded scenes. Our method achieves top-ranked results on the two benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 21:51:22 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 04:21:03 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Fang", "Kuan", ""], ["Xiang", "Yu", ""], ["Li", "Xiaocheng", ""], ["Savarese", "Silvio", ""]]}, {"id": "1711.02782", "submitter": "Sharan Narang", "authors": "Sharan Narang, Eric Undersander, Gregory Diamos", "title": "Block-Sparse Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are used in state-of-the-art models in\ndomains such as speech recognition, machine translation, and language\nmodelling. Sparsity is a technique to reduce compute and memory requirements of\ndeep learning models. Sparse RNNs are easier to deploy on devices and high-end\nserver processors. Even though sparse operations need less compute and memory\nrelative to their dense counterparts, the speed-up observed by using sparse\noperations is less than expected on different hardware platforms. In order to\naddress this issue, we investigate two different approaches to induce block\nsparsity in RNNs: pruning blocks of weights in a layer and using group lasso\nregularization to create blocks of weights with zeros. Using these techniques,\nwe demonstrate that we can create block-sparse RNNs with sparsity ranging from\n80% to 90% with small loss in accuracy. This allows us to reduce the model size\nby roughly 10x. Additionally, we can prune a larger dense network to recover\nthis loss in accuracy while maintaining high block sparsity and reducing the\noverall parameter count. Our technique works with a variety of block sizes up\nto 32x32. Block-sparse RNNs eliminate overheads related to data storage and\nirregular memory accesses while increasing hardware efficiency compared to\nunstructured sparsity.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 00:57:54 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Narang", "Sharan", ""], ["Undersander", "Eric", ""], ["Diamos", "Gregory", ""]]}, {"id": "1711.02807", "submitter": "Mark Raugas", "authors": "Nicole Nichols, Mark Raugas, Robert Jasper, Nathan Hilliard", "title": "Faster Fuzzing: Reinitialization with Deep Neural Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the performance of the American Fuzzy Lop (AFL) fuzz testing\nframework by using Generative Adversarial Network (GAN) models to reinitialize\nthe system with novel seed files. We assess performance based on the temporal\nrate at which we produce novel and unseen code paths. We compare this approach\nto seed file generation from a random draw of bytes observed in the training\nseed files. The code path lengths and variations were not sufficiently diverse\nto fully replace AFL input generation. However, augmenting native AFL with\nthese additional code paths demonstrated improvements over AFL alone.\nSpecifically, experiments showed the GAN was faster and more effective than the\nLSTM and out-performed a random augmentation strategy, as measured by the\nnumber of unique code paths discovered. GAN helps AFL discover 14.23% more code\npaths than the random strategy in the same amount of CPU time, finds 6.16% more\nunique code paths, and finds paths that are on average 13.84% longer. Using GAN\nshows promise as a reinitialization strategy for AFL to help the fuzzer\nexercise deep paths in software.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 02:43:18 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Nichols", "Nicole", ""], ["Raugas", "Mark", ""], ["Jasper", "Robert", ""], ["Hilliard", "Nathan", ""]]}, {"id": "1711.02810", "submitter": "Biswarup Bhattacharya", "authors": "Biswarup Bhattacharya, Abhishek Sinha", "title": "Deep Fault Analysis and Subset Selection in Solar Power Grids", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-availability of reliable and sustainable electric power is a major\nproblem in the developing world. Renewable energy sources like solar are not\nvery lucrative in the current stage due to various uncertainties like weather,\nstorage, land use among others. There also exists various other issues like\nmis-commitment of power, absence of intelligent fault analysis, congestion,\netc. In this paper, we propose a novel deep learning-based system for\npredicting faults and selecting power generators optimally so as to reduce\ncosts and ensure higher reliability in solar power systems. The results are\nhighly encouraging and they suggest that the approaches proposed in this paper\nhave the potential to be applied successfully in the developing world.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 03:09:51 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Bhattacharya", "Biswarup", ""], ["Sinha", "Abhishek", ""]]}, {"id": "1711.02827", "submitter": "Dylan Hadfield-Menell", "authors": "Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell,\n  Anca Dragan", "title": "Inverse Reward Design", "comments": "Advances in Neural Information Processing Systems 30 (NIPS 2017)\n  Revised Oct 2020 to fix a typo in Eq. 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous agents optimize the reward function we give them. What they don't\nknow is how hard it is for us to design a reward function that actually\ncaptures what we want. When designing the reward, we might think of some\nspecific training scenarios, and make sure that the reward will lead to the\nright behavior in those scenarios. Inevitably, agents encounter new scenarios\n(e.g., new types of terrain) where optimizing that same reward may lead to\nundesired behavior. Our insight is that reward functions are merely\nobservations about what the designer actually wants, and that they should be\ninterpreted in the context in which they were designed. We introduce inverse\nreward design (IRD) as the problem of inferring the true objective based on the\ndesigned reward and the training MDP. We introduce approximate methods for\nsolving IRD problems, and use their solution to plan risk-averse behavior in\ntest MDPs. Empirical results suggest that this approach can help alleviate\nnegative side effects of misspecified reward functions and mitigate reward\nhacking.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 04:44:32 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 15:41:58 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Hadfield-Menell", "Dylan", ""], ["Milli", "Smitha", ""], ["Abbeel", "Pieter", ""], ["Russell", "Stuart", ""], ["Dragan", "Anca", ""]]}, {"id": "1711.02831", "submitter": "Biswarup Bhattacharya", "authors": "Arna Ghosh, Biswarup Bhattacharya, Somnath Basu Roy Chowdhury", "title": "SIMILARnet: Simultaneous Intelligent Localization and Recognition\n  Network", "comments": "5 pages; 2 figures; 2 tables; All authors have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global Average Pooling (GAP) [4] has been used previously to generate class\nactivation for image classification tasks. The motivation behind SIMILARnet\ncomes from the fact that the convolutional filters possess position information\nof the essential features and hence, combination of the feature maps could help\nus locate the class instances in an image. We propose a biologically inspired\nmodel that is free of differential connections and doesn't require separate\ntraining thereby reducing computation overhead. Our novel architecture\ngenerates promising results and unlike existing methods, the model is not\nsensitive to the input image size, thus promising wider application. Codes for\nthe experiment and illustrations can be found at:\nhttps://github.com/brcsomnath/Advanced-GAP .\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 05:08:26 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Ghosh", "Arna", ""], ["Bhattacharya", "Biswarup", ""], ["Chowdhury", "Somnath Basu Roy", ""]]}, {"id": "1711.02857", "submitter": "Jianqiao Wangni", "authors": "Jianqiao Wangni, Dahua Lin", "title": "Learning Sparse Visual Representations with Leaky Capped Norm\n  Regularizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity inducing regularization is an important part for learning\nover-complete visual representations. Despite the popularity of $\\ell_1$\nregularization, in this paper, we investigate the usage of non-convex\nregularizations in this problem. Our contribution consists of three parts.\nFirst, we propose the leaky capped norm regularization (LCNR), which allows\nmodel weights below a certain threshold to be regularized more strongly as\nopposed to those above, therefore imposes strong sparsity and only introduces\ncontrollable estimation bias. We propose a majorization-minimization algorithm\nto optimize the joint objective function. Second, our study over monocular 3D\nshape recovery and neural networks with LCNR outperforms $\\ell_1$ and other\nnon-convex regularizations, achieving state-of-the-art performance and faster\nconvergence. Third, we prove a theoretical global convergence speed on the 3D\nrecovery problem. To the best of our knowledge, this is the first convergence\nanalysis of the 3D recovery problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 07:54:41 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Wangni", "Jianqiao", ""], ["Lin", "Dahua", ""]]}, {"id": "1711.02877", "submitter": "Michel Fliess", "authors": "C\\'edric Join, Emmanuel Delaleau, Michel Fliess, Claude H. Moog", "title": "Un r\\'esultat intrigant en commande sans mod\\`ele", "comments": "in French,\n  https://www.openscience.fr/Un-resultat-intrigant-en-commande-sans-modele", "journal-ref": "ISTE OpenScience Automatique, vol. 1, 2017", "doi": null, "report-no": null, "categories": "cs.SY cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An elementary mathematical example proves, thanks to the Routh-Hurwitz\ncriterion, a result that is intriguing with respect to today's practical\nunderstanding of model-free control, i.e., an \"intelligent\" proportional\ncontroller (iP) may turn to be more difficult to tune than an intelligent\nproportional-derivative one (iPD). The vast superiority of iPDs when compared\nto classic PIDs is shown via computer simulations. The introduction as well as\nthe conclusion analyse model-free control in the light of recent advances.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 09:26:09 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Join", "C\u00e9dric", ""], ["Delaleau", "Emmanuel", ""], ["Fliess", "Michel", ""], ["Moog", "Claude H.", ""]]}, {"id": "1711.02974", "submitter": "Michel  Barlaud", "authors": "Cyprien Gilet, Marie Deprez, Jean-Baptiste Caillau and Michel Barlaud", "title": "Clustering with feature selection using alternating minimization,\n  Application to computational biology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with unsupervised clustering with feature selection. The\nproblem is to estimate both labels and a sparse projection matrix of weights.\nTo address this combinatorial non-convex problem maintaining a strict control\non the sparsity of the matrix of weights, we propose an alternating\nminimization of the Frobenius norm criterion. We provide a new efficient\nalgorithm named K-sparse which alternates k-means with projection-gradient\nminimization. The projection-gradient step is a method of splitting type, with\nexact projection on the $\\ell^1$ ball to promote sparsity. The convergence of\nthe gradient-projection step is addressed, and a preliminary analysis of the\nalternating minimization is made. The Frobenius norm criterion converges as the\nnumber of iterates in Algorithm K-sparse goes to infinity. Experiments on\nSingle Cell RNA sequencing datasets show that our method significantly improves\nthe results of PCA k-means, spectral clustering, SIMLR, and Sparcl methods, and\nachieves a relevant selection of genes. The complexity of K-sparse is linear in\nthe number of samples (cells), so that the method scales up to large datasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 14:42:55 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 09:45:42 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 14:29:53 GMT"}, {"version": "v4", "created": "Fri, 24 May 2019 12:04:34 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Gilet", "Cyprien", ""], ["Deprez", "Marie", ""], ["Caillau", "Jean-Baptiste", ""], ["Barlaud", "Michel", ""]]}, {"id": "1711.03026", "submitter": "Biswarup Bhattacharya", "authors": "Biswarup Bhattacharya, Abhishek Sinha", "title": "Intelligent Fault Analysis in Electrical Power Grids", "comments": "In proceedings of the 29th IEEE International Conference on Tools\n  with Artificial Intelligence (ICTAI) 2017 (full paper); 6 pages; 13 figures", "journal-ref": null, "doi": "10.1109/ICTAI.2017.00151", "report-no": null, "categories": "cs.SY cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power grids are one of the most important components of infrastructure in\ntoday's world. Every nation is dependent on the security and stability of its\nown power grid to provide electricity to the households and industries. A\nmalfunction of even a small part of a power grid can cause loss of\nproductivity, revenue and in some cases even life. Thus, it is imperative to\ndesign a system which can detect the health of the power grid and take\nprotective measures accordingly even before a serious anomaly takes place. To\nachieve this objective, we have set out to create an artificially intelligent\nsystem which can analyze the grid information at any given time and determine\nthe health of the grid through the usage of sophisticated formal models and\nnovel machine learning techniques like recurrent neural networks. Our system\nsimulates grid conditions including stimuli like faults, generator output\nfluctuations, load fluctuations using Siemens PSS/E software and this data is\ntrained using various classifiers like SVM, LSTM and subsequently tested. The\nresults are excellent with our methods giving very high accuracy for the data.\nThis model can easily be scaled to handle larger and more complex grid\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 16:03:04 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Bhattacharya", "Biswarup", ""], ["Sinha", "Abhishek", ""]]}, {"id": "1711.03067", "submitter": "Ting Chen", "authors": "Ting Chen, Martin Renqiang Min and Yizhou Sun", "title": "Learning K-way D-dimensional Discrete Code For Compact Embedding\n  Representations", "comments": "NIPS'17 DISCML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding methods such as word embedding have become pillars for many\napplications containing discrete structures. Conventional embedding methods\ndirectly associate each symbol with a continuous embedding vector, which is\nequivalent to applying linear transformation based on \"one-hot\" encoding of the\ndiscrete symbols. Despite its simplicity, such approach yields number of\nparameters that grows linearly with the vocabulary size and can lead to\noverfitting. In this work we propose a much more compact K-way D-dimensional\ndiscrete encoding scheme to replace the \"one-hot\" encoding. In \"KD encoding\",\neach symbol is represented by a $D$-dimensional code, and each of its dimension\nhas a cardinality of $K$. The final symbol embedding vector can be generated by\ncomposing the code embedding vectors. To learn the semantically meaningful\ncode, we derive a relaxed discrete optimization technique based on stochastic\ngradient descent. By adopting the new coding system, the efficiency of\nparameterization can be significantly improved (from linear to logarithmic),\nand this can also mitigate the over-fitting problem. In our experiments with\nlanguage modeling, the number of embedding parameters can be reduced by 97\\%\nwhile achieving similar or better performance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 17:46:55 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 06:12:54 GMT"}, {"version": "v3", "created": "Sun, 10 Dec 2017 22:00:30 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Chen", "Ting", ""], ["Min", "Martin Renqiang", ""], ["Sun", "Yizhou", ""]]}, {"id": "1711.03087", "submitter": "Jonathan C. Campbell", "authors": "Jonathan C. Campbell (1) and Clark Verbrugge (1) ((1) McGill\n  University)", "title": "Exploration in NetHack With Secret Discovery", "comments": "11 pages, 11 figures. Accepted in IEEE Transactions on Games.\n  Revision adds BotHack comparison, result breakdown by num. map rooms, and\n  improved optimal solution", "journal-ref": null, "doi": "10.1109/TG.2018.2861759", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roguelike games generally feature exploration problems as a critical, yet\noften repetitive element of gameplay. Automated approaches, however, face\nchallenges in terms of optimality, as well as due to incomplete information,\nsuch as from the presence of secret doors. This paper presents an algorithmic\napproach to exploration of roguelike dungeon environments. Our design aims to\nminimize exploration time, balancing coverage and discovery of secret areas\nwith resource cost. Our algorithm is based on the concept of occupancy maps\npopular in robotics, adapted to encourage efficient discovery of secret access\npoints. Through extensive experimentation on NetHack maps we show that this\ntechnique is significantly more efficient than simpler greedy approaches and an\nexisting automated player. We further investigate optimized parameterization\nfor the algorithm through a comprehensive data analysis. These results point\ntowards better automation for players as well as heuristics applicable to fully\nautomated gameplay.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 18:40:00 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 21:12:48 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Campbell", "Jonathan C.", ""], ["Verbrugge", "Clark", ""]]}, {"id": "1711.03147", "submitter": "Clemente Rubio-Manzano", "authors": "Clemente Rubio-Manzano, Martin Pereira-Fari\\~na", "title": "On the incorporation of interval-valued fuzzy sets into the Bousi-Prolog\n  system: declarative semantics, implementation and applications", "comments": null, "journal-ref": "Interactions Between Computational Intelligence and Mathematics\n  Studies in Computational Intelligence, vol 794. Springer 2018", "doi": "10.1007/978-3-030-01632-6_1", "report-no": null, "categories": "cs.AI cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyse the benefits of incorporating interval-valued fuzzy\nsets into the Bousi-Prolog system. A syntax, declarative semantics and im-\nplementation for this extension is presented and formalised. We show, by using\npotential applications, that fuzzy logic programming frameworks enhanced with\nthem can correctly work together with lexical resources and ontologies in order\nto improve their capabilities for knowledge representation and reasoning.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 20:25:43 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Rubio-Manzano", "Clemente", ""], ["Pereira-Fari\u00f1a", "Martin", ""]]}, {"id": "1711.03198", "submitter": "Fang Liu", "authors": "Fang Liu, Swapna Buccapatnam, Ness Shroff", "title": "Information Directed Sampling for Stochastic Bandits with Graph Feedback", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic multi-armed bandit problems with graph feedback, where\nthe decision maker is allowed to observe the neighboring actions of the chosen\naction. We allow the graph structure to vary with time and consider both\ndeterministic and Erd\\H{o}s-R\\'enyi random graph models. For such a graph\nfeedback model, we first present a novel analysis of Thompson sampling that\nleads to tighter performance bound than existing work. Next, we propose new\nInformation Directed Sampling based policies that are graph-aware in their\ndecision making. Under the deterministic graph case, we establish a Bayesian\nregret bound for the proposed policies that scales with the clique cover number\nof the graph instead of the number of actions. Under the random graph case, we\nprovide a Bayesian regret bound for the proposed policies that scales with the\nratio of the number of actions over the expected number of observations per\niteration. To the best of our knowledge, this is the first analytical result\nfor stochastic bandits with random graph feedback. Finally, using numerical\nevaluations, we demonstrate that our proposed IDS policies outperform existing\napproaches, including adaptions of upper confidence bound, $\\epsilon$-greedy\nand Exp3 algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 22:47:59 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Liu", "Fang", ""], ["Buccapatnam", "Swapna", ""], ["Shroff", "Ness", ""]]}, {"id": "1711.03225", "submitter": "Qizhe Xie", "authors": "Qizhe Xie, Guokun Lai, Zihang Dai, Eduard Hovy", "title": "Large-scale Cloze Test Dataset Created by Teachers", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloze tests are widely adopted in language exams to evaluate students'\nlanguage proficiency. In this paper, we propose the first large-scale\nhuman-created cloze test dataset CLOTH, containing questions used in\nmiddle-school and high-school language exams. With missing blanks carefully\ncreated by teachers and candidate choices purposely designed to be nuanced,\nCLOTH requires a deeper language understanding and a wider attention span than\npreviously automatically-generated cloze datasets. We test the performance of\ndedicatedly designed baseline models including a language model trained on the\nOne Billion Word Corpus and show humans outperform them by a significant\nmargin. We investigate the source of the performance gap, trace model\ndeficiencies to some distinct properties of CLOTH, and identify the limited\nability of comprehending the long-term context to be the key bottleneck.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 01:41:12 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 17:06:01 GMT"}, {"version": "v3", "created": "Tue, 28 Aug 2018 01:51:13 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Xie", "Qizhe", ""], ["Lai", "Guokun", ""], ["Dai", "Zihang", ""], ["Hovy", "Eduard", ""]]}, {"id": "1711.03237", "submitter": "James Wu", "authors": "Dr. W. A. Rivera and James C. Wu", "title": "CogSciK: Clustering for Cognitive Science Motivated Decision Making", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational models of decisionmaking must contend with the variance of\ncontext and any number of possible decisions that a defined strategic actor can\nmake at a given time. Relying on cognitive science theory, the authors have\ncreated an algorithm that captures the orientation of the actor towards an\nobject and arrays the possible decisions available to that actor based on their\ngiven intersubjective orientation. This algorithm, like a traditional K-means\nclustering algorithm, relies on a core-periphery structure that gives the\nlikelihood of moves as those closest to the cluster's centroid. The result is\nan algorithm that enables unsupervised classification of an array of decision\npoints belonging to an actor's present state and deeply rooted in cognitive\nscience theory.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 02:28:59 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Rivera", "Dr. W. A.", ""], ["Wu", "James C.", ""]]}, {"id": "1711.03243", "submitter": "Yewen Pu", "authors": "Yewen Pu, Zachery Miranda, Armando Solar-Lezama, Leslie Pack Kaelbling", "title": "Selecting Representative Examples for Program Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program synthesis is a class of regression problems where one seeks a\nsolution, in the form of a source-code program, mapping the inputs to their\ncorresponding outputs exactly. Due to its precise and combinatorial nature,\nprogram synthesis is commonly formulated as a constraint satisfaction problem,\nwhere input-output examples are encoded as constraints and solved with a\nconstraint solver. A key challenge of this formulation is scalability: while\nconstraint solvers work well with a few well-chosen examples, a large set of\nexamples can incur significant overhead in both time and memory. We describe a\nmethod to discover a subset of examples that is both small and representative:\nthe subset is constructed iteratively, using a neural network to predict the\nprobability of unchosen examples conditioned on the chosen examples in the\nsubset, and greedily adding the least probable example. We empirically evaluate\nthe representativeness of the subsets constructed by our method, and\ndemonstrate such subsets can significantly improve synthesis time and\nstability.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 03:38:15 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 00:34:06 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 04:06:10 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Pu", "Yewen", ""], ["Miranda", "Zachery", ""], ["Solar-Lezama", "Armando", ""], ["Kaelbling", "Leslie Pack", ""]]}, {"id": "1711.03331", "submitter": "Alexander Scheidler", "authors": "Alexander Scheidler and Leon Thurner and Martin Braun", "title": "Heuristic Optimization for Automated Distribution System Planning in\n  Network Integration Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network integration studies try to assess the impact of future developments,\nsuch as the increase of Renewable Energy Sources or the introduction of Smart\nGrid Technologies, on large-scale network areas. Goals can be to support\nstrategic alignment in the regulatory framework or to adapt the network\nplanning principles of Distribution System Operators. This study outlines an\napproach for the automated distribution system planning that can calculate\nnetwork reconfiguration, reinforcement and extension plans in a fully automated\nfashion. This allows the estimation of the expected cost in massive\nprobabilistic simulations of large numbers of real networks and constitutes a\ncore component of a framework for large-scale network integration studies.\nExemplary case study results are presented that were performed in cooperation\nwith different major distribution system operators. The case studies cover the\nestimation of expected network reinforcement costs, technical and economical\nassessment of smart grid technologies and structural network optimisation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 11:25:21 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 12:30:26 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Scheidler", "Alexander", ""], ["Thurner", "Leon", ""], ["Braun", "Martin", ""]]}, {"id": "1711.03430", "submitter": "Nicolas Troquard", "authors": "Nicolas Troquard, Roberto Confalonieri, Pietro Galliani, Rafael\n  Penaloza, Daniele Porello, Oliver Kutz", "title": "Repairing Ontologies via Axiom Weakening", "comments": "To appear AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology engineering is a hard and error-prone task, in which small changes\nmay lead to errors, or even produce an inconsistent ontology. As ontologies\ngrow in size, the need for automated methods for repairing inconsistencies\nwhile preserving as much of the original knowledge as possible increases. Most\nprevious approaches to this task are based on removing a few axioms from the\nontology to regain consistency. We propose a new method based on weakening\nthese axioms to make them less restrictive, employing the use of refinement\noperators. We introduce the theoretical framework for weakening DL ontologies,\npropose algorithms to repair ontologies based on the framework, and provide an\nanalysis of the computational complexity. Through an empirical analysis made\nover real-life ontologies, we show that our approach preserves significantly\nmore of the original knowledge of the ontology than removing axioms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:39:41 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Troquard", "Nicolas", ""], ["Confalonieri", "Roberto", ""], ["Galliani", "Pietro", ""], ["Penaloza", "Rafael", ""], ["Porello", "Daniele", ""], ["Kutz", "Oliver", ""]]}, {"id": "1711.03438", "submitter": "Baoxu Shi", "authors": "Baoxu Shi, Tim Weninger", "title": "Open-World Knowledge Graph Completion", "comments": "8 pages, accepted to AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graphs (KGs) have been applied to many tasks including Web search,\nlink prediction, recommendation, natural language processing, and entity\nlinking. However, most KGs are far from complete and are growing at a rapid\npace. To address these problems, Knowledge Graph Completion (KGC) has been\nproposed to improve KGs by filling in its missing connections. Unlike existing\nmethods which hold a closed-world assumption, i.e., where KGs are fixed and new\nentities cannot be easily added, in the present work we relax this assumption\nand propose a new open-world KGC task. As a first attempt to solve this task we\nintroduce an open-world KGC model called ConMask. This model learns embeddings\nof the entity's name and parts of its text-description to connect unseen\nentities to the KG. To mitigate the presence of noisy text descriptions,\nConMask uses a relationship-dependent content masking to extract relevant\nsnippets and then trains a fully convolutional neural network to fuse the\nextracted snippets with entities in the KG. Experiments on large data sets,\nboth old and new, show that ConMask performs well in the open-world KGC task\nand even outperforms existing KGC models on the standard closed-world KGC task.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:58:55 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Shi", "Baoxu", ""], ["Weninger", "Tim", ""]]}, {"id": "1711.03467", "submitter": "Mathias Lechner", "authors": "Mathias Lechner, Radu Grosu, Ramin M. Hasani", "title": "Worm-level Control through Search-based Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through natural evolution, nervous systems of organisms formed near-optimal\nstructures to express behavior. Here, we propose an effective way to create\ncontrol agents, by \\textit{re-purposing} the function of biological neural\ncircuit models, to govern similar real world applications. We model the\ntap-withdrawal (TW) neural circuit of the nematode, \\textit{C. elegans}, a\ncircuit responsible for the worm's reflexive response to external mechanical\ntouch stimulations, and learn its synaptic and neural parameters as a policy\nfor controlling the inverted pendulum problem. For reconfiguration of the\npurpose of the TW neural circuit, we manipulate a search-based reinforcement\nlearning. We show that our neural policy performs as good as existing\ntraditional control theory and machine learning approaches. A video\ndemonstration of the performance of our method can be accessed at\n\\url{https://youtu.be/o-Ia5IVyff8}.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 16:43:59 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Lechner", "Mathias", ""], ["Grosu", "Radu", ""], ["Hasani", "Ramin M.", ""]]}, {"id": "1711.03481", "submitter": "David Eriksson", "authors": "Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, Andrew Gordon\n  Wilson", "title": "Scalable Log Determinants for Gaussian Process Kernel Learning", "comments": "Appears at Advances in Neural Information Processing Systems 30\n  (NIPS), 2017", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS), 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For applications as varied as Bayesian neural networks, determinantal point\nprocesses, elliptical graphical models, and kernel learning for Gaussian\nprocesses (GPs), one must compute a log determinant of an $n \\times n$ positive\ndefinite matrix, and its derivatives - leading to prohibitive\n$\\mathcal{O}(n^3)$ computations. We propose novel $\\mathcal{O}(n)$ approaches\nto estimating these quantities from only fast matrix vector multiplications\n(MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and\nsurrogate models, and converge quickly even for kernel matrices that have\nchallenging spectra. We leverage these approximations to develop a scalable\nGaussian process approach to kernel learning. We find that Lanczos is generally\nsuperior to Chebyshev for kernel learning, and that a surrogate approach can be\nhighly efficient and accurate with popular kernels.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 17:25:30 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Dong", "Kun", ""], ["Eriksson", "David", ""], ["Nickisch", "Hannes", ""], ["Bindel", "David", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1711.03483", "submitter": "Eloi Zablocki", "authors": "\\'Eloi Zablocki, Benjamin Piwowarski, Laure Soulier, Patrick Gallinari", "title": "Learning Multi-Modal Word Representation Grounded in Visual Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing the semantics of words is a long-standing problem for the\nnatural language processing community. Most methods compute word semantics\ngiven their textual context in large corpora. More recently, researchers\nattempted to integrate perceptual and visual features. Most of these works\nconsider the visual appearance of objects to enhance word representations but\nthey ignore the visual environment and context in which objects appear. We\npropose to unify text-based techniques with vision-based techniques by\nsimultaneously leveraging textual and visual context to learn multimodal word\nembeddings. We explore various choices for what can serve as a visual context\nand present an end-to-end method to integrate visual context elements in a\nmultimodal skip-gram model. We provide experiments and extensive analysis of\nthe obtained results.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 17:28:07 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Zablocki", "\u00c9loi", ""], ["Piwowarski", "Benjamin", ""], ["Soulier", "Laure", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1711.03512", "submitter": "Gerrit van den Burg", "authors": "Gerrit J. J. van den Burg, Alfred O. Hero", "title": "Fast Meta-Learning for Adaptive Hierarchical Classifier Design", "comments": "Code available at: https://github.com/HeroResearchGroup/SmartSVM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new splitting criterion for a meta-learning approach to\nmulticlass classifier design that adaptively merges the classes into a\ntree-structured hierarchy of increasingly difficult binary classification\nproblems. The classification tree is constructed from empirical estimates of\nthe Henze-Penrose bounds on the pairwise Bayes misclassification rates that\nrank the binary subproblems in terms of difficulty of classification. The\nproposed empirical estimates of the Bayes error rate are computed from the\nminimal spanning tree (MST) of the samples from each pair of classes. Moreover,\na meta-learning technique is presented for quantifying the one-vs-rest Bayes\nerror rate for each individual class from a single MST on the entire dataset.\nExtensive simulations on benchmark datasets show that the proposed hierarchical\nmethod can often be learned much faster than competing methods, while achieving\ncompetitive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 18:22:32 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Burg", "Gerrit J. J. van den", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1711.03536", "submitter": "Ahmed Elgammal", "authors": "Ahmed Elgammal, Yan Kang, Milko Den Leeuw", "title": "Picasso, Matisse, or a Fake? Automated Analysis of Drawings at the\n  Stroke Level for Attribution and Authentication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a computational approach for analysis of strokes in line\ndrawings by artists. We aim at developing an AI methodology that facilitates\nattribution of drawings of unknown authors in a way that is not easy to be\ndeceived by forged art. The methodology used is based on quantifying the\ncharacteristics of individual strokes in drawings. We propose a novel algorithm\nfor segmenting individual strokes. We designed and compared different\nhand-crafted and learned features for the task of quantifying stroke\ncharacteristics. We also propose and compare different classification methods\nat the drawing level. We experimented with a dataset of 300 digitized drawings\nwith over 80 thousands strokes. The collection mainly consisted of drawings of\nPablo Picasso, Henry Matisse, and Egon Schiele, besides a small number of\nrepresentative works of other artists. The experiments shows that the proposed\nmethodology can classify individual strokes with accuracy 70%-90%, and\naggregate over drawings with accuracy above 80%, while being robust to be\ndeceived by fakes (with accuracy 100% for detecting fakes in most settings).\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 20:26:40 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Elgammal", "Ahmed", ""], ["Kang", "Yan", ""], ["Leeuw", "Milko Den", ""]]}, {"id": "1711.03537", "submitter": "Janneth Chicaiza", "authors": "Nelson Piedra, Janneth Chicaiza, Jorge Lopez-Vargas, Edmundo Tovar", "title": "Discovery of potential collaboration networks from open knowledge\n  sources", "comments": "2 pages, International Conference on Knowledge Engineering and\n  Semantic Web", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scientific publishing conveys the outputs of an academic or research\nactivity, in this sense; it also reflects the efforts and issues in which\npeople engage. To identify potential collaborative networks one of the simplest\napproaches is to leverage the co-authorship relations. In this approach,\nsemantic and hierarchic relationships defined by a Knowledge Organization\nSystem are used in order to improve the system's ability to recommend potential\nnetworks beyond the lexical or syntactic analysis of the topics or concepts\nthat are of interest to academics.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 20:58:16 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Piedra", "Nelson", ""], ["Chicaiza", "Janneth", ""], ["Lopez-Vargas", "Jorge", ""], ["Tovar", "Edmundo", ""]]}, {"id": "1711.03539", "submitter": "Fang Liu", "authors": "Fang Liu, Joohyun Lee, Ness Shroff", "title": "A Change-Detection based Framework for Piecewise-stationary Multi-Armed\n  Bandit Problem", "comments": "accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-armed bandit problem has been extensively studied under the\nstationary assumption. However in reality, this assumption often does not hold\nbecause the distributions of rewards themselves may change over time. In this\npaper, we propose a change-detection (CD) based framework for multi-armed\nbandit problems under the piecewise-stationary setting, and study a class of\nchange-detection based UCB (Upper Confidence Bound) policies, CD-UCB, that\nactively detects change points and restarts the UCB indices. We then develop\nCUSUM-UCB and PHT-UCB, that belong to the CD-UCB class and use cumulative sum\n(CUSUM) and Page-Hinkley Test (PHT) to detect changes. We show that CUSUM-UCB\nobtains the best known regret upper bound under mild assumptions. We also\ndemonstrate the regret reduction of the CD-UCB policies over arbitrary\nBernoulli rewards and Yahoo! datasets of webpage click-through rates.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 22:48:20 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 21:25:15 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Liu", "Fang", ""], ["Lee", "Joohyun", ""], ["Shroff", "Ness", ""]]}, {"id": "1711.03543", "submitter": "Anush Sankaran", "authors": "Akshay Sethi, Anush Sankaran, Naveen Panwar, Shreya Khare, Senthil\n  Mani", "title": "DLPaper2Code: Auto-generation of Code from Deep Learning Research Papers", "comments": "AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an abundance of research papers in deep learning, reproducibility or\nadoption of the existing works becomes a challenge. This is due to the lack of\nopen source implementations provided by the authors. Further, re-implementing\nresearch papers in a different library is a daunting task. To address these\nchallenges, we propose a novel extensible approach, DLPaper2Code, to extract\nand understand deep learning design flow diagrams and tables available in a\nresearch paper and convert them to an abstract computational graph. The\nextracted computational graph is then converted into execution ready source\ncode in both Keras and Caffe, in real-time. An arXiv-like website is created\nwhere the automatically generated designs is made publicly available for 5,000\nresearch papers. The generated designs could be rated and edited using an\nintuitive drag-and-drop UI framework in a crowdsourced manner. To evaluate our\napproach, we create a simulated dataset with over 216,000 valid design\nvisualizations using a manually defined grammar. Experiments on the simulated\ndataset show that the proposed framework provide more than $93\\%$ accuracy in\nflow diagram content extraction.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 10:00:19 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Sethi", "Akshay", ""], ["Sankaran", "Anush", ""], ["Panwar", "Naveen", ""], ["Khare", "Shreya", ""], ["Mani", "Senthil", ""]]}, {"id": "1711.03580", "submitter": "Kananat Suwanviwatana", "authors": "Kananat Suwanviwatana, Hiroyuki Iida", "title": "First Results from Using Game Refinement Measure and Learning\n  Coefficient in Scrabble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the entertainment experience and learning experience in\nScrabble. It proposes a new measure from the educational point of view, which\nwe call learning coefficient, based on the balance between the learner's skill\nand the challenge in Scrabble. Scrabble variants, generated using different\nsize of board and dictionary, are analyzed with two measures of game refinement\nand learning coefficient. The results show that 13x13 Scrabble yields the best\nentertainment experience and 15x15 (standard) Scrabble with 4% of original\ndictionary size yields the most effective environment for language learners.\nMoreover, 15x15 Scrabble with 10% of original dictionary size has a good\nbalance between entertainment and learning experience.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 10:39:42 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Suwanviwatana", "Kananat", ""], ["Iida", "Hiroyuki", ""]]}, {"id": "1711.03637", "submitter": "Shruti Kulkarni", "authors": "Shruti R. Kulkarni, John M. Alexiades, Bipin Rajendran", "title": "Learning and Real-time Classification of Hand-written Digits With\n  Spiking Neural Networks", "comments": "4 pages, 4 figures, 1 table, accepted at ICECS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel spiking neural network (SNN) for automated, real-time\nhandwritten digit classification and its implementation on a GP-GPU platform.\nInformation processing within the network, from feature extraction to\nclassification is implemented by mimicking the basic aspects of neuronal spike\ninitiation and propagation in the brain. The feature extraction layer of the\nSNN uses fixed synaptic weight maps to extract the key features of the image\nand the classifier layer uses the recently developed NormAD approximate\ngradient descent based supervised learning algorithm for spiking neural\nnetworks to adjust the synaptic weights. On the standard MNIST database images\nof handwritten digits, our network achieves an accuracy of 99.80% on the\ntraining set and 98.06% on the test set, with nearly 7x fewer parameters\ncompared to the state-of-the-art spiking networks. We further use this network\nin a GPU based user-interface system demonstrating real-time SNN simulation to\ninfer digits written by different users. On a test set of 500 such images, this\nreal-time platform achieves an accuracy exceeding 97% while making a prediction\nwithin an SNN emulation time of less than 100ms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 23:01:42 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Kulkarni", "Shruti R.", ""], ["Alexiades", "John M.", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1711.03640", "submitter": "Anakha Vasanthakumari Babu", "authors": "Anakha V Babu, Bipin Rajendran", "title": "Stochastic Deep Learning in Memristive Networks", "comments": "4 pages, 5 figures, accepted at ICECS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of stochastically trained deep neural networks\n(DNNs) whose synaptic weights are implemented using emerging memristive devices\nthat exhibit limited dynamic range, resolution, and variability in their\nprogramming characteristics. We show that a key device parameter to optimize\nthe learning efficiency of DNNs is the variability in its programming\ncharacteristics. DNNs with such memristive synapses, even with dynamic range as\nlow as $15$ and only $32$ discrete levels, when trained based on stochastic\nupdates suffer less than $3\\%$ loss in accuracy compared to floating point\nsoftware baseline. We also study the performance of stochastic memristive DNNs\nwhen used as inference engines with noise corrupted data and find that if the\ndevice variability can be minimized, the relative degradation in performance\nfor the Stochastic DNN is better than that of the software baseline. Hence, our\nstudy presents a new optimization corner for memristive devices for building\nlarge noise-immune deep learning systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 23:09:36 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Babu", "Anakha V", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1711.03676", "submitter": "Patrick M. Pilarski", "authors": "Patrick M. Pilarski, Richard S. Sutton, Kory W. Mathewson, Craig\n  Sherstan, Adam S. R. Parker, Ann L. Edwards", "title": "Communicative Capital for Prosthetic Agents", "comments": "33 pages, 10 figures; unpublished technical report undergoing peer\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an overarching perspective on the role that machine\nintelligence can play in enhancing human abilities, especially those that have\nbeen diminished due to injury or illness. As a primary contribution, we develop\nthe hypothesis that assistive devices, and specifically artificial arms and\nhands, can and should be viewed as agents in order for us to most effectively\nimprove their collaboration with their human users. We believe that increased\nagency will enable more powerful interactions between human users and next\ngeneration prosthetic devices, especially when the sensorimotor space of the\nprosthetic technology greatly exceeds the conventional control and\ncommunication channels available to a prosthetic user. To more concretely\nexamine an agency-based view on prosthetic devices, we propose a new schema for\ninterpreting the capacity of a human-machine collaboration as a function of\nboth the human's and machine's degrees of agency. We then introduce the idea of\ncommunicative capital as a way of thinking about the communication resources\ndeveloped by a human and a machine during their ongoing interaction. Using this\nschema of agency and capacity, we examine the benefits and disadvantages of\nincreasing the agency of a prosthetic limb. To do so, we present an analysis of\nexamples from the literature where building communicative capital has enabled a\nprogression of fruitful, task-directed interactions between prostheses and\ntheir human users. We then describe further work that is needed to concretely\nevaluate the hypothesis that prostheses are best thought of as agents. The\nagent-based viewpoint developed in this article significantly extends current\nthinking on how best to support the natural, functional use of increasingly\ncomplex prosthetic enhancements, and opens the door for more powerful\ninteractions between humans and their assistive technologies.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 03:19:59 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Pilarski", "Patrick M.", ""], ["Sutton", "Richard S.", ""], ["Mathewson", "Kory W.", ""], ["Sherstan", "Craig", ""], ["Parker", "Adam S. R.", ""], ["Edwards", "Ann L.", ""]]}, {"id": "1711.03678", "submitter": "Michael Janner", "authors": "Michael Janner, Jiajun Wu, Tejas D. Kulkarni, Ilker Yildirim, Joshua\n  B. Tenenbaum", "title": "Self-Supervised Intrinsic Image Decomposition", "comments": "NIPS 2017 camera-ready version, project page:\n  http://rin.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic decomposition from a single image is a highly challenging task, due\nto its inherent ambiguity and the scarcity of training data. In contrast to\ntraditional fully supervised learning approaches, in this paper we propose\nlearning intrinsic image decomposition by explaining the input image. Our\nmodel, the Rendered Intrinsics Network (RIN), joins together an image\ndecomposition pipeline, which predicts reflectance, shape, and lighting\nconditions given a single image, with a recombination function, a learned\nshading model used to recompose the original input based off of intrinsic image\npredictions. Our network can then use unsupervised reconstruction error as an\nadditional signal to improve its intermediate representations. This allows\nlarge-scale unlabeled data to be useful during training, and also enables\ntransferring learned knowledge to images of unseen object categories, lighting\nconditions, and shapes. Extensive experiments demonstrate that our method\nperforms well on both intrinsic image decomposition and knowledge transfer.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 03:31:27 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 22:52:18 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Janner", "Michael", ""], ["Wu", "Jiajun", ""], ["Kulkarni", "Tejas D.", ""], ["Yildirim", "Ilker", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1711.03726", "submitter": "Prakhar Gupta", "authors": "Prakhar Gupta, Shubh Gupta, Ajaykrishnan Jayagopal, Sourav Pal, Ritwik\n  Sinha", "title": "Saliency Prediction for Mobile User Interfaces", "comments": "Paper accepted at WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce models for saliency prediction for mobile user interfaces. A\nmobile interface may include elements like buttons, text, etc. in addition to\nnatural images which enable performing a variety of tasks. Saliency in natural\nimages is a well studied area. However, given the difference in what\nconstitutes a mobile interface, and the usage context of these devices, we\npostulate that saliency prediction for mobile interface images requires a fresh\napproach. Mobile interface design involves operating on elements, the building\nblocks of the interface. We first collected eye-gaze data from mobile devices\nfor free viewing task. Using this data, we develop a novel autoencoder based\nmulti-scale deep learning model that provides saliency prediction at the mobile\ninterface element level. Compared to saliency prediction approaches developed\nfor natural images, we show that our approach performs significantly better on\na range of established metrics.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 08:38:44 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 01:58:19 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 05:26:59 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Gupta", "Prakhar", ""], ["Gupta", "Shubh", ""], ["Jayagopal", "Ajaykrishnan", ""], ["Pal", "Sourav", ""], ["Sinha", "Ritwik", ""]]}, {"id": "1711.03752", "submitter": "Gabriel Navarro", "authors": "F. J. Lobillo and Luis Merino and Gabriel Navarro and Evangelina\n  Santos", "title": "Lattice embeddings between types of fuzzy sets. Closed-valued fuzzy sets", "comments": null, "journal-ref": null, "doi": "10.1016/j.fss.2018.04.014", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deal with the problem of extending Zadeh's operators on\nfuzzy sets (FSs) to interval-valued (IVFSs), set-valued (SVFSs) and type-2\n(T2FSs) fuzzy sets. Namely, it is known that seeing FSs as SVFSs, or T2FSs,\nwhose membership degrees are singletons is not order-preserving. We then\ndescribe a family of lattice embeddings from FSs to SVFSs. Alternatively, if\nthe former singleton viewpoint is required, we reformulate the intersection on\nhesitant fuzzy sets and introduce what we have called closed-valued fuzzy sets.\nThis new type of fuzzy sets extends standard union and intersection on FSs. In\naddition, it allows handling together membership degrees of different nature\nas, for instance, closed intervals and finite sets. Finally, all these\nconstructions are viewed as T2FSs forming a chain of lattices.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 10:06:51 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Lobillo", "F. J.", ""], ["Merino", "Luis", ""], ["Navarro", "Gabriel", ""], ["Santos", "Evangelina", ""]]}, {"id": "1711.03817", "submitter": "Anna Harutyunyan", "authors": "Anna Harutyunyan, Peter Vrancx, Pierre-Luc Bacon, Doina Precup, Ann\n  Nowe", "title": "Learning with Options that Terminate Off-Policy", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A temporally abstract action, or an option, is specified by a policy and a\ntermination condition: the policy guides option behavior, and the termination\ncondition roughly determines its length. Generally, learning with longer\noptions (like learning with multi-step returns) is known to be more efficient.\nHowever, if the option set for the task is not ideal, and cannot express the\nprimitive optimal policy exactly, shorter options offer more flexibility and\ncan yield a better solution. Thus, the termination condition puts learning\nefficiency at odds with solution quality. We propose to resolve this dilemma by\ndecoupling the behavior and target terminations, just like it is done with\npolicies in off-policy learning. To this end, we give a new algorithm,\nQ(\\beta), that learns the solution with respect to any termination condition,\nregardless of how the options actually terminate. We derive Q(\\beta) by casting\nlearning with options into a common framework with well-studied multi-step\noff-policy learning. We validate our algorithm empirically, and show that it\nholds up to its motivating claims.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 13:49:47 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 12:57:35 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Harutyunyan", "Anna", ""], ["Vrancx", "Peter", ""], ["Bacon", "Pierre-Luc", ""], ["Precup", "Doina", ""], ["Nowe", "Ann", ""]]}, {"id": "1711.03846", "submitter": "Brett Israelsen", "authors": "Brett W Israelsen, Nisar R Ahmed", "title": "\"Dave...I can assure you...that it's going to be all right...\" -- A\n  definition, case for, and survey of algorithmic assurances in human-autonomy\n  trust relationships", "comments": "final version of accepted manuscript", "journal-ref": null, "doi": "10.1145/3267338", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People who design, use, and are affected by autonomous artificially\nintelligent agents want to be able to \\emph{trust} such agents -- that is, to\nknow that these agents will perform correctly, to understand the reasoning\nbehind their actions, and to know how to use them appropriately. Many\ntechniques have been devised to assess and influence human trust in\nartificially intelligent agents. However, these approaches are typically ad\nhoc, and have not been formally related to each other or to formal trust\nmodels. This paper presents a survey of \\emph{algorithmic assurances}, i.e.\nprogrammed components of agent operation that are expressly designed to\ncalibrate user trust in artificially intelligent agents. Algorithmic assurances\nare first formally defined and classified from the perspective of formally\nmodeled human-artificially intelligent agent trust relationships. Building on\nthese definitions, a synthesis of research across communities such as machine\nlearning, human-computer interaction, robotics, e-commerce, and others reveals\nthat assurance algorithms naturally fall along a spectrum in terms of their\nimpact on an agent's core functionality, with seven notable classes ranging\nfrom integral assurances (which impact an agent's core functionality) to\nsupplemental assurances (which have no direct effect on agent performance).\nCommon approaches within each of these classes are identified and discussed;\nbenefits and drawbacks of different approaches are also investigated.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 19:00:29 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 17:38:47 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 19:03:43 GMT"}, {"version": "v4", "created": "Tue, 28 Aug 2018 17:07:30 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Israelsen", "Brett W", ""], ["Ahmed", "Nisar R", ""]]}, {"id": "1711.03892", "submitter": "Tomas Teijeiro", "authors": "Tom\\'as Teijeiro, Constantino A. Garc\\'ia, Daniel Castro, Paulo\n  F\\'elix", "title": "Arrhythmia Classification from the Abductive Interpretation of Short\n  Single-Lead ECG Records", "comments": "4 pages, 3 figures. Presented in the Computing in Cardiology 2017\n  conference", "journal-ref": null, "doi": "10.22489/CinC.2017.166-054", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new method for the rhythm classification of short\nsingle-lead ECG records, using a set of high-level and clinically meaningful\nfeatures provided by the abductive interpretation of the records. These\nfeatures include morphological and rhythm-related features that are used to\nbuild two classifiers: one that evaluates the record globally, using aggregated\nvalues for each feature; and another one that evaluates the record as a\nsequence, using a Recurrent Neural Network fed with the individual features for\neach detected heartbeat. The two classifiers are finally combined using the\nstacking technique, providing an answer by means of four target classes: Normal\nsinus rhythm, Atrial fibrillation, Other anomaly, and Noisy. The approach has\nbeen validated against the 2017 Physionet/CinC Challenge dataset, obtaining a\nfinal score of 0.83 and ranking first in the competition.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 15:47:21 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Teijeiro", "Tom\u00e1s", ""], ["Garc\u00eda", "Constantino A.", ""], ["Castro", "Daniel", ""], ["F\u00e9lix", "Paulo", ""]]}, {"id": "1711.03902", "submitter": "Tarek Richard Besold", "authors": "Tarek R. Besold, Artur d'Avila Garcez, Sebastian Bader, Howard Bowman,\n  Pedro Domingos, Pascal Hitzler, Kai-Uwe Kuehnberger, Luis C. Lamb, Daniel\n  Lowd, Priscila Machado Vieira Lima, Leo de Penning, Gadi Pinkas, Hoifung\n  Poon, Gerson Zaverucha", "title": "Neural-Symbolic Learning and Reasoning: A Survey and Interpretation", "comments": "58 pages, work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study and understanding of human behaviour is relevant to computer\nscience, artificial intelligence, neural computation, cognitive science,\nphilosophy, psychology, and several other areas. Presupposing cognition as\nbasis of behaviour, among the most prominent tools in the modelling of\nbehaviour are computational-logic systems, connectionist models of cognition,\nand models of uncertainty. Recent studies in cognitive science, artificial\nintelligence, and psychology have produced a number of cognitive models of\nreasoning, learning, and language that are underpinned by computation. In\naddition, efforts in computer science research have led to the development of\ncognitive computational systems integrating machine learning and automated\nreasoning. Such systems have shown promise in a range of applications,\nincluding computational biology, fault diagnosis, training and assessment in\nsimulators, and software verification. This joint survey reviews the personal\nideas and views of several researchers on neural-symbolic learning and\nreasoning. The article is organised in three parts: Firstly, we frame the scope\nand goals of neural-symbolic computation and have a look at the theoretical\nfoundations. We then proceed to describe the realisations of neural-symbolic\ncomputation, systems, and applications. Finally we present the challenges\nfacing the area and avenues for further research.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 16:14:22 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Besold", "Tarek R.", ""], ["Garcez", "Artur d'Avila", ""], ["Bader", "Sebastian", ""], ["Bowman", "Howard", ""], ["Domingos", "Pedro", ""], ["Hitzler", "Pascal", ""], ["Kuehnberger", "Kai-Uwe", ""], ["Lamb", "Luis C.", ""], ["Lowd", "Daniel", ""], ["Lima", "Priscila Machado Vieira", ""], ["de Penning", "Leo", ""], ["Pinkas", "Gadi", ""], ["Poon", "Hoifung", ""], ["Zaverucha", "Gerson", ""]]}, {"id": "1711.03938", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez,\n  Vladlen Koltun", "title": "CARLA: An Open Urban Driving Simulator", "comments": "Published at the 1st Conference on Robot Learning (CoRL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce CARLA, an open-source simulator for autonomous driving research.\nCARLA has been developed from the ground up to support development, training,\nand validation of autonomous urban driving systems. In addition to open-source\ncode and protocols, CARLA provides open digital assets (urban layouts,\nbuildings, vehicles) that were created for this purpose and can be used freely.\nThe simulation platform supports flexible specification of sensor suites and\nenvironmental conditions. We use CARLA to study the performance of three\napproaches to autonomous driving: a classic modular pipeline, an end-to-end\nmodel trained via imitation learning, and an end-to-end model trained via\nreinforcement learning. The approaches are evaluated in controlled scenarios of\nincreasing difficulty, and their performance is examined via metrics provided\nby CARLA, illustrating the platform's utility for autonomous driving research.\nThe supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 17:54:40 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Ros", "German", ""], ["Codevilla", "Felipe", ""], ["Lopez", "Antonio", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1711.03987", "submitter": "Pan Hu", "authors": "Pan Hu, Boris Motik, Ian Horrocks", "title": "Optimised Maintenance of Datalog Materialisations", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To efficiently answer queries, datalog systems often materialise all\nconsequences of a datalog program, so the materialisation must be updated\nwhenever the input facts change. Several solutions to the materialisation\nupdate problem have been proposed. The Delete/Rederive (DRed) and the\nBackward/Forward (B/F) algorithms solve this problem for general datalog, but\nboth contain steps that evaluate rules 'backwards' by matching their heads to a\nfact and evaluating the partially instantiated rule bodies as queries. We show\nthat this can be a considerable source of overhead even on very small updates.\nIn contrast, the Counting algorithm does not evaluate the rules 'backwards',\nbut it can handle only nonrecursive rules. We present two hybrid approaches\nthat combine DRed and B/F with Counting so as to reduce or even eliminate\n'backward' rule evaluation while still handling arbitrary datalog programs. We\nshow empirically that our hybrid algorithms are usually significantly faster\nthan existing approaches, sometimes by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 19:10:20 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 14:06:40 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Hu", "Pan", ""], ["Motik", "Boris", ""], ["Horrocks", "Ian", ""]]}, {"id": "1711.04013", "submitter": "Alessandro Ronca", "authors": "Alessandro Ronca, Mark Kaminski, Bernardo Cuenca Grau, Boris Motik,\n  Ian Horrocks", "title": "Stream Reasoning in Temporal Datalog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been an increasing interest in extending\ntraditional stream processing engines with logical, rule-based, reasoning\ncapabilities. This poses significant theoretical and practical challenges since\nrules can derive new information and propagate it both towards past and future\ntime points; as a result, streamed query answers can depend on data that has\nnot yet been received, as well as on data that arrived far in the past. Stream\nreasoning algorithms, however, must be able to stream out query answers as soon\nas possible, and can only keep a limited number of previous input facts in\nmemory. In this paper, we propose novel reasoning problems to deal with these\nchallenges, and study their computational properties on Datalog extended with a\ntemporal sort and the successor function (a core rule-based language for stream\nreasoning applications).\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 21:11:17 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 11:06:52 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Ronca", "Alessandro", ""], ["Kaminski", "Mark", ""], ["Grau", "Bernardo Cuenca", ""], ["Motik", "Boris", ""], ["Horrocks", "Ian", ""]]}, {"id": "1711.04022", "submitter": "Hamid Eghbal-zadeh", "authors": "Hamid Eghbal-zadeh, Matthias Dorfer and Gerhard Widmer", "title": "Deep Within-Class Covariance Analysis for Robust Audio Representation\n  Learning", "comments": "11 pages, 3 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) can learn effective features, though\nhave been shown to suffer from a performance drop when the distribution of the\ndata changes from training to test data. In this paper we analyze the internal\nrepresentations of CNNs and observe that the representations of unseen data in\neach class, spread more (with higher variance) in the embedding space of the\nCNN compared to representations of the training data. More importantly, this\ndifference is more extreme if the unseen data comes from a shifted\ndistribution. Based on this observation, we objectively evaluate the degree of\nrepresentation's variance in each class via eigenvalue decomposition on the\nwithin-class covariance of the internal representations of CNNs and observe the\nsame behaviour. This can be problematic as larger variances might lead to\nmis-classification if the sample crosses the decision boundary of its class. We\napply nearest neighbor classification on the representations and empirically\nshow that the embeddings with the high variance actually have significantly\nworse KNN classification performances, although this could not be foreseen from\ntheir end-to-end classification results. To tackle this problem, we propose\nDeep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that\nsignificantly reduces the within-class covariance of a DNN's representation,\nimproving performance on unseen test data from a shifted distribution. We\nempirically evaluate DWCCA on two datasets for Acoustic Scene Classification\n(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA\nsignificantly improve the network's internal representation, it also increases\nthe end-to-end classification accuracy, especially when the test set exhibits a\ndistribution shift. By adding DWCCA to a VGG network, we achieve around 6\npercentage points improvement in the case of a distribution mismatch.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 21:39:12 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 09:48:48 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Eghbal-zadeh", "Hamid", ""], ["Dorfer", "Matthias", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1711.04036", "submitter": "Daniel Lopez-Martinez", "authors": "Daniel Lopez-Martinez, Ognjen Rudovic, Rosalind Picard", "title": "Physiological and behavioral profiling for nociceptive pain estimation\n  using personalized multitask learning", "comments": "NIPS Machine Learning for Health 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pain is a subjective experience commonly measured through patient's self\nreport. While there exist numerous situations in which automatic pain\nestimation methods may be preferred, inter-subject variability in physiological\nand behavioral pain responses has hindered the development of such methods. In\nthis work, we address this problem by introducing a novel personalized\nmultitask machine learning method for pain estimation based on individual\nphysiological and behavioral pain response profiles, and show its advantages in\na dataset containing multimodal responses to nociceptive heat pain.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 22:36:27 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Lopez-Martinez", "Daniel", ""], ["Rudovic", "Ognjen", ""], ["Picard", "Rosalind", ""]]}, {"id": "1711.04071", "submitter": "Liwei Cai", "authors": "Liwei Cai, William Yang Wang", "title": "KBGAN: Adversarial Learning for Knowledge Graph Embeddings", "comments": "To appear at NAACL HLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce KBGAN, an adversarial learning framework to improve the\nperformances of a wide range of existing knowledge graph embedding models.\nBecause knowledge graphs typically only contain positive facts, sampling useful\nnegative training examples is a non-trivial task. Replacing the head or tail\nentity of a fact with a uniformly randomly selected entity is a conventional\nmethod for generating negative facts, but the majority of the generated\nnegative facts can be easily discriminated from positive facts, and will\ncontribute little towards the training. Inspired by generative adversarial\nnetworks (GANs), we use one knowledge graph embedding model as a negative\nsample generator to assist the training of our desired model, which acts as the\ndiscriminator in GANs. This framework is independent of the concrete form of\ngenerator and discriminator, and therefore can utilize a wide variety of\nknowledge graph embedding models as its building blocks. In experiments, we\nadversarially train two translation-based models, TransE and TransD, each with\nassistance from one of the two probability-based models, DistMult and ComplEx.\nWe evaluate the performances of KBGAN on the link prediction task, using three\nknowledge base completion datasets: FB15k-237, WN18 and WN18RR. Experimental\nresults show that adversarial training substantially improves the performances\nof target embedding models under various settings.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 03:46:53 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 01:31:45 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 14:36:17 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Cai", "Liwei", ""], ["Wang", "William Yang", ""]]}, {"id": "1711.04076", "submitter": "Saeid Tizpaz-Niari", "authors": "Saeid Tizpaz-Niari, Pavol Cerny, Bor-Yuh Evan Chang, Ashutosh Trivedi", "title": "Differential Performance Debugging with Discriminant Regression Trees", "comments": "To Appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential performance debugging is a technique to find performance\nproblems. It applies in situations where the performance of a program is\n(unexpectedly) different for different classes of inputs. The task is to\nexplain the differences in asymptotic performance among various input classes\nin terms of program internals. We propose a data-driven technique based on\ndiscriminant regression tree (DRT) learning problem where the goal is to\ndiscriminate among different classes of inputs. We propose a new algorithm for\nDRT learning that first clusters the data into functional clusters, capturing\ndifferent asymptotic performance classes, and then invokes off-the-shelf\ndecision tree learning algorithms to explain these clusters. We focus on linear\nfunctional clusters and adapt classical clustering algorithms (K-means and\nspectral) to produce them. For the K-means algorithm, we generalize the notion\nof the cluster centroid from a point to a linear function. We adapt spectral\nclustering by defining a novel kernel function to capture the notion of linear\nsimilarity between two data points. We evaluate our approach on benchmarks\nconsisting of Java programs where we are interested in debugging performance.\nWe show that our algorithm significantly outperforms other well-known\nregression tree learning algorithms in terms of running time and accuracy of\nclassification.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 04:50:53 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 05:19:48 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Tizpaz-Niari", "Saeid", ""], ["Cerny", "Pavol", ""], ["Chang", "Bor-Yuh Evan", ""], ["Trivedi", "Ashutosh", ""]]}, {"id": "1711.04078", "submitter": "Baihan Lin", "authors": "Avinash Bukkittu, Baihan Lin, Trung Vu, Itsik Pe'er", "title": "Parkinson's Disease Digital Biomarker Discovery with Optimized\n  Transitions and Inferred Markov Emissions", "comments": "10th RECOMB/ISCB Conference on Regulatory & Systems Genomics with\n  DREAM Challenges", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We search for digital biomarkers from Parkinson's Disease by observing\napproximate repetitive patterns matching hypothesized step and stride periodic\ncycles. These observations were modeled as a cycle of hidden states with\nrandomness allowing deviation from a canonical pattern of transitions and\nemissions, under the hypothesis that the averaged features of hidden states\nwould serve to informatively characterize classes of patients/controls. We\npropose a Hidden Semi-Markov Model (HSMM), a latent-state model, emitting\n3D-acceleration vectors. Transitions and emissions are inferred from data. We\nfit separate models per unique device and training label. Hidden Markov Models\n(HMM) force geometric distributions of the duration spent at each state before\ntransition to a new state. Instead, our HSMM allows us to specify the\ndistribution of state duration. This modified version is more effective because\nwe are interested more in each state's duration than the sequence of distinct\nstates, allowing inclusion of these durations the feature vector.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 05:06:20 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Bukkittu", "Avinash", ""], ["Lin", "Baihan", ""], ["Vu", "Trung", ""], ["Pe'er", "Itsik", ""]]}, {"id": "1711.04079", "submitter": "Kaixiang Mo", "authors": "Kaixiang Mo, Yu Zhang, Qiang Yang, Pascale Fung", "title": "Fine Grained Knowledge Transfer for Personalized Task-oriented Dialogue\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a personalized dialogue system requires a lot of data, and the data\ncollected for a single user is usually insufficient. One common practice for\nthis problem is to share training dialogues between different users and train\nmultiple sequence-to-sequence dialogue models together with transfer learning.\nHowever, current sequence-to-sequence transfer learning models operate on the\nentire sentence, which might cause negative transfer if different personal\ninformation from different users is mixed up. We propose a personalized decoder\nmodel to transfer finer granularity phrase-level knowledge between different\nusers while keeping personal preferences of each user intact. A novel personal\ncontrol gate is introduced, enabling the personalized decoder to switch between\ngenerating personalized phrases and shared phrases. The proposed personalized\ndecoder model can be easily combined with various deep models and can be\ntrained with reinforcement learning. Real-world experimental results\ndemonstrate that the phrase-level personalized decoder improves the BLEU over\nmultiple sentence-level transfer baseline models by as much as 7.5%.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 05:14:02 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mo", "Kaixiang", ""], ["Zhang", "Yu", ""], ["Yang", "Qiang", ""], ["Fung", "Pascale", ""]]}, {"id": "1711.04090", "submitter": "Xianda Zhou", "authors": "Xianda Zhou, William Yang Wang", "title": "MojiTalk: Generating Emotional Responses at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating emotional language is a key step towards building empathetic\nnatural language processing agents. However, a major challenge for this line of\nresearch is the lack of large-scale labeled training data, and previous studies\nare limited to only small sets of human annotated sentiment labels.\nAdditionally, explicitly controlling the emotion and sentiment of generated\ntext is also difficult. In this paper, we take a more radical approach: we\nexploit the idea of leveraging Twitter data that are naturally labeled with\nemojis. More specifically, we collect a large corpus of Twitter conversations\nthat include emojis in the response, and assume the emojis convey the\nunderlying emotions of the sentence. We then introduce a reinforced conditional\nvariational encoder approach to train a deep generative model on these\nconversations, which allows us to use emojis to control the emotion of the\ngenerated text. Experimentally, we show in our quantitative and qualitative\nanalyses that the proposed models can successfully generate high-quality\nabstractive conversation responses in accordance with designated emotions.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 07:20:51 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 05:10:42 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Zhou", "Xianda", ""], ["Wang", "William Yang", ""]]}, {"id": "1711.04101", "submitter": "Laknath Semage", "authors": "Laknath Semage", "title": "Recommender Systems with Random Walks: A Survey", "comments": "15 pages, a survey paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recommender engines have become an integral component in today's e-commerce\nsystems. From recommending books in Amazon to finding friends in social\nnetworks such as Facebook, they have become omnipresent.\n  Generally, recommender systems can be classified into two main categories:\ncontent based and collaborative filtering based models. Both these models build\nrelationships between users and items to provide recommendations. Content based\nsystems achieve this task by utilizing features extracted from the context\navailable, whereas collaborative systems use shared interests between user-item\nsubsets.\n  There is another relatively unexplored approach for providing recommendations\nthat utilizes a stochastic process named random walks. This study is a survey\nexploring use cases of random walks in recommender systems and an attempt at\nclassifying them.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 08:43:06 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Semage", "Laknath", ""]]}, {"id": "1711.04203", "submitter": "Robert Mok", "authors": "Nikolaus Kriegeskorte and Robert M. Mok", "title": "Building machines that adapt and compute like brains", "comments": "Commentary on: Lake BM, Ullman TD, Tenenbaum JB, Gershman SJ. (2017)\n  Building machines that learn and think like people. Behavioral and Brain\n  Sciences, 40", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building machines that learn and think like humans is essential not only for\ncognitive science, but also for computational neuroscience, whose ultimate goal\nis to understand how cognition is implemented in biological brains. A new\ncognitive computational neuroscience should build cognitive-level and neural-\nlevel models, understand their relationships, and test both types of models\nwith both brain and behavioral data.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 22:02:52 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kriegeskorte", "Nikolaus", ""], ["Mok", "Robert M.", ""]]}, {"id": "1711.04204", "submitter": "Frank F. Xu", "authors": "Frank F. Xu, Bill Yuchen Lin, Kenny Q. Zhu", "title": "Automatic Extraction of Commonsense LocatedNear Knowledge", "comments": "Accepted by ACL 2018. A preliminary version is presented on\n  AKBC@NIPS'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LocatedNear relation is a kind of commonsense knowledge describing two\nphysical objects that are typically found near each other in real life. In this\npaper, we study how to automatically extract such relationship through a\nsentence-level relation classifier and aggregating the scores of entity pairs\nfrom a large corpus. Also, we release two benchmark datasets for evaluation and\nfuture research.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 22:25:55 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 01:39:18 GMT"}, {"version": "v3", "created": "Sun, 13 May 2018 00:01:04 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Xu", "Frank F.", ""], ["Lin", "Bill Yuchen", ""], ["Zhu", "Kenny Q.", ""]]}, {"id": "1711.04258", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng, Zenglin Xu", "title": "Unified Spectral Clustering with Optimal Graph", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering has found extensive use in many areas. Most traditional\nspectral clustering algorithms work in three separate steps: similarity graph\nconstruction; continuous labels learning; discretizing the learned labels by\nk-means clustering. Such common practice has two potential flaws, which may\nlead to severe information loss and performance degradation. First, predefined\nsimilarity graph might not be optimal for subsequent clustering. It is\nwell-accepted that similarity graph highly affects the clustering results. To\nthis end, we propose to automatically learn similarity information from data\nand simultaneously consider the constraint that the similarity matrix has exact\nc connected components if there are c clusters. Second, the discrete solution\nmay deviate from the spectral solution since k-means method is well-known as\nsensitive to the initialization of cluster centers. In this work, we transform\nthe candidate solution into a new one that better approximates the discrete\none. Finally, those three subtasks are integrated into a unified framework,\nwith each subtask iteratively boosted by using the results of the others\ntowards an overall optimal solution. It is known that the performance of a\nkernel method is largely determined by the choice of kernels. To tackle this\npractical problem of how to select the most suitable kernel for a particular\ndata set, we further extend our model to incorporate multiple kernel learning\nability. Extensive experiments demonstrate the superiority of our proposed\nmethod as compared to existing clustering approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 09:20:25 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""], ["Xu", "Zenglin", ""]]}, {"id": "1711.04259", "submitter": "Francesco Leofante", "authors": "Francesco Leofante, Erika \\'Abrah\\'am, Tim Niemueller, Gerhard\n  Lakemeyer, Armando Tacchella", "title": "On the Synthesis of Guaranteed-Quality Plans for Robot Fleets in\n  Logistics Scenarios via Optimization Modulo Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In manufacturing, the increasing involvement of autonomous robots in\nproduction processes poses new challenges on the production management. In this\npaper we report on the usage of Optimization Modulo Theories (OMT) to solve\ncertain multi-robot scheduling problems in this area. Whereas currently\nexisting methods are heuristic, our approach guarantees optimality for the\ncomputed solution. We do not only present our final method but also its\nchronological development, and draw some general observations for the\ndevelopment of OMT-based approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 09:31:25 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Leofante", "Francesco", ""], ["\u00c1brah\u00e1m", "Erika", ""], ["Niemueller", "Tim", ""], ["Lakemeyer", "Gerhard", ""], ["Tacchella", "Armando", ""]]}, {"id": "1711.04309", "submitter": "Joshua Gans", "authors": "Joshua S. Gans", "title": "Self-Regulating Artificial General Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we examine the paperclip apocalypse concern for artificial general\nintelligence (or AGI) whereby a superintelligent AI with a simple goal (ie.,\nproducing paperclips) accumulates power so that all resources are devoted\ntowards that simple goal and are unavailable for any other use. We provide\nconditions under which a paper apocalypse can arise but also show that, under\ncertain architectures for recursive self-improvement of AIs, that a paperclip\nAI may refrain from allowing power capabilities to be developed. The reason is\nthat such developments pose the same control problem for the AI as they do for\nhumans (over AIs) and hence, threaten to deprive it of resources for its\nprimary goal.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 15:19:56 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 21:00:42 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Gans", "Joshua S.", ""]]}, {"id": "1711.04323", "submitter": "Idan Schwartz", "authors": "Idan Schwartz, Alexander G. Schwing, Tamir Hazan", "title": "High-Order Attention Models for Visual Question Answering", "comments": "9 pages, 8 figures, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for algorithms that enable cognitive abilities is an important part\nof machine learning. A common trait in many recently investigated\ncognitive-like tasks is that they take into account different data modalities,\nsuch as visual and textual input. In this paper we propose a novel and\ngenerally applicable form of attention mechanism that learns high-order\ncorrelations between various data modalities. We show that high-order\ncorrelations effectively direct the appropriate attention to the relevant\nelements in the different data modalities that are required to solve the joint\ntask. We demonstrate the effectiveness of our high-order attention mechanism on\nthe task of visual question answering (VQA), where we achieve state-of-the-art\nperformance on the standard VQA dataset.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 17:30:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Schwartz", "Idan", ""], ["Schwing", "Alexander G.", ""], ["Hazan", "Tamir", ""]]}, {"id": "1711.04329", "submitter": "Shiyue Zhang", "authors": "Shiyue Zhang, Pengtao Xie, Dong Wang, Eric P. Xing", "title": "Medical Diagnosis From Laboratory Tests by Combining Generative and\n  Discriminative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A primary goal of computational phenotype research is to conduct medical\ndiagnosis. In hospital, physicians rely on massive clinical data to make\ndiagnosis decisions, among which laboratory tests are one of the most important\nresources. However, the longitudinal and incomplete nature of laboratory test\ndata casts a significant challenge on its interpretation and usage, which may\nresult in harmful decisions by both human physicians and automatic diagnosis\nsystems. In this work, we take advantage of deep generative models to deal with\nthe complex laboratory tests. Specifically, we propose an end-to-end\narchitecture that involves a deep generative variational recurrent neural\nnetworks (VRNN) to learn robust and generalizable features, and a\ndiscriminative neural network (NN) model to learn diagnosis decision making,\nand the two models are trained jointly. Our experiments are conducted on a\ndataset involving 46,252 patients, and the 50 most frequent tests are used to\npredict the 50 most common diagnoses. The results show that our model, VRNN+NN,\nsignificantly (p<0.001) outperforms other baseline models. Moreover, we\ndemonstrate that the representations learned by the joint training are more\ninformative than those learned by pure generative models. Finally, we find that\nour model offers a surprisingly good imputation for missing values.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 17:58:42 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 21:40:58 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Zhang", "Shiyue", ""], ["Xie", "Pengtao", ""], ["Wang", "Dong", ""], ["Xing", "Eric P.", ""]]}, {"id": "1711.04436", "submitter": "Xiaojun Xu", "authors": "Xiaojun Xu, Chang Liu, Dawn Song", "title": "SQLNet: Generating Structured Queries From Natural Language Without\n  Reinforcement Learning", "comments": "Submitting to ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing SQL queries from natural language is a long-standing open\nproblem and has been attracting considerable interest recently. Toward solving\nthe problem, the de facto approach is to employ a sequence-to-sequence-style\nmodel. Such an approach will necessarily require the SQL queries to be\nserialized. Since the same SQL query may have multiple equivalent\nserializations, training a sequence-to-sequence-style model is sensitive to the\nchoice from one of them. This phenomenon is documented as the \"order-matters\"\nproblem. Existing state-of-the-art approaches rely on reinforcement learning to\nreward the decoder when it generates any of the equivalent serializations.\nHowever, we observe that the improvement from reinforcement learning is\nlimited.\n  In this paper, we propose a novel approach, i.e., SQLNet, to fundamentally\nsolve this problem by avoiding the sequence-to-sequence structure when the\norder does not matter. In particular, we employ a sketch-based approach where\nthe sketch contains a dependency graph so that one prediction can be done by\ntaking into consideration only the previous predictions that it depends on. In\naddition, we propose a sequence-to-set model as well as the column attention\nmechanism to synthesize the query based on the sketch. By combining all these\nnovel techniques, we show that SQLNet can outperform the prior art by 9% to 13%\non the WikiSQL task.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 06:41:29 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Xu", "Xiaojun", ""], ["Liu", "Chang", ""], ["Song", "Dawn", ""]]}, {"id": "1711.04438", "submitter": "Zongyi Li", "authors": "Brendan Juba, Zongyi Li, Evan Miller", "title": "Learning Abduction under Partial Observability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Juba recently proposed a formulation of learning abductive reasoning from\nexamples, in which both the relative plausibility of various explanations, as\nwell as which explanations are valid, are learned directly from data. The main\nshortcoming of this formulation of the task is that it assumes access to\nfull-information (i.e., fully specified) examples; relatedly, it offers no role\nfor declarative background knowledge, as such knowledge is rendered redundant\nin the abduction task by complete information. In this work, we extend the\nformulation to utilize such partially specified examples, along with\ndeclarative background knowledge about the missing data. We show that it is\npossible to use implicitly learned rules together with the explicitly given\ndeclarative knowledge to support hypotheses in the course of abduction. We\nobserve that when a small explanation exists, it is possible to obtain a\nmuch-improved guarantee in the challenging exception-tolerant setting. Such\nsmall, human-understandable explanations are of particular interest for\npotential applications of the task.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 06:51:40 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 22:35:49 GMT"}, {"version": "v3", "created": "Sat, 25 Nov 2017 00:21:16 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Juba", "Brendan", ""], ["Li", "Zongyi", ""], ["Miller", "Evan", ""]]}, {"id": "1711.04498", "submitter": "Yong Zhang", "authors": "Yong Zhang, Hongming Zhou, Nganmeng Tan, Saeed Bagheri, Meng Joo Er", "title": "Targeted Advertising Based on Browsing History", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Audience interest, demography, purchase behavior and other possible\nclassifications are ex- tremely important factors to be carefully studied in a\ntargeting campaign. This information can help advertisers and publishers\ndeliver advertisements to the right audience group. How- ever, it is not easy\nto collect such information, especially for the online audience with whom we\nhave limited interaction and minimum deterministic knowledge. In this paper, we\npro- pose a predictive framework that can estimate online audience demographic\nattributes based on their browsing histories. Under the proposed framework,\nfirst, we retrieve the content of the websites visited by audience, and\nrepresent the content as website feature vectors; second, we aggregate the\nvectors of websites that audience have visited and arrive at feature vectors\nrepresenting the users; finally, the support vector machine is exploited to\npredict the audience demographic attributes. The key to achieving good\nprediction performance is preparing representative features of the audience.\nWord Embedding, a widely used tech- nique in natural language processing tasks,\ntogether with term frequency-inverse document frequency weighting scheme is\nused in the proposed method. This new representation ap- proach is unsupervised\nand very easy to implement. The experimental results demonstrate that the new\naudience feature representation method is more powerful than existing baseline\nmethods, leading to a great improvement in prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 10:06:22 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Zhang", "Yong", ""], ["Zhou", "Hongming", ""], ["Tan", "Nganmeng", ""], ["Bagheri", "Saeed", ""], ["Er", "Meng Joo", ""]]}, {"id": "1711.04518", "submitter": "Marius St\\\"ark", "authors": "Marius St\\\"ark, Damian Backes, Christian Kehl", "title": "A Supervised Learning Concept for Reducing User Interaction in Passenger\n  Cars", "comments": "4 pages, 9 figures, concept only", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article an automation system for human-machine-interfaces (HMI) for\nsetpoint adjustment using supervised learning is presented. We use HMIs of\nmulti-modal thermal conditioning systems in passenger cars as example for a\ncomplex setpoint selection system. The goal is the reduction of interaction\ncomplexity up to full automation. The approach is not limited to climate\ncontrol applications but can be extended to other setpoint-based HMIs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 10:58:58 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["St\u00e4rk", "Marius", ""], ["Backes", "Damian", ""], ["Kehl", "Christian", ""]]}, {"id": "1711.04528", "submitter": "Thomas Elsken", "authors": "Thomas Elsken, Jan-Hendrik Metzen, Frank Hutter", "title": "Simple And Efficient Architecture Search for Convolutional Neural\n  Networks", "comments": "Under review as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have recently had a lot of success for many tasks. However,\nneural network architectures that perform well are still typically designed\nmanually by experts in a cumbersome trial-and-error process. We propose a new\nmethod to automatically search for well-performing CNN architectures based on a\nsimple hill climbing procedure whose operators apply network morphisms,\nfollowed by short optimization runs by cosine annealing. Surprisingly, this\nsimple method yields competitive results, despite only requiring resources in\nthe same order of magnitude as training a single network. E.g., on CIFAR-10,\nour method designs and trains networks with an error rate below 6% in only 12\nhours on a single GPU; training for one day reduces this error further, to\nalmost 5%.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 11:23:36 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Elsken", "Thomas", ""], ["Metzen", "Jan-Hendrik", ""], ["Hutter", "Frank", ""]]}, {"id": "1711.04556", "submitter": "Libor Bukata", "authors": "Libor Bukata, Premysl Sucha, Zdenek Hanzalek", "title": "Solving the Resource Constrained Project Scheduling Problem Using the\n  Parallel Tabu Search Designed for the CUDA Platform", "comments": "Published in Journal of Parallel and Distributed Computing", "journal-ref": "Journal of Parallel and Distributed Computing, 77 (2015), 58-68", "doi": "10.1016/j.jpdc.2014.11.005", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, a parallel Tabu Search algorithm for the Resource Constrained\nProject Scheduling Problem is proposed. To deal with this NP-hard combinatorial\nproblem many optimizations have been performed. For example, a resource\nevaluation algorithm is selected by a heuristic and an effective Tabu List was\ndesigned. In addition to that, a capacity-indexed resource evaluation algorithm\nwas proposed and the GPU (Graphics Processing Unit) version uses a homogeneous\nmodel to reduce the required communication bandwidth. According to the\nexperiments, the GPU version outperforms the optimized parallel CPU version\nwith respect to the computational time and the quality of solutions. In\ncomparison with other existing heuristics, the proposed solution often gives\nbetter quality solutions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 12:52:49 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Bukata", "Libor", ""], ["Sucha", "Premysl", ""], ["Hanzalek", "Zdenek", ""]]}, {"id": "1711.04564", "submitter": "Markus M\\\"uller", "authors": "Markus M\\\"uller, Sebastian St\\\"uker, Alex Waibel", "title": "Phonemic and Graphemic Multilingual CTC Based Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training automatic speech recognition (ASR) systems requires large amounts of\ndata in the target language in order to achieve good performance. Whereas large\ntraining corpora are readily available for languages like English, there exists\na long tail of languages which do suffer from a lack of resources. One method\nto handle data sparsity is to use data from additional source languages and\nbuild a multilingual system. Recently, ASR systems based on recurrent neural\nnetworks (RNNs) trained with connectionist temporal classification (CTC) have\ngained substantial research interest. In this work, we extended our previous\napproach towards training CTC-based systems multilingually. Our systems feature\na global phone set, based on the joint phone sets of each source language. We\nevaluated the use of different language combinations as well as the addition of\nLanguage Feature Vectors (LFVs). As contrastive experiment, we built systems\nbased on graphemes as well. Systems having a multilingual phone set are known\nto suffer in performance compared to their monolingual counterparts. With our\nproposed approach, we could reduce the gap between these mono- and multilingual\nsetups, using either graphemes or phonemes.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 13:13:40 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["M\u00fcller", "Markus", ""], ["St\u00fcker", "Sebastian", ""], ["Waibel", "Alex", ""]]}, {"id": "1711.04569", "submitter": "Markus M\\\"uller", "authors": "Markus M\\\"uller, Sebastian St\\\"uker, Alex Waibel", "title": "Multilingual Adaptation of RNN Based ASR Systems", "comments": "5 pages, 1 figure, to appear in 2018 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on multilingual systems based on recurrent neural\nnetworks (RNNs), trained using the Connectionist Temporal Classification (CTC)\nloss function. Using a multilingual set of acoustic units poses difficulties.\nTo address this issue, we proposed Language Feature Vectors (LFVs) to train\nlanguage adaptive multilingual systems. Language adaptation, in contrast to\nspeaker adaptation, needs to be applied not only on the feature level, but also\nto deeper layers of the network. In this work, we therefore extended our\nprevious approach by introducing a novel technique which we call \"modulation\".\nBased on this method, we modulated the hidden layers of RNNs using LFVs. We\nevaluated this approach in both full and low resource conditions, as well as\nfor grapheme and phone based systems. Lower error rates throughout the\ndifferent conditions could be achieved by the use of the modulation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 13:22:54 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 13:44:46 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["M\u00fcller", "Markus", ""], ["St\u00fcker", "Sebastian", ""], ["Waibel", "Alex", ""]]}, {"id": "1711.04623", "submitter": "Zachary Kenton", "authors": "Stanis{\\l}aw Jastrz\\k{e}bski, Zachary Kenton, Devansh Arpit, Nicolas\n  Ballas, Asja Fischer, Yoshua Bengio, Amos Storkey", "title": "Three Factors Influencing Minima in SGD", "comments": "First two authors contributed equally. Short version accepted into\n  ICLR workshop. Accepted to Artificial Neural Networks and Machine Learning,\n  ICANN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the dynamical and convergent properties of stochastic gradient\ndescent (SGD) applied to Deep Neural Networks (DNNs). Characterizing the\nrelation between learning rate, batch size and the properties of the final\nminima, such as width or generalization, remains an open question. In order to\ntackle this problem we investigate the previously proposed approximation of SGD\nby a stochastic differential equation (SDE). We theoretically argue that three\nfactors - learning rate, batch size and gradient covariance - influence the\nminima found by SGD. In particular we find that the ratio of learning rate to\nbatch size is a key determinant of SGD dynamics and of the width of the final\nminima, and that higher values of the ratio lead to wider minima and often\nbetter generalization. We confirm these findings experimentally. Further, we\ninclude experiments which show that learning rate schedules can be replaced\nwith batch size schedules and that the ratio of learning rate to batch size is\nan important factor influencing the memorization process.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 15:11:56 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 16:22:54 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 09:29:55 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Jastrz\u0119bski", "Stanis\u0142aw", ""], ["Kenton", "Zachary", ""], ["Arpit", "Devansh", ""], ["Ballas", "Nicolas", ""], ["Fischer", "Asja", ""], ["Bengio", "Yoshua", ""], ["Storkey", "Amos", ""]]}, {"id": "1711.04708", "submitter": "Anuj Karpatne", "authors": "Anuj Karpatne, Imme Ebert-Uphoff, Sai Ravela, Hassan Ali Babaie, and\n  Vipin Kumar", "title": "Machine Learning for the Geosciences: Challenges and Opportunities", "comments": "Under review at IEEE Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geosciences is a field of great societal relevance that requires solutions to\nseveral urgent problems facing our humanity and the planet. As geosciences\nenters the era of big data, machine learning (ML) -- that has been widely\nsuccessful in commercial domains -- offers immense potential to contribute to\nproblems in geosciences. However, problems in geosciences have several unique\nchallenges that are seldom found in traditional applications, requiring novel\nproblem formulations and methodologies in machine learning. This article\nintroduces researchers in the machine learning (ML) community to these\nchallenges offered by geoscience problems and the opportunities that exist for\nadvancing both machine learning and geosciences. We first highlight typical\nsources of geoscience data and describe their properties that make it\nchallenging to use traditional machine learning techniques. We then describe\nsome of the common categories of geoscience problems where machine learning can\nplay a role, and discuss some of the existing efforts and promising directions\nfor methodological development in machine learning. We conclude by discussing\nsome of the emerging research themes in machine learning that are applicable\nacross all problems in the geosciences, and the importance of a deep\ncollaboration between machine learning and geosciences for synergistic\nadvancements in both disciplines.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:16:38 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Karpatne", "Anuj", ""], ["Ebert-Uphoff", "Imme", ""], ["Ravela", "Sai", ""], ["Babaie", "Hassan Ali", ""], ["Kumar", "Vipin", ""]]}, {"id": "1711.04710", "submitter": "Anuj Karpatne", "authors": "Gowtham Atluri, Anuj Karpatne, and Vipin Kumar", "title": "Spatio-Temporal Data Mining: A Survey of Problems and Methods", "comments": "Accepted for publication at ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large volumes of spatio-temporal data are increasingly collected and studied\nin diverse domains including, climate science, social sciences, neuroscience,\nepidemiology, transportation, mobile health, and Earth sciences.\nSpatio-temporal data differs from relational data for which computational\napproaches are developed in the data mining community for multiple decades, in\nthat both spatial and temporal attributes are available in addition to the\nactual measurements/attributes. The presence of these attributes introduces\nadditional challenges that needs to be dealt with. Approaches for mining\nspatio-temporal data have been studied for over a decade in the data mining\ncommunity. In this article we present a broad survey of this relatively young\nfield of spatio-temporal data mining. We discuss different types of\nspatio-temporal data and the relevant data mining questions that arise in the\ncontext of analyzing each of these datasets. Based on the nature of the data\nmining problem studied, we classify literature on spatio-temporal data mining\ninto six major categories: clustering, predictive learning, change detection,\nfrequent pattern mining, anomaly detection, and relationship mining. We discuss\nthe various forms of spatio-temporal data mining problems in each of these\ncategories.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:17:29 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 17:31:54 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Atluri", "Gowtham", ""], ["Karpatne", "Anuj", ""], ["Kumar", "Vipin", ""]]}, {"id": "1711.04883", "submitter": "Peter Boyle", "authors": "Peter Boyle, Michael Chuvelev, Guido Cossu, Christopher Kelly,\n  Christoph Lehner, Lawrence Meadows", "title": "Accelerating HPC codes on Intel(R) Omni-Path Architecture networks: From\n  particle physics to Machine Learning", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss practical methods to ensure near wirespeed performance from\nclusters with either one or two Intel(R) Omni-Path host fabric interfaces (HFI)\nper node, and Intel(R) Xeon Phi(TM) 72xx (Knight's Landing) processors, and\nusing the Linux operating system.\n  The study evaluates the performance improvements achievable and the required\nprogramming approaches in two distinct example problems: firstly in Cartesian\ncommunicator halo exchange problems, appropriate for structured grid PDE\nsolvers that arise in quantum chromodynamics simulations of particle physics,\nand secondly in gradient reduction appropriate to synchronous stochastic\ngradient descent for machine learning. As an example, we accelerate a published\nBaidu Research reduction code and obtain a factor of ten speedup over the\noriginal code using the techniques discussed in this paper. This displays how a\nfactor of ten speedup in strongly scaled distributed machine learning could be\nachieved when synchronous stochastic gradient descent is massively parallelised\nwith a fixed mini-batch size.\n  We find a significant improvement in performance robustness when memory is\nobtained using carefully allocated 2MB \"huge\" virtual memory pages, implying\nthat either non-standard allocation routines should be used for communication\nbuffers. These can be accessed via a LD\\_PRELOAD override in the manner\nsuggested by libhugetlbfs. We make use of a the Intel(R) MPI 2019 library\n\"Technology Preview\" and underlying software to enable thread concurrency\nthroughout the communication software stake via multiple PSM2 endpoints per\nprocess and use of multiple independent MPI communicators. When using a single\nMPI process per node, we find that this greatly accelerates delivered bandwidth\nin many core Intel(R) Xeon Phi processors.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 22:51:30 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Boyle", "Peter", ""], ["Chuvelev", "Michael", ""], ["Cossu", "Guido", ""], ["Kelly", "Christopher", ""], ["Lehner", "Christoph", ""], ["Meadows", "Lawrence", ""]]}, {"id": "1711.04971", "submitter": "Srikanta Bedathur", "authors": "Rema Ananthanarayanan and Pranay Kr. Lohia and Srikanta Bedathur", "title": "DataVizard: Recommending Visual Presentations for Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the appropriate visual presentation of the data such that it\npreserves the semantics of the underlying data and at the same time provides an\nintuitive summary of the data is an important, often the final step of data\nanalytics. Unfortunately, this is also a step involving significant human\neffort starting from selection of groups of columns in the structured results\nfrom analytics stages, to the selection of right visualization by experimenting\nwith various alternatives. In this paper, we describe our \\emph{DataVizard}\nsystem aimed at reducing this overhead by automatically recommending the most\nappropriate visual presentation for the structured result. Specifically, we\nconsider the following two scenarios: first, when one needs to visualize the\nresults of a structured query such as SQL; and the second, when one has\nacquired a data table with an associated short description (e.g., tables from\nthe Web). Using a corpus of real-world database queries (and their results) and\na number of statistical tables crawled from the Web, we show that DataVizard is\ncapable of recommending visual presentations with high accuracy. We also\npresent the results of a user survey that we conducted in order to assess user\nviews of the suitability of the presented charts vis-a-vis the plain text\ncaptions of the data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 06:43:30 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Ananthanarayanan", "Rema", ""], ["Lohia", "Pranay Kr.", ""], ["Bedathur", "Srikanta", ""]]}, {"id": "1711.04981", "submitter": "Yi Tay", "authors": "Yi Tay, Minh C. Phan, Luu Anh Tuan, Siu Cheung Hui", "title": "SkipFlow: Incorporating Neural Coherence Features for End-to-End\n  Automatic Text Scoring", "comments": "Accepted to AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has demonstrated tremendous potential for Automatic Text\nScoring (ATS) tasks. In this paper, we describe a new neural architecture that\nenhances vanilla neural network models with auxiliary neural coherence\nfeatures. Our new method proposes a new \\textsc{SkipFlow} mechanism that models\nrelationships between snapshots of the hidden representations of a long\nshort-term memory (LSTM) network as it reads. Subsequently, the semantic\nrelationships between multiple snapshots are used as auxiliary features for\nprediction. This has two main benefits. Firstly, essays are typically long\nsequences and therefore the memorization capability of the LSTM network may be\ninsufficient. Implicit access to multiple snapshots can alleviate this problem\nby acting as a protection against vanishing gradients. The parameters of the\n\\textsc{SkipFlow} mechanism also acts as an auxiliary memory. Secondly,\nmodeling relationships between multiple positions allows our model to learn\nfeatures that represent and approximate textual coherence. In our model, we\ncall this \\textit{neural coherence} features. Overall, we present a unified\ndeep learning architecture that generates neural coherence features as it reads\nin an end-to-end fashion. Our approach demonstrates state-of-the-art\nperformance on the benchmark ASAP dataset, outperforming not only feature\nengineering baselines but also other deep learning models.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 07:20:23 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Tay", "Yi", ""], ["Phan", "Minh C.", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1711.04994", "submitter": "Mikael Henaff", "authors": "Mikael Henaff, Junbo Zhao and Yann LeCun", "title": "Prediction Under Uncertainty with Error-Encoding Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a new framework for performing temporal predictions\nin the presence of uncertainty. It is based on a simple idea of disentangling\ncomponents of the future state which are predictable from those which are\ninherently unpredictable, and encoding the unpredictable components into a\nlow-dimensional latent variable which is fed into a forward model. Our method\nuses a supervised training objective which is fast and easy to train. We\nevaluate it in the context of video prediction on multiple datasets and show\nthat it is able to consistently generate diverse predictions without the need\nfor alternating minimization over a latent space or adversarial training.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 08:32:43 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 07:32:36 GMT"}, {"version": "v3", "created": "Thu, 30 Nov 2017 23:11:58 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Henaff", "Mikael", ""], ["Zhao", "Junbo", ""], ["LeCun", "Yann", ""]]}, {"id": "1711.05078", "submitter": "Raghuram Bharadwaj Diddigi", "authors": "Diddigi Raghuram Bharadwaj, Sai Koti Reddy Danda, Krishnasuri\n  Narayanam, Shalabh Bhatnagar", "title": "A unified decision making framework for supply and demand management in\n  microgrid networks", "comments": null, "journal-ref": null, "doi": "10.1109/SmartGridComm.2018.8587514", "report-no": null, "categories": "eess.SY cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers two important problems -- on the supply-side and\ndemand-side respectively and studies both in a unified framework. On the supply\nside, we study the problem of energy sharing among microgrids with the goal of\nmaximizing profit obtained from selling power while at the same time not\ndeviating much from the customer demand. On the other hand, under shortage of\npower, this problem becomes one of deciding the amount of power to be bought\nwith dynamically varying prices. On the demand side, we consider the problem of\noptimally scheduling the time-adjustable demand - i.e., of loads with flexible\ntime windows in which they can be scheduled. While previous works have treated\nthese two problems in isolation, we combine these problems together and provide\na unified Markov decision process (MDP) framework for these problems. We then\napply the Q-learning algorithm, a popular model-free reinforcement learning\ntechnique, to obtain the optimal policy. Through simulations, we show that the\npolicy obtained by solving our MDP model provides more profit to the\nmicrogrids.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 12:28:40 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 12:07:49 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Bharadwaj", "Diddigi Raghuram", ""], ["Danda", "Sai Koti Reddy", ""], ["Narayanam", "Krishnasuri", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1711.05090", "submitter": "Thomas Guyet", "authors": "Thomas Guyet (LACODAM), Yves Moinard (LACODAM), Ren\\'e Quiniou\n  (LACODAM), Torsten Schaub (LACODAM)", "title": "Efficiency Analysis of ASP Encodings for Sequential Pattern Mining Tasks", "comments": null, "journal-ref": "Bruno Pinaud; Fabrice Guillet; Bruno Cremilleux; Cyril de Runz.\n  Advances in Knowledge Discovery and Management, 7, Springer, pp.41--81, 2017,\n  978-3-319-65405-8", "doi": null, "report-no": null, "categories": "cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the use of Answer Set Programming (ASP) to mine\nsequential patterns. ASP is a high-level declarative logic programming paradigm\nfor high level encoding combinatorial and optimization problem solving as well\nas knowledge representation and reasoning. Thus, ASP is a good candidate for\nimplementing pattern mining with background knowledge, which has been a data\nmining issue for a long time. We propose encodings of the classical sequential\npattern mining tasks within two representations of embeddings (fill-gaps vs\nskip-gaps) and for various kinds of patterns: frequent, constrained and\ncondensed. We compare the computational performance of these encodings with\neach other to get a good insight into the efficiency of ASP encodings. The\nresults show that the fill-gaps strategy is better on real problems due to\nlower memory consumption. Finally, compared to a constraint programming\napproach (CPSM), another declarative programming paradigm, our proposal showed\ncomparable performance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:09:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Guyet", "Thomas", "", "LACODAM"], ["Moinard", "Yves", "", "LACODAM"], ["Quiniou", "Ren\u00e9", "", "LACODAM"], ["Schaub", "Torsten", "", "LACODAM"]]}, {"id": "1711.05098", "submitter": "Athanasios Lagopoulos", "authors": "Athanasios Lagopoulos, Grigorios Tsoumakas, Georgios Papadopoulos", "title": "Web Robot Detection in Academic Publishing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent industry reports assure the rise of web robots which comprise more\nthan half of the total web traffic. They not only threaten the security,\nprivacy and efficiency of the web but they also distort analytics and metrics,\ndoubting the veracity of the information being promoted. In the academic\npublishing domain, this can cause articles to be faulty presented as prominent\nand influential. In this paper, we present our approach on detecting web robots\nin academic publishing websites. We use different supervised learning\nalgorithms with a variety of characteristics deriving from both the log files\nof the server and the content served by the website. Our approach relies on the\nassumption that human users will be interested in specific domains or articles,\nwhile web robots crawl a web library incoherently. We experiment with features\nadopted in previous studies with the addition of novel semantic characteristics\nwhich derive after performing a semantic analysis using the Latent Dirichlet\nAllocation (LDA) algorithm. Our real-world case study shows promising results,\npinpointing the significance of semantic features in the web robot detection\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:20:56 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Lagopoulos", "Athanasios", ""], ["Tsoumakas", "Grigorios", ""], ["Papadopoulos", "Georgios", ""]]}, {"id": "1711.05105", "submitter": "Mehdi Sadeqi", "authors": "Mehdi Sadeqi, Robert C. Holte and Sandra Zilles", "title": "An Empirical Study of the Effects of Spurious Transitions on\n  Abstraction-based Heuristics", "comments": "38 pages, 9 figures, appendix with 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient solution of state space search problems is often attempted by\nguiding search algorithms with heuristics (estimates of the distance from any\nstate to the goal). A popular way for creating heuristic functions is by using\nan abstract version of the state space. However, the quality of\nabstraction-based heuristic functions, and thus the speed of search, can suffer\nfrom spurious transitions, i.e., state transitions in the abstract state space\nfor which no corresponding transitions in the reachable component of the\noriginal state space exist. Our first contribution is a quantitative study\ndemonstrating that the harmful effects of spurious transitions on heuristic\nfunctions can be substantial, in terms of both the increase in the number of\nabstract states and the decrease in the heuristic values, which may slow down\nsearch. Our second contribution is an empirical study on the benefits of\nremoving a certain kind of spurious transition, namely those that involve\nstates with a pair of mutually exclusive (mutex) variablevalue assignments. In\nthe context of state space planning, a mutex pair is a pair of variable-value\nassignments that does not occur in any reachable state. Detecting mutex pairs\nis a problem that has been addressed frequently in the planning literature. Our\nstudy shows that there are cases in which mutex detection helps to eliminate\nharmful spurious transitions to a large extent and thus to speed up search\nsubstantially.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:27:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Sadeqi", "Mehdi", ""], ["Holte", "Robert C.", ""], ["Zilles", "Sandra", ""]]}, {"id": "1711.05116", "submitter": "Shuohang Wang", "authors": "Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu\n  Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, Murray Campbell", "title": "Evidence Aggregation for Answer Re-Ranking in Open-Domain Question\n  Answering", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular recent approach to answering open-domain questions is to first\nsearch for question-related passages and then apply reading comprehension\nmodels to extract answers. Existing methods usually extract answers from single\npassages independently. But some questions require a combination of evidence\nfrom across different sources to answer correctly. In this paper, we propose\ntwo models which make use of multiple passages to generate their answers. Both\nuse an answer-reranking approach which reorders the answer candidates generated\nby an existing state-of-the-art QA model. We propose two methods, namely,\nstrength-based re-ranking and coverage-based re-ranking, to make use of the\naggregated evidence from different passages to better determine the answer. Our\nmodels have achieved state-of-the-art results on three public open-domain QA\ndatasets: Quasar-T, SearchQA and the open-domain version of TriviaQA, with\nabout 8 percentage points of improvement over the former two datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:39:51 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 15:50:25 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Wang", "Shuohang", ""], ["Yu", "Mo", ""], ["Jiang", "Jing", ""], ["Zhang", "Wei", ""], ["Guo", "Xiaoxiao", ""], ["Chang", "Shiyu", ""], ["Wang", "Zhiguo", ""], ["Klinger", "Tim", ""], ["Tesauro", "Gerald", ""], ["Campbell", "Murray", ""]]}, {"id": "1711.05136", "submitter": "Guillaume Bellec", "authors": "Guillaume Bellec, David Kappel, Wolfgang Maass and Robert Legenstein", "title": "Deep Rewiring: Training very sparse deep networks", "comments": "Accepted for publication at ICLR 2018. 10 pages (12 with references,\n  24 with appendix), 4 Figures in the main text. Reviews are available at:\n  https://openreview.net/forum?id=BJ_wN01C- . This recent version contains\n  minor corrections in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic hardware tends to pose limits on the connectivity of deep\nnetworks that one can run on them. But also generic hardware and software\nimplementations of deep learning run more efficiently for sparse networks.\nSeveral methods exist for pruning connections of a neural network after it was\ntrained without connectivity constraints. We present an algorithm, DEEP R, that\nenables us to train directly a sparsely connected neural network. DEEP R\nautomatically rewires the network during supervised training so that\nconnections are there where they are most needed for the task, while its total\nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used\nto train very sparse feedforward and recurrent neural networks on standard\nbenchmark tasks with just a minor loss in performance. DEEP R is based on a\nrigorous theoretical foundation that views rewiring as stochastic sampling of\nnetwork configurations from a posterior.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 15:02:47 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 18:33:53 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 15:57:44 GMT"}, {"version": "v4", "created": "Mon, 5 Feb 2018 11:01:41 GMT"}, {"version": "v5", "created": "Tue, 7 Aug 2018 18:12:10 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Bellec", "Guillaume", ""], ["Kappel", "David", ""], ["Maass", "Wolfgang", ""], ["Legenstein", "Robert", ""]]}, {"id": "1711.05165", "submitter": "Sean Welleck", "authors": "Sean Welleck, Jialin Mao, Kyunghyun Cho, Zheng Zhang", "title": "Saliency-based Sequential Image Attention with Multiset Prediction", "comments": "To appear in Advances in Neural Information Processing Systems 30\n  (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans process visual scenes selectively and sequentially using attention.\nCentral to models of human visual attention is the saliency map. We propose a\nhierarchical visual architecture that operates on a saliency map and uses a\nnovel attention mechanism to sequentially focus on salient regions and take\nadditional glimpses within those regions. The architecture is motivated by\nhuman visual attention, and is used for multi-label image classification on a\nnovel multiset task, demonstrating that it achieves high precision and recall\nwhile localizing objects with its attention. Unlike conventional multi-label\nimage classification models, the model supports multiset prediction due to a\nreinforcement-learning based training process that allows for arbitrary label\npermutation and multiple instances per label.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 16:16:36 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Welleck", "Sean", ""], ["Mao", "Jialin", ""], ["Cho", "Kyunghyun", ""], ["Zhang", "Zheng", ""]]}, {"id": "1711.05216", "submitter": "Francesco Scarcello", "authors": "Georg Gottlob, Gianlugi Greco, Francesco Scarcello", "title": "Tree Projections and Constraint Optimization Problems: Fixed-Parameter\n  Tractability and Parallel Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree projections provide a unifying framework to deal with most structural\ndecomposition methods of constraint satisfaction problems (CSPs). Within this\nframework, a CSP instance is decomposed into a number of sub-problems, called\nviews, whose solutions are either already available or can be computed\nefficiently. The goal is to arrange portions of these views in a tree-like\nstructure, called tree projection, which determines an efficiently solvable CSP\ninstance equivalent to the original one. Deciding whether a tree projection\nexists is NP-hard. Solution methods have therefore been proposed in the\nliterature that do not require a tree projection to be given, and that either\ncorrectly decide whether the given CSP instance is satisfiable, or return that\na tree projection actually does not exist. These approaches had not been\ngeneralized so far on CSP extensions for optimization problems, where the goal\nis to compute a solution of maximum value/minimum cost. The paper fills the\ngap, by exhibiting a fixed-parameter polynomial-time algorithm that either\ndisproves the existence of tree projections or computes an optimal solution,\nwith the parameter being the size of the expression of the objective function\nto be optimized over all possible solutions (and not the size of the whole\nconstraint formula, used in related works). Tractability results are also\nestablished for the problem of returning the best K solutions. Finally,\nparallel algorithms for such optimization problems are proposed and analyzed.\nGiven that the classes of acyclic hypergraphs, hypergraphs of bounded\ntreewidth, and hypergraphs of bounded generalized hypertree width are all\ncovered as special cases of the tree projection framework, the results in this\npaper directly apply to these classes. These classes are extensively considered\nin the CSP setting, as well as in conjunctive database query evaluation and\noptimization.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 17:30:08 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Gottlob", "Georg", ""], ["Greco", "Gianlugi", ""], ["Scarcello", "Francesco", ""]]}, {"id": "1711.05227", "submitter": "Boris Motik", "authors": "Michael Benedikt and Boris Motik and Efthymia Tsamoura", "title": "Goal-Driven Query Answering for Existential Rules with Equality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the magic sets for Datalog, we present a novel goal-driven\napproach for answering queries over terminating existential rules with equality\n(aka TGDs and EGDs). Our technique improves the performance of query answering\nby pruning the consequences that are not relevant for the query. This is\nchallenging in our setting because equalities can potentially affect all\npredicates in a dataset. We address this problem by combining the existing\nsingularization technique with two new ingredients: an algorithm for\nidentifying the rules relevant to a query and a new magic sets algorithm. We\nshow empirically that our technique can significantly improve the performance\nof query answering, and that it can mean the difference between answering a\nquery in a few seconds or not being able to process the query at all.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 18:00:38 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 20:09:27 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Benedikt", "Michael", ""], ["Motik", "Boris", ""], ["Tsamoura", "Efthymia", ""]]}, {"id": "1711.05240", "submitter": "Omer Goldman", "authors": "Omer Goldman and Veronica Latcinnik and Udi Naveh and Amir Globerson\n  and Jonathan Berant", "title": "Weakly-supervised Semantic Parsing with Abstract Examples", "comments": "CNLVR,NLVR. Accepted to ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training semantic parsers from weak supervision (denotations) rather than\nstrong supervision (programs) complicates training in two ways. First, a large\nsearch space of potential programs needs to be explored at training time to\nfind a correct program. Second, spurious programs that accidentally lead to a\ncorrect denotation add noise to training. In this work we propose that in\nclosed worlds with clear semantic types, one can substantially alleviate these\nproblems by utilizing an abstract representation, where tokens in both the\nlanguage utterance and program are lifted to an abstract form. We show that\nthese abstractions can be defined with a handful of lexical rules and that they\nresult in sharing between different examples that alleviates the difficulties\nin training. To test our approach, we develop the first semantic parser for\nCNLVR, a challenging visual reasoning dataset, where the search space is large\nand overcoming spuriousness is critical, because denotations are either TRUE or\nFALSE, and thus random programs are likely to lead to a correct denotation. Our\nmethod substantially improves performance, and reaches 82.5% accuracy, a 14.7%\nabsolute accuracy improvement compared to the best reported accuracy so far.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 18:29:05 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 12:12:06 GMT"}, {"version": "v3", "created": "Sat, 12 May 2018 20:12:13 GMT"}, {"version": "v4", "created": "Thu, 24 May 2018 14:22:57 GMT"}, {"version": "v5", "created": "Wed, 13 Mar 2019 09:30:38 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Goldman", "Omer", ""], ["Latcinnik", "Veronica", ""], ["Naveh", "Udi", ""], ["Globerson", "Amir", ""], ["Berant", "Jonathan", ""]]}, {"id": "1711.05246", "submitter": "Sean Welleck", "authors": "Sean Welleck, Zixin Yao, Yu Gai, Jialin Mao, Zheng Zhang, Kyunghyun\n  Cho", "title": "Loss Functions for Multiset Prediction", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of multiset prediction. The goal of multiset prediction\nis to train a predictor that maps an input to a multiset consisting of multiple\nitems. Unlike existing problems in supervised learning, such as classification,\nranking and sequence generation, there is no known order among items in a\ntarget multiset, and each item in the multiset may appear more than once,\nmaking this problem extremely challenging. In this paper, we propose a novel\nmultiset loss function by viewing this problem from the perspective of\nsequential decision making. The proposed multiset loss function is empirically\nevaluated on two families of datasets, one synthetic and the other real, with\nvarying levels of difficulty, against various baseline loss functions including\nreinforcement learning, sequence, and aggregated distribution matching loss\nfunctions. The experiments reveal the effectiveness of the proposed loss\nfunction over the others.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 18:43:22 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 18:32:36 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Welleck", "Sean", ""], ["Yao", "Zixin", ""], ["Gai", "Yu", ""], ["Mao", "Jialin", ""], ["Zhang", "Zheng", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1711.05255", "submitter": "Qianli Ma", "authors": "Qianli Ma, Lifeng Shen, Garrison W. Cottrell", "title": "Deep-ESN: A Multiple Projection-encoding Hierarchical Reservoir\n  Computing Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an efficient recurrent neural network (RNN) model, reservoir computing\n(RC) models, such as Echo State Networks, have attracted widespread attention\nin the last decade. However, while they have had great success with time series\ndata [1], [2], many time series have a multiscale structure, which a\nsingle-hidden-layer RC model may have difficulty capturing. In this paper, we\npropose a novel hierarchical reservoir computing framework we call Deep Echo\nState Networks (Deep-ESNs). The most distinctive feature of a Deep-ESN is its\nability to deal with time series through hierarchical projections.\nSpecifically, when an input time series is projected into the high-dimensional\necho-state space of a reservoir, a subsequent encoding layer (e.g., a PCA,\nautoencoder, or a random projection) can project the echo-state representations\ninto a lower-dimensional space. These low-dimensional representations can then\nbe processed by another ESN. By using projection layers and encoding layers\nalternately in the hierarchical framework, a Deep-ESN can not only attenuate\nthe effects of the collinearity problem in ESNs, but also fully take advantage\nof the temporal kernel property of ESNs to explore multiscale dynamics of time\nseries. To fuse the multiscale representations obtained by each reservoir, we\nadd connections from each encoding layer to the last output layer. Theoretical\nanalyses prove that stability of a Deep-ESN is guaranteed by the echo state\nproperty (ESP), and the time complexity is equivalent to a conventional ESN.\nExperimental results on some artificial and real world time series demonstrate\nthat Deep-ESNs can capture multiscale dynamics, and outperform both standard\nESNs and previous hierarchical ESN-based models.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 13:33:46 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Ma", "Qianli", ""], ["Shen", "Lifeng", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1711.05401", "submitter": "Chandrahas Dewangan", "authors": "Srinivas Ravishankar, Chandrahas, Partha Pratim Talukdar", "title": "Revisiting Simple Neural Networks for Learning Representations of\n  Knowledge Graphs", "comments": "7 pages, submitted to and accepted in Automated Knowledge Base\n  Construction (AKBC) Workshop 2017, at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning vector representations for entities and\nrelations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This\nproblem has received significant attention in the past few years and multiple\nmethods have been proposed. Most of the existing methods in the literature use\na predefined characteristic scoring function for evaluating the correctness of\nKG triples. These scoring functions distinguish correct triples (high score)\nfrom incorrect ones (low score). However, their performance vary across\ndifferent datasets. In this work, we demonstrate that a simple neural network\nbased score function can consistently achieve near start-of-the-art performance\non multiple datasets. We also quantitatively demonstrate biases in standard\nbenchmark datasets, and highlight the need to perform evaluation spanning\nvarious datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 04:12:27 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 10:02:28 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 07:20:37 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Ravishankar", "Srinivas", ""], ["Chandrahas", "", ""], ["Talukdar", "Partha Pratim", ""]]}, {"id": "1711.05412", "submitter": "Dianmu Zhang", "authors": "Dianmu Zhang and Blake Hannaford", "title": "IKBT: solving closed-form Inverse Kinematics with Behavior Tree", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serial robot arms have complicated kinematic equations which must be solved\nto write effective arm planning and control software (the Inverse Kinematics\nProblem). Existing software packages for inverse kinematics often rely on\nnumerical methods which have significant shortcomings. Here we report a new\nsymbolic inverse kinematics solver which overcomes the limitations of numerical\nmethods, and the shortcomings of previous symbolic software packages. We\nintegrate Behavior Trees, an execution planning framework previously used for\ncontrolling intelligent robot behavior, to organize the equation solving\nprocess, and a modular architecture for each solution technique. The system\nsuccessfully solved, generated a LaTex report, and generated a Python code\ntemplate for 18 out of 19 example robots of 4-6 DOF. The system is readily\nextensible, maintainable, and multi-platform with few dependencies. The\ncomplete package is available with a Modified BSD license on Github.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 05:19:18 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 01:27:29 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 08:42:56 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Zhang", "Dianmu", ""], ["Hannaford", "Blake", ""]]}, {"id": "1711.05435", "submitter": "Takuma Ebisu", "authors": "Takuma Ebisu and Ryutaro Ichise", "title": "TorusE: Knowledge Graph Embedding on a Lie Group", "comments": "accepted for AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs are useful for many artificial intelligence (AI) tasks.\nHowever, knowledge graphs often have missing facts. To populate the graphs,\nknowledge graph embedding models have been developed. Knowledge graph embedding\nmodels map entities and relations in a knowledge graph to a vector space and\npredict unknown triples by scoring candidate triples. TransE is the first\ntranslation-based method and it is well known because of its simplicity and\nefficiency for knowledge graph completion. It employs the principle that the\ndifferences between entity embeddings represent their relations. The principle\nseems very simple, but it can effectively capture the rules of a knowledge\ngraph. However, TransE has a problem with its regularization. TransE forces\nentity embeddings to be on a sphere in the embedding vector space. This\nregularization warps the embeddings and makes it difficult for them to fulfill\nthe abovementioned principle. The regularization also affects adversely the\naccuracies of the link predictions. On the other hand, regularization is\nimportant because entity embeddings diverge by negative sampling without it.\nThis paper proposes a novel embedding model, TorusE, to solve the\nregularization problem. The principle of TransE can be defined on any Lie\ngroup. A torus, which is one of the compact Lie groups, can be chosen for the\nembedding space to avoid regularization. To the best of our knowledge, TorusE\nis the first model that embeds objects on other than a real or complex vector\nspace, and this paper is the first to formally discuss the problem of\nregularization of TransE. Our approach outperforms other state-of-the-art\napproaches such as TransE, DistMult and ComplEx on a standard link prediction\ntask. We show that TorusE is scalable to large-size knowledge graphs and is\nfaster than the original TransE.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 07:44:22 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Ebisu", "Takuma", ""], ["Ichise", "Ryutaro", ""]]}, {"id": "1711.05457", "submitter": "Hakaru Tamukoh", "authors": "Sansei Hori, Yutaro Ishida, Yuta Kiyama, Yuichiro Tanaka, Yuki Kuroda,\n  Masataka Hisano, Yuto Imamura, Tomotaka Himaki, Yuma Yoshimoto, Yoshiya\n  Aratani, Kouhei Hashimoto, Gouki Iwamoto, Hiroto Fujita, Takashi Morie,\n  Hakaru Tamukoh", "title": "Hibikino-Musashi@Home 2017 Team Description Paper", "comments": "8 pages; RoboCup 2017 @Home Open Platform League team description\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our team Hibikino-Musashi@Home was founded in 2010. It is based in Kitakyushu\nScience and Research Park, Japan. Since 2010, we have participated in the\nRoboCup@Home Japan open competition open-platform league every year. Currently,\nthe Hibikino-Musashi@Home team has 24 members from seven different laboratories\nbased in the Kyushu Institute of Technology. Our home-service robots are used\nas platforms for both education and implementation of our research outcomes. In\nthis paper, we introduce our team and the technologies that we have implemented\nin our robots.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 08:55:11 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Hori", "Sansei", ""], ["Ishida", "Yutaro", ""], ["Kiyama", "Yuta", ""], ["Tanaka", "Yuichiro", ""], ["Kuroda", "Yuki", ""], ["Hisano", "Masataka", ""], ["Imamura", "Yuto", ""], ["Himaki", "Tomotaka", ""], ["Yoshimoto", "Yuma", ""], ["Aratani", "Yoshiya", ""], ["Hashimoto", "Kouhei", ""], ["Iwamoto", "Gouki", ""], ["Fujita", "Hiroto", ""], ["Morie", "Takashi", ""], ["Tamukoh", "Hakaru", ""]]}, {"id": "1711.05508", "submitter": "Patrick Rodler", "authors": "Patrick Rodler, Wolfgang Schmid, Konstantin Schekotihin", "title": "A Generally Applicable, Highly Scalable Measurement Computation and\n  Optimization Approach to Sequential Model-Based Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-Based Diagnosis deals with the identification of the real cause of a\nsystem's malfunction based on a formal system model and observations of the\nsystem behavior. When a malfunction is detected, there is usually not enough\ninformation available to pinpoint the real cause and one needs to discriminate\nbetween multiple fault hypotheses (called diagnoses). To this end, Sequential\nDiagnosis approaches ask an oracle for additional system measurements.\n  This work presents strategies for (optimal) measurement selection in\nmodel-based sequential diagnosis. In particular, assuming a set of leading\ndiagnoses being given, we show how queries (sets of measurements) can be\ncomputed and optimized along two dimensions: expected number of queries and\ncost per query. By means of a suitable decoupling of two optimizations and a\nclever search space reduction the computations are done without any inference\nengine calls. For the full search space, we give a method requiring only a\npolynomial number of inferences and show how query properties can be guaranteed\nwhich existing methods do not provide. Evaluation results using real-world\nproblems indicate that the new method computes (virtually) optimal queries\ninstantly independently of the size and complexity of the considered diagnosis\nproblems and outperforms equally general methods not exploiting the proposed\ntheory by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 11:44:03 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Rodler", "Patrick", ""], ["Schmid", "Wolfgang", ""], ["Schekotihin", "Konstantin", ""]]}, {"id": "1711.05509", "submitter": "Jan Konecny", "authors": "Jan Konecny", "title": "Note on Representing attribute reduction and concepts in concepts\n  lattice using graphs", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mao H. (2017, Representing attribute reduction and concepts in concept\nlattice using graphs. Soft Computing 21(24):7293--7311) claims to make\ncontributions to the study of reduction of attributes in concept lattices by\nusing graph theory. We show that her results are either trivial or already\nwell-known and all three algorithms proposed in the paper are incorrect.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 11:44:26 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 09:54:53 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Konecny", "Jan", ""]]}, {"id": "1711.05541", "submitter": "Stuart Armstrong", "authors": "Stuart Armstrong, Xavier O'Rorke", "title": "Good and safe uses of AI Oracles", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is possible that powerful and potentially dangerous artificial\nintelligence (AI) might be developed in the future. An Oracle is a design which\naims to restrain the impact of a potentially dangerous AI by restricting the\nagent to no actions besides answering questions. Unfortunately, most Oracles\nwill be motivated to gain more control over the world by manipulating users\nthrough the content of their answers, and Oracles of potentially high\nintelligence might be very successful at this\n\\citep{DBLP:journals/corr/AlfonsecaCACAR16}. In this paper we present two\ndesigns for Oracles which, even under pessimistic assumptions, will not\nmanipulate their users into releasing them and yet will still be incentivised\nto provide their users with helpful answers. The first design is the\ncounterfactual Oracle -- which choses its answer as if it expected nobody to\never read it. The second design is the low-bandwidth Oracle -- which is limited\nby the quantity of information it can transmit.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 12:47:17 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 11:01:01 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 17:17:11 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 16:06:38 GMT"}, {"version": "v5", "created": "Tue, 5 Jun 2018 11:13:48 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Armstrong", "Stuart", ""], ["O'Rorke", "Xavier", ""]]}, {"id": "1711.05557", "submitter": "Chee Seng Chan", "authors": "Ying Hua Tan, Chee Seng Chan", "title": "Phrase-based Image Captioning with Hierarchical LSTM Model", "comments": "17 pages, 12 figures, ACCV2016 extension, phrase-based image\n  captioning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic generation of caption to describe the content of an image has been\ngaining a lot of research interests recently, where most of the existing works\ntreat the image caption as pure sequential data. Natural language, however\npossess a temporal hierarchy structure, with complex dependencies between each\nsubsequence. In this paper, we propose a phrase-based hierarchical Long\nShort-Term Memory (phi-LSTM) model to generate image description. In contrast\nto the conventional solutions that generate caption in a pure sequential\nmanner, our proposed model decodes image caption from phrase to sentence. It\nconsists of a phrase decoder at the bottom hierarchy to decode noun phrases of\nvariable length, and an abbreviated sentence decoder at the upper hierarchy to\ndecode an abbreviated form of the image description. A complete image caption\nis formed by combining the generated phrases with sentence during the inference\nstage. Empirically, our proposed model shows a better or competitive result on\nthe Flickr8k, Flickr30k and MS-COCO datasets in comparison to the state-of-the\nart models. We also show that our proposed model is able to generate more novel\ncaptions (not seen in the training data) which are richer in word contents in\nall these three datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 10:48:59 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Tan", "Ying Hua", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1711.05626", "submitter": "Pankaj Gupta", "authors": "Pankaj Gupta, Subburam Rajaram, Hinrich Sch\\\"utze, Bernt Andrassy", "title": "Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time", "comments": "In Proceedings of the 16th Annual Conference of the North American\n  Chapter of the Association for Computational Linguistics: Human Language\n  Technologies (NAACL-HLT 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic topic modeling facilitates the identification of topical trends over\ntime in temporal collections of unstructured documents. We introduce a novel\nunsupervised neural dynamic topic model named as Recurrent Neural\nNetwork-Replicated Softmax Model (RNNRSM), where the discovered topics at each\ntime influence the topic discovery in the subsequent time steps. We account for\nthe temporal ordering of documents by explicitly modeling a joint distribution\nof latent topical dependencies over time, using distributional estimators with\ntemporal recurrent connections. Applying RNN-RSM to 19 years of articles on NLP\nresearch, we demonstrate that compared to state-of-the art topic models, RNNRSM\nshows better generalization, topic interpretation, evolution and trends. We\nalso introduce a metric (named as SPAN) to quantify the capability of dynamic\ntopic model to capture word evolution in topics over time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 15:33:59 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 09:17:46 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Gupta", "Pankaj", ""], ["Rajaram", "Subburam", ""], ["Sch\u00fctze", "Hinrich", ""], ["Andrassy", "Bernt", ""]]}, {"id": "1711.05627", "submitter": "Senjian An Dr.", "authors": "Senjian An, Farid Boussaid, Mohammed Bennamoun, Ferdous Sohel", "title": "Exploiting Layerwise Convexity of Rectifier Networks with Sign\n  Constrained Weights", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By introducing sign constraints on the weights, this paper proposes sign\nconstrained rectifier networks (SCRNs), whose training can be solved\nefficiently by the well known majorization-minimization (MM) algorithms. We\nprove that the proposed two-hidden-layer SCRNs, which exhibit negative weights\nin the second hidden layer and negative weights in the output layer, are\ncapable of separating any two (or more) disjoint pattern sets. Furthermore, the\nproposed two-hidden-layer SCRNs can decompose the patterns of each class into\nseveral clusters so that each cluster is convexly separable from all the\npatterns from the other classes. This provides a means to learn the pattern\nstructures and analyse the discriminant factors between different classes of\npatterns.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 10:20:44 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["An", "Senjian", ""], ["Boussaid", "Farid", ""], ["Bennamoun", "Mohammed", ""], ["Sohel", "Ferdous", ""]]}, {"id": "1711.05715", "submitter": "Zachary Lipton", "authors": "Zachary Lipton, Xiujun Li, Jianfeng Gao, Lihong Li, Faisal Ahmed, Li\n  Deng", "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for\n  Task-Oriented Dialogue Systems", "comments": "Duplicate of article already in the arXiv: arXiv:1608.05081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm that significantly improves the efficiency of\nexploration for deep Q-learning agents in dialogue systems. Our agents explore\nvia Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop\nneural network. Our algorithm learns much faster than common exploration\nstrategies such as \\epsilon-greedy, Boltzmann, bootstrapping, and\nintrinsic-reward-based ones. Additionally, we show that spiking the replay\nbuffer with experiences from just a few successful episodes can make Q-learning\nfeasible when it might otherwise fail.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 18:23:48 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 04:22:45 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Lipton", "Zachary", ""], ["Li", "Xiujun", ""], ["Gao", "Jianfeng", ""], ["Li", "Lihong", ""], ["Ahmed", "Faisal", ""], ["Deng", "Li", ""]]}, {"id": "1711.05726", "submitter": "Aditya Modi", "authors": "Aditya Modi, Nan Jiang, Satinder Singh, Ambuj Tewari", "title": "Markov Decision Processes with Continuous Side Information", "comments": null, "journal-ref": "PMLR Volume 83: Algorithmic Learning Theory, 7-9 April 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a reinforcement learning (RL) setting in which the agent\ninteracts with a sequence of episodic MDPs. At the start of each episode the\nagent has access to some side-information or context that determines the\ndynamics of the MDP for that episode. Our setting is motivated by applications\nin healthcare where baseline measurements of a patient at the start of a\ntreatment episode form the context that may provide information about how the\npatient might respond to treatment decisions. We propose algorithms for\nlearning in such Contextual Markov Decision Processes (CMDPs) under an\nassumption that the unobserved MDP parameters vary smoothly with the observed\ncontext. We also give lower and upper PAC bounds under the smoothness\nassumption. Because our lower bound has an exponential dependence on the\ndimension, we consider a tractable linear setting where the context is used to\ncreate linear combinations of a finite set of MDPs. For the linear setting, we\ngive a PAC learning algorithm based on KWIK learning techniques.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 18:49:16 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Modi", "Aditya", ""], ["Jiang", "Nan", ""], ["Singh", "Satinder", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1711.05738", "submitter": "C Lee Giles", "authors": "G.Z. Sun, C.L. Giles, H.H. Chen, Y.C. Lee", "title": "The Neural Network Pushdown Automaton: Model, Stack and Learning\n  Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": "UMIACS-TR-93-77", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for neural networks to learn complex languages or grammars, they\nmust have sufficient computational power or resources to recognize or generate\nsuch languages. Though many approaches have been discussed, one ob- vious\napproach to enhancing the processing power of a recurrent neural network is to\ncouple it with an external stack memory - in effect creating a neural network\npushdown automata (NNPDA). This paper discusses in detail this NNPDA - its\nconstruction, how it can be trained and how useful symbolic information can be\nextracted from the trained network.\n  In order to couple the external stack to the neural network, an optimization\nmethod is developed which uses an error function that connects the learning of\nthe state automaton of the neural network to the learning of the operation of\nthe external stack. To minimize the error function using gradient descent\nlearning, an analog stack is designed such that the action and storage of\ninformation in the stack are continuous. One interpretation of a continuous\nstack is the probabilistic storage of and action on data. After training on\nsample strings of an unknown source grammar, a quantization procedure extracts\nfrom the analog stack and neural network a discrete pushdown automata (PDA).\nSimulations show that in learning deterministic context-free grammars - the\nbalanced parenthesis language, 1*n0*n, and the deterministic Palindrome - the\nextracted PDA is correct in the sense that it can correctly recognize unseen\nstrings of arbitrary length. In addition, the extracted PDAs can be shown to be\nidentical or equivalent to the PDAs of the source grammars which were used to\ngenerate the training strings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 18:26:49 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Sun", "G. Z.", ""], ["Giles", "C. L.", ""], ["Chen", "H. H.", ""], ["Lee", "Y. C.", ""]]}, {"id": "1711.05766", "submitter": "Zhipeng Ding", "authors": "Zhipeng Ding, Greg Fleishman, Xiao Yang, Paul Thompson, Roland Kwitt,\n  Marc Niethammer", "title": "Fast Predictive Simple Geodesic Regression", "comments": "19 pages, 10 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable image registration and regression are important tasks in medical\nimage analysis. However, they are computationally expensive, especially when\nanalyzing large-scale datasets that contain thousands of images. Hence, cluster\ncomputing is typically used, making the approaches dependent on such\ncomputational infrastructure. Even larger computational resources are required\nas study sizes increase. This limits the use of deformable image registration\nand regression for clinical applications and as component algorithms for other\nimage analysis approaches. We therefore propose using a fast predictive\napproach to perform image registrations. In particular, we employ these fast\nregistration predictions to approximate a simplified geodesic regression model\nto capture longitudinal brain changes. The resulting method is orders of\nmagnitude faster than the standard optimization-based regression model and\nhence facilitates large-scale analysis on a single graphics processing unit\n(GPU). We evaluate our results on 3D brain magnetic resonance images (MRI) from\nthe ADNI datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 19:30:20 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Ding", "Zhipeng", ""], ["Fleishman", "Greg", ""], ["Yang", "Xiao", ""], ["Thompson", "Paul", ""], ["Kwitt", "Roland", ""], ["Niethammer", "Marc", ""]]}, {"id": "1711.05767", "submitter": "Avinash Achar", "authors": "Avinash Achar, Venkatesh Sarangan, R Rohith, Anand Sivasubramaniam", "title": "Predicting vehicular travel times by modeling heterogeneous influences\n  between arterial roads", "comments": "13 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting travel times of vehicles in urban settings is a useful and\ntangible quantity of interest in the context of intelligent transportation\nsystems. We address the problem of travel time prediction in arterial roads\nusing data sampled from probe vehicles. There is only a limited literature on\nmethods using data input from probe vehicles. The spatio-temporal dependencies\ncaptured by existing data driven approaches are either too detailed or very\nsimplistic. We strike a balance of the existing data driven approaches to\naccount for varying degrees of influence a given road may experience from its\nneighbors, while controlling the number of parameters to be learnt.\nSpecifically, we use a NoisyOR conditional probability distribution (CPD) in\nconjunction with a dynamic bayesian network (DBN) to model state transitions of\nvarious roads. We propose an efficient algorithm to learn model parameters. We\npropose an algorithm for predicting travel times on trips of arbitrary\ndurations. Using synthetic and real world data traces we demonstrate the\nsuperior performance of the proposed method under different traffic conditions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 19:31:55 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Achar", "Avinash", ""], ["Sarangan", "Venkatesh", ""], ["Rohith", "R", ""], ["Sivasubramaniam", "Anand", ""]]}, {"id": "1711.05788", "submitter": "Huaiyang Zhong", "authors": "Xiaocheng Li, Huaiyang Zhong, Margaret L. Brandeau", "title": "Quantile Markov Decision Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of a traditional Markov decision process (MDP) is to maximize\nexpected cumulativereward over a defined horizon (possibly infinite). In many\napplications, however, a decision maker may beinterested in optimizing a\nspecific quantile of the cumulative reward instead of its expectation. In this\npaperwe consider the problem of optimizing the quantiles of the cumulative\nrewards of a Markov decision process(MDP), which we refer to as a quantile\nMarkov decision process (QMDP). We provide analytical resultscharacterizing the\noptimal QMDP value function and present a dynamic programming-based algorithm\ntosolve for the optimal policy. The algorithm also extends to the MDP problem\nwith a conditional value-at-risk(CVaR) objective. We illustrate the practical\nrelevance of our model by evaluating it on an HIV treatmentinitiation problem,\nwhere patients aim to balance the potential benefits and risks of the\ntreatment.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 20:24:51 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 22:46:28 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 23:47:35 GMT"}, {"version": "v4", "created": "Tue, 4 Aug 2020 08:33:36 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Li", "Xiaocheng", ""], ["Zhong", "Huaiyang", ""], ["Brandeau", "Margaret L.", ""]]}, {"id": "1711.05816", "submitter": "Francis Jeffry Pelletier", "authors": "Allen P. Hazen and Francis Jeffry Pelletier", "title": "K3, L3, LP, RM3, A3, FDE: How to Make Many-Valued Logics Work for You", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate some well-known (and a few not-so-well-known) many-valued\nlogics that have a small number (3 or 4) of truth values. For some of them we\ncomplain that they do not have any \\emph{logical} use (despite their perhaps\nhaving some intuitive semantic interest) and we look at ways to add features so\nas to make them useful, while retaining their intuitive appeal. At the end, we\nshow some surprising results in the system FDE, and its relationships with\nfeatures of other logics. We close with some new examples of \"synonymous\nlogics.\" An Appendix contains a natural deduction system for our augmented FDE,\nand proofs of soundness and completeness.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 21:40:01 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Hazen", "Allen P.", ""], ["Pelletier", "Francis Jeffry", ""]]}, {"id": "1711.05825", "submitter": "Richard Everitt", "authors": "Richard G. Everitt", "title": "Bootstrapped synthetic likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) and synthetic likelihood (SL)\ntechniques have enabled the use of Bayesian inference for models that may be\nsimulated, but for which the likelihood cannot be evaluated pointwise at values\nof an unknown parameter $\\theta$. The main idea in ABC and SL is to, for\ndifferent values of $\\theta$ (usually chosen using a Monte Carlo algorithm),\nbuild estimates of the likelihood based on simulations from the model\nconditional on $\\theta$. The quality of these estimates determines the\nefficiency of an ABC/SL algorithm. In standard ABC/SL, the only means to\nimprove an estimated likelihood at $\\theta$ is to simulate more times from the\nmodel conditional on $\\theta$, which is infeasible in cases where the simulator\nis computationally expensive. In this paper we describe how to use\nbootstrapping as a means for improving SL estimates whilst using fewer\nsimulations from the model, and also investigate its use in ABC. Further, we\ninvestigate the use of the bag of little bootstraps as a means for applying\nthis approach to large datasets, yielding Monte Carlo algorithms that\naccurately approximate posterior distributions whilst only simulating\nsubsamples of the full data. Examples of the approach applied to i.i.d.,\ntemporal and spatial data are given.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 22:13:48 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 23:16:04 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Everitt", "Richard G.", ""]]}, {"id": "1711.05851", "submitter": "Rajarshi Das", "authors": "Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan\n  Durugkar, Akshay Krishnamurthy, Alex Smola, Andrew McCallum", "title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in\n  Knowledge Bases using Reinforcement Learning", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases (KB), both automatically and manually constructed, are often\nincomplete --- many valid facts can be inferred from the KB by synthesizing\nexisting information. A popular approach to KB completion is to infer new\nrelations by combinatory reasoning over the information found along other paths\nconnecting a pair of entities. Given the enormous size of KBs and the\nexponential number of paths, previous path-based models have considered only\nthe problem of predicting a missing relation given two entities or evaluating\nthe truth of a proposed triple. Additionally, these methods have traditionally\nused random paths between fixed entity pairs or more recently learned to pick\npaths between them. We propose a new algorithm MINERVA, which addresses the\nmuch more difficult and practical task of answering questions where the\nrelation is known, but only one entity. Since random walks are impractical in a\nsetting with combinatorially many destinations from a start node, we present a\nneural reinforcement learning approach which learns how to navigate the graph\nconditioned on the input query to find predictive paths. Empirically, this\napproach obtains state-of-the-art results on several datasets, significantly\noutperforming prior methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 23:45:18 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 06:56:06 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Das", "Rajarshi", ""], ["Dhuliawala", "Shehzaad", ""], ["Zaheer", "Manzil", ""], ["Vilnis", "Luke", ""], ["Durugkar", "Ishan", ""], ["Krishnamurthy", "Akshay", ""], ["Smola", "Alex", ""], ["McCallum", "Andrew", ""]]}, {"id": "1711.05858", "submitter": "Shima Kamyab", "authors": "Shima Kamyab, S. Zohreh Azimifar", "title": "End-to-end 3D shape inverse rendering of different classes of objects\n  from a single input image", "comments": "16 pages, 12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a semi-supervised deep framework is proposed for the problem of\n3D shape inverse rendering from a single 2D input image. The main structure of\nproposed framework consists of unsupervised pre-trained components which\nsignificantly reduce the need to labeled data for training the whole framework.\nusing labeled data has the advantage of achieving to accurate results without\nthe need to predefined assumptions about image formation process. Three main\ncomponents are used in the proposed network: an encoder which maps 2D input\nimage to a representation space, a 3D decoder which decodes a representation to\na 3D structure and a mapping component in order to map 2D to 3D representation.\nThe only part that needs label for training is the mapping part with not too\nmany parameters. The other components in the network can be pre-trained\nunsupervised using only 2D images or 3D data in each case. The way of\nreconstructing 3D shapes in the decoder component, inspired by the model based\nmethods for 3D reconstruction, maps a low dimensional representation to 3D\nshape space with the advantage of extracting the basis vectors of shape space\nfrom training data itself and is not restricted to a small set of examples as\nused in predefined models. Therefore, the proposed framework deals directly\nwith coordinate values of the point cloud representation which leads to achieve\ndense 3D shapes in the output. The experimental results on several benchmark\ndatasets of objects and human faces and comparing with recent similar methods\nshows the power of proposed network in recovering more details from single 2D\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 19:13:57 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Kamyab", "Shima", ""], ["Azimifar", "S. Zohreh", ""]]}, {"id": "1711.05900", "submitter": "Dhanya Sridhar", "authors": "Dhanya Sridhar, Jay Pujara, Lise Getoor", "title": "Using Noisy Extractions to Discover Causal Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases (KB) constructed through information extraction from text\nplay an important role in query answering and reasoning. In this work, we study\na particular reasoning task, the problem of discovering causal relationships\nbetween entities, known as causal discovery. There are two contrasting types of\napproaches to discovering causal knowledge. One approach attempts to identify\ncausal relationships from text using automatic extraction techniques, while the\nother approach infers causation from observational data. However, extractions\nalone are often insufficient to capture complex patterns and full observational\ndata is expensive to obtain. We introduce a probabilistic method for fusing\nnoisy extractions with observational data to discover causal knowledge. We\npropose a principled approach that uses the probabilistic soft logic (PSL)\nframework to encode well-studied constraints to recover long-range patterns and\nconsistent predictions, while cheaply acquired extractions provide a proxy for\nunseen observations. We apply our method gene regulatory networks and show the\npromise of exploiting KB signals in causal discovery, suggesting a critical,\nnew area of research.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 02:57:00 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Sridhar", "Dhanya", ""], ["Pujara", "Jay", ""], ["Getoor", "Lise", ""]]}, {"id": "1711.05905", "submitter": "Yijia Wang", "authors": "Yijia Wang, Yan Wan and Zhijian Wang", "title": "Using experimental game theory to transit human values to ethical AI", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing the reflection of game theory and ethics, we develop a mathematical\nrepresentation to bridge the gap between the concepts in moral philosophy\n(e.g., Kantian and Utilitarian) and AI ethics industry technology standard\n(e.g., IEEE P7000 standard series for Ethical AI). As an application, we\ndemonstrate how human value can be obtained from the experimental game theory\n(e.g., trust game experiment) so as to build an ethical AI. Moreover, an\napproach to test the ethics (rightness or wrongness) of a given AI algorithm by\nusing an iterated Prisoner's Dilemma Game experiment is discussed as an\nexample. Compared with existing mathematical frameworks and testing method on\nAI ethics technology, the advantages of the proposed approach are analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 03:30:29 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Wang", "Yijia", ""], ["Wan", "Yan", ""], ["Wang", "Zhijian", ""]]}, {"id": "1711.05928", "submitter": "Datong-Paul Zhou", "authors": "Datong P. Zhou, Claire J. Tomlin", "title": "Budget-Constrained Multi-Armed Bandits with Multiple Plays", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multi-armed bandit problem with multiple plays and a budget\nconstraint for both the stochastic and the adversarial setting. At each round,\nexactly $K$ out of $N$ possible arms have to be played (with $1\\leq K \\leq N$).\nIn addition to observing the individual rewards for each arm played, the player\nalso learns a vector of costs which has to be covered with an a-priori defined\nbudget $B$. The game ends when the sum of current costs associated with the\nplayed arms exceeds the remaining budget.\n  Firstly, we analyze this setting for the stochastic case, for which we assume\neach arm to have an underlying cost and reward distribution with support\n$[c_{\\min}, 1]$ and $[0, 1]$, respectively. We derive an Upper Confidence Bound\n(UCB) algorithm which achieves $O(NK^4 \\log B)$ regret.\n  Secondly, for the adversarial case in which the entire sequence of rewards\nand costs is fixed in advance, we derive an upper bound on the regret of order\n$O(\\sqrt{NB\\log(N/K)})$ utilizing an extension of the well-known\n$\\texttt{Exp3}$ algorithm. We also provide upper bounds that hold with high\nprobability and a lower bound of order $\\Omega((1 - K/N)^2 \\sqrt{NB/K})$.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 05:07:34 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Zhou", "Datong P.", ""], ["Tomlin", "Claire J.", ""]]}, {"id": "1711.06004", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel", "title": "Remedies against the Vocabulary Gap in Information Retrieval", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines rely heavily on term-based approaches that represent queries\nand documents as bags of words. Text---a document or a query---is represented\nby a bag of its words that ignores grammar and word order, but retains word\nfrequency counts. When presented with a search query, the engine then ranks\ndocuments according to their relevance scores by computing, among other things,\nthe matching degrees between query and document terms. While term-based\napproaches are intuitive and effective in practice, they are based on the\nhypothesis that documents that exactly contain the query terms are highly\nrelevant regardless of query semantics. Inversely, term-based approaches assume\ndocuments that do not contain query terms as irrelevant. However, it is known\nthat a high matching degree at the term level does not necessarily mean high\nrelevance and, vice versa, documents that match null query terms may still be\nrelevant. Consequently, there exists a vocabulary gap between queries and\ndocuments that occurs when both use different words to describe the same\nconcepts. It is the alleviation of the effect brought forward by this\nvocabulary gap that is the topic of this dissertation. More specifically, we\npropose (1) methods to formulate an effective query from complex textual\nstructures and (2) latent vector space models that circumvent the vocabulary\ngap in information retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 09:50:52 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Van Gysel", "Christophe", ""]]}, {"id": "1711.06006", "submitter": "Paulo Rauber", "authors": "Paulo Rauber, Avinash Ummadisingu, Filipe Mutz, Juergen Schmidhuber", "title": "Hindsight policy gradients", "comments": "Accepted to ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reinforcement learning agent that needs to pursue different goals across\nepisodes requires a goal-conditional policy. In addition to their potential to\ngeneralize desirable behavior to unseen goals, such policies may also enable\nhigher-level planning based on subgoals. In sparse-reward environments, the\ncapacity to exploit information about the degree to which an arbitrary goal has\nbeen achieved while another goal was intended appears crucial to enable sample\nefficient learning. However, reinforcement learning agents have only recently\nbeen endowed with such capacity for hindsight. In this paper, we demonstrate\nhow hindsight can be introduced to policy gradient methods, generalizing this\nidea to a broad class of successful algorithms. Our experiments on a diverse\nselection of sparse-reward environments show that hindsight leads to a\nremarkable increase in sample efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 10:05:31 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 14:11:06 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2019 10:46:44 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Rauber", "Paulo", ""], ["Ummadisingu", "Avinash", ""], ["Mutz", "Filipe", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1711.06030", "submitter": "Barton Lee", "authors": "Haris Aziz and Barton E. Lee", "title": "Sub-committee Approval Voting and Generalised Justified Representation\n  Axioms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social choice is replete with various settings including single-winner\nvoting, multi-winner voting, probabilistic voting, multiple referenda, and\npublic decision making. We study a general model of social choice called\nSub-Committee Voting (SCV) that simultaneously generalizes these settings. We\nthen focus on sub-committee voting with approvals and propose extensions of the\njustified representation axioms that have been considered for proportional\nrepresentation in approval-based committee voting. We study the properties and\nrelations of these axioms. For each of the axioms, we analyse whether a\nrepresentative committee exists and also examine the complexity of computing\nand verifying such a committee.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 11:24:39 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Aziz", "Haris", ""], ["Lee", "Barton E.", ""]]}, {"id": "1711.06035", "submitter": "Martijn Van Otterlo", "authors": "Martijn van Otterlo", "title": "From Algorithmic Black Boxes to Adaptive White Boxes: Declarative\n  Decision-Theoretic Ethical Programs as Codes of Ethics", "comments": "7 pages, 1 figure, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethics of algorithms is an emerging topic in various disciplines such as\nsocial science, law, and philosophy, but also artificial intelligence (AI). The\nvalue alignment problem expresses the challenge of (machine) learning values\nthat are, in some way, aligned with human requirements or values. In this paper\nI argue for looking at how humans have formalized and communicated values, in\nprofessional codes of ethics, and for exploring declarative decision-theoretic\nethical programs (DDTEP) to formalize codes of ethics. This renders machine\nethical reasoning and decision-making, as well as learning, more transparent\nand hopefully more accountable. The paper includes proof-of-concept examples of\nknown toy dilemmas and gatekeeping domains such as archives and libraries.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 11:29:54 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["van Otterlo", "Martijn", ""]]}, {"id": "1711.06128", "submitter": "Ho-Pun Lam", "authors": "Ho-Pun Lam and Mustafa Hashmi", "title": "Enabling Reasoning with LegalRuleML", "comments": "25 pages. Under consideration for publication in Theory and Practice\n  of Logic Programming (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In order to automate verification process, regulatory rules written in\nnatural language need to be translated into a format that machines can\nunderstand. However, none of the existing formalisms can fully represent the\nelements that appear in legal norms. For instance, most of these formalisms do\nnot provide features to capture the behavior of deontic effects, which is an\nimportant aspect in automated compliance checking. This paper presents an\napproach for transforming legal norms represented using LegalRuleML to a\nvariant of Modal Defeasible Logic (and vice versa) such that a legal statement\nrepresented using LegalRuleML can be transformed into a machine-readable format\nthat can be understood and reasoned about depending upon the client's\npreferences.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 05:00:58 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 05:15:32 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Lam", "Ho-Pun", ""], ["Hashmi", "Mustafa", ""]]}, {"id": "1711.06202", "submitter": "Simone Silvetti", "authors": "Laura Nenzi, Simone Silvetti, Ezio Bartocci and Luca Bortolussi", "title": "A Robust Genetic Algorithm for Learning Temporal Specifications from\n  Data", "comments": "16 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of mining signal temporal logical requirements from a\ndataset of regular (good) and anomalous (bad) trajectories of a dynamical\nsystem. We assume the training set to be labeled by human experts and that we\nhave access only to a limited amount of data, typically noisy. We provide a\nsystematic approach to synthesize both the syntactical structure and the\nparameters of the temporal logic formula using a two-steps procedure: first, we\nleverage a novel evolutionary algorithm for learning the structure of the\nformula; second, we perform the parameter synthesis operating on the\nstatistical emulation of the average robustness for a candidate formula w.r.t.\nits parameters. We compare our results with our previous work [{BufoBSBLB14]\nand with a recently proposed decision-tree [bombara_decision_2016] based\nmethod. We present experimental results on two case studies: an anomalous\ntrajectory detection problem of a naval surveillance system and the\ncharacterization of an Ineffective Respiratory effort, showing the usefulness\nof our work.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:31:08 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 15:46:56 GMT"}, {"version": "v3", "created": "Wed, 1 Aug 2018 10:19:24 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Nenzi", "Laura", ""], ["Silvetti", "Simone", ""], ["Bartocci", "Ezio", ""], ["Bortolussi", "Luca", ""]]}, {"id": "1711.06299", "submitter": "Pieter Libin", "authors": "Pieter Libin, Timothy Verstraeten, Diederik M. Roijers, Jelena Grujic,\n  Kristof Theys, Philippe Lemey, Ann Now\\'e", "title": "Bayesian Best-Arm Identification for Selecting Influenza Mitigation\n  Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pandemic influenza has the epidemic potential to kill millions of people.\nWhile various preventive measures exist (i.a., vaccination and school\nclosures), deciding on strategies that lead to their most effective and\nefficient use remains challenging. To this end, individual-based\nepidemiological models are essential to assist decision makers in determining\nthe best strategy to curb epidemic spread. However, individual-based models are\ncomputationally intensive and it is therefore pivotal to identify the optimal\nstrategy using a minimal amount of model evaluations. Additionally, as\nepidemiological modeling experiments need to be planned, a computational budget\nneeds to be specified a priori. Consequently, we present a new sampling\ntechnique to optimize the evaluation of preventive strategies using fixed\nbudget best-arm identification algorithms. We use epidemiological modeling\ntheory to derive knowledge about the reward distribution which we exploit using\nBayesian best-arm identification algorithms (i.e., Top-two Thompson sampling\nand BayesGap). We evaluate these algorithms in a realistic experimental setting\nand demonstrate that it is possible to identify the optimal strategy using only\na limited number of model evaluations, i.e., 2-to-3 times faster compared to\nthe uniform sampling method, the predominant technique used for epidemiological\ndecision making in the literature. Finally, we contribute and evaluate a\nstatistic for Top-two Thompson sampling to inform the decision makers about the\nconfidence of an arm recommendation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 19:40:10 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 12:06:19 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Libin", "Pieter", ""], ["Verstraeten", "Timothy", ""], ["Roijers", "Diederik M.", ""], ["Grujic", "Jelena", ""], ["Theys", "Kristof", ""], ["Lemey", "Philippe", ""], ["Now\u00e9", "Ann", ""]]}, {"id": "1711.06301", "submitter": "Yuan Yang", "authors": "Yuan Yang", "title": "One Model for the Learning of Language", "comments": "This is a draft write-up of an undergraduate project. A full journal\n  version is still under preparation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major target of linguistics and cognitive science has been to understand\nwhat class of learning systems can acquire the key structures of natural\nlanguage. Until recently, the computational requirements of language have been\nused to argue that learning is impossible without a highly constrained\nhypothesis space. Here, we describe a learning system that is maximally\nunconstrained, operating over the space of all computations, and is able to\nacquire several of the key structures present natural language from positive\nevidence alone. The model successfully acquires regular (e.g. $(ab)^n$),\ncontext-free (e.g. $a^n b^n$, $x x^R$), and context-sensitive (e.g.\n$a^nb^nc^n$, $a^nb^mc^nd^m$, $xx$) formal languages. Our approach develops the\nconcept of factorized programs in Bayesian program induction in order to help\nmanage the complexity of representation. We show in learning, the model\npredicts several phenomena empirically observed in human grammar acquisition\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 19:41:15 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 18:15:06 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Yang", "Yuan", ""]]}, {"id": "1711.06317", "submitter": "Ehsan Hemmati", "authors": "Mansour Sheikhan, Ehsan Hemmati, Reza Shahnazi", "title": "GA-PSO-Optimized Neural-Based Control Scheme for Adaptive Congestion\n  Control to Improve Performance in Multimedia Applications", "comments": "arXiv admin note: text overlap with arXiv:1711.06356", "journal-ref": "Majlesi Journal of Electrical Engineering, [S.l.], v. 6, n. 1,\n  jan. 2012", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active queue control aims to improve the overall communication network\nthroughput while providing lower delay and small packet loss rate. The basic\nidea is to actively trigger packet dropping (or marking provided by explicit\ncongestion notification (ECN)) before buffer overflow. In this paper, two\nartificial neural networks (ANN)-based control schemes are proposed for\nadaptive queue control in TCP communication networks. The structure of these\ncontrollers is optimized using genetic algorithm (GA) and the output weights of\nANNs are optimized using particle swarm optimization (PSO) algorithm. The\ncontrollers are radial bias function (RBF)-based, but to improve the robustness\nof RBF controller, an error-integral term is added to RBF equation in the\nsecond scheme. Experimental results show that GA- PSO-optimized improved RBF\n(I-RBF) model controls network congestion effectively in terms of link\nutilization with a low packet loss rate and outperform Drop Tail,\nproportional-integral (PI), random exponential marking (REM), and adaptive\nrandom early detection (ARED) controllers.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 20:52:37 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Sheikhan", "Mansour", ""], ["Hemmati", "Ehsan", ""], ["Shahnazi", "Reza", ""]]}, {"id": "1711.06347", "submitter": "Daniel Karapetyan Dr", "authors": "Daniel Karapetyan and Boris Goldengorin", "title": "Conditional Markov Chain Search for the Simple Plant Location Problem\n  improves upper bounds on twelve K\\\"orkel-Ghosh instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a family of hard benchmark instances for the Simple Plant Location\nProblem (also known as the Uncapacitated Facility Location Problem). The recent\nattempt by Fischetti et al. to tackle the K\\\"orkel-Ghosh instances resulted in\nseven new optimal solutions and 22 improved upper bounds. We use automated\ngeneration of heuristics to obtain a new algorithm for the Simple Plant\nLocation Problem. In our experiments, our new algorithm matched all the\nprevious best known and optimal solutions, and further improved 12 upper\nbounds, all within shorter time budgets compared to the previous efforts.\n  Our algorithm design process is split into two phases: (i) development of\nalgorithmic components such as local search procedures and mutation operators,\nand (ii) composition of a metaheuristic from the available components. Phase\n(i) requires human expertise and often can be completed by implementing several\nsimple domain-specific routines known from the literature. Phase (ii) is\nentirely automated by employing the Conditional Markov Chain Search (CMCS)\nframework. In CMCS, a metaheuristic is flexibly defined by a set of parameters,\ncalled configuration. Then the process of composition of a metaheuristic from\nthe algorithmic components is reduced to an optimisation problem seeking the\nbest performing CMCS configuration.\n  We discuss the problem of comparing configurations, and propose a new\nefficient technique to select the best performing configuration from a large\nset. To employ this method, we restrict the original CMCS to a simple\ndeterministic case that leaves us with a finite and manageable number of\nmeaningful configurations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 23:00:42 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Karapetyan", "Daniel", ""], ["Goldengorin", "Boris", ""]]}, {"id": "1711.06350", "submitter": "Mirco Musolesi", "authors": "Gatis Mikelsons and Matthew Smith and Abhinav Mehrotra and Mirco\n  Musolesi", "title": "Towards Deep Learning Models for Psychological State Prediction using\n  Smartphone Data: Challenges and Opportunities", "comments": "6 pages, 2 figures, In Proceedings of the NIPS Workshop on Machine\n  Learning for Healthcare 2017 (ML4H 2017). Colocated with NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in exploiting mobile sensing technologies and\nmachine learning techniques for mental health monitoring and intervention.\nResearchers have effectively used contextual information, such as mobility,\ncommunication and mobile phone usage patterns for quantifying individuals' mood\nand wellbeing. In this paper, we investigate the effectiveness of neural\nnetwork models for predicting users' level of stress by using the location\ninformation collected by smartphones. We characterize the mobility patterns of\nindividuals using the GPS metrics presented in the literature and employ these\nmetrics as input to the network. We evaluate our approach on the open-source\nStudentLife dataset. Moreover, we discuss the challenges and trade-offs\ninvolved in building machine learning models for digital mental health and\nhighlight potential future work in this direction.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 23:18:03 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Mikelsons", "Gatis", ""], ["Smith", "Matthew", ""], ["Mehrotra", "Abhinav", ""], ["Musolesi", "Mirco", ""]]}, {"id": "1711.06351", "submitter": "Anselm Rothe", "authors": "Anselm Rothe, Brenden M. Lake, Todd M. Gureckis", "title": "Question Asking as Program Generation", "comments": "Published in Advances in Neural Information Processing Systems (NIPS)\n  30, December 2017", "journal-ref": "Rothe, A., Lake, B. M., and Gureckis, T. M. (2017). Question\n  asking as program generation. Advances in Neural Information Processing\n  Systems 30", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hallmark of human intelligence is the ability to ask rich, creative, and\nrevealing questions. Here we introduce a cognitive model capable of\nconstructing human-like questions. Our approach treats questions as formal\nprograms that, when executed on the state of the world, output an answer. The\nmodel specifies a probability distribution over a complex, compositional space\nof programs, favoring concise programs that help the agent learn in the current\ncontext. We evaluate our approach by modeling the types of open-ended questions\ngenerated by humans who were attempting to learn about an ambiguous situation\nin a game. We find that our model predicts what questions people will ask, and\ncan creatively produce novel questions that were not present in the training\nset. In addition, we compare a number of model variants, finding that both\nquestion informativeness and complexity are important for producing human-like\nquestions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 23:27:04 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Rothe", "Anselm", ""], ["Lake", "Brenden M.", ""], ["Gureckis", "Todd M.", ""]]}, {"id": "1711.06362", "submitter": "David Narv\\'aez", "authors": "David E. Narv\\'aez", "title": "Exploring the Use of Shatter for AllSAT Through Ramsey-Type Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the context of SAT solvers, Shatter is a popular tool for symmetry\nbreaking on CNF formulas. Nevertheless, little has been said about its use in\nthe context of AllSAT problems: problems where we are interested in listing all\nthe models of a Boolean formula. AllSAT has gained much popularity in recent\nyears due to its many applications in domains like model checking, data mining,\netc. One example of a particularly transparent application of AllSAT to other\nfields of computer science is computational Ramsey theory. In this paper we\nstudy the effect of incorporating Shatter to the workflow of using Boolean\nformulas to generate all possible edge colorings of a graph avoiding prescribed\nmonochromatic subgraphs. Generating complete sets of colorings is an important\nbuilding block in computational Ramsey theory. We identify two drawbacks in the\nna\\\"ive use of Shatter to break the symmetries of Boolean formulas encoding\nRamsey-type problems for graphs: a \"blow-up\" in the number of models and the\ngeneration of incomplete sets of colorings. The issues presented in this work\nare not intended to discourage the use of Shatter as a preprocessing tool for\nAllSAT problems in combinatorial computing but to help researchers properly use\nthis tool by avoiding these potential pitfalls. To this end, we provide\nstrategies and additional tools to cope with the negative effects of using\nShatter for AllSAT. While the specific application addressed in this paper is\nthat of Ramsey-type problems, the analysis we carry out applies to many other\nareas in which highly-symmetrical Boolean formulas arise and we wish to find\nall of their models.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 00:50:36 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Narv\u00e1ez", "David E.", ""]]}, {"id": "1711.06363", "submitter": "Renato Hermoza Aragon\\'es", "authors": "Renato Hermoza and Ivan Sipiran", "title": "3D Reconstruction of Incomplete Archaeological Objects Using a\n  Generative Adversarial Network", "comments": "6 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data-driven approach to aid the repairing and conservation of\narchaeological objects: ORGAN, an object reconstruction generative adversarial\nnetwork (GAN). By using an encoder-decoder 3D deep neural network on a GAN\narchitecture, and combining two loss objectives: a completion loss and an\nImproved Wasserstein GAN loss, we can train a network to effectively predict\nthe missing geometry of damaged objects. As archaeological objects can greatly\ndiffer between them, the network is conditioned on a variable, which can be a\nculture, a region or any metadata of the object. In our results, we show that\nour method can recover most of the information from damaged objects, even in\ncases where more than half of the voxels are missing, without producing many\nerrors.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 00:58:53 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 18:12:27 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Hermoza", "Renato", ""], ["Sipiran", "Ivan", ""]]}, {"id": "1711.06431", "submitter": "Housam Khalifa Bashier Babiker", "authors": "Housam Khalifa Bashier Babiker and Randy Goebel", "title": "Using KL-divergence to focus Deep Visual Explanation", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for explaining the image classification predictions of\ndeep convolution neural networks, by highlighting the pixels in the image which\ninfluence the final class prediction. Our method requires the identification of\na heuristic method to select parameters hypothesized to be most relevant in\nthis prediction, and here we use Kullback-Leibler divergence to provide this\nfocus. Overall, our approach helps in understanding and interpreting deep\nnetwork predictions and we hope contributes to a foundation for such\nunderstanding of deep learning networks. In this brief paper, our experiments\nevaluate the performance of two popular networks in this context of\ninterpretability.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 06:53:17 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 06:18:18 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Babiker", "Housam Khalifa Bashier", ""], ["Goebel", "Randy", ""]]}, {"id": "1711.06498", "submitter": "Victoria Hodge", "authors": "Victoria Hodge, Sam Devlin, Nick Sephton, Florian Block, Anders\n  Drachen and Peter Cowling", "title": "Win Prediction in Esports: Mixed-Rank Match Prediction in Multi-player\n  Online Battle Arena Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Esports has emerged as a popular genre for players as well as spectators,\nsupporting a global entertainment industry. Esports analytics has evolved to\naddress the requirement for data-driven feedback, and is focused on\ncyber-athlete evaluation, strategy and prediction. Towards the latter, previous\nwork has used match data from a variety of player ranks from hobbyist to\nprofessional players. However, professional players have been shown to behave\ndifferently than lower ranked players. Given the comparatively limited supply\nof professional data, a key question is thus whether mixed-rank match datasets\ncan be used to create data-driven models which predict winners in professional\nmatches and provide a simple in-game statistic for viewers and broadcasters.\nHere we show that, although there is a slightly reduced accuracy, mixed-rank\ndatasets can be used to predict the outcome of professional matches, with\nsuitably optimized configurations.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 11:18:31 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Hodge", "Victoria", ""], ["Devlin", "Sam", ""], ["Sephton", "Nick", ""], ["Block", "Florian", ""], ["Drachen", "Anders", ""], ["Cowling", "Peter", ""]]}, {"id": "1711.06517", "submitter": "Moshe BenBassat Professor", "authors": "Moshe BenBassat", "title": "Wikipedia for Smart Machines and Double Deep Machine Learning", "comments": "10 pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very important breakthroughs in data centric deep learning algorithms led to\nimpressive performance in transactional point applications of Artificial\nIntelligence (AI) such as Face Recognition, or EKG classification. With all due\nappreciation, however, knowledge blind data only machine learning algorithms\nhave severe limitations for non-transactional AI applications, such as medical\ndiagnosis beyond the EKG results. Such applications require deeper and broader\nknowledge in their problem solving capabilities, e.g. integrating anatomy and\nphysiology knowledge with EKG results and other patient findings. Following a\nreview and illustrations of such limitations for several real life AI\napplications, we point at ways to overcome them. The proposed Wikipedia for\nSmart Machines initiative aims at building repositories of software structures\nthat represent humanity science & technology knowledge in various parts of\nlife; knowledge that we all learn in schools, universities and during our\nprofessional life. Target readers for these repositories are smart machines;\nnot human. AI software developers will have these Reusable Knowledge structures\nreadily available, hence, the proposed name ReKopedia. Big Data is by now a\nmature technology, it is time to focus on Big Knowledge. Some will be derived\nfrom data, some will be obtained from mankind gigantic repository of knowledge.\nWikipedia for smart machines along with the new Double Deep Learning approach\noffer a paradigm for integrating datacentric deep learning algorithms with\nalgorithms that leverage deep knowledge, e.g. evidential reasoning and\ncausality reasoning. For illustration, a project is described to produce\nReKopedia knowledge modules for medical diagnosis of about 1,000 disorders.\nData is important, but knowledge deep, basic, and commonsense is equally\nimportant.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 12:59:22 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 05:54:17 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["BenBassat", "Moshe", ""]]}, {"id": "1711.06550", "submitter": "Pouya Ghaemmaghami Tabrizi", "authors": "Pouya Ghaemmaghami", "title": "Reconstruction of the External Stimuli from Brain Signals", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the rapid advances in Brain-computer Interfacing (BCI) and continuous\neffort to improve the accuracy of brain decoding systems, the urge for the\nsystems to reconstruct the experiences of the users has been widely\nacknowledged. This urge has been investigated by some researchers during the\npast years in terms of reconstruction of the naturalistic images, abstract\nimages, video and audio. In this study, we try to tackle this issue by\nregressing the stimuli spectrogram using the spectrogram analysis of the brain\nsignals. The results of our regression-based method suggest the feasibility of\nsuch reconstructions using the neuroimaging techniques that are appropriate for\nout-of-lab scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 16:52:48 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Ghaemmaghami", "Pouya", ""]]}, {"id": "1711.06562", "submitter": "Joose Rajam\\\"aki", "authors": "Joose Rajam\\\"aki and Perttu H\\\"am\\\"al\\\"ainen", "title": "An Iterative Closest Points Approach to Neural Generative Models", "comments": "Indexing errors have been corrected to this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple way to learn a transformation that maps samples of one\ndistribution to the samples of another distribution. Our algorithm comprises an\niteration of 1) drawing samples from some simple distribution and transforming\nthem using a neural network, 2) determining pairwise correspondences between\nthe transformed samples and training data (or a minibatch), and 3) optimizing\nthe weights of the neural network being trained to minimize the distances\nbetween the corresponding vectors. This can be considered as a variant of the\nIterative Closest Points (ICP) algorithm, common in geometric computer vision,\nalthough ICP typically operates on sensor point clouds and linear transforms\ninstead of random sample sets and neural nonlinear transforms. We demonstrate\nthe algorithm on simple synthetic data and MNIST data. We furthermore\ndemonstrate that the algorithm is capable of handling distributions with both\ncontinuous and discrete variables.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 08:07:47 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 11:32:46 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2018 15:21:40 GMT"}, {"version": "v4", "created": "Mon, 2 Jul 2018 06:23:01 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Rajam\u00e4ki", "Joose", ""], ["H\u00e4m\u00e4l\u00e4inen", "Perttu", ""]]}, {"id": "1711.06583", "submitter": "Pawe{\\l} Liskowski", "authors": "Pawe{\\l} Liskowski, Wojciech Ja\\'skowski, Krzysztof Krawiec", "title": "Learning to Play Othello with Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TG.2018.2799997", "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving superhuman playing level by AlphaGo corroborated the capabilities\nof convolutional neural architectures (CNNs) for capturing complex spatial\npatterns. This result was to a great extent due to several analogies between Go\nboard states and 2D images CNNs have been designed for, in particular\ntranslational invariance and a relatively large board. In this paper, we verify\nwhether CNN-based move predictors prove effective for Othello, a game with\nsignificantly different characteristics, including a much smaller board size\nand complete lack of translational invariance. We compare several CNN\narchitectures and board encodings, augment them with state-of-the-art\nextensions, train on an extensive database of experts' moves, and examine them\nwith respect to move prediction accuracy and playing strength. The empirical\nevaluation confirms high capabilities of neural move predictors and suggests a\nstrong correlation between prediction accuracy and playing strength. The best\nCNNs not only surpass all other 1-ply Othello players proposed to date but\ndefeat (2-ply) Edax, the best open-source Othello player.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 15:14:20 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Liskowski", "Pawe\u0142", ""], ["Ja\u015bkowski", "Wojciech", ""], ["Krawiec", "Krzysztof", ""]]}, {"id": "1711.06588", "submitter": "Osamu Hirose", "authors": "Osamu Hirose", "title": "Dependent landmark drift: robust point set registration with a Gaussian\n  mixture model and a statistical shape model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of point set registration is to find point-by-point correspondences\nbetween point sets, each of which characterizes the shape of an object. Because\nlocal preservation of object geometry is assumed, prevalent algorithms in the\narea can often elegantly solve the problems without using geometric information\nspecific to the objects. This means that registration performance can be\nfurther improved by using prior knowledge of object geometry. In this paper, we\npropose a novel point set registration method using the Gaussian mixture model\nwith prior shape information encoded as a statistical shape model. Our\ntransformation model is defined as a combination of the similar transformation,\nmotion coherence, and the statistical shape model. Therefore, the proposed\nmethod works effectively if the target point set includes outliers and missing\nregions, or if it is rotated. The computational cost can be reduced to linear,\nand therefore the method is scalable to large point sets. The effectiveness of\nthe method will be verified through comparisons with existing algorithms using\ndatasets concerning human body shapes, hands, and faces.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 15:24:17 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 01:11:50 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 01:00:59 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Hirose", "Osamu", ""]]}, {"id": "1711.06605", "submitter": "Francesco Corucci", "authors": "Francesco Corucci, Nick Cheney, Francesco Giorgio-Serchi, Josh Bongard\n  and Cecilia Laschi", "title": "Evolving soft locomotion in aquatic and terrestrial environments:\n  effects of material properties and environmental transitions", "comments": "37 pages, 22 figures, currently under review (journal)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing soft robots poses considerable challenges: automated design\napproaches may be particularly appealing in this field, as they promise to\noptimize complex multi-material machines with very little or no human\nintervention. Evolutionary soft robotics is concerned with the application of\noptimization algorithms inspired by natural evolution in order to let soft\nrobots (both morphologies and controllers) spontaneously evolve within\nphysically-realistic simulated environments, figuring out how to satisfy a set\nof objectives defined by human designers. In this paper a powerful evolutionary\nsystem is put in place in order to perform a broad investigation on the\nfree-form evolution of walking and swimming soft robots in different\nenvironments. Three sets of experiments are reported, tackling different\naspects of the evolution of soft locomotion. The first two sets explore the\neffects of different material properties on the evolution of terrestrial and\naquatic soft locomotion: particularly, we show how different materials lead to\nthe evolution of different morphologies, behaviors, and energy-performance\ntradeoffs. It is found that within our simplified physics world stiffer robots\nevolve more sophisticated and effective gaits and morphologies on land, while\nsofter ones tend to perform better in water. The third set of experiments\nstarts investigating the effect and potential benefits of major environmental\ntransitions (land - water) during evolution. Results provide interesting\nmorphological exaptation phenomena, and point out a potential asymmetry between\nland-water and water-land transitions: while the first type of transition\nappears to be detrimental, the second one seems to have some beneficial\neffects.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 16:01:27 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Corucci", "Francesco", ""], ["Cheney", "Nick", ""], ["Giorgio-Serchi", "Francesco", ""], ["Bongard", "Josh", ""], ["Laschi", "Cecilia", ""]]}, {"id": "1711.06623", "submitter": "Dan Barnes", "authors": "Dan Barnes, Will Maddern, Geoffrey Pascoe and Ingmar Posner", "title": "Driven to Distraction: Self-Supervised Distractor Learning for Robust\n  Monocular Visual Odometry in Urban Environments", "comments": "International Conference on Robotics and Automation (ICRA), 2018.\n  Video summary: http://youtu.be/ebIrBn_nc-k", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised approach to ignoring \"distractors\" in camera\nimages for the purposes of robustly estimating vehicle motion in cluttered\nurban environments. We leverage offline multi-session mapping approaches to\nautomatically generate a per-pixel ephemerality mask and depth map for each\ninput image, which we use to train a deep convolutional network. At run-time we\nuse the predicted ephemerality and depth as an input to a monocular visual\nodometry (VO) pipeline, using either sparse features or dense photometric\nmatching. Our approach yields metric-scale VO using only a single camera and\ncan recover the correct egomotion even when 90% of the image is obscured by\ndynamic, independently moving objects. We evaluate our robust VO methods on\nmore than 400km of driving from the Oxford RobotCar Dataset and demonstrate\nreduced odometry drift and significantly improved egomotion estimation in the\npresence of large moving vehicles in urban traffic.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 16:54:40 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 14:29:23 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Barnes", "Dan", ""], ["Maddern", "Will", ""], ["Pascoe", "Geoffrey", ""], ["Posner", "Ingmar", ""]]}, {"id": "1711.06632", "submitter": "Chang Zhou", "authors": "Chang Zhou, Jinze Bai, Junshuai Song, Xiaofei Liu, Zhengchao Zhao,\n  Xiusi Chen, Jun Gao", "title": "ATRank: An Attention-Based User Behavior Modeling Framework for\n  Recommendation", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A user can be represented as what he/she does along the history. A common way\nto deal with the user modeling problem is to manually extract all kinds of\naggregated features over the heterogeneous behaviors, which may fail to fully\nrepresent the data itself due to limited human instinct. Recent works usually\nuse RNN-based methods to give an overall embedding of a behavior sequence,\nwhich then could be exploited by the downstream applications. However, this can\nonly preserve very limited information, or aggregated memories of a person.\nWhen a downstream application requires to facilitate the modeled user features,\nit may lose the integrity of the specific highly correlated behavior of the\nuser, and introduce noises derived from unrelated behaviors. This paper\nproposes an attention based user behavior modeling framework called ATRank,\nwhich we mainly use for recommendation tasks. Heterogeneous user behaviors are\nconsidered in our model that we project all types of behaviors into multiple\nlatent semantic spaces, where influence can be made among the behaviors via\nself-attention. Downstream applications then can use the user behavior vectors\nvia vanilla attention. Experiments show that ATRank can achieve better\nperformance and faster training process. We further explore ATRank to use one\nunified model to predict different types of user behaviors at the same time,\nshowing a comparable performance with the highly optimized individual models.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 17:15:53 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 06:57:20 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Zhou", "Chang", ""], ["Bai", "Jinze", ""], ["Song", "Junshuai", ""], ["Liu", "Xiaofei", ""], ["Zhao", "Zhengchao", ""], ["Chen", "Xiusi", ""], ["Gao", "Jun", ""]]}, {"id": "1711.06677", "submitter": "Johanni Brea", "authors": "Johanni Brea", "title": "Is prioritized sweeping the better episodic control?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Episodic control has been proposed as a third approach to reinforcement\nlearning, besides model-free and model-based control, by analogy with the three\ntypes of human memory. i.e. episodic, procedural and semantic memory. But the\ntheoretical properties of episodic control are not well investigated. Here I\nshow that in deterministic tree Markov decision processes, episodic control is\nequivalent to a form of prioritized sweeping in terms of sample efficiency as\nwell as memory and computation demands. For general deterministic and\nstochastic environments, prioritized sweeping performs better even when memory\nand computation demands are restricted to be equal to those of episodic\ncontrol. These results suggest generalizations of prioritized sweeping to\npartially observable environments, its combined use with function approximation\nand the search for possible implementations of prioritized sweeping in brains.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 07:47:12 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 20:25:43 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Brea", "Johanni", ""]]}, {"id": "1711.06744", "submitter": "Ni Lao", "authors": "Fan Yang, Jiazhong Nie, William W. Cohen, Ni Lao", "title": "Learning to Organize Knowledge and Answer Questions with N-Gram Machines", "comments": "Presented at ICLR 2018 workshop\n  https://iclr.cc/Conferences/2018/Schedule?showEvent=580", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep neural networks have great success in natural language\nprocessing, they are limited at more knowledge intensive AI tasks, such as\nopen-domain Question Answering (QA). Existing end-to-end deep QA models need to\nprocess the entire text after observing the question, and therefore their\ncomplexity in responding a question is linear in the text size. This is\nprohibitive for practical tasks such as QA from Wikipedia, a novel, or the Web.\nWe propose to solve this scalability issue by using symbolic meaning\nrepresentations, which can be indexed and retrieved efficiently with complexity\nthat is independent of the text size. We apply our approach, called the N-Gram\nMachine (NGM), to three representative tasks. First as proof-of-concept, we\ndemonstrate that NGM successfully solves the bAbI tasks of synthetic text.\nSecond, we show that NGM scales to large corpus by experimenting on \"life-long\nbAbI\", a special version of bAbI that contains millions of sentences. Lastly on\nthe WikiMovies dataset, we use NGM to induce latent structure (i.e. schema) and\nanswer questions from natural language Wikipedia text, with only QA pairs as\nweak supervision.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 22:02:53 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 22:59:15 GMT"}, {"version": "v3", "created": "Sun, 1 Jul 2018 07:09:45 GMT"}, {"version": "v4", "created": "Sun, 3 Mar 2019 02:42:26 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Yang", "Fan", ""], ["Nie", "Jiazhong", ""], ["Cohen", "William W.", ""], ["Lao", "Ni", ""]]}, {"id": "1711.06761", "submitter": "Matthew Riemer", "authors": "Matthew Riemer, Tim Klinger, Djallel Bouneffouf, Michele Franceschini", "title": "Scalable Recollections for Continual Lifelong Learning", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the recent success of Deep Learning applied to a variety of single\ntasks, it is natural to consider more human-realistic settings. Perhaps the\nmost difficult of these settings is that of continual lifelong learning, where\nthe model must learn online over a continuous stream of non-stationary data. A\nsuccessful continual lifelong learning system must have three key capabilities:\nit must learn and adapt over time, it must not forget what it has learned, and\nit must be efficient in both training time and memory. Recent techniques have\nfocused their efforts primarily on the first two capabilities while questions\nof efficiency remain largely unexplored. In this paper, we consider the problem\nof efficient and effective storage of experiences over very large time-frames.\nIn particular we consider the case where typical experiences are O(n) bits and\nmemories are limited to O(k) bits for k << n. We present a novel scalable\narchitecture and training algorithm in this challenging domain and provide an\nextensive evaluation of its performance. Our results show that we can achieve\nconsiderable gains on top of state-of-the-art methods such as GEM.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:00:11 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 01:10:31 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 14:32:41 GMT"}, {"version": "v4", "created": "Thu, 20 Dec 2018 04:37:37 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Riemer", "Matthew", ""], ["Klinger", "Tim", ""], ["Bouneffouf", "Djallel", ""], ["Franceschini", "Michele", ""]]}, {"id": "1711.06794", "submitter": "Pan Lu", "authors": "Pan Lu, Hongsheng Li, Wei Zhang, Jianyong Wang, Xiaogang Wang", "title": "Co-attending Free-form Regions and Detections with Multi-modal\n  Multiplicative Feature Embedding for Visual Question Answering", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Visual Question Answering (VQA) task has gained increasing\nattention in artificial intelligence. Existing VQA methods mainly adopt the\nvisual attention mechanism to associate the input question with corresponding\nimage regions for effective question answering. The free-form region based and\nthe detection-based visual attention mechanisms are mostly investigated, with\nthe former ones attending free-form image regions and the latter ones attending\npre-specified detection-box regions. We argue that the two attention mechanisms\nare able to provide complementary information and should be effectively\nintegrated to better solve the VQA problem. In this paper, we propose a novel\ndeep neural network for VQA that integrates both attention mechanisms. Our\nproposed framework effectively fuses features from free-form image regions,\ndetection boxes, and question representations via a multi-modal multiplicative\nfeature embedding scheme to jointly attend question-related free-form image\nregions and detection boxes for more accurate question answering. The proposed\nmethod is extensively evaluated on two publicly available datasets, COCO-QA and\nVQA, and outperforms state-of-the-art approaches. Source code is available at\nhttps://github.com/lupantech/dual-mfa-vqa.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 02:07:34 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 08:34:43 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Lu", "Pan", ""], ["Li", "Hongsheng", ""], ["Zhang", "Wei", ""], ["Wang", "Jianyong", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1711.06800", "submitter": "Yiwei Bai", "authors": "Johan Bjorck, Yiwei Bai, Xiaojian Wu, Yexiang Xue, Mark C. Whitmore,\n  Carla Gomes", "title": "Scalable Relaxations of Sparse Packing Constraints: Optimal Biocontrol\n  in Predator-Prey Network", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascades represent rapid changes in networks. A cascading phenomenon of\necological and economic impact is the spread of invasive species in geographic\nlandscapes. The most promising management strategy is often biocontrol, which\nentails introducing a natural predator able to control the invading population,\na setting that can be treated as two interacting cascades of predator and prey\npopulations. We formulate and study a nonlinear problem of optimal biocontrol:\noptimally seeding the predator cascade over time to minimize the harmful prey\npopulation. Recurring budgets, which typically face conservation organizations,\nnaturally leads to sparse constraints which make the problem amenable to\napproximation algorithms. Available methods based on continuous relaxations\nscale poorly, to remedy this we develop a novel and scalable randomized\nalgorithm based on a width relaxation, applicable to a broad class of\ncombinatorial optimization problems. We evaluate our contributions in the\ncontext of biocontrol for the insect pest Hemlock Wolly Adelgid (HWA) in\neastern North America. Our algorithm outperforms competing methods in terms of\nscalability and solution quality, and finds near optimal strategies for the\ncontrol of the HWA for fine-grained networks -- an important problem in\ncomputational sustainability.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 02:47:41 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 18:28:02 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 08:29:57 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Bjorck", "Johan", ""], ["Bai", "Yiwei", ""], ["Wu", "Xiaojian", ""], ["Xue", "Yexiang", ""], ["Whitmore", "Mark C.", ""], ["Gomes", "Carla", ""]]}, {"id": "1711.06821", "submitter": "Guillem Collell", "authors": "Guillem Collell, Luc Van Gool, Marie-Francine Moens", "title": "Acquiring Common Sense Spatial Knowledge through Implicit Spatial\n  Templates", "comments": "To appear at AAAI 2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial understanding is a fundamental problem with wide-reaching real-world\napplications. The representation of spatial knowledge is often modeled with\nspatial templates, i.e., regions of acceptability of two objects under an\nexplicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with\nprior work that restricts spatial templates to explicit spatial prepositions\n(e.g., \"glass on table\"), here we extend this concept to implicit spatial\nlanguage, i.e., those relationships (generally actions) for which the spatial\narrangement of the objects is only implicitly implied (e.g., \"man riding\nhorse\"). In contrast with explicit relationships, predicting spatial\narrangements from implicit spatial language requires significant common sense\nspatial understanding. Here, we introduce the task of predicting spatial\ntemplates for two objects under a relationship, which can be seen as a spatial\nquestion-answering task with a (2D) continuous output (\"where is the man w.r.t.\na horse when the man is walking the horse?\"). We present two simple\nneural-based models that leverage annotated images and structured text to learn\nthis task. The good performance of these models reveals that spatial locations\nare to a large extent predictable from implicit spatial language. Crucially,\nthe models attain similar performance in a challenging generalized setting,\nwhere the object-relation-object combinations (e.g.,\"man walking dog\") have\nnever been seen before. Next, we go one step further by presenting the models\nwith unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging\nword embeddings enables the models to output accurate spatial predictions,\nproving that the models acquire solid common sense spatial knowledge allowing\nfor such generalization.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 07:00:44 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 02:41:36 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 15:23:13 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Collell", "Guillem", ""], ["Van Gool", "Luc", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1711.06871", "submitter": "Inmo Jang", "authors": "Inmo Jang, Hyo-Sang Shin, Antonios Tsourdos", "title": "Anonymous Hedonic Game for Task Allocation in a Large-Scale Multiple\n  Agent System", "comments": "Accepted by IEEE Transactions on Robotics (on 22 May 2018)", "journal-ref": "Published in IEEE Transactions on Robotics, 2018", "doi": "10.1109/TRO.2018.2858292", "report-no": null, "categories": "cs.MA cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel game-theoretical autonomous decision-making\nframework to address a task allocation problem for a swarm of multiple agents.\nWe consider cooperation of self-interested agents, and show that our proposed\ndecentralized algorithm guarantees convergence of agents with social inhibition\nto a Nash stable partition (i.e., social agreement) within polynomial time. The\nalgorithm is simple and executable based on local interactions with neighbor\nagents under a strongly-connected communication network and even in\nasynchronous environments. We analytically present a mathematical formulation\nfor computing the lower bound of suboptimality of the solution, and\nadditionally show that 50% of suboptimality can be at least guaranteed if\nsocial utilities are non-decreasing functions with respect to the number of\nco-working agents. The results of numerical experiments confirm that the\nproposed framework is scalable, fast adaptable against dynamical environments,\nand robust even in a realistic situation.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 14:56:36 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 03:30:42 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Jang", "Inmo", ""], ["Shin", "Hyo-Sang", ""], ["Tsourdos", "Antonios", ""]]}, {"id": "1711.06892", "submitter": "Falk Lieder", "authors": "Frederick Callaway and Sayan Gul and Paul M. Krueger and Thomas L.\n  Griffiths and Falk Lieder", "title": "Learning to select computations", "comments": null, "journal-ref": "Proceedings of the 34th Conference of Uncertainty in Artificial\n  Intelligence (2018)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient use of limited computational resources is an essential\ningredient of intelligence. Selecting computations optimally according to\nrational metareasoning would achieve this, but this is computationally\nintractable. Inspired by psychology and neuroscience, we propose the first\nconcrete and domain-general learning algorithm for approximating the optimal\nselection of computations: Bayesian metalevel policy search (BMPS). We derive\nthis general, sample-efficient search algorithm for a computation-selecting\nmetalevel policy based on the insight that the value of information lies\nbetween the myopic value of information and the value of perfect information.\nWe evaluate BMPS on three increasingly difficult metareasoning problems: when\nto terminate computation, how to allocate computation between competing\noptions, and planning. Across all three domains, BMPS achieved near-optimal\nperformance and compared favorably to previously proposed metareasoning\nheuristics. Finally, we demonstrate the practical utility of BMPS in an\nemergency management scenario, even accounting for the overhead of\nmetareasoning.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 16:42:48 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 22:12:17 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 22:13:18 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Callaway", "Frederick", ""], ["Gul", "Sayan", ""], ["Krueger", "Paul M.", ""], ["Griffiths", "Thomas L.", ""], ["Lieder", "Falk", ""]]}, {"id": "1711.06922", "submitter": "Mikhail Pavlov", "authors": "Mikhail Pavlov, Sergey Kolesnikov, Sergey M. Plis", "title": "Run, skeleton, run: skeletal model in a physics-based simulation", "comments": "Corrected typos and spelling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our approach to solve a physics-based reinforcement\nlearning challenge \"Learning to Run\" with objective to train\nphysiologically-based human model to navigate a complex obstacle course as\nquickly as possible. The environment is computationally expensive, has a\nhigh-dimensional continuous action space and is stochastic. We benchmark state\nof the art policy-gradient methods and test several improvements, such as layer\nnormalization, parameter noise, action and state reflecting, to stabilize\ntraining and improve its sample-efficiency. We found that the Deep\nDeterministic Policy Gradient method is the most efficient method for this\nenvironment and the improvements we have introduced help to stabilize training.\nLearned models are able to generalize to new physical scenarios, e.g. different\nobstacle courses.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 20:18:16 GMT"}, {"version": "v2", "created": "Sun, 28 Jan 2018 09:29:07 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Pavlov", "Mikhail", ""], ["Kolesnikov", "Sergey", ""], ["Plis", "Sergey M.", ""]]}, {"id": "1711.06930", "submitter": "Andrea Celli", "authors": "Andrea Celli and Nicola Gatti", "title": "Computational Results for Extensive-Form Adversarial Team Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide, to the best of our knowledge, the first computational study of\nextensive-form adversarial team games. These games are sequential, zero-sum\ngames in which a team of players, sharing the same utility function, faces an\nadversary. We define three different scenarios according to the communication\ncapabilities of the team. In the first, the teammates can communicate and\ncorrelate their actions both before and during the play. In the second, they\ncan only communicate before the play. In the third, no communication is\npossible at all. We define the most suitable solution concepts, and we study\nthe inefficiency caused by partial or null communication, showing that the\ninefficiency can be arbitrarily large in the size of the game tree.\nFurthermore, we study the computational complexity of the equilibrium-finding\nproblem in the three scenarios mentioned above, and we provide, for each of the\nthree scenarios, an exact algorithm. Finally, we empirically evaluate the\nscalability of the algorithms in random games and the inefficiency caused by\npartial or null communication.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 21:50:29 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Celli", "Andrea", ""], ["Gatti", "Nicola", ""]]}, {"id": "1711.07071", "submitter": "Evgeny Ivanko", "authors": "Evgeny Ivanko", "title": "The destiny of constant structure discrete time closed semantic systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constant structure closed semantic systems are the systems each element of\nwhich receives its definition through the correspondent unchangeable set of\nother elements of the system. Discrete time means here that the definitions of\nthe elements change iteratively and simultaneously based on the \"neighbor\nportraits\" from the previous iteration. I prove that the iterative redefinition\nprocess in such class of systems will quickly degenerate into a series of\npairwise isomorphic states and discuss some directions of further research.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 20:15:35 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Ivanko", "Evgeny", ""]]}, {"id": "1711.07111", "submitter": "Marisa Vasconcelos", "authors": "Marisa Vasconcelos, Carlos Cardonha, Bernardo Gon\\c{c}alves", "title": "Modeling Epistemological Principles for Bias Mitigation in AI Systems:\n  An Illustration in Hiring Decisions", "comments": null, "journal-ref": "2018 AAAI/ACM Conference on AI, Ethics, and Society", "doi": "10.1145/3278721.3278751", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) has been used extensively in automatic decision\nmaking in a broad variety of scenarios, ranging from credit ratings for loans\nto recommendations of movies. Traditional design guidelines for AI models focus\nessentially on accuracy maximization, but recent work has shown that\neconomically irrational and socially unacceptable scenarios of discrimination\nand unfairness are likely to arise unless these issues are explicitly\naddressed. This undesirable behavior has several possible sources, such as\nbiased datasets used for training that may not be detected in black-box models.\nAfter pointing out connections between such bias of AI and the problem of\ninduction, we focus on Popper's contributions after Hume's, which offer a\nlogical theory of preferences. An AI model can be preferred over others on\npurely rational grounds after one or more attempts at refutation based on\naccuracy and fairness. Inspired by such epistemological principles, this paper\nproposes a structured approach to mitigate discrimination and unfairness caused\nby bias in AI systems. In the proposed computational framework, models are\nselected and enhanced after attempts at refutation. To illustrate our\ndiscussion, we focus on hiring decision scenarios where an AI system filters in\nwhich job applicants should go to the interview phase.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 00:27:57 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Vasconcelos", "Marisa", ""], ["Cardonha", "Carlos", ""], ["Gon\u00e7alves", "Bernardo", ""]]}, {"id": "1711.07131", "submitter": "Kuang-Huei Lee", "authors": "Kuang-Huei Lee, Xiaodong He, Lei Zhang, Linjun Yang", "title": "CleanNet: Transfer Learning for Scalable Image Classifier Training with\n  Label Noise", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning image classification models\nwith label noise. Existing approaches depending on human supervision are\ngenerally not scalable as manually identifying correct or incorrect labels is\ntime-consuming, whereas approaches not relying on human supervision are\nscalable but less effective. To reduce the amount of human supervision for\nlabel noise cleaning, we introduce CleanNet, a joint neural embedding network,\nwhich only requires a fraction of the classes being manually verified to\nprovide the knowledge of label noise that can be transferred to other classes.\nWe further integrate CleanNet and conventional convolutional neural network\nclassifier into one framework for image classification learning. We demonstrate\nthe effectiveness of the proposed algorithm on both of the label noise\ndetection task and the image classification on noisy data task on several\nlarge-scale datasets. Experimental results show that CleanNet can reduce label\nnoise detection error rate on held-out classes where no human supervision\navailable by 41.5% compared to current weakly supervised methods. It also\nachieves 47% of the performance gain of verifying all images with only 3.2%\nimages verified on an image classification task. Source code and dataset will\nbe available at kuanghuei.github.io/CleanNetProject.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 03:50:53 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 23:07:58 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Lee", "Kuang-Huei", ""], ["He", "Xiaodong", ""], ["Zhang", "Lei", ""], ["Yang", "Linjun", ""]]}, {"id": "1711.07154", "submitter": "Ke Wang", "authors": "Ke Wang, Zhendong Su", "title": "Interactive, Intelligent Tutoring for Auxiliary Constructions in\n  Geometry Proofs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometry theorem proving forms a major and challenging component in the K-12\nmathematics curriculum. A particular difficult task is to add auxiliary\nconstructions (i.e, additional lines or points) to aid proof discovery.\nAlthough there exist many intelligent tutoring systems proposed for geometry\nproofs, few teach students how to find auxiliary constructions. And the few\nexceptions are all limited by their underlying reasoning processes for\nsupporting auxiliary constructions. This paper tackles these weaknesses of\nprior systems by introducing an interactive geometry tutor, the Advanced\nGeometry Proof Tutor (AGPT). It leverages a recent automated geometry prover to\nprovide combined benefits that any geometry theorem prover or intelligent\ntutoring system alone cannot accomplish. In particular, AGPT not only can\nautomatically process images of geometry problems directly, but also can\ninteractively train and guide students toward discovering auxiliary\nconstructions on their own. We have evaluated AGPT via a pilot study with 78\nhigh school students. The study results show that, on training students how to\nfind auxiliary constructions, there is no significant perceived difference\nbetween AGPT and human tutors, and AGPT is significantly more effective than\nthe state-of-the-art geometry solver that produces human-readable proofs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 05:39:58 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Wang", "Ke", ""], ["Su", "Zhendong", ""]]}, {"id": "1711.07163", "submitter": "Ke Wang", "authors": "Ke Wang, Rishabh Singh, Zhendong Su", "title": "Dynamic Neural Program Embedding for Program Repair", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural program embeddings have shown much promise recently for a variety of\nprogram analysis tasks, including program synthesis, program repair, fault\nlocalization, etc. However, most existing program embeddings are based on\nsyntactic features of programs, such as raw token sequences or abstract syntax\ntrees. Unlike images and text, a program has an unambiguous semantic meaning\nthat can be difficult to capture by only considering its syntax (i.e.\nsyntactically similar pro- grams can exhibit vastly different run-time\nbehavior), which makes syntax-based program embeddings fundamentally limited.\nThis paper proposes a novel semantic program embedding that is learned from\nprogram execution traces. Our key insight is that program states expressed as\nsequential tuples of live variable values not only captures program semantics\nmore precisely, but also offer a more natural fit for Recurrent Neural Networks\nto model. We evaluate different syntactic and semantic program embeddings on\npredicting the types of errors that students make in their submissions to an\nintroductory programming class and two exercises on the CodeHunt education\nplatform. Evaluation results show that our new semantic program embedding\nsignificantly outperforms the syntactic program embeddings based on token\nsequences and abstract syntax trees. In addition, we augment a search-based\nprogram repair system with the predictions obtained from our se- mantic\nembedding, and show that search efficiency is also significantly improved.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 06:02:06 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 22:15:32 GMT"}, {"version": "v3", "created": "Sun, 25 Feb 2018 22:48:31 GMT"}, {"version": "v4", "created": "Sat, 30 Jun 2018 00:33:27 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Wang", "Ke", ""], ["Singh", "Rishabh", ""], ["Su", "Zhendong", ""]]}, {"id": "1711.07170", "submitter": "Jiren Jin", "authors": "Jiren Jin, Richard G. Calland, Takeru Miyato, Brian K. Vogel, Hideki\n  Nakayama", "title": "Parameter Reference Loss for Unsupervised Domain Adaptation", "comments": "Add experiments that compare parameter reference loss with existing\n  methods using the same architecture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning in computer vision is mainly attributed to an\nabundance of data. However, collecting large-scale data is not always possible,\nespecially for the supervised labels. Unsupervised domain adaptation (UDA) aims\nto utilize labeled data from a source domain to learn a model that generalizes\nto a target domain of unlabeled data. A large amount of existing work uses\nSiamese network-based models, where two streams of neural networks process the\nsource and the target domain data respectively. Nevertheless, most of these\napproaches focus on minimizing the domain discrepancy, overlooking the\nimportance of preserving the discriminative ability for target domain features.\nAnother important problem in UDA research is how to evaluate the methods\nproperly. Common evaluation procedures require target domain labels for\nhyper-parameter tuning and model selection, contradicting the definition of the\nUDA task. Hence we propose a more reasonable evaluation principle that avoids\nthis contradiction by simply adopting the latest snapshot of a model for\nevaluation. This adds an extra requirement for UDA methods besides the main\nperformance criteria: the stability during training. We design a novel method\nthat connects the target domain stream to the source domain stream with a\nParameter Reference Loss (PRL) to solve these problems simultaneously.\nExperiments on various datasets show that the proposed PRL not only improves\nthe performance on the target domain, but also stabilizes the training\nprocedure. As a result, PRL based models do not need the contradictory model\nselection, and thus are more suitable for practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 06:40:43 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 10:19:34 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Jin", "Jiren", ""], ["Calland", "Richard G.", ""], ["Miyato", "Takeru", ""], ["Vogel", "Brian K.", ""], ["Nakayama", "Hideki", ""]]}, {"id": "1711.07214", "submitter": "Chao Qian", "authors": "Chao Qian, Yang Yu, Ke Tang, Xin Yao, Zhi-Hua Zhou", "title": "Maximizing Non-monotone/Non-submodular Functions by Multi-objective\n  Evolutionary Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms (EAs) are a kind of nature-inspired general-purpose\noptimization algorithm, and have shown empirically good performance in solving\nvarious real-word optimization problems. However, due to the highly randomized\nand complex behavior, the theoretical analysis of EAs is difficult and is an\nongoing challenge, which has attracted a lot of research attentions. During the\nlast two decades, promising results on the running time analysis (one essential\ntheoretical aspect) of EAs have been obtained, while most of them focused on\nisolated combinatorial optimization problems, which do not reflect the\ngeneral-purpose nature of EAs. To provide a general theoretical explanation of\nthe behavior of EAs, it is desirable to study the performance of EAs on a\ngeneral class of combinatorial optimization problems. To the best of our\nknowledge, this direction has been rarely touched and the only known result is\nthe provably good approximation guarantees of EAs for the problem class of\nmaximizing monotone submodular set functions with matroid constraints, which\nincludes many NP-hard combinatorial optimization problems. The aim of this work\nis to contribute to this line of research. As many combinatorial optimization\nproblems also involve non-monotone or non-submodular objective functions, we\nconsider these two general problem classes, maximizing non-monotone submodular\nfunctions without constraints and maximizing monotone non-submodular functions\nwith a size constraint. We prove that a simple multi-objective EA called GSEMO\ncan generally achieve good approximation guarantees in polynomial expected\nrunning time.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 09:21:19 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Qian", "Chao", ""], ["Yu", "Yang", ""], ["Tang", "Ke", ""], ["Yao", "Xin", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1711.07273", "submitter": "Phillip Lord Dr", "authors": "Phillip Lord, Robert Stevens", "title": "Facets, Tiers and Gems: Ontology Patterns for Hypernormalisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are many methodologies and techniques for easing the task of ontology\nbuilding. Here we describe the intersection of two of these: ontology\nnormalisation and fully programmatic ontology development. The first of these\ndescribes a standardized organisation for an ontology, with singly inherited\nself-standing entities, and a number of small taxonomies of refining entities.\nThe former are described and defined in terms of the latter and used to manage\nthe polyhierarchy of the self-standing entities. Fully programmatic development\nis a technique where an ontology is developed using a domain-specific language\nwithin a programming language, meaning that as well defining ontological\nentities, it is possible to add arbitrary patterns or new syntax within the\nsame environment. We describe how new patterns can be used to enable a new\nstyle of ontology development that we call hypernormalisation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 12:05:18 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Lord", "Phillip", ""], ["Stevens", "Robert", ""]]}, {"id": "1711.07280", "submitter": "Peter Anderson", "authors": "Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko\n  S\\\"underhauf, Ian Reid, Stephen Gould, Anton van den Hengel", "title": "Vision-and-Language Navigation: Interpreting visually-grounded\n  navigation instructions in real environments", "comments": "CVPR 2018 Spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robot that can carry out a natural-language instruction has been a dream\nsince before the Jetsons cartoon series imagined a life of leisure mediated by\na fleet of attentive robot helpers. It is a dream that remains stubbornly\ndistant. However, recent advances in vision and language methods have made\nincredible progress in closely related areas. This is significant because a\nrobot interpreting a natural-language navigation instruction on the basis of\nwhat it sees is carrying out a vision and language process that is similar to\nVisual Question Answering. Both tasks can be interpreted as visually grounded\nsequence-to-sequence translation problems, and many of the same methods are\napplicable. To enable and encourage the application of vision and language\nmethods to the problem of interpreting visually-grounded navigation\ninstructions, we present the Matterport3D Simulator -- a large-scale\nreinforcement learning environment based on real imagery. Using this simulator,\nwhich can in future support a range of embodied vision and language tasks, we\nprovide the first benchmark dataset for visually-grounded natural language\nnavigation in real buildings -- the Room-to-Room (R2R) dataset.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 12:17:47 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 01:48:34 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 22:57:44 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Anderson", "Peter", ""], ["Wu", "Qi", ""], ["Teney", "Damien", ""], ["Bruce", "Jake", ""], ["Johnson", "Mark", ""], ["S\u00fcnderhauf", "Niko", ""], ["Reid", "Ian", ""], ["Gould", "Stephen", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1711.07321", "submitter": "Guangming Lang", "authors": "Guangming Lang", "title": "Related family-based attribute reduction of covering information systems\n  when varying attribute sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical situations, there are many dynamic covering information systems\nwith variations of attributes, but there are few studies on related\nfamily-based attribute reduction of dynamic covering information systems. In\nthis paper, we first investigate updated mechanisms of constructing attribute\nreducts for consistent and inconsistent covering information systems when\nvarying attribute sets by using related families. Then we employ examples to\nillustrate how to compute attribute reducts of dynamic covering information\nsystems with variations of attribute sets. Finally, the experimental results\nillustrates that the related family-based methods are effective to perform\nattribute reduction of dynamic covering information systems when attribute sets\nare varying with time.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 08:54:28 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Lang", "Guangming", ""]]}, {"id": "1711.07329", "submitter": "Sanjiban Choudhury", "authors": "Sanjiban Choudhury, Siddhartha Srinivasa, Sebastian Scherer", "title": "Bayesian Active Edge Evaluation on Expensive Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots operate in environments with varying implicit structure. For instance,\na helicopter flying over terrain encounters a very different arrangement of\nobstacles than a robotic arm manipulating objects on a cluttered table top.\nState-of-the-art motion planning systems do not exploit this structure, thereby\nexpending valuable planning effort searching for implausible solutions. We are\ninterested in planning algorithms that actively infer the underlying structure\nof the valid configuration space during planning in order to find solutions\nwith minimal effort. Consider the problem of evaluating edges on a graph to\nquickly discover collision-free paths. Evaluating edges is expensive, both for\nrobots with complex geometries like robot arms, and for robots with limited\nonboard computation like UAVs. Until now, this challenge has been addressed via\nlaziness i.e. deferring edge evaluation until absolutely necessary, with the\nhope that edges turn out to be valid. However, all edges are not alike in value\n- some have a lot of potentially good paths flowing through them, and some\nothers encode the likelihood of neighbouring edges being valid. This leads to\nour key insight - instead of passive laziness, we can actively choose edges\nthat reduce the uncertainty about the validity of paths. We show that this is\nequivalent to the Bayesian active learning paradigm of decision region\ndetermination (DRD). However, the DRD problem is not only combinatorially hard,\nbut also requires explicit enumeration of all possible worlds. We propose a\nnovel framework that combines two DRD algorithms, DIRECT and BISECT, to\novercome both issues. We show that our approach outperforms several\nstate-of-the-art algorithms on a spectrum of planning problems for mobile\nrobots, manipulators and autonomous helicopters.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 14:43:59 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Choudhury", "Sanjiban", ""], ["Srinivasa", "Siddhartha", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1711.07341", "submitter": "Hsin-Yuan Huang", "authors": "Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, Weizhu Chen", "title": "FusionNet: Fusing via Fully-Aware Attention with Application to Machine\n  Comprehension", "comments": "Published in Sixth International Conference on Learning\n  Representations (ICLR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new neural structure called FusionNet, which extends\nexisting attention approaches from three perspectives. First, it puts forward a\nnovel concept of \"history of word\" to characterize attention information from\nthe lowest word-level embedding up to the highest semantic-level\nrepresentation. Second, it introduces an improved attention scoring function\nthat better utilizes the \"history of word\" concept. Third, it proposes a\nfully-aware multi-level attention mechanism to capture the complete information\nin one text (such as a question) and exploit it in its counterpart (such as\ncontext or passage) layer by layer. We apply FusionNet to the Stanford Question\nAnswering Dataset (SQuAD) and it achieves the first position for both single\nand ensemble model on the official SQuAD leaderboard at the time of writing\n(Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two\nadversarial SQuAD datasets and it sets up the new state-of-the-art on both\ndatasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to\n51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 03:52:41 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 04:56:45 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Huang", "Hsin-Yuan", ""], ["Zhu", "Chenguang", ""], ["Shen", "Yelong", ""], ["Chen", "Weizhu", ""]]}, {"id": "1711.07364", "submitter": "Jarom\\'ir Janisch", "authors": "Jarom\\'ir Janisch, Tom\\'a\\v{s} Pevn\\'y and Viliam Lis\\'y", "title": "Classification with Costly Features using Deep Reinforcement Learning", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a classification problem where each feature can be acquired for a\ncost and the goal is to optimize a trade-off between the expected\nclassification error and the feature cost. We revisit a former approach that\nhas framed the problem as a sequential decision-making problem and solved it by\nQ-learning with a linear approximation, where individual actions are either\nrequests for feature values or terminate the episode by providing a\nclassification decision. On a set of eight problems, we demonstrate that by\nreplacing the linear approximation with neural networks the approach becomes\ncomparable to the state-of-the-art algorithms developed specifically for this\nproblem. The approach is flexible, as it can be improved with any new\nreinforcement learning enhancement, it allows inclusion of pre-trained\nhigh-performance classifier, and unlike prior art, its performance is robust\nacross all evaluated datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 15:14:29 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 17:09:14 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Janisch", "Jarom\u00edr", ""], ["Pevn\u00fd", "Tom\u00e1\u0161", ""], ["Lis\u00fd", "Viliam", ""]]}, {"id": "1711.07387", "submitter": "Sam Kriegman", "authors": "Sam Kriegman, Nick Cheney, Josh Bongard", "title": "How morphological development can guide evolution", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-018-31868-7", "report-no": null, "categories": "q-bio.PE cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organisms result from adaptive processes interacting across different time\nscales. One such interaction is that between development and evolution. Models\nhave shown that development sweeps over several traits in a single agent,\nsometimes exposing promising static traits. Subsequent evolution can then\ncanalize these rare traits. Thus, development can, under the right conditions,\nincrease evolvability. Here, we report on a previously unknown phenomenon when\nembodied agents are allowed to develop and evolve: Evolution discovers body\nplans robust to control changes, these body plans become genetically\nassimilated, yet controllers for these agents are not assimilated. This allows\nevolution to continue climbing fitness gradients by tinkering with the\ndevelopmental programs for controllers within these permissive body plans. This\nexposes a previously unknown detail about the Baldwin effect: instead of all\nuseful traits becoming genetically assimilated, only traits that render the\nagent robust to changes in other traits become assimilated. We refer to this as\ndifferential canalization. This finding also has implications for the\nevolutionary design of artificial and embodied agents such as robots: robots\nrobust to internal changes in their controllers may also be robust to external\nchanges in their environment, such as transferal from simulation to reality or\ndeployment in novel environments.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 15:51:34 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 00:46:09 GMT"}, {"version": "v3", "created": "Wed, 2 May 2018 01:42:03 GMT"}, {"version": "v4", "created": "Fri, 3 Aug 2018 23:55:21 GMT"}, {"version": "v5", "created": "Fri, 7 Sep 2018 21:12:14 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Kriegman", "Sam", ""], ["Cheney", "Nick", ""], ["Bongard", "Josh", ""]]}, {"id": "1711.07414", "submitter": "Bernease Herman", "authors": "Bernease Herman", "title": "The Promise and Peril of Human Evaluation for Model Interpretability", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning.\n  I'm not happy with the writing and presentation of these ideas and hope to\n  submit an updated and extended version in 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transparency, user trust, and human comprehension are popular ethical\nmotivations for interpretable machine learning. In support of these goals,\nresearchers evaluate model explanation performance using humans and real world\napplications. This alone presents a challenge in many areas of artificial\nintelligence. In this position paper, we propose a distinction between\ndescriptive and persuasive explanations. We discuss reasoning suggesting that\nfunctional interpretability may be correlated with cognitive function and user\npreferences. If this is indeed the case, evaluation and optimization using\nfunctional metrics could perpetuate implicit cognitive bias in explanations\nthat threaten transparency. Finally, we propose two potential research\ndirections to disambiguate cognitive function and explanation models, retaining\ncontrol over the tradeoff between accuracy and interpretability.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:05:11 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 13:01:44 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Herman", "Bernease", ""]]}, {"id": "1711.07425", "submitter": "Kevin Feigelis", "authors": "Kevin T. Feigelis, Blue Sheffer, Daniel L. K. Yamins", "title": "Modular Continual Learning in a Unified Visual Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core aspect of human intelligence is the ability to learn new tasks quickly\nand switch between them flexibly. Here, we describe a modular continual\nreinforcement learning paradigm inspired by these abilities. We first introduce\na visual interaction environment that allows many types of tasks to be unified\nin a single framework. We then describe a reward map prediction scheme that\nlearns new tasks robustly in the very large state and action spaces required by\nsuch an environment. We investigate how properties of module architecture\ninfluence efficiency of task learning, showing that a module motif\nincorporating specific design principles (e.g. early bottlenecks, low-order\npolynomial nonlinearities, and symmetry) significantly outperforms more\nstandard neural network motifs, needing fewer training examples and fewer\nneurons to achieve high levels of performance. Finally, we present a\nmeta-controller architecture for task switching based on a dynamic neural\nvoting scheme, which allows new modules to use information learned from\npreviously-seen tasks to substantially improve their own learning efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:31:12 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 04:31:00 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Feigelis", "Kevin T.", ""], ["Sheffer", "Blue", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1711.07446", "submitter": "Avgoustinos Vouros", "authors": "Avgoustinos Vouros, Tiago V. Gehring, Kinga Szydlowska, Artur Janusz,\n  Mike Croucher, Katarzyna Lukasiuk, Witold Konopka, Carmen Sandi, Zehai Tu,\n  Eleni Vasilaki", "title": "A generalised framework for detailed classification of swimming paths\n  inside the Morris Water Maze", "comments": null, "journal-ref": "Scientific Reports volume 8, Article number: 15089 (2018)", "doi": "10.1038/s41598-018-33456-1", "report-no": null, "categories": "q-bio.QM cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Morris Water Maze is commonly used in behavioural neuroscience for the\nstudy of spatial learning with rodents. Over the years, various methods of\nanalysing rodent data collected in this task have been proposed. These methods\nspan from classical performance measurements (e.g. escape latency, rodent\nspeed, quadrant preference) to more sophisticated methods of categorisation\nwhich classify the animal swimming path into behavioural classes known as\nstrategies. Classification techniques provide additional insight in relation to\nthe actual animal behaviours but still only a limited amount of studies utilise\nthem mainly because they highly depend on machine learning knowledge. We have\npreviously demonstrated that the animals implement various strategies and by\nclassifying whole trajectories can lead to the loss of important information.\nIn this work, we developed a generalised and robust classification methodology\nwhich implements majority voting to boost the classification performance and\nsuccessfully nullify the need of manual tuning. Based on this framework, we\nbuilt a complete software, capable of performing the full analysis described in\nthis paper. The software provides an easy to use graphical user interface (GUI)\nthrough which users can enter their trajectory data, segment and label them and\nfinally generate reports and figures of the results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 18:12:19 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 18:25:17 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Vouros", "Avgoustinos", ""], ["Gehring", "Tiago V.", ""], ["Szydlowska", "Kinga", ""], ["Janusz", "Artur", ""], ["Croucher", "Mike", ""], ["Lukasiuk", "Katarzyna", ""], ["Konopka", "Witold", ""], ["Sandi", "Carmen", ""], ["Tu", "Zehai", ""], ["Vasilaki", "Eleni", ""]]}, {"id": "1711.07459", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Francis Li, Brendan Chwyl, and Alexander Wong", "title": "SquishedNets: Squishing SqueezeNet further for edge device scenarios via\n  deep evolutionary synthesis", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks have been shown in recent years to outperform\nother machine learning methods in a wide range of applications, one of the\nbiggest challenges with enabling deep neural networks for widespread deployment\non edge devices such as mobile and other consumer devices is high computational\nand memory requirements. Recently, there has been greater exploration into\nsmall deep neural network architectures that are more suitable for edge\ndevices, with one of the most popular architectures being SqueezeNet, with an\nincredibly small model size of 4.8MB. Taking further advantage of the notion\nthat many applications of machine learning on edge devices are often\ncharacterized by a low number of target classes, this study explores the\nutility of combining architectural modifications and an evolutionary synthesis\nstrategy for synthesizing even smaller deep neural architectures based on the\nmore recent SqueezeNet v1.1 macroarchitecture for applications with fewer\ntarget classes. In particular, architectural modifications are first made to\nSqueezeNet v1.1 to accommodate for a 10-class ImageNet-10 dataset, and then an\nevolutionary synthesis strategy is leveraged to synthesize more efficient deep\nneural networks based on this modified macroarchitecture. The resulting\nSquishedNets possess model sizes ranging from 2.4MB to 0.95MB (~5.17X smaller\nthan SqueezeNet v1.1, or 253X smaller than AlexNet). Furthermore, the\nSquishedNets are still able to achieve accuracies ranging from 81.2% to 77%,\nand able to process at speeds of 156 images/sec to as much as 256 images/sec on\na Nvidia Jetson TX1 embedded chip. These preliminary results show that a\ncombination of architectural modifications and an evolutionary synthesis\nstrategy can be a useful tool for producing very small deep neural network\narchitectures that are well-suited for edge device scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 18:50:05 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Li", "Francis", ""], ["Chwyl", "Brendan", ""], ["Wong", "Alexander", ""]]}, {"id": "1711.07477", "submitter": "Emiliano De Cristofaro", "authors": "Lucky Onwuzurike, Enrico Mariconti, Panagiotis Andriotis, Emiliano De\n  Cristofaro, Gordon Ross, and Gianluca Stringhini", "title": "MaMaDroid: Detecting Android Malware by Building Markov Chains of\n  Behavioral Models (Extended Version)", "comments": "A preliminary version of this paper appears in the Proceedings of the\n  24th Network and Distributed System Security Symposium (NDSS 2017)\n  [arXiv:1612.04433]. This is the extended version, to appear in the ACM\n  Transactions on Privacy and Security (ACM TOPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Android has become increasingly popular, so has malware targeting it, thus\npushing the research community to propose different detection techniques.\nHowever, the constant evolution of the Android ecosystem, and of malware\nitself, makes it hard to design robust tools that can operate for long periods\nof time without the need for modifications or costly re-training. Aiming to\naddress this issue, we set to detect malware from a behavioral point of view,\nmodeled as the sequence of abstracted API calls. We introduce MaMaDroid, a\nstatic-analysis based system that abstracts the API calls performed by an app\nto their class, package, or family, and builds a model from their sequences\nobtained from the call graph of an app as Markov chains. This ensures that the\nmodel is more resilient to API changes and the features set is of manageable\nsize. We evaluate MaMaDroid using a dataset of 8.5K benign and 35.5K malicious\napps collected over a period of six years, showing that it effectively detects\nmalware (with up to 0.99 F-measure) and keeps its detection capabilities for\nlong periods of time (up to 0.87 F-measure two years after training). We also\nshow that MaMaDroid remarkably outperforms DroidAPIMiner, a state-of-the-art\ndetection system that relies on the frequency of (raw) API calls. Aiming to\nassess whether MaMaDroid's effectiveness mainly stems from the API abstraction\nor from the sequencing modeling, we also evaluate a variant of it that uses\nfrequency (instead of sequences), of abstracted API calls. We find that it is\nnot as accurate, failing to capture maliciousness when trained on malware\nsamples that include API calls that are equally or more frequently used by\nbenign apps.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 11:11:52 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 13:17:41 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Onwuzurike", "Lucky", ""], ["Mariconti", "Enrico", ""], ["Andriotis", "Panagiotis", ""], ["De Cristofaro", "Emiliano", ""], ["Ross", "Gordon", ""], ["Stringhini", "Gianluca", ""]]}, {"id": "1711.07478", "submitter": "Melrose Roderick", "authors": "Melrose Roderick, James MacGlashan, Stefanie Tellex", "title": "Implementing the Deep Q-Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Deep Q-Network proposed by Mnih et al. [2015] has become a benchmark and\nbuilding point for much deep reinforcement learning research. However,\nreplicating results for complex systems is often challenging since original\nscientific publications are not always able to describe in detail every\nimportant parameter setting and software engineering solution. In this paper,\nwe present results from our work reproducing the results of the DQN paper. We\nhighlight key areas in the implementation that were not covered in great detail\nin the original paper to make it easier for researchers to replicate these\nresults, including termination conditions and gradient descent algorithms.\nFinally, we discuss methods for improving the computational performance and\nprovide our own implementation that is designed to work with a range of\ndomains, and not just the original Arcade Learning Environment [Bellemare et\nal., 2013].\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 16:40:33 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Roderick", "Melrose", ""], ["MacGlashan", "James", ""], ["Tellex", "Stefanie", ""]]}, {"id": "1711.07479", "submitter": "Gino Brunner", "authors": "Gino Brunner, Oliver Richter, Yuyi Wang, Roger Wattenhofer", "title": "Teaching a Machine to Read Maps with Deep Reinforcement Learning", "comments": "Paper accepted at 32nd AAAI Conference on Artificial Intelligence,\n  AAAI 2018, New Orleans, Louisiana, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to use a 2D map to navigate a complex 3D environment is quite\nremarkable, and even difficult for many humans. Localization and navigation is\nalso an important problem in domains such as robotics, and has recently become\na focus of the deep reinforcement learning community. In this paper we teach a\nreinforcement learning agent to read a map in order to find the shortest way\nout of a random maze it has never seen before. Our system combines several\nstate-of-the-art methods such as A3C and incorporates novel elements such as a\nrecurrent localization cell. Our agent learns to localize itself based on 3D\nfirst person images and an approximate orientation angle. The agent generalizes\nwell to bigger mazes, showing that it learned useful localization and\nnavigation capabilities.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 16:45:58 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Brunner", "Gino", ""], ["Richter", "Oliver", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1711.07600", "submitter": "Yu Cheng", "authors": "Yu Cheng, Shaddin Dughmi, David Kempe", "title": "On the Distortion of Voting with Multiple Representative Candidates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study positional voting rules when candidates and voters are embedded in a\ncommon metric space, and cardinal preferences are naturally given by distances\nin the metric space. In a positional voting rule, each candidate receives a\nscore from each ballot based on the ballot's rank order; the candidate with the\nhighest total score wins the election. The cost of a candidate is his sum of\ndistances to all voters, and the distortion of an election is the ratio between\nthe cost of the elected candidate and the cost of the optimum candidate. We\nconsider the case when candidates are representative of the population, in the\nsense that they are drawn i.i.d. from the population of the voters, and analyze\nthe expected distortion of positional voting rules.\n  Our main result is a clean and tight characterization of positional voting\nrules that have constant expected distortion (independent of the number of\ncandidates and the metric space). Our characterization result immediately\nimplies constant expected distortion for Borda Count and elections in which\neach voter approves a constant fraction of all candidates. On the other hand,\nwe obtain super-constant expected distortion for Plurality, Veto, and approving\na constant number of candidates. These results contrast with previous results\non voting with metric preferences: When the candidates are chosen\nadversarially, all of the preceding voting rules have distortion linear in the\nnumber of candidates or voters. Thus, the model of representative candidates\nallows us to distinguish voting rules which seem equally bad in the worst case.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 01:51:25 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Cheng", "Yu", ""], ["Dughmi", "Shaddin", ""], ["Kempe", "David", ""]]}, {"id": "1711.07613", "submitter": "Qi Wu", "authors": "Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, and Anton van den Hengel", "title": "Are You Talking to Me? Reasoned Visual Dialog Generation through\n  Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Visual Dialogue task requires an agent to engage in a conversation about\nan image with a human. It represents an extension of the Visual Question\nAnswering task in that the agent needs to answer a question about an image, but\nit needs to do so in light of the previous dialogue that has taken place. The\nkey challenge in Visual Dialogue is thus maintaining a consistent, and natural\ndialogue while continuing to answer questions correctly. We present a novel\napproach that combines Reinforcement Learning and Generative Adversarial\nNetworks (GANs) to generate more human-like responses to questions. The GAN\nhelps overcome the relative paucity of training data, and the tendency of the\ntypical MLE-based approach to generate overly terse answers. Critically, the\nGAN is tightly integrated into the attention mechanism that generates\nhuman-interpretable reasons for each answer. This means that the discriminative\nmodel of the GAN has the task of assessing whether a candidate answer is\ngenerated by a human or not, given the provided reason. This is significant\nbecause it drives the generative model to produce high quality answers that are\nwell supported by the associated reasoning. The method also generates the\nstate-of-the-art results on the primary benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 03:11:49 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Wu", "Qi", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1711.07614", "submitter": "Qi Wu", "authors": "Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang, Jianfeng Lu, and Anton\n  van den Hengel", "title": "Asking the Difficult Questions: Goal-Oriented Visual Question Generation\n  via Intermediate Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress in a variety of vision-and-language problems,\ndeveloping a method capable of asking intelligent, goal-oriented questions\nabout images is proven to be an inscrutable challenge. Towards this end, we\npropose a Deep Reinforcement Learning framework based on three new intermediate\nrewards, namely goal-achieved, progressive and informativeness that encourage\nthe generation of succinct questions, which in turn uncover valuable\ninformation towards the overall goal. By directly optimizing for questions that\nwork quickly towards fulfilling the overall goal, we avoid the tendency of\nexisting methods to generate long series of insane queries that add little\nvalue. We evaluate our model on the GuessWhat?! dataset and show that the\nresulting questions can help a standard Guesser identify a specific object in\nan image at a much higher success rate.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 03:15:30 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Zhang", "Junjie", ""], ["Wu", "Qi", ""], ["Shen", "Chunhua", ""], ["Zhang", "Jian", ""], ["Lu", "Jianfeng", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1711.07621", "submitter": "Siddharth Barman", "authors": "Siddharth Barman, Arpita Biswas, Sanath Kumar Krishnamurthy, and Y.\n  Narahari", "title": "Groupwise Maximin Fair Allocation of Indivisible Goods", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of allocating indivisible goods among n agents in a fair\nmanner. For this problem, maximin share (MMS) is a well-studied solution\nconcept which provides a fairness threshold. Specifically, maximin share is\ndefined as the minimum utility that an agent can guarantee for herself when\nasked to partition the set of goods into n bundles such that the remaining\n(n-1) agents pick their bundles adversarially. An allocation is deemed to be\nfair if every agent gets a bundle whose valuation is at least her maximin\nshare.\n  Even though maximin shares provide a natural benchmark for fairness, it has\nits own drawbacks and, in particular, it is not sufficient to rule out\nunsatisfactory allocations. Motivated by these considerations, in this work we\ndefine a stronger notion of fairness, called groupwise maximin share guarantee\n(GMMS). In GMMS, we require that the maximin share guarantee is achieved not\njust with respect to the grand bundle, but also among all the subgroups of\nagents. Hence, this solution concept strengthens MMS and provides an ex-post\nfairness guarantee. We show that in specific settings, GMMS allocations always\nexist. We also establish the existence of approximate GMMS allocations under\nadditive valuations, and develop a polynomial-time algorithm to find such\nallocations. Moreover, we establish a scale of fairness wherein we show that\nGMMS implies approximate envy freeness.\n  Finally, we empirically demonstrate the existence of GMMS allocations in a\nlarge set of randomly generated instances. For the same set of instances, we\nadditionally show that our algorithm achieves an approximation factor better\nthan the established, worst-case bound.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 04:00:30 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Barman", "Siddharth", ""], ["Biswas", "Arpita", ""], ["Krishnamurthy", "Sanath Kumar", ""], ["Narahari", "Y.", ""]]}, {"id": "1711.07632", "submitter": "Xiaopeng Yang", "authors": "Xiaopeng Yang, Xiaowen Lin, Shunda Suo, and Ming Li", "title": "Generating Thematic Chinese Poetry using Conditional Variational\n  Autoencoders with Hybrid Decoders", "comments": "Accepted by IJCAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer poetry generation is our first step towards computer writing.\nWriting must have a theme. The current approaches of using sequence-to-sequence\nmodels with attention often produce non-thematic poems. We present a novel\nconditional variational autoencoder with a hybrid decoder adding the\ndeconvolutional neural networks to the general recurrent neural networks to\nfully learn topic information via latent variables. This approach significantly\nimproves the relevance of the generated poems by representing each line of the\npoem not only in a context-sensitive manner but also in a holistic way that is\nhighly related to the given keyword and the learned topic. A proposed augmented\nword2vec model further improves the rhythm and symmetry. Tests show that the\ngenerated poems by our approach are mostly satisfying with regulated rules and\nconsistent themes, and 73.42% of them receive an Overall score no less than 3\n(the highest score is 5).\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 04:40:38 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 22:05:22 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 15:52:12 GMT"}, {"version": "v4", "created": "Thu, 5 Mar 2020 15:39:39 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Yang", "Xiaopeng", ""], ["Lin", "Xiaowen", ""], ["Suo", "Shunda", ""], ["Li", "Ming", ""]]}, {"id": "1711.07656", "submitter": "Yi Tay", "authors": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "title": "Cross Temporal Recurrent Networks for Ranking Question Answer Pairs", "comments": "Accepted to AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal gates play a significant role in modern recurrent-based neural\nencoders, enabling fine-grained control over recursive compositional operations\nover time. In recurrent models such as the long short-term memory (LSTM),\ntemporal gates control the amount of information retained or discarded over\ntime, not only playing an important role in influencing the learned\nrepresentations but also serving as a protection against vanishing gradients.\nThis paper explores the idea of learning temporal gates for sequence pairs\n(question and answer), jointly influencing the learned representations in a\npairwise manner. In our approach, temporal gates are learned via 1D\nconvolutional layers and then subsequently cross applied across question and\nanswer for joint learning. Empirically, we show that this conceptually simple\nsharing of temporal gates can lead to competitive performance across multiple\nbenchmarks. Intuitively, what our network achieves can be interpreted as\nlearning representations of question and answer pairs that are aware of what\neach other is remembering or forgetting, i.e., pairwise temporal gating. Via\nextensive experiments, we show that our proposed model achieves\nstate-of-the-art performance on two community-based QA datasets and competitive\nperformance on one factoid-based QA dataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:26:39 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Tay", "Yi", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1711.07661", "submitter": "Dalin Zhang", "authors": "Kaixuan Chen, Lina Yao, Tao Gu, Zhiwen Yu, Xianzhi Wang, Dalin Zhang", "title": "Fullie and Wiselie: A Dual-Stream Recurrent Convolutional Attention\n  Model for Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal features play a key role in wearable sensor based Human Activity\nRecognition (HAR). Selecting the most salient features adaptively is a\npromising way to maximize the effectiveness of multimodal sensor data. In this\nregard, we propose a \"collect fully and select wisely (Fullie and Wiselie)\"\nprinciple as well as a dual-stream recurrent convolutional attention model,\nRecurrent Attention and Activity Frame (RAAF), to improve the recognition\nperformance. We first collect modality features and the relations between each\npair of features to generate activity frames, and then introduce an attention\nmechanism to select the most prominent regions from activity frames precisely.\nThe selected frames not only maximize the utilization of valid features but\nalso reduce the number of features to be computed effectively. We further\nanalyze the hyper-parameters, accuracy, interpretability, and annotation\ndependency of the proposed model based on extensive experiments. The results\nshow that RAAF achieves competitive performance on two benchmarked datasets and\nworks well in real life scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:42:32 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Chen", "Kaixuan", ""], ["Yao", "Lina", ""], ["Gu", "Tao", ""], ["Yu", "Zhiwen", ""], ["Wang", "Xianzhi", ""], ["Zhang", "Dalin", ""]]}, {"id": "1711.07676", "submitter": "Ashley Edwards", "authors": "Ashley D. Edwards, Charles L. Isbell Jr", "title": "Transferring Agent Behaviors from Videos via Motion GANs", "comments": "Deep Reinforcement Learning Symposium, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major bottleneck for developing general reinforcement learning agents is\ndetermining rewards that will yield desirable behaviors under various\ncircumstances. We introduce a general mechanism for automatically specifying\nmeaningful behaviors from raw pixels. In particular, we train a generative\nadversarial network to produce short sub-goals represented through motion\ntemplates. We demonstrate that this approach generates visually meaningful\nbehaviors in unknown environments with novel agents and describe how these\nmotions can be used to train reinforcement learning agents.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 08:51:31 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Edwards", "Ashley D.", ""], ["Isbell", "Charles L.", "Jr"]]}, {"id": "1711.07682", "submitter": "Gino Brunner", "authors": "Gino Brunner, Yuyi Wang, Roger Wattenhofer, Jonas Wiesendanger", "title": "JamBot: Music Theory Aware Chord Based Generation of Polyphonic Music\n  with LSTMs", "comments": "Paper presented at the 29th International Conference on Tools with\n  Artificial Intelligence, ICTAI 2017, Boston, MA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.IT cs.LG eess.AS math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for the generation of polyphonic music based on\nLSTMs. We generate music in two steps. First, a chord LSTM predicts a chord\nprogression based on a chord embedding. A second LSTM then generates polyphonic\nmusic from the predicted chord progression. The generated music sounds pleasing\nand harmonic, with only few dissonant notes. It has clear long-term structure\nthat is similar to what a musician would play during a jam session. We show\nthat our approach is sensible from a music theory perspective by evaluating the\nlearned chord embeddings. Surprisingly, our simple model managed to extract the\ncircle of fifths, an important tool in music theory, from the dataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 09:19:16 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Brunner", "Gino", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""], ["Wiesendanger", "Jonas", ""]]}, {"id": "1711.07784", "submitter": "Davide Bacciu", "authors": "Davide Bacciu", "title": "Hidden Tree Markov Networks: Deep and Wide Learning for Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces the Hidden Tree Markov Network (HTN), a\nneuro-probabilistic hybrid fusing the representation power of generative models\nfor trees with the incremental and discriminative learning capabilities of\nneural networks. We put forward a modular architecture in which multiple\ngenerative models of limited complexity are trained to learn structural feature\ndetectors whose outputs are then combined and integrated by neural layers at a\nlater stage. In this respect, the model is both deep, thanks to the unfolding\nof the generative models on the input structures, as well as wide, given the\npotentially large number of generative modules that can be trained in parallel.\nExperimental results show that the proposed approach can outperform\nstate-of-the-art syntactic kernels as well as generative kernels built on the\nsame probabilistic model as the HTN.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 13:50:34 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Bacciu", "Davide", ""]]}, {"id": "1711.07821", "submitter": "Wilfredo Ariel G\\'omez Bueno WAGomez", "authors": "Edson Florez, Nelson Diaz, Wilfredo Gomez, Lola Bautista, Dario\n  Delgado", "title": "Evaluation of bioinspired algorithms for the solution of the job\n  scheduling problem", "comments": "in Spanish", "journal-ref": "I+D Revista de Investigaciones 2017", "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research we used bio-inspired metaheuristics, as artificial immune\nsystems and ant colony algorithms that are based on a number of characteristics\nand behaviors of living things that are interesting in the computer science\narea. This paper presents an evaluation of bio-inspired solutions to\ncombinatorial optimization problem, called the Job Shop Scheduling or planning\nwork, in a simple way the objective is to find a configuration or job stream\nthat has the least amount of time to be executed in machine settings. The\nperformance of the algorithms was characterized and evaluated for reference\ninstances of the job shop scheduling problem, comparing the quality of the\nsolutions obtained with respect to the best known solution of the most\neffective methods. The solutions were evaluated in two aspects, first in\nrelation of quality of solutions, taking as reference the makespan and secondly\nin relation of performance, taking the number evaluations performed by the\nalgorithm to obtain the best solution.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:57:14 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Florez", "Edson", ""], ["Diaz", "Nelson", ""], ["Gomez", "Wilfredo", ""], ["Bautista", "Lola", ""], ["Delgado", "Dario", ""]]}, {"id": "1711.07832", "submitter": "Daniel J Mankowitz", "authors": "Daniel J. Mankowitz, Aviv Tamar, Shie Mannor", "title": "Situationally Aware Options", "comments": "arXiv admin note: substantial text overlap with arXiv:1610.02847", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical abstractions, also known as options -- a type of temporally\nextended action (Sutton et. al. 1999) that enables a reinforcement learning\nagent to plan at a higher level, abstracting away from the lower-level details.\nIn this work, we learn reusable options whose parameters can vary, encouraging\ndifferent behaviors, based on the current situation. In principle, these\nbehaviors can include vigor, defence or even risk-averseness. These are some\nexamples of what we refer to in the broader context as Situational Awareness\n(SA). We incorporate SA, in the form of vigor, into hierarchical RL by defining\nand learning situationally aware options in a Probabilistic Goal Semi-Markov\nDecision Process (PG-SMDP). This is achieved using our Situationally Aware\noPtions (SAP) policy gradient algorithm which comes with a theoretical\nconvergence guarantee. We learn reusable options in different scenarios in a\nRoboCup soccer domain (i.e., winning/losing). These options learn to execute\nwith different levels of vigor resulting in human-like behaviours such as\n`time-wasting' in the winning scenario. We show the potential of the agent to\nexit bad local optima using reusable options in RoboCup. Finally, using SAP,\nthe agent mitigates feature-based model misspecification in a Bottomless Pit of\nDeath domain.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 08:11:12 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Mankowitz", "Daniel J.", ""], ["Tamar", "Aviv", ""], ["Mannor", "Shie", ""]]}, {"id": "1711.07875", "submitter": "Paolo Dragone", "authors": "Paolo Dragone, Stefano Teso, Andrea Passerini", "title": "Constructive Preference Elicitation over Hybrid Combinatorial Spaces", "comments": "AAAI 2018, computing methodologies, machine learning, learning\n  paradigms, supervised learning, structured outputs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preference elicitation is the task of suggesting a highly preferred\nconfiguration to a decision maker. The preferences are typically learned by\nquerying the user for choice feedback over pairs or sets of objects. In its\nconstructive variant, new objects are synthesized \"from scratch\" by maximizing\nan estimate of the user utility over a combinatorial (possibly infinite) space\nof candidates. In the constructive setting, most existing elicitation\ntechniques fail because they rely on exhaustive enumeration of the candidates.\nA previous solution explicitly designed for constructive tasks comes with no\nformal performance guarantees, and can be very expensive in (or unapplicable\nto) problems with non-Boolean attributes. We propose the Choice Perceptron, a\nPerceptron-like algorithm for learning user preferences from set-wise choice\nfeedback over constructive domains and hybrid Boolean-numeric feature spaces.\nWe provide a theoretical analysis on the attained regret that holds for a large\nclass of query selection strategies, and devise a heuristic strategy that aims\nat optimizing the regret in practice. Finally, we demonstrate its effectiveness\nby empirical evaluation against existing competitors on constructive scenarios\nof increasing complexity.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 16:20:24 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 16:56:59 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Dragone", "Paolo", ""], ["Teso", "Stefano", ""], ["Passerini", "Andrea", ""]]}, {"id": "1711.07894", "submitter": "Kun ho Kim", "authors": "Yanan Sui, Kun ho Kim, Joel W. Burdick", "title": "Quantifying Performance of Bipedal Standing with Multi-channel EMG", "comments": null, "journal-ref": "IROS 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spinal cord stimulation has enabled humans with motor complete spinal cord\ninjury (SCI) to independently stand and recover some lost autonomic function.\nQuantifying the quality of bipedal standing under spinal stimulation is\nimportant for spinal rehabilitation therapies and for new strategies that seek\nto combine spinal stimulation and rehabilitative robots (such as exoskeletons)\nin real time feedback. To study the potential for automated electromyography\n(EMG) analysis in SCI, we evaluated the standing quality of paralyzed patients\nundergoing electrical spinal cord stimulation using both video and\nmulti-channel surface EMG recordings during spinal stimulation therapy\nsessions. The quality of standing under different stimulation settings was\nquantified manually by experienced clinicians. By correlating features of the\nrecorded EMG activity with the expert evaluations, we show that multi-channel\nEMG recording can provide accurate, fast, and robust estimation for the quality\nof bipedal standing in spinally stimulated SCI patients. Moreover, our analysis\nshows that the total number of EMG channels needed to effectively predict\nstanding quality can be reduced while maintaining high estimation accuracy,\nwhich provides more flexibility for rehabilitation robotic systems to\nincorporate EMG recordings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 16:40:26 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Sui", "Yanan", ""], ["Kim", "Kun ho", ""], ["Burdick", "Joel W.", ""]]}, {"id": "1711.07970", "submitter": "Arthur Pajot", "authors": "Emmanuel de Bezenac, Arthur Pajot, Patrick Gallinari", "title": "Deep Learning for Physical Processes: Incorporating Prior Scientific\n  Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the use of Deep Learning methods for modeling complex phenomena\nlike those occurring in natural physical processes. With the large amount of\ndata gathered on these phenomena the data intensive paradigm could begin to\nchallenge more traditional approaches elaborated over the years in fields like\nmaths or physics. However, despite considerable successes in a variety of\napplication domains, the machine learning field is not yet ready to handle the\nlevel of complexity required by such problems. Using an example application,\nnamely Sea Surface Temperature Prediction, we show how general background\nknowledge gained from physics could be used as a guideline for designing\nefficient Deep Learning models. In order to motivate the approach and to assess\nits generality we demonstrate a formal link between the solution of a class of\ndifferential equations underlying a large family of physical phenomena and the\nproposed model. Experiments and comparison with series of baselines including a\nstate of the art numerical approach is then provided.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 18:49:47 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 16:43:39 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["de Bezenac", "Emmanuel", ""], ["Pajot", "Arthur", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1711.07979", "submitter": "Georgios Theocharous", "authors": "Georgios Theocharous and Zheng Wen and Yasin Abbasi-Yadkori and Nikos\n  Vlassis", "title": "Posterior Sampling for Large Scale Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a practical non-episodic PSRL algorithm that unlike recent\nstate-of-the-art PSRL algorithms uses a deterministic, model-independent\nepisode switching schedule. Our algorithm termed deterministic schedule PSRL\n(DS-PSRL) is efficient in terms of time, sample, and space complexity. We prove\na Bayesian regret bound under mild assumptions. Our result is more generally\napplicable to multiple parameters and continuous state action problems. We\ncompare our algorithm with state-of-the-art PSRL algorithms on standard\ndiscrete and continuous problems from the literature. Finally, we show how the\nassumptions of our algorithm satisfy a sensible parametrization for a large\nclass of problems in sequential recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 00:43:24 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 23:55:15 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 22:06:00 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Theocharous", "Georgios", ""], ["Wen", "Zheng", ""], ["Abbasi-Yadkori", "Yasin", ""], ["Vlassis", "Nikos", ""]]}, {"id": "1711.08006", "submitter": "Ning Xie", "authors": "Ning Xie, Md Kamruzzaman Sarker, Derek Doran, Pascal Hitzler, Michael\n  Raymer", "title": "Relating Input Concepts to Convolutional Neural Network Decisions", "comments": "10 pages (including references), 9 figures, paper accepted by NIPS\n  IEVDL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many current methods to interpret convolutional neural networks (CNNs) use\nvisualization techniques and words to highlight concepts of the input seemingly\nrelevant to a CNN's decision. The methods hypothesize that the recognition of\nthese concepts are instrumental in the decision a CNN reaches, but the nature\nof this relationship has not been well explored. To address this gap, this\npaper examines the quality of a concept's recognition by a CNN and the degree\nto which the recognitions are associated with CNN decisions. The study\nconsiders a CNN trained for scene recognition over the ADE20k dataset. It uses\na novel approach to find and score the strength of minimally distributed\nrepresentations of input concepts (defined by objects in scene images) across\nlate stage feature maps. Subsequent analysis finds evidence that concept\nrecognition impacts decision making. Strong recognition of concepts\nfrequently-occurring in few scenes are indicative of correct decisions, but\nrecognizing concepts common to many scenes may mislead the network.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 19:37:13 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Xie", "Ning", ""], ["Sarker", "Md Kamruzzaman", ""], ["Doran", "Derek", ""], ["Hitzler", "Pascal", ""], ["Raymer", "Michael", ""]]}, {"id": "1711.08010", "submitter": "Zhong Meng", "authors": "Zhong Meng, Zhuo Chen, Vadim Mazalov, Jinyu Li, Yifan Gong", "title": "Unsupervised Adaptation with Domain Separation Networks for Robust\n  Speech Recognition", "comments": "8 pages, 1 figure, ASRU 2017", "journal-ref": "2017 IEEE Automatic Speech Recognition and Understanding Workshop\n  (ASRU), Okinawa, 2017, pp. 214-221", "doi": "10.1109/ASRU.2017.8268938", "report-no": null, "categories": "cs.CL cs.AI cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation of speech signal aims at adapting a\nwell-trained source-domain acoustic model to the unlabeled data from target\ndomain. This can be achieved by adversarial training of deep neural network\n(DNN) acoustic models to learn an intermediate deep representation that is both\nsenone-discriminative and domain-invariant. Specifically, the DNN is trained to\njointly optimize the primary task of senone classification and the secondary\ntask of domain classification with adversarial objective functions. In this\nwork, instead of only focusing on learning a domain-invariant feature (i.e. the\nshared component between domains), we also characterize the difference between\nthe source and target domain distributions by explicitly modeling the private\ncomponent of each domain through a private component extractor DNN. The private\ncomponent is trained to be orthogonal with the shared component and thus\nimplicitly increases the degree of domain-invariance of the shared component. A\nreconstructor DNN is used to reconstruct the original speech feature from the\nprivate and shared components as a regularization. This domain separation\nframework is applied to the unsupervised environment adaptation task and\nachieved 11.08% relative WER reduction from the gradient reversal layer\ntraining, a representative adversarial training method, for automatic speech\nrecognition on CHiME-3 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 19:44:37 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 15:57:25 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Meng", "Zhong", ""], ["Chen", "Zhuo", ""], ["Mazalov", "Vadim", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "1711.08028", "submitter": "Rasmus Berg Palm", "authors": "Rasmus Berg Palm, Ulrich Paquet, Ole Winther", "title": "Recurrent Relational Networks", "comments": "Accepted at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with learning to solve tasks that require a chain of\ninterdependent steps of relational inference, like answering complex questions\nabout the relationships between objects, or solving puzzles where the smaller\nelements of a solution mutually constrain each other. We introduce the\nrecurrent relational network, a general purpose module that operates on a graph\nrepresentation of objects. As a generalization of Santoro et al. [2017]'s\nrelational network, it can augment any neural network model with the capacity\nto do many-step relational reasoning. We achieve state of the art results on\nthe bAbI textual question-answering dataset with the recurrent relational\nnetwork, consistently solving 20/20 tasks. As bAbI is not particularly\nchallenging from a relational reasoning point of view, we introduce\nPretty-CLEVR, a new diagnostic dataset for relational reasoning. In the\nPretty-CLEVR set-up, we can vary the question to control for the number of\nrelational reasoning steps that are required to obtain the answer. Using\nPretty-CLEVR, we probe the limitations of multi-layer perceptrons, relational\nand recurrent relational networks. Finally, we show how recurrent relational\nnetworks can learn to solve Sudoku puzzles from supervised training data, a\nchallenging task requiring upwards of 64 steps of relational reasoning. We\nachieve state-of-the-art results amongst comparable methods by solving 96.6% of\nthe hardest Sudoku puzzles.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 20:34:48 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 11:44:06 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 07:44:25 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2018 15:11:23 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Palm", "Rasmus Berg", ""], ["Paquet", "Ulrich", ""], ["Winther", "Ole", ""]]}, {"id": "1711.08068", "submitter": "Daniel L\\'evy", "authors": "Daniel Levy, Stefano Ermon", "title": "Deterministic Policy Optimization by Combining Pathwise and Score\n  Function Estimators for Discrete Action Spaces", "comments": "In AAAI 2018 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy optimization methods have shown great promise in solving complex\nreinforcement and imitation learning tasks. While model-free methods are\nbroadly applicable, they often require many samples to optimize complex\npolicies. Model-based methods greatly improve sample-efficiency but at the cost\nof poor generalization, requiring a carefully handcrafted model of the system\ndynamics for each task. Recently, hybrid methods have been successful in\ntrading off applicability for improved sample-complexity. However, these have\nbeen limited to continuous action spaces. In this work, we present a new hybrid\nmethod based on an approximation of the dynamics as an expectation over the\nnext state under the current policy. This relaxation allows us to derive a\nnovel hybrid policy gradient estimator, combining score function and pathwise\nderivative estimators, that is applicable to discrete action spaces. We show\nsignificant gains in sample complexity, ranging between $1.7$ and $25\\times$,\nwhen learning parameterized policies on Cart Pole, Acrobot, Mountain Car and\nHand Mass. Our method is applicable to both discrete and continuous action\nspaces, when competing pathwise methods are limited to the latter.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 22:05:18 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Levy", "Daniel", ""], ["Ermon", "Stefano", ""]]}, {"id": "1711.08080", "submitter": "Christian Kroer", "authors": "Christian Kroer, Gabriele Farina, Tuomas Sandholm", "title": "Robust Stackelberg Equilibria in Extensive-Form Games and Extension to\n  Limited Lookahead", "comments": "Published at AAAI18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stackelberg equilibria have become increasingly important as a solution\nconcept in computational game theory, largely inspired by practical problems\nsuch as security settings. In practice, however, there is typically uncertainty\nregarding the model about the opponent. This paper is, to our knowledge, the\nfirst to investigate Stackelberg equilibria under uncertainty in extensive-form\ngames, one of the broadest classes of game. We introduce robust Stackelberg\nequilibria, where the uncertainty is about the opponent's payoffs, as well as\nones where the opponent has limited lookahead and the uncertainty is about the\nopponent's node evaluation function. We develop a new mixed-integer program for\nthe deterministic limited-lookahead setting. We then extend the program to the\nrobust setting for Stackelberg equilibrium under unlimited and under limited\nlookahead by the opponent. We show that for the specific case of interval\nuncertainty about the opponent's payoffs (or about the opponent's node\nevaluations in the case of limited lookahead), robust Stackelberg equilibria\ncan be computed with a mixed-integer program that is of the same asymptotic\nsize as that for the deterministic setting.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 23:13:15 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Kroer", "Christian", ""], ["Farina", "Gabriele", ""], ["Sandholm", "Tuomas", ""]]}, {"id": "1711.08101", "submitter": "Levi Lelis", "authors": "Rubens O. Moraes and Levi H. S. Lelis", "title": "Asymmetric Action Abstractions for Multi-Unit Control in Adversarial\n  Real-Time Games", "comments": "AAAI'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action abstractions restrict the number of legal actions available during\nsearch in multi-unit real-time adversarial games, thus allowing algorithms to\nfocus their search on a set of promising actions. Optimal strategies derived\nfrom un-abstracted spaces are guaranteed to be no worse than optimal strategies\nderived from action-abstracted spaces. In practice, however, due to real-time\nconstraints and the state space size, one is only able to derive good\nstrategies in un-abstracted spaces in small-scale games. In this paper we\nintroduce search algorithms that use an action abstraction scheme we call\nasymmetric abstraction. Asymmetric abstractions retain the un-abstracted\nspaces' theoretical advantage over regularly abstracted spaces while still\nallowing the search algorithms to derive effective strategies, even in\nlarge-scale games. Empirical results on combat scenarios that arise in a\nreal-time strategy game show that our search algorithms are able to\nsubstantially outperform state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 01:35:29 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Moraes", "Rubens O.", ""], ["Lelis", "Levi H. S.", ""]]}, {"id": "1711.08151", "submitter": "Shufeng Kong", "authors": "Shufeng Kong, Jae Hee Lee, Sanjiang Li", "title": "Multiagent Simple Temporal Problem: The Arc-Consistency Approach", "comments": "Accepted by The Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Simple Temporal Problem (STP) is a fundamental temporal reasoning problem\nand has recently been extended to the Multiagent Simple Temporal Problem\n(MaSTP). In this paper we present a novel approach that is based on enforcing\narc-consistency (AC) on the input (multiagent) simple temporal network. We show\nthat the AC-based approach is sufficient for solving both the STP and MaSTP and\nprovide efficient algorithms for them. As our AC-based approach does not impose\nnew constraints between agents, it does not violate the privacy of the agents\nand is superior to the state-of-the-art approach to MaSTP. Empirical\nevaluations on diverse benchmark datasets also show that our AC-based\nalgorithms for STP and MaSTP are significantly more efficient than existing\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 06:43:21 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Kong", "Shufeng", ""], ["Lee", "Jae Hee", ""], ["Li", "Sanjiang", ""]]}, {"id": "1711.08228", "submitter": "Rong Zhang", "authors": "Tong Mo, Rong Zhang, Weiping Li, Jingbo Zhang, Zhonghai Wu and Wei Tan", "title": "An influence-based fast preceding questionnaire model for elderly\n  assessments", "comments": "Accepted by the journal \"Intelligent Data Analysis\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the efficiency of elderly assessments, an influence-based fast\npreceding questionnaire model (FPQM) is proposed. Compared with traditional\nassessments, the FPQM optimizes questionnaires by reordering their attributes.\nThe values of low-ranking attributes can be predicted by the values of the\nhigh-ranking attributes. Therefore, the number of attributes can be reduced\nwithout redesigning the questionnaires. A new function for calculating the\ninfluence of the attributes is proposed based on probability theory. Reordering\nand reducing algorithms are given based on the attributes' influences. The\nmodel is verified through a practical application. The practice in an\nelderly-care company shows that the FPQM can reduce the number of attributes by\n90.56% with a prediction accuracy of 98.39%. Compared with other methods, such\nas the Expert Knowledge, Rough Set and C4.5 methods, the FPQM achieves the best\nperformance. In addition, the FPQM can also be applied to other questionnaires.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 11:10:39 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Mo", "Tong", ""], ["Zhang", "Rong", ""], ["Li", "Weiping", ""], ["Zhang", "Jingbo", ""], ["Wu", "Zhonghai", ""], ["Tan", "Wei", ""]]}, {"id": "1711.08237", "submitter": "Guy Tennenholtz", "authors": "Guy Tennenholtz, Constantine Caramanis, Shie Mannor", "title": "The Stochastic Firefighter Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics of infectious diseases spread is crucial in determining their\nrisk and offering ways to contain them. We study sequential vaccination of\nindividuals in networks. In the original (deterministic) version of the\nFirefighter problem, a fire breaks out at some node of a given graph. At each\ntime step, b nodes can be protected by a firefighter and then the fire spreads\nto all unprotected neighbors of the nodes on fire. The process ends when the\nfire can no longer spread. We extend the Firefighter problem to a probabilistic\nsetting, where the infection is stochastic. We devise a simple policy that only\nvaccinates neighbors of infected nodes and is optimal on regular trees and on\ngeneral graphs for a sufficiently large budget. We derive methods for\ncalculating upper and lower bounds of the expected number of infected\nindividuals, as well as provide estimates on the budget needed for containment\nin expectation. We calculate these explicitly on trees, d-dimensional grids,\nand Erd\\H{o}s R\\'{e}nyi graphs. Finally, we construct a state-dependent budget\nallocation strategy and demonstrate its superiority over constant budget\nallocation on real networks following a first order acquaintance vaccination\npolicy.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 11:39:21 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Tennenholtz", "Guy", ""], ["Caramanis", "Constantine", ""], ["Mannor", "Shie", ""]]}, {"id": "1711.08275", "submitter": "Jung-Su Ha", "authors": "Jung-Su Ha, Hyeok-Joo Chae, Han-Lim Choi", "title": "Approximate Inference-based Motion Planning by Learning and Exploiting\n  Low-Dimensional Latent Variable Models", "comments": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L), 2018", "journal-ref": null, "doi": "10.1109/LRA.2018.2856915", "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an efficient framework to generate a motion plan of a\nrobot with high degrees of freedom (e.g., a humanoid robot).\nHigh-dimensionality of the robot configuration space often leads to\ndifficulties in utilizing the widely-used motion planning algorithms, since the\nvolume of the decision space increases exponentially with the number of\ndimensions. To handle complications arising from the large decision space, and\nto solve a corresponding motion planning problem efficiently, two key concepts\nare adopted in this work: First, the Gaussian process latent variable model\n(GP-LVM) is utilized for low-dimensional representation of the original\nconfiguration space. Second, an approximate inference algorithm is used,\nexploiting through the duality between control and estimation, to explore the\ndecision space and to compute a high-quality motion trajectory of the robot.\nUtilizing the GP-LVM and the duality between control and estimation, we\nconstruct a fully probabilistic generative model with which a high-dimensional\nmotion planning problem is transformed into a tractable inference problem.\nFinally, we compute the motion trajectory via an approximate inference\nalgorithm based on a variant of the particle filter. The resulting motions can\nbe viewed in the supplemental video. ( https://youtu.be/kngEaOR4Esc )\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 13:35:07 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 09:51:12 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Ha", "Jung-Su", ""], ["Chae", "Hyeok-Joo", ""], ["Choi", "Han-Lim", ""]]}, {"id": "1711.08319", "submitter": "Mark Burgin", "authors": "Mark Burgin", "title": "Systems, Actors and Agents: Operation in a multicomponent environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent approach has become popular in computer science and technology.\nHowever, the conventional models of multi-agent and multicomponent systems\nimplicitly or explicitly assume existence of absolute time or even do not\ninclude time in the set of defining parameters. At the same time, it is proved\ntheoretically and validated experimentally that there are different times and\ntime scales in a variety of real systems - physical, chemical, biological,\nsocial, informational, etc. Thus, the goal of this work is construction of a\nmulti-agent multicomponent system models with concurrency of processes and\ndiversity of actions. To achieve this goal, a mathematical system actor model\nis elaborated and its properties are studied.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 01:31:36 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Burgin", "Mark", ""]]}, {"id": "1711.08333", "submitter": "Marti Sanchez-Fibla", "authors": "M. S\\'anchez-Fibla, C. Moulin-Frier, X. Arsiwalla and P. Verschure", "title": "A correlational analysis of multiagent sensorimotor interactions:\n  clustering autonomous and controllable entities", "comments": "6 pages, 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A first step to reach Theory of Mind (ToM) abilities (attribution of beliefs\nto others) in synthetic agents through sensorimotor interactions, would be to\ntag sensory data with agent typology and action intentions: autonomous agent X\nmoved an object under the box. We propose a dual arm robotic setup in which ToM\ncould be probed. We then discuss what measures can be extracted from\nsensorimotor interaction data (based on a correlation analysis) in the proposed\nsetup that allow to distinguish self than other and other/inanimate from\nother/active with intentions. We finally discuss what elements are missing in\ncurrent cognitive architectures to be able to acquire ToM abilities in\nsynthetic agents from sensorimotor interactions, bottom-up from reactive agent\ninteraction behaviors and top-down from the optimization of social behaviour\nand cooperation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 15:21:12 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["S\u00e1nchez-Fibla", "M.", ""], ["Moulin-Frier", "C.", ""], ["Arsiwalla", "X.", ""], ["Verschure", "P.", ""]]}, {"id": "1711.08345", "submitter": "Karthik Abinav Sankararaman", "authors": "John P Dickerson and Karthik A Sankararaman and Aravind Srinivasan and\n  Pan Xu", "title": "Allocation Problems in Ride-Sharing Platforms: Online Matching with\n  Offline Reusable Resources", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite matching markets pair agents on one side of a market with agents,\nitems, or contracts on the opposing side. Prior work addresses online bipartite\nmatching markets, where agents arrive over time and are dynamically matched to\na known set of disposable resources. In this paper, we propose a new model,\nOnline Matching with (offline) Reusable Resources under Known Adversarial\nDistributions (OM-RR-KAD), in which resources on the offline side are reusable\ninstead of disposable; that is, once matched, resources become available again\nat some point in the future. We show that our model is tractable by presenting\nan LP-based adaptive algorithm that achieves an online competitive ratio of 1/2\n- eps for any given eps greater than 0. We also show that no non-adaptive\nalgorithm can achieve a ratio of 1/2 + o(1) based on the same benchmark LP.\nThrough a data-driven analysis on a massive openly-available dataset, we show\nour model is robust enough to capture the application of taxi dispatching\nservices and ride-sharing systems. We also present heuristics that perform well\nin practice.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 15:39:57 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 23:54:31 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Dickerson", "John P", ""], ["Sankararaman", "Karthik A", ""], ["Srinivasan", "Aravind", ""], ["Xu", "Pan", ""]]}, {"id": "1711.08378", "submitter": "Matthew Botvinick", "authors": "M. Botvinick, D.G.T. Barrett, P. Battaglia, N. de Freitas, D. Kumaran,\n  J. Z Leibo, T. Lillicrap, J. Modayil, S. Mohamed, N.C. Rabinowitz, D. J.\n  Rezende, A. Santoro, T. Schaul, C. Summerfield, G. Wayne, T. Weber, D.\n  Wierstra, S. Legg and D. Hassabis", "title": "Building Machines that Learn and Think for Themselves: Commentary on\n  Lake et al., Behavioral and Brain Sciences, 2017", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We agree with Lake and colleagues on their list of key ingredients for\nbuilding humanlike intelligence, including the idea that model-based reasoning\nis essential. However, we favor an approach that centers on one additional\ningredient: autonomy. In particular, we aim toward agents that can both build\nand exploit their own internal models, with minimal human hand-engineering. We\nbelieve an approach centered on autonomous learning has the greatest chance of\nsuccess as we scale toward real-world complexity, tackling domains for which\nready-made formal models are not available. Here we survey several important\nexamples of the progress that has been made toward building autonomous agents\nwith humanlike abilities, and highlight some outstanding challenges.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 16:35:29 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Botvinick", "M.", ""], ["Barrett", "D. G. T.", ""], ["Battaglia", "P.", ""], ["de Freitas", "N.", ""], ["Kumaran", "D.", ""], ["Leibo", "J. Z", ""], ["Lillicrap", "T.", ""], ["Modayil", "J.", ""], ["Mohamed", "S.", ""], ["Rabinowitz", "N. C.", ""], ["Rezende", "D. J.", ""], ["Santoro", "A.", ""], ["Schaul", "T.", ""], ["Summerfield", "C.", ""], ["Wayne", "G.", ""], ["Weber", "T.", ""], ["Wierstra", "D.", ""], ["Legg", "S.", ""], ["Hassabis", "D.", ""]]}, {"id": "1711.08478", "submitter": "Nicholas Carlini", "authors": "Nicholas Carlini, David Wagner", "title": "MagNet and \"Efficient Defenses Against Adversarial Attacks\" are Not\n  Robust to Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MagNet and \"Efficient Defenses...\" were recently proposed as a defense to\nadversarial examples. We find that we can construct adversarial examples that\ndefeat these defenses with only a slight increase in distortion.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 19:18:52 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Carlini", "Nicholas", ""], ["Wagner", "David", ""]]}, {"id": "1711.08512", "submitter": "Son-Il Kwak", "authors": "Choe Un-Chol, Yun Kum-Il, Kwak Son-Il", "title": "A Study on Modeling of Inputting Electrical Power of Ultra High Power\n  Electric Furnace by using Fuzzy Rule and Regression Model", "comments": "8 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  : In this paper a method to make inputting electrical model upon factors that\naffect melting process of high ultra power(UHP) electric furnace by using fuzzy\nrule and regression model is suggested and its effectiveness is verified with\nsimulation experiment.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:31:28 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Un-Chol", "Choe", ""], ["Kum-Il", "Yun", ""], ["Son-Il", "Kwak", ""]]}, {"id": "1711.08534", "submitter": "William Wang", "authors": "William Wang, Angelina Wang, Aviv Tamar, Xi Chen, Pieter Abbeel", "title": "Safer Classification by Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discriminative approach to classification using deep neural networks has\nbecome the de-facto standard in various fields. Complementing recent\nreservations about safety against adversarial examples, we show that\nconventional discriminative methods can easily be fooled to provide incorrect\nlabels with very high confidence to out of distribution examples. We posit that\na generative approach is the natural remedy for this problem, and propose a\nmethod for classification using generative models. At training time, we learn a\ngenerative model for each class, while at test time, given an example to\nclassify, we query each generator for its most similar generation, and select\nthe class corresponding to the most similar one. Our approach is general and\ncan be used with expressive models such as GANs and VAEs. At test time, our\nmethod accurately \"knows when it does not know,\" and provides resilience to out\nof distribution examples while maintaining competitive performance for standard\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 23:32:20 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 23:47:59 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Wang", "William", ""], ["Wang", "Angelina", ""], ["Tamar", "Aviv", ""], ["Chen", "Xi", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1711.08819", "submitter": "Piotr Mirowski", "authors": "Kory Wallace Mathewson and Piotr Mirowski", "title": "Improvised Comedy as a Turing Test", "comments": "4 pages, 3 figures. Presented at 31st Conference on Neural\n  Information Processing Systems 2017. Workshop on Machine Learning for\n  Creativity and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The best improvisational theatre actors can make any scene partner, of any\nskill level or ability, appear talented and proficient in the art form, and\nthus \"make them shine\". To challenge this improvisational paradigm, we built an\nartificial intelligence (AI) trained to perform live shows alongside human\nactors for human audiences. Over the course of 30 performances to a combined\naudience of almost 3000 people, we have refined theatrical games which involve\ncombinations of human and (at times, adversarial) AI actors. We have developed\nspecific scene structures to include audience participants in interesting ways.\nFinally, we developed a complete show structure that submitted the audience to\na Turing test and observed their suspension of disbelief, which we believe is\nkey for human/non-human theatre co-creation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 20:13:34 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 00:25:58 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Mathewson", "Kory Wallace", ""], ["Mirowski", "Piotr", ""]]}, {"id": "1711.08859", "submitter": "Aleksandar Zelji\\'c", "authors": "Aleksandar Zeljic, Peter Backeman, Christoph M. Wintersteiger, Philipp\n  Ruemmer", "title": "Exploring Approximations for Floating-Point Arithmetic using UppSAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of solving floating-point constraints obtained from\nsoftware verification. We present UppSAT --- a new implementation of a\nsystematic approximation refinement framework [ZWR17] as an abstract SMT\nsolver. Provided with an approximation and a decision procedure (implemented in\nan off-the-shelf SMT solver), UppSAT yields an approximating SMT solver.\nAdditionally, UppSAT includes a library of predefined approximation components\nwhich can be combined and extended to define new encodings, orderings and\nsolving strategies. We propose that UppSAT can be used as a sandbox for easy\nand flexible exploration of new approximations. To substantiate this, we\nexplore several approximations of floating-point arithmetic. Approximations can\nbe viewed as a composition of an encoding into a target theory, a precision\nordering, and a number of strategies for model reconstruction and precision (or\napproximation) refinement. We present encodings of floating-point arithmetic\ninto reduced precision floating-point arithmetic, real-arithmetic, and\nfixed-point arithmetic (encoded in the theory of bit-vectors). In an\nexperimental evaluation, we compare the advantages and disadvantages of\napproximating solvers obtained by combining various encodings and decision\nprocedures (based on existing state-of-the-art SMT solvers for floating-point,\nreal, and bit-vector arithmetic).\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 02:23:41 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 14:54:19 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Zeljic", "Aleksandar", ""], ["Backeman", "Peter", ""], ["Wintersteiger", "Christoph M.", ""], ["Ruemmer", "Philipp", ""]]}, {"id": "1711.08921", "submitter": "Pascal Kerschke", "authors": "Pascal Kerschke and Heike Trautmann", "title": "Automated Algorithm Selection on Continuous Black-Box Problems By\n  Combining Exploratory Landscape Analysis and Machine Learning", "comments": "This is the author's final version, and the article has been accepted\n  for publication in Evolutionary Computation", "journal-ref": null, "doi": "10.1162/evco_a_00236", "report-no": null, "categories": "stat.ML cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build upon previous work on designing informative and\nefficient Exploratory Landscape Analysis features for characterizing problems'\nlandscapes and show their effectiveness in automatically constructing algorithm\nselection models in continuous black-box optimization problems. Focussing on\nalgorithm performance results of the COCO platform of several years, we\nconstruct a representative set of high-performing complementary solvers and\npresent an algorithm selection model that - compared to the portfolio's single\nbest solver - on average requires less than half of the resources for solving a\ngiven problem. Therefore, there is a huge gain in efficiency compared to\nclassical ensemble methods combined with an increased insight into problem\ncharacteristics and algorithm properties by using informative features. Acting\non the assumption that the function set of the Black-Box Optimization Benchmark\nis representative enough for practical applications the model allows for\nselecting the best suited optimization algorithm within the considered set for\nunseen problems prior to the optimization itself based on a small sample of\nfunction evaluations. Note that such a sample can even be reused for the\ninitial population of an evolutionary (optimization) algorithm so that even the\nfeature costs become negligible.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 10:35:02 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 18:37:29 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 07:05:21 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Kerschke", "Pascal", ""], ["Trautmann", "Heike", ""]]}, {"id": "1711.08946", "submitter": "Arash Tavakoli", "authors": "Arash Tavakoli, Fabio Pardo, Petar Kormushev", "title": "Action Branching Architectures for Deep Reinforcement Learning", "comments": "AAAI 2018, NIPS 2017 Deep RL Symposium, code:\n  https://github.com/atavakol/action-branching-agents", "journal-ref": "AAAI 32: 4131-4138 (2018)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete-action algorithms have been central to numerous recent successes of\ndeep reinforcement learning. However, applying these algorithms to\nhigh-dimensional action tasks requires tackling the combinatorial increase of\nthe number of possible actions with the number of action dimensions. This\nproblem is further exacerbated for continuous-action tasks that require fine\ncontrol of actions via discretization. In this paper, we propose a novel neural\narchitecture featuring a shared decision module followed by several network\nbranches, one for each action dimension. This approach achieves a linear\nincrease of the number of network outputs with the number of degrees of freedom\nby allowing a level of independence for each individual action dimension. To\nillustrate the approach, we present a novel agent, called Branching Dueling\nQ-Network (BDQ), as a branching variant of the Dueling Double Deep Q-Network\n(Dueling DDQN). We evaluate the performance of our agent on a set of\nchallenging continuous control tasks. The empirical results show that the\nproposed agent scales gracefully to environments with increasing action\ndimensionality and indicate the significance of the shared decision module in\ncoordination of the distributed action branches. Furthermore, we show that the\nproposed agent performs competitively against a state-of-the-art continuous\ncontrol algorithm, Deep Deterministic Policy Gradient (DDPG).\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 12:45:30 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 04:01:24 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Tavakoli", "Arash", ""], ["Pardo", "Fabio", ""], ["Kormushev", "Petar", ""]]}, {"id": "1711.09048", "submitter": "Francisco Garcia", "authors": "Francisco M. Garcia, Bruno C. da Silva, and Philip S. Thomas", "title": "A Compression-Inspired Framework for Macro Discovery", "comments": "Accepted as Extended Abstract, AAMAS, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of how a reinforcement learning agent\ntasked with solving a set of related Markov decision processes can use\nknowledge acquired early in its lifetime to improve its ability to more rapidly\nsolve novel, but related, tasks. One way of exploiting this experience is by\nidentifying recurrent patterns in trajectories obtained from well-performing\npolicies. We propose a three-step framework in which an agent 1) generates a\nset of candidate open-loop macros by compressing trajectories drawn from\nnear-optimal policies; 2) evaluates the value of each macro; and 3) selects a\nmaximally diverse subset of macros that spans the space of policies typically\nrequired for solving the set of related tasks. Our experiments show that\nextending the original primitive action-set of the agent with the identified\nmacros allows it to more rapidly learn an optimal policy in unseen, but similar\nMDPs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 16:58:45 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2019 05:24:49 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 19:47:14 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Garcia", "Francisco M.", ""], ["da Silva", "Bruno C.", ""], ["Thomas", "Philip S.", ""]]}, {"id": "1711.09055", "submitter": "Giovanni Saponaro", "authors": "Giovanni Saponaro, Lorenzo Jamone, Alexandre Bernardino and Giampiero\n  Salvi", "title": "Interactive Robot Learning of Gestures, Language and Affordances", "comments": "code available at https://github.com/gsaponaro/glu-gestures", "journal-ref": "International Workshop on Grounding Language Understanding (GLU),\n  Satellite of Interspeech 2017", "doi": "10.21437/GLU.2017-17", "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing field in robotics and Artificial Intelligence (AI) research is\nhuman-robot collaboration, whose target is to enable effective teamwork between\nhumans and robots. However, in many situations human teams are still superior\nto human-robot teams, primarily because human teams can easily agree on a\ncommon goal with language, and the individual members observe each other\neffectively, leveraging their shared motor repertoire and sensorimotor\nresources. This paper shows that for cognitive robots it is possible, and\nindeed fruitful, to combine knowledge acquired from interacting with elements\nof the environment (affordance exploration) with the probabilistic observation\nof another agent's actions.\n  We propose a model that unites (i) learning robot affordances and word\ndescriptions with (ii) statistical recognition of human gestures with vision\nsensors. We discuss theoretical motivations, possible implementations, and we\nshow initial results which highlight that, after having acquired knowledge of\nits surrounding environment, a humanoid robot can generalize this knowledge to\nthe case when it observes another agent (human partner) performing the same\nmotor actions previously executed during training.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 17:34:32 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Saponaro", "Giovanni", ""], ["Jamone", "Lorenzo", ""], ["Bernardino", "Alexandre", ""], ["Salvi", "Giampiero", ""]]}, {"id": "1711.09057", "submitter": "Alejandro Torre\\~no", "authors": "Alejandro Torre\\~no, Eva Onaindia, Anton\\'in Komenda, Michal\n  \\v{S}tolba", "title": "Cooperative Multi-Agent Planning: A Survey", "comments": "34 pages, 4 figures, 4 tables", "journal-ref": "ACM Computing Surveys, Volume 50, Number 6, Article 84.\n  Publication date: November 2017", "doi": "10.1145/3128584", "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative multi-agent planning (MAP) is a relatively recent research field\nthat combines technologies, algorithms and techniques developed by the\nArtificial Intelligence Planning and Multi-Agent Systems communities. While\nplanning has been generally treated as a single-agent task, MAP generalizes\nthis concept by considering multiple intelligent agents that work cooperatively\nto develop a course of action that satisfies the goals of the group.\n  This paper reviews the most relevant approaches to MAP, putting the focus on\nthe solvers that took part in the 2015 Competition of Distributed and\nMulti-Agent Planning, and classifies them according to their key features and\nrelative performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 17:43:14 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Torre\u00f1o", "Alejandro", ""], ["Onaindia", "Eva", ""], ["Komenda", "Anton\u00edn", ""], ["\u0160tolba", "Michal", ""]]}, {"id": "1711.09142", "submitter": "Zhuo Xu", "authors": "Zhuo Xu, Haonan Chang, and Masayoshi Tomizuka", "title": "Cascade Attribute Learning Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the cascade attribute learning network (CALNet), which can learn\nattributes in a control task separately and assemble them together. Our\ncontribution is twofold: first we propose attribute learning in reinforcement\nlearning (RL). Attributes used to be modeled using constraint functions or\nterms in the objective function, making it hard to transfer. Attribute\nlearning, on the other hand, models these task properties as modules in the\npolicy network. We also propose using novel cascading compensative networks in\nthe CALNet to learn and assemble attributes. Using the CALNet, one can zero\nshoot an unseen task by separately learning all its attributes, and assembling\nthe attribute modules. We have validated the capacity of our model on a wide\nvariety of control problems with attributes in time, position, velocity and\nacceleration phases.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 21:12:52 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Xu", "Zhuo", ""], ["Chang", "Haonan", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1711.09186", "submitter": "Xinyang Deng", "authors": "Xinyang Deng and Wen Jiang", "title": "D numbers theory based game-theoretic framework in adversarial decision\n  making under fuzzy environment", "comments": "59 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial decision making is a particular type of decision making problem\nwhere the gain a decision maker obtains as a result of his decisions is\naffected by the actions taken by others. Representation of alternatives'\nevaluations and methods to find the optimal alternative are two important\naspects in the adversarial decision making. The aim of this study is to develop\na general framework for solving the adversarial decision making issue under\nuncertain environment. By combining fuzzy set theory, game theory and D numbers\ntheory (DNT), a DNT based game-theoretic framework for adversarial decision\nmaking under fuzzy environment is presented. Within the proposed framework or\nmodel, fuzzy set theory is used to model the uncertain evaluations of decision\nmakers to alternatives, the non-exclusiveness among fuzzy evaluations are taken\ninto consideration by using DNT, and the conflict of interests among decision\nmakers is considered in a two-person non-constant sum game theory perspective.\nAn illustrative application is given to demonstrate the effectiveness of the\nproposed model. This work, on one hand, has developed an effective framework\nfor adversarial decision making under fuzzy environment; One the other hand, it\nhas further improved the basis of DNT as a generalization of Dempster-Shafer\ntheory for uncertainty reasoning.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 04:16:43 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Deng", "Xinyang", ""], ["Jiang", "Wen", ""]]}, {"id": "1711.09268", "submitter": "Daniel L\\'evy", "authors": "Daniel Levy, Matthew D. Hoffman, Jascha Sohl-Dickstein", "title": "Generalizing Hamiltonian Monte Carlo with Neural Networks", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general-purpose method to train Markov chain Monte Carlo\nkernels, parameterized by deep neural networks, that converge and mix quickly\nto their target distribution. Our method generalizes Hamiltonian Monte Carlo\nand is trained to maximize expected squared jumped distance, a proxy for mixing\nspeed. We demonstrate large empirical gains on a collection of simple but\nchallenging distributions, for instance achieving a 106x improvement in\neffective sample size in one case, and mixing when standard HMC makes no\nmeasurable progress in a second. Finally, we show quantitative and qualitative\ngains on a real-world task: latent-variable generative modeling. We release an\nopen source TensorFlow implementation of the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 18:08:02 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 06:55:12 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 21:05:40 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Levy", "Daniel", ""], ["Hoffman", "Matthew D.", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1711.09357", "submitter": "Yao Lu", "authors": "Linqing Liu, Yao Lu, Min Yang, Qiang Qu, Jia Zhu, Hongyan Li", "title": "Generative Adversarial Network for Abstractive Text Summarization", "comments": "AAAI 2018 abstract, Supplemental material:\n  http://likicode.com/textsum/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an adversarial process for abstractive text\nsummarization, in which we simultaneously train a generative model G and a\ndiscriminative model D. In particular, we build the generator G as an agent of\nreinforcement learning, which takes the raw text as input and predicts the\nabstractive summarization. We also build a discriminator which attempts to\ndistinguish the generated summary from the ground truth summary. Extensive\nexperiments demonstrate that our model achieves competitive ROUGE scores with\nthe state-of-the-art methods on CNN/Daily Mail dataset. Qualitatively, we show\nthat our model is able to generate more abstractive, readable and diverse\nsummaries.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 09:45:04 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Liu", "Linqing", ""], ["Lu", "Yao", ""], ["Yang", "Min", ""], ["Qu", "Qiang", ""], ["Zhu", "Jia", ""], ["Li", "Hongyan", ""]]}, {"id": "1711.09395", "submitter": "Igor Melnyk", "authors": "Igor Melnyk, Cicero Nogueira dos Santos, Kahini Wadhawan, Inkit Padhi,\n  Abhishek Kumar", "title": "Improved Neural Text Attribute Transfer with Non-parallel Data", "comments": "NIPS 2017 Workshop on Learning Disentangled Representations: from\n  Perception to Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text attribute transfer using non-parallel data requires methods that can\nperform disentanglement of content and linguistic attributes. In this work, we\npropose multiple improvements over the existing approaches that enable the\nencoder-decoder framework to cope with the text attribute transfer from\nnon-parallel data. We perform experiments on the sentiment transfer task using\ntwo datasets. For both datasets, our proposed method outperforms a strong\nbaseline in two of the three employed evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 14:42:52 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 23:20:56 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Melnyk", "Igor", ""], ["Santos", "Cicero Nogueira dos", ""], ["Wadhawan", "Kahini", ""], ["Padhi", "Inkit", ""], ["Kumar", "Abhishek", ""]]}, {"id": "1711.09401", "submitter": "Long Ouyang", "authors": "Long Ouyang and Michael C. Frank", "title": "Pedagogical learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common assumption in machine learning is that training data are i.i.d.\nsamples from some distribution. Processes that generate i.i.d. samples are, in\na sense, uninformative---they produce data without regard to how good this data\nis for learning. By contrast, cognitive science research has shown that when\npeople generate training data for others (i.e., teaching), they deliberately\nselect examples that are helpful for learning. Because the data is more\ninformative, learning can require less data. Interestingly, such examples are\nmost effective when learners know that the data were pedagogically generated\n(as opposed to randomly generated). We call this pedagogical learning---when a\nlearner assumes that evidence comes from a helpful teacher. In this work, we\nask how pedagogical learning might work for machine learning algorithms.\nStudying this question requires understanding how people actually teach complex\nconcepts with examples, so we conducted a behavioral study examining how people\nteach regular expressions using example strings. We found that teachers'\nexamples contain powerful clustering structure that can greatly facilitate\nlearning. We then develop a model of teaching and show a proof of concept that\nusing this model inside of a learner can improve performance.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 15:17:02 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 22:13:42 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Ouyang", "Long", ""], ["Frank", "Michael C.", ""]]}, {"id": "1711.09414", "submitter": "Boyu Liu", "authors": "Boyu Liu, Yanzhao Wang, Yu-Wing Tai, Chi-Keung Tang", "title": "MAVOT: Memory-Augmented Video Object Tracking", "comments": "Submitted to CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a one-shot learning approach for video object tracking. The\nproposed algorithm requires seeing the object to be tracked only once, and\nemploys an external memory to store and remember the evolving features of the\nforeground object as well as backgrounds over time during tracking. With the\nrelevant memory retrieved and updated in each tracking, our tracking model is\ncapable of maintaining long-term memory of the object, and thus can naturally\ndeal with hard tracking scenarios including partial and total occlusion, motion\nchanges and large scale and shape variations. In our experiments we use the\nImageNet ILSVRC2015 video detection dataset to train and use the VOT-2016\nbenchmark to test and compare our Memory-Augmented Video Object Tracking\n(MAVOT) model. From the results, we conclude that given its oneshot property\nand simplicity in design, MAVOT is an attractive approach in visual tracking\nbecause it shows good performance on VOT-2016 benchmark and is among the top 5\nperformers in accuracy and robustness in occlusion, motion changes and empty\ntarget.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 16:20:45 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Liu", "Boyu", ""], ["Wang", "Yanzhao", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1711.09441", "submitter": "Matteo Brunelli", "authors": "Bice Cavallo and Matteo Brunelli", "title": "A general unified framework for interval pairwise comparison matrices", "comments": null, "journal-ref": "International Journal of Approximate Reasoning, 93, 178--198, 2018", "doi": "10.1016/j.ijar.2017.11.002", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval Pairwise Comparison Matrices have been widely used to account for\nuncertain statements concerning the preferences of decision makers. Several\napproaches have been proposed in the literature, such as multiplicative and\nfuzzy interval matrices. In this paper, we propose a general unified approach\nto Interval Pairwise Comparison Matrices, based on Abelian linearly ordered\ngroups. In this framework, we generalize some consistency conditions provided\nfor multiplicative and/or fuzzy interval pairwise comparison matrices and\nprovide inclusion relations between them. Then, we provide a concept of\ndistance between intervals that, together with a notion of mean defined over\nreal continuous Abelian linearly ordered groups, allows us to provide a\nconsistency index and an indeterminacy index. In this way, by means of suitable\nisomorphisms between Abelian linearly ordered groups, we will be able to\ncompare the inconsistency and the indeterminacy of different kinds of Interval\nPairwise Comparison Matrices, e.g. multiplicative, additive, and fuzzy, on a\nunique Cartesian coordinate system.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 19:15:24 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Cavallo", "Bice", ""], ["Brunelli", "Matteo", ""]]}, {"id": "1711.09442", "submitter": "Lucas Lamata", "authors": "U. Alvarez-Rodriguez, M. Sanz, L. Lamata, E. Solano", "title": "Quantum Artificial Life in an IBM Quantum Computer", "comments": null, "journal-ref": "Scientific Reports 8, 14793 (2018)", "doi": "10.1038/s41598-018-33125-3", "report-no": null, "categories": "quant-ph cond-mat.mes-hall cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first experimental realization of a quantum artificial life\nalgorithm in a quantum computer. The quantum biomimetic protocol encodes\ntailored quantum behaviors belonging to living systems, namely,\nself-replication, mutation, interaction between individuals, and death, into\nthe cloud quantum computer IBM ibmqx4. In this experiment, entanglement spreads\nthroughout generations of individuals, where genuine quantum information\nfeatures are inherited through genealogical networks. As a pioneering\nproof-of-principle, experimental data fits the ideal model with accuracy.\nThereafter, these and other models of quantum artificial life, for which no\nclassical device may predict its quantum supremacy evolution, can be further\nexplored in novel generations of quantum computers. Quantum biomimetics,\nquantum machine learning, and quantum artificial intelligence will move forward\nhand in hand through more elaborate levels of quantum complexity.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 19:44:41 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 09:46:56 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Alvarez-Rodriguez", "U.", ""], ["Sanz", "M.", ""], ["Lamata", "L.", ""], ["Solano", "E.", ""]]}, {"id": "1711.09561", "submitter": "Emad Barsoum", "authors": "Emad Barsoum, John Kender and Zicheng Liu", "title": "HP-GAN: Probabilistic 3D human motion prediction via GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting and understanding human motion dynamics has many applications,\nsuch as motion synthesis, augmented reality, security, and autonomous vehicles.\nDue to the recent success of generative adversarial networks (GAN), there has\nbeen much interest in probabilistic estimation and synthetic data generation\nusing deep neural network architectures and learning algorithms.\n  We propose a novel sequence-to-sequence model for probabilistic human motion\nprediction, trained with a modified version of improved Wasserstein generative\nadversarial networks (WGAN-GP), in which we use a custom loss function designed\nfor human motion prediction. Our model, which we call HP-GAN, learns a\nprobability density function of future human poses conditioned on previous\nposes. It predicts multiple sequences of possible future human poses, each from\nthe same input sequence but a different vector z drawn from a random\ndistribution. Furthermore, to quantify the quality of the non-deterministic\npredictions, we simultaneously train a motion-quality-assessment model that\nlearns the probability that a given skeleton sequence is a real human motion.\n  We test our algorithm on two of the largest skeleton datasets: NTURGB-D and\nHuman3.6M. We train our model on both single and multiple action types. Its\npredictive power for long-term motion estimation is demonstrated by generating\nmultiple plausible futures of more than 30 frames from just 10 frames of input.\nWe show that most sequences generated from the same input have more than 50\\%\nprobabilities of being judged as a real human sequence. We will release all the\ncode used in this paper to Github.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 07:07:11 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Barsoum", "Emad", ""], ["Kender", "John", ""], ["Liu", "Zicheng", ""]]}, {"id": "1711.09601", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach,\n  Tinne Tuytelaars", "title": "Memory Aware Synapses: Learning what (not) to forget", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can learn in a continuous manner. Old rarely utilized knowledge can be\noverwritten by new incoming information while important, frequently used\nknowledge is prevented from being erased. In artificial learning systems,\nlifelong learning so far has focused mainly on accumulating knowledge over\ntasks and overcoming catastrophic forgetting. In this paper, we argue that,\ngiven the limited model capacity and the unlimited new information to be\nlearned, knowledge has to be preserved or erased selectively. Inspired by\nneuroplasticity, we propose a novel approach for lifelong learning, coined\nMemory Aware Synapses (MAS). It computes the importance of the parameters of a\nneural network in an unsupervised and online manner. Given a new sample which\nis fed to the network, MAS accumulates an importance measure for each parameter\nof the network, based on how sensitive the predicted output function is to a\nchange in this parameter. When learning a new task, changes to important\nparameters can then be penalized, effectively preventing important knowledge\nrelated to previous tasks from being overwritten. Further, we show an\ninteresting connection between a local version of our method and Hebb's\nrule,which is a model for the learning process in the brain. We test our method\non a sequence of object recognition tasks and on the challenging problem of\nlearning an embedding for predicting $<$subject, predicate, object$>$ triplets.\nWe show state-of-the-art performance and, for the first time, the ability to\nadapt the importance of the parameters based on unlabeled data towards what the\nnetwork needs (not) to forget, which may vary depending on test conditions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 09:48:44 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 10:46:56 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 16:41:42 GMT"}, {"version": "v4", "created": "Fri, 5 Oct 2018 08:40:30 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Babiloni", "Francesca", ""], ["Elhoseiny", "Mohamed", ""], ["Rohrbach", "Marcus", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1711.09602", "submitter": "Aniruddh Raghu", "authors": "Aniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter\n  Szolovits, Marzyeh Ghassemi", "title": "Deep Reinforcement Learning for Sepsis Treatment", "comments": "Extensions on earlier work (arXiv:1705.08422). Accepted at workshop\n  on Machine Learning For Health at the conference on Neural Information\n  Processing Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sepsis is a leading cause of mortality in intensive care units and costs\nhospitals billions annually. Treating a septic patient is highly challenging,\nbecause individual patients respond very differently to medical interventions\nand there is no universally agreed-upon treatment for sepsis. In this work, we\npropose an approach to deduce treatment policies for septic patients by using\ncontinuous state-space models and deep reinforcement learning. Our model learns\nclinically interpretable treatment policies, similar in important aspects to\nthe treatment policies of physicians. The learned policies could be used to aid\nintensive care clinicians in medical decision making and improve the likelihood\nof patient survival.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 09:49:57 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Raghu", "Aniruddh", ""], ["Komorowski", "Matthieu", ""], ["Ahmed", "Imran", ""], ["Celi", "Leo", ""], ["Szolovits", "Peter", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "1711.09681", "submitter": "YoungJoon Yoo", "authors": "YoungJoon Yoo, Seonguk Park, Junyoung Choi, Sangdoo Yun, Nojun Kwak", "title": "Butterfly Effect: Bidirectional Control of Classification Performance by\n  Small Additive Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new algorithm for controlling classification results by\ngenerating a small additive perturbation without changing the classifier\nnetwork. Our work is inspired by existing works generating adversarial\nperturbation that worsens classification performance. In contrast to the\nexisting methods, our work aims to generate perturbations that can enhance\noverall classification performance. To solve this performance enhancement\nproblem, we newly propose a perturbation generation network (PGN) influenced by\nthe adversarial learning strategy. In our problem, the information in a large\nexternal dataset is summarized by a small additive perturbation, which helps to\nimprove the performance of the classifier trained with the target dataset. In\naddition to this performance enhancement problem, we show that the proposed PGN\ncan be adopted to solve the classical adversarial problem without utilizing the\ninformation on the target classifier. The mentioned characteristics of our\nmethod are verified through extensive experiments on publicly available visual\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:32:45 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 05:39:10 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Yoo", "YoungJoon", ""], ["Park", "Seonguk", ""], ["Choi", "Junyoung", ""], ["Yun", "Sangdoo", ""], ["Kwak", "Nojun", ""]]}, {"id": "1711.09684", "submitter": "Aniruddha Tammewar", "authors": "Aniruddha Tammewar, Monik Pamecha, Chirag Jain, Apurva Nagvenkar,\n  Krupal Modi", "title": "Production Ready Chatbots: Generate if not Retrieve", "comments": "DEEPDIAL-18, AAAI-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a hybrid model that combines a neural\nconversational model and a rule-based graph dialogue system that assists users\nin scheduling reminders through a chat conversation. The graph based system has\nhigh precision and provides a grammatically accurate response but has a low\nrecall. The neural conversation model can cater to a variety of requests, as it\ngenerates the responses word by word as opposed to using canned responses. The\nhybrid system shows significant improvements over the existing baseline system\nof rule based approach and caters to complex queries with a domain-restricted\nneural model. Restricting the conversation topic and combination of graph based\nretrieval system with a neural generative model makes the final system robust\nenough for a real world application.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:40:15 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Tammewar", "Aniruddha", ""], ["Pamecha", "Monik", ""], ["Jain", "Chirag", ""], ["Nagvenkar", "Apurva", ""], ["Modi", "Krupal", ""]]}, {"id": "1711.09724", "submitter": "Tianyu Liu", "authors": "Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang and Zhifang Sui", "title": "Table-to-text Generation by Structure-aware Seq2seq Learning", "comments": "Accepted by AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table-to-text generation aims to generate a description for a factual table\nwhich can be viewed as a set of field-value records. To encode both the content\nand the structure of a table, we propose a novel structure-aware seq2seq\narchitecture which consists of field-gating encoder and description generator\nwith dual attention. In the encoding phase, we update the cell memory of the\nLSTM unit by a field gate and its corresponding field value in order to\nincorporate field information into table representation. In the decoding phase,\ndual attention mechanism which contains word level attention and field level\nattention is proposed to model the semantic relevance between the generated\ndescription and the table. We conduct experiments on the \\texttt{WIKIBIO}\ndataset which contains over 700k biographies and corresponding infoboxes from\nWikipedia. The attention visualizations and case studies show that our model is\ncapable of generating coherent and informative descriptions based on the\ncomprehensive understanding of both the content and the structure of a table.\nAutomatic evaluations also show our model outperforms the baselines by a great\nmargin. Code for this work is available on\nhttps://github.com/tyliupku/wiki2bio.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 14:55:17 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Liu", "Tianyu", ""], ["Wang", "Kexiang", ""], ["Sha", "Lei", ""], ["Chang", "Baobao", ""], ["Sui", "Zhifang", ""]]}, {"id": "1711.09744", "submitter": "Clemente Rubio-Manzano", "authors": "Clemente Rubio-Manzano, Tomas Lermanda Senoceain", "title": "How linguistic descriptions of data can help to the teaching-learning\n  process in higher education, case of study: artificial intelligence", "comments": null, "journal-ref": "Journal of Intelligent & Fuzzy Systems, vol. 37, no. 6, pp.\n  8397-8415, 2019", "doi": "10.3233/JIFS-190935", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence is a central topic in the computer science\ncurriculum. From the year 2011 a project-based learning methodology based on\ncomputer games has been designed and implemented into the intelligence\nartificial course at the University of the Bio-Bio. The project aims to develop\nsoftware-controlled agents (bots) which are programmed by using heuristic\nalgorithms seen during the course. This methodology allows us to obtain good\nlearning results, however several challenges have been founded during its\nimplementation.\n  In this paper we show how linguistic descriptions of data can help to provide\nstudents and teachers with technical and personalized feedback about the\nlearned algorithms. Algorithm behavior profile and a new Turing test for\ncomputer games bots based on linguistic modelling of complex phenomena are also\nproposed in order to deal with such challenges.\n  In order to show and explore the possibilities of this new technology, a web\nplatform has been designed and implemented by one of authors and its\nincorporation in the process of assessment allows us to improve the teaching\nlearning process.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 15:13:53 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 14:00:27 GMT"}, {"version": "v3", "created": "Tue, 30 Jan 2018 20:00:15 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Rubio-Manzano", "Clemente", ""], ["Senoceain", "Tomas Lermanda", ""]]}, {"id": "1711.09767", "submitter": "Matan Sela", "authors": "Matan Sela, Pingmei Xu, Junfeng He, Vidhya Navalpakkam and Dmitry\n  Lagun", "title": "GazeGAN - Unpaired Adversarial Image Generation for Gaze Estimation", "comments": "Project was done when the first author was at Google Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has demonstrated the ability to estimate gaze on mobile\ndevices by performing inference on the image from the phone's front-facing\ncamera, and without requiring specialized hardware. While this offers wide\npotential applications such as in human-computer interaction, medical diagnosis\nand accessibility (e.g., hands free gaze as input for patients with motor\ndisorders), current methods are limited as they rely on collecting data from\nreal users, which is a tedious and expensive process that is hard to scale\nacross devices. There have been some attempts to synthesize eye region data\nusing 3D models that can simulate various head poses and camera settings,\nhowever these lack in realism.\n  In this paper, we improve upon a recently suggested method, and propose a\ngenerative adversarial framework to generate a large dataset of high resolution\ncolorful images with high diversity (e.g., in subjects, head pose, camera\nsettings) and realism, while simultaneously preserving the accuracy of gaze\nlabels. The proposed approach operates on extended regions of the eye, and even\ncompletes missing parts of the image. Using this rich synthesized dataset, and\nwithout using any additional training data from real users, we demonstrate\nimprovements over state-of-the-art for estimating 2D gaze position on mobile\ndevices. We further demonstrate cross-device generalization of model\nperformance, as well as improved robustness to diverse head pose, blur and\ndistance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 15:32:36 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Sela", "Matan", ""], ["Xu", "Pingmei", ""], ["He", "Junfeng", ""], ["Navalpakkam", "Vidhya", ""], ["Lagun", "Dmitry", ""]]}, {"id": "1711.09784", "submitter": "Nicholas Frosst", "authors": "Nicholas Frosst, Geoffrey Hinton", "title": "Distilling a Neural Network Into a Soft Decision Tree", "comments": "presented at the CEX workshop at AI*IA 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have proved to be a very effective way to perform\nclassification tasks. They excel when the input data is high dimensional, the\nrelationship between the input and the output is complicated, and the number of\nlabeled training examples is large. But it is hard to explain why a learned\nnetwork makes a particular classification decision on a particular test case.\nThis is due to their reliance on distributed hierarchical representations. If\nwe could take the knowledge acquired by the neural net and express the same\nknowledge in a model that relies on hierarchical decisions instead, explaining\na particular decision would be much easier. We describe a way of using a\ntrained neural net to create a type of soft decision tree that generalizes\nbetter than one learned directly from the training data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 15:50:50 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Frosst", "Nicholas", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1711.09883", "submitter": "Jan Leike", "authors": "Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom\n  Everitt, Andrew Lefrancq, Laurent Orseau, Shane Legg", "title": "AI Safety Gridworlds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a suite of reinforcement learning environments illustrating\nvarious safety properties of intelligent agents. These problems include safe\ninterruptibility, avoiding side effects, absent supervisor, reward gaming, safe\nexploration, as well as robustness to self-modification, distributional shift,\nand adversaries. To measure compliance with the intended safe behavior, we\nequip each environment with a performance function that is hidden from the\nagent. This allows us to categorize AI safety problems into robustness and\nspecification problems, depending on whether the performance function\ncorresponds to the observed reward function. We evaluate A2C and Rainbow, two\nrecent deep reinforcement learning agents, on our environments and show that\nthey are not able to solve them satisfactorily.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 18:57:13 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 17:40:36 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Leike", "Jan", ""], ["Martic", "Miljan", ""], ["Krakovna", "Victoria", ""], ["Ortega", "Pedro A.", ""], ["Everitt", "Tom", ""], ["Lefrancq", "Andrew", ""], ["Orseau", "Laurent", ""], ["Legg", "Shane", ""]]}, {"id": "1711.09990", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Identification of Strong Edges in AMP Chain Graphs", "comments": "arXiv admin note: text overlap with arXiv:1303.0691", "journal-ref": "UAI 2018", "doi": null, "report-no": null, "categories": "math.CO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The essential graph is a distinguished member of a Markov equivalence class\nof AMP chain graphs. However, the directed edges in the essential graph are not\nnecessarily strong or invariant, i.e. they may not be shared by every member of\nthe equivalence class. Likewise for the undirected edges. In this paper, we\ndevelop a procedure for identifying which edges in an essential graph are\nstrong. We also show how this makes it possible to bound some causal effects\nwhen the true chain graph is unknown.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 13:04:19 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 10:14:48 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2018 08:24:15 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1711.10046", "submitter": "Morteza Mardani", "authors": "Morteza Mardani, Hatef Monajemi, Vardan Papyan, Shreyas Vasanawala,\n  David Donoho, and John Pauly", "title": "Recurrent Generative Adversarial Networks for Proximal Learning and\n  Automated Compressive Image Recovery", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering images from undersampled linear measurements typically leads to an\nill-posed linear inverse problem, that asks for proper statistical priors.\nBuilding effective priors is however challenged by the low train and test\noverhead dictated by real-time tasks; and the need for retrieving visually\n\"plausible\" and physically \"feasible\" images with minimal hallucination. To\ncope with these challenges, we design a cascaded network architecture that\nunrolls the proximal gradient iterations by permeating benefits from generative\nresidual networks (ResNet) to modeling the proximal operator. A mixture of\npixel-wise and perceptual costs is then deployed to train proximals. The\noverall architecture resembles back-and-forth projection onto the intersection\nof feasible and plausible images. Extensive computational experiments are\nexamined for a global task of reconstructing MR images of pediatric patients,\nand a more local task of superresolving CelebA faces, that are insightful to\ndesign efficient architectures. Our observations indicate that for MRI\nreconstruction, a recurrent ResNet with a single residual block effectively\nlearns the proximal. This simple architecture appears to significantly\noutperform the alternative deep ResNet architecture by 2dB SNR, and the\nconventional compressed-sensing MRI by 4dB SNR with 100x faster inference. For\nimage superresolution, our preliminary results indicate that modeling the\ndenoising proximal demands deep ResNets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 23:45:02 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Mardani", "Morteza", ""], ["Monajemi", "Hatef", ""], ["Papyan", "Vardan", ""], ["Vasanawala", "Shreyas", ""], ["Donoho", "David", ""], ["Pauly", "John", ""]]}, {"id": "1711.10055", "submitter": "Sumeet Singh", "authors": "Sumeet Singh, Jonathan Lacotte, Anirudha Majumdar, Marco Pavone", "title": "Risk-sensitive Inverse Reinforcement Learning via Semi- and\n  Non-Parametric Methods", "comments": "Submitted to International Journal of Robotics Research; Revision 1:\n  (i) Clarified minor technical points; (ii) Revised proof for Theorem 3 to\n  hold under weaker assumptions; (iii) Added additional figures and expanded\n  discussions to improve readability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on Inverse Reinforcement Learning (IRL) typically assumes that\nhumans take actions in order to minimize the expected value of a cost function,\ni.e., that humans are risk neutral. Yet, in practice, humans are often far from\nbeing risk neutral. To fill this gap, the objective of this paper is to devise\na framework for risk-sensitive IRL in order to explicitly account for a human's\nrisk sensitivity. To this end, we propose a flexible class of models based on\ncoherent risk measures, which allow us to capture an entire spectrum of risk\npreferences from risk-neutral to worst-case. We propose efficient\nnon-parametric algorithms based on linear programming and semi-parametric\nalgorithms based on maximum likelihood for inferring a human's underlying risk\nmeasure and cost function for a rich class of static and dynamic\ndecision-making settings. The resulting approach is demonstrated on a simulated\ndriving game with ten human participants. Our method is able to infer and mimic\na wide range of qualitatively different driving styles from highly risk-averse\nto risk-neutral in a data-efficient manner. Moreover, comparisons of the\nRisk-Sensitive (RS) IRL approach with a risk-neutral model show that the RS-IRL\nframework more accurately captures observed participant behavior both\nqualitatively and quantitatively, especially in scenarios where catastrophic\noutcomes such as collisions can occur.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 00:07:10 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 07:24:55 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Singh", "Sumeet", ""], ["Lacotte", "Jonathan", ""], ["Majumdar", "Anirudha", ""], ["Pavone", "Marco", ""]]}, {"id": "1711.10105", "submitter": "Qingquan Song", "authors": "Qingquan Song, Hancheng Ge, James Caverlee, Xia Hu", "title": "Tensor Completion Algorithms in Big Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor completion is a problem of filling the missing or unobserved entries\nof partially observed tensors. Due to the multidimensional character of tensors\nin describing complex datasets, tensor completion algorithms and their\napplications have received wide attention and achievement in areas like data\nmining, computer vision, signal processing, and neuroscience. In this survey,\nwe provide a modern overview of recent advances in tensor completion algorithms\nfrom the perspective of big data analytics characterized by diverse variety,\nlarge volume, and high velocity. We characterize these advances from four\nperspectives: general tensor completion algorithms, tensor completion with\nauxiliary information (variety), scalable tensor completion algorithms\n(volume), and dynamic tensor completion algorithms (velocity). Further, we\nidentify several tensor completion applications on real-world data-driven\nproblems and present some common experimental frameworks popularized in the\nliterature. Our goal is to summarize these popular methods and introduce them\nto researchers and practitioners for promoting future research and\napplications. We conclude with a discussion of key challenges and promising\nresearch directions in this community for future exploration.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 03:44:29 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 02:26:17 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Song", "Qingquan", ""], ["Ge", "Hancheng", ""], ["Caverlee", "James", ""], ["Hu", "Xia", ""]]}, {"id": "1711.10123", "submitter": "Jaehee Jang", "authors": "Jaehee Jang and Byungook Na and Sungroh Yoon", "title": "Homomorphic Parameter Compression for Distributed Deep Learning Training", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training of deep neural networks has received significant\nresearch interest, and its major approaches include implementations on multiple\nGPUs and clusters. Parallelization can dramatically improve the efficiency of\ntraining deep and complicated models with large-scale data. A fundamental\nbarrier against the speedup of DNN training, however, is the trade-off between\ncomputation and communication time. In other words, increasing the number of\nworker nodes decreases the time consumed in computation while simultaneously\nincreasing communication overhead under constrained network bandwidth,\nespecially in commodity hardware environments. To alleviate this trade-off, we\nsuggest the idea of homomorphic parameter compression, which compresses\nparameters with the least expense and trains the DNN with the compressed\nrepresentation. Although the specific method is yet to be discovered, we\ndemonstrate that there is a high probability that the homomorphism can reduce\nthe communication overhead, thanks to little compression and decompression\ntimes. We also provide theoretical speedup of homomorphic compression.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 04:47:59 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Jang", "Jaehee", ""], ["Na", "Byungook", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1711.10125", "submitter": "Yen-Chang Hsu", "authors": "Yen-Chang Hsu, Zhaoyang Lv, Zsolt Kira", "title": "Learning to cluster in order to transfer across domains and tasks", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel method to perform transfer learning across\ndomains and tasks, formulating it as a problem of learning to cluster. The key\ninsight is that, in addition to features, we can transfer similarity\ninformation and this is sufficient to learn a similarity function and\nclustering network to perform both domain adaptation and cross-task transfer\nlearning. We begin by reducing categorical information to pairwise constraints,\nwhich only considers whether two instances belong to the same class or not.\nThis similarity is category-agnostic and can be learned from data in the source\ndomain using a similarity network. We then present two novel approaches for\nperforming transfer learning using this similarity function. First, for\nunsupervised domain adaptation, we design a new loss function to regularize\nclassification with a constrained clustering loss, hence learning a clustering\nnetwork with the transferred similarity metric generating the training inputs.\nSecond, for cross-task learning (i.e., unsupervised clustering with unseen\ncategories), we propose a framework to reconstruct and estimate the number of\nsemantic clusters, again using the clustering network. Since the similarity\nnetwork is noisy, the key is to use a robust clustering algorithm, and we show\nthat our formulation is more robust than the alternative constrained and\nunconstrained clustering approaches. Using this method, we first show state of\nthe art results for the challenging cross-task problem, applied on Omniglot and\nImageNet. Our results show that we can reconstruct semantic clusters with high\naccuracy. We then evaluate the performance of cross-domain transfer using\nimages from the Office-31 and SVHN-MNIST tasks and present top accuracy on both\ndatasets. Our approach doesn't explicitly deal with domain discrepancy. If we\ncombine with a domain adaptation loss, it shows further improvement.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 04:59:58 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 15:54:59 GMT"}, {"version": "v3", "created": "Sat, 17 Mar 2018 04:42:49 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Hsu", "Yen-Chang", ""], ["Lv", "Zhaoyang", ""], ["Kira", "Zsolt", ""]]}, {"id": "1711.10137", "submitter": "Jake Bruce", "authors": "Jake Bruce, Niko Suenderhauf, Piotr Mirowski, Raia Hadsell, Michael\n  Milford", "title": "One-Shot Reinforcement Learning for Robot Navigation with Interactive\n  Replay", "comments": "NIPS Workshop on Acting and Interacting in the Real World: Challenges\n  in Robot Learning", "journal-ref": "Bruce, Jake, et al. \"One-Shot Reinforcement Learning for Robot\n  Navigation with Interactive Replay.\" Proceedings of the NIPS Workshop on\n  Acting and Interacting in the Real World: Challenges in Robot Learning. 2017", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, model-free reinforcement learning algorithms have been shown to\nsolve challenging problems by learning from extensive interaction with the\nenvironment. A significant issue with transferring this success to the robotics\ndomain is that interaction with the real world is costly, but training on\nlimited experience is prone to overfitting. We present a method for learning to\nnavigate, to a fixed goal and in a known environment, on a mobile robot. The\nrobot leverages an interactive world model built from a single traversal of the\nenvironment, a pre-trained visual feature encoder, and stochastic environmental\naugmentation, to demonstrate successful zero-shot transfer under real-world\nenvironmental variations without fine-tuning.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 06:03:14 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 02:56:39 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Bruce", "Jake", ""], ["Suenderhauf", "Niko", ""], ["Mirowski", "Piotr", ""], ["Hadsell", "Raia", ""], ["Milford", "Michael", ""]]}, {"id": "1711.10166", "submitter": "Tomas Kliegr", "authors": "Tomas Kliegr", "title": "QCBA: Postoptimization of Quantitative Attributes in Classifiers based\n  on Association Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to prediscretize numeric attributes before they can be used in\nassociation rule learning is a source of inefficiencies in the resulting\nclassifier. This paper describes several new rule tuning steps aiming to\nrecover information lost in the discretization of numeric (quantitative)\nattributes, and a new rule pruning strategy, which further reduces the size of\nthe classification models. We demonstrate the effectiveness of the proposed\nmethods on postoptimization of models generated by three state-of-the-art\nassociation rule classification algorithms: Classification based on\nAssociations (Liu, 1998), Interpretable Decision Sets (Lakkaraju et al, 2016),\nand Scalable Bayesian Rule Lists (Yang, 2017). Benchmarks on 22 datasets from\nthe UCI repository show that the postoptimized models are consistently smaller\n-- typically by about 50% -- and have better classification performance on most\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 08:09:14 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 12:22:17 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Kliegr", "Tomas", ""]]}, {"id": "1711.10177", "submitter": "Kevin O'Regan", "authors": "Guglielmo Montone, J. Kevin O'Regan, Alexander V. Terekhov", "title": "Gradual Tuning: a better way of Fine Tuning the parameters of a Deep\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an alternative strategy for fine-tuning the\nparameters of a network. We named the technique Gradual Tuning. Once trained on\na first task, the network is fine-tuned on a second task by modifying a\nprogressively larger set of the network's parameters. We test Gradual Tuning on\ndifferent transfer learning tasks, using networks of different sizes trained\nwith different regularization techniques. The result shows that compared to the\nusual fine tuning, our approach significantly reduces catastrophic forgetting\nof the initial task, while still retaining comparable if not better performance\non the new task.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 08:48:39 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Montone", "Guglielmo", ""], ["O'Regan", "J. Kevin", ""], ["Terekhov", "Alexander V.", ""]]}, {"id": "1711.10185", "submitter": "Kevin O'Regan", "authors": "Guglielmo Montone, J. Kevin O'Regan, Alexander V. Terekhov", "title": "Hyper-dimensional computing for a visual question-answering system that\n  is trainable end-to-end", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a system for visual question answering. Our\narchitecture is composed of two parts, the first part creates the logical\nknowledge base given the image. The second part evaluates questions against the\nknowledge base. Differently from previous work, the knowledge base is\nrepresented using hyper-dimensional computing. This choice has the advantage\nthat all the operations in the system, namely creating the knowledge base and\nevaluating the questions against it, are differentiable, thereby making the\nsystem easily trainable in an end-to-end fashion.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 09:00:55 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Montone", "Guglielmo", ""], ["O'Regan", "J. Kevin", ""], ["Terekhov", "Alexander V.", ""]]}, {"id": "1711.10207", "submitter": "Mohsen Ahmadi Fahandar", "authors": "Mohsen Ahmadi Fahandar, Eyke H\\\"ullermeier", "title": "Learning to Rank based on Analogical Reasoning", "comments": "Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18), 8\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object ranking or \"learning to rank\" is an important problem in the realm of\npreference learning. On the basis of training data in the form of a set of\nrankings of objects represented as feature vectors, the goal is to learn a\nranking function that predicts a linear order of any new set of objects. In\nthis paper, we propose a new approach to object ranking based on principles of\nanalogical reasoning. More specifically, our inference pattern is formalized in\nterms of so-called analogical proportions and can be summarized as follows:\nGiven objects $A,B,C,D$, if object $A$ is known to be preferred to $B$, and $C$\nrelates to $D$ as $A$ relates to $B$, then $C$ is (supposedly) preferred to\n$D$. Our method applies this pattern as a main building block and combines it\nwith ideas and techniques from instance-based learning and rank aggregation.\nBased on first experimental results for data sets from various domains (sports,\neducation, tourism, etc.), we conclude that our approach is highly competitive.\nIt appears to be specifically interesting in situations in which the objects\nare coming from different subdomains, and which hence require a kind of\nknowledge transfer.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 09:51:18 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Fahandar", "Mohsen Ahmadi", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "1711.10241", "submitter": "Mithun Chakraborty", "authors": "Nawal Benabbou, Mithun Chakraborty, Vinh Ho Xuan, Jakub Sliwinski,\n  Yair Zick", "title": "The Price of Quota-based Diversity in Assignment Problems", "comments": null, "journal-ref": "TEAC 8.3.14 (2020) 1-32", "doi": "10.1145/3411513", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and analyze an extension to the matching problem on a weighted\nbipartite graph: Assignment with Type Constraints. The two parts of the graph\nare partitioned into subsets called types and blocks; we seek a matching with\nthe largest sum of weights under the constraint that there is a pre-specified\ncap on the number of vertices matched in every type-block pair. Our primary\nmotivation stems from the public housing program of Singapore, accounting for\nover 70% of its residential real estate. To promote ethnic diversity within its\nhousing projects, Singapore imposes ethnicity quotas: each new housing\ndevelopment comprises blocks of flats and each ethnicity-based group in the\npopulation must not own more than a certain percentage of flats in a block.\nOther domains using similar hard capacity constraints include matching\nprospective students to schools or medical residents to hospitals. Limiting\nagents' choices for ensuring diversity in this manner naturally entails some\nwelfare loss. One of our goals is to study the trade-off between diversity and\nsocial welfare in such settings. We first show that, while the classic\nassignment program is polynomial-time computable, adding diversity constraints\nmakes it computationally intractable; however, we identify a\n$\\tfrac{1}{2}$-approximation algorithm, as well as reasonable assumptions on\nthe weights that permit poly-time algorithms. Next, we provide two upper bounds\non the price of diversity -- a measure of the loss in welfare incurred by\nimposing diversity constraints -- as functions of natural problem parameters.\nWe conclude the paper with simulations based on publicly available data from\ntwo diversity-constrained allocation problems -- Singapore Public Housing and\nChicago School Choice -- which shed light on how the constrained maximization\nas well as lottery-based variants perform in practice.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 11:58:54 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 08:54:55 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2018 06:43:20 GMT"}, {"version": "v4", "created": "Wed, 12 Sep 2018 07:51:10 GMT"}, {"version": "v5", "created": "Sat, 1 Dec 2018 10:53:13 GMT"}, {"version": "v6", "created": "Wed, 5 Dec 2018 10:07:25 GMT"}, {"version": "v7", "created": "Sat, 19 Jan 2019 07:08:11 GMT"}, {"version": "v8", "created": "Sat, 3 Oct 2020 10:03:44 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Benabbou", "Nawal", ""], ["Chakraborty", "Mithun", ""], ["Xuan", "Vinh Ho", ""], ["Sliwinski", "Jakub", ""], ["Zick", "Yair", ""]]}, {"id": "1711.10314", "submitter": "Shayegan Omidshafiei", "authors": "Shayegan Omidshafiei, Dong-Ki Kim, Jason Pazis, Jonathan P. How", "title": "Crossmodal Attentive Skill Learner", "comments": "International Conference on Autonomous Agents and Multiagent Systems\n  (AAMAS) 2018, NIPS 2017 Deep Reinforcement Learning Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Crossmodal Attentive Skill Learner (CASL), integrated\nwith the recently-introduced Asynchronous Advantage Option-Critic (A2OC)\narchitecture [Harb et al., 2017] to enable hierarchical reinforcement learning\nacross multiple sensory inputs. We provide concrete examples where the approach\nnot only improves performance in a single task, but accelerates transfer to new\ntasks. We demonstrate the attention mechanism anticipates and identifies useful\nlatent features, while filtering irrelevant sensor modalities during execution.\nWe modify the Arcade Learning Environment [Bellemare et al., 2013] to support\naudio queries, and conduct evaluations of crossmodal learning in the Atari 2600\ngame Amidar. Finally, building on the recent work of Babaeizadeh et al. [2017],\nwe open-source a fast hybrid CPU-GPU implementation of CASL.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 14:38:21 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 23:43:31 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 14:39:29 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Omidshafiei", "Shayegan", ""], ["Kim", "Dong-Ki", ""], ["Pazis", "Jason", ""], ["How", "Jonathan P.", ""]]}, {"id": "1711.10317", "submitter": "Chao Zhao", "authors": "Chao Zhao and Min Zhao and Yi Guan", "title": "Classification of entities via their descriptive sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypernym identification of open-domain entities is crucial for taxonomy\nconstruction as well as many higher-level applications. Current methods suffer\nfrom either low precision or low recall. To decrease the difficulty of this\nproblem, we adopt a classification-based method. We pre-define a concept\ntaxonomy and classify an entity to one of its leaf concept, based on the name\nand description information of the entity. A convolutional neural network\nclassifier and a K-means clustering module are adopted for classification. We\napplied this system to 2.1 million Baidu Baike entities, and 1.1 million of\nthem were successfully identified with a precision of 99.36%.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 14:49:06 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Zhao", "Chao", ""], ["Zhao", "Min", ""], ["Guan", "Yi", ""]]}, {"id": "1711.10328", "submitter": "Philipp Oettershagen", "authors": "Philipp Oettershagen, Julian F\\\"orster, Lukas Wirth, Jacques Amb\\\"uhl\n  and Roland Siegwart", "title": "Meteorology-Aware Multi-Goal Path Planning for Large-Scale Inspection\n  Missions with Long-Endurance Solar-Powered Aircraft", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solar-powered aircraft promise significantly increased flight endurance over\nconventional aircraft. While this makes them promising candidates for\nlarge-scale aerial inspection missions, their structural fragility necessitates\nthat adverse weather is avoided using appropriate path planning methods. This\npaper therefore presents MetPASS, the Meteorology-aware Path Planning and\nAnalysis Software for Solar-powered UAVs. MetPASS is the first path planning\nframework in the literature that considers all aspects that influence the\nsafety or performance of solar-powered flight: It avoids environmental risks\n(thunderstorms, rain, wind, wind gusts and humidity) and exploits advantageous\nregions (high sun radiation or tailwind). It also avoids system risks such as\nlow battery state of charge and returns safe paths through cluttered terrain.\nMetPASS imports weather data from global meteorological models, propagates the\naircraft state through an energetic system model, and then combines both into a\ncost function. A combination of dynamic programming techniques and an\nA*-search-algorithm with a custom heuristic is leveraged to plan globally\noptimal paths in station-keeping, point-to-point or multi-goal aerial\ninspection missions with coverage guarantees. A full software implementation\nincluding a GUI is provided. The planning methods are verified using three\nmissions of ETH Zurich's AtlantikSolar UAV: An 81-hour continuous solar-powered\nstation-keeping flight, a 4000km Atlantic crossing from Newfoundland to\nPortugal, and two multi-glacier aerial inspection missions above the Arctic\nOcean performed near Greenland in summer 2017.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 15:06:48 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Oettershagen", "Philipp", ""], ["F\u00f6rster", "Julian", ""], ["Wirth", "Lukas", ""], ["Amb\u00fchl", "Jacques", ""], ["Siegwart", "Roland", ""]]}, {"id": "1711.10331", "submitter": "Xu Sun", "authors": "Xu Sun, Weiwei Sun, Shuming Ma, Xuancheng Ren, Yi Zhang, Wenjie Li,\n  Houfeng Wang", "title": "Complex Structure Leads to Overfitting: A Structure Regularization\n  Decoding Method for Natural Language Processing", "comments": "arXiv admin note: text overlap with arXiv:1411.6243", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent systems on structured prediction focus on increasing the level of\nstructural dependencies within the model. However, our study suggests that\ncomplex structures entail high overfitting risks. To control the\nstructure-based overfitting, we propose to conduct structure regularization\ndecoding (SR decoding). The decoding of the complex structure model is\nregularized by the additionally trained simple structure model. We\ntheoretically analyze the quantitative relations between the structural\ncomplexity and the overfitting risk. The analysis shows that complex structure\nmodels are prone to the structure-based overfitting. Empirical evaluations show\nthat the proposed method improves the performance of the complex structure\nmodels by reducing the structure-based overfitting. On the sequence labeling\ntasks, the proposed method substantially improves the performance of the\ncomplex neural network models. The maximum F1 error rate reduction is 36.4% for\nthe third-order model. The proposed method also works for the parsing task. The\nmaximum UAS improvement is 5.5% for the tri-sibling model. The results are\ncompetitive with or better than the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 07:47:02 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Sun", "Xu", ""], ["Sun", "Weiwei", ""], ["Ma", "Shuming", ""], ["Ren", "Xuancheng", ""], ["Zhang", "Yi", ""], ["Li", "Wenjie", ""], ["Wang", "Houfeng", ""]]}, {"id": "1711.10355", "submitter": "Basheer Qolomany", "authors": "Basheer Qolomany, Ala Al-Fuqaha, Driss Benhaddou, Ajay Gupta", "title": "Role of Deep LSTM Neural Networks And WiFi Networks in Support of\n  Occupancy Prediction in Smart Buildings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing how many people occupy a building, and where they are located, is a\nkey component of smart building services. Commercial, industrial and\nresidential buildings often incorporate systems used to determine occupancy.\nHowever, relatively simple sensor technology and control algorithms limit the\neffectiveness of smart building services. In this paper we propose to replace\nsensor technology with time series models that can predict the number of\noccupants at a given location and time. We use Wi-Fi data sets readily\navailable in abundance for smart building services and train Auto Regression\nIntegrating Moving Average (ARIMA) models and Long Short-Term Memory (LSTM)\ntime series models. As a use case scenario of smart building services, these\nmodels allow forecasting of the number of people at a given time and location\nin 15, 30 and 60 minutes time intervals at building as well as Access Point\n(AP) level. For LSTM, we build our models in two ways: a separate model for\nevery time scale, and a combined model for the three time scales. Our\nexperiments show that LSTM combined model reduced the computational resources\nwith respect to the number of neurons by 74.48 % for the AP level, and by 67.13\n% for the building level. Further, the root mean square error (RMSE) was\nreduced by 88.2% - 93.4% for LSTM in comparison to ARIMA for the building\nlevels models and by 80.9% - 87% for the AP level models.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 15:44:11 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Qolomany", "Basheer", ""], ["Al-Fuqaha", "Ala", ""], ["Benhaddou", "Driss", ""], ["Gupta", "Ajay", ""]]}, {"id": "1711.10401", "submitter": "Kumar Sankar Ray", "authors": "Rajesh Misra, Kumar S. Ray", "title": "A Modification of Particle Swarm Optimization using Random Walk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle swarm optimization comes under lot of changes after James Kennedy\nand Russell Eberhart first proposes the idea in 1995. The changes has been done\nmainly on Inertia parameters in velocity updating equation so that the\nconvergence rate will be higher. We are proposing a novel approach where\nparticles movement will not be depend on its velocity rather it will be decided\nby constrained biased random walk of particles. In random walk every particles\nmovement based on two significant parameters, one is random process like toss\nof a coin and other is how much displacement a particle should have. In our\napproach we exploit this idea by performing a biased random operation and based\non the outcome of that random operation, PSO particles choose the direction of\nthe path and move non-uniformly into the solution space. This constrained,\nnon-uniform movement helps the random walking particle to converge quicker then\nclassical PSO. In our constrained biased random walking approach, we no longer\nneeded velocity term (Vi), rather we introduce a new parameter (K) which is a\nprobabilistic function. No global best particle (PGbest), local best particle\n(PLbest), Constriction parameter (W) are required rather we use a new term\ncalled Ptarg which is loosely influenced by PGbest.We test our algorithm on\nfive different benchmark functions, and also compare its performance with\nclassical PSO and Quantum Particle Swarm Optimization (QPSO).This new approach\nhave been shown significantly better than basic PSO and sometime outperform\nQPSO in terms of convergence, search space, number of iterations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 10:59:34 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 13:27:37 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Misra", "Rajesh", ""], ["Ray", "Kumar S.", ""]]}, {"id": "1711.10455", "submitter": "Brendan Fong", "authors": "Brendan Fong, David I. Spivak, R\\'emy Tuy\\'eras", "title": "Backprop as Functor: A compositional perspective on supervised learning", "comments": "13 pages + 4 page appendix", "journal-ref": "LICS 2019", "doi": null, "report-no": null, "categories": "math.CT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A supervised learning algorithm searches over a set of functions $A \\to B$\nparametrised by a space $P$ to find the best approximation to some ideal\nfunction $f\\colon A \\to B$. It does this by taking examples $(a,f(a)) \\in\nA\\times B$, and updating the parameter according to some rule. We define a\ncategory where these update rules may be composed, and show that gradient\ndescent---with respect to a fixed step size and an error function satisfying a\ncertain property---defines a monoidal functor from a category of parametrised\nfunctions to this category of update rules. This provides a structural\nperspective on backpropagation, as well as a broad generalisation of neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 18:34:45 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 00:21:49 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 15:11:06 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Fong", "Brendan", ""], ["Spivak", "David I.", ""], ["Tuy\u00e9ras", "R\u00e9my", ""]]}, {"id": "1711.10521", "submitter": "Fatmatulzehra Uslu Ms.", "authors": "Fatmatulzehra Uslu and Anil Anthony Bharath", "title": "A Recursive Bayesian Approach To Describe Retinal Vasculature Geometry", "comments": "26 pages,13 figures, journal paper", "journal-ref": null, "doi": "10.1016/j.patcog.2018.10.017", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demographic studies suggest that changes in the retinal vasculature geometry,\nespecially in vessel width, are associated with the incidence or progression of\neye-related or systemic diseases. To date, the main information source for\nwidth estimation from fundus images has been the intensity profile between\nvessel edges. However, there are many factors affecting the intensity profile:\npathologies, the central light reflex and local illumination levels, to name a\nfew. In this study, we introduce three information sources for width\nestimation. These are the probability profiles of vessel interior, centreline\nand edge locations generated by a deep network. The probability profiles\nprovide direct access to vessel geometry and are used in the likelihood\ncalculation for a Bayesian method, particle filtering. We also introduce a\ngeometric model which can handle non-ideal conditions of the probability\nprofiles. Our experiments conducted on the REVIEW dataset yielded consistent\nestimates of vessel width, even in cases when one of the vessel edges is\ndifficult to identify. Moreover, our results suggest that the method is better\nthan human observers at locating edges of low contrast vessels.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 19:28:27 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Uslu", "Fatmatulzehra", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1711.10558", "submitter": "Biswarup Bhattacharya", "authors": "Biswarup Bhattacharya, Iftikhar Burhanuddin, Abhilasha Sancheti,\n  Kushal Satya", "title": "Intent-Aware Contextual Recommendation System", "comments": "Presented at the 5th International Workshop on Data Science and Big\n  Data Analytics (DSBDA), 17th IEEE International Conference on Data Mining\n  (ICDM) 2017; 8 pages; 4 figures; Due to the limitation \"The abstract field\n  cannot be longer than 1,920 characters,\" the abstract appearing here is\n  slightly shorter than the one in the PDF file", "journal-ref": null, "doi": "10.1109/ICDMW.2017.8", "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems take inputs from user history, use an internal ranking\nalgorithm to generate results and possibly optimize this ranking based on\nfeedback. However, often the recommender system is unaware of the actual intent\nof the user and simply provides recommendations dynamically without properly\nunderstanding the thought process of the user. An intelligent recommender\nsystem is not only useful for the user but also for businesses which want to\nlearn the tendencies of their users. Finding out tendencies or intents of a\nuser is a difficult problem to solve.\n  Keeping this in mind, we sought out to create an intelligent system which\nwill keep track of the user's activity on a web-application as well as\ndetermine the intent of the user in each session. We devised a way to encode\nthe user's activity through the sessions. Then, we have represented the\ninformation seen by the user in a high dimensional format which is reduced to\nlower dimensions using tensor factorization techniques. The aspect of intent\nawareness (or scoring) is dealt with at this stage. Finally, combining the user\nactivity data with the contextual information gives the recommendation score.\nThe final recommendations are then ranked using filtering and collaborative\nrecommendation techniques to show the top-k recommendations to the user. A\nprovision for feedback is also envisioned in the current system which informs\nthe model to update the various weights in the recommender system. Our overall\nmodel aims to combine both frequency-based and context-based recommendation\nsystems and quantify the intent of a user to provide better recommendations.\n  We ran experiments on real-world timestamped user activity data, in the\nsetting of recommending reports to the users of a business analytics tool and\nthe results are better than the baselines. We also tuned certain aspects of our\nmodel to arrive at optimized results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:58:52 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Bhattacharya", "Biswarup", ""], ["Burhanuddin", "Iftikhar", ""], ["Sancheti", "Abhilasha", ""], ["Satya", "Kushal", ""]]}, {"id": "1711.10561", "submitter": "Maziar Raissi", "authors": "Maziar Raissi, Paris Perdikaris, and George Em Karniadakis", "title": "Physics Informed Deep Learning (Part I): Data-driven Solutions of\n  Nonlinear Partial Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NA math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce physics informed neural networks -- neural networks that are\ntrained to solve supervised learning tasks while respecting any given law of\nphysics described by general nonlinear partial differential equations. In this\ntwo part treatise, we present our developments in the context of solving two\nmain classes of problems: data-driven solution and data-driven discovery of\npartial differential equations. Depending on the nature and arrangement of the\navailable data, we devise two distinct classes of algorithms, namely continuous\ntime and discrete time models. The resulting neural networks form a new class\nof data-efficient universal function approximators that naturally encode any\nunderlying physical laws as prior information. In this first part, we\ndemonstrate how these networks can be used to infer solutions to partial\ndifferential equations, and obtain physics-informed surrogate models that are\nfully differentiable with respect to all input coordinates and free parameters.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:21:59 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Raissi", "Maziar", ""], ["Perdikaris", "Paris", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1711.10563", "submitter": "Ronald Kemker", "authors": "Ronald Kemker and Christopher Kanan", "title": "FearNet: Brain-Inspired Model for Incremental Learning", "comments": "To appear in ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental class learning involves sequentially learning classes in bursts\nof examples from the same class. This violates the assumptions that underlie\nmethods for training standard deep neural networks, and will cause them to\nsuffer from catastrophic forgetting. Arguably, the best method for incremental\nclass learning is iCaRL, but it requires storing training examples for each\nclass, making it challenging to scale. Here, we propose FearNet for incremental\nclass learning. FearNet is a generative model that does not store previous\nexamples, making it memory efficient. FearNet uses a brain-inspired dual-memory\nsystem in which new memories are consolidated from a network for recent\nmemories inspired by the mammalian hippocampal complex to a network for\nlong-term storage inspired by medial prefrontal cortex. Memory consolidation is\ninspired by mechanisms that occur during sleep. FearNet also uses a module\ninspired by the basolateral amygdala for determining which memory system to use\nfor recall. FearNet achieves state-of-the-art performance at incremental class\nlearning on image (CIFAR-100, CUB-200) and audio classification (AudioSet)\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:26:15 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 20:32:27 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Kemker", "Ronald", ""], ["Kanan", "Christopher", ""]]}, {"id": "1711.10566", "submitter": "Maziar Raissi", "authors": "Maziar Raissi, Paris Perdikaris, and George Em Karniadakis", "title": "Physics Informed Deep Learning (Part II): Data-driven Discovery of\n  Nonlinear Partial Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.AP math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce physics informed neural networks -- neural networks that are\ntrained to solve supervised learning tasks while respecting any given law of\nphysics described by general nonlinear partial differential equations. In this\nsecond part of our two-part treatise, we focus on the problem of data-driven\ndiscovery of partial differential equations. Depending on whether the available\ndata is scattered in space-time or arranged in fixed temporal snapshots, we\nintroduce two main classes of algorithms, namely continuous time and discrete\ntime models. The effectiveness of our approach is demonstrated using a wide\nrange of benchmark problems in mathematical physics, including conservation\nlaws, incompressible fluid flow, and the propagation of nonlinear shallow-water\nwaves.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:29:35 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Raissi", "Maziar", ""], ["Perdikaris", "Paris", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1711.10574", "submitter": "Mehmet Aydin", "authors": "Mehmet Emin Aydin and Ryan Fellows", "title": "A reinforcement learning algorithm for building collaboration in\n  multi-agent systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a proof-of concept study for demonstrating the viability\nof building collaboration among multiple agents through standard Q learning\nalgorithm embedded in particle swarm optimisation. Collaboration is formulated\nto be achieved among the agents via some sort competition, where the agents are\nexpected to balance their action in such a way that none of them drifts away of\nthe team and none intervene any fellow neighbours territory. Particles are\ndevised with Q learning algorithm for self training to learn how to act as\nmembers of a swarm and how to produce collaborative/collective behaviours. The\nproduced results are supportive to the algorithmic structures suggesting that a\nsubstantive collaboration can be build via proposed learning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:46:42 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 15:58:28 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Aydin", "Mehmet Emin", ""], ["Fellows", "Ryan", ""]]}, {"id": "1711.10604", "submitter": "Dustin Tran", "authors": "Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas\n  Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt Hoffman, Rif A. Saurous", "title": "TensorFlow Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TensorFlow Distributions library implements a vision of probability\ntheory adapted to the modern deep-learning paradigm of end-to-end\ndifferentiable computation. Building on two basic abstractions, it offers\nflexible building blocks for probabilistic computation. Distributions provide\nfast, numerically stable methods for generating samples and computing\nstatistics, e.g., log density. Bijectors provide composable volume-tracking\ntransformations with automatic caching. Together these enable modular\nconstruction of high dimensional distributions and transformations not possible\nwith previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible\nresidual networks). They are the workhorse behind deep probabilistic\nprogramming systems like Edward and empower fast black-box inference in\nprobabilistic models built on deep-network components. TensorFlow Distributions\nhas proven an important part of the TensorFlow toolkit within Google and in the\nbroader deep learning community.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 23:05:15 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Dillon", "Joshua V.", ""], ["Langmore", "Ian", ""], ["Tran", "Dustin", ""], ["Brevdo", "Eugene", ""], ["Vasudevan", "Srinivas", ""], ["Moore", "Dave", ""], ["Patton", "Brian", ""], ["Alemi", "Alex", ""], ["Hoffman", "Matt", ""], ["Saurous", "Rif A.", ""]]}, {"id": "1711.10644", "submitter": "Seungkyun Hong", "authors": "Seungkyun Hong, Seongchan Kim, Minsu Joh, Sa-kwang Song", "title": "PSIque: Next Sequence Prediction of Satellite Images using a\n  Convolutional Sequence-to-Sequence Network", "comments": "Workshop on Deep Learning for Physical Sciences (DLPS 2017), NIPS\n  2017, Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting unseen weather phenomena is an important issue for disaster\nmanagement. In this paper, we suggest a model for a convolutional\nsequence-to-sequence autoencoder for predicting undiscovered weather situations\nfrom previous satellite images. We also propose a symmetric skip connection\nbetween encoder and decoder modules to produce more comprehensive image\npredictions. To examine our model performance, we conducted experiments for\neach suggested model to predict future satellite images from historical\nsatellite images. A specific combination of skip connection and\nsequence-to-sequence autoencoder was able to generate closest prediction from\nthe ground truth image.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 02:02:13 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 21:25:33 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Hong", "Seungkyun", ""], ["Kim", "Seongchan", ""], ["Joh", "Minsu", ""], ["Song", "Sa-kwang", ""]]}, {"id": "1711.10755", "submitter": "Rui Feng", "authors": "Rui Feng, Yang Yang, Wenjie Hu, Fei Wu, Yueting Zhuang", "title": "Representation Learning for Scale-free Networks", "comments": "8 figures; accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embedding aims to learn the low-dimensional representations of\nvertexes in a network, while structure and inherent properties of the network\nis preserved. Existing network embedding works primarily focus on preserving\nthe microscopic structure, such as the first- and second-order proximity of\nvertexes, while the macroscopic scale-free property is largely ignored.\nScale-free property depicts the fact that vertex degrees follow a heavy-tailed\ndistribution (i.e., only a few vertexes have high degrees) and is a critical\nproperty of real-world networks, such as social networks. In this paper, we\nstudy the problem of learning representations for scale-free networks. We first\ntheoretically analyze the difficulty of embedding and reconstructing a\nscale-free network in the Euclidean space, by converting our problem to the\nsphere packing problem. Then, we propose the \"degree penalty\" principle for\ndesigning scale-free property preserving network embedding algorithm: punishing\nthe proximity between high-degree vertexes. We introduce two implementations of\nour principle by utilizing the spectral techniques and a skip-gram model\nrespectively. Extensive experiments on six datasets show that our algorithms\nare able to not only reconstruct heavy-tailed distributed degree distribution,\nbut also outperform state-of-the-art embedding models in various network mining\ntasks, such as vertex classification and link prediction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 10:15:17 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Feng", "Rui", ""], ["Yang", "Yang", ""], ["Hu", "Wenjie", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1711.10768", "submitter": "Marco Pavan", "authors": "Dario De Nart, Dante Degl'Innocenti, Marco Pavan", "title": "Leveraging Conversation Structure on Social Media to Identify\n  Potentially Influential Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks have a community providing feedback on comments that allows\nto identify opinion leaders and users whose positions are unwelcome. Other\nplatforms are not backed by such tools. Having a picture of the community's\nreactions to a published content is a non trivial problem. In this work we\npropose a novel approach using Abstract Argumentation Frameworks and machine\nlearning to describe interactions between users. Our experiments provide\nevidence that modelling the flow of a conversation with the primitives of AAF\ncan support the identification of users who produce consistently appreciated\ncontent without modelling such content.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 10:59:35 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["De Nart", "Dario", ""], ["Degl'Innocenti", "Dante", ""], ["Pavan", "Marco", ""]]}, {"id": "1711.10789", "submitter": "Thomas Moerland", "authors": "Thomas M. Moerland, Joost Broekens and Catholijn M. Jonker", "title": "Efficient exploration with Double Uncertain Value Networks", "comments": "Deep Reinforcement Learning Symposium @ Conference on Neural\n  Information Processing Systems (NIPS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies directed exploration for reinforcement learning agents by\ntracking uncertainty about the value of each available action. We identify two\nsources of uncertainty that are relevant for exploration. The first originates\nfrom limited data (parametric uncertainty), while the second originates from\nthe distribution of the returns (return uncertainty). We identify methods to\nlearn these distributions with deep neural networks, where we estimate\nparametric uncertainty with Bayesian drop-out, while return uncertainty is\npropagated through the Bellman equation as a Gaussian distribution. Then, we\nidentify that both can be jointly estimated in one network, which we call the\nDouble Uncertain Value Network. The policy is directly derived from the learned\ndistributions based on Thompson sampling. Experimental results show that both\ntypes of uncertainty may vastly improve learning in domains with a strong\nexploration challenge.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:41:41 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Moerland", "Thomas M.", ""], ["Broekens", "Joost", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "1711.10795", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Eva Mohedano, Kevin McGuinness, Xavier Giro-i-Nieto and Noel E.\n  O'Connor", "title": "Saliency Weighted Convolutional Features for Instance Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores attention models to weight the contribution of local\nconvolutional representations for the instance search task. We present a\nretrieval framework based on bags of local convolutional features (BLCF) that\nbenefits from saliency weighting to build an efficient image representation.\nThe use of human visual attention models (saliency) allows significant\nimprovements in retrieval performance without the need to conduct region\nanalysis or spatial verification, and without requiring any feature fine\ntuning. We investigate the impact of different saliency models, finding that\nhigher performance on saliency benchmarks does not necessarily equate to\nimproved performance when used in instance search tasks. The proposed approach\noutperforms the state-of-the-art on the challenging INSTRE benchmark by a large\nmargin, and provides similar performance on the Oxford and Paris benchmarks\ncompared to more complex methods that use off-the-shelf representations. The\nsource code used in this project is available at\nhttps://imatge-upc.github.io/salbow/\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:46:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Mohedano", "Eva", ""], ["McGuinness", "Kevin", ""], ["Giro-i-Nieto", "Xavier", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1711.10868", "submitter": "Michel Fliess", "authors": "Vincent Rocher, C\\'edric Join, St\\'ephane Mottelet, Jean Bernier,\n  Sabrina Rechdaoui-Gu\\'erin, Sam Azimi, Paul Lessard, Andr\\'e Pauss, Michel\n  Fliess", "title": "La production de nitrites lors de la d\\'enitrification des eaux us\\'ees\n  par biofiltration - Strat\\'egie de contr\\^ole et de r\\'eduction des\n  concentrations r\\'esiduelles", "comments": "in french, Journal of Water Science, to appear", "journal-ref": "Revue des Sciences de l'Eau, 31(1), 2018, 61-73", "doi": "10.7202/1047053ar", "report-no": null, "categories": "cs.SY cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent popularity of post-denitrification processes in the greater Paris\narea wastewater treatment plants has caused a resurgence of the presence of\nnitrite in the Seine river. Controlling the production of nitrite during the\npost-denitrification has thus become a major technical issue. Research studies\nhave been led in the MOCOPEE program (www.mocopee.com) to better understand the\nunderlying mechanisms behind the production of nitrite during wastewater\ndenitrification and to develop technical tools (measurement and control\nsolutions) to assist on-site reductions of nitrite productions. Prior studies\nhave shown that typical methanol dosage strategies produce a varying\ncarbon-to-nitrogen ratio in the reactor, which in turn leads to unstable\nnitrite concentrations in the effluent. The possibility of adding a model-free\ncontrol to the actual classical dosage strategy has thus been tested on the\nSimBio model, which simulates the behavior of wastewater biofilters. The\ncorresponding \"intelligent\" feedback loop, which is using effluent nitrite\nconcentrations, compensates the classical strategy only when needed. Simulation\nresults show a clear improvement in average nitrite concentration level and\nlevel stability in the effluent, without a notable overcost in methanol.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 18:49:32 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Rocher", "Vincent", ""], ["Join", "C\u00e9dric", ""], ["Mottelet", "St\u00e9phane", ""], ["Bernier", "Jean", ""], ["Rechdaoui-Gu\u00e9rin", "Sabrina", ""], ["Azimi", "Sam", ""], ["Lessard", "Paul", ""], ["Pauss", "Andr\u00e9", ""], ["Fliess", "Michel", ""]]}, {"id": "1711.10907", "submitter": "Olexandr Isayev", "authors": "Mariya Popova, Olexandr Isayev, Alexander Tropsha", "title": "Deep Reinforcement Learning for De-Novo Drug Design", "comments": null, "journal-ref": "Science Advances, 2018, vol. 4, no. 7, eaap7885", "doi": "10.1126/sciadv.aap7885", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel computational strategy for de novo design of molecules\nwith desired properties termed ReLeaSE (Reinforcement Learning for Structural\nEvolution). Based on deep and reinforcement learning approaches, ReLeaSE\nintegrates two deep neural networks - generative and predictive - that are\ntrained separately but employed jointly to generate novel targeted chemical\nlibraries. ReLeaSE employs simple representation of molecules by their SMILES\nstrings only. Generative models are trained with stack-augmented memory network\nto produce chemically feasible SMILES strings, and predictive models are\nderived to forecast the desired properties of the de novo generated compounds.\nIn the first phase of the method, generative and predictive models are trained\nseparately with a supervised learning algorithm. In the second phase, both\nmodels are trained jointly with the reinforcement learning approach to bias the\ngeneration of new chemical structures towards those with the desired physical\nand/or biological properties. In the proof-of-concept study, we have employed\nthe ReLeaSE method to design chemical libraries with a bias toward structural\ncomplexity or biased toward compounds with either maximal, minimal, or specific\nrange of physical properties such as melting point or hydrophobicity, as well\nas to develop novel putative inhibitors of JAK2. The approach proposed herein\ncan find a general use for generating targeted chemical libraries of novel\ncompounds optimized for either a single desired property or multiple\nproperties.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 15:10:49 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 17:13:56 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Popova", "Mariya", ""], ["Isayev", "Olexandr", ""], ["Tropsha", "Alexander", ""]]}, {"id": "1711.10938", "submitter": "Fulton Wang", "authors": "Fulton Wang, Cynthia Rudin", "title": "Extreme Dimension Reduction for Handling Covariate Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the covariate shift learning scenario, the training and test covariate\ndistributions differ, so that a predictor's average loss over the training and\ntest distributions also differ. In this work, we explore the potential of\nextreme dimension reduction, i.e. to very low dimensions, in improving the\nperformance of importance weighting methods for handling covariate shift, which\nfail in high dimensions due to potentially high train/test covariate divergence\nand the inability to accurately estimate the requisite density ratios. We first\nformulate and solve a problem optimizing over linear subspaces a combination of\ntheir predictive utility and train/test divergence within. Applying it to\nsimulated and real data, we show extreme dimension reduction helps sometimes\nbut not always, due to a bias introduced by dimension reduction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:20:06 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 15:44:49 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Wang", "Fulton", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1711.10958", "submitter": "Kevin Kilgour", "authors": "Blaise Ag\\\"uera y Arcas, Beat Gfeller, Ruiqi Guo, Kevin Kilgour,\n  Sanjiv Kumar, James Lyon, Julian Odell, Marvin Ritter, Dominik Roblek,\n  Matthew Sharifi, Mihajlo Velimirovi\\'c", "title": "Now Playing: Continuous low-power music recognition", "comments": "Authors are listed in alphabetical order by last name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing music recognition applications require a connection to a server that\nperforms the actual recognition. In this paper we present a low-power music\nrecognizer that runs entirely on a mobile device and automatically recognizes\nmusic without user interaction. To reduce battery consumption, a small music\ndetector runs continuously on the mobile device's DSP chip and wakes up the\nmain application processor only when it is confident that music is present.\nOnce woken, the recognizer on the application processor is provided with a few\nseconds of audio which is fingerprinted and compared to the stored fingerprints\nin the on-device fingerprint database of tens of thousands of songs. Our\npresented system, Now Playing, has a daily battery usage of less than 1% on\naverage, respects user privacy by running entirely on-device and can passively\nrecognize a wide range of music.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:42:52 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Arcas", "Blaise Ag\u00fcera y", ""], ["Gfeller", "Beat", ""], ["Guo", "Ruiqi", ""], ["Kilgour", "Kevin", ""], ["Kumar", "Sanjiv", ""], ["Lyon", "James", ""], ["Odell", "Julian", ""], ["Ritter", "Marvin", ""], ["Roblek", "Dominik", ""], ["Sharifi", "Matthew", ""], ["Velimirovi\u0107", "Mihajlo", ""]]}, {"id": "1711.11017", "submitter": "Ethan Perez", "authors": "Simon Brodeur, Ethan Perez, Ankesh Anand, Florian Golemo, Luca\n  Celotti, Florian Strub, Jean Rouat, Hugo Larochelle, Aaron Courville", "title": "HoME: a Household Multimodal Environment", "comments": "Presented at NIPS 2017's Visually-Grounded Interaction and Language\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.RO cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HoME: a Household Multimodal Environment for artificial agents\nto learn from vision, audio, semantics, physics, and interaction with objects\nand other agents, all within a realistic context. HoME integrates over 45,000\ndiverse 3D house layouts based on the SUNCG dataset, a scale which may\nfacilitate learning, generalization, and transfer. HoME is an open-source,\nOpenAI Gym-compatible platform extensible to tasks in reinforcement learning,\nlanguage grounding, sound-based navigation, robotics, multi-agent learning, and\nmore. We hope HoME better enables artificial agents to learn as humans do: in\nan interactive, multimodal, and richly contextualized setting.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 18:45:59 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Brodeur", "Simon", ""], ["Perez", "Ethan", ""], ["Anand", "Ankesh", ""], ["Golemo", "Florian", ""], ["Celotti", "Luca", ""], ["Strub", "Florian", ""], ["Rouat", "Jean", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1711.11027", "submitter": "Serhii Havrylov", "authors": "Arthur Bra\\v{z}inskas, Serhii Havrylov, Ivan Titov", "title": "Embedding Words as Distributions with a Bayesian Skip-gram Model", "comments": "COLING 2018. For the associated code, see\n  https://github.com/ixlan/BSG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for embedding words as probability densities in a\nlow-dimensional space. Rather than assuming that a word embedding is fixed\nacross the entire text collection, as in standard word embedding methods, in\nour Bayesian model we generate it from a word-specific prior density for each\noccurrence of a given word. Intuitively, for each word, the prior density\nencodes the distribution of its potential 'meanings'. These prior densities are\nconceptually similar to Gaussian embeddings. Interestingly, unlike the Gaussian\nembeddings, we can also obtain context-specific densities: they encode\nuncertainty about the sense of a word given its context and correspond to\nposterior distributions within our model. The context-dependent densities have\nmany potential applications: for example, we show that they can be directly\nused in the lexical substitution task. We describe an effective estimation\nmethod based on the variational autoencoding framework. We also demonstrate\nthat our embeddings achieve competitive results on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 18:55:48 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 15:44:44 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Bra\u017einskas", "Arthur", ""], ["Havrylov", "Serhii", ""], ["Titov", "Ivan", ""]]}, {"id": "1711.11068", "submitter": "Rafa{\\l} Muszy\\'nski", "authors": "Rafa{\\l} Muszy\\'nski, Jun Wang", "title": "Happiness Pursuit: Personality Learning in a Society of Agents", "comments": "5 pages, 3 figures, Presented at the Aligned Artificial Intelligence\n  Workshop at the 31st Conference on Neural Information Processing Systems\n  (NIPS 2017), Long Beach, CA, USA; changed the footnote to workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling personality is a challenging problem with applications spanning\ncomputer games, virtual assistants, online shopping and education. Many\ntechniques have been tried, ranging from neural networks to computational\ncognitive architectures. However, most approaches rely on examples with\nhand-crafted features and scenarios. Here, we approach learning a personality\nby training agents using a Deep Q-Network (DQN) model on rewards based on\npsychoanalysis, against hand-coded AI in the game of Pong. As a result, we\nobtain 4 agents, each with its own personality. Then, we define happiness of an\nagent, which can be seen as a measure of alignment with agent's objective\nfunction, and study it when agents play both against hand-coded AI, and against\neach other. We find that the agents that achieve higher happiness during\ntesting against hand-coded AI, have lower happiness when competing against each\nother. This suggests that higher happiness in testing is a sign of overfitting\nin learning to interact with hand-coded AI, and leads to worse performance\nagainst agents with different personalities.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:24:53 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 04:44:59 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Muszy\u0144ski", "Rafa\u0142", ""], ["Wang", "Jun", ""]]}, {"id": "1711.11124", "submitter": "Adit Krishnan", "authors": "Adit Krishnan, Ashish Sharma, Hari Sundaram", "title": "Improving Latent User Models in Online Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern social platforms are characterized by the presence of rich\nuser-behavior data associated with the publication, sharing and consumption of\ntextual content. Users interact with content and with each other in a complex\nand dynamic social environment while simultaneously evolving over time. In\norder to effectively characterize users and predict their future behavior in\nsuch a setting, it is necessary to overcome several challenges. Content\nheterogeneity and temporal inconsistency of behavior data result in severe\nsparsity at the user level. In this paper, we propose a novel\nmutual-enhancement framework to simultaneously partition and learn latent\nactivity profiles of users. We propose a flexible user partitioning approach to\neffectively discover rare behaviors and tackle user-level sparsity. We\nextensively evaluate the proposed framework on massive datasets from real-world\nplatforms including Q&A networks and interactive online courses (MOOCs). Our\nresults indicate significant gains over state-of-the-art behavior models ( 15%\navg ) in a varied range of tasks and our gains are further magnified for users\nwith limited interaction data. The proposed algorithms are amenable to\nparallelization, scale linearly in the size of datasets, and provide\nflexibility to model diverse facets of user behavior.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 06:46:09 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 08:29:57 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Krishnan", "Adit", ""], ["Sharma", "Ashish", ""], ["Sundaram", "Hari", ""]]}, {"id": "1711.11135", "submitter": "Xin Wang", "authors": "Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, William Yang Wang", "title": "Video Captioning via Hierarchical Reinforcement Learning", "comments": "CVPR 2018, with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning is the task of automatically generating a textual\ndescription of the actions in a video. Although previous work (e.g.\nsequence-to-sequence model) has shown promising results in abstracting a coarse\ndescription of a short video, it is still very challenging to caption a video\ncontaining multiple fine-grained actions with a detailed description. This\npaper aims to address the challenge by proposing a novel hierarchical\nreinforcement learning framework for video captioning, where a high-level\nManager module learns to design sub-goals and a low-level Worker module\nrecognizes the primitive actions to fulfill the sub-goal. With this\ncompositional framework to reinforce video captioning at different levels, our\napproach significantly outperforms all the baseline methods on a newly\nintroduced large-scale dataset for fine-grained video captioning. Furthermore,\nour non-ensemble model has already achieved the state-of-the-art results on the\nwidely-used MSR-VTT dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 22:23:59 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 08:38:19 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 07:06:47 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Wang", "Xin", ""], ["Chen", "Wenhu", ""], ["Wu", "Jiawei", ""], ["Wang", "Yuan-Fang", ""], ["Wang", "William Yang", ""]]}, {"id": "1711.11157", "submitter": "Yitao Liang", "authors": "Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, Guy Van den Broeck", "title": "A Semantic Loss Function for Deep Learning with Symbolic Knowledge", "comments": "This version appears in the Proceedings of the 35th International\n  Conference on Machine Learning (ICML 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a novel methodology for using symbolic knowledge in deep\nlearning. From first principles, we derive a semantic loss function that\nbridges between neural output vectors and logical constraints. This loss\nfunction captures how close the neural network is to satisfying the constraints\non its output. An experimental evaluation shows that it effectively guides the\nlearner to achieve (near-)state-of-the-art results on semi-supervised\nmulti-class classification. Moreover, it significantly increases the ability of\nthe neural network to predict structured objects, such as rankings and paths.\nThese discrete concepts are tremendously difficult to learn, and benefit from a\ntight integration of deep learning and symbolic reasoning methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 23:49:55 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 00:05:58 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Xu", "Jingyi", ""], ["Zhang", "Zilu", ""], ["Friedman", "Tal", ""], ["Liang", "Yitao", ""], ["Broeck", "Guy Van den", ""]]}, {"id": "1711.11175", "submitter": "Sahin Geyik", "authors": "Sahin Cem Geyik, Jianqiang Shen, Shahriar Shariat, Ali Dasdan, Santanu\n  Kolay", "title": "Towards Data Quality Assessment in Online Advertising", "comments": "10 pages, 7 Figures. This work has been presented in the KDD 2016\n  Workshop on Enterprise Intelligence", "journal-ref": "KDD 2016 Workshop on Enterprise Intelligence", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online advertising, our aim is to match the advertisers with the most\nrelevant users to optimize the campaign performance. In the pursuit of\nachieving this goal, multiple data sources provided by the advertisers or\nthird-party data providers are utilized to choose the set of users according to\nthe advertisers' targeting criteria. In this paper, we present a framework that\ncan be applied to assess the quality of such data sources in large scale. This\nframework efficiently evaluates the similarity of a specific data source\ncategorization to that of the ground truth, especially for those cases when the\nground truth is accessible only in aggregate, and the user-level information is\nanonymized or unavailable due to privacy reasons. We propose multiple\nmethodologies within this framework, present some preliminary assessment\nresults, and evaluate how the methodologies compare to each other. We also\npresent two use cases where we can utilize the data quality assessment results:\nthe first use case is targeting specific user categories, and the second one is\nforecasting the desirable audiences we can reach for an online advertising\ncampaign with pre-set targeting criteria.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 01:22:45 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Geyik", "Sahin Cem", ""], ["Shen", "Jianqiang", ""], ["Shariat", "Shahriar", ""], ["Dasdan", "Ali", ""], ["Kolay", "Santanu", ""]]}, {"id": "1711.11180", "submitter": "Dhaval Adjodah", "authors": "Dhaval Adjodah, Dan Calacci, Yan Leng, Peter Krafft, Esteban Moro,\n  Alex Pentland", "title": "Improved Learning in Evolution Strategies via Sparser Inter-Agent\n  Network Topologies", "comments": "This paper is obsolete", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We draw upon a previously largely untapped literature on human collective\nintelligence as a source of inspiration for improving deep learning. Implicit\nin many algorithms that attempt to solve Deep Reinforcement Learning (DRL)\ntasks is the network of processors along which parameter values are shared. So\nfar, existing approaches have implicitly utilized fully-connected networks, in\nwhich all processors are connected. However, the scientific literature on human\ncollective intelligence suggests that complete networks may not always be the\nmost effective information network structures for distributed search through\ncomplex spaces. Here we show that alternative topologies can improve deep\nneural network training: we find that sparser networks learn higher rewards\nfaster, leading to learning improvements at lower communication costs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 01:42:54 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 21:34:40 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Adjodah", "Dhaval", ""], ["Calacci", "Dan", ""], ["Leng", "Yan", ""], ["Krafft", "Peter", ""], ["Moro", "Esteban", ""], ["Pentland", "Alex", ""]]}, {"id": "1711.11200", "submitter": "Hyunwoo Lee", "authors": "Hyunwoo Lee, Jooyoung Kim, Dojun Yang and Joon-Ho Kim", "title": "Embedded Real-Time Fall Detection Using Deep Learning For Elderly Care", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a real-time embedded fall detection system using a\nDVS(Dynamic Vision Sensor) that has never been used for traditional fall\ndetection, a dataset for fall detection using that, and a DVS-TN(DVS-Temporal\nNetwork). The first contribution is building a DVS Falls Dataset, which made\nour network to recognize a much greater variety of falls than the existing\ndatasets that existed before and solved privacy issues using the DVS. Secondly,\nwe introduce the DVS-TN : optimized deep learning network to detect falls using\nDVS. Finally, we implemented a fall detection system which can run on\nlow-computing H/W with real-time, and tested on DVS Falls Dataset that takes\ninto account various falls situations. Our approach achieved 95.5% on the\nF1-score and operates at 31.25 FPS on NVIDIA Jetson TX1 board.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 03:07:14 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Lee", "Hyunwoo", ""], ["Kim", "Jooyoung", ""], ["Yang", "Dojun", ""], ["Kim", "Joon-Ho", ""]]}, {"id": "1711.11225", "submitter": "Yunhao Tang", "authors": "Yunhao Tang and Alp Kucukelbir", "title": "Variational Deep Q Network", "comments": "12 pages, 5 figures, Second workshop on Bayesian Deep Learning (NIPS\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework that directly tackles the probability distribution of\nthe value function parameters in Deep Q Network (DQN), with powerful\nvariational inference subroutines to approximate the posterior of the\nparameters. We will establish the equivalence between our proposed surrogate\nobjective and variational inference loss. Our new algorithm achieves efficient\nexploration and performs well on large scale chain Markov Decision Process\n(MDP).\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 04:52:09 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Tang", "Yunhao", ""], ["Kucukelbir", "Alp", ""]]}, {"id": "1711.11231", "submitter": "Shu Guo", "authors": "Shu Guo, Quan Wang, Lihong Wang, Bin Wang, Li Guo", "title": "Knowledge Graph Embedding with Iterative Guidance from Soft Rules", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of\ncurrent research. Combining such an embedding model with logic rules has\nrecently attracted increasing attention. Most previous attempts made a one-time\ninjection of logic rules, ignoring the interactive nature between embedding\nlearning and logical inference. And they focused only on hard rules, which\nalways hold with no exception and usually require extensive manual effort to\ncreate or validate. In this paper, we propose Rule-Guided Embedding (RUGE), a\nnovel paradigm of KG embedding with iterative guidance from soft rules. RUGE\nenables an embedding model to learn simultaneously from 1) labeled triples that\nhave been directly observed in a given KG, 2) unlabeled triples whose labels\nare going to be predicted iteratively, and 3) soft rules with various\nconfidence levels extracted automatically from the KG. In the learning process,\nRUGE iteratively queries rules to obtain soft labels for unlabeled triples, and\nintegrates such newly labeled triples to update the embedding model. Through\nthis iterative procedure, knowledge embodied in logic rules may be better\ntransferred into the learned embeddings. We evaluate RUGE in link prediction on\nFreebase and YAGO. Experimental results show that: 1) with rule knowledge\ninjected iteratively, RUGE achieves significant and consistent improvements\nover state-of-the-art baselines; and 2) despite their uncertainties,\nautomatically extracted soft rules are highly beneficial to KG embedding, even\nthose with moderate confidence levels. The code and data used for this paper\ncan be obtained from https://github.com/iieir-km/RUGE.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 05:13:33 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Guo", "Shu", ""], ["Wang", "Quan", ""], ["Wang", "Lihong", ""], ["Wang", "Bin", ""], ["Guo", "Li", ""]]}, {"id": "1711.11289", "submitter": "Himanshu Sahni", "authors": "Himanshu Sahni, Saurabh Kumar, Farhan Tejani, Charles Isbell", "title": "Learning to Compose Skills", "comments": "Presented at NIPS 2017 Deep RL Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a differentiable framework capable of learning a wide variety of\ncompositions of simple policies that we call skills. By recursively composing\nskills with themselves, we can create hierarchies that display complex\nbehavior. Skill networks are trained to generate skill-state embeddings that\nare provided as inputs to a trainable composition function, which in turn\noutputs a policy for the overall task. Our experiments on an environment\nconsisting of multiple collect and evade tasks show that this architecture is\nable to quickly build complex skills from simpler ones. Furthermore, the\nlearned composition function displays some transfer to unseen combinations of\nskills, allowing for zero-shot generalizations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 09:47:28 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Sahni", "Himanshu", ""], ["Kumar", "Saurabh", ""], ["Tejani", "Farhan", ""], ["Isbell", "Charles", ""]]}, {"id": "1711.11383", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "title": "Learning to Learn from Weak Supervision by Full Supervision", "comments": "Accepted at NIPS Workshop on Meta-Learning (MetaLearn 2017), Long\n  Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for training neural networks when we have\na large set of data with weak labels and a small amount of data with true\nlabels. In our proposed model, we train two neural networks: a target network,\nthe learner and a confidence network, the meta-learner. The target network is\noptimized to perform a given task and is trained using a large set of unlabeled\ndata that are weakly annotated. We propose to control the magnitude of the\ngradient updates to the target network using the scores provided by the second\nconfidence network, which is trained on a small amount of supervised data. Thus\nwe avoid that the weight updates computed from noisy labels harm the quality of\nthe target network model.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 13:32:45 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Severyn", "Aliaksei", ""], ["Rothe", "Sascha", ""], ["Kamps", "Jaap", ""]]}, {"id": "1711.11443", "submitter": "Pierre Stock", "authors": "Pierre Stock and Moustapha Cisse", "title": "ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and\n  Uncovering Biases", "comments": "ECCV 2018 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ConvNets and Imagenet have driven the recent success of deep learning for\nimage classification. However, the marked slowdown in performance improvement\ncombined with the lack of robustness of neural networks to adversarial examples\nand their tendency to exhibit undesirable biases question the reliability of\nthese methods. This work investigates these questions from the perspective of\nthe end-user by using human subject studies and explanations. The contribution\nof this study is threefold. We first experimentally demonstrate that the\naccuracy and robustness of ConvNets measured on Imagenet are vastly\nunderestimated. Next, we show that explanations can mitigate the impact of\nmisclassified adversarial examples from the perspective of the end-user. We\nfinally introduce a novel tool for uncovering the undesirable biases learned by\na model. These contributions also show that explanations are a valuable tool\nboth for improving our understanding of ConvNets' predictions and for designing\nmore reliable models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 14:50:55 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 16:57:30 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Stock", "Pierre", ""], ["Cisse", "Moustapha", ""]]}, {"id": "1711.11508", "submitter": "Ming Liu", "authors": "Ming Liu, Bo Lang, and Zepeng Gu", "title": "Calculating Semantic Similarity between Academic Articles using Topic\n  Event and Ontology", "comments": "21 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining semantic similarity between academic documents is crucial to many\ntasks such as plagiarism detection, automatic technical survey and semantic\nsearch. Current studies mostly focus on semantic similarity between concepts,\nsentences and short text fragments. However, document-level semantic matching\nis still based on statistical information in surface level, neglecting article\nstructures and global semantic meanings, which may cause the deviation in\ndocument understanding. In this paper, we focus on the document-level semantic\nsimilarity issue for academic literatures with a novel method. We represent\nacademic articles with topic events that utilize multiple information profiles,\nsuch as research purposes, methodologies and domains to integrally describe the\nresearch work, and calculate the similarity between topic events based on the\ndomain ontology to acquire the semantic similarity between articles.\nExperiments show that our approach achieves significant performance compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 16:58:46 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Liu", "Ming", ""], ["Lang", "Bo", ""], ["Gu", "Zepeng", ""]]}, {"id": "1711.11543", "submitter": "Abhishek Das", "authors": "Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh,\n  Dhruv Batra", "title": "Embodied Question Answering", "comments": "20 pages, 13 figures, Webpage: https://embodiedqa.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where\nan agent is spawned at a random location in a 3D environment and asked a\nquestion (\"What color is the car?\"). In order to answer, the agent must first\nintelligently navigate to explore the environment, gather information through\nfirst-person (egocentric) vision, and then answer the question (\"orange\").\n  This challenging task requires a range of AI skills -- active perception,\nlanguage understanding, goal-driven navigation, commonsense reasoning, and\ngrounding of language into actions. In this work, we develop the environments,\nend-to-end-trained reinforcement learning agents, and evaluation protocols for\nEmbodiedQA.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:06:47 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 16:55:05 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Das", "Abhishek", ""], ["Datta", "Samyak", ""], ["Gkioxari", "Georgia", ""], ["Lee", "Stefan", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1711.11565", "submitter": "Weipeng He", "authors": "Weipeng He, Petr Motlicek and Jean-Marc Odobez", "title": "Deep Neural Networks for Multiple Speaker Detection and Localization", "comments": "Accepted for ICRA 2018", "journal-ref": "2018 IEEE International Conference on Robotics and Automation\n  (ICRA), Brisbane, Australia, 2018, pp. 74-79", "doi": "10.1109/ICRA.2018.8461267", "report-no": null, "categories": "cs.SD cs.AI cs.MM cs.RO eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use neural networks for simultaneous detection and localization\nof multiple sound sources in human-robot interaction. In contrast to\nconventional signal processing techniques, neural network-based sound source\nlocalization methods require fewer strong assumptions about the environment.\nPrevious neural network-based methods have been focusing on localizing a single\nsound source, which do not extend to multiple sources in terms of detection and\nlocalization. In this paper, we thus propose a likelihood-based encoding of the\nnetwork output, which naturally allows the detection of an arbitrary number of\nsources. In addition, we investigate the use of sub-band cross-correlation\ninformation as features for better localization in sound mixtures, as well as\nthree different network architectures based on different motivations.\nExperiments on real data recorded from a robot show that our proposed methods\nsignificantly outperform the popular spatial spectrum-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:35:22 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 17:27:05 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 09:04:36 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["He", "Weipeng", ""], ["Motlicek", "Petr", ""], ["Odobez", "Jean-Marc", ""]]}]