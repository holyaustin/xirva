[{"id": "1512.00047", "submitter": "Florentin Smarandache", "authors": "Florentin Smarandache", "title": "Symbolic Neutrosophic Theory", "comments": "195 pages, several graphs, Published as book in Bruxelles, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic (or Literal) Neutrosophic Theory is referring to the use of abstract\nsymbols (i.e. the letters T, I, F, or their refined indexed letters Tj, Ik, Fl)\nin neutrosophics. We extend the dialectical triad thesis-antithesis-synthesis\nto the neutrosophic tetrad thesis-antithesis-neutrothesis-neutrosynthesis. The\nwe introduce the neutrosophic system that is a quasi or (t,i,f) classical\nsystem, in the sense that the neutrosophic system deals with quasi-terms\n(concepts, attributes, etc.). Then the notions of Neutrosophic Axiom,\nNeutrosophic Deducibility, Degree of Contradiction (Dissimilarity) of Two\nNeutrosophic Axioms, etc. Afterwards a new type of structures, called (t, i, f)\nNeutrosophic Structures, and we show particular cases of such structures in\ngeometry and in algebra. Also, a short history of the neutrosophic set,\nneutrosophic numerical components and neutrosophic literal components,\nneutrosophic numbers, etc. We construct examples of splitting the literal\nindeterminacy (I) into literal subindeterminacies (I1, I2, and so on, Ir), and\nto define a multiplication law of these literal subindeterminacies in order to\nbe able to build refined I neutrosophic algebraic structures. We define three\nneutrosophic actions and their properties. We then introduce the prevalence\norder on T,I,F with respect to a given neutrosophic operator. And the\nrefinement of neutrosophic entities A, neutA, and antiA. Then we extend the\nclassical logical operators to neutrosophic literal (symbolic) logical\noperators and to refined literal (symbolic) logical operators, and we define\nthe refinement neutrosophic literal (symbolic) space. We introduce the\nneutrosophic quadruple numbers (a+bT+cI+dF) and the refined neutrosophic\nquadruple numbers. Then we define an absorbance law, based on a prevalence\norder, in order to multiply the neutrosophic quadruple numbers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 00:32:31 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Smarandache", "Florentin", ""]]}, {"id": "1512.00112", "submitter": "Shashank Srivastava", "authors": "Shashank Srivastava, Snigdha Chaturvedi and Tom Mitchell", "title": "Inferring Interpersonal Relations in Narrative Summaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing relationships between people is fundamental for the\nunderstanding of narratives. In this work, we address the problem of inferring\nthe polarity of relationships between people in narrative summaries. We\nformulate the problem as a joint structured prediction for each narrative, and\npresent a model that combines evidence from linguistic and semantic features,\nas well as features based on the structure of the social community in the text.\nWe also provide a clustering-based approach that can exploit regularities in\nnarrative types. e.g., learn an affinity for love-triangles in romantic\nstories. On a dataset of movie summaries from Wikipedia, our structured models\nprovide more than a 30% error-reduction over a competitive baseline that\nconsiders pairs of characters in isolation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 01:11:46 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Srivastava", "Shashank", ""], ["Chaturvedi", "Snigdha", ""], ["Mitchell", "Tom", ""]]}, {"id": "1512.00165", "submitter": "Galit Bary", "authors": "Galit Bary", "title": "Learning Using 1-Local Membership Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic machine learning algorithms learn from labelled examples. For\nexample, to design a machine translation system, a typical training set will\nconsist of English sentences and their translation. There is a stronger model,\nin which the algorithm can also query for labels of new examples it creates.\nE.g, in the translation task, the algorithm can create a new English sentence,\nand request its translation from the user during training. This combination of\nexamples and queries has been widely studied. Yet, despite many theoretical\nresults, query algorithms are almost never used. One of the main causes for\nthis is a report (Baum and Lang, 1992) on very disappointing empirical\nperformance of a query algorithm. These poor results were mainly attributed to\nthe fact that the algorithm queried for labels of examples that are artificial,\nand impossible to interpret by humans.\n  In this work we study a new model of local membership queries (Awasthi et\nal., 2012), which tries to resolve the problem of artificial queries. In this\nmodel, the algorithm is only allowed to query the labels of examples which are\nclose to examples from the training set. E.g., in translation, the algorithm\ncan change individual words in a sentence it has already seen, and then ask for\nthe translation. In this model, the examples queried by the algorithm will be\nclose to natural examples and hence, hopefully, will not appear as artificial\nor random. We focus on 1-local queries (i.e., queries of distance 1 from an\nexample in the training sample). We show that 1-local membership queries are\nalready stronger than the standard learning model. We also present an\nexperiment on a well known NLP task of sentiment analysis. In this experiment,\nthe users were asked to provide more information than merely indicating the\nlabel. We present results that illustrate that this extra information is\nbeneficial in practice.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 07:40:49 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Bary", "Galit", ""]]}, {"id": "1512.00177", "submitter": "Yiming Cui", "authors": "Yiming Cui, Shijin Wang, Jianfeng Li", "title": "LSTM Neural Reordering Feature for Statistical Machine Translation", "comments": "6 pages, accepted by NAACL2016 short paper", "journal-ref": null, "doi": "10.18653/v1/N16-1112", "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks are powerful models, which have been widely\napplied into many aspects of machine translation, such as language modeling and\ntranslation modeling. Though notable improvements have been made in these\nareas, the reordering problem still remains a challenge in statistical machine\ntranslations. In this paper, we present a novel neural reordering model that\ndirectly models word pairs and alignment. By utilizing LSTM recurrent neural\nnetworks, much longer context could be learned for reordering prediction.\nExperimental results on NIST OpenMT12 Arabic-English and Chinese-English\n1000-best rescoring task show that our LSTM neural reordering feature is robust\nand achieves significant improvements over various baseline systems.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 08:43:19 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 01:17:22 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 10:01:49 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Cui", "Yiming", ""], ["Wang", "Shijin", ""], ["Li", "Jianfeng", ""]]}, {"id": "1512.00250", "submitter": "Keyan Ghazi-Zahedi", "authors": "Keyan Ghazi-Zahedi and Daniel F.B. Haeufle and Guido Montufar and Syn\n  Schmitt and Nihat Ay", "title": "Evaluating Morphological Computation in Muscle and DC-motor Driven\n  Models of Human Hopping", "comments": "10 pages, 4 figures, 1 table, 5 algorithms", "journal-ref": null, "doi": "10.3389/frobt.2016.00042", "report-no": null, "categories": "cs.AI cs.IT cs.RO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of embodied artificial intelligence, morphological computation\nrefers to processes which are conducted by the body (and environment) that\notherwise would have to be performed by the brain. Exploiting environmental and\nmorphological properties is an important feature of embodied systems. The main\nreason is that it allows to significantly reduce the controller complexity. An\nimportant aspect of morphological computation is that it cannot be assigned to\nan embodied system per se, but that it is, as we show, behavior- and\nstate-dependent. In this work, we evaluate two different measures of\nmorphological computation that can be applied in robotic systems and in\ncomputer simulations of biological movement. As an example, these measures were\nevaluated on muscle and DC-motor driven hopping models. We show that a\nstate-dependent analysis of the hopping behaviors provides additional insights\nthat cannot be gained from the averaged measures alone. This work includes\nalgorithms and computer code for the measures.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 13:26:33 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 11:01:14 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2015 10:39:46 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Ghazi-Zahedi", "Keyan", ""], ["Haeufle", "Daniel F. B.", ""], ["Montufar", "Guido", ""], ["Schmitt", "Syn", ""], ["Ay", "Nihat", ""]]}, {"id": "1512.00306", "submitter": "Luiz Capretz Dr.", "authors": "Wei Lin Du, Luiz Fernando Capretz, Ali Bou Nassif, Danny Ho", "title": "A Hybrid Intelligent Model for Software Cost Estimation", "comments": null, "journal-ref": "Journal of Computer Science, 9(11):1506-1513, 2013", "doi": "10.3844/ajbb.2013.1506-1513", "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate software development effort estimation is critical to the success of\nsoftware projects. Although many techniques and algorithmic models have been\ndeveloped and implemented by practitioners, accurate software development\neffort prediction is still a challenging endeavor in the field of software\nengineering, especially in handling uncertain and imprecise inputs and\ncollinear characteristics. In this paper, a hybrid in-telligent model combining\na neural network model integrated with fuzzy model (neuro-fuzzy model) has been\nused to improve the accuracy of estimating software cost. The performance of\nthe proposed model is assessed by designing and conducting evaluation with\npublished project and industrial data. Results have shown that the proposed\nmodel demonstrates the ability of improving the estimation accuracy by 18%\nbased on the Mean Magnitude of Relative Error (MMRE) criterion.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 15:47:29 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Du", "Wei Lin", ""], ["Capretz", "Luiz Fernando", ""], ["Nassif", "Ali Bou", ""], ["Ho", "Danny", ""]]}, {"id": "1512.00355", "submitter": "Amrita Saha", "authors": "Amrita Saha, Sathish Indurthi, Shantanu Godbole, Subendhu Rongali and\n  Vikas C. Raykar", "title": "Taxonomy grounded aggregation of classifiers with different label sets", "comments": "Under review by AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the problem of aggregating the label predictions of diverse\nclassifiers using a class taxonomy. Such a taxonomy may not have been available\nor referenced when the individual classifiers were designed and trained, yet\nmapping the output labels into the taxonomy is desirable to integrate the\neffort spent in training the constituent classifiers. A hierarchical taxonomy\nrepresenting some domain knowledge may be different from, but partially\nmappable to, the label sets of the individual classifiers. We present a\nheuristic approach and a principled graphical model to aggregate the label\npredictions by grounding them into the available taxonomy. Our model aggregates\nthe labels using the taxonomy structure as constraints to find the most likely\nhierarchically consistent class. We experimentally validate our proposed method\non image and text classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 17:32:16 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Saha", "Amrita", ""], ["Indurthi", "Sathish", ""], ["Godbole", "Shantanu", ""], ["Rongali", "Subendhu", ""], ["Raykar", "Vikas C.", ""]]}, {"id": "1512.00397", "submitter": "Ehsaneddin Asgari", "authors": "Ehsaneddin Asgari, Kiavash Garakani and Mohammad R.K Mofrad", "title": "A New Approach for Scalable Analysis of Microbial Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microbial communities play important roles in the function and maintenance of\nvarious biosystems, ranging from human body to the environment. Current methods\nfor analysis of microbial communities are typically based on taxonomic\nphylogenetic alignment using 16S rRNA metagenomic or Whole Genome Sequencing\ndata. In typical characterizations of microbial communities, studies deal with\nbillions of micobial sequences, aligning them to a phylogenetic tree. We\nintroduce a new approach for the efficient analysis of microbial communities.\nOur new reference-free analysis tech- nique is based on n-gram sequence\nanalysis of 16S rRNA data and reduces the processing data size dramatically (by\n105 fold), without requiring taxonomic alignment. The proposed approach is\napplied to characterize phenotypic microbial community differ- ences in\ndifferent settings. Specifically, we applied this approach in classification of\nmicrobial com- munities across different body sites, characterization of oral\nmicrobiomes associated with healthy and diseased individuals, and\nclassification of microbial communities longitudinally during the develop- ment\nof infants. Different dimensionality reduction methods are introduced that\noffer a more scalable analysis framework, while minimizing the loss in\nclassification accuracies. Among dimensionality re- duction techniques, we\npropose a continuous vector representation for microbial communities, which can\nwidely be used for deep learning applications in microbial informatics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 19:27:13 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Asgari", "Ehsaneddin", ""], ["Garakani", "Kiavash", ""], ["Mofrad", "Mohammad R. K", ""]]}, {"id": "1512.00442", "submitter": "Ke Li", "authors": "Ke Li, Jitendra Malik", "title": "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing", "comments": "13 pages, 6 figures; International Conference on Machine Learning\n  (ICML), 2016. This version corrects a typo in the pseudocode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for retrieving k-nearest neighbours suffer from the curse of\ndimensionality. We argue this is caused in part by inherent deficiencies of\nspace partitioning, which is the underlying strategy used by most existing\nmethods. We devise a new strategy that avoids partitioning the vector space and\npresent a novel randomized algorithm that runs in time linear in dimensionality\nof the space and sub-linear in the intrinsic dimensionality and the size of the\ndataset and takes space constant in dimensionality of the space and linear in\nthe size of the dataset. The proposed algorithm allows fine-grained control\nover accuracy and speed on a per-query basis, automatically adapts to\nvariations in data density, supports dynamic updates to the dataset and is\neasy-to-implement. We show appealing theoretical properties and demonstrate\nempirically that the proposed algorithm outperforms locality-sensitivity\nhashing (LSH) in terms of approximation quality, speed and space efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 20:53:16 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 18:47:10 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 06:51:49 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1512.00570", "submitter": "Xinchen Yan", "authors": "Xinchen Yan, Jimei Yang, Kihyuk Sohn, Honglak Lee", "title": "Attribute2Image: Conditional Image Generation from Visual Attributes", "comments": "19 pages, accepted by ECCV 2016, The 14th European Conference on\n  Computer Vision (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a novel problem of generating images from visual\nattributes. We model the image as a composite of foreground and background and\ndevelop a layered generative model with disentangled latent variables that can\nbe learned end-to-end using a variational auto-encoder. We experiment with\nnatural images of faces and birds and demonstrate that the proposed models are\ncapable of generating realistic and diverse samples with disentangled latent\nrepresentations. We use a general energy minimization algorithm for posterior\ninference of latent variables given novel images. Therefore, the learned\ngenerative models show excellent quantitative and visual results in the tasks\nof attribute-conditioned image reconstruction and completion.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 04:07:28 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 08:55:32 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Yan", "Xinchen", ""], ["Yang", "Jimei", ""], ["Sohn", "Kihyuk", ""], ["Lee", "Honglak", ""]]}, {"id": "1512.00573", "submitter": "Lawson Wong", "authors": "Lawson L.S. Wong, Thanard Kurutach, Leslie Pack Kaelbling, Tom\\'as\n  Lozano-P\\'erez", "title": "Object-based World Modeling in Semi-Static Environments with Dependent\n  Dirichlet-Process Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accomplish tasks in human-centric indoor environments, robots need to\nrepresent and understand the world in terms of objects and their attributes. We\nrefer to this attribute-based representation as a world model, and consider how\nto acquire it via noisy perception and maintain it over time, as objects are\nadded, changed, and removed in the world. Previous work has framed this as\nmultiple-target tracking problem, where objects are potentially in motion at\nall times. Although this approach is general, it is computationally expensive.\nWe argue that such generality is not needed in typical world modeling tasks,\nwhere objects only change state occasionally. More efficient approaches are\nenabled by restricting ourselves to such semi-static environments.\n  We consider a previously-proposed clustering-based world modeling approach\nthat assumed static environments, and extend it to semi-static domains by\napplying a dependent Dirichlet-process (DDP) mixture model. We derive a novel\nMAP inference algorithm under this model, subject to data association\nconstraints. We demonstrate our approach improves computational performance in\nsemi-static environments.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 04:32:02 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Wong", "Lawson L. S.", ""], ["Kurutach", "Thanard", ""], ["Kaelbling", "Leslie Pack", ""], ["Lozano-P\u00e9rez", "Tom\u00e1s", ""]]}, {"id": "1512.00743", "submitter": "Amogh Gudi", "authors": "Amogh Gudi", "title": "Recognizing Semantic Features in Faces using Deep Learning", "comments": "Thesis, M.Sc. Artificial Intelligence, University of Amsterdam, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human face constantly conveys information, both consciously and\nsubconsciously. However, as basic as it is for humans to visually interpret\nthis information, it is quite a big challenge for machines. Conventional\nsemantic facial feature recognition and analysis techniques are already in use\nand are based on physiological heuristics, but they suffer from lack of\nrobustness and high computation time. This thesis aims to explore ways for\nmachines to learn to interpret semantic information available in faces in an\nautomated manner without requiring manual design of feature detectors, using\nthe approach of Deep Learning. This thesis provides a study of the effects of\nvarious factors and hyper-parameters of deep neural networks in the process of\ndetermining an optimal network configuration for the task of semantic facial\nfeature recognition. This thesis explores the effectiveness of the system to\nrecognize the various semantic features (like emotions, age, gender, ethnicity\netc.) present in faces. Furthermore, the relation between the effect of\nhigh-level concepts on low level features is explored through an analysis of\nthe similarities in low-level descriptors of different semantic features. This\nthesis also demonstrates a novel idea of using a deep network to generate 3-D\nActive Appearance Models of faces from real-world 2-D images.\n  For a more detailed report on this work, please see [arXiv:1512.00743v1].\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 15:46:26 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 13:33:44 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Gudi", "Amogh", ""]]}, {"id": "1512.00880", "submitter": "Diederik Aerts", "authors": "Diederik Aerts and Massimiliano Sassoli de Bianchi", "title": "The GTR-model: a universal framework for quantum-like measurements", "comments": "33 pages, 5 figures", "journal-ref": "In: Probing the Meaning of Quantum Mechanics. Superpositions,\n  Dynamics, Semantics and Identity, pp. 91-140. Eds. D. Aerts, C. De Ronde, H.\n  Freytes and R. Giuntini, World Scientific Publishing Company, Singapore\n  (2016)", "doi": "10.1142/9789813146280_0005", "report-no": null, "categories": "quant-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a very general geometrico-dynamical description of physical or\nmore abstract entities, called the 'general tension-reduction' (GTR) model,\nwhere not only states, but also measurement-interactions can be represented,\nand the associated outcome probabilities calculated. Underlying the model is\nthe hypothesis that indeterminism manifests as a consequence of unavoidable\nfluctuations in the experimental context, in accordance with the\n'hidden-measurements interpretation' of quantum mechanics. When the structure\nof the state space is Hilbertian, and measurements are of the 'universal' kind,\ni.e., are the result of an average over all possible ways of selecting an\noutcome, the GTR-model provides the same predictions of the Born rule, and\ntherefore provides a natural completed version of quantum mechanics. However,\nwhen the structure of the state space is non-Hilbertian and/or not all possible\nways of selecting an outcome are available to be actualized, the predictions of\nthe model generally differ from the quantum ones, especially when sequential\nmeasurements are considered. Some paradigmatic examples will be discussed,\ntaken from physics and human cognition. Particular attention will be given to\nsome known psychological effects, like question order effects and response\nreplicability, which we show are able to generate non-Hilbertian statistics. We\nalso suggest a realistic interpretation of the GTR-model, when applied to human\ncognition and decision, which we think could become the generally adopted\ninterpretative framework in quantum cognition research.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 21:51:31 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Aerts", "Diederik", ""], ["de Bianchi", "Massimiliano Sassoli", ""]]}, {"id": "1512.00964", "submitter": "Ryo Nakahashi", "authors": "Ryo Nakahashi, Chris L. Baker, Joshua B. Tenenbaum", "title": "Modeling Human Understanding of Complex Intentional Action with a\n  Bayesian Nonparametric Subgoal Model", "comments": "Accepted at AAAI 16", "journal-ref": "Proceedings of 30th conference on artificial intelligence (AAAI\n  2016) pp. 3754--3760", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most human behaviors consist of multiple parts, steps, or subtasks. These\nstructures guide our action planning and execution, but when we observe others,\nthe latent structure of their actions is typically unobservable, and must be\ninferred in order to learn new skills by demonstration, or to assist others in\ncompleting their tasks. For example, an assistant who has learned the subgoal\nstructure of a colleague's task can more rapidly recognize and support their\nactions as they unfold. Here we model how humans infer subgoals from\nobservations of complex action sequences using a nonparametric Bayesian model,\nwhich assumes that observed actions are generated by approximately rational\nplanning over unknown subgoal sequences. We test this model with a behavioral\nexperiment in which humans observed different series of goal-directed actions,\nand inferred both the number and composition of the subgoal sequences\nassociated with each goal. The Bayesian model predicts human subgoal inferences\nwith high accuracy, and significantly better than several alternative models\nand straightforward heuristics. Motivated by this result, we simulate how\nlearning and inference of subgoals can improve performance in an artificial\nuser assistance task. The Bayesian model learns the correct subgoals from fewer\nobservations, and better assists users by more rapidly and accurately inferring\nthe goal of their actions than alternative approaches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 06:44:35 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Nakahashi", "Ryo", ""], ["Baker", "Chris L.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1512.00965", "submitter": "Pengcheng Yin", "authors": "Pengcheng Yin, Zhengdong Lu, Hang Li, Ben Kao", "title": "Neural Enquirer: Learning to Query Tables with Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed Neural Enquirer as a neural network architecture to execute a\nnatural language (NL) query on a knowledge-base (KB) for answers. Basically,\nNeural Enquirer finds the distributed representation of a query and then\nexecutes it on knowledge-base tables to obtain the answer as one of the values\nin the tables. Unlike similar efforts in end-to-end training of semantic\nparsers, Neural Enquirer is fully \"neuralized\": it not only gives\ndistributional representation of the query and the knowledge-base, but also\nrealizes the execution of compositional queries as a series of differentiable\noperations, with intermediate results (consisting of annotations of the tables\nat different levels) saved on multiple layers of memory. Neural Enquirer can be\ntrained with gradient descent, with which not only the parameters of the\ncontrolling components and semantic parsing component, but also the embeddings\nof the tables and query words can be learned from scratch. The training can be\ndone in an end-to-end fashion, but it can take stronger guidance, e.g., the\nstep-by-step supervision for complicated queries, and benefit from it. Neural\nEnquirer is one step towards building neural network systems which seek to\nunderstand language by executing it on real-world. Our experiments show that\nNeural Enquirer can learn to execute fairly complicated NL queries on tables\nwith rich structures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 06:46:27 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 02:46:25 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Yin", "Pengcheng", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Kao", "Ben", ""]]}, {"id": "1512.00977", "submitter": "Liu Feng", "authors": "Feng Liu, Yong Shi", "title": "A Study on Artificial Intelligence IQ and Standard Intelligent Model", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, potential threats of artificial intelligence (AI) to human have\ntriggered a large controversy in society, behind which, the nature of the issue\nis whether the artificial intelligence (AI) system can be evaluated\nquantitatively. This article analyzes and evaluates the challenges that the AI\ndevelopment level is facing, and proposes that the evaluation methods for the\nhuman intelligence test and the AI system are not uniform; and the key reason\nfor which is that none of the models can uniformly describe the AI system and\nthe beings like human. Aiming at this problem, a standard intelligent system\nmodel is established in this study to describe the AI system and the beings\nlike human uniformly. Based on the model, the article makes an abstract\nmathematical description, and builds the standard intelligent machine\nmathematical model; expands the Von Neumann architecture and proposes the\nLiufeng - Shiyong architecture; gives the definition of the artificial\nintelligence IQ, and establishes the artificial intelligence scale and the\nevaluation method; conduct the test on 50 search engines and three human\nsubjects at different ages across the world, and finally obtains the ranking of\nthe absolute IQ and deviation IQ ranking for artificial intelligence IQ 2014.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 07:45:32 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Liu", "Feng", ""], ["Shi", "Yong", ""]]}, {"id": "1512.01027", "submitter": "Firas Hamze", "authors": "Firas Hamze, Evgeny Andryash", "title": "Discrete Equilibrium Sampling with Arbitrary Nonequilibrium Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cond-mat.dis-nn cond-mat.stat-mech cs.AI physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for performing statistical sampling, expectation\nestimation, and partition function approximation using \\emph{arbitrary}\nheuristic stochastic processes defined over discrete state spaces. Using a\nhighly parallel construction we call the \\emph{sequential constraining\nprocess}, we are able to simultaneously generate states with the heuristic\nprocess and accurately estimate their probabilities, even when they are far too\nsmall to be realistically inferred by direct counting. After showing that both\ntheoretically correct importance sampling and Markov chain Monte Carlo are\npossible using the sequential constraining process, we integrate it into a\nmethodology called \\emph{state space sampling}, extending the ideas of state\nspace search from computer science to the sampling context. The methodology\ncomprises a dynamic data structure that constructs a robust Bayesian model of\nthe statistics generated by the heuristic process subject to an accuracy\nconstraint, the posterior Kullback-Leibler divergence. Sampling from the\ndynamic structure will generally yield partial states, which are completed by\nrecursively calling the heuristic to refine the structure and resuming the\nsampling. Our experiments on various Ising models suggest that state space\nsampling enables heuristic state generation with accurate probability\nestimates, demonstrated by illustrating the convergence of a simulated\nannealing process to the Boltzmann distribution with increasing run length.\nConsequently, heretofore unprecedented direct importance sampling using the\n\\emph{final} (marginal) distribution of a generic stochastic process is\nallowed, potentially augmenting the range of algorithms at the Monte Carlo\npractitioner's disposal.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 10:28:58 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Hamze", "Firas", ""], ["Andryash", "Evgeny", ""]]}, {"id": "1512.01041", "submitter": "Pietro Codara", "authors": "Stefano Aguzzoli, Pietro Codara, Tommaso Flaminio, Brunella Gerla,\n  Diego Valota", "title": "Querying with {\\L}ukasiewicz logic", "comments": null, "journal-ref": "2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),\n  pp.1-8", "doi": "10.1109/FUZZ-IEEE.2015.7338061", "report-no": null, "categories": "cs.LO cs.AI cs.DB math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present, by way of case studies, a proof of concept, based\non a prototype working on a automotive data set, aimed at showing the potential\nusefulness of using formulas of {\\L}ukasiewicz propositional logic to query\ndatabases in a fuzzy way. Our approach distinguishes itself for its stress on\nthe purely linguistic, contraposed with numeric, formulations of queries. Our\nqueries are expressed in the pure language of logic, and when we use (integer)\nnumbers, these stand for shortenings of formulas on the syntactic level, and\nserve as linguistic hedges on the semantic one. Our case-study queries aim\nfirst at showing that each numeric-threshold fuzzy query is simulated by a\n{\\L}ukasiewicz formula. Then they focus on the expressing power of\n{\\L}ukasiewicz logic which easily allows for updating queries by clauses and\nfor modifying them through a potentially infinite variety of linguistic hedges\nimplemented with a uniform syntactic mechanism. Finally we shall hint how,\nalready at propositional level, {\\L}ukasiewicz natural semantics enjoys a\ndegree of reflection, allowing to write syntactically simple queries that\nsemantically work as meta-queries weighing the contribution of simpler ones.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 11:26:40 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Aguzzoli", "Stefano", ""], ["Codara", "Pietro", ""], ["Flaminio", "Tommaso", ""], ["Gerla", "Brunella", ""], ["Valota", "Diego", ""]]}, {"id": "1512.01110", "submitter": "Yang Song", "authors": "Yang Song, Jun Zhu", "title": "Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization", "comments": "Accepted to AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian matrix completion has been studied based on a low-rank matrix\nfactorization formulation with promising results. However, little work has been\ndone on Bayesian matrix completion based on the more direct spectral\nregularization formulation. We fill this gap by presenting a novel Bayesian\nmatrix completion method based on spectral regularization. In order to\ncircumvent the difficulties of dealing with the orthonormality constraints of\nsingular vectors, we derive a new equivalent form with relaxed constraints,\nwhich then leads us to design an adaptive version of spectral regularization\nfeasible for Bayesian inference. Our Bayesian method requires no parameter\ntuning and can infer the number of latent factors automatically. Experiments on\nsynthetic and real datasets demonstrate encouraging results on rank recovery\nand collaborative filtering, with notably good results for very sparse\nmatrices.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 15:16:19 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2015 02:51:22 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Song", "Yang", ""], ["Zhu", "Jun", ""]]}, {"id": "1512.01124", "submitter": "Peter Sunehag", "authors": "Peter Sunehag, Richard Evans, Gabriel Dulac-Arnold, Yori Zwols, Daniel\n  Visentin and Ben Coppin", "title": "Deep Reinforcement Learning with Attention for Slate Markov Decision\n  Processes with High-Dimensional States and Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world problems come with action spaces represented as feature\nvectors. Although high-dimensional control is a largely unsolved problem, there\nhas recently been progress for modest dimensionalities. Here we report on a\nsuccessful attempt at addressing problems of dimensionality as high as $2000$,\nof a particular form. Motivated by important applications such as\nrecommendation systems that do not fit the standard reinforcement learning\nframeworks, we introduce Slate Markov Decision Processes (slate-MDPs). A\nSlate-MDP is an MDP with a combinatorial action space consisting of slates\n(tuples) of primitive actions of which one is executed in an underlying MDP.\nThe agent does not control the choice of this executed action and the action\nmight not even be from the slate, e.g., for recommendation systems for which\nall recommendations can be ignored. We use deep Q-learning based on feature\nrepresentations of both the state and action to learn the value of whole\nslates. Unlike existing methods, we optimize for both the combinatorial and\nsequential aspects of our tasks. The new agent's superiority over agents that\neither ignore the combinatorial or sequential long-term value aspect is\ndemonstrated on a range of environments with dynamics from a real-world\nrecommendation system. Further, we use deep deterministic policy gradients to\nlearn a policy that for each position of the slate, guides attention towards\nthe part of the action space in which the value is the highest and we only\nevaluate actions in this area. The attention is used within a sequentially\ngreedy procedure leveraging submodularity. Finally, we show how introducing\nrisk-seeking can dramatically improve the agents performance and ability to\ndiscover more far reaching strategies.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 15:51:30 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 17:34:55 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Sunehag", "Peter", ""], ["Evans", "Richard", ""], ["Dulac-Arnold", "Gabriel", ""], ["Zwols", "Yori", ""], ["Visentin", "Daniel", ""], ["Coppin", "Ben", ""]]}, {"id": "1512.01173", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Jun Zhu", "title": "Building Memory with Concept Learning Capabilities from Large-scale\n  Knowledge Base", "comments": "Accepted to NIPS 2015 Cognitive Computation workshop (CoCo@NIPS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new perspective on neural knowledge base (KB) embeddings, from\nwhich we build a framework that can model symbolic knowledge in the KB together\nwith its learning process. We show that this framework well regularizes\nprevious neural KB embedding model for superior performance in reasoning tasks,\nwhile having the capabilities of dealing with unseen entities, that is, to\nlearn their embeddings from natural language descriptions, which is very like\nhuman's behavior of learning semantic concepts.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 17:52:50 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Shi", "Jiaxin", ""], ["Zhu", "Jun", ""]]}, {"id": "1512.01249", "submitter": "Ronald Meester", "authors": "Timber Kerkvliet and Ronald Meester", "title": "Quantifying knowledge with a new calculus for belief functions - a\n  generalization of probability theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first show that there are practical situations in for instance forensic\nand gambling settings, in which applying classical probability theory, that is,\nbased on the axioms of Kolmogorov, is problematic. We then introduce and\ndiscuss Shafer belief functions. Technically, Shafer belief functions\ngeneralize probability distributions. Philosophically, they pertain to\nindividual or shared knowledge of facts, rather than to facts themselves, and\ntherefore can be interpreted as generalizing epistemic probability, that is,\nprobability theory interpreted epistemologically. Belief functions are more\nflexible and better suited to deal with certain types of uncertainty than\nclassical probability distributions. We develop a new calculus for belief\nfunctions which does not use the much criticized Dempster's rule of\ncombination, by generalizing the classical notions of conditioning and\nindependence in a natural and uncontroversial way. Using this calculus, we\nexplain our rejection of Dempster's rule in detail. We apply the new theory to\na number of examples, including a gambling example and an example in a forensic\nsetting. We prove a law of large numbers for belief functions and offer a\nbetting interpretation similar to the Dutch Book Theorem for probability\ndistributions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 13:33:03 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Kerkvliet", "Timber", ""], ["Meester", "Ronald", ""]]}, {"id": "1512.01250", "submitter": "Timber Kerkvliet", "authors": "Timber Kerkvliet and Ronald Meester", "title": "Assessing forensic evidence by computing belief functions", "comments": "arXiv admin note: text overlap with arXiv:1512.01249. Accepted for\n  publication in Law, Probability and Risk", "journal-ref": "Law, Probability & Risk (2016) 15 (2),127-153", "doi": "10.1093/lpr/mgw002", "report-no": null, "categories": "math.PR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first discuss certain problems with the classical probabilistic approach\nfor assessing forensic evidence, in particular its inability to distinguish\nbetween lack of belief and disbelief, and its inability to model complete\nignorance within a given population. We then discuss Shafer belief functions, a\ngeneralization of probability distributions, which can deal with both these\nobjections. We use a calculus of belief functions which does not use the much\ncriticized Dempster rule of combination, but only the very natural\nDempster-Shafer conditioning. We then apply this calculus to some classical\nforensic problems like the various island problems and the problem of parental\nidentification. If we impose no prior knowledge apart from assuming that the\nculprit or parent belongs to a given population (something which is possible in\nour setting), then our answers differ from the classical ones when uniform or\nother priors are imposed. We can actually retrieve the classical answers by\nimposing the relevant priors, so our setup can and should be interpreted as a\ngeneralization of the classical methodology, allowing more flexibility. We show\nhow our calculus can be used to develop an analogue of Bayes' rule, with belief\nfunctions instead of classical probabilities. We also discuss consequences of\nour theory for legal practice.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 13:37:40 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 18:57:56 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Kerkvliet", "Timber", ""], ["Meester", "Ronald", ""]]}, {"id": "1512.01272", "submitter": "Vikash Mansinghka", "authors": "Vikash Mansinghka, Patrick Shafto, Eric Jonas, Cap Petschulat, Max\n  Gasner, Joshua B. Tenenbaum", "title": "CrossCat: A Fully Bayesian Nonparametric Method for Analyzing\n  Heterogeneous, High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a widespread need for statistical methods that can analyze\nhigh-dimensional datasets with- out imposing restrictive or opaque modeling\nassumptions. This paper describes a domain-general data analysis method called\nCrossCat. CrossCat infers multiple non-overlapping views of the data, each\nconsisting of a subset of the variables, and uses a separate nonparametric\nmixture to model each view. CrossCat is based on approximately Bayesian\ninference in a hierarchical, nonparamet- ric model for data tables. This model\nconsists of a Dirichlet process mixture over the columns of a data table in\nwhich each mixture component is itself an independent Dirichlet process mixture\nover the rows; the inner mixture components are simple parametric models whose\nform depends on the types of data in the table. CrossCat combines strengths of\nmixture modeling and Bayesian net- work structure learning. Like mixture\nmodeling, CrossCat can model a broad class of distributions by positing latent\nvariables, and produces representations that can be efficiently conditioned and\nsampled from for prediction. Like Bayesian networks, CrossCat represents the\ndependencies and independencies between variables, and thus remains accurate\nwhen there are multiple statistical signals. Inference is done via a scalable\nGibbs sampling scheme; this paper shows that it works well in practice. This\npaper also includes empirical results on heterogeneous tabular data of up to 10\nmillion cells, such as hospital cost and quality measures, voting records,\nunemployment rates, gene expression measurements, and images of handwritten\ndigits. CrossCat infers structure that is consistent with accepted findings and\ncommon-sense knowledge in multiple domains and yields predictive accuracy\ncompetitive with generative, discriminative, and model-free alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 22:39:37 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Mansinghka", "Vikash", ""], ["Shafto", "Patrick", ""], ["Jonas", "Eric", ""], ["Petschulat", "Cap", ""], ["Gasner", "Max", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1512.01325", "submitter": "Babak Saleh", "authors": "Babak Saleh, Ahmed Elgammal, Jacob Feldman, Ali Farhadi", "title": "Toward a Taxonomy and Computational Models of Abnormalities in Images", "comments": "To appear in the Thirtieth AAAI Conference on Artificial Intelligence\n  (AAAI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system can spot an abnormal image, and reason about what\nmakes it strange. This task has not received enough attention in computer\nvision. In this paper we study various types of atypicalities in images in a\nmore comprehensive way than has been done before. We propose a new dataset of\nabnormal images showing a wide range of atypicalities. We design human subject\nexperiments to discover a coarse taxonomy of the reasons for abnormality. Our\nexperiments reveal three major categories of abnormality: object-centric,\nscene-centric, and contextual. Based on this taxonomy, we propose a\ncomprehensive computational model that can predict all different types of\nabnormality in images and outperform prior arts in abnormality recognition.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 06:29:53 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Saleh", "Babak", ""], ["Elgammal", "Ahmed", ""], ["Feldman", "Jacob", ""], ["Farhadi", "Ali", ""]]}, {"id": "1512.01370", "submitter": "Yantao Jia", "authors": "Yantao Jia, Yuanzhuo Wang, Hailun Lin, Xiaolong Jin, Xueqi Cheng", "title": "Locally Adaptive Translation for Knowledge Graph Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding aims to represent entities and relations in a\nlarge-scale knowledge graph as elements in a continuous vector space. Existing\nmethods, e.g., TransE and TransH, learn embedding representation by defining a\nglobal margin-based loss function over the data. However, the optimal loss\nfunction is determined during experiments whose parameters are examined among a\nclosed set of candidates. Moreover, embeddings over two knowledge graphs with\ndifferent entities and relations share the same set of candidate loss\nfunctions, ignoring the locality of both graphs. This leads to the limited\nperformance of embedding related applications. In this paper, we propose a\nlocally adaptive translation method for knowledge graph embedding, called\nTransA, to find the optimal loss function by adaptively determining its margin\nover different knowledge graphs. Experiments on two benchmark data sets\ndemonstrate the superiority of the proposed method, as compared to\nthe-state-of-the-art ones.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 11:09:55 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Jia", "Yantao", ""], ["Wang", "Yuanzhuo", ""], ["Lin", "Hailun", ""], ["Jin", "Xiaolong", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1512.01503", "submitter": "Quang Minh Ha", "authors": "Quang Minh Ha, Yves Deville, Quang Dung Pham, Minh Ho\\`ang H\\`a", "title": "On the Min-cost Traveling Salesman Problem with Drone", "comments": "We proposed arXiv:1509.08764 as the first report about our research\n  on TSP-D. However due to a critical error in the experiment, we changed the\n  research approach and method and propose arXiv:1512.01503. Now it seems\n  arXiv:1509.08764 received new citations. we would like to withdraw\n  arXiv:1512.01503 and replaced arXiv:1509.08764 with our latest work", "journal-ref": null, "doi": "10.1016/j.trc.2017.11.015", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Once known to be used exclusively in military domain, unmanned aerial\nvehicles (drones) have stepped up to become a part of new logistic method in\ncommercial sector called \"last-mile delivery\". In this novel approach, small\nunmanned aerial vehicles (UAV), also known as drones, are deployed alongside\nwith trucks to deliver goods to customers in order to improve the service\nquality or reduce the transportation cost. It gives rise to a new variant of\nthe traveling salesman problem (TSP), of which we call TSP with drone (TSP-D).\nIn this article, we consider a variant of TSP-D where the main objective is to\nminimize the total transportation cost. We also propose two heuristics: \"Drone\nFirst, Truck Second\" (DFTS) and \"Truck First, Drone Second\" (TFDS), to\neffectively solve the problem. The former constructs route for drone first\nwhile the latter constructs route for truck first. We solve a TSP to generate\nroute for truck and propose a mixed integer programming (MIP) formulation with\ndifferent profit functions to build route for drone. Numerical results obtained\non many instances with different sizes and characteristics are presented.\nRecommendations on promising algorithm choices are also provided.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 18:23:41 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2015 06:21:51 GMT"}, {"version": "v3", "created": "Sun, 22 May 2016 17:06:40 GMT"}, {"version": "v4", "created": "Thu, 26 May 2016 13:14:33 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Ha", "Quang Minh", ""], ["Deville", "Yves", ""], ["Pham", "Quang Dung", ""], ["H\u00e0", "Minh Ho\u00e0ng", ""]]}, {"id": "1512.01537", "submitter": "Elliot Meyerson", "authors": "Alexander Braylan, Mark Hollenbeck, Elliot Meyerson, Risto\n  Miikkulainen", "title": "Reuse of Neural Modules for General Video Game Playing", "comments": "Accepted at AAAI 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general approach to knowledge transfer is introduced in which an agent\ncontrolled by a neural network adapts how it reuses existing networks as it\nlearns in a new domain. Networks trained for a new domain can improve their\nperformance by routing activation selectively through previously learned neural\nstructure, regardless of how or for what it was learned. A neuroevolution\nimplementation of this approach is presented with application to\nhigh-dimensional sequential decision-making domains. This approach is more\ngeneral than previous approaches to neural transfer for reinforcement learning.\nIt is domain-agnostic and requires no prior assumptions about the nature of\ntask relatedness or mappings. The method is analyzed in a stochastic version of\nthe Arcade Learning Environment, demonstrating that it improves performance in\nsome of the more complex Atari 2600 games, and that the success of transfer can\nbe predicted based on a high-level characterization of game dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 20:43:30 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Braylan", "Alexander", ""], ["Hollenbeck", "Mark", ""], ["Meyerson", "Elliot", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "1512.01587", "submitter": "Sahil Garg", "authors": "Sahil Garg, Aram Galstyan, Ulf Hermjakob, and Daniel Marcu", "title": "Extracting Biomolecular Interactions Using Semantic Parsing of\n  Biomedical Text", "comments": "Appearing in Proceedings of the Thirtieth AAAI Conference on\n  Artificial Intelligence (AAAI-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advance the state of the art in biomolecular interaction extraction with\nthree contributions: (i) We show that deep, Abstract Meaning Representations\n(AMR) significantly improve the accuracy of a biomolecular interaction\nextraction system when compared to a baseline that relies solely on surface-\nand syntax-based features; (ii) In contrast with previous approaches that infer\nrelations on a sentence-by-sentence basis, we expand our framework to enable\nconsistent predictions over sets of sentences (documents); (iii) We further\nmodify and expand a graph kernel learning framework to enable concurrent\nexploitation of automatically induced AMR (semantic) and dependency structure\n(syntactic) representations. Our experiments show that our approach yields\ninteraction extraction systems that are more robust in environments where there\nis a significant mismatch between training and test conditions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 22:58:29 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Garg", "Sahil", ""], ["Galstyan", "Aram", ""], ["Hermjakob", "Ulf", ""], ["Marcu", "Daniel", ""]]}, {"id": "1512.01613", "submitter": "Fei Gao", "authors": "Wei-Hao Mao, Fei Gao, Yi-Jin Dong, Wen-Ming Li", "title": "A Novel Paradigm for Calculating Ramsey Number via Artificial Bee Colony\n  Algorithm", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ramsey number is of vital importance in Ramsey's theorem. This paper\nproposed a novel methodology for constructing Ramsey graphs about R(3,10),\nwhich uses Artificial Bee Colony optimization(ABC) to raise the lower bound of\nRamsey number R(3,10). The r(3,10)-graph contains two limitations, that is,\nneither complete graphs of order 3 nor independent sets of order 10. To resolve\nthese limitations, a special mathematical model is put in the paradigm to\nconvert the problems into discrete optimization whose smaller minimizers are\ncorrespondent to bigger lower bound as approximation of inf R(3,10). To\ndemonstrate the potential of the proposed method, simulations are done to to\nminimize the amount of these two types of graphs. For the first time, four\nr(3,9,39) graphs with best approximation for inf R(3,10) are reported in\nsimulations to support the current lower bound for R(3,10). The experiments'\nresults show that the proposed paradigm for Ramsey number's calculation driven\nby ABC is a successful method with the advantages of high precision and\nrobustness.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 02:42:28 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 12:03:26 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Mao", "Wei-Hao", ""], ["Gao", "Fei", ""], ["Dong", "Yi-Jin", ""], ["Li", "Wen-Ming", ""]]}, {"id": "1512.01629", "submitter": "Yinlam Chow", "authors": "Yinlam Chow and Mohammad Ghavamzadeh and Lucas Janson and Marco Pavone", "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria", "comments": "arXiv admin note: substantial text overlap with arXiv:1406.3339", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many sequential decision-making problems one is interested in minimizing\nan expected cumulative cost while taking into account \\emph{risk}, i.e.,\nincreased awareness of events of small probability and high consequences.\nAccordingly, the objective of this paper is to present efficient reinforcement\nlearning algorithms for risk-constrained Markov decision processes (MDPs),\nwhere risk is represented via a chance constraint or a constraint on the\nconditional value-at-risk (CVaR) of the cumulative cost. We collectively refer\nto such problems as percentile risk-constrained MDPs.\n  Specifically, we first derive a formula for computing the gradient of the\nLagrangian function for percentile risk-constrained MDPs. Then, we devise\npolicy gradient and actor-critic algorithms that (1) estimate such gradient,\n(2) update the policy in the descent direction, and (3) update the Lagrange\nmultiplier in the ascent direction. For these algorithms we prove convergence\nto locally optimal policies. Finally, we demonstrate the effectiveness of our\nalgorithms in an optimal stopping problem and an online marketing application.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 06:39:32 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 06:37:48 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 08:07:59 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Chow", "Yinlam", ""], ["Ghavamzadeh", "Mohammad", ""], ["Janson", "Lucas", ""], ["Pavone", "Marco", ""]]}, {"id": "1512.01715", "submitter": "Hang Qi", "authors": "Hang Qi, Tianfu Wu, Mun-Wai Lee, Song-Chun Zhu", "title": "A Restricted Visual Turing Test for Deep Scene and Event Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a restricted visual Turing test (VTT) for story-line\nbased deep understanding in long-term and multi-camera captured videos. Given a\nset of videos of a scene (such as a multi-room office, a garden, and a parking\nlot.) and a sequence of story-line based queries, the task is to provide\nanswers either simply in binary form \"true/false\" (to a polar query) or in an\naccurate natural language description (to a non-polar query). Queries, polar or\nnon-polar, consist of view-based queries which can be answered from a\nparticular camera view and scene-centered queries which involves joint\ninference across different cameras. The story lines are collected to cover\nspatial, temporal and causal understanding of input videos. The data and\nqueries distinguish our VTT from recently proposed visual question answering in\nimages and video captioning. A vision system is proposed to perform joint video\nand query parsing which integrates different vision modules, a knowledge base\nand a query engine. The system provides unified interfaces for different\nmodules so that individual modules can be reconfigured to test a new method. We\nprovide a benchmark dataset and a toolkit for ontology guided story-line query\ngeneration which consists of about 93.5 hours videos captured in four different\nlocations and 3,426 queries split into 127 story lines. We also provide a\nbaseline implementation and result analyses.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 00:40:02 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 19:19:25 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Qi", "Hang", ""], ["Wu", "Tianfu", ""], ["Lee", "Mun-Wai", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1512.01752", "submitter": "Sujith Ravi", "authors": "Sujith Ravi, Qiming Diao", "title": "Large Scale Distributed Semi-Supervised Learning Using Streaming\n  Approximation", "comments": "10 pages", "journal-ref": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics (AISTATS), JMLR: W&CP volume 51, pp. 519-528,\n  2016", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional graph-based semi-supervised learning (SSL) approaches, even\nthough widely applied, are not suited for massive data and large label\nscenarios since they scale linearly with the number of edges $|E|$ and distinct\nlabels $m$. To deal with the large label size problem, recent works propose\nsketch-based methods to approximate the distribution on labels per node thereby\nachieving a space reduction from $O(m)$ to $O(\\log m)$, under certain\nconditions. In this paper, we present a novel streaming graph-based SSL\napproximation that captures the sparsity of the label distribution and ensures\nthe algorithm propagates labels accurately, and further reduces the space\ncomplexity per node to $O(1)$. We also provide a distributed version of the\nalgorithm that scales well to large data sizes. Experiments on real-world\ndatasets demonstrate that the new method achieves better performance than\nexisting state-of-the-art algorithms with significant reduction in memory\nfootprint. We also study different graph construction mechanisms for natural\nlanguage applications and propose a robust graph augmentation strategy trained\nusing state-of-the-art unsupervised deep learning architectures that yields\nfurther significant quality gains.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 06:58:57 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 19:40:37 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Ravi", "Sujith", ""], ["Diao", "Qiming", ""]]}, {"id": "1512.01764", "submitter": "Piotr Szczepa\\'nski", "authors": "Piotr Lech Szczepa\\'nski", "title": "Fast Algorithms for Game-Theoretic Centrality Measures", "comments": "Doctoral Dissertation at Warsaw University of Technology, Faculty of\n  Electronics and Information Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this dissertation, we analyze the computational properties of\ngame-theoretic centrality measures. The key idea behind game-theoretic approach\nto network analysis is to treat nodes as players in a cooperative game, where\nthe value of each coalition of nodes is determined by certain graph properties.\nNext, the centrality of any individual node is determined by a chosen\ngame-theoretic solution concept (notably, the Shapley value) in the same way as\nthe payoff of a player in a cooperative game. On one hand, the advantage of\ngame-theoretic centrality measures is that nodes are ranked not only according\nto their individual roles but also according to how they contribute to the role\nplayed by all possible subsets of nodes. On the other hand, the disadvantage is\nthat the game-theoretic solution concepts are typically computationally\nchallenging. The main contribution of this dissertation is that we show that a\nwide variety of game-theoretic solution concepts on networks can be computed in\npolynomial time. Our focus is on centralities based on the Shapley value and\nits various extensions, such as the Semivalues and Coalitional Semivalues.\nFurthermore, we prove #P-hardness of computing the Shapley value in\nconnectivity games and propose an algorithm to compute it. Finally, we analyse\ncomputational properties of generalized version of cooperative games in which\norder of player matters. We propose a new representation for such games, called\ngeneralized marginal contribution networks, that allows for polynomial\ncomputation in the size of the representation of two dedicated extensions of\nthe Shapley value to this class of games.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 09:56:48 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Szczepa\u0144ski", "Piotr Lech", ""]]}, {"id": "1512.01872", "submitter": "Pranav Rajpurkar", "authors": "Pranav Rajpurkar, Toki Migimatsu, Jeff Kiske, Royce Cheng-Yue, Sameep\n  Tandon, Tao Wang, Andrew Ng", "title": "Driverseat: Crowdstrapping Learning Tasks for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While emerging deep-learning systems have outclassed knowledge-based\napproaches in many tasks, their application to detection tasks for autonomous\ntechnologies remains an open field for scientific exploration. Broadly, there\nare two major developmental bottlenecks: the unavailability of comprehensively\nlabeled datasets and of expressive evaluation strategies. Approaches for\nlabeling datasets have relied on intensive hand-engineering, and strategies for\nevaluating learning systems have been unable to identify failure-case\nscenarios. Human intelligence offers an untapped approach for breaking through\nthese bottlenecks. This paper introduces Driverseat, a technology for embedding\ncrowds around learning systems for autonomous driving. Driverseat utilizes\ncrowd contributions for (a) collecting complex 3D labels and (b) tagging\ndiverse scenarios for ready evaluation of learning systems. We demonstrate how\nDriverseat can crowdstrap a convolutional neural network on the lane-detection\ntask. More generally, crowdstrapping introduces a valuable paradigm for any\ntechnology that can benefit from leveraging the powerful combination of human\nand computer intelligence.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 01:34:23 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Rajpurkar", "Pranav", ""], ["Migimatsu", "Toki", ""], ["Kiske", "Jeff", ""], ["Cheng-Yue", "Royce", ""], ["Tandon", "Sameep", ""], ["Wang", "Tao", ""], ["Ng", "Andrew", ""]]}, {"id": "1512.01885", "submitter": "Ardavan Salehi Nobandegani", "authors": "Ardavan Salehi Nobandegani, Ioannis N. Psaromiligkos", "title": "Probabilistic Structural Controllability in Causal Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans routinely confront the following key question which could be viewed as\na probabilistic variant of the controllability problem: While faced with an\nuncertain environment governed by causal structures, how should they practice\ntheir autonomy by intervening on driver variables, in order to increase (or\ndecrease) the probability of attaining their desired (or undesired) state for\nsome target variable? In this paper, for the first time, the problem of\nprobabilistic controllability in Causal Bayesian Networks (CBNs) is studied.\nMore specifically, the aim of this paper is two-fold: (i) to introduce and\nformalize the problem of probabilistic structural controllability in CBNs, and\n(ii) to identify a sufficient set of driver variables for the purpose of\nprobabilistic structural controllability of a generic CBN. We also elaborate on\nthe nature of minimality the identified set of driver variables satisfies. In\nthis context, the term \"structural\" signifies the condition wherein solely the\nstructure of the CBN is known.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 02:23:49 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Nobandegani", "Ardavan Salehi", ""], ["Psaromiligkos", "Ioannis N.", ""]]}, {"id": "1512.01915", "submitter": "Guifei Jiang", "authors": "Guifei Jiang and Dongmo Zhang and Laurent Perrussel", "title": "Knowledge Sharing in Coalitions", "comments": "This version corrected errors in its previous version published at\n  AI'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to investigate the interplay between knowledge\nshared by a group of agents and its coalition ability. We investigate this\nrelation in the standard context of imperfect information concurrent game. We\nassume that whenever a set of agents form a coalition to achieve a goal, they\nshare their knowledge before acting. Based on this assumption, we propose a new\nsemantics for alternating-time temporal logic with imperfect information and\nperfect recall. It turns out that this semantics is sufficient to preserve all\nthe desirable properties of coalition ability in traditional coalitional\nlogics. Meanwhile, we investigate how knowledge sharing within a group of\nagents contributes to its coalitional ability through the interplay of\nepistemic and coalition modalities. This work provides a partial answer to the\nquestion: which kind of group knowledge is required for a group to achieve\ntheir goals in the context of imperfect information.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 05:27:07 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 09:00:32 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Jiang", "Guifei", ""], ["Zhang", "Dongmo", ""], ["Perrussel", "Laurent", ""]]}, {"id": "1512.01926", "submitter": "Kamil Rocki", "authors": "Kamil Rocki", "title": "Thinking Required", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a theory of a single general-purpose learning algorithm which\ncould explain the principles its operation. It assumes the initial rough\narchitecture, a small library of simple innate circuits which are prewired at\nbirth. and proposes that all significant mental algorithms are learned. Given\ncurrent understanding and observations, this paper reviews and lists the\ningredients of such an algorithm from architectural and functional\nperspectives.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 06:37:49 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Rocki", "Kamil", ""]]}, {"id": "1512.02011", "submitter": "Vincent Francois-Lavet", "authors": "Vincent Fran\\c{c}ois-Lavet, Raphael Fonteneau, Damien Ernst", "title": "How to Discount Deep Reinforcement Learning: Towards New Dynamic\n  Strategies", "comments": "NIPS 2015 Deep Reinforcement Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using deep neural nets as function approximator for reinforcement learning\ntasks have recently been shown to be very powerful for solving problems\napproaching real-world complexity. Using these results as a benchmark, we\ndiscuss the role that the discount factor may play in the quality of the\nlearning process of a deep Q-network (DQN). When the discount factor\nprogressively increases up to its final value, we empirically show that it is\npossible to significantly reduce the number of learning steps. When used in\nconjunction with a varying learning rate, we empirically show that it\noutperforms original DQN on several experiments. We relate this phenomenon with\nthe instabilities of neural networks when they are used in an approximate\nDynamic Programming setting. We also describe the possibility to fall within a\nlocal optimum during the learning process, thus connecting our discussion with\nthe exploration/exploitation dilemma.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 12:25:18 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 10:33:00 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Fran\u00e7ois-Lavet", "Vincent", ""], ["Fonteneau", "Raphael", ""], ["Ernst", "Damien", ""]]}, {"id": "1512.02078", "submitter": "Yanjing Wang", "authors": "Kai Li and Yanjing Wang", "title": "From rules to runs: A dynamic epistemic take on imperfect information\n  games", "comments": "draft of a paper accepted by Studies in Logic (published by Sun\n  Yat-Sen University)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the literature of game theory, the information sets of extensive form\ngames have different interpretations, which may lead to confusions and\nparadoxical cases. We argue that the problem lies in the mix-up of two\ninterpretations of the extensive form game structures: game rules or game runs\nwhich do not always coincide. In this paper, we try to separate and connect\nthese two views by proposing a dynamic epistemic framework in which we can\ncompute the runs step by step from the game rules plus the given assumptions of\nthe players. We propose a modal logic to describe players' knowledge and its\nchange during the plays, and provide a complete axiomatization. We also show\nthat, under certain conditions, the mix-up of the rules and the runs is not\nharmful due to the structural similarity of the two.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 15:01:18 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Li", "Kai", ""], ["Wang", "Yanjing", ""]]}, {"id": "1512.02140", "submitter": "A. Mani", "authors": "A Mani", "title": "Contamination-Free Measures and Algebraic Operations", "comments": "Preprint of FUZZIEEE'2013 Conference Paper", "journal-ref": "IEEE Xplore, 2013", "doi": "10.1109/FUZZ-IEEE.2013.6622521", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An open concept of rough evolution and an axiomatic approach to granules was\nalso developed recently by the present author. Subsequently the concepts were\nused in the formal framework of rough Y-systems (RYS) for developing on\ngranular correspondences by her. These have since been used for a new approach\ntowards comparison of rough algebraic semantics across different semantic\ndomains by way of correspondences that preserve rough evolution and try to\navoid contamination. In this research paper, new methods are proposed and a\nsemantics for handling possibly contaminated operations and structured bigness\nis developed. These would also be of natural interest for relative consistency\nof one collection of knowledge relative other.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 16:50:05 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Mani", "A", ""]]}, {"id": "1512.02266", "submitter": "Manuele Leonelli", "authors": "Manuele Leonelli, Christiane G\\\"orgen and Jim Q. Smith", "title": "Sensitivity analysis, multilinearity and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitivity methods for the analysis of the outputs of discrete Bayesian\nnetworks have been extensively studied and implemented in different software\npackages. These methods usually focus on the study of sensitivity functions and\non the impact of a parameter change to the Chan-Darwiche distance. Although not\nfully recognized, the majority of these results heavily rely on the multilinear\nstructure of atomic probabilities in terms of the conditional probability\nparameters associated with this type of network. By defining a statistical\nmodel through the polynomial expression of its associated defining conditional\nprobabilities, we develop a unifying approach to sensitivity methods applicable\nto a large suite of models including extensions of Bayesian networks, for\ninstance context-specific and dynamic ones, and chain event graphs. By then\nfocusing on models whose defining polynomial is multilinear, our algebraic\napproach enables us to prove that the Chan-Darwiche distance is minimized for a\ncertain class of multi-parameter contemporaneous variations when parameters are\nproportionally covaried.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 22:24:31 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 15:39:27 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Leonelli", "Manuele", ""], ["G\u00f6rgen", "Christiane", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1512.02406", "submitter": "Yi-Chun Chen", "authors": "Yi-Chun Chen, Tim Allan Wheeler, Mykel John Kochenderfer", "title": "Learning Discrete Bayesian Networks from Continuous Data", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research 59 (2017) 103-132", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning Bayesian networks from raw data can help provide insights into the\nrelationships between variables. While real data often contains a mixture of\ndiscrete and continuous-valued variables, many Bayesian network structure\nlearning algorithms assume all random variables are discrete. Thus, continuous\nvariables are often discretized when learning a Bayesian network. However, the\nchoice of discretization policy has significant impact on the accuracy, speed,\nand interpretability of the resulting models. This paper introduces a\nprincipled Bayesian discretization method for continuous variables in Bayesian\nnetworks with quadratic complexity instead of the cubic complexity of other\nstandard techniques. Empirical demonstrations show that the proposed method is\nsuperior to the established minimum description length algorithm. In addition,\nthis paper shows how to incorporate existing methods into the structure\nlearning process to discretize all continuous variables and simultaneously\nlearn Bayesian network structures.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 11:12:04 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 08:00:55 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 02:44:57 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chen", "Yi-Chun", ""], ["Wheeler", "Tim Allan", ""], ["Kochenderfer", "Mykel John", ""]]}, {"id": "1512.02752", "submitter": "Qi Mao", "authors": "Qi Mao, Li Wang, Ivor W. Tsang, Yijun Sun", "title": "A Novel Regularized Principal Graph Learning Framework on Explicit Graph\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific datasets are of high dimension, and the analysis usually\nrequires visual manipulation by retaining the most important structures of\ndata. Principal curve is a widely used approach for this purpose. However, many\nexisting methods work only for data with structures that are not\nself-intersected, which is quite restrictive for real applications. A few\nmethods can overcome the above problem, but they either require complicated\nhuman-made rules for a specific task with lack of convergence guarantee and\nadaption flexibility to different tasks, or cannot obtain explicit structures\nof data. To address these issues, we develop a new regularized principal graph\nlearning framework that captures the local information of the underlying graph\nstructure based on reversed graph embedding. As showcases, models that can\nlearn a spanning tree or a weighted undirected $\\ell_1$ graph are proposed, and\na new learning algorithm is developed that learns a set of principal points and\na graph structure from data, simultaneously. The new algorithm is simple with\nguaranteed convergence. We then extend the proposed framework to deal with\nlarge-scale data. Experimental results on various synthetic and six real world\ndatasets show that the proposed method compares favorably with baselines and\ncan uncover the underlying structure correctly.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 04:57:18 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2016 14:34:14 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Mao", "Qi", ""], ["Wang", "Li", ""], ["Tsang", "Ivor W.", ""], ["Sun", "Yijun", ""]]}, {"id": "1512.03012", "submitter": "Manolis Savva", "authors": "Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan,\n  Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su,\n  Jianxiong Xiao, Li Yi, and Fisher Yu", "title": "ShapeNet: An Information-Rich 3D Model Repository", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.CG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ShapeNet: a richly-annotated, large-scale repository of shapes\nrepresented by 3D CAD models of objects. ShapeNet contains 3D models from a\nmultitude of semantic categories and organizes them under the WordNet taxonomy.\nIt is a collection of datasets providing many semantic annotations for each 3D\nmodel such as consistent rigid alignments, parts and bilateral symmetry planes,\nphysical sizes, keywords, as well as other planned annotations. Annotations are\nmade available through a public web-based interface to enable data\nvisualization of object attributes, promote data-driven geometric analysis, and\nprovide a large-scale quantitative benchmark for research in computer graphics\nand vision. At the time of this technical report, ShapeNet has indexed more\nthan 3,000,000 models, 220,000 models out of which are classified into 3,135\ncategories (WordNet synsets). In this report we describe the ShapeNet effort as\na whole, provide details for all currently available datasets, and summarize\nfuture plans.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 19:42:48 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Chang", "Angel X.", ""], ["Funkhouser", "Thomas", ""], ["Guibas", "Leonidas", ""], ["Hanrahan", "Pat", ""], ["Huang", "Qixing", ""], ["Li", "Zimo", ""], ["Savarese", "Silvio", ""], ["Savva", "Manolis", ""], ["Song", "Shuran", ""], ["Su", "Hao", ""], ["Xiao", "Jianxiong", ""], ["Yi", "Li", ""], ["Yu", "Fisher", ""]]}, {"id": "1512.03020", "submitter": "Hamidreza Chinaei", "authors": "Hamidreza Chinaei, Mohsen Rais-Ghasem, Frank Rudzicz", "title": "Learning measures of semi-additive behaviour", "comments": "7 pages, 11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In business analytics, measure values, such as sales numbers or volumes of\ncargo transported, are often summed along values of one or more corresponding\ncategories, such as time or shipping container. However, not every measure\nshould be added by default (e.g., one might more typically want a mean over the\nheights of a set of people); similarly, some measures should only be summed\nwithin certain constraints (e.g., population measures need not be summed over\nyears). In systems such as Watson Analytics, the exact additive behaviour of a\nmeasure is often determined by a human expert. In this work, we propose a small\nset of features for this issue. We use these features in a case-based reasoning\napproach, where the system suggests an aggregation behaviour, with 86% accuracy\nin our collected dataset.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 19:52:55 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Chinaei", "Hamidreza", ""], ["Rais-Ghasem", "Mohsen", ""], ["Rudzicz", "Frank", ""]]}, {"id": "1512.03375", "submitter": "Peter H Jin", "authors": "Peter H. Jin and Kurt Keutzer", "title": "Convolutional Monte Carlo Rollouts in Go", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a MCTS-based Go-playing program which uses\nconvolutional networks in all parts. Our method performs MCTS in batches,\nexplores the Monte Carlo search tree using Thompson sampling and a\nconvolutional network, and evaluates convnet-based rollouts on the GPU. We\nachieve strong win rates against open source Go programs and attain competitive\nresults against state of the art convolutional net-based Go-playing programs.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 19:32:48 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Jin", "Peter H.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1512.03516", "submitter": "Madan Rao Mohan", "authors": "A.M. Mohan Rao", "title": "Subsumptive reflection in SNOMED CT: a large description logic-based\n  terminology for diagnosis", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Description logic (DL) based biomedical terminology (SNOMED CT) is used\nroutinely in medical practice. However, diagnostic inference using such\nterminology is precluded by its complexity. Here we propose a model that\nsimplifies these inferential components. We propose three concepts that\nclassify clinical features and examined their effect on inference using SNOMED\nCT. We used PAIRS (Physician Assistant Artificial Intelligence Reference\nSystem) database (1964 findings for 485 disorders, 18 397 disease feature\nlinks) for our analysis. We also use a 50-million medical word corpus for\nestimating the vectors of disease-feature links. Our major results are 10% of\nfinding-disorder links are concomitant in both assertion and negation where as\n90% are either concomitant in assertion or negation. Logical implications of\nPAIRS data on SNOMED CT include 70% of the links do not share any common system\nwhile 18% share organ and 12% share both system and organ. Applications of\nthese principles for inference are discussed and suggestions are made for\nderiving a diagnostic process using SNOMED CT. Limitations of these processes\nand suggestions for improvements are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 04:27:50 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Rao", "A. M. Mohan", ""]]}, {"id": "1512.03899", "submitter": "Mathew Joseph", "authors": "Mathew Joseph, Gabriel Kuper, Till Mossakowski, Luciano Serafini", "title": "Query Answering over Contextualized RDF/OWL Knowledge with\n  Forall-Existential Bridge Rules: Decidable Finite Extension Classes (Post\n  Print)", "comments": null, "journal-ref": "Semantic Web (IOS Press) Vol 7:1 Pages 25-61. 2016", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of contextualized knowledge in the Semantic Web (SW) has\nled to the popularity of knowledge formats such as \\emph{quads} in the SW\ncommunity. A quad is an extension of an RDF triple with contextual information\nof the triple. In this paper, we study the problem of query answering over\nquads augmented with forall-existential bridge rules that enable\ninteroperability of reasoning between triples in various contexts. We call a\nset of quads together with such expressive bridge rules, a quad-system. Query\nanswering over quad-systems is undecidable, in general. We derive decidable\nclasses of quad-systems, for which query answering can be done using forward\nchaining. Sound, complete and terminating procedures, which are adaptations of\nthe well known chase algorithm, are provided for these classes for deciding\nquery entailment. Safe, msafe, and csafe class of quad-systems restrict the\nstructure of blank nodes generated during the chase computation process to be\ndirected acyclic graphs (DAGs) of bounded depth. RR and restricted RR classes\ndo not allow the generation of blank nodes during the chase computation\nprocess. Both data and combined complexity of query entailment has been\nestablished for the classes derived. We further show that quad-systems are\nequivalent to forall-existential rules whose predicates are restricted to\nternary arity, modulo polynomial time translations. We subsequently show that\nthe technique of safety, strictly subsumes in expressivity, some of the well\nknown and expressive techniques, such as joint acyclicity and model faithful\nacyclicity, used for decidability guarantees in the realm of forall-existential\nrules.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 09:56:38 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Joseph", "Mathew", ""], ["Kuper", "Gabriel", ""], ["Mossakowski", "Till", ""], ["Serafini", "Luciano", ""]]}, {"id": "1512.04021", "submitter": "Guido Governatori", "authors": "Guido Governatori, Francesco Olivieri, Simone Scannapieco, Antonino\n  Rotolo and Matteo Cristani", "title": "The Rationale behind the Concept of Goal", "comments": null, "journal-ref": "Theory and Practice of Logic Programming 16 (2016) 296-324", "doi": "10.1017/S1471068416000053", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a fresh look at the concept of goal and advances that\nmotivational attitudes like desire, goal and intention are just facets of the\nbroader notion of (acceptable) outcome. We propose to encode the preferences of\nan agent as sequences of \"alternative acceptable outcomes\". We then study how\nthe agent's beliefs and norms can be used to filter the mental attitudes out of\nthe sequences of alternative acceptable outcomes. Finally, we formalise such\nintuitions in a novel Modal Defeasible Logic and we prove that the resulting\nformalisation is computationally feasible.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 09:19:27 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Governatori", "Guido", ""], ["Olivieri", "Francesco", ""], ["Scannapieco", "Simone", ""], ["Rotolo", "Antonino", ""], ["Cristani", "Matteo", ""]]}, {"id": "1512.04087", "submitter": "Harm van Seijen", "authors": "Harm van Seijen and A. Rupam Mahmood and Patrick M. Pilarski and\n  Marlos C. Machado and Richard S. Sutton", "title": "True Online Temporal-Difference Learning", "comments": "This is the published JMLR version. It is a much improved version.\n  The main changes are: 1) re-structuring of the article; 2) additional\n  analysis on the forward view; 3) empirical comparison of traditional and new\n  forward view; 4) added discussion of other true online papers; 5) updated\n  discussion for non-linear function approximation", "journal-ref": "Journal of Machine Learning Research (JMLR), 17(145):1-40, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The temporal-difference methods TD($\\lambda$) and Sarsa($\\lambda$) form a\ncore part of modern reinforcement learning. Their appeal comes from their good\nperformance, low computational cost, and their simple interpretation, given by\ntheir forward view. Recently, new versions of these methods were introduced,\ncalled true online TD($\\lambda$) and true online Sarsa($\\lambda$), respectively\n(van Seijen & Sutton, 2014). These new versions maintain an exact equivalence\nwith the forward view at all times, whereas the traditional versions only\napproximate it for small step-sizes. We hypothesize that these true online\nmethods not only have better theoretical properties, but also dominate the\nregular methods empirically. In this article, we put this hypothesis to the\ntest by performing an extensive empirical comparison. Specifically, we compare\nthe performance of true online TD($\\lambda$)/Sarsa($\\lambda$) with regular\nTD($\\lambda$)/Sarsa($\\lambda$) on random MRPs, a real-world myoelectric\nprosthetic arm, and a domain from the Arcade Learning Environment. We use\nlinear function approximation with tabular, binary, and non-binary features.\nOur results suggest that the true online methods indeed dominate the regular\nmethods. Across all domains/representations the learning speed of the true\nonline methods are often better, but never worse than that of the regular\nmethods. An additional advantage is that no choice between traces has to be\nmade for the true online methods. Besides the empirical results, we provide an\nin-depth analysis of the theory behind true online temporal-difference\nlearning. In addition, we show that new true online temporal-difference methods\ncan be derived by making changes to the online forward view and then rewriting\nthe update equations.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 17:13:33 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 18:56:23 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["van Seijen", "Harm", ""], ["Mahmood", "A. Rupam", ""], ["Pilarski", "Patrick M.", ""], ["Machado", "Marlos C.", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1512.04097", "submitter": "Cristian Molinaro", "authors": "Marco Calautti, Sergio Greco, Cristian Molinaro, Irina Trubitsyna", "title": "Using Linear Constraints for Logic Program Termination Analysis", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": "Theory and Practice of Logic Programming 16 (2016) 353-377", "doi": "10.1017/S1471068416000077", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely acknowledged that function symbols are an important feature in\nanswer set programming, as they make modeling easier, increase the expressive\npower, and allow us to deal with infinite domains. The main issue with their\nintroduction is that the evaluation of a program might not terminate and\nchecking whether it terminates or not is undecidable. To cope with this\nproblem, several classes of logic programs have been proposed where the use of\nfunction symbols is restricted but the program evaluation termination is\nguaranteed. Despite the significant body of work in this area, current\napproaches do not include many simple practical programs whose evaluation\nterminates. In this paper, we present the novel classes of rule-bounded and\ncycle-bounded programs, which overcome different limitations of current\napproaches by performing a more global analysis of how terms are propagated\nfrom the body to the head of rules. Results on the correctness, the complexity,\nand the expressivity of the proposed approach are provided.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 18:36:54 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 13:15:04 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Calautti", "Marco", ""], ["Greco", "Sergio", ""], ["Molinaro", "Cristian", ""], ["Trubitsyna", "Irina", ""]]}, {"id": "1512.04105", "submitter": "Lucas Lehnert", "authors": "Lucas Lehnert and Doina Precup", "title": "Policy Gradient Methods for Off-policy Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy learning refers to the problem of learning the value function of a\nway of behaving, or policy, while following a different policy. Gradient-based\noff-policy learning algorithms, such as GTD and TDC/GQ, converge even when\nusing function approximation and incremental updates. However, they have been\ndeveloped for the case of a fixed behavior policy. In control problems, one\nwould like to adapt the behavior policy over time to become more greedy with\nrespect to the existing value function. In this paper, we present the first\ngradient-based learning algorithms for this problem, which rely on the\nframework of policy gradient in order to modify the behavior policy. We present\nderivations of the algorithms, a convergence theorem, and empirical evidence\nshowing that they compare favorably to existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 19:20:14 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Lehnert", "Lucas", ""], ["Precup", "Doina", ""]]}, {"id": "1512.04114", "submitter": "Emiliano De Cristofaro", "authors": "Luca Melis and Apostolos Pyrgelis and Emiliano De Cristofaro", "title": "Building and Measuring Privacy-Preserving Predictive Blacklists", "comments": "Obsolete paper. For more up-to-date work on collaborative predictive\n  blacklisting, see arXiv:1810.02649", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (Withdrawn) Collaborative security initiatives are increasingly often\nadvocated to improve timeliness and effectiveness of threat mitigation. Among\nthese, collaborative predictive blacklisting (CPB) aims to forecast attack\nsources based on alerts contributed by multiple organizations that might be\ntargeted in similar ways. Alas, CPB proposals thus far have only focused on\nimproving hit counts, but overlooked the impact of collaboration on false\npositives and false negatives. Moreover, sharing threat intelligence often\nprompts important privacy, confidentiality, and liability issues. In this\npaper, we first provide a comprehensive measurement analysis of two\nstate-of-the-art CPB systems: one that uses a trusted central party to collect\nalerts [Soldo et al., Infocom'10] and a peer-to-peer one relying on controlled\ndata sharing [Freudiger et al., DIMVA'15], studying the impact of collaboration\non both correct and incorrect predictions. Then, we present a novel\nprivacy-friendly approach that significantly improves over previous work,\nachieving a better balance of true and false positive rates, while minimizing\ninformation disclosure. Finally, we present an extension that allows our system\nto scale to very large numbers of organizations.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 20:05:53 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 08:45:45 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 08:59:38 GMT"}, {"version": "v4", "created": "Wed, 1 Mar 2017 16:08:02 GMT"}, {"version": "v5", "created": "Mon, 8 Oct 2018 01:57:55 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Melis", "Luca", ""], ["Pyrgelis", "Apostolos", ""], ["De Cristofaro", "Emiliano", ""]]}, {"id": "1512.04134", "submitter": "Qifei Wang", "authors": "Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy", "title": "Evaluation of Pose Tracking Accuracy in the First and Second Generations\n  of Microsoft Kinect", "comments": "10 pages, IEEE International Conference on Healthcare Informatics\n  2015 (ICHI 2015)", "journal-ref": null, "doi": "10.1109/ICHI.2015.54", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microsoft Kinect camera and its skeletal tracking capabilities have been\nembraced by many researchers and commercial developers in various applications\nof real-time human movement analysis. In this paper, we evaluate the accuracy\nof the human kinematic motion data in the first and second generation of the\nKinect system, and compare the results with an optical motion capture system.\nWe collected motion data in 12 exercises for 10 different subjects and from\nthree different viewpoints. We report on the accuracy of the joint localization\nand bone length estimation of Kinect skeletons in comparison to the motion\ncapture. We also analyze the distribution of the joint localization offsets by\nfitting a mixture of Gaussian and uniform distribution models to determine the\noutliers in the Kinect motion data. Our analysis shows that overall Kinect 2\nhas more robust and more accurate tracking of human pose as compared to Kinect\n1.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 22:58:55 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Wang", "Qifei", ""], ["Kurillo", "Gregorij", ""], ["Ofli", "Ferda", ""], ["Bajcsy", "Ruzena", ""]]}, {"id": "1512.04295", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Luca Benini", "title": "Origami: A 803 GOp/s/W Convolutional Network Accelerator", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2592330", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ever increasing number of computer vision and image/video processing\nchallenges are being approached using deep convolutional neural networks,\nobtaining state-of-the-art results in object recognition and detection,\nsemantic segmentation, action recognition, optical flow and superresolution.\nHardware acceleration of these algorithms is essential to adopt these\nimprovements in embedded and mobile computer vision systems. We present a new\narchitecture, design and implementation as well as the first reported silicon\nmeasurements of such an accelerator, outperforming previous work in terms of\npower-, area- and I/O-efficiency. The manufactured device provides up to 196\nGOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power\nefficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it\nthe first architecture scalable to TOp/s performance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 13:06:43 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 22:56:41 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "1512.04358", "submitter": "Theodore Patkos", "authors": "Theodore Patkos, Dimitris Plexousakis, Abdelghani Chibani, Yacine\n  Amirat", "title": "An Event Calculus Production Rule System for Reasoning in Dynamic and\n  Uncertain Domains", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": "Theory and Practice of Logic Programming 16 (2016) 325-352", "doi": "10.1017/S1471068416000065", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action languages have emerged as an important field of Knowledge\nRepresentation for reasoning about change and causality in dynamic domains.\nThis article presents Cerbere, a production system designed to perform online\ncausal, temporal and epistemic reasoning based on the Event Calculus. The\nframework implements the declarative semantics of the underlying logic theories\nin a forward-chaining rule-based reasoning system, coupling the high\nexpressiveness of its formalisms with the efficiency of rule-based systems. To\nillustrate its applicability, we present both the modeling of benchmark\nproblems in the field, as well as its utilization in the challenging domain of\nsmart spaces. A hybrid framework that combines logic-based with probabilistic\nreasoning has been developed, that aims to accommodate activity recognition and\nmonitoring tasks in smart spaces. Under consideration in Theory and Practice of\nLogic Programming (TPLP)\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 15:18:58 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 17:57:42 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Patkos", "Theodore", ""], ["Plexousakis", "Dimitris", ""], ["Chibani", "Abdelghani", ""], ["Amirat", "Yacine", ""]]}, {"id": "1512.04387", "submitter": "Yura Perov N", "authors": "Yura N Perov, Tuan Anh Le, Frank Wood", "title": "Data-driven Sequential Monte Carlo in Probabilistic Programming", "comments": "Black Box Learning and Inference, NIPS 2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of Markov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC)\nalgorithms in existing probabilistic programming systems suboptimally use only\nmodel priors as proposal distributions. In this work, we describe an approach\nfor training a discriminative model, namely a neural network, in order to\napproximate the optimal proposal by using posterior estimates from previous\nruns of inference. We show an example that incorporates a data-driven proposal\nfor use in a non-parametric model in the Anglican probabilistic programming\nsystem. Our results show that data-driven proposals can significantly improve\ninference performance so that considerably fewer particles are necessary to\nperform a good posterior estimation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 16:18:32 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 19:34:52 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Perov", "Yura N", ""], ["Le", "Tuan Anh", ""], ["Wood", "Frank", ""]]}, {"id": "1512.04419", "submitter": "Mehrnoosh Sadrzadeh", "authors": "Esma Balkir, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh", "title": "Sentence Entailment in Compositional Distributional Semantics", "comments": "8 pages, 1 figure, 2 tables, short version presented in the\n  International Symposium on Artificial Intelligence and Mathematics (ISAIM),\n  2016", "journal-ref": "Ann Math Artif Intell (2018) 82: 189.\n  https://doi.org/10.1007/s10472-017-9570-x", "doi": "10.1007/s10472-017-9570-x", "report-no": null, "categories": "cs.CL cs.AI math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional semantic models provide vector representations for words by\ngathering co-occurrence frequencies from corpora of text. Compositional\ndistributional models extend these from words to phrases and sentences. In\ncategorical compositional distributional semantics, phrase and sentence\nrepresentations are functions of their grammatical structure and\nrepresentations of the words therein. In this setting, grammatical structures\nare formalised by morphisms of a compact closed category and meanings of words\nare formalised by objects of the same category. These can be instantiated in\nthe form of vectors or density matrices. This paper concerns the applications\nof this model to phrase and sentence level entailment. We argue that\nentropy-based distances of vectors and density matrices provide a good\ncandidate to measure word-level entailment, show the advantage of density\nmatrices over vectors for word level entailments, and prove that these\ndistances extend compositionally from words to phrases and sentences. We\nexemplify our theoretical constructions on real data and a toy entailment\ndataset and provide preliminary experimental evidence.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 17:36:35 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 10:49:35 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Balkir", "Esma", ""], ["Kartsaklis", "Dimitri", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "1512.04467", "submitter": "Jeremie Guiochet", "authors": "J\\'er\\'emie Guiochet (LAAS-TSF), Quynh Anh Do Hoang (LAAS-TSF),\n  Mohamed Kaaniche (LAAS-TSF)", "title": "A Model for Safety Case Confidence Assessment", "comments": null, "journal-ref": "34th International Conference on Computer Safety, Reliability and\n  Security, Sep 2015, Delft, Netherlands. Springer, Lecture Notes in Computer\n  Science, Vol. 9337, Programming and Software Engineering, Springer, 2015,\n  http://safecomp2015.tudelft.nl/", "doi": "10.1007/978-3-319-24255-2_23", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a safety case is a common approach to make expert judgement explicit\nabout safety of a system. The issue of confidence in such argumentation is\nstill an open research field. Providing quantitative estimation of confidence\nis an interesting approach to manage complexity of arguments. This paper\nexplores the main current approaches, and proposes a new model for quantitative\nconfidence estimation based on Belief Theory for its definition, and on\nBayesian Belief Networks for its propagation in safety case networks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 15:24:22 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Guiochet", "J\u00e9r\u00e9mie", "", "LAAS-TSF"], ["Hoang", "Quynh Anh Do", "", "LAAS-TSF"], ["Kaaniche", "Mohamed", "", "LAAS-TSF"]]}, {"id": "1512.04652", "submitter": "Mitra Montazeri", "authors": "Mitra Montazeri, Mahdieh Soleymani Baghshah, Ahmad Enhesari", "title": "Hyper-Heuristic Algorithm for Finding Efficient Features in Diagnose of\n  Lung Cancer Disease", "comments": "Published in the Journal of Basic and Applied Scientific Research,\n  2013", "journal-ref": "J. Basic Appl. Sci. Res, 2013. 3(10): p. 134-140", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Lung cancer was known as primary cancers and the survival rate of\ncancer is about 15%. Early detection of lung cancer is the leading factor in\nsurvival rate. All symptoms (features) of lung cancer do not appear until the\ncancer spreads to other areas. It needs an accurate early detection of lung\ncancer, for increasing the survival rate. For accurate detection, it need\ncharacterizes efficient features and delete redundancy features among all\nfeatures. Feature selection is the problem of selecting informative features\namong all features. Materials and Methods: Lung cancer database consist of 32\npatient records with 57 features. This database collected by Hong and Youngand\nindexed in the University of California Irvine repository. Experimental\ncontents include the extracted from the clinical data and X-ray data, etc. The\ndata described 3 types of pathological lung cancers and all features are taking\nan integer value 0-3. In our study, new method is proposed for identify\nefficient features of lung cancer. It is based on Hyper-Heuristic. Results: We\nobtained an accuracy of 80.63% using reduced 11 feature set. The proposed\nmethod compare to the accuracy of 5 machine learning feature selections. The\naccuracy of these 5 methods are 60.94, 57.81, 68.75, 60.94 and 68.75.\nConclusions: The proposed method has better performance with the highest level\nof accuracy. Therefore, the proposed model is recommended for identifying an\nefficient symptom of Disease. These finding are very important in health\nresearch, particularly in allocation of medical resources for patients who\npredicted as high-risks\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 05:15:07 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 11:07:25 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Montazeri", "Mitra", ""], ["Baghshah", "Mahdieh Soleymani", ""], ["Enhesari", "Ahmad", ""]]}, {"id": "1512.04792", "submitter": "Han Xiao Bookman", "authors": "Han Xiao, Minlie Huang, Xiaoyan Zhu", "title": "From One Point to A Manifold: Knowledge Graph Embedding For Precise Link\n  Prediction", "comments": "arXiv admin note: text overlap with arXiv:1509.05488", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding aims at offering a numerical knowledge\nrepresentation paradigm by transforming the entities and relations into\ncontinuous vector space. However, existing methods could not characterize the\nknowledge graph in a fine degree to make a precise prediction. There are two\nreasons: being an ill-posed algebraic system and applying an overstrict\ngeometric form. As precise prediction is critical, we propose an manifold-based\nembedding principle (\\textbf{ManifoldE}) which could be treated as a well-posed\nalgebraic system that expands the position of golden triples from one point in\ncurrent models to a manifold in ours. Extensive experiments show that the\nproposed models achieve substantial improvements against the state-of-the-art\nbaselines especially for the precise prediction task, and yet maintain high\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 14:24:44 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2015 14:14:51 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2016 09:47:10 GMT"}, {"version": "v4", "created": "Tue, 13 Jun 2017 06:38:13 GMT"}, {"version": "v5", "created": "Sat, 17 Jun 2017 03:59:43 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Xiao", "Han", ""], ["Huang", "Minlie", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1512.04860", "submitter": "Marc G. Bellemare", "authors": "Marc G. Bellemare, Georg Ostrovski, Arthur Guez, Philip S. Thomas and\n  R\\'emi Munos", "title": "Increasing the Action Gap: New Operators for Reinforcement Learning", "comments": null, "journal-ref": "Bellemare, Marc G., Ostrovski, G., Guez, A., Thomas, Philip S.,\n  and Munos, Remi. Increasing the Action Gap: New Operators for Reinforcement\n  Learning. Proceedings of the AAAI Conference on Artificial Intelligence, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces new optimality-preserving operators on Q-functions. We\nfirst describe an operator for tabular representations, the consistent Bellman\noperator, which incorporates a notion of local policy consistency. We show that\nthis local consistency leads to an increase in the action gap at each state;\nincreasing this gap, we argue, mitigates the undesirable effects of\napproximation and estimation errors on the induced greedy policies. This\noperator can also be applied to discretized continuous space and time problems,\nand we provide empirical results evidencing superior performance in this\ncontext. Extending the idea of a locally consistent operator, we then derive\nsufficient conditions for an operator to preserve optimality, leading to a\nfamily of operators which includes our consistent Bellman operator. As\ncorollaries we provide a proof of optimality for Baird's advantage learning\nalgorithm and derive other gap-increasing operators with interesting\nproperties. We conclude with an empirical study on 60 Atari 2600 games\nillustrating the strong potential of these new operators.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 17:13:49 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Bellemare", "Marc G.", ""], ["Ostrovski", "Georg", ""], ["Guez", "Arthur", ""], ["Thomas", "Philip S.", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1512.04976", "submitter": "Adam Krasuski", "authors": "Adam Krasuski", "title": "Conditions for Normative Decision Making at the Fire Ground", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the changes in an attitude to decision making at the fire ground.\nThe changes are driven by the recent technological shift. The emerging new\napproaches in sensing and data processing (under common umbrella of\nCyber-Physical Systems) allow for leveling off the gap, between humans and\nmachines, in perception of the fire ground. Furthermore, results from\ndescriptive decision theory question the rationality of human choices. This\ncreates the need for searching and testing new approaches for decision making\nduring emergency. We propose the framework that addresses this need. The\nprimary feature of the framework are possibilities for incorporation of\nnormative and prescriptive approaches to decision making. The framework also\nallows for comparison of the performance of decisions, between human and\nmachine.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 21:37:06 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Krasuski", "Adam", ""]]}, {"id": "1512.05006", "submitter": "Vikash Mansinghka", "authors": "Vikash Mansinghka, Richard Tibbetts, Jay Baxter, Pat Shafto, Baxter\n  Eaves", "title": "BayesDB: A probabilistic programming system for querying the probable\n  implications of data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to make statistical inference broadly accessible to\nnon-statisticians without sacrificing mathematical rigor or inference quality?\nThis paper describes BayesDB, a probabilistic programming platform that aims to\nenable users to query the probable implications of their data as directly as\nSQL databases enable them to query the data itself. This paper focuses on four\naspects of BayesDB: (i) BQL, an SQL-like query language for Bayesian data\nanalysis, that answers queries by averaging over an implicit space of\nprobabilistic models; (ii) techniques for implementing BQL using a broad class\nof multivariate probabilistic models; (iii) a semi-parametric Bayesian\nmodel-builder that auomatically builds ensembles of factorial mixture models to\nserve as baselines; and (iv) MML, a \"meta-modeling\" language for imposing\nqualitative constraints on the model-builder and combining baseline models with\ncustom algorithmic and statistical models that can be implemented in external\nsoftware. BayesDB is illustrated using three applications: cleaning and\nexploring a public database of Earth satellites; assessing the evidence for\ntemporal dependence between macroeconomic indicators; and analyzing a salary\nsurvey.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 23:09:41 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Mansinghka", "Vikash", ""], ["Tibbetts", "Richard", ""], ["Baxter", "Jay", ""], ["Shafto", "Pat", ""], ["Eaves", "Baxter", ""]]}, {"id": "1512.05245", "submitter": "Fergal Byrne", "authors": "Fergal Byrne", "title": "Symphony from Synapses: Neocortex as a Universal Dynamical Systems\n  Modeller using Hierarchical Temporal Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Reverse engineering the brain is proving difficult, perhaps impossible. While\nmany believe that this is just a matter of time and effort, a different\napproach might help. Here, we describe a very simple idea which explains the\npower of the brain as well as its structure, exploiting complex dynamics rather\nthan abstracting it away. Just as a Turing Machine is a Universal Digital\nComputer operating in a world of symbols, we propose that the brain is a\nUniversal Dynamical Systems Modeller, evolved bottom-up (itself using nested\nnetworks of interconnected, self-organised dynamical systems) to prosper in a\nworld of dynamical systems.\n  Recent progress in Applied Mathematics has produced startling evidence of\nwhat happens when abstract Dynamical Systems interact. Key latent information\ndescribing system A can be extracted by system B from very simple signals, and\nsignals can be used by one system to control and manipulate others. Using these\nfacts, we show how a region of the neocortex uses its dynamics to intrinsically\n\"compute\" about the external and internal world.\n  Building on an existing \"static\" model of cortical computation (Hawkins'\nHierarchical Temporal Memory - HTM), we describe how a region of neocortex can\nbe viewed as a network of components which together form a Dynamical Systems\nmodelling module, connected via sensory and motor pathways to the external\nworld, and forming part of a larger dynamical network in the brain.\n  Empirical modelling and simulations of Dynamical HTM are possible with simple\nextensions and combinations of currently existing open source software. We list\na number of relevant projects.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 16:58:06 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Byrne", "Fergal", ""]]}, {"id": "1512.05247", "submitter": "Steven Schockaert", "authors": "Sofie De Clercq, Steven Schockaert, Martine De Cock, Ann Now\\'e", "title": "Solving stable matching problems using answer set programming", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP). arXiv admin note: substantial text overlap with arXiv:1302.7251", "journal-ref": "Theory and Practice of Logic Programming 16 (2016) 247-268", "doi": "10.1017/S147106841600003X", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the introduction of the stable marriage problem (SMP) by Gale and\nShapley (1962), several variants and extensions have been investigated. While\nthis variety is useful to widen the application potential, each variant\nrequires a new algorithm for finding the stable matchings. To address this\nissue, we propose an encoding of the SMP using answer set programming (ASP),\nwhich can straightforwardly be adapted and extended to suit the needs of\nspecific applications. The use of ASP also means that we can take advantage of\nhighly efficient off-the-shelf solvers. To illustrate the flexibility of our\napproach, we show how our ASP encoding naturally allows us to select optimal\nstable matchings, i.e. matchings that are optimal according to some\nuser-specified criterion. To the best of our knowledge, our encoding offers the\nfirst exact implementation to find sex-equal, minimum regret, egalitarian or\nmaximum cardinality stable matchings for SMP instances in which individuals may\ndesignate unacceptable partners and ties between preferences are allowed.\n  This paper is under consideration in Theory and Practice of Logic Programming\n(TPLP).\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 16:59:14 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["De Clercq", "Sofie", ""], ["Schockaert", "Steven", ""], ["De Cock", "Martine", ""], ["Now\u00e9", "Ann", ""]]}, {"id": "1512.05294", "submitter": "Harini Suresh", "authors": "Harini Suresh", "title": "Feature Representation for ICU Mortality", "comments": "This article has been withdrawn due by the author due to the need for\n  more testing to verify results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good predictors of ICU Mortality have the potential to identify high-risk\npatients earlier, improve ICU resource allocation, or create more accurate\npopulation-level risk models. Machine learning practitioners typically make\nchoices about how to represent features in a particular model, but these\nchoices are seldom evaluated quantitatively. This study compares the\nperformance of different representations of clinical event data from MIMIC II\nin a logistic regression model to predict 36-hour ICU mortality. The most\ncommon representations are linear (normalized counts) and binary (yes/no).\nThese, along with a new representation termed \"hill\", are compared using both\nL1 and L2 regularization. Results indicate that the introduced \"hill\"\nrepresentation outperforms both the binary and linear representations, the hill\nrepresentation thus has the potential to improve existing models of ICU\nmortality.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 19:36:06 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 21:59:58 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Suresh", "Harini", ""]]}, {"id": "1512.05406", "submitter": "Siheng Chen", "authors": "Siheng Chen and Rohan Varma and Aarti Singh and Jelena Kova\\v{c}evi\\'c", "title": "Signal Representations on Graphs: Tools and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT cs.SI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for representing and modeling data on graphs. Based on\nthis framework, we study three typical classes of graph signals: smooth graph\nsignals, piecewise-constant graph signals, and piecewise-smooth graph signals.\nFor each class, we provide an explicit definition of the graph signals and\nconstruct a corresponding graph dictionary with desirable properties. We then\nstudy how such graph dictionary works in two standard tasks: approximation and\nsampling followed with recovery, both from theoretical as well as algorithmic\nperspectives. Finally, for each class, we present a case study of a real-world\nproblem by using the proposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 22:55:13 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Chen", "Siheng", ""], ["Varma", "Rohan", ""], ["Singh", "Aarti", ""], ["Kova\u010devi\u0107", "Jelena", ""]]}, {"id": "1512.05467", "submitter": "Marian-Andrei Rizoiu", "authors": "Marian-Andrei Rizoiu, Julien Velcin, St\\'ephane Lallich", "title": "Unsupervised Feature Construction for Improving Data Representation and\n  Semantics", "comments": null, "journal-ref": "Journal of Intelligent Information Systems, vol. 40, iss. 3, pp.\n  501-527, 2013", "doi": "10.1007/s10844-013-0235-x", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature-based format is the main data representation format used by machine\nlearning algorithms. When the features do not properly describe the initial\ndata, performance starts to degrade. Some algorithms address this problem by\ninternally changing the representation space, but the newly-constructed\nfeatures are rarely comprehensible. We seek to construct, in an unsupervised\nway, new features that are more appropriate for describing a given dataset and,\nat the same time, comprehensible for a human user. We propose two algorithms\nthat construct the new features as conjunctions of the initial primitive\nfeatures or their negations. The generated feature sets have reduced\ncorrelations between features and succeed in catching some of the hidden\nrelations between individuals in a dataset. For example, a feature like $sky\n\\wedge \\neg building \\wedge panorama$ would be true for non-urban images and is\nmore informative than simple features expressing the presence or the absence of\nan object. The notion of Pareto optimality is used to evaluate feature sets and\nto obtain a balance between total correlation and the complexity of the\nresulted feature set. Statistical hypothesis testing is used in order to\nautomatically determine the values of the parameters used for constructing a\ndata-dependent feature set. We experimentally show that our approaches achieve\nthe construction of informative feature sets for multiple datasets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 05:18:05 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Rizoiu", "Marian-Andrei", ""], ["Velcin", "Julien", ""], ["Lallich", "St\u00e9phane", ""]]}, {"id": "1512.05484", "submitter": "Mohsen Malmir", "authors": "Mohsen Malmir, Karan Sikka, Deborah Forster, Ian Fasel, Javier R.\n  Movellan, Garrison W. Cottrell", "title": "Deep Active Object Recognition by Joint Label and Action Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An active object recognition system has the advantage of being able to act in\nthe environment to capture images that are more suited for training and that\nlead to better performance at test time. In this paper, we propose a deep\nconvolutional neural network for active object recognition that simultaneously\npredicts the object label, and selects the next action to perform on the object\nwith the aim of improving recognition performance. We treat active object\nrecognition as a reinforcement learning problem and derive the cost function to\ntrain the network for joint prediction of the object label and the action. A\ngenerative model of object similarities based on the Dirichlet distribution is\nproposed and embedded in the network for encoding the state of the system. The\ntraining is carried out by simultaneously minimizing the label and action\nprediction errors using gradient descent. We empirically show that the proposed\nnetwork is able to predict both the object label and the actions on GERMS, a\ndataset for active object recognition. We compare the test label prediction\naccuracy of the proposed model with Dirichlet and Naive Bayes state encoding.\nThe results of experiments suggest that the proposed model equipped with\nDirichlet state encoding is superior in performance, and selects images that\nlead to better training and higher accuracy of label prediction at test time.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 07:33:45 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Malmir", "Mohsen", ""], ["Sikka", "Karan", ""], ["Forster", "Deborah", ""], ["Fasel", "Ian", ""], ["Movellan", "Javier R.", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1512.05504", "submitter": "Shreyas Sekar", "authors": "Elliot Anshelevich and Shreyas Sekar", "title": "Blind, Greedy, and Random: Ordinal Approximation Algorithms for Matching\n  and Clustering", "comments": "This paper contains the results that appeared in a AAAI'16 paper\n  along with some new results for other problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Matching and other related problems in a partial information setting\nwhere the agents' utilities for being matched to other agents are hidden and\nthe mechanism only has access to ordinal preference information. Our model is\nmotivated by the fact that in many settings, agents cannot express the\nnumerical values of their utility for different outcomes, but are still able to\nrank the outcomes in their order of preference. Specifically, we study problems\nwhere the ground truth exists in the form of a weighted graph, and look to\ndesign algorithms that approximate the true optimum matching using only the\npreference orderings for each agent (induced by the hidden weights) as input.\nIf no restrictions are placed on the weights, then one cannot hope to do better\nthan the simple greedy algorithm, which yields a half optimal matching. Perhaps\nsurprisingly, we show that by imposing a little structure on the weights, we\ncan improve upon the trivial algorithm significantly: we design a\n1.6-approximation algorithm for instances where the hidden weights obey the\nmetric inequality. Using our algorithms for matching as a black-box, we also\ndesign new approximation algorithms for other closely related problems: these\ninclude a a 3.2-approximation for the problem of clustering agents into equal\nsized partitions, a 4-approximation algorithm for Densest k-subgraph, and a\n2.14-approximation algorithm for Max TSP. These results are the first\nnon-trivial ordinal approximation algorithms for such problems, and indicate\nthat we can design robust algorithms even when we are agnostic to the precise\nagent utilities.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 09:29:48 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 21:29:51 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Anshelevich", "Elliot", ""], ["Sekar", "Shreyas", ""]]}, {"id": "1512.05509", "submitter": "Denis Steckelmacher", "authors": "Denis Steckelmacher and Peter Vrancx", "title": "An Empirical Comparison of Neural Architectures for Reinforcement\n  Learning in Partially Observable Environments", "comments": "Presented at the 27th Benelux Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the performance of fitted neural Q iteration for\nreinforcement learning in several partially observable environments, using\nthree recurrent neural network architectures: Long Short-Term Memory, Gated\nRecurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of\nseveral thousands candidate architectures. A variant of fitted Q iteration,\nbased on Advantage values instead of Q values, is also explored. The results\nshow that GRU performs significantly better than LSTM and MUT1 for most of the\nproblems considered, requiring less training episodes and less CPU time before\nlearning a very good policy. Advantage learning also tends to produce better\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 09:45:51 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Steckelmacher", "Denis", ""], ["Vrancx", "Peter", ""]]}, {"id": "1512.05569", "submitter": "Mohit Verma", "authors": "Mohit Verma and J. Rajasankar", "title": "A thermodynamical approach towards multi-criteria decision making (MCDM)", "comments": null, "journal-ref": "Applied Soft Computing 2017, Volume 52, Pages 323--332", "doi": "10.1016/j.asoc.2016.10.033", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-criteria decision making (MCDM) problems, ratings are assigned to\nthe alternatives on different criteria by the expert group. In this paper, we\npropose a thermodynamically consistent model for MCDM using the analogies for\nthermodynamical indicators - energy, exergy and entropy. The most commonly used\nmethod for analysing MCDM problem is Technique for Order of Preference by\nSimilarity to Ideal Solution (TOPSIS). The conventional TOPSIS method uses a\nmeasure similar to that of energy for the ranking of alternatives. We\ndemonstrate that the ranking of the alternatives is more meaningful if we use\nexergy in place of energy. The use of exergy is superior due to the inclusion\nof a factor accounting for the quality of the ratings by the expert group. The\nunevenness in the ratings by the experts is measured by entropy. The procedure\nfor the calculation of the thermodynamical indicators is explained in both\ncrisp and fuzzy environment. Finally, two case studies are carried out to\ndemonstrate effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 13:02:36 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Verma", "Mohit", ""], ["Rajasankar", "J.", ""]]}, {"id": "1512.05665", "submitter": "Ulrich Schaechtle", "authors": "Ulrich Schaechtle, Ben Zinberg, Alexey Radul, Kostas Stathis and\n  Vikash K. Mansinghka", "title": "Probabilistic Programming with Gaussian Process Memoization", "comments": "36 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes (GPs) are widely used tools in statistics, machine\nlearning, robotics, computer vision, and scientific computation. However,\ndespite their popularity, they can be difficult to apply; all but the simplest\nclassification or regression applications require specification and inference\nover complex covariance functions that do not admit simple analytical\nposteriors. This paper shows how to embed Gaussian processes in any\nhigher-order probabilistic programming language, using an idiom based on\nmemoization, and demonstrates its utility by implementing and extending classic\nand state-of-the-art GP applications. The interface to Gaussian processes,\ncalled gpmem, takes an arbitrary real-valued computational process as input and\nreturns a statistical emulator that automatically improve as the original\nprocess is invoked and its input-output behavior is recorded. The flexibility\nof gpmem is illustrated via three applications: (i) robust GP regression with\nhierarchical hyper-parameter learning, (ii) discovering symbolic expressions\nfrom time-series data by fully Bayesian structure learning over kernels\ngenerated by a stochastic grammar, and (iii) a bandit formulation of Bayesian\noptimization with automatic inference and action selection. All applications\nshare a single 50-line Python library and require fewer than 20 lines of\nprobabilistic code each.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 16:46:10 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 10:55:02 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Schaechtle", "Ulrich", ""], ["Zinberg", "Ben", ""], ["Radul", "Alexey", ""], ["Stathis", "Kostas", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1512.05742", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Laurent Charlin,\n  Joelle Pineau", "title": "A Survey of Available Corpora for Building Data-Driven Dialogue Systems", "comments": "56 pages including references and appendix, 5 tables and 1 figure;\n  Under review for the Dialogue & Discourse journal. Update: paper has been\n  rewritten and now includes several new datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, several areas of speech and language understanding\nhave witnessed substantial breakthroughs from the use of data-driven models. In\nthe area of dialogue systems, the trend is less obvious, and most practical\nsystems are still built through significant engineering and expert knowledge.\nNevertheless, several recent results suggest that data-driven approaches are\nfeasible and quite promising. To facilitate research in this area, we have\ncarried out a wide survey of publicly available datasets suitable for\ndata-driven learning of dialogue systems. We discuss important characteristics\nof these datasets, how they can be used to learn diverse dialogue strategies,\nand their other potential uses. We also examine methods for transfer learning\nbetween datasets and the use of external knowledge. Finally, we discuss\nappropriate choice of evaluation metrics for the learning objective.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 19:52:39 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 04:58:05 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 01:15:32 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Lowe", "Ryan", ""], ["Henderson", "Peter", ""], ["Charlin", "Laurent", ""], ["Pineau", "Joelle", ""]]}, {"id": "1512.05832", "submitter": "Owain Evans", "authors": "Owain Evans, Andreas Stuhlmueller, Noah D. Goodman", "title": "Learning the Preferences of Ignorant, Inconsistent Agents", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important use of machine learning is to learn what people value. What\nposts or photos should a user be shown? Which jobs or activities would a person\nfind rewarding? In each case, observations of people's past choices can inform\nour inferences about their likes and preferences. If we assume that choices are\napproximately optimal according to some utility function, we can treat\npreference inference as Bayesian inverse planning. That is, given a prior on\nutility functions and some observed choices, we invert an optimal\ndecision-making process to infer a posterior distribution on utility functions.\nHowever, people often deviate from approximate optimality. They have false\nbeliefs, their planning is sub-optimal, and their choices may be temporally\ninconsistent due to hyperbolic discounting and other biases. We demonstrate how\nto incorporate these deviations into algorithms for preference inference by\nconstructing generative models of planning for agents who are subject to false\nbeliefs and time inconsistency. We explore the inferences these models make\nabout preferences, beliefs, and biases. We present a behavioral experiment in\nwhich human subjects perform preference inference given the same observations\nof choices as our model. Results show that human subjects (like our model)\nexplain choices in terms of systematic deviations from optimal behavior and\nsuggest that they take such deviations into account when inferring preferences.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 00:24:08 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Evans", "Owain", ""], ["Stuhlmueller", "Andreas", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1512.05849", "submitter": "Miles Brundage", "authors": "Miles Brundage", "title": "Modeling Progress in AI", "comments": "AAAI 2016 Workshop on AI, Ethics, and Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Participants in recent discussions of AI-related issues ranging from\nintelligence explosion to technological unemployment have made diverse claims\nabout the nature, pace, and drivers of progress in AI. However, these theories\nare rarely specified in enough detail to enable systematic evaluation of their\nassumptions or to extrapolate progress quantitatively, as is often done with\nsome success in other technological domains. After reviewing relevant\nliteratures and justifying the need for more rigorous modeling of AI progress,\nthis paper contributes to that research program by suggesting ways to account\nfor the relationship between hardware speed increases and algorithmic\nimprovements in AI, the role of human inputs in enabling AI capabilities, and\nthe relationships between different sub-fields of AI. It then outlines ways of\ntailoring AI progress models to generate insights on the specific issue of\ntechnological unemployment, and outlines future directions for research on AI\nprogress.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 04:17:39 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Brundage", "Miles", ""]]}, {"id": "1512.05868", "submitter": "Iddan Golomb", "authors": "Michal Feldman, Amos Fiat, Iddan Golomb", "title": "On Voting and Facility Location", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study mechanisms for candidate selection that seek to minimize the social\ncost, where voters and candidates are associated with points in some underlying\nmetric space. The social cost of a candidate is the sum of its distances to\neach voter. Some of our work assumes that these points can be modeled on a real\nline, but other results of ours are more general.\n  A question closely related to candidate selection is that of minimizing the\nsum of distances for facility location. The difference is that in our setting\nthere is a fixed set of candidates, whereas the large body of work on facility\nlocation seems to consider every point in the metric space to be a possible\ncandidate. This gives rise to three types of mechanisms which differ in the\ngranularity of their input space (voting, ranking and location mechanisms). We\nstudy the relationships between these three classes of mechanisms.\n  While it may seem that Black's 1948 median algorithm is optimal for candidate\nselection on the line, this is not the case. We give matching upper and lower\nbounds for a variety of settings. In particular, when candidates and voters are\non the line, our universally truthful spike mechanism gives a [tight]\napproximation of two. When assessing candidate selection mechanisms, we seek\nseveral desirable properties: (a) efficiency (minimizing the social cost) (b)\ntruthfulness (dominant strategy incentive compatibility) and (c) simplicity (a\nsmaller input space). We quantify the effect that truthfulness and simplicity\nimpose on the efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 07:48:16 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Feldman", "Michal", ""], ["Fiat", "Amos", ""], ["Golomb", "Iddan", ""]]}, {"id": "1512.05875", "submitter": "Alessandro Fontana", "authors": "Alessandro Fontana", "title": "Quadripolar Relational Model: a framework for the description of\n  borderline and narcissistic personality disorders", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Borderline personality disorder and narcissistic personality disorder are\nimportant nosographic entities and have been subject of intensive\ninvestigations. The currently prevailing psychodynamic theory for mental\ndisorders is based on the repertoire of defense mechanisms employed. Another\nline of research is concerned with the study of psychological traumas and\ndissociation as a defensive response. Both theories can be used to shed light\non some aspects of pathological mental functioning, and have many points of\ncontact. This work merges these two psychological theories, and builds a model\nof mental function in a relational context called Quadripolar Relational Model.\nThe model, which is enriched with ideas borrowed from the field of computer\nscience, leads to a new therapeutic proposal for psychological traumas and\npersonality disorders.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 09:08:49 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 10:54:11 GMT"}, {"version": "v3", "created": "Wed, 10 Aug 2016 09:01:18 GMT"}, {"version": "v4", "created": "Tue, 25 Oct 2016 07:07:23 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Fontana", "Alessandro", ""]]}, {"id": "1512.05986", "submitter": "Vlado Menkovski", "authors": "Vlado Menkovski, Zharko Aleksovski, Axel Saalbach, Hannes Nickisch", "title": "Can Pretrained Neural Networks Detect Anatomy?", "comments": "NIPS 2015 Workshop on Machine Learning in Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks demonstrated outstanding empirical results in\ncomputer vision and speech recognition tasks where labeled training data is\nabundant. In medical imaging, there is a huge variety of possible imaging\nmodalities and contrasts, where annotated data is usually very scarce. We\npresent two approaches to deal with this challenge. A network pretrained in a\ndifferent domain with abundant data is used as a feature extractor, while a\nsubsequent classifier is trained on a small target dataset; and a deep\narchitecture trained with heavy augmentation and equipped with sophisticated\nregularization methods. We test the approaches on a corpus of X-ray images to\ndesign an anatomy detection system.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 15:16:31 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Menkovski", "Vlado", ""], ["Aleksovski", "Zharko", ""], ["Saalbach", "Axel", ""], ["Nickisch", "Hannes", ""]]}, {"id": "1512.06034", "submitter": "Marco Manna", "authors": "Weronika T. Adrian and Nicola Leone and Marco Manna", "title": "Ontology-driven Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homogeneous unstructured data (HUD) are collections of unstructured documents\nthat share common properties, such as similar layout, common file format, or\ncommon domain of values. Building on such properties, it would be desirable to\nautomatically process HUD to access the main information through a semantic\nlayer -- typically an ontology -- called semantic view. Hence, we propose an\nontology-based approach for extracting semantically rich information from HUD,\nby integrating and extending recent technologies and results from the fields of\nclassical information extraction, table recognition, ontologies, text\nannotation, and logic programming. Moreover, we design and implement a system,\nnamed KnowRex, that has been successfully applied to curriculum vitae in the\nEuropass style to offer a semantic view of them, and be able, for example, to\nselect those which exhibit required skills.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 16:56:02 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Adrian", "Weronika T.", ""], ["Leone", "Nicola", ""], ["Manna", "Marco", ""]]}, {"id": "1512.06211", "submitter": "Agnieszka Lawrynowicz", "authors": "C. Maria Keet and Agnieszka Lawrynowicz", "title": "Test-Driven Development of ontologies (extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging ontology authoring methods to add knowledge to an ontology focus on\nameliorating the validation bottleneck. The verification of the newly added\naxiom is still one of trying and seeing what the reasoner says, because a\nsystematic testbed for ontology authoring is missing. We sought to address this\nby introducing the approach of test-driven development for ontology authoring.\nWe specify 36 generic tests, as TBox queries and TBox axioms tested through\nindividuals, and structure their inner workings in an `open box'-way, which\ncover the OWL 2 DL language features. This is implemented as a Protege plugin\nso that one can perform a TDD test as a black box test. We evaluated the two\ntest approaches on their performance. The TBox queries were faster, and that\neffect is more pronounced the larger the ontology is. We provide a general\nsequence of a TDD process for ontology engineering as a foundation for a TDD\nmethodology.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 09:15:24 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Keet", "C. Maria", ""], ["Lawrynowicz", "Agnieszka", ""]]}, {"id": "1512.06293", "submitter": "Thomas Wiatowski", "authors": "Thomas Wiatowski and Helmut B\\\"olcskei", "title": "A Mathematical Theory of Deep Convolutional Neural Networks for Feature\n  Extraction", "comments": "IEEE Transactions on Information Theory, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.LG math.FA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have led to breakthrough results in\nnumerous practical machine learning tasks such as classification of images in\nthe ImageNet data set, control-policy-learning to play Atari games or the board\ngame Go, and image captioning. Many of these applications first perform feature\nextraction and then feed the results thereof into a trainable classifier. The\nmathematical analysis of deep convolutional neural networks for feature\nextraction was initiated by Mallat, 2012. Specifically, Mallat considered\nso-called scattering networks based on a wavelet transform followed by the\nmodulus non-linearity in each network layer, and proved translation invariance\n(asymptotically in the wavelet scale parameter) and deformation stability of\nthe corresponding feature extractor. This paper complements Mallat's results by\ndeveloping a theory that encompasses general convolutional transforms, or in\nmore technical parlance, general semi-discrete frames (including\nWeyl-Heisenberg filters, curvelets, shearlets, ridgelets, wavelets, and learned\nfilters), general Lipschitz-continuous non-linearities (e.g., rectified linear\nunits, shifted logistic sigmoids, hyperbolic tangents, and modulus functions),\nand general Lipschitz-continuous pooling operators emulating, e.g.,\nsub-sampling and averaging. In addition, all of these elements can be different\nin different network layers. For the resulting feature extractor we prove a\ntranslation invariance result of vertical nature in the sense of the features\nbecoming progressively more translation-invariant with increasing network\ndepth, and we establish deformation sensitivity bounds that apply to signal\nclasses such as, e.g., band-limited functions, cartoon functions, and Lipschitz\nfunctions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 22:31:24 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 10:12:30 GMT"}, {"version": "v3", "created": "Tue, 24 Oct 2017 06:44:21 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Wiatowski", "Thomas", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1512.06427", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Towards Integrated Glance To Restructuring in Combinatorial Optimization", "comments": "31 pages, 34 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper focuses on a new class of combinatorial problems which consists in\nrestructuring of solutions (as sets/structures) in combinatorial optimization.\nTwo main features of the restructuring process are examined: (i) a cost of the\nrestructuring, (ii) a closeness to a goal solution. Three types of the\nrestructuring problems are under study: (a) one-stage structuring, (b)\nmulti-stage structuring, and (c) structuring over changed element set.\nOne-criterion and multicriteria problem formulations can be considered. The\nrestructuring problems correspond to redesign (improvement, upgrade) of modular\nsystems or solutions. The restructuring approach is described and illustrated\n(problem statements, solving schemes, examples) for the following combinatorial\noptimization problems: knapsack problem, multiple choice problem, assignment\nproblem, spanning tree problems, clustering problem, multicriteria ranking\n(sorting) problem, morphological clique problem. Numerical examples illustrate\nthe restructuring problems and solving schemes.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 20:27:23 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1512.06492", "submitter": "Qifei Wang", "authors": "Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy", "title": "Remote Health Coaching System and Human Motion Data Analysis for\n  Physical Therapy with Microsoft Kinect", "comments": "6 pages, Computer Vision for Accessible and Affordable HealthCare\n  Workshop (ICCV2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the recent progress we have made for the computer\nvision technologies in physical therapy with the accessible and affordable\ndevices. We first introduce the remote health coaching system we build with\nMicrosoft Kinect. Since the motion data captured by Kinect is noisy, we\ninvestigate the data accuracy of Kinect with respect to the high accuracy\nmotion capture system. We also propose an outlier data removal algorithm based\non the data distribution. In order to generate the kinematic parameter from the\nnoisy data captured by Kinect, we propose a kinematic filtering algorithm based\non Unscented Kalman Filter and the kinematic model of human skeleton. The\nproposed algorithm can obtain smooth kinematic parameter with reduced noise\ncompared to the kinematic parameter generated from the raw motion data from\nKinect.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 04:58:36 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Wang", "Qifei", ""], ["Kurillo", "Gregorij", ""], ["Ofli", "Ferda", ""], ["Bajcsy", "Ruzena", ""]]}, {"id": "1512.06633", "submitter": "Kuldeep Meel", "authors": "Kuldeep S. Meel, Moshe Vardi, Supratik Chakraborty, Daniel J. Fremont,\n  Sanjit A. Seshia, Dror Fried, Alexander Ivrii and Sharad Malik", "title": "Constrained Sampling and Counting: Universal Hashing Meets SAT Solving", "comments": "Appears in proceedings of AAAI-16 Workshop on Beyond NP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained sampling and counting are two fundamental problems in artificial\nintelligence with a diverse range of applications, spanning probabilistic\nreasoning and planning to constrained-random verification. While the theory of\nthese problems was thoroughly investigated in the 1980s, prior work either did\nnot scale to industrial size instances or gave up correctness guarantees to\nachieve scalability. Recently, we proposed a novel approach that combines\nuniversal hashing and SAT solving and scales to formulas with hundreds of\nthousands of variables without giving up correctness guarantees. This paper\nprovides an overview of the key ingredients of the approach and discusses\nchallenges that need to be overcome to handle larger real-world instances.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 14:10:10 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Meel", "Kuldeep S.", ""], ["Vardi", "Moshe", ""], ["Chakraborty", "Supratik", ""], ["Fremont", "Daniel J.", ""], ["Seshia", "Sanjit A.", ""], ["Fried", "Dror", ""], ["Ivrii", "Alexander", ""], ["Malik", "Sharad", ""]]}, {"id": "1512.06747", "submitter": "Skyler Seto", "authors": "Skyler Seto, Wenyu Zhang, Yichen Zhou", "title": "Multivariate Time Series Classification Using Dynamic Time Warping\n  Template Selection for Human Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and computationally efficient means for classifying human activities\nhave been the subject of extensive research efforts. Most current research\nfocuses on extracting complex features to achieve high classification accuracy.\nWe propose a template selection approach based on Dynamic Time Warping, such\nthat complex feature extraction and domain knowledge is avoided. We demonstrate\nthe predictive capability of the algorithm on both simulated and real\nsmartphone data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 18:36:53 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Seto", "Skyler", ""], ["Zhang", "Wenyu", ""], ["Zhou", "Yichen", ""]]}, {"id": "1512.06789", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega, Daniel A. Braun, Justin Dyer, Kee-Eung Kim and\n  Naftali Tishby", "title": "Information-Theoretic Bounded Rationality", "comments": "47 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounded rationality, that is, decision-making and planning under resource\nlimitations, is widely regarded as an important open problem in artificial\nintelligence, reinforcement learning, computational neuroscience and economics.\nThis paper offers a consolidated presentation of a theory of bounded\nrationality based on information-theoretic ideas. We provide a conceptual\njustification for using the free energy functional as the objective function\nfor characterizing bounded-rational decisions. This functional possesses three\ncrucial properties: it controls the size of the solution space; it has Monte\nCarlo planners that are exact, yet bypass the need for exhaustive search; and\nit captures model uncertainty arising from lack of evidence or from interacting\nwith other agents having unknown intentions. We discuss the single-step\ndecision-making case, and show how to extend it to sequential decisions using\nequivalence transformations. This extension yields a very general class of\ndecision problems that encompass classical decision rules (e.g. EXPECTIMAX and\nMINIMAX) as limit cases, as well as trust- and risk-sensitive planning.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 19:58:46 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Braun", "Daniel A.", ""], ["Dyer", "Justin", ""], ["Kim", "Kee-Eung", ""], ["Tishby", "Naftali", ""]]}, {"id": "1512.06863", "submitter": "Julian McAuley", "authors": "Julian McAuley and Alex Yang", "title": "Addressing Complex and Subjective Product-Related Queries with Customer\n  Reviews", "comments": "WWW 2016; 14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews are often our first port of call when considering products and\npurchases online. When evaluating a potential purchase, we may have a specific\nquery in mind, e.g. `will this baby seat fit in the overhead compartment of a\n747?' or `will I like this album if I liked Taylor Swift's 1989?'. To answer\nsuch questions we must either wade through huge volumes of consumer reviews\nhoping to find one that is relevant, or otherwise pose our question directly to\nthe community via a Q/A system.\n  In this paper we hope to fuse these two paradigms: given a large volume of\npreviously answered queries about products, we hope to automatically learn\nwhether a review of a product is relevant to a given query. We formulate this\nas a machine learning problem using a mixture-of-experts-type framework---here\neach review is an `expert' that gets to vote on the response to a particular\nquery; simultaneously we learn a relevance function such that `relevant'\nreviews are those that vote correctly. At test time this learned relevance\nfunction allows us to surface reviews that are relevant to new queries\non-demand. We evaluate our system, Moqa, on a novel corpus of 1.4 million\nquestions (and answers) and 13 million reviews. We show quantitatively that it\nis effective at addressing both binary and open-ended queries, and\nqualitatively that it surfaces reviews that human evaluators consider to be\nrelevant.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 21:01:07 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["McAuley", "Julian", ""], ["Yang", "Alex", ""]]}, {"id": "1512.06945", "submitter": "EPTCS", "authors": "Fernando S\\'aenz-P\\'erez (Universidad Complutense de Madrid)", "title": "Restricted Predicates for Hypothetical Datalog", "comments": "In Proceedings PROLE 2015, arXiv:1512.06178", "journal-ref": "EPTCS 200, 2015, pp. 64-79", "doi": "10.4204/EPTCS.200.5", "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothetical Datalog is based on an intuitionistic semantics rather than on a\nclassical logic semantics, and embedded implications are allowed in rule\nbodies. While the usual implication (i.e., the neck of a Horn clause) stands\nfor inferring facts, an embedded implication plays the role of assuming its\npremise for deriving its consequence. A former work introduced both a formal\nframework and a goal-oriented tabled implementation, allowing negation in rule\nbodies. While in that work positive assumptions for both facts and rules can\noccur in the premise, negative assumptions are not allowed. In this work, we\ncover this subject by introducing a new concept: a restricted predicate, which\nallows negative assumptions by pruning the usual semantics of a predicate. This\nnew setting has been implemented in the deductive system DES.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 03:16:55 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["S\u00e1enz-P\u00e9rez", "Fernando", "", "Universidad Complutense de Madrid"]]}, {"id": "1512.06992", "submitter": "Christos Dimitrakakis", "authors": "Zuhe Zhang, Benjamin Rubinstein, Christos Dimitrakakis", "title": "On the Differential Privacy of Bayesian Inference", "comments": "AAAI 2016, Feb 2016, Phoenix, Arizona, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to communicate findings of Bayesian inference to third parties,\nwhile preserving the strong guarantee of differential privacy. Our main\ncontributions are four different algorithms for private Bayesian inference on\nproba-bilistic graphical models. These include two mechanisms for adding noise\nto the Bayesian updates, either directly to the posterior parameters, or to\ntheir Fourier transform so as to preserve update consistency. We also utilise a\nrecently introduced posterior sampling mechanism, for which we prove bounds for\nthe specific but general case of discrete Bayesian networks; and we introduce a\nmaximum-a-posteriori private mechanism. Our analysis includes utility and\nprivacy bounds, with a novel focus on the influence of graph structure on\nprivacy. Worked examples and experiments with Bayesian na{\\\"i}ve Bayes and\nBayesian linear regression illustrate the application of our mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 09:22:39 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Zhang", "Zuhe", ""], ["Rubinstein", "Benjamin", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "1512.07048", "submitter": "Roel Bertens", "authors": "Roel Bertens and Jilles Vreeken and Arno Siebes", "title": "Beauty and Brains: Detecting Anomalous Pattern Co-Occurrences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our world is filled with both beautiful and brainy people, but how often does\na Nobel Prize winner also wins a beauty pageant? Let us assume that someone who\nis both very beautiful and very smart is more rare than what we would expect\nfrom the combination of the number of beautiful and brainy people. Of course\nthere will still always be some individuals that defy this stereotype; these\nbeautiful brainy people are exactly the class of anomaly we focus on in this\npaper. They do not posses intrinsically rare qualities, it is the unexpected\ncombination of factors that makes them stand out.\n  In this paper we define the above described class of anomaly and propose a\nmethod to quickly identify them in transaction data. Further, as we take a\npattern set based approach, our method readily explains why a transaction is\nanomalous. The effectiveness of our method is thoroughly verified with a wide\nrange of experiments on both real world and synthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 12:15:12 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 15:55:56 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Bertens", "Roel", ""], ["Vreeken", "Jilles", ""], ["Siebes", "Arno", ""]]}, {"id": "1512.07056", "submitter": "Roel Bertens", "authors": "Roel Bertens and Jilles Vreeken and Arno Siebes", "title": "Keeping it Short and Simple: Summarising Complex Event Sequences with\n  Multivariate Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to obtain concise descriptions of discrete multivariate\nsequential data. In particular, how to do so in terms of rich multivariate\nsequential patterns that can capture potentially highly interesting\n(cor)relations between sequences. To this end we allow our pattern language to\nspan over the domains (alphabets) of all sequences, allow patterns to overlap\ntemporally, as well as allow for gaps in their occurrences.\n  We formalise our goal by the Minimum Description Length principle, by which\nour objective is to discover the set of patterns that provides the most\nsuccinct description of the data. To discover high-quality pattern sets\ndirectly from data, we introduce DITTO, a highly efficient algorithm that\napproximates the ideal result very well.\n  Experiments show that DITTO correctly discovers the patterns planted in\nsynthetic data. Moreover, it scales favourably with the length of the data, the\nnumber of attributes, the alphabet sizes. On real data, ranging from sensor\nnetworks to annotated text, DITTO discovers easily interpretable summaries that\nprovide clear insight in both the univariate and multivariate structure.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 12:35:32 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 16:19:05 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Bertens", "Roel", ""], ["Vreeken", "Jilles", ""], ["Siebes", "Arno", ""]]}, {"id": "1512.07143", "submitter": "Marc Bola\\~nos", "authors": "Mariella Dimiccoli and Marc Bola\\~nos and Estefania Talavera and\n  Maedeh Aghaei and Stavri G. Nikolov and Petia Radeva", "title": "SR-Clustering: Semantic Regularized Clustering for Egocentric Photo\n  Streams Segmentation", "comments": "23 pages, 10 figures, 2 tables. In Press in Computer Vision and Image\n  Understanding Journal", "journal-ref": null, "doi": "10.1016/j.cviu.2016.10.005", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While wearable cameras are becoming increasingly popular, locating relevant\ninformation in large unstructured collections of egocentric images is still a\ntedious and time consuming processes. This paper addresses the problem of\norganizing egocentric photo streams acquired by a wearable camera into\nsemantically meaningful segments. First, contextual and semantic information is\nextracted for each image by employing a Convolutional Neural Networks approach.\nLater, by integrating language processing, a vocabulary of concepts is defined\nin a semantic space. Finally, by exploiting the temporal coherence in photo\nstreams, images which share contextual and semantic attributes are grouped\ntogether. The resulting temporal segmentation is particularly suited for\nfurther analysis, ranging from activity and event recognition to semantic\nindexing and summarization. Experiments over egocentric sets of nearly 17,000\nimages, show that the proposed approach outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 16:13:54 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 09:40:11 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Dimiccoli", "Mariella", ""], ["Bola\u00f1os", "Marc", ""], ["Talavera", "Estefania", ""], ["Aghaei", "Maedeh", ""], ["Nikolov", "Stavri G.", ""], ["Radeva", "Petia", ""]]}, {"id": "1512.07162", "submitter": "Xi'ao Ma", "authors": "Xi'ao Ma, Guoyin Wang, Hong Yu", "title": "Heuristic algorithms for finding distribution reducts in probabilistic\n  rough set model", "comments": "44 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute reduction is one of the most important topics in rough set theory.\nHeuristic attribute reduction algorithms have been presented to solve the\nattribute reduction problem. It is generally known that fitness functions play\na key role in developing heuristic attribute reduction algorithms. The\nmonotonicity of fitness functions can guarantee the validity of heuristic\nattribute reduction algorithms. In probabilistic rough set model, distribution\nreducts can ensure the decision rules derived from the reducts are compatible\nwith those derived from the original decision table. However, there are few\nstudies on developing heuristic attribute reduction algorithms for finding\ndistribution reducts. This is partly due to the fact that there are no\nmonotonic fitness functions that are used to design heuristic attribute\nreduction algorithms in probabilistic rough set model. The main objective of\nthis paper is to develop heuristic attribute reduction algorithms for finding\ndistribution reducts in probabilistic rough set model. For one thing, two\nmonotonic fitness functions are constructed, from which equivalence definitions\nof distribution reducts can be obtained. For another, two modified monotonic\nfitness functions are proposed to evaluate the significance of attributes more\neffectively. On this basis, two heuristic attribute reduction algorithms for\nfinding distribution reducts are developed based on addition-deletion method\nand deletion method. In particular, the monotonicity of fitness functions\nguarantees the rationality of the proposed heuristic attribute reduction\nalgorithms. Results of experimental analysis are included to quantify the\neffectiveness of the proposed fitness functions and distribution reducts.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 17:17:45 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Ma", "Xi'ao", ""], ["Wang", "Guoyin", ""], ["Yu", "Hong", ""]]}, {"id": "1512.07430", "submitter": "Robert Kent", "authors": "Robert E. Kent", "title": "The ERA of FOLE: Foundation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the representation of ontologies in the first-order\nlogical environment FOLE (Kent 2013). An ontology defines the primitives with\nwhich to model the knowledge resources for a community of discourse (Gruber\n2009). These primitives, consisting of classes, relationships and properties,\nare represented by the entity-relationship-attribute ERA data model (Chen\n1976). An ontology uses formal axioms to constrain the interpretation of these\nprimitives. In short, an ontology specifies a logical theory. This paper is the\nfirst in a series of three papers that provide a rigorous mathematical\nrepresentation for the ERA data model in particular, and ontologies in general,\nwithin the first-order logical environment FOLE. The first two papers show how\nFOLE represents the formalism and semantics of (many-sorted) first-order logic\nin a classification form corresponding to ideas discussed in the Information\nFlow Framework (IFF). In particular, this first paper provides a foundation\nthat connects elements of the ERA data model with components of the first-order\nlogical environment FOLE, and the second paper provides a superstructure that\nextends FOLE to the formalisms of first-order logic. The third paper defines an\ninterpretation of FOLE in terms of the transformational passage, first\ndescribed in (Kent 2013), from the classification form of first-order logic to\nan equivalent interpretation form, thereby defining the formalism and semantics\nof first-order logical/relational database systems (Kent 2011). The FOLE\nrepresentation follows a conceptual structures approach, that is completely\ncompatible with formal concept analysis (Ganter and Wille 1999) and information\nflow (Barwise and Seligman 1997).\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 11:00:15 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Kent", "Robert E.", ""]]}, {"id": "1512.07487", "submitter": "Emilio Leonardi", "authors": "Alessandro Nordio, Alberto Tarable, Emilio Leonardi, Marco Ajmone\n  Marsan", "title": "Selecting the top-quality item through crowd scoring", "comments": "To be published, ACM TOMPECS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate crowdsourcing algorithms for finding the top-quality item\nwithin a large collection of objects with unknown intrinsic quality values.\nThis is an important problem with many relevant applications, for example in\nnetworked recommendation systems. The core of the algorithms is that objects\nare distributed to crowd workers, who return a noisy and biased evaluation. All\nreceived evaluations are then combined, to identify the top-quality object. We\nfirst present a simple probabilistic model for the system under investigation.\nThen, we devise and study a class of efficient adaptive algorithms to assign in\nan effective way objects to workers. We compare the performance of several\nalgorithms, which correspond to different choices of the design\nparameters/metrics. In the simulations we show that some of the algorithms\nachieve near optimal performance for a suitable setting of the system\nparameters.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 14:23:15 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 08:50:48 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Nordio", "Alessandro", ""], ["Tarable", "Alberto", ""], ["Leonardi", "Emilio", ""], ["Marsan", "Marco Ajmone", ""]]}, {"id": "1512.07590", "submitter": "Elliot Anshelevich", "authors": "Elliot Anshelevich and John Postl", "title": "Randomized Social Choice Functions Under Metric Preferences", "comments": "Portions appeared in IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We determine the quality of randomized social choice mechanisms in a setting\nin which the agents have metric preferences: every agent has a cost for each\nalternative, and these costs form a metric. We assume that these costs are\nunknown to the mechanisms (and possibly even to the agents themselves), which\nmeans we cannot simply select the optimal alternative, i.e. the alternative\nthat minimizes the total agent cost (or median agent cost). However, we do\nassume that the agents know their ordinal preferences that are induced by the\nmetric space. We examine randomized social choice functions that require only\nthis ordinal information and select an alternative that is good in expectation\nwith respect to the costs from the metric. To quantify how good a randomized\nsocial choice function is, we bound the distortion, which is the worst-case\nratio between expected cost of the alternative selected and the cost of the\noptimal alternative. We provide new distortion bounds for a variety of\nrandomized mechanisms, for both general metrics and for important special\ncases. Our results show a sizable improvement in distortion over deterministic\nmechanisms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 19:09:35 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 18:42:38 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Anshelevich", "Elliot", ""], ["Postl", "John", ""]]}, {"id": "1512.07636", "submitter": "Petros Boufounos", "authors": "Petros T Boufounos, Shantanu Rane, Hassan Mansour", "title": "Representation and Coding of Signal Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches to signal representation and coding theory have traditionally\nfocused on how to best represent signals using parsimonious representations\nthat incur the lowest possible distortion. Classical examples include linear\nand non-linear approximations, sparse representations, and rate-distortion\ntheory. Very often, however, the goal of processing is to extract specific\ninformation from the signal, and the distortion should be measured on the\nextracted information. The corresponding representation should, therefore,\nrepresent that information as parsimoniously as possible, without necessarily\naccurately representing the signal itself.\n  In this paper, we examine the problem of encoding signals such that\nsufficient information is preserved about their pairwise distances and their\ninner products. For that goal, we consider randomized embeddings as an encoding\nmechanism and provide a framework to analyze their performance. We also\ndemonstrate that it is possible to design the embedding such that it represents\ndifferent ranges of distances with different precision. These embeddings also\nallow the computation of kernel inner products with control on their inner\nproduct-preserving properties. Our results provide a broad framework to design\nand analyze embeddins, and generalize existing results in this area, such as\nrandom Fourier kernels and universal embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 21:04:31 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Boufounos", "Petros T", ""], ["Rane", "Shantanu", ""], ["Mansour", "Hassan", ""]]}, {"id": "1512.07650", "submitter": "Yahel David", "authors": "Yahel David and Nahum Shimkin", "title": "The Max $K$-Armed Bandit: PAC Lower Bounds and Efficient Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1508.05608", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Max $K$-Armed Bandit problem, where a learning agent is faced\nwith several stochastic arms, each a source of i.i.d. rewards of unknown\ndistribution. At each time step the agent chooses an arm, and observes the\nreward of the obtained sample. Each sample is considered here as a separate\nitem with the reward designating its value, and the goal is to find an item\nwith the highest possible value. Our basic assumption is a known lower bound on\nthe {\\em tail function} of the reward distributions. Under the PAC framework,\nwe provide a lower bound on the sample complexity of any\n$(\\epsilon,\\delta)$-correct algorithm, and propose an algorithm that attains\nthis bound up to logarithmic factors. We analyze the robustness of the proposed\nalgorithm and in addition, we compare the performance of this algorithm to the\nvariant in which the arms are not distinguishable by the agent and are chosen\nrandomly at each stage. Interestingly, when the maximal rewards of the arms\nhappen to be similar, the latter approach may provide better performance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 22:11:02 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["David", "Yahel", ""], ["Shimkin", "Nahum", ""]]}, {"id": "1512.07679", "submitter": "Gabriel Dulac-Arnold", "authors": "Gabriel Dulac-Arnold and Richard Evans and Hado van Hasselt and Peter\n  Sunehag and Timothy Lillicrap and Jonathan Hunt and Timothy Mann and\n  Theophane Weber and Thomas Degris and Ben Coppin", "title": "Deep Reinforcement Learning in Large Discrete Action Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to reason in an environment with a large number of discrete\nactions is essential to bringing reinforcement learning to a larger class of\nproblems. Recommender systems, industrial plants and language models are only\nsome of the many real-world tasks involving large numbers of discrete actions\nfor which current methods are difficult or even often impossible to apply. An\nability to generalize over the set of actions as well as sub-linear complexity\nrelative to the size of the set are both necessary to handle such tasks.\nCurrent approaches are not able to provide both of these, which motivates the\nwork in this paper. Our proposed approach leverages prior information about the\nactions to embed them in a continuous space upon which it can generalize.\nAdditionally, approximate nearest-neighbor methods allow for logarithmic-time\nlookup complexity relative to the number of actions, which is necessary for\ntime-wise tractable training. This combined approach allows reinforcement\nlearning methods to be applied to large-scale learning problems previously\nintractable with current methods. We demonstrate our algorithm's abilities on a\nseries of tasks having up to one million actions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 01:31:40 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 11:27:36 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Dulac-Arnold", "Gabriel", ""], ["Evans", "Richard", ""], ["van Hasselt", "Hado", ""], ["Sunehag", "Peter", ""], ["Lillicrap", "Timothy", ""], ["Hunt", "Jonathan", ""], ["Mann", "Timothy", ""], ["Weber", "Theophane", ""], ["Degris", "Thomas", ""], ["Coppin", "Ben", ""]]}, {"id": "1512.07721", "submitter": "Sam Fletcher", "authors": "Sam Fletcher, Md Zahidul Islam", "title": "Measuring pattern retention in anonymized data -- where one measure is\n  not enough", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore how modifying data to preserve privacy affects the\nquality of the patterns discoverable in the data. For any analysis of modified\ndata to be worth doing, the data must be as close to the original as possible.\nTherein lies a problem -- how does one make sure that modified data still\ncontains the information it had before modification? This question is not the\nsame as asking if an accurate classifier can be built from the modified data.\nOften in the literature, the prediction accuracy of a classifier made from\nmodified (anonymized) data is used as evidence that the data is similar to the\noriginal. We demonstrate that this is not the case, and we propose a new\nmethodology for measuring the retention of the patterns that existed in the\noriginal data. We then use our methodology to design three measures that can be\neasily implemented, each measuring aspects of the data that no pre-existing\ntechniques can measure. These measures do not negate the usefulness of\nprediction accuracy or other measures -- they are complementary to them, and\nsupport our argument that one measure is almost never enough.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 05:36:02 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Fletcher", "Sam", ""], ["Islam", "Md Zahidul", ""]]}, {"id": "1512.07734", "submitter": "Zhichun Wang", "authors": "Zhichun Wang and Juanzi Li", "title": "RDF2Rules: Learning Rules from RDF Knowledge Bases by Mining Frequent\n  Predicate Cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several large-scale RDF knowledge bases have been built and applied\nin many knowledge-based applications. To further increase the number of facts\nin RDF knowledge bases, logic rules can be used to predict new facts based on\nthe existing ones. Therefore, how to automatically learn reliable rules from\nlarge-scale knowledge bases becomes increasingly important. In this paper, we\npropose a novel rule learning approach named RDF2Rules for RDF knowledge bases.\nRDF2Rules first mines frequent predicate cycles (FPCs), a kind of interesting\nfrequent patterns in knowledge bases, and then generates rules from the mined\nFPCs. Because each FPC can produce multiple rules, and effective pruning\nstrategy is used in the process of mining FPCs, RDF2Rules works very\nefficiently. Another advantage of RDF2Rules is that it uses the entity type\ninformation when generates and evaluates rules, which makes the learned rules\nmore accurate. Experiments show that our approach outperforms the compared\napproach in terms of both efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 07:19:01 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Wang", "Zhichun", ""], ["Li", "Juanzi", ""]]}, {"id": "1512.07931", "submitter": "Hugh Chen", "authors": "Hugh Chen, Yusuf Erol, Eric Shen, Stuart Russell", "title": "Probabilistic Model-Based Approach for Heart Beat Detection", "comments": null, "journal-ref": null, "doi": "10.1088/0967-3334/37/9/1404", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, hospitals are ubiquitous and integral to modern society. Patients\nflow in and out of a veritable whirlwind of paperwork, consultations, and\npotential inpatient admissions, through an abstracted system that is not\nwithout flaws. One of the biggest flaws in the medical system is perhaps an\nunexpected one: the patient alarm system. One longitudinal study reported an\n88.8% rate of false alarms, with other studies reporting numbers of similar\nmagnitudes. These false alarm rates lead to a number of deleterious effects\nthat manifest in a significantly lower standard of care across clinics.\n  This paper discusses a model-based probabilistic inference approach to\nidentifying variables at a detection level. We design a generative model that\ncomplies with an overview of human physiology and perform approximate Bayesian\ninference. One primary goal of this paper is to justify a Bayesian modeling\napproach to increasing robustness in a physiological domain.\n  We use three data sets provided by Physionet, a research resource for complex\nphysiological signals, in the form of the Physionet 2014 Challenge set-p1 and\nset-p2, as well as the MGH/MF Waveform Database. On the extended data set our\nalgorithm is on par with the other top six submissions to the Physionet 2014\nchallenge.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 23:24:24 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Chen", "Hugh", ""], ["Erol", "Yusuf", ""], ["Shen", "Eric", ""], ["Russell", "Stuart", ""]]}, {"id": "1512.07942", "submitter": "Krzysztof Chalupka", "authors": "Krzysztof Chalupka, Pietro Perona and Frederick Eberhardt", "title": "Multi-Level Cause-Effect Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a domain-general account of causation that applies to settings in\nwhich macro-level causal relations between two systems are of interest, but the\nrelevant causal features are poorly understood and have to be aggregated from\nvast arrays of micro-measurements. Our approach generalizes that of Chalupka et\nal. (2015) to the setting in which the macro-level effect is not specified. We\nformalize the connection between micro- and macro-variables in such situations\nand provide a coherent framework describing causal relations at multiple levels\nof analysis. We present an algorithm that discovers macro-variable causes and\neffects from micro-level measurements obtained from an experiment. We further\nshow how to design experiments to discover macro-variables from observational\nmicro-variable data. Finally, we show that under specific conditions, one can\nidentify multiple levels of causal structure. Throughout the article, we use a\nsimulated neuroscience multi-unit recording experiment to illustrate the ideas\nand the algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 01:08:07 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Perona", "Pietro", ""], ["Eberhardt", "Frederick", ""]]}, {"id": "1512.07943", "submitter": "Alexander Kott", "authors": "Alexander Kott, Michael Ownby", "title": "Toward a Research Agenda in Adversarial Reasoning: Computational\n  Approaches to Anticipating the Opponent's Intent and Actions", "comments": "A version of this paper was presented at the SPIE Symposium on\n  Enabling Technologies for Simulation Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines adversarial reasoning as computational approaches to\ninferring and anticipating an enemy's perceptions, intents and actions. It\nargues that adversarial reasoning transcends the boundaries of game theory and\nmust also leverage such disciplines as cognitive modeling, control theory, AI\nplanning and others. To illustrate the challenges of applying adversarial\nreasoning to real-world problems, the paper explores the lessons learned in the\nCADET - a battle planning system that focuses on brigade-level ground\noperations and involves adversarial reasoning. From this example of current\ncapabilities, the paper proceeds to describe RAID - a DARPA program that aims\nto build capabilities in adversarial reasoning, and how such capabilities would\naddress practical requirements in Defense and other application areas.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 01:27:55 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Kott", "Alexander", ""], ["Ownby", "Michael", ""]]}, {"id": "1512.08030", "submitter": "Sukru Burc Eryilmaz", "authors": "Sukru Burc Eryilmaz, Duygu Kuzum, Shimeng Yu, H.-S. Philip Wong", "title": "Device and System Level Design Considerations for\n  Analog-Non-Volatile-Memory Based Neuromorphic Architectures", "comments": "4 pages, In Electron Devices Meeting (IEDM), 2015 IEEE International\n  (pp. 4.1). IEEE. Original paper can be found here:\n  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7409622. Abstract can\n  be found here:\n  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7409622&refinements%3D4224410500%26filter%3DAND%28p_IS_Number%3A7409598%29", "journal-ref": "Electron Devices Meeting (IEDM), IEEE International\n  ,pp.4.1.1-4.1.4, 2015", "doi": "10.1109/IEDM.2015.7409622", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives an overview of recent progress in the brain inspired\ncomputing field with a focus on implementation using emerging memories as\nelectronic synapses. Design considerations and challenges such as requirements\nand design targets on multilevel states, device variability, programming\nenergy, array-level connectivity, fan-in/fanout, wire energy, and IR drop are\npresented. Wires are increasingly important in design decisions, especially for\nlarge systems, and cycle-to-cycle variations have large impact on learning\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 19:43:05 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 18:01:43 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Eryilmaz", "Sukru Burc", ""], ["Kuzum", "Duygu", ""], ["Yu", "Shimeng", ""], ["Wong", "H. -S. Philip", ""]]}, {"id": "1512.08048", "submitter": "Sudip Mittal", "authors": "Sandeep Nair Narayanan, Sudip Mittal, Anupam Joshi", "title": "Using Data Analytics to Detect Anomalous States in Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicles are becoming more and more connected, this opens up a larger attack\nsurface which not only affects the passengers inside vehicles, but also people\naround them. These vulnerabilities exist because modern systems are built on\nthe comparatively less secure and old CAN bus framework which lacks even basic\nauthentication. Since a new protocol can only help future vehicles and not\nolder vehicles, our approach tries to solve the issue as a data analytics\nproblem and use machine learning techniques to secure cars. We develop a Hidden\nMarkov Model to detect anomalous states from real data collected from vehicles.\nUsing this model, while a vehicle is in operation, we are able to detect and\nissue alerts. Our model could be integrated as a plug-n-play device in all new\nand old cars.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 23:05:51 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Narayanan", "Sandeep Nair", ""], ["Mittal", "Sudip", ""], ["Joshi", "Anupam", ""]]}, {"id": "1512.08120", "submitter": "Fanhua Shang", "authors": "Fanhua Shang and James Cheng and Hong Cheng", "title": "Regularized Orthogonal Tensor Decompositions for Multi-Relational\n  Learning", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-relational learning has received lots of attention from researchers in\nvarious research communities. Most existing methods either suffer from\nsuperlinear per-iteration cost, or are sensitive to the given ranks. To address\nboth issues, we propose a scalable core tensor trace norm Regularized\nOrthogonal Iteration Decomposition (ROID) method for full or incomplete tensor\nanalytics, which can be generalized as a graph Laplacian regularized version by\nusing auxiliary information or a sparse higher-order orthogonal iteration\n(SHOOI) version. We first induce the equivalence relation of the Schatten\np-norm (0<p<\\infty) of a low multi-linear rank tensor and its core tensor. Then\nwe achieve a much smaller matrix trace norm minimization problem. Finally, we\ndevelop two efficient augmented Lagrange multiplier algorithms to solve our\nproblems with convergence guarantees. Extensive experiments using both real and\nsynthetic datasets, even though with only a few observations, verified both the\nefficiency and effectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 15:26:05 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2016 15:32:15 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Shang", "Fanhua", ""], ["Cheng", "James", ""], ["Cheng", "Hong", ""]]}, {"id": "1512.08451", "submitter": "Khalifeh AlJadda", "authors": "Khalifeh AlJadda, Rene Ranzinger, Melody Porterfield, Brent Weatherly,\n  Mohammed Korayem, John A. Miller, Khaled Rasheed, Krys J. Kochut, William S.\n  York", "title": "GELATO and SAGE: An Integrated Framework for MS Annotation", "comments": "To be submitted to Bioinformatics journal, Oxford press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several algorithms and tools have been developed to (semi) automate the\nprocess of glycan identification by interpreting Mass Spectrometric data.\nHowever, each has limitations when annotating MSn data with thousands of MS\nspectra using uncurated public databases. Moreover, the existing tools are not\ndesigned to manage MSn data where n > 2. We propose a novel software package to\nautomate the annotation of tandem MS data. This software consists of two major\ncomponents. The first, is a free, semi-automated MSn data interpreter called\nthe Glycomic Elucidation and Annotation Tool (GELATO). This tool extends and\nautomates the functionality of existing open source projects, namely,\nGlycoWorkbench (GWB) and GlycomeDB. The second is a machine learning model\ncalled Smart Anotation Enhancement Graph (SAGE), which learns the behavior of\nglycoanalysts to select annotations generated by GELATO that emulate human\ninterpretation of the spectra.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 16:40:18 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 16:48:09 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["AlJadda", "Khalifeh", ""], ["Ranzinger", "Rene", ""], ["Porterfield", "Melody", ""], ["Weatherly", "Brent", ""], ["Korayem", "Mohammed", ""], ["Miller", "John A.", ""], ["Rasheed", "Khaled", ""], ["Kochut", "Krys J.", ""], ["York", "William S.", ""]]}, {"id": "1512.08525", "submitter": "Khalifeh AlJadda", "authors": "Khalifeh AlJadda, Mohammed Korayem, Camilo Ortiz, Trey Grainger, John\n  A. Miller, Khaled Rasheed, Krys J. Kochut, William S. York, Rene Ranzinger,\n  Melody Porterfield", "title": "Mining Massive Hierarchical Data Using a Scalable Probabilistic\n  Graphical Model", "comments": "To be submitted to Big Data Journal. arXiv admin note: substantial\n  text overlap with arXiv:1407.5656", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Graphical Models (PGM) are very useful in the fields of machine\nlearning and data mining. The crucial limitation of those models,however, is\nthe scalability. The Bayesian Network, which is one of the most common PGMs\nused in machine learning and data mining, demonstrates this limitation when the\ntraining data consists of random variables, each of them has a large set of\npossible values. In the big data era, one would expect new extensions to the\nexisting PGMs to handle the massive amount of data produced these days by\ncomputers, sensors and other electronic devices. With hierarchical data - data\nthat is arranged in a treelike structure with several levels - one would expect\nto see hundreds of thousands or millions of values distributed over even just a\nsmall number of levels. When modeling this kind of hierarchical data across\nlarge data sets, Bayesian Networks become infeasible for representing the\nprobability distributions. In this paper we introduce an extension to Bayesian\nNetworks to handle massive sets of hierarchical data in a reasonable amount of\ntime and space. The proposed model achieves perfect precision of 1.0 and high\nrecall of 0.93 when it is used as multi-label classifier for the annotation of\nmass spectrometry data. On another data set of 1.5 billion search logs provided\nby CareerBuilder.com the model was able to predict latent semantic\nrelationships between search keywords with accuracy up to 0.80.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 21:02:20 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["AlJadda", "Khalifeh", ""], ["Korayem", "Mohammed", ""], ["Ortiz", "Camilo", ""], ["Grainger", "Trey", ""], ["Miller", "John A.", ""], ["Rasheed", "Khaled", ""], ["Kochut", "Krys J.", ""], ["York", "William S.", ""], ["Ranzinger", "Rene", ""], ["Porterfield", "Melody", ""]]}, {"id": "1512.08553", "submitter": "Wolfgang Garn", "authors": "Wolfgang Garn and Panos Louvieris", "title": "Conditional probability generation methods for high reliability\n  effects-based decision making", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making is often based on Bayesian networks. The building blocks for\nBayesian networks are its conditional probability tables (CPTs). These tables\nare obtained by parameter estimation methods, or they are elicited from subject\nmatter experts (SME). Some of these knowledge representations are insufficient\napproximations. Using knowledge fusion of cause and effect observations lead to\nbetter predictive decisions. We propose three new methods to generate CPTs,\nwhich even work when only soft evidence is provided. The first two are novel\nways of mapping conditional expectations to the probability space. The third is\na column extraction method, which obtains CPTs from nonlinear functions such as\nthe multinomial logistic regression. Case studies on military effects and burnt\nforest desertification have demonstrated that so derived CPTs have highly\nreliable predictive power, including superiority over the CPTs obtained from\nSMEs. In this context, new quality measures for determining the goodness of a\nCPT and for comparing CPTs with each other have been introduced. The predictive\npower and enhanced reliability of decision making based on the novel CPT\ngeneration methods presented in this paper have been confirmed and validated\nwithin the context of the case studies.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 23:08:30 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Garn", "Wolfgang", ""], ["Louvieris", "Panos", ""]]}, {"id": "1512.08710", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Massimiliano Sassoli de Bianchi and Sandro Sozzo", "title": "On the Foundations of the Brussels Operational-Realistic Approach to\n  Cognition", "comments": "21 pages", "journal-ref": "Frontiers in Physics 4, 17, (2016)", "doi": "10.3389/fphy.2016.00017", "report-no": null, "categories": "cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific community is becoming more and more interested in the research\nthat applies the mathematical formalism of quantum theory to model human\ndecision-making. In this paper, we provide the theoretical foundations of the\nquantum approach to cognition that we developed in Brussels. These foundations\nrest on the results of two decade studies on the axiomatic and\noperational-realistic approaches to the foundations of quantum physics. The\ndeep analogies between the foundations of physics and cognition lead us to\ninvestigate the validity of quantum theory as a general and unitary framework\nfor cognitive processes, and the empirical success of the Hilbert space models\nderived by such investigation provides a strong theoretical confirmation of\nthis validity. However, two situations in the cognitive realm, 'question order\neffects' and 'response replicability', indicate that even the Hilbert space\nframework could be insufficient to reproduce the collected data. This does not\nmean that the mentioned operational-realistic approach would be incorrect, but\nsimply that a larger class of measurements would be in force in human\ncognition, so that an extended quantum formalism may be needed to deal with all\nof them. As we will explain, the recently derived 'extended Bloch\nrepresentation' of quantum theory (and the associated 'general\ntension-reduction' model) precisely provides such extended formalism, while\nremaining within the same unitary interpretative framework.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 16:07:03 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Aerts", "Diederik", ""], ["de Bianchi", "Massimiliano Sassoli", ""], ["Sozzo", "Sandro", ""]]}, {"id": "1512.08811", "submitter": "Piotr Szwed PhD", "authors": "Piotr Szwed", "title": "Combining Fuzzy Cognitive Maps and Discrete Random Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an extension to the Fuzzy Cognitive Maps (FCMs) that\naims at aggregating a number of reasoning tasks into a one parallel run. The\ndescribed approach consists in replacing real-valued activation levels of\nconcepts (and further influence weights) by random variables. Such extension,\nfollowed by the implemented software tool, allows for determining ranges\nreached by concept activation levels, sensitivity analysis as well as\nstatistical analysis of multiple reasoning results. We replace multiplication\nand addition operators appearing in the FCM state equation by appropriate\nconvolutions applicable for discrete random variables. To make the model\ncomputationally feasible, it is further augmented with aggregation operations\nfor discrete random variables. We discuss four implemented aggregators, as well\nas we report results of preliminary tests.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 22:41:28 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Szwed", "Piotr", ""]]}, {"id": "1512.08849", "submitter": "Shuohang Wang", "authors": "Shuohang Wang and Jing Jiang", "title": "Learning Natural Language Inference with LSTM", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language inference (NLI) is a fundamentally important task in natural\nlanguage processing that has many applications. The recently released Stanford\nNatural Language Inference (SNLI) corpus has made it possible to develop and\nevaluate learning-centered methods such as deep neural networks for natural\nlanguage inference (NLI). In this paper, we propose a special long short-term\nmemory (LSTM) architecture for NLI. Our model builds on top of a recently\nproposed neural attention model for NLI but is based on a significantly\ndifferent idea. Instead of deriving sentence embeddings for the premise and the\nhypothesis to be used for classification, our solution uses a match-LSTM to\nperform word-by-word matching of the hypothesis with the premise. This LSTM is\nable to place more emphasis on important word-level matching results. In\nparticular, we observe that this LSTM remembers important mismatches that are\ncritical for predicting the contradiction or the neutral relationship label. On\nthe SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 05:02:53 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 11:54:29 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Wang", "Shuohang", ""], ["Jiang", "Jing", ""]]}, {"id": "1512.08899", "submitter": "Peter Sch\\\"uller", "authors": "Peter Sch\\\"uller", "title": "Modeling Variations of First-Order Horn Abduction in Answer Set\n  Programming", "comments": "Technical Report", "journal-ref": "Fundamenta Informaticae, vol. 149, no. 1-2, pp. 159-207, 2016", "doi": "10.3233/FI-2016-1446", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study abduction in First Order Horn logic theories where all atoms can be\nabduced and we are looking for preferred solutions with respect to three\nobjective functions: cardinality minimality, coherence, and weighted abduction.\nWe represent this reasoning problem in Answer Set Programming (ASP), in order\nto obtain a flexible framework for experimenting with global constraints and\nobjective functions, and to test the boundaries of what is possible with ASP.\nRealizing this problem in ASP is challenging as it requires value invention and\nequivalence between certain constants, because the Unique Names Assumption does\nnot hold in general. To permit reasoning in cyclic theories, we formally\ndescribe fine-grained variations of limiting Skolemization. We identify term\nequivalence as a main instantiation bottleneck, and improve the efficiency of\nour approach with on-demand constraints that were used to eliminate the same\nbottleneck in state-of-the-art solvers. We evaluate our approach experimentally\non the ACCEL benchmark for plan recognition in Natural Language Understanding.\nOur encodings are publicly available, modular, and our approach is more\nefficient than state-of-the-art solvers on the ACCEL benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 10:22:14 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 13:26:15 GMT"}, {"version": "v3", "created": "Sun, 30 Oct 2016 14:00:03 GMT"}, {"version": "v4", "created": "Wed, 31 Jan 2018 18:39:07 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Sch\u00fcller", "Peter", ""]]}, {"id": "1512.08949", "submitter": "Nihar Shah", "authors": "Nihar B. Shah and Martin J. Wainwright", "title": "Simple, Robust and Optimal Ranking from Pairwise Comparisons", "comments": "Changes in version 2: In addition to recovery in the exact and\n  Hamming metrics, v2 analyzes a general, abstract recovery criterion based on\n  a notion of \"allowed sets\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider data in the form of pairwise comparisons of n items, with the\ngoal of precisely identifying the top k items for some value of k < n, or\nalternatively, recovering a ranking of all the items. We analyze the Copeland\ncounting algorithm that ranks the items in order of the number of pairwise\ncomparisons won, and show it has three attractive features: (a) its\ncomputational efficiency leads to speed-ups of several orders of magnitude in\ncomputation time as compared to prior work; (b) it is robust in that\ntheoretical guarantees impose no conditions on the underlying matrix of\npairwise-comparison probabilities, in contrast to some prior work that applies\nonly to the BTL parametric model; and (c) it is an optimal method up to\nconstant factors, meaning that it achieves the information-theoretic limits for\nrecovering the top k-subset. We extend our results to obtain sharp guarantees\nfor approximate recovery under the Hamming distortion metric, and more\ngenerally, to any arbitrary error requirement that satisfies a simple and\nnatural monotonicity condition.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 14:25:23 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 03:52:36 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Shah", "Nihar B.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1512.08969", "submitter": "Josef Moudrik", "authors": "Josef Moud\\v{r}\\'ik, Petr Baudi\\v{s}, Roman Neruda", "title": "Evaluating Go Game Records for Prediction of Player Attributes", "comments": null, "journal-ref": "Computational Intelligence and Games (CIG), 2015 IEEE Conference\n  on , vol., no., pp.162-168, Aug. 31 2015-Sept. 2 2015", "doi": "10.1109/CIG.2015.7317909", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a way of extracting and aggregating per-move evaluations from sets\nof Go game records. The evaluations capture different aspects of the games such\nas played patterns or statistic of sente/gote sequences. Using machine learning\nalgorithms, the evaluations can be utilized to predict different relevant\ntarget variables. We apply this methodology to predict the strength and playing\nstyle of the player (e.g. territoriality or aggressivity) with good accuracy.\nWe propose a number of possible applications including aiding in Go study,\nseeding real-work ranks of internet players or tuning of Go-playing programs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 15:09:51 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Moud\u0159\u00edk", "Josef", ""], ["Baudi\u0161", "Petr", ""], ["Neruda", "Roman", ""]]}, {"id": "1512.09075", "submitter": "Philip Thomas", "authors": "Philip S. Thomas and Billy Okal", "title": "A Notation for Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper specifies a notation for Markov decision processes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 19:34:01 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 14:30:43 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Thomas", "Philip S.", ""], ["Okal", "Billy", ""]]}, {"id": "1512.09204", "submitter": "Weici Hu", "authors": "Weici Hu, Peter I. Frazier", "title": "Bayes-Optimal Effort Allocation in Crowdsourcing: Bounds and Index\n  Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider effort allocation in crowdsourcing, where we wish to assign\nlabeling tasks to imperfect homogeneous crowd workers to maximize overall\naccuracy in a continuous-time Bayesian setting, subject to budget and time\nconstraints. The Bayes-optimal policy for this problem is the solution to a\npartially observable Markov decision process, but the curse of dimensionality\nrenders the computation infeasible. Based on the Lagrangian Relaxation\ntechnique in Adelman & Mersereau (2008), we provide a computationally tractable\ninstance-specific upper bound on the value of this Bayes-optimal policy, which\ncan in turn be used to bound the optimality gap of any other sub-optimal\npolicy. In an approach similar in spirit to the Whittle index for restless\nmultiarmed bandits, we provide an index policy for effort allocation in\ncrowdsourcing and demonstrate numerically that it outperforms other stateof-\narts and performs close to optimal solution.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 03:09:33 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Hu", "Weici", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1512.09254", "submitter": "Josef Moudrik", "authors": "Josef Moud\\v{r}\\'ik, Roman Neruda", "title": "Evolving Non-linear Stacking Ensembles for Prediction of Go Player\n  Attributes", "comments": "Published in 2015 IEEE Symposium Series on Computational Intelligence", "journal-ref": null, "doi": "10.1109/SSCI.2015.235", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an application of non-linear stacking ensembles for\nprediction of Go player attributes. An evolutionary algorithm is used to form a\ndiverse ensemble of base learners, which are then aggregated by a stacking\nensemble. This methodology allows for an efficient prediction of different\nattributes of Go players from sets of their games. These attributes can be\nfairly general, in this work, we used the strength and style of the players.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 10:37:04 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Moud\u0159\u00edk", "Josef", ""], ["Neruda", "Roman", ""]]}, {"id": "1512.09354", "submitter": "Fabio D'Andreagiovanni", "authors": "Fabio D'Andreagiovanni, Fabian Mett, Jonad Pulaj", "title": "An (MI)LP-based Primal Heuristic for 3-Architecture Connected Facility\n  Location in Urban Access Network Design", "comments": "This is the authors' final version of the paper published in:\n  Squillero G., Burelli P. (eds), EvoApplications 2016: Applications of\n  Evolutionary Computation, LNCS 9597, pp. 283-298, 2016. DOI:\n  10.1007/978-3-319-31204-0_19. The final publication is available at Springer\n  via http://dx.doi.org/10.1007/978-3-319-31204-0_19", "journal-ref": null, "doi": "10.1007/978-3-319-31204-0_19", "report-no": null, "categories": "math.OC cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the 3-architecture Connected Facility Location Problem arising\nin the design of urban telecommunication access networks. We propose an\noriginal optimization model for the problem that includes additional variables\nand constraints to take into account wireless signal coverage. Since the\nproblem can prove challenging even for modern state-of-the art optimization\nsolvers, we propose to solve it by an original primal heuristic which combines\na probabilistic fixing procedure, guided by peculiar Linear Programming\nrelaxations, with an exact MIP heuristic, based on a very large neighborhood\nsearch. Computational experiments on a set of realistic instances show that our\nheuristic can find solutions associated with much lower optimality gaps than a\nstate-of-the-art solver.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 19:53:16 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 23:01:42 GMT"}, {"version": "v3", "created": "Thu, 27 Apr 2017 21:36:17 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["D'Andreagiovanni", "Fabio", ""], ["Mett", "Fabian", ""], ["Pulaj", "Jonad", ""]]}]